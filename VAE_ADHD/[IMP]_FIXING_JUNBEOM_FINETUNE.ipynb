{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33b1ab9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "168df326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3fd0ce",
   "metadata": {},
   "source": [
    "First, lookikng at why some thigns are missing and what `norm5` is and so on!! \n",
    "\n",
    "There are like 4 modes (classifier, encoder, CNN_flatten, barlow_encoder)\n",
    "\n",
    "1. During Finetuning : `classifier` mode is used\n",
    "```python\n",
    "    def __init__():\n",
    "        if self.mode == \"classifier\":\n",
    "            # Final batch norm\n",
    "            self.features.add_module('norm5', nn.BatchNorm3d(num_features))\n",
    "            # Linear layer\n",
    "            self.classifier = nn.Linear(num_features, num_classes)\n",
    "\n",
    "    def forward():\n",
    "        if self.mode == \"classifier\":\n",
    "            # Final batch norm\n",
    "            self.features.add_module('norm5', nn.BatchNorm3d(num_features))\n",
    "            # Linear layer\n",
    "            self.classifier = nn.Linear(num_features, num_classes)\n",
    "```\n",
    "\n",
    "2. BT pretraining : `barlow_encoder` was used\n",
    "```python\n",
    "    def __init__():\n",
    "        elif self.mode == \"barlow_encoder\":\n",
    "            #fc layer \n",
    "            self.fc = nn.Linear(num_features, num_classes)\n",
    "    def forward():\n",
    "        elif self.mode == \"barlow_encoder\":\n",
    "            out = F.relu(features, inplace=True)\n",
    "            out = F.adaptive_avg_pool3d(out, 1)\n",
    "            out = torch.flatten(out, 1)\n",
    "            out = self.fc(out) #passes through on fc layer \n",
    "            #fc : set to identity when doing pretraining\n",
    "        \n",
    "3. yAware pretraining : `encoder` was used \n",
    "```python\n",
    "\n",
    "    def __init__():\n",
    "        elif self.mode == \"encoder\":\n",
    "            self.hidden_representation = nn.Linear(num_features, 512)\n",
    "            self.head_projection = nn.Linear(512, 128)\n",
    "    def forward():\n",
    "        elif self.mode == \"encoder\":\n",
    "            out = F.relu(features, inplace=True)\n",
    "            out = F.adaptive_avg_pool3d(out, 1)\n",
    "            out = torch.flatten(out, 1)\n",
    "\n",
    "            out = self.hidden_representation(out)\n",
    "            out = F.relu(out, inplace=True)\n",
    "            out = self.head_projection(out)\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "Therefore, the `features.norm5.XXX` features that are only present in the new checkpoint is the one from the classifier!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211e8854",
   "metadata": {},
   "source": [
    "therefore, out of \n",
    "```\n",
    "{'classifier.bias', 'features.norm5.running_mean', 'features.norm5.weight', 'classifier.weight', 'features.norm5.bias', 'features.norm5.running_var', 'features.norm5.num_batches_tracked'}\n",
    "```\n",
    "we must freeze : all `features`, but unfreeze `classifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8635d8c",
   "metadata": {},
   "source": [
    "#### for reference, let's look at the evaluate.py code of the <mark> original BT code </mark>\n",
    "\n",
    "```python\n",
    "def main_worker(gpu.args):\n",
    "    ...\n",
    "    \n",
    "    model = models.resnet50().cuda(gpu)\n",
    "    state_dict = torch.load(args.pretrained, map_location='cpu')\n",
    "    missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)\n",
    "    assert missing_keys == ['fc.weight', 'fc.bias'] and unexpected_keys == []\n",
    "    model.fc.weight.data.normal_(mean=0.0, std=0.01)\n",
    "    model.fc.bias.data.zero_()\n",
    "    if args.weights == 'freeze':\n",
    "        model.requires_grad_(False)\n",
    "        model.fc.requires_grad_(True)\n",
    "    classifier_parameters, model_parameters = [], []\n",
    "    for name, param in model.named_parameters():\n",
    "        if name in {'fc.weight', 'fc.bias'}:\n",
    "            classifier_parameters.append(param)\n",
    "        else:\n",
    "            model_parameters.append(param)\n",
    "    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[gpu])\n",
    "    \n",
    "    \n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss().cuda(gpu)\n",
    "\n",
    "    param_groups = [dict(params=classifier_parameters, lr=args.lr_classifier)]\n",
    "    if args.weights == 'finetune':\n",
    "        param_groups.append(dict(params=model_parameters, lr=args.lr_backbone))\n",
    "    optimizer = optim.SGD(param_groups, 0, momentum=0.9, weight_decay=args.weight_decay)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, args.epochs)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "    for epoch in range(start_epoch, args.epochs):\n",
    "        # train\n",
    "        if args.weights == 'finetune':\n",
    "            model.train()\n",
    "        elif args.weights == 'freeze':\n",
    "            model.eval()\n",
    "        else:\n",
    "            assert False\n",
    "            \n",
    "            \n",
    "        # sanity check\n",
    "        if args.weights == 'freeze':\n",
    "            reference_state_dict = torch.load(args.pretrained, map_location='cpu')\n",
    "            model_state_dict = model.module.state_dict()\n",
    "            for k in reference_state_dict:\n",
    "                assert torch.equal(model_state_dict[k].cpu(), reference_state_dict[k]), k\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07a4224",
   "metadata": {},
   "source": [
    "got 66% memory footprint (33% memory footprint decrease)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5b7966",
   "metadata": {},
   "source": [
    "### 1. BT보기 \n",
    "BT : 위에서 보듯이, does not encode anyting and just does relu and flatening\n",
    "\n",
    "therefore, `org_list` below has nothing missing from the `new_list`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cfbb519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720 727\n",
      "====what the new ckpt has that the old one doesn't====\n",
      "{'classifier.weight', 'classifier.bias', 'features.norm5.num_batches_tracked', 'features.norm5.running_var', 'features.norm5.running_mean', 'features.norm5.bias', 'features.norm5.weight'}\n",
      "====what the old ckpt has that the new one doesn't\n",
      "set()\n",
      "tensor(0, device='cuda:0')\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([2])\n",
      "torch.Size([2, 1024])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org_pth = \"/scratch/connectome/dyhan316/VAE_ADHD/junbeom_finetuning/weights/resnet50_200.pth\"\n",
    "new_pth = \"/scratch/connectome/dyhan316/VAE_ADHD/junbeom_finetuning/ckpts/ADCN/resnet50_200/resnet50_200_f_10_model.pt\"\n",
    "\n",
    "org = torch.load(org_pth)\n",
    "new = torch.load(new_pth)\n",
    "\n",
    "#match newkeys to old keys\n",
    "\n",
    "org_list = [i for i in org.keys()]\n",
    "new_list = ['.'.join(i.split('.')[1:]) for i in new.keys()]\n",
    "\n",
    "#print(\"org list every 100\")\n",
    "#print(org_list[-10:])\n",
    "#print(\"new list every 100 \")\n",
    "#print(new_list[-10:])\n",
    "\n",
    "\n",
    "print(len(org_list), len(new_list))\n",
    "\n",
    "##let's look at what makes the two different\n",
    "print(\"====what the new ckpt has that the old one doesn't====\")\n",
    "print(set(new_list).difference(org_list))\n",
    "print(\"====what the old ckpt has that the new one doesn't\")\n",
    "print(set(org_list).difference(new_list))\n",
    "\n",
    "\n",
    "print(new['module.features.norm5.num_batches_tracked'])\n",
    "print(new['module.features.norm5.bias'].shape)\n",
    "print(new['module.features.norm5.weight'].shape)\n",
    "print(new['module.classifier.bias'].shape)\n",
    "print(new['module.classifier.weight'].shape)\n",
    "\n",
    "\n",
    "## comapring the values\n",
    "torch.equal(org['features.denseblock4.denselayer16.conv2.weight'], new['module.features.denseblock4.denselayer16.conv2.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "769b26eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======org=====\n",
      "classifier.weight: doesn't exist\n",
      "classifier.bias: doesn't exist\n",
      "cov0.weight :  tensor(0.0760, device='cuda:0')\n",
      "norm2.running_var :  tensor(0.4556, device='cuda:0')\n",
      "16.conv2.weight :  tensor(0.0242, device='cuda:0')\n",
      "tensor(133464, device='cuda:0')\n",
      "======new=====\n",
      "classifier.weight:  tensor(0.0179, device='cuda:0')\n",
      "classifier.bias:  tensor(0.0017, device='cuda:0')\n",
      "cov0.weight :  tensor(0.0760, device='cuda:0')\n",
      "norm2.running_var :  tensor(0.4556, device='cuda:0')\n",
      "16.conv2.weight :  tensor(0.0242, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "###BT, after changing the finetune.py\n",
    "\n",
    "##running variance : changes\n",
    "\n",
    "print(\"======org=====\")\n",
    "print(\"classifier.weight: doesn't exist\")\n",
    "print(\"classifier.bias: doesn't exist\")\n",
    "print(\"cov0.weight : \", torch.std(org['features.conv0.weight']))\n",
    "print(\"norm2.running_var : \",torch.std(org['features.denseblock4.denselayer16.norm2.running_var']))\n",
    "print(\"16.conv2.weight : \", torch.std(org['features.denseblock4.denselayer16.conv2.weight']))\n",
    "#number of batches tracked\n",
    "#print(org['features.denseblock1.denselayer1.norm1.num_batches_tracked']) \n",
    "\n",
    "\n",
    "\n",
    "print(\"======new=====\")\n",
    "print(\"classifier.weight: \", torch.std(new['module.classifier.weight']))\n",
    "print(\"classifier.bias: \", torch.std(new['module.classifier.bias']))\n",
    "print(\"cov0.weight : \", torch.std(new['module.features.conv0.weight']))\n",
    "print(\"norm2.running_var : \",torch.std(new['module.features.denseblock4.denselayer16.norm2.running_var']))\n",
    "print(\"16.conv2.weight : \", torch.std(new['module.features.denseblock4.denselayer16.conv2.weight']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1562ae74",
   "metadata": {},
   "source": [
    "#### 2.1. yAware준범쌤것 보기 (note that didn't use the full checkpoint, as oppose to 2.2\n",
    "**checking yAware code**\n",
    "code used \n",
    "\n",
    "```\n",
    "(VAE_3DCNN_older_MONAI)  dyhan316@node1:/scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/REAL_TRAINING_checkpoint_ABCD_batch_64_2_gpu_epoch_500_lambd_0.00102_DROP_LAST_BATCH$ python3 main.py --pretrained_path ./weights/y-Aware_Contrastive_MRI_epoch_99.pth --mode finetuning --train_num 100 --task_name AD/MCI --layer_control freeze --stratify balan --random_seed 0\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d169181a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "724 727\n",
      "====what the new ckpt has that the old one doesn't\n",
      "{'module.classifier.bias', 'module.features.norm5.weight', 'module.features.norm5.num_batches_tracked', 'module.features.norm5.bias', 'module.classifier.weight', 'module.features.norm5.running_mean', 'module.features.norm5.running_var'}\n",
      "====what the old ckpt has that the new one doesn't\n",
      "{'module.head_projection.bias', 'module.hidden_representation.weight', 'module.hidden_representation.bias', 'module.head_projection.weight'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]]],\n",
       "\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "\n",
       "        [[[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]]]], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org_pth = \"/scratch/connectome/dyhan316/VAE_ADHD/junbeom_finetuning/weights/y-Aware_Contrastive_MRI_epoch_99.pth\"\n",
    "new_pth = \"/scratch/connectome/dyhan316/VAE_ADHD/junbeom_finetuning/ckpts/ADMCI/y-Aware_Contrastive_MRI_epoch_99/y-Aware_Contrastive_MRI_epoch_99_f_10_model.pt\"\n",
    "\n",
    "\n",
    "org = torch.load(org_pth)['model'] #여기서는 model을 직접 지정해줘야함!\n",
    "new = torch.load(new_pth)\n",
    "\n",
    "#match newkeys to old keys\n",
    "\n",
    "org_list = [i for i in org.keys()]\n",
    "new_list = [i for i in new.keys()]\n",
    "\n",
    "print(len(org_list), len(new_list))\n",
    "\n",
    "##let's look at what makes the two different\n",
    "print(\"====what the new ckpt has that the old one doesn't\")\n",
    "print(set(new_list).difference(org_list))\n",
    "print(\"====what the old ckpt has that the new one doesn't\")\n",
    "print(set(org_list).difference(new_list))\n",
    "\n",
    "## comapring the values\n",
    "org['module.features.denseblock4.denselayer16.conv2.weight'] - new['module.features.denseblock4.denselayer16.conv2.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c401a32d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'module.features.no'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43morg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodule.features.no\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'module.features.no'"
     ]
    }
   ],
   "source": [
    "org['module.features.no']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7febc2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======org=====\n",
      "classifier.weight: doesn't exist\n",
      "classifier.bias: doesn't exist\n",
      "cov0.weight :  tensor(0.0765, device='cuda:0')\n",
      "norm2.running_var :  tensor(0.1937, device='cuda:0')\n",
      "16.conv2.weight :  tensor(0.0220, device='cuda:0')\n",
      "======new=====\n",
      "classifier.weight:  tensor(8.6719, device='cuda:0')\n",
      "classifier.bias:  tensor(4.2664, device='cuda:0')\n",
      "cov0.weight :  tensor(0.0765, device='cuda:0')\n",
      "norm2.running_var :  tensor(0.1937, device='cuda:0')\n",
      "16.conv2.weight :  tensor(0.0220, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "###BT, after changing the finetune.py\n",
    "\n",
    "##running variance : changes\n",
    "\n",
    "print(\"======org=====\")\n",
    "print(\"classifier.weight: doesn't exist\")\n",
    "print(\"classifier.bias: doesn't exist\")\n",
    "print(\"cov0.weight : \", torch.std(org['module.features.conv0.weight']))\n",
    "print(\"norm2.running_var : \",torch.std(org['module.features.denseblock4.denselayer16.norm2.running_var']))\n",
    "print(\"16.conv2.weight : \", torch.std(org['module.features.denseblock4.denselayer16.conv2.weight']))\n",
    "\n",
    "\n",
    "print(\"======new=====\")\n",
    "print(\"classifier.weight: \", torch.std(new['module.classifier.weight']))\n",
    "print(\"classifier.bias: \", torch.std(new['module.classifier.bias']))\n",
    "print(\"cov0.weight : \", torch.std(new['module.features.conv0.weight']))\n",
    "print(\"norm2.running_var : \",torch.std(new['module.features.denseblock4.denselayer16.norm2.running_var']))\n",
    "print(\"16.conv2.weight : \", torch.std(new['module.features.denseblock4.denselayer16.conv2.weight']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb4ab72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VAE_3DCNN_older_MONAI_USE_THIS",
   "language": "python",
   "name": "vae_3dcnn_older_monai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
