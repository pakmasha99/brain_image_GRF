"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 26595.600067138672, "training_acc": 48.0, "val_loss": 21555.57403564453, "val_acc": 72.0}
{"epoch": 1, "training_loss": 76002.64630126953, "training_acc": 72.0, "val_loss": 10677.859497070312, "val_acc": 28.0}
{"epoch": 2, "training_loss": 28709.012756347656, "training_acc": 28.0, "val_loss": 9761.99722290039, "val_acc": 72.0}
{"epoch": 3, "training_loss": 46750.187744140625, "training_acc": 72.0, "val_loss": 18557.12127685547, "val_acc": 72.0}
{"epoch": 4, "training_loss": 73629.29113769531, "training_acc": 72.0, "val_loss": 15799.668884277344, "val_acc": 72.0}
{"epoch": 5, "training_loss": 57148.195068359375, "training_acc": 72.0, "val_loss": 5532.00569152832, "val_acc": 72.0}
{"epoch": 6, "training_loss": 19089.4052734375, "training_acc": 60.0, "val_loss": 7554.5806884765625, "val_acc": 28.0}
{"epoch": 7, "training_loss": 20797.1847076416, "training_acc": 44.0, "val_loss": 2747.641372680664, "val_acc": 72.0}
{"epoch": 8, "training_loss": 10745.137145996094, "training_acc": 72.0, "val_loss": 272.91886806488037, "val_acc": 72.0}
{"epoch": 9, "training_loss": 10841.541137695312, "training_acc": 64.0, "val_loss": 6326.531982421875, "val_acc": 28.0}
{"epoch": 10, "training_loss": 18167.88946533203, "training_acc": 48.0, "val_loss": 5378.853225708008, "val_acc": 72.0}
{"epoch": 11, "training_loss": 22980.450073242188, "training_acc": 72.0, "val_loss": 5645.69091796875, "val_acc": 72.0}
{"epoch": 12, "training_loss": 19378.194732666016, "training_acc": 72.0, "val_loss": 1537.9889488220215, "val_acc": 28.0}
{"epoch": 13, "training_loss": 3961.039505004883, "training_acc": 48.0, "val_loss": 1143.1598663330078, "val_acc": 28.0}
{"epoch": 14, "training_loss": 4027.985549926758, "training_acc": 56.0, "val_loss": 4724.702072143555, "val_acc": 72.0}
{"epoch": 15, "training_loss": 19486.494567871094, "training_acc": 72.0, "val_loss": 3655.5091857910156, "val_acc": 72.0}
{"epoch": 16, "training_loss": 10101.667251586914, "training_acc": 72.0, "val_loss": 9274.068450927734, "val_acc": 28.0}
{"epoch": 17, "training_loss": 35564.880126953125, "training_acc": 28.0, "val_loss": 940.7924652099609, "val_acc": 72.0}
{"epoch": 18, "training_loss": 6819.725830078125, "training_acc": 72.0, "val_loss": 3464.075469970703, "val_acc": 72.0}
{"epoch": 19, "training_loss": 12027.025360107422, "training_acc": 72.0, "val_loss": 1187.6676559448242, "val_acc": 28.0}
{"epoch": 20, "training_loss": 6087.4058837890625, "training_acc": 36.0, "val_loss": 941.657543182373, "val_acc": 72.0}
{"epoch": 21, "training_loss": 4492.170074462891, "training_acc": 62.0, "val_loss": 1204.7184944152832, "val_acc": 72.0}
{"epoch": 22, "training_loss": 5013.6781005859375, "training_acc": 72.0, "val_loss": 123.47371578216553, "val_acc": 72.0}
{"epoch": 23, "training_loss": 11954.838562011719, "training_acc": 50.0, "val_loss": 2630.7395935058594, "val_acc": 28.0}
{"epoch": 24, "training_loss": 12036.506774902344, "training_acc": 46.0, "val_loss": 7708.510589599609, "val_acc": 72.0}
{"epoch": 25, "training_loss": 34320.16247558594, "training_acc": 72.0, "val_loss": 9337.329864501953, "val_acc": 72.0}
{"epoch": 26, "training_loss": 34985.14270019531, "training_acc": 72.0, "val_loss": 4492.698287963867, "val_acc": 72.0}
{"epoch": 27, "training_loss": 12892.563743591309, "training_acc": 72.0, "val_loss": 13239.266967773438, "val_acc": 28.0}
{"epoch": 28, "training_loss": 57156.11083984375, "training_acc": 28.0, "val_loss": 7095.412445068359, "val_acc": 28.0}
{"epoch": 29, "training_loss": 22782.623901367188, "training_acc": 46.0, "val_loss": 8642.811584472656, "val_acc": 72.0}
{"epoch": 30, "training_loss": 39637.994384765625, "training_acc": 72.0, "val_loss": 12518.380737304688, "val_acc": 72.0}
{"epoch": 31, "training_loss": 49127.130126953125, "training_acc": 72.0, "val_loss": 9751.193237304688, "val_acc": 72.0}
{"epoch": 32, "training_loss": 35719.127197265625, "training_acc": 72.0, "val_loss": 1593.8346862792969, "val_acc": 72.0}
{"epoch": 33, "training_loss": 20570.414672851562, "training_acc": 52.0, "val_loss": 12422.650146484375, "val_acc": 28.0}
{"epoch": 34, "training_loss": 38409.286193847656, "training_acc": 28.0, "val_loss": 4664.448928833008, "val_acc": 72.0}
{"epoch": 35, "training_loss": 28061.763427734375, "training_acc": 72.0, "val_loss": 11586.066436767578, "val_acc": 72.0}
{"epoch": 36, "training_loss": 46888.36999511719, "training_acc": 72.0, "val_loss": 11095.954132080078, "val_acc": 72.0}
{"epoch": 37, "training_loss": 40500.53894042969, "training_acc": 72.0, "val_loss": 5213.795852661133, "val_acc": 72.0}
{"epoch": 38, "training_loss": 14385.896631240845, "training_acc": 72.0, "val_loss": 14532.84912109375, "val_acc": 28.0}
{"epoch": 39, "training_loss": 62958.92431640625, "training_acc": 28.0, "val_loss": 11638.228607177734, "val_acc": 28.0}
{"epoch": 40, "training_loss": 33708.290466308594, "training_acc": 40.0, "val_loss": 5269.736099243164, "val_acc": 72.0}
{"epoch": 41, "training_loss": 24201.818359375, "training_acc": 72.0, "val_loss": 7479.713439941406, "val_acc": 72.0}
