"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 39835.72152709961, "training_acc": 40.0, "val_loss": 18917.266845703125, "val_acc": 72.0}
{"epoch": 1, "training_loss": 58713.71130371094, "training_acc": 72.0, "val_loss": 16559.628295898438, "val_acc": 28.0}
{"epoch": 2, "training_loss": 58483.99060058594, "training_acc": 28.0, "val_loss": 4538.879776000977, "val_acc": 72.0}
{"epoch": 3, "training_loss": 25947.535400390625, "training_acc": 72.0, "val_loss": 10922.110748291016, "val_acc": 72.0}
{"epoch": 4, "training_loss": 42368.26123046875, "training_acc": 72.0, "val_loss": 6267.9779052734375, "val_acc": 72.0}
{"epoch": 5, "training_loss": 17335.849605560303, "training_acc": 72.0, "val_loss": 18164.768981933594, "val_acc": 28.0}
{"epoch": 6, "training_loss": 75738.60278320312, "training_acc": 28.0, "val_loss": 9243.00537109375, "val_acc": 28.0}
{"epoch": 7, "training_loss": 29797.931396484375, "training_acc": 44.0, "val_loss": 9744.913482666016, "val_acc": 72.0}
{"epoch": 8, "training_loss": 42884.95935058594, "training_acc": 72.0, "val_loss": 14077.091979980469, "val_acc": 72.0}
{"epoch": 9, "training_loss": 55533.35107421875, "training_acc": 72.0, "val_loss": 11359.051513671875, "val_acc": 72.0}
{"epoch": 10, "training_loss": 40481.023681640625, "training_acc": 72.0, "val_loss": 2600.941848754883, "val_acc": 72.0}
{"epoch": 11, "training_loss": 19875.788330078125, "training_acc": 56.0, "val_loss": 12033.695220947266, "val_acc": 28.0}
{"epoch": 12, "training_loss": 36739.76190185547, "training_acc": 28.0, "val_loss": 4693.373107910156, "val_acc": 72.0}
{"epoch": 13, "training_loss": 25918.111328125, "training_acc": 72.0, "val_loss": 11785.225677490234, "val_acc": 72.0}
{"epoch": 14, "training_loss": 47747.32458496094, "training_acc": 72.0, "val_loss": 11710.921478271484, "val_acc": 72.0}
{"epoch": 15, "training_loss": 43730.83947753906, "training_acc": 72.0, "val_loss": 6130.513381958008, "val_acc": 72.0}
{"epoch": 16, "training_loss": 18935.93357849121, "training_acc": 72.0, "val_loss": 11094.947052001953, "val_acc": 28.0}
{"epoch": 17, "training_loss": 48740.074462890625, "training_acc": 28.0, "val_loss": 6433.221435546875, "val_acc": 28.0}
{"epoch": 18, "training_loss": 18931.016235351562, "training_acc": 50.0, "val_loss": 8045.361328125, "val_acc": 72.0}
{"epoch": 19, "training_loss": 34453.37902832031, "training_acc": 72.0, "val_loss": 11504.911804199219, "val_acc": 72.0}
{"epoch": 20, "training_loss": 45241.59948730469, "training_acc": 72.0, "val_loss": 9104.576110839844, "val_acc": 72.0}
{"epoch": 21, "training_loss": 31053.67041015625, "training_acc": 72.0, "val_loss": 1451.413631439209, "val_acc": 72.0}
{"epoch": 22, "training_loss": 20431.53369140625, "training_acc": 54.0, "val_loss": 13827.157592773438, "val_acc": 28.0}
{"epoch": 23, "training_loss": 44854.45153808594, "training_acc": 28.0, "val_loss": 3506.857681274414, "val_acc": 72.0}
{"epoch": 24, "training_loss": 19280.427001953125, "training_acc": 72.0, "val_loss": 10132.99789428711, "val_acc": 72.0}
{"epoch": 25, "training_loss": 41307.5595703125, "training_acc": 72.0, "val_loss": 10160.63461303711, "val_acc": 72.0}
{"epoch": 26, "training_loss": 37234.56799316406, "training_acc": 72.0, "val_loss": 4748.759841918945, "val_acc": 72.0}
{"epoch": 27, "training_loss": 12255.991819381714, "training_acc": 72.0, "val_loss": 14352.903747558594, "val_acc": 28.0}
{"epoch": 28, "training_loss": 62141.16650390625, "training_acc": 28.0, "val_loss": 10904.47998046875, "val_acc": 28.0}
{"epoch": 29, "training_loss": 30331.741485595703, "training_acc": 44.0, "val_loss": 5728.424835205078, "val_acc": 72.0}
{"epoch": 30, "training_loss": 26772.262329101562, "training_acc": 72.0, "val_loss": 8424.166107177734, "val_acc": 72.0}
{"epoch": 31, "training_loss": 31920.985473632812, "training_acc": 72.0, "val_loss": 4851.39045715332, "val_acc": 72.0}
{"epoch": 32, "training_loss": 13387.269821166992, "training_acc": 72.0, "val_loss": 10032.791137695312, "val_acc": 28.0}
{"epoch": 33, "training_loss": 44579.518798828125, "training_acc": 28.0, "val_loss": 4061.600875854492, "val_acc": 28.0}
{"epoch": 34, "training_loss": 23050.760375976562, "training_acc": 34.0, "val_loss": 9611.785125732422, "val_acc": 72.0}
{"epoch": 35, "training_loss": 41687.15026855469, "training_acc": 72.0, "val_loss": 12709.54818725586, "val_acc": 72.0}
{"epoch": 36, "training_loss": 49566.05847167969, "training_acc": 72.0, "val_loss": 9792.723083496094, "val_acc": 72.0}
{"epoch": 37, "training_loss": 34973.47344970703, "training_acc": 72.0, "val_loss": 1897.2850799560547, "val_acc": 72.0}
{"epoch": 38, "training_loss": 15704.387451171875, "training_acc": 60.0, "val_loss": 11756.607055664062, "val_acc": 28.0}
{"epoch": 39, "training_loss": 37964.18615722656, "training_acc": 28.0, "val_loss": 4003.01513671875, "val_acc": 72.0}
{"epoch": 40, "training_loss": 21812.207275390625, "training_acc": 72.0, "val_loss": 9651.284790039062, "val_acc": 72.0}
