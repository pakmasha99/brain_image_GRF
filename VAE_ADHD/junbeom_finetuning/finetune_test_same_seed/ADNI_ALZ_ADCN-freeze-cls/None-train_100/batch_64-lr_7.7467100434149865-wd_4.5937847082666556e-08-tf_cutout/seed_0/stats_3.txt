"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 23302.763549804688, "training_acc": 50.0, "val_loss": 20778.497314453125, "val_acc": 72.0}
{"epoch": 1, "training_loss": 71990.08197021484, "training_acc": 72.0, "val_loss": 17964.61639404297, "val_acc": 28.0}
{"epoch": 2, "training_loss": 57344.56994628906, "training_acc": 28.0, "val_loss": 7337.392425537109, "val_acc": 72.0}
{"epoch": 3, "training_loss": 45400.16943359375, "training_acc": 72.0, "val_loss": 15913.604736328125, "val_acc": 72.0}
{"epoch": 4, "training_loss": 61924.890380859375, "training_acc": 72.0, "val_loss": 11198.595428466797, "val_acc": 72.0}
{"epoch": 5, "training_loss": 37605.637451171875, "training_acc": 72.0, "val_loss": 3669.7025299072266, "val_acc": 28.0}
{"epoch": 6, "training_loss": 16172.295959472656, "training_acc": 28.0, "val_loss": 2883.4657669067383, "val_acc": 72.0}
{"epoch": 7, "training_loss": 12500.121643066406, "training_acc": 72.0, "val_loss": 5376.26953125, "val_acc": 72.0}
{"epoch": 8, "training_loss": 19890.71014404297, "training_acc": 72.0, "val_loss": 1442.4616813659668, "val_acc": 72.0}
{"epoch": 9, "training_loss": 14813.384521484375, "training_acc": 56.0, "val_loss": 6000.417327880859, "val_acc": 28.0}
{"epoch": 10, "training_loss": 17949.348358154297, "training_acc": 46.0, "val_loss": 5033.306503295898, "val_acc": 72.0}
{"epoch": 11, "training_loss": 21577.683715820312, "training_acc": 72.0, "val_loss": 5001.65901184082, "val_acc": 72.0}
{"epoch": 12, "training_loss": 16415.81787109375, "training_acc": 72.0, "val_loss": 3596.173858642578, "val_acc": 28.0}
{"epoch": 13, "training_loss": 11433.228302001953, "training_acc": 28.0, "val_loss": 3849.2691040039062, "val_acc": 72.0}
{"epoch": 14, "training_loss": 17176.463928222656, "training_acc": 72.0, "val_loss": 6903.216552734375, "val_acc": 72.0}
{"epoch": 15, "training_loss": 26630.023681640625, "training_acc": 72.0, "val_loss": 4057.286834716797, "val_acc": 72.0}
{"epoch": 16, "training_loss": 12896.50662612915, "training_acc": 72.0, "val_loss": 10987.590789794922, "val_acc": 28.0}
{"epoch": 17, "training_loss": 42982.37744140625, "training_acc": 28.0, "val_loss": 735.7770442962646, "val_acc": 28.0}
{"epoch": 18, "training_loss": 10357.319213867188, "training_acc": 46.0, "val_loss": 11589.161682128906, "val_acc": 72.0}
{"epoch": 19, "training_loss": 50627.52685546875, "training_acc": 72.0, "val_loss": 15824.177551269531, "val_acc": 72.0}
{"epoch": 20, "training_loss": 62738.94580078125, "training_acc": 72.0, "val_loss": 13576.243591308594, "val_acc": 72.0}
{"epoch": 21, "training_loss": 49088.39880371094, "training_acc": 72.0, "val_loss": 5969.050598144531, "val_acc": 72.0}
{"epoch": 22, "training_loss": 15073.561782836914, "training_acc": 72.0, "val_loss": 16351.091003417969, "val_acc": 28.0}
{"epoch": 23, "training_loss": 73088.19311523438, "training_acc": 28.0, "val_loss": 16462.332153320312, "val_acc": 28.0}
{"epoch": 24, "training_loss": 44360.4287109375, "training_acc": 28.0, "val_loss": 7680.3863525390625, "val_acc": 72.0}
{"epoch": 25, "training_loss": 38878.46838378906, "training_acc": 72.0, "val_loss": 18510.118103027344, "val_acc": 72.0}
{"epoch": 26, "training_loss": 79280.61376953125, "training_acc": 72.0, "val_loss": 22006.268310546875, "val_acc": 72.0}
{"epoch": 27, "training_loss": 87094.00122070312, "training_acc": 72.0, "val_loss": 18832.725524902344, "val_acc": 72.0}
{"epoch": 28, "training_loss": 71479.814453125, "training_acc": 72.0, "val_loss": 11226.101684570312, "val_acc": 72.0}
{"epoch": 29, "training_loss": 38596.76446533203, "training_acc": 72.0, "val_loss": 1142.521095275879, "val_acc": 28.0}
{"epoch": 30, "training_loss": 11309.381774902344, "training_acc": 28.0, "val_loss": 175.9243130683899, "val_acc": 72.0}
{"epoch": 31, "training_loss": 2382.4241485595703, "training_acc": 72.0, "val_loss": 27.61722505092621, "val_acc": 68.0}
{"epoch": 32, "training_loss": 9230.946678161621, "training_acc": 44.0, "val_loss": 796.3123798370361, "val_acc": 72.0}
{"epoch": 33, "training_loss": 4989.9930419921875, "training_acc": 72.0, "val_loss": 1412.48140335083, "val_acc": 72.0}
{"epoch": 34, "training_loss": 9523.2255859375, "training_acc": 44.0, "val_loss": 1349.881935119629, "val_acc": 72.0}
{"epoch": 35, "training_loss": 6618.182769775391, "training_acc": 72.0, "val_loss": 1288.9850616455078, "val_acc": 72.0}
{"epoch": 36, "training_loss": 6771.755645751953, "training_acc": 56.0, "val_loss": 772.9191780090332, "val_acc": 72.0}
{"epoch": 37, "training_loss": 3079.873565673828, "training_acc": 72.0, "val_loss": 1175.144100189209, "val_acc": 28.0}
{"epoch": 38, "training_loss": 8584.4208984375, "training_acc": 34.0, "val_loss": 2988.9162063598633, "val_acc": 72.0}
{"epoch": 39, "training_loss": 10533.209991455078, "training_acc": 72.0, "val_loss": 652.288818359375, "val_acc": 28.0}
{"epoch": 40, "training_loss": 3071.4649810791016, "training_acc": 46.0, "val_loss": 1121.750259399414, "val_acc": 72.0}
{"epoch": 41, "training_loss": 2597.7491664886475, "training_acc": 62.0, "val_loss": 2021.3787078857422, "val_acc": 72.0}
{"epoch": 42, "training_loss": 9113.04800415039, "training_acc": 72.0, "val_loss": 1495.6801414489746, "val_acc": 72.0}
{"epoch": 43, "training_loss": 8088.965637207031, "training_acc": 54.0, "val_loss": 465.69533348083496, "val_acc": 72.0}
{"epoch": 44, "training_loss": 2018.407814025879, "training_acc": 72.0, "val_loss": 2608.720588684082, "val_acc": 28.0}
{"epoch": 45, "training_loss": 7595.846405029297, "training_acc": 48.0, "val_loss": 1772.5275039672852, "val_acc": 72.0}
{"epoch": 46, "training_loss": 5927.834571838379, "training_acc": 72.0, "val_loss": 2956.974983215332, "val_acc": 28.0}
{"epoch": 47, "training_loss": 8541.943794250488, "training_acc": 44.0, "val_loss": 891.070556640625, "val_acc": 72.0}
{"epoch": 48, "training_loss": 3523.2554473876953, "training_acc": 56.0, "val_loss": 2232.406997680664, "val_acc": 72.0}
{"epoch": 49, "training_loss": 10167.880279541016, "training_acc": 72.0, "val_loss": 2497.7121353149414, "val_acc": 72.0}
{"epoch": 50, "training_loss": 7465.266841888428, "training_acc": 72.0, "val_loss": 7159.819030761719, "val_acc": 28.0}
