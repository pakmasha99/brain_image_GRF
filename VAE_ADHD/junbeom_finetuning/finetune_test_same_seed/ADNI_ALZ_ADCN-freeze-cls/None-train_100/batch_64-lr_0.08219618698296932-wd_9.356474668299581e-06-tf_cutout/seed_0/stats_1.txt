"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 360.7846031188965, "training_acc": 72.0, "val_loss": 200.4281759262085, "val_acc": 72.0}
{"epoch": 1, "training_loss": 629.3704719543457, "training_acc": 72.0, "val_loss": 258.032488822937, "val_acc": 28.0}
{"epoch": 2, "training_loss": 923.5309467315674, "training_acc": 28.0, "val_loss": 30.758705735206604, "val_acc": 72.0}
{"epoch": 3, "training_loss": 229.58501815795898, "training_acc": 72.0, "val_loss": 112.17854022979736, "val_acc": 72.0}
{"epoch": 4, "training_loss": 433.3520793914795, "training_acc": 72.0, "val_loss": 66.29306077957153, "val_acc": 72.0}
{"epoch": 5, "training_loss": 186.01440501213074, "training_acc": 72.0, "val_loss": 129.36569452285767, "val_acc": 28.0}
{"epoch": 6, "training_loss": 480.4377326965332, "training_acc": 28.0, "val_loss": 18.651454150676727, "val_acc": 72.0}
{"epoch": 7, "training_loss": 136.72582530975342, "training_acc": 72.0, "val_loss": 64.5189642906189, "val_acc": 72.0}
{"epoch": 8, "training_loss": 242.49255847930908, "training_acc": 72.0, "val_loss": 24.814486503601074, "val_acc": 72.0}
{"epoch": 9, "training_loss": 143.81204319000244, "training_acc": 56.0, "val_loss": 39.79898691177368, "val_acc": 28.0}
{"epoch": 10, "training_loss": 145.32672548294067, "training_acc": 44.0, "val_loss": 53.462159633636475, "val_acc": 72.0}
{"epoch": 11, "training_loss": 217.98949146270752, "training_acc": 72.0, "val_loss": 41.01809561252594, "val_acc": 72.0}
{"epoch": 12, "training_loss": 123.08748745918274, "training_acc": 72.0, "val_loss": 66.12759232521057, "val_acc": 28.0}
{"epoch": 13, "training_loss": 220.61634969711304, "training_acc": 28.0, "val_loss": 33.56626033782959, "val_acc": 72.0}
{"epoch": 14, "training_loss": 178.67949390411377, "training_acc": 72.0, "val_loss": 62.15654015541077, "val_acc": 72.0}
{"epoch": 15, "training_loss": 227.3611216545105, "training_acc": 72.0, "val_loss": 18.60143691301346, "val_acc": 72.0}
{"epoch": 16, "training_loss": 196.8610439300537, "training_acc": 44.0, "val_loss": 44.9133038520813, "val_acc": 28.0}
{"epoch": 17, "training_loss": 221.8008108139038, "training_acc": 30.0, "val_loss": 58.750903606414795, "val_acc": 72.0}
{"epoch": 18, "training_loss": 238.25715827941895, "training_acc": 72.0, "val_loss": 48.872411251068115, "val_acc": 72.0}
{"epoch": 19, "training_loss": 164.20226764678955, "training_acc": 72.0, "val_loss": 44.84313130378723, "val_acc": 28.0}
{"epoch": 20, "training_loss": 167.5136866569519, "training_acc": 28.0, "val_loss": 25.80432891845703, "val_acc": 72.0}
{"epoch": 21, "training_loss": 149.88920402526855, "training_acc": 72.0, "val_loss": 47.87439703941345, "val_acc": 72.0}
{"epoch": 22, "training_loss": 165.54128122329712, "training_acc": 72.0, "val_loss": 17.216597497463226, "val_acc": 28.0}
{"epoch": 23, "training_loss": 120.25106048583984, "training_acc": 47.0, "val_loss": 14.620202779769897, "val_acc": 72.0}
{"epoch": 24, "training_loss": 67.49027013778687, "training_acc": 72.0, "val_loss": 34.96441841125488, "val_acc": 72.0}
{"epoch": 25, "training_loss": 128.79233288764954, "training_acc": 72.0, "val_loss": 15.061675012111664, "val_acc": 76.0}
{"epoch": 26, "training_loss": 74.99864196777344, "training_acc": 60.0, "val_loss": 14.589415490627289, "val_acc": 72.0}
{"epoch": 27, "training_loss": 71.05952143669128, "training_acc": 72.0, "val_loss": 19.545018672943115, "val_acc": 72.0}
{"epoch": 28, "training_loss": 72.84996843338013, "training_acc": 72.0, "val_loss": 20.187915861606598, "val_acc": 28.0}
{"epoch": 29, "training_loss": 74.58869171142578, "training_acc": 44.0, "val_loss": 19.648408889770508, "val_acc": 72.0}
{"epoch": 30, "training_loss": 74.60717582702637, "training_acc": 72.0, "val_loss": 16.9290691614151, "val_acc": 28.0}
{"epoch": 31, "training_loss": 64.30489683151245, "training_acc": 74.0, "val_loss": 16.143345832824707, "val_acc": 72.0}
{"epoch": 32, "training_loss": 63.34170413017273, "training_acc": 72.0, "val_loss": 15.11622816324234, "val_acc": 52.0}
{"epoch": 33, "training_loss": 59.9960663318634, "training_acc": 72.0, "val_loss": 15.646736323833466, "val_acc": 72.0}
{"epoch": 34, "training_loss": 62.423702239990234, "training_acc": 72.0, "val_loss": 15.849168598651886, "val_acc": 28.0}
{"epoch": 35, "training_loss": 69.27531290054321, "training_acc": 72.0, "val_loss": 16.456034779548645, "val_acc": 28.0}
{"epoch": 36, "training_loss": 65.10247659683228, "training_acc": 73.0, "val_loss": 14.630040526390076, "val_acc": 72.0}
{"epoch": 37, "training_loss": 59.23303484916687, "training_acc": 72.0, "val_loss": 17.097029089927673, "val_acc": 28.0}
{"epoch": 38, "training_loss": 65.86539387702942, "training_acc": 63.0, "val_loss": 15.428978204727173, "val_acc": 72.0}
{"epoch": 39, "training_loss": 67.72311329841614, "training_acc": 72.0, "val_loss": 15.832014381885529, "val_acc": 72.0}
{"epoch": 40, "training_loss": 63.10099792480469, "training_acc": 72.0, "val_loss": 14.70012217760086, "val_acc": 76.0}
{"epoch": 41, "training_loss": 67.4836106300354, "training_acc": 48.0, "val_loss": 21.485288441181183, "val_acc": 72.0}
{"epoch": 42, "training_loss": 94.91550636291504, "training_acc": 72.0, "val_loss": 14.737862348556519, "val_acc": 72.0}
{"epoch": 43, "training_loss": 99.26299238204956, "training_acc": 52.0, "val_loss": 15.984256565570831, "val_acc": 72.0}
{"epoch": 44, "training_loss": 73.041508436203, "training_acc": 72.0, "val_loss": 21.65413200855255, "val_acc": 72.0}
{"epoch": 45, "training_loss": 75.94035363197327, "training_acc": 72.0, "val_loss": 23.667022585868835, "val_acc": 28.0}
