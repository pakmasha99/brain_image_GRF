"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 290.922643661499, "training_acc": 50.0, "val_loss": 235.72630882263184, "val_acc": 72.0}
{"epoch": 1, "training_loss": 763.2625675201416, "training_acc": 72.0, "val_loss": 103.45379114151001, "val_acc": 28.0}
{"epoch": 2, "training_loss": 303.57746982574463, "training_acc": 28.0, "val_loss": 84.54046249389648, "val_acc": 72.0}
{"epoch": 3, "training_loss": 416.0374946594238, "training_acc": 72.0, "val_loss": 143.0420160293579, "val_acc": 72.0}
{"epoch": 4, "training_loss": 539.4575576782227, "training_acc": 72.0, "val_loss": 74.10589456558228, "val_acc": 72.0}
{"epoch": 5, "training_loss": 183.43042969703674, "training_acc": 72.0, "val_loss": 161.88119649887085, "val_acc": 28.0}
{"epoch": 6, "training_loss": 632.9103107452393, "training_acc": 28.0, "val_loss": 16.972503066062927, "val_acc": 28.0}
{"epoch": 7, "training_loss": 153.46290397644043, "training_acc": 72.0, "val_loss": 109.81959104537964, "val_acc": 72.0}
{"epoch": 8, "training_loss": 456.1461429595947, "training_acc": 72.0, "val_loss": 117.41223335266113, "val_acc": 72.0}
{"epoch": 9, "training_loss": 428.3031349182129, "training_acc": 72.0, "val_loss": 48.52539598941803, "val_acc": 72.0}
{"epoch": 10, "training_loss": 142.23973941802979, "training_acc": 62.0, "val_loss": 76.57607793807983, "val_acc": 28.0}
{"epoch": 11, "training_loss": 217.6811065673828, "training_acc": 28.0, "val_loss": 44.05454099178314, "val_acc": 72.0}
{"epoch": 12, "training_loss": 197.56863403320312, "training_acc": 72.0, "val_loss": 66.07732772827148, "val_acc": 72.0}
{"epoch": 13, "training_loss": 238.38318634033203, "training_acc": 72.0, "val_loss": 18.231818079948425, "val_acc": 72.0}
{"epoch": 14, "training_loss": 155.58190059661865, "training_acc": 56.0, "val_loss": 57.02677369117737, "val_acc": 28.0}
{"epoch": 15, "training_loss": 185.4433274269104, "training_acc": 44.0, "val_loss": 55.44280409812927, "val_acc": 72.0}
{"epoch": 16, "training_loss": 229.07793807983398, "training_acc": 72.0, "val_loss": 51.97092294692993, "val_acc": 72.0}
{"epoch": 17, "training_loss": 169.0759735107422, "training_acc": 72.0, "val_loss": 42.951762676239014, "val_acc": 28.0}
{"epoch": 18, "training_loss": 166.03843641281128, "training_acc": 28.0, "val_loss": 28.619274497032166, "val_acc": 72.0}
{"epoch": 19, "training_loss": 148.72062492370605, "training_acc": 72.0, "val_loss": 52.72746682167053, "val_acc": 72.0}
{"epoch": 20, "training_loss": 187.89271068572998, "training_acc": 72.0, "val_loss": 14.879533648490906, "val_acc": 72.0}
{"epoch": 21, "training_loss": 99.64582777023315, "training_acc": 60.0, "val_loss": 17.564930021762848, "val_acc": 28.0}
{"epoch": 22, "training_loss": 87.55559015274048, "training_acc": 53.0, "val_loss": 41.0299152135849, "val_acc": 72.0}
{"epoch": 23, "training_loss": 152.17744755744934, "training_acc": 72.0, "val_loss": 14.948134124279022, "val_acc": 72.0}
{"epoch": 24, "training_loss": 116.42811393737793, "training_acc": 50.0, "val_loss": 14.756838977336884, "val_acc": 72.0}
{"epoch": 25, "training_loss": 93.28073596954346, "training_acc": 72.0, "val_loss": 32.7484667301178, "val_acc": 72.0}
{"epoch": 26, "training_loss": 106.3826117515564, "training_acc": 72.0, "val_loss": 36.418142914772034, "val_acc": 28.0}
{"epoch": 27, "training_loss": 126.80440068244934, "training_acc": 28.0, "val_loss": 24.571864306926727, "val_acc": 72.0}
{"epoch": 28, "training_loss": 100.81706190109253, "training_acc": 72.0, "val_loss": 25.35051703453064, "val_acc": 72.0}
{"epoch": 29, "training_loss": 88.89058995246887, "training_acc": 72.0, "val_loss": 25.921571254730225, "val_acc": 28.0}
{"epoch": 30, "training_loss": 92.86544871330261, "training_acc": 42.0, "val_loss": 22.300145030021667, "val_acc": 72.0}
{"epoch": 31, "training_loss": 82.266632437706, "training_acc": 72.0, "val_loss": 17.401717603206635, "val_acc": 28.0}
{"epoch": 32, "training_loss": 69.46445775032043, "training_acc": 54.0, "val_loss": 14.811001718044281, "val_acc": 72.0}
{"epoch": 33, "training_loss": 72.34575748443604, "training_acc": 44.0, "val_loss": 20.941969752311707, "val_acc": 72.0}
{"epoch": 34, "training_loss": 82.13806223869324, "training_acc": 72.0, "val_loss": 16.31731688976288, "val_acc": 72.0}
{"epoch": 35, "training_loss": 62.98690438270569, "training_acc": 62.0, "val_loss": 16.447237133979797, "val_acc": 28.0}
{"epoch": 36, "training_loss": 74.07469582557678, "training_acc": 76.0, "val_loss": 17.95039474964142, "val_acc": 72.0}
{"epoch": 37, "training_loss": 61.29961323738098, "training_acc": 72.0, "val_loss": 23.76779168844223, "val_acc": 28.0}
{"epoch": 38, "training_loss": 88.16149592399597, "training_acc": 42.0, "val_loss": 21.684855222702026, "val_acc": 72.0}
{"epoch": 39, "training_loss": 74.28111004829407, "training_acc": 72.0, "val_loss": 25.518804788589478, "val_acc": 28.0}
{"epoch": 40, "training_loss": 95.27417850494385, "training_acc": 38.0, "val_loss": 17.764364182949066, "val_acc": 72.0}
{"epoch": 41, "training_loss": 65.0002052783966, "training_acc": 72.0, "val_loss": 17.34244078397751, "val_acc": 28.0}
{"epoch": 42, "training_loss": 60.91228544712067, "training_acc": 66.0, "val_loss": 20.56301385164261, "val_acc": 72.0}
{"epoch": 43, "training_loss": 75.65447974205017, "training_acc": 72.0, "val_loss": 21.247631311416626, "val_acc": 28.0}
