"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 10406.472602844238, "training_acc": 42.0, "val_loss": 5516.306686401367, "val_acc": 72.0}
{"epoch": 1, "training_loss": 17256.853240966797, "training_acc": 72.0, "val_loss": 4438.52424621582, "val_acc": 28.0}
{"epoch": 2, "training_loss": 15323.19155883789, "training_acc": 28.0, "val_loss": 1508.297061920166, "val_acc": 72.0}
{"epoch": 3, "training_loss": 7775.293731689453, "training_acc": 72.0, "val_loss": 3370.548629760742, "val_acc": 72.0}
{"epoch": 4, "training_loss": 13093.248229980469, "training_acc": 72.0, "val_loss": 2155.754852294922, "val_acc": 72.0}
{"epoch": 5, "training_loss": 6439.618591308594, "training_acc": 72.0, "val_loss": 3518.484115600586, "val_acc": 28.0}
{"epoch": 6, "training_loss": 14166.996032714844, "training_acc": 28.0, "val_loss": 385.79277992248535, "val_acc": 28.0}
{"epoch": 7, "training_loss": 2670.4877166748047, "training_acc": 52.0, "val_loss": 3768.7705993652344, "val_acc": 72.0}
{"epoch": 8, "training_loss": 16534.01953125, "training_acc": 72.0, "val_loss": 5226.169204711914, "val_acc": 72.0}
{"epoch": 9, "training_loss": 20708.318481445312, "training_acc": 72.0, "val_loss": 4511.033248901367, "val_acc": 72.0}
{"epoch": 10, "training_loss": 16435.173828125, "training_acc": 72.0, "val_loss": 2238.78173828125, "val_acc": 72.0}
{"epoch": 11, "training_loss": 6648.5712966918945, "training_acc": 72.0, "val_loss": 4149.660491943359, "val_acc": 28.0}
{"epoch": 12, "training_loss": 18652.039001464844, "training_acc": 28.0, "val_loss": 3802.286148071289, "val_acc": 28.0}
{"epoch": 13, "training_loss": 10043.562854766846, "training_acc": 44.0, "val_loss": 1347.3599433898926, "val_acc": 72.0}
{"epoch": 14, "training_loss": 6443.995361328125, "training_acc": 72.0, "val_loss": 1998.5347747802734, "val_acc": 72.0}
{"epoch": 15, "training_loss": 7375.484039306641, "training_acc": 72.0, "val_loss": 768.7325477600098, "val_acc": 72.0}
{"epoch": 16, "training_loss": 3599.802764892578, "training_acc": 56.0, "val_loss": 957.8057289123535, "val_acc": 28.0}
{"epoch": 17, "training_loss": 3167.319725036621, "training_acc": 48.0, "val_loss": 1381.1874389648438, "val_acc": 72.0}
{"epoch": 18, "training_loss": 5699.257583618164, "training_acc": 72.0, "val_loss": 1260.252857208252, "val_acc": 72.0}
{"epoch": 19, "training_loss": 3921.225387573242, "training_acc": 72.0, "val_loss": 1140.4706001281738, "val_acc": 28.0}
{"epoch": 20, "training_loss": 4537.661407470703, "training_acc": 28.0, "val_loss": 839.4526481628418, "val_acc": 72.0}
{"epoch": 21, "training_loss": 4145.116729736328, "training_acc": 72.0, "val_loss": 1827.393913269043, "val_acc": 72.0}
{"epoch": 22, "training_loss": 7070.6781005859375, "training_acc": 72.0, "val_loss": 1100.5233764648438, "val_acc": 72.0}
{"epoch": 23, "training_loss": 3207.680917739868, "training_acc": 72.0, "val_loss": 2687.904739379883, "val_acc": 28.0}
{"epoch": 24, "training_loss": 10507.232849121094, "training_acc": 28.0, "val_loss": 41.16296172142029, "val_acc": 72.0}
{"epoch": 25, "training_loss": 368.6193675994873, "training_acc": 72.0, "val_loss": 564.8474216461182, "val_acc": 72.0}
{"epoch": 26, "training_loss": 1854.1611976623535, "training_acc": 72.0, "val_loss": 977.9561996459961, "val_acc": 28.0}
{"epoch": 27, "training_loss": 2446.025415420532, "training_acc": 28.0, "val_loss": 1516.4076805114746, "val_acc": 72.0}
{"epoch": 28, "training_loss": 7694.966979980469, "training_acc": 72.0, "val_loss": 3044.661331176758, "val_acc": 72.0}
{"epoch": 29, "training_loss": 12208.185180664062, "training_acc": 72.0, "val_loss": 2677.8079986572266, "val_acc": 72.0}
{"epoch": 30, "training_loss": 9678.796142578125, "training_acc": 72.0, "val_loss": 779.5044898986816, "val_acc": 72.0}
{"epoch": 31, "training_loss": 3011.039581298828, "training_acc": 66.0, "val_loss": 2074.623489379883, "val_acc": 28.0}
{"epoch": 32, "training_loss": 5611.131798744202, "training_acc": 40.0, "val_loss": 327.64952182769775, "val_acc": 72.0}
{"epoch": 33, "training_loss": 1061.7393436431885, "training_acc": 72.0, "val_loss": 1042.8815841674805, "val_acc": 28.0}
{"epoch": 34, "training_loss": 3284.8898849487305, "training_acc": 38.0, "val_loss": 286.3128662109375, "val_acc": 72.0}
{"epoch": 35, "training_loss": 786.1905097961426, "training_acc": 60.0, "val_loss": 643.3874130249023, "val_acc": 72.0}
{"epoch": 36, "training_loss": 2799.8765029907227, "training_acc": 72.0, "val_loss": 611.930513381958, "val_acc": 72.0}
{"epoch": 37, "training_loss": 1475.3790922164917, "training_acc": 72.0, "val_loss": 2460.5648040771484, "val_acc": 28.0}
{"epoch": 38, "training_loss": 9350.71044921875, "training_acc": 28.0, "val_loss": 412.71281242370605, "val_acc": 72.0}
{"epoch": 39, "training_loss": 2535.368942260742, "training_acc": 72.0, "val_loss": 1362.7965927124023, "val_acc": 72.0}
{"epoch": 40, "training_loss": 5180.631423950195, "training_acc": 72.0, "val_loss": 595.8754539489746, "val_acc": 72.0}
{"epoch": 41, "training_loss": 2819.378204345703, "training_acc": 56.0, "val_loss": 303.2579183578491, "val_acc": 28.0}
{"epoch": 42, "training_loss": 2395.308151245117, "training_acc": 44.0, "val_loss": 1859.0961456298828, "val_acc": 72.0}
{"epoch": 43, "training_loss": 7655.83349609375, "training_acc": 72.0, "val_loss": 1899.2219924926758, "val_acc": 72.0}
