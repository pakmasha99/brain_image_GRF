"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 19602.501945495605, "training_acc": 72.0, "val_loss": 5847.651672363281, "val_acc": 72.0}
{"epoch": 1, "training_loss": 25602.449096679688, "training_acc": 58.0, "val_loss": 2468.316650390625, "val_acc": 28.0}
{"epoch": 2, "training_loss": 13424.1826171875, "training_acc": 44.0, "val_loss": 8502.810668945312, "val_acc": 72.0}
{"epoch": 3, "training_loss": 34922.00842285156, "training_acc": 72.0, "val_loss": 9402.21939086914, "val_acc": 72.0}
{"epoch": 4, "training_loss": 34345.84814453125, "training_acc": 72.0, "val_loss": 4842.354965209961, "val_acc": 72.0}
{"epoch": 5, "training_loss": 13587.84684753418, "training_acc": 72.0, "val_loss": 9593.632507324219, "val_acc": 28.0}
{"epoch": 6, "training_loss": 40061.85107421875, "training_acc": 28.0, "val_loss": 6645.052337646484, "val_acc": 28.0}
{"epoch": 7, "training_loss": 16203.378433227539, "training_acc": 52.0, "val_loss": 4059.7923278808594, "val_acc": 72.0}
{"epoch": 8, "training_loss": 18243.190063476562, "training_acc": 72.0, "val_loss": 6086.777114868164, "val_acc": 72.0}
{"epoch": 9, "training_loss": 23267.682678222656, "training_acc": 72.0, "val_loss": 3985.789108276367, "val_acc": 72.0}
{"epoch": 10, "training_loss": 12082.679260253906, "training_acc": 72.0, "val_loss": 2643.0410385131836, "val_acc": 28.0}
{"epoch": 11, "training_loss": 11547.691650390625, "training_acc": 28.0, "val_loss": 606.4756393432617, "val_acc": 72.0}
{"epoch": 12, "training_loss": 3684.825210571289, "training_acc": 72.0, "val_loss": 1483.1181526184082, "val_acc": 72.0}
{"epoch": 13, "training_loss": 4507.728134155273, "training_acc": 72.0, "val_loss": 2918.4459686279297, "val_acc": 28.0}
{"epoch": 14, "training_loss": 8931.233673095703, "training_acc": 28.0, "val_loss": 2176.882743835449, "val_acc": 72.0}
{"epoch": 15, "training_loss": 10976.041442871094, "training_acc": 72.0, "val_loss": 4222.383880615234, "val_acc": 72.0}
{"epoch": 16, "training_loss": 15990.900695800781, "training_acc": 72.0, "val_loss": 2596.380043029785, "val_acc": 72.0}
{"epoch": 17, "training_loss": 7890.366020202637, "training_acc": 72.0, "val_loss": 4968.146514892578, "val_acc": 28.0}
{"epoch": 18, "training_loss": 20049.273376464844, "training_acc": 28.0, "val_loss": 294.6406841278076, "val_acc": 28.0}
{"epoch": 19, "training_loss": 7582.27197265625, "training_acc": 38.0, "val_loss": 6253.3660888671875, "val_acc": 72.0}
{"epoch": 20, "training_loss": 25510.218688964844, "training_acc": 72.0, "val_loss": 8168.663787841797, "val_acc": 72.0}
{"epoch": 21, "training_loss": 31783.369384765625, "training_acc": 72.0, "val_loss": 6986.144256591797, "val_acc": 72.0}
{"epoch": 22, "training_loss": 25530.94677734375, "training_acc": 72.0, "val_loss": 3005.428886413574, "val_acc": 72.0}
{"epoch": 23, "training_loss": 8159.878266334534, "training_acc": 71.0, "val_loss": 7984.943389892578, "val_acc": 28.0}
{"epoch": 24, "training_loss": 33499.095703125, "training_acc": 28.0, "val_loss": 6738.060760498047, "val_acc": 28.0}
{"epoch": 25, "training_loss": 14832.646028995514, "training_acc": 56.0, "val_loss": 2358.0453872680664, "val_acc": 72.0}
{"epoch": 26, "training_loss": 11172.942687988281, "training_acc": 72.0, "val_loss": 3856.332778930664, "val_acc": 72.0}
{"epoch": 27, "training_loss": 14639.487060546875, "training_acc": 72.0, "val_loss": 1853.9615631103516, "val_acc": 72.0}
{"epoch": 28, "training_loss": 4538.393756866455, "training_acc": 71.0, "val_loss": 5256.826019287109, "val_acc": 28.0}
{"epoch": 29, "training_loss": 20057.374938964844, "training_acc": 28.0, "val_loss": 221.34137153625488, "val_acc": 72.0}
{"epoch": 30, "training_loss": 2307.4471435546875, "training_acc": 72.0, "val_loss": 1623.8780975341797, "val_acc": 72.0}
{"epoch": 31, "training_loss": 5649.4232177734375, "training_acc": 72.0, "val_loss": 540.0552272796631, "val_acc": 28.0}
{"epoch": 32, "training_loss": 2220.439826965332, "training_acc": 40.0, "val_loss": 226.77905559539795, "val_acc": 68.0}
{"epoch": 33, "training_loss": 3733.2626953125, "training_acc": 54.0, "val_loss": 223.87564182281494, "val_acc": 68.0}
{"epoch": 34, "training_loss": 1504.273780822754, "training_acc": 72.0, "val_loss": 367.7358627319336, "val_acc": 28.0}
{"epoch": 35, "training_loss": 2752.8821563720703, "training_acc": 43.0, "val_loss": 1351.888656616211, "val_acc": 72.0}
{"epoch": 36, "training_loss": 4482.835960388184, "training_acc": 72.0, "val_loss": 1425.169277191162, "val_acc": 28.0}
{"epoch": 37, "training_loss": 4187.218402862549, "training_acc": 40.0, "val_loss": 217.23816394805908, "val_acc": 68.0}
{"epoch": 38, "training_loss": 3291.0970153808594, "training_acc": 52.0, "val_loss": 471.2082862854004, "val_acc": 72.0}
{"epoch": 39, "training_loss": 2233.2999267578125, "training_acc": 72.0, "val_loss": 312.84875869750977, "val_acc": 72.0}
{"epoch": 40, "training_loss": 3742.2903442382812, "training_acc": 56.0, "val_loss": 105.30186891555786, "val_acc": 64.0}
{"epoch": 41, "training_loss": 837.2401351928711, "training_acc": 71.0, "val_loss": 137.83001899719238, "val_acc": 72.0}
{"epoch": 42, "training_loss": 3179.278289794922, "training_acc": 59.0, "val_loss": 266.96014404296875, "val_acc": 68.0}
{"epoch": 43, "training_loss": 1218.6866340637207, "training_acc": 72.0, "val_loss": 164.49759006500244, "val_acc": 52.0}
{"epoch": 44, "training_loss": 1033.1540832519531, "training_acc": 60.0, "val_loss": 143.06074380874634, "val_acc": 76.0}
{"epoch": 45, "training_loss": 3294.336212158203, "training_acc": 56.0, "val_loss": 269.67172622680664, "val_acc": 68.0}
{"epoch": 46, "training_loss": 1384.3390045166016, "training_acc": 72.0, "val_loss": 159.61430072784424, "val_acc": 68.0}
{"epoch": 47, "training_loss": 1041.8502883911133, "training_acc": 56.0, "val_loss": 382.26232528686523, "val_acc": 72.0}
{"epoch": 48, "training_loss": 2595.020523071289, "training_acc": 58.0, "val_loss": 615.9577369689941, "val_acc": 72.0}
{"epoch": 49, "training_loss": 2770.501968383789, "training_acc": 72.0, "val_loss": 248.95145893096924, "val_acc": 68.0}
{"epoch": 50, "training_loss": 5145.3067626953125, "training_acc": 52.0, "val_loss": 633.9571475982666, "val_acc": 28.0}
{"epoch": 51, "training_loss": 6387.842529296875, "training_acc": 38.0, "val_loss": 4276.308059692383, "val_acc": 72.0}
{"epoch": 52, "training_loss": 17960.767944335938, "training_acc": 72.0, "val_loss": 4730.2001953125, "val_acc": 72.0}
{"epoch": 53, "training_loss": 17486.027404785156, "training_acc": 72.0, "val_loss": 1908.2483291625977, "val_acc": 72.0}
{"epoch": 54, "training_loss": 6892.426040649414, "training_acc": 52.0, "val_loss": 850.5911827087402, "val_acc": 28.0}
{"epoch": 55, "training_loss": 3646.6142578125, "training_acc": 48.0, "val_loss": 2735.5751037597656, "val_acc": 72.0}
{"epoch": 56, "training_loss": 11098.130767822266, "training_acc": 72.0, "val_loss": 2369.9169158935547, "val_acc": 72.0}
{"epoch": 57, "training_loss": 7718.294692993164, "training_acc": 72.0, "val_loss": 2420.2089309692383, "val_acc": 28.0}
{"epoch": 58, "training_loss": 8653.12890625, "training_acc": 28.0, "val_loss": 1518.5555458068848, "val_acc": 72.0}
{"epoch": 59, "training_loss": 8134.9805908203125, "training_acc": 72.0, "val_loss": 3296.013641357422, "val_acc": 72.0}
