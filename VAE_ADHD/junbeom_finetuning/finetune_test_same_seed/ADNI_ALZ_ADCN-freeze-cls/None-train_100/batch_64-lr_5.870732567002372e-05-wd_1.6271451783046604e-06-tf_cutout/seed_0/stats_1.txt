"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 66.69895768165588, "training_acc": 72.0, "val_loss": 16.472594439983368, "val_acc": 28.0}
{"epoch": 1, "training_loss": 65.74730706214905, "training_acc": 72.0, "val_loss": 16.262200474739075, "val_acc": 28.0}
{"epoch": 2, "training_loss": 64.88863921165466, "training_acc": 72.0, "val_loss": 16.065774857997894, "val_acc": 28.0}
{"epoch": 3, "training_loss": 64.12975215911865, "training_acc": 72.0, "val_loss": 15.885117650032043, "val_acc": 28.0}
{"epoch": 4, "training_loss": 63.456655979156494, "training_acc": 72.0, "val_loss": 15.721060335636139, "val_acc": 28.0}
{"epoch": 5, "training_loss": 62.820146560668945, "training_acc": 72.0, "val_loss": 15.574423968791962, "val_acc": 28.0}
{"epoch": 6, "training_loss": 62.260053873062134, "training_acc": 72.0, "val_loss": 15.444907546043396, "val_acc": 28.0}
{"epoch": 7, "training_loss": 61.70907711982727, "training_acc": 72.0, "val_loss": 15.332163870334625, "val_acc": 72.0}
{"epoch": 8, "training_loss": 61.134750843048096, "training_acc": 72.0, "val_loss": 15.236400067806244, "val_acc": 72.0}
{"epoch": 9, "training_loss": 61.00925374031067, "training_acc": 72.0, "val_loss": 15.15272855758667, "val_acc": 72.0}
{"epoch": 10, "training_loss": 60.56198215484619, "training_acc": 72.0, "val_loss": 15.087702870368958, "val_acc": 72.0}
{"epoch": 11, "training_loss": 60.2852144241333, "training_acc": 72.0, "val_loss": 15.031343698501587, "val_acc": 72.0}
{"epoch": 12, "training_loss": 60.07687711715698, "training_acc": 72.0, "val_loss": 14.982688426971436, "val_acc": 72.0}
{"epoch": 13, "training_loss": 59.951154947280884, "training_acc": 72.0, "val_loss": 14.942209422588348, "val_acc": 72.0}
{"epoch": 14, "training_loss": 59.77189326286316, "training_acc": 72.0, "val_loss": 14.911413192749023, "val_acc": 72.0}
{"epoch": 15, "training_loss": 59.60122346878052, "training_acc": 72.0, "val_loss": 14.887376129627228, "val_acc": 72.0}
{"epoch": 16, "training_loss": 59.51568555831909, "training_acc": 72.0, "val_loss": 14.86743837594986, "val_acc": 72.0}
{"epoch": 17, "training_loss": 59.434165954589844, "training_acc": 72.0, "val_loss": 14.851005375385284, "val_acc": 72.0}
{"epoch": 18, "training_loss": 59.42229104042053, "training_acc": 72.0, "val_loss": 14.838157594203949, "val_acc": 72.0}
{"epoch": 19, "training_loss": 59.3508677482605, "training_acc": 72.0, "val_loss": 14.829356968402863, "val_acc": 72.0}
{"epoch": 20, "training_loss": 59.32769775390625, "training_acc": 72.0, "val_loss": 14.82333093881607, "val_acc": 72.0}
{"epoch": 21, "training_loss": 59.29486536979675, "training_acc": 72.0, "val_loss": 14.819228649139404, "val_acc": 72.0}
{"epoch": 22, "training_loss": 59.26007437705994, "training_acc": 72.0, "val_loss": 14.816613495349884, "val_acc": 72.0}
{"epoch": 23, "training_loss": 59.25559973716736, "training_acc": 72.0, "val_loss": 14.814962446689606, "val_acc": 72.0}
{"epoch": 24, "training_loss": 59.255244970321655, "training_acc": 72.0, "val_loss": 14.814211428165436, "val_acc": 72.0}
{"epoch": 25, "training_loss": 59.26578235626221, "training_acc": 72.0, "val_loss": 14.814053475856781, "val_acc": 72.0}
{"epoch": 26, "training_loss": 59.27346110343933, "training_acc": 72.0, "val_loss": 14.81432169675827, "val_acc": 72.0}
{"epoch": 27, "training_loss": 59.294453620910645, "training_acc": 72.0, "val_loss": 14.814777672290802, "val_acc": 72.0}
{"epoch": 28, "training_loss": 59.275452613830566, "training_acc": 72.0, "val_loss": 14.814896881580353, "val_acc": 72.0}
{"epoch": 29, "training_loss": 59.288569688797, "training_acc": 72.0, "val_loss": 14.814862608909607, "val_acc": 72.0}
{"epoch": 30, "training_loss": 59.260783672332764, "training_acc": 72.0, "val_loss": 14.814643561840057, "val_acc": 72.0}
{"epoch": 31, "training_loss": 59.28657531738281, "training_acc": 72.0, "val_loss": 14.814406633377075, "val_acc": 72.0}
{"epoch": 32, "training_loss": 59.26573133468628, "training_acc": 72.0, "val_loss": 14.814385771751404, "val_acc": 72.0}
{"epoch": 33, "training_loss": 59.2584263086319, "training_acc": 72.0, "val_loss": 14.814379811286926, "val_acc": 72.0}
{"epoch": 34, "training_loss": 59.26618981361389, "training_acc": 72.0, "val_loss": 14.81456309556961, "val_acc": 72.0}
{"epoch": 35, "training_loss": 59.262158155441284, "training_acc": 72.0, "val_loss": 14.81458693742752, "val_acc": 72.0}
{"epoch": 36, "training_loss": 59.26383113861084, "training_acc": 72.0, "val_loss": 14.814691245555878, "val_acc": 72.0}
{"epoch": 37, "training_loss": 59.259862184524536, "training_acc": 72.0, "val_loss": 14.814889430999756, "val_acc": 72.0}
{"epoch": 38, "training_loss": 59.26116394996643, "training_acc": 72.0, "val_loss": 14.814959466457367, "val_acc": 72.0}
{"epoch": 39, "training_loss": 59.25591254234314, "training_acc": 72.0, "val_loss": 14.814901351928711, "val_acc": 72.0}
{"epoch": 40, "training_loss": 59.26400876045227, "training_acc": 72.0, "val_loss": 14.81480747461319, "val_acc": 72.0}
{"epoch": 41, "training_loss": 59.256134271621704, "training_acc": 72.0, "val_loss": 14.814774692058563, "val_acc": 72.0}
{"epoch": 42, "training_loss": 59.25579023361206, "training_acc": 72.0, "val_loss": 14.814575016498566, "val_acc": 72.0}
{"epoch": 43, "training_loss": 59.254077196121216, "training_acc": 72.0, "val_loss": 14.814393222332, "val_acc": 72.0}
{"epoch": 44, "training_loss": 59.264853715896606, "training_acc": 72.0, "val_loss": 14.814232289791107, "val_acc": 72.0}
