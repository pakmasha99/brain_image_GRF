"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 449.4848518371582, "training_acc": 72.0, "val_loss": 350.20365715026855, "val_acc": 72.0}
{"epoch": 1, "training_loss": 1085.3276748657227, "training_acc": 72.0, "val_loss": 400.2184867858887, "val_acc": 28.0}
{"epoch": 2, "training_loss": 1396.3314781188965, "training_acc": 28.0, "val_loss": 72.23563194274902, "val_acc": 72.0}
{"epoch": 3, "training_loss": 438.27920722961426, "training_acc": 72.0, "val_loss": 196.15944623947144, "val_acc": 72.0}
{"epoch": 4, "training_loss": 754.6547260284424, "training_acc": 72.0, "val_loss": 111.98805570602417, "val_acc": 72.0}
{"epoch": 5, "training_loss": 316.1114909648895, "training_acc": 72.0, "val_loss": 249.1882562637329, "val_acc": 28.0}
{"epoch": 6, "training_loss": 992.4968528747559, "training_acc": 28.0, "val_loss": 20.030716061592102, "val_acc": 28.0}
{"epoch": 7, "training_loss": 180.48772144317627, "training_acc": 48.0, "val_loss": 192.6659345626831, "val_acc": 72.0}
{"epoch": 8, "training_loss": 816.4459114074707, "training_acc": 72.0, "val_loss": 231.5154790878296, "val_acc": 72.0}
{"epoch": 9, "training_loss": 880.6792602539062, "training_acc": 72.0, "val_loss": 143.17944049835205, "val_acc": 72.0}
{"epoch": 10, "training_loss": 436.6529541015625, "training_acc": 72.0, "val_loss": 143.8625454902649, "val_acc": 28.0}
{"epoch": 11, "training_loss": 649.7908515930176, "training_acc": 28.0, "val_loss": 40.91280996799469, "val_acc": 28.0}
{"epoch": 12, "training_loss": 210.2657175064087, "training_acc": 48.0, "val_loss": 174.62159395217896, "val_acc": 72.0}
{"epoch": 13, "training_loss": 729.0434818267822, "training_acc": 72.0, "val_loss": 226.108980178833, "val_acc": 72.0}
{"epoch": 14, "training_loss": 883.4302368164062, "training_acc": 72.0, "val_loss": 169.78280544281006, "val_acc": 72.0}
{"epoch": 15, "training_loss": 547.9561767578125, "training_acc": 72.0, "val_loss": 18.145515024662018, "val_acc": 72.0}
{"epoch": 16, "training_loss": 386.22377014160156, "training_acc": 56.0, "val_loss": 288.716721534729, "val_acc": 28.0}
{"epoch": 17, "training_loss": 960.7689819335938, "training_acc": 28.0, "val_loss": 52.35324501991272, "val_acc": 72.0}
{"epoch": 18, "training_loss": 306.37390899658203, "training_acc": 72.0, "val_loss": 172.43703603744507, "val_acc": 72.0}
{"epoch": 19, "training_loss": 691.9145202636719, "training_acc": 72.0, "val_loss": 170.00117301940918, "val_acc": 72.0}
{"epoch": 20, "training_loss": 629.6426372528076, "training_acc": 72.0, "val_loss": 79.33956980705261, "val_acc": 72.0}
{"epoch": 21, "training_loss": 230.05777049064636, "training_acc": 56.0, "val_loss": 123.88856410980225, "val_acc": 28.0}
{"epoch": 22, "training_loss": 394.45003032684326, "training_acc": 28.0, "val_loss": 68.46120953559875, "val_acc": 72.0}
{"epoch": 23, "training_loss": 362.3167419433594, "training_acc": 72.0, "val_loss": 146.71162366867065, "val_acc": 72.0}
{"epoch": 24, "training_loss": 576.2610206604004, "training_acc": 72.0, "val_loss": 105.92596530914307, "val_acc": 72.0}
{"epoch": 25, "training_loss": 329.97319507598877, "training_acc": 72.0, "val_loss": 78.22415232658386, "val_acc": 28.0}
{"epoch": 26, "training_loss": 357.97351837158203, "training_acc": 28.0, "val_loss": 27.798375487327576, "val_acc": 72.0}
{"epoch": 27, "training_loss": 163.7555389404297, "training_acc": 72.0, "val_loss": 72.03591465950012, "val_acc": 72.0}
{"epoch": 28, "training_loss": 265.84033966064453, "training_acc": 72.0, "val_loss": 14.682179689407349, "val_acc": 72.0}
{"epoch": 29, "training_loss": 118.39611530303955, "training_acc": 62.0, "val_loss": 14.89071398973465, "val_acc": 72.0}
{"epoch": 30, "training_loss": 84.98666620254517, "training_acc": 72.0, "val_loss": 40.70643484592438, "val_acc": 72.0}
{"epoch": 31, "training_loss": 132.69138360023499, "training_acc": 72.0, "val_loss": 59.57009196281433, "val_acc": 28.0}
{"epoch": 32, "training_loss": 172.99268698692322, "training_acc": 44.0, "val_loss": 33.50653052330017, "val_acc": 72.0}
{"epoch": 33, "training_loss": 134.78810214996338, "training_acc": 72.0, "val_loss": 15.557809174060822, "val_acc": 28.0}
{"epoch": 34, "training_loss": 87.38479089736938, "training_acc": 58.0, "val_loss": 28.482970595359802, "val_acc": 72.0}
{"epoch": 35, "training_loss": 128.00999450683594, "training_acc": 72.0, "val_loss": 21.625186502933502, "val_acc": 72.0}
{"epoch": 36, "training_loss": 149.16304779052734, "training_acc": 52.0, "val_loss": 16.33032262325287, "val_acc": 72.0}
{"epoch": 37, "training_loss": 80.59400224685669, "training_acc": 72.0, "val_loss": 20.909270644187927, "val_acc": 72.0}
{"epoch": 38, "training_loss": 77.72395253181458, "training_acc": 62.0, "val_loss": 14.738787710666656, "val_acc": 72.0}
{"epoch": 39, "training_loss": 55.65990126132965, "training_acc": 72.0, "val_loss": 20.864947140216827, "val_acc": 72.0}
{"epoch": 40, "training_loss": 82.75770235061646, "training_acc": 72.0, "val_loss": 14.982977509498596, "val_acc": 72.0}
{"epoch": 41, "training_loss": 60.041969299316406, "training_acc": 72.0, "val_loss": 15.208134055137634, "val_acc": 72.0}
{"epoch": 42, "training_loss": 62.44312596321106, "training_acc": 72.0, "val_loss": 16.354629397392273, "val_acc": 72.0}
{"epoch": 43, "training_loss": 64.53323817253113, "training_acc": 72.0, "val_loss": 32.514846324920654, "val_acc": 28.0}
{"epoch": 44, "training_loss": 86.34122252464294, "training_acc": 56.0, "val_loss": 37.87080943584442, "val_acc": 72.0}
{"epoch": 45, "training_loss": 139.80831813812256, "training_acc": 72.0, "val_loss": 26.247432827949524, "val_acc": 28.0}
{"epoch": 46, "training_loss": 90.65698051452637, "training_acc": 28.0, "val_loss": 51.881009340286255, "val_acc": 72.0}
{"epoch": 47, "training_loss": 240.4235315322876, "training_acc": 72.0, "val_loss": 59.58762764930725, "val_acc": 72.0}
