"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 911.8591499328613, "training_acc": 34.0, "val_loss": 332.6624870300293, "val_acc": 72.0}
{"epoch": 1, "training_loss": 1055.6529388427734, "training_acc": 72.0, "val_loss": 149.8235583305359, "val_acc": 28.0}
{"epoch": 2, "training_loss": 488.74690532684326, "training_acc": 28.0, "val_loss": 113.6967420578003, "val_acc": 72.0}
{"epoch": 3, "training_loss": 605.7086563110352, "training_acc": 72.0, "val_loss": 207.19304084777832, "val_acc": 72.0}
{"epoch": 4, "training_loss": 781.3717727661133, "training_acc": 72.0, "val_loss": 106.62548542022705, "val_acc": 72.0}
{"epoch": 5, "training_loss": 295.40638399124146, "training_acc": 72.0, "val_loss": 198.74361753463745, "val_acc": 28.0}
{"epoch": 6, "training_loss": 703.707763671875, "training_acc": 28.0, "val_loss": 51.473450660705566, "val_acc": 72.0}
{"epoch": 7, "training_loss": 293.94921493530273, "training_acc": 72.0, "val_loss": 137.4450445175171, "val_acc": 72.0}
{"epoch": 8, "training_loss": 546.4077529907227, "training_acc": 72.0, "val_loss": 88.69541883468628, "val_acc": 72.0}
{"epoch": 9, "training_loss": 244.07554984092712, "training_acc": 72.0, "val_loss": 187.6633882522583, "val_acc": 28.0}
{"epoch": 10, "training_loss": 736.671220779419, "training_acc": 28.0, "val_loss": 18.174634873867035, "val_acc": 28.0}
{"epoch": 11, "training_loss": 230.33200645446777, "training_acc": 36.0, "val_loss": 136.33452653884888, "val_acc": 72.0}
{"epoch": 12, "training_loss": 565.7615509033203, "training_acc": 72.0, "val_loss": 131.61299228668213, "val_acc": 72.0}
{"epoch": 13, "training_loss": 475.8967275619507, "training_acc": 72.0, "val_loss": 19.502700865268707, "val_acc": 72.0}
{"epoch": 14, "training_loss": 200.06612586975098, "training_acc": 66.0, "val_loss": 151.8119215965271, "val_acc": 28.0}
{"epoch": 15, "training_loss": 418.6445162296295, "training_acc": 42.0, "val_loss": 63.26781511306763, "val_acc": 72.0}
{"epoch": 16, "training_loss": 289.980167388916, "training_acc": 72.0, "val_loss": 79.2765200138092, "val_acc": 72.0}
{"epoch": 17, "training_loss": 265.3890209197998, "training_acc": 72.0, "val_loss": 42.68700182437897, "val_acc": 28.0}
{"epoch": 18, "training_loss": 144.94308018684387, "training_acc": 28.0, "val_loss": 41.353461146354675, "val_acc": 72.0}
{"epoch": 19, "training_loss": 212.99644947052002, "training_acc": 72.0, "val_loss": 43.76184642314911, "val_acc": 72.0}
{"epoch": 20, "training_loss": 149.2752878665924, "training_acc": 54.0, "val_loss": 27.77753174304962, "val_acc": 28.0}
{"epoch": 21, "training_loss": 113.20992469787598, "training_acc": 46.0, "val_loss": 48.46372902393341, "val_acc": 72.0}
{"epoch": 22, "training_loss": 171.27199792861938, "training_acc": 72.0, "val_loss": 25.824829936027527, "val_acc": 28.0}
{"epoch": 23, "training_loss": 89.22141742706299, "training_acc": 44.0, "val_loss": 31.526735424995422, "val_acc": 72.0}
{"epoch": 24, "training_loss": 128.0121169090271, "training_acc": 72.0, "val_loss": 14.884187281131744, "val_acc": 72.0}
{"epoch": 25, "training_loss": 109.02610206604004, "training_acc": 54.0, "val_loss": 30.38393259048462, "val_acc": 72.0}
{"epoch": 26, "training_loss": 142.41679430007935, "training_acc": 72.0, "val_loss": 32.85571336746216, "val_acc": 72.0}
{"epoch": 27, "training_loss": 152.76703691482544, "training_acc": 50.0, "val_loss": 17.26260930299759, "val_acc": 72.0}
{"epoch": 28, "training_loss": 78.40283489227295, "training_acc": 72.0, "val_loss": 15.200337767601013, "val_acc": 64.0}
{"epoch": 29, "training_loss": 80.43323755264282, "training_acc": 52.0, "val_loss": 36.15240752696991, "val_acc": 72.0}
{"epoch": 30, "training_loss": 158.74266719818115, "training_acc": 72.0, "val_loss": 29.865533113479614, "val_acc": 72.0}
{"epoch": 31, "training_loss": 156.43171787261963, "training_acc": 52.0, "val_loss": 16.23576283454895, "val_acc": 72.0}
{"epoch": 32, "training_loss": 74.08331942558289, "training_acc": 72.0, "val_loss": 16.537831723690033, "val_acc": 72.0}
{"epoch": 33, "training_loss": 118.47748517990112, "training_acc": 50.0, "val_loss": 30.449724197387695, "val_acc": 72.0}
{"epoch": 34, "training_loss": 129.27667713165283, "training_acc": 72.0, "val_loss": 30.717548727989197, "val_acc": 72.0}
{"epoch": 35, "training_loss": 112.54111909866333, "training_acc": 56.0, "val_loss": 14.90594893693924, "val_acc": 72.0}
{"epoch": 36, "training_loss": 76.09743452072144, "training_acc": 72.0, "val_loss": 15.104566514492035, "val_acc": 72.0}
{"epoch": 37, "training_loss": 89.91197538375854, "training_acc": 54.0, "val_loss": 31.10327124595642, "val_acc": 72.0}
{"epoch": 38, "training_loss": 129.78023624420166, "training_acc": 72.0, "val_loss": 23.437438905239105, "val_acc": 72.0}
{"epoch": 39, "training_loss": 135.6603136062622, "training_acc": 54.0, "val_loss": 18.004092574119568, "val_acc": 72.0}
{"epoch": 40, "training_loss": 80.3639988899231, "training_acc": 72.0, "val_loss": 16.379711031913757, "val_acc": 72.0}
{"epoch": 41, "training_loss": 80.06299877166748, "training_acc": 60.0, "val_loss": 21.03697955608368, "val_acc": 72.0}
{"epoch": 42, "training_loss": 86.2101902961731, "training_acc": 72.0, "val_loss": 16.039615869522095, "val_acc": 28.0}
{"epoch": 43, "training_loss": 65.63173604011536, "training_acc": 57.0, "val_loss": 25.177469849586487, "val_acc": 72.0}
