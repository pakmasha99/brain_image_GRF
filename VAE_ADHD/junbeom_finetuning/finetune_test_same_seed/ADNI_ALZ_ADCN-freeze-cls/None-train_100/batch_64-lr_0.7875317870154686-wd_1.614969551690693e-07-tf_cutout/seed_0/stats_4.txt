"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 3419.4833984375, "training_acc": 44.0, "val_loss": 2094.6590423583984, "val_acc": 72.0}
{"epoch": 1, "training_loss": 6965.159683227539, "training_acc": 72.0, "val_loss": 1148.1799125671387, "val_acc": 28.0}
{"epoch": 2, "training_loss": 3458.2632026672363, "training_acc": 28.0, "val_loss": 837.5456809997559, "val_acc": 72.0}
{"epoch": 3, "training_loss": 3760.21484375, "training_acc": 72.0, "val_loss": 1569.4381713867188, "val_acc": 72.0}
{"epoch": 4, "training_loss": 6197.081298828125, "training_acc": 72.0, "val_loss": 1238.2551193237305, "val_acc": 72.0}
{"epoch": 5, "training_loss": 4343.256851196289, "training_acc": 72.0, "val_loss": 127.72142887115479, "val_acc": 72.0}
{"epoch": 6, "training_loss": 3452.948272705078, "training_acc": 48.0, "val_loss": 1968.4661865234375, "val_acc": 28.0}
{"epoch": 7, "training_loss": 6293.444931030273, "training_acc": 28.0, "val_loss": 456.7039966583252, "val_acc": 72.0}
{"epoch": 8, "training_loss": 2794.837417602539, "training_acc": 72.0, "val_loss": 1324.3714332580566, "val_acc": 72.0}
{"epoch": 9, "training_loss": 5446.779373168945, "training_acc": 72.0, "val_loss": 1352.1489143371582, "val_acc": 72.0}
{"epoch": 10, "training_loss": 5034.59016418457, "training_acc": 72.0, "val_loss": 734.2063903808594, "val_acc": 72.0}
{"epoch": 11, "training_loss": 1949.2502326965332, "training_acc": 72.0, "val_loss": 1121.1238861083984, "val_acc": 28.0}
{"epoch": 12, "training_loss": 5230.465728759766, "training_acc": 28.0, "val_loss": 900.4559516906738, "val_acc": 28.0}
{"epoch": 13, "training_loss": 2687.4873123168945, "training_acc": 44.0, "val_loss": 682.1856498718262, "val_acc": 72.0}
{"epoch": 14, "training_loss": 3002.5932998657227, "training_acc": 72.0, "val_loss": 951.0900497436523, "val_acc": 72.0}
{"epoch": 15, "training_loss": 3668.3256759643555, "training_acc": 72.0, "val_loss": 600.0576496124268, "val_acc": 72.0}
{"epoch": 16, "training_loss": 2162.286763191223, "training_acc": 72.0, "val_loss": 652.7074813842773, "val_acc": 28.0}
{"epoch": 17, "training_loss": 2565.710090637207, "training_acc": 28.0, "val_loss": 139.07830715179443, "val_acc": 72.0}
{"epoch": 18, "training_loss": 873.6080856323242, "training_acc": 72.0, "val_loss": 361.4776134490967, "val_acc": 72.0}
{"epoch": 19, "training_loss": 1243.61372756958, "training_acc": 72.0, "val_loss": 106.15875720977783, "val_acc": 28.0}
{"epoch": 20, "training_loss": 393.85237979888916, "training_acc": 44.0, "val_loss": 52.56861448287964, "val_acc": 72.0}
{"epoch": 21, "training_loss": 549.4078330993652, "training_acc": 58.0, "val_loss": 95.24882435798645, "val_acc": 72.0}
{"epoch": 22, "training_loss": 535.8382453918457, "training_acc": 72.0, "val_loss": 36.993324756622314, "val_acc": 72.0}
{"epoch": 23, "training_loss": 766.5070419311523, "training_acc": 62.0, "val_loss": 230.13875484466553, "val_acc": 28.0}
{"epoch": 24, "training_loss": 1070.046272277832, "training_acc": 46.0, "val_loss": 679.7040462493896, "val_acc": 72.0}
{"epoch": 25, "training_loss": 2813.060203552246, "training_acc": 72.0, "val_loss": 758.1316947937012, "val_acc": 72.0}
{"epoch": 26, "training_loss": 2807.707588195801, "training_acc": 72.0, "val_loss": 291.6919469833374, "val_acc": 72.0}
{"epoch": 27, "training_loss": 945.599193572998, "training_acc": 62.0, "val_loss": 268.5260772705078, "val_acc": 28.0}
{"epoch": 28, "training_loss": 1008.7312202453613, "training_acc": 46.0, "val_loss": 458.8521480560303, "val_acc": 72.0}
{"epoch": 29, "training_loss": 1847.56489944458, "training_acc": 72.0, "val_loss": 363.7659788131714, "val_acc": 72.0}
{"epoch": 30, "training_loss": 1178.4517517089844, "training_acc": 72.0, "val_loss": 675.9328365325928, "val_acc": 28.0}
{"epoch": 31, "training_loss": 2469.3884506225586, "training_acc": 28.0, "val_loss": 248.85015487670898, "val_acc": 72.0}
{"epoch": 32, "training_loss": 1465.617416381836, "training_acc": 72.0, "val_loss": 596.0611343383789, "val_acc": 72.0}
{"epoch": 33, "training_loss": 2259.1114959716797, "training_acc": 72.0, "val_loss": 284.73994731903076, "val_acc": 72.0}
{"epoch": 34, "training_loss": 1361.5911178588867, "training_acc": 48.0, "val_loss": 49.50660169124603, "val_acc": 28.0}
{"epoch": 35, "training_loss": 572.8154373168945, "training_acc": 48.0, "val_loss": 647.1732139587402, "val_acc": 72.0}
{"epoch": 36, "training_loss": 2703.7956466674805, "training_acc": 72.0, "val_loss": 656.097412109375, "val_acc": 72.0}
{"epoch": 37, "training_loss": 2254.821876525879, "training_acc": 72.0, "val_loss": 99.34589266777039, "val_acc": 72.0}
{"epoch": 38, "training_loss": 1796.4002685546875, "training_acc": 54.0, "val_loss": 1128.3915519714355, "val_acc": 28.0}
{"epoch": 39, "training_loss": 3326.509578704834, "training_acc": 28.0, "val_loss": 578.6936283111572, "val_acc": 72.0}
{"epoch": 40, "training_loss": 2876.4393920898438, "training_acc": 72.0, "val_loss": 1282.0914268493652, "val_acc": 72.0}
{"epoch": 41, "training_loss": 5282.136260986328, "training_acc": 72.0, "val_loss": 1310.0808143615723, "val_acc": 72.0}
