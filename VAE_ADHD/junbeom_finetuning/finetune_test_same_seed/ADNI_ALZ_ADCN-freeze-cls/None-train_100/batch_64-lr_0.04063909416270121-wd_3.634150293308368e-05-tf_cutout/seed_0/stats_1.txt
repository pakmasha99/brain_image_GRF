"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 183.36024951934814, "training_acc": 72.0, "val_loss": 93.16970705986023, "val_acc": 72.0}
{"epoch": 1, "training_loss": 271.1380088329315, "training_acc": 72.0, "val_loss": 170.64123153686523, "val_acc": 28.0}
{"epoch": 2, "training_loss": 617.1452121734619, "training_acc": 28.0, "val_loss": 15.153196454048157, "val_acc": 72.0}
{"epoch": 3, "training_loss": 124.14699649810791, "training_acc": 72.0, "val_loss": 81.49241209030151, "val_acc": 72.0}
{"epoch": 4, "training_loss": 344.29702377319336, "training_acc": 72.0, "val_loss": 85.12406945228577, "val_acc": 72.0}
{"epoch": 5, "training_loss": 312.99742221832275, "training_acc": 72.0, "val_loss": 36.28040254116058, "val_acc": 72.0}
{"epoch": 6, "training_loss": 163.9139256477356, "training_acc": 44.0, "val_loss": 45.450374484062195, "val_acc": 28.0}
{"epoch": 7, "training_loss": 157.88616275787354, "training_acc": 30.0, "val_loss": 19.143569469451904, "val_acc": 72.0}
{"epoch": 8, "training_loss": 82.90267872810364, "training_acc": 72.0, "val_loss": 22.28236198425293, "val_acc": 72.0}
{"epoch": 9, "training_loss": 82.41959118843079, "training_acc": 72.0, "val_loss": 18.886961042881012, "val_acc": 28.0}
{"epoch": 10, "training_loss": 76.10134601593018, "training_acc": 28.0, "val_loss": 15.182481706142426, "val_acc": 72.0}
{"epoch": 11, "training_loss": 74.67460489273071, "training_acc": 72.0, "val_loss": 18.59627068042755, "val_acc": 72.0}
{"epoch": 12, "training_loss": 66.54282474517822, "training_acc": 72.0, "val_loss": 20.7727313041687, "val_acc": 28.0}
{"epoch": 13, "training_loss": 79.87591171264648, "training_acc": 28.0, "val_loss": 16.265952587127686, "val_acc": 72.0}
{"epoch": 14, "training_loss": 67.9583740234375, "training_acc": 72.0, "val_loss": 20.588524639606476, "val_acc": 72.0}
{"epoch": 15, "training_loss": 77.53499293327332, "training_acc": 72.0, "val_loss": 16.672617197036743, "val_acc": 28.0}
{"epoch": 16, "training_loss": 66.70675039291382, "training_acc": 72.0, "val_loss": 14.778876304626465, "val_acc": 72.0}
{"epoch": 17, "training_loss": 61.90290904045105, "training_acc": 72.0, "val_loss": 15.78347384929657, "val_acc": 72.0}
{"epoch": 18, "training_loss": 61.8122136592865, "training_acc": 72.0, "val_loss": 15.968164801597595, "val_acc": 28.0}
{"epoch": 19, "training_loss": 62.51404786109924, "training_acc": 72.0, "val_loss": 15.162308514118195, "val_acc": 72.0}
{"epoch": 20, "training_loss": 60.68641400337219, "training_acc": 72.0, "val_loss": 15.01014530658722, "val_acc": 72.0}
{"epoch": 21, "training_loss": 59.85454773902893, "training_acc": 72.0, "val_loss": 15.124344825744629, "val_acc": 72.0}
{"epoch": 22, "training_loss": 59.924302101135254, "training_acc": 72.0, "val_loss": 15.16713798046112, "val_acc": 72.0}
{"epoch": 23, "training_loss": 60.6419517993927, "training_acc": 72.0, "val_loss": 14.761580526828766, "val_acc": 72.0}
{"epoch": 24, "training_loss": 58.96684646606445, "training_acc": 72.0, "val_loss": 15.051981806755066, "val_acc": 72.0}
{"epoch": 25, "training_loss": 66.7428035736084, "training_acc": 72.0, "val_loss": 14.957946538925171, "val_acc": 72.0}
{"epoch": 26, "training_loss": 60.123637437820435, "training_acc": 72.0, "val_loss": 14.987771213054657, "val_acc": 72.0}
{"epoch": 27, "training_loss": 63.558475971221924, "training_acc": 72.0, "val_loss": 14.822918176651001, "val_acc": 72.0}
{"epoch": 28, "training_loss": 59.16129446029663, "training_acc": 72.0, "val_loss": 15.37618339061737, "val_acc": 28.0}
{"epoch": 29, "training_loss": 60.25060749053955, "training_acc": 72.0, "val_loss": 17.25497841835022, "val_acc": 72.0}
{"epoch": 30, "training_loss": 69.11186599731445, "training_acc": 72.0, "val_loss": 14.77854996919632, "val_acc": 72.0}
{"epoch": 31, "training_loss": 72.96122765541077, "training_acc": 48.0, "val_loss": 14.953386783599854, "val_acc": 72.0}
{"epoch": 32, "training_loss": 69.29087781906128, "training_acc": 72.0, "val_loss": 16.796329617500305, "val_acc": 72.0}
{"epoch": 33, "training_loss": 71.44904732704163, "training_acc": 72.0, "val_loss": 16.17586463689804, "val_acc": 28.0}
{"epoch": 34, "training_loss": 60.84955930709839, "training_acc": 72.0, "val_loss": 18.068549036979675, "val_acc": 72.0}
{"epoch": 35, "training_loss": 69.74821925163269, "training_acc": 72.0, "val_loss": 15.78925997018814, "val_acc": 28.0}
{"epoch": 36, "training_loss": 64.69922947883606, "training_acc": 73.0, "val_loss": 14.767752587795258, "val_acc": 72.0}
{"epoch": 37, "training_loss": 61.485878467559814, "training_acc": 72.0, "val_loss": 15.337890386581421, "val_acc": 72.0}
{"epoch": 38, "training_loss": 63.08951115608215, "training_acc": 72.0, "val_loss": 15.160970389842987, "val_acc": 68.0}
{"epoch": 39, "training_loss": 63.36376166343689, "training_acc": 72.0, "val_loss": 15.06454199552536, "val_acc": 72.0}
{"epoch": 40, "training_loss": 60.32595753669739, "training_acc": 72.0, "val_loss": 15.21928608417511, "val_acc": 68.0}
{"epoch": 41, "training_loss": 60.214353799819946, "training_acc": 72.0, "val_loss": 15.31364768743515, "val_acc": 72.0}
{"epoch": 42, "training_loss": 60.4386785030365, "training_acc": 72.0, "val_loss": 15.138103067874908, "val_acc": 68.0}
{"epoch": 43, "training_loss": 60.24100041389465, "training_acc": 72.0, "val_loss": 14.722009003162384, "val_acc": 72.0}
{"epoch": 44, "training_loss": 59.128376483917236, "training_acc": 72.0, "val_loss": 14.811623096466064, "val_acc": 72.0}
{"epoch": 45, "training_loss": 60.045594453811646, "training_acc": 72.0, "val_loss": 15.043815970420837, "val_acc": 76.0}
{"epoch": 46, "training_loss": 59.953869104385376, "training_acc": 72.0, "val_loss": 14.699335396289825, "val_acc": 72.0}
{"epoch": 47, "training_loss": 60.22501063346863, "training_acc": 72.0, "val_loss": 15.501876175403595, "val_acc": 28.0}
{"epoch": 48, "training_loss": 61.63572573661804, "training_acc": 72.0, "val_loss": 14.642241597175598, "val_acc": 72.0}
{"epoch": 49, "training_loss": 59.253326416015625, "training_acc": 72.0, "val_loss": 14.671702682971954, "val_acc": 72.0}
{"epoch": 50, "training_loss": 59.05471086502075, "training_acc": 72.0, "val_loss": 14.652001857757568, "val_acc": 72.0}
{"epoch": 51, "training_loss": 59.03911852836609, "training_acc": 72.0, "val_loss": 15.444090962409973, "val_acc": 72.0}
{"epoch": 52, "training_loss": 61.007171869277954, "training_acc": 72.0, "val_loss": 15.049704909324646, "val_acc": 68.0}
{"epoch": 53, "training_loss": 64.33451128005981, "training_acc": 73.0, "val_loss": 18.20780336856842, "val_acc": 72.0}
{"epoch": 54, "training_loss": 77.19958448410034, "training_acc": 72.0, "val_loss": 14.763368666172028, "val_acc": 72.0}
{"epoch": 55, "training_loss": 66.65311431884766, "training_acc": 58.0, "val_loss": 14.828664064407349, "val_acc": 72.0}
{"epoch": 56, "training_loss": 72.65895318984985, "training_acc": 72.0, "val_loss": 16.838164627552032, "val_acc": 72.0}
{"epoch": 57, "training_loss": 62.682918071746826, "training_acc": 72.0, "val_loss": 19.87343430519104, "val_acc": 28.0}
{"epoch": 58, "training_loss": 70.96349692344666, "training_acc": 46.0, "val_loss": 20.09071111679077, "val_acc": 72.0}
{"epoch": 59, "training_loss": 75.18747782707214, "training_acc": 72.0, "val_loss": 17.286567389965057, "val_acc": 28.0}
{"epoch": 60, "training_loss": 68.76215386390686, "training_acc": 57.0, "val_loss": 17.497186362743378, "val_acc": 72.0}
{"epoch": 61, "training_loss": 71.71458125114441, "training_acc": 72.0, "val_loss": 14.855489134788513, "val_acc": 72.0}
{"epoch": 62, "training_loss": 62.65215182304382, "training_acc": 58.0, "val_loss": 15.075431764125824, "val_acc": 68.0}
{"epoch": 63, "training_loss": 76.92497444152832, "training_acc": 72.0, "val_loss": 15.339507162570953, "val_acc": 72.0}
{"epoch": 64, "training_loss": 66.60960531234741, "training_acc": 56.0, "val_loss": 14.971722662448883, "val_acc": 68.0}
{"epoch": 65, "training_loss": 67.35818600654602, "training_acc": 72.0, "val_loss": 16.17887020111084, "val_acc": 72.0}
{"epoch": 66, "training_loss": 77.49762606620789, "training_acc": 48.0, "val_loss": 14.761029183864594, "val_acc": 72.0}
{"epoch": 67, "training_loss": 58.56816279888153, "training_acc": 72.0, "val_loss": 16.8220192193985, "val_acc": 72.0}
