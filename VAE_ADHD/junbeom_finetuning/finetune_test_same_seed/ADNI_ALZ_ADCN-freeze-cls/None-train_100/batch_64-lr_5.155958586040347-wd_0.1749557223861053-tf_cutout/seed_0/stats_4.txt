"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 20051.120639801025, "training_acc": 72.0, "val_loss": 4892.377853393555, "val_acc": 28.0}
{"epoch": 1, "training_loss": 20893.59783935547, "training_acc": 48.0, "val_loss": 4493.62678527832, "val_acc": 72.0}
{"epoch": 2, "training_loss": 12012.35131072998, "training_acc": 72.0, "val_loss": 6524.806213378906, "val_acc": 28.0}
{"epoch": 3, "training_loss": 22753.434020996094, "training_acc": 40.0, "val_loss": 1028.7493705749512, "val_acc": 72.0}
{"epoch": 4, "training_loss": 4030.448486328125, "training_acc": 64.0, "val_loss": 2727.9916763305664, "val_acc": 72.0}
{"epoch": 5, "training_loss": 8591.933624267578, "training_acc": 72.0, "val_loss": 1661.3380432128906, "val_acc": 28.0}
{"epoch": 6, "training_loss": 8077.596252441406, "training_acc": 44.0, "val_loss": 1417.8935050964355, "val_acc": 72.0}
{"epoch": 7, "training_loss": 3629.737522125244, "training_acc": 58.0, "val_loss": 2800.930595397949, "val_acc": 72.0}
{"epoch": 8, "training_loss": 9559.634750366211, "training_acc": 72.0, "val_loss": 211.97240352630615, "val_acc": 72.0}
{"epoch": 9, "training_loss": 4264.8658447265625, "training_acc": 58.0, "val_loss": 1772.793960571289, "val_acc": 72.0}
{"epoch": 10, "training_loss": 5672.664375305176, "training_acc": 72.0, "val_loss": 1263.955020904541, "val_acc": 28.0}
{"epoch": 11, "training_loss": 6412.698089599609, "training_acc": 44.0, "val_loss": 1206.6628456115723, "val_acc": 72.0}
{"epoch": 12, "training_loss": 2983.5677552223206, "training_acc": 58.0, "val_loss": 2044.6159362792969, "val_acc": 72.0}
{"epoch": 13, "training_loss": 6659.433059692383, "training_acc": 72.0, "val_loss": 654.9033164978027, "val_acc": 28.0}
{"epoch": 14, "training_loss": 4979.580505371094, "training_acc": 44.0, "val_loss": 1315.9857749938965, "val_acc": 72.0}
{"epoch": 15, "training_loss": 3270.9918479919434, "training_acc": 72.0, "val_loss": 3137.416458129883, "val_acc": 28.0}
{"epoch": 16, "training_loss": 9568.032196044922, "training_acc": 46.0, "val_loss": 543.7033176422119, "val_acc": 72.0}
{"epoch": 17, "training_loss": 3650.753372192383, "training_acc": 56.0, "val_loss": 1945.0571060180664, "val_acc": 72.0}
{"epoch": 18, "training_loss": 6482.506561279297, "training_acc": 72.0, "val_loss": 348.9748477935791, "val_acc": 28.0}
{"epoch": 19, "training_loss": 6035.65185546875, "training_acc": 36.0, "val_loss": 1175.7408142089844, "val_acc": 72.0}
{"epoch": 20, "training_loss": 2881.1371738910675, "training_acc": 72.0, "val_loss": 2004.91943359375, "val_acc": 28.0}
{"epoch": 21, "training_loss": 7196.90283203125, "training_acc": 46.0, "val_loss": 893.3231353759766, "val_acc": 72.0}
{"epoch": 22, "training_loss": 3236.716667175293, "training_acc": 56.0, "val_loss": 2191.798782348633, "val_acc": 72.0}
{"epoch": 23, "training_loss": 7297.519317626953, "training_acc": 72.0, "val_loss": 159.93847846984863, "val_acc": 28.0}
{"epoch": 24, "training_loss": 3295.6600341796875, "training_acc": 46.0, "val_loss": 1395.702838897705, "val_acc": 72.0}
{"epoch": 25, "training_loss": 3692.941650390625, "training_acc": 72.0, "val_loss": 2613.070297241211, "val_acc": 28.0}
{"epoch": 26, "training_loss": 9197.618682861328, "training_acc": 42.0, "val_loss": 542.8502559661865, "val_acc": 72.0}
{"epoch": 27, "training_loss": 3894.267120361328, "training_acc": 54.0, "val_loss": 1914.7777557373047, "val_acc": 72.0}
{"epoch": 28, "training_loss": 6355.564163208008, "training_acc": 72.0, "val_loss": 329.1253089904785, "val_acc": 28.0}
{"epoch": 29, "training_loss": 4912.096038818359, "training_acc": 40.0, "val_loss": 1222.9670524597168, "val_acc": 72.0}
{"epoch": 30, "training_loss": 3736.9772052764893, "training_acc": 52.0, "val_loss": 2473.396873474121, "val_acc": 72.0}
{"epoch": 31, "training_loss": 8487.402526855469, "training_acc": 72.0, "val_loss": 316.6408061981201, "val_acc": 72.0}
{"epoch": 32, "training_loss": 2155.7242279052734, "training_acc": 66.0, "val_loss": 1454.582118988037, "val_acc": 72.0}
{"epoch": 33, "training_loss": 4642.871810913086, "training_acc": 72.0, "val_loss": 1208.51411819458, "val_acc": 28.0}
{"epoch": 34, "training_loss": 5472.400421142578, "training_acc": 46.0, "val_loss": 1093.188190460205, "val_acc": 72.0}
{"epoch": 35, "training_loss": 2980.258882522583, "training_acc": 56.0, "val_loss": 2325.216484069824, "val_acc": 72.0}
{"epoch": 36, "training_loss": 8292.17056274414, "training_acc": 72.0, "val_loss": 374.2541313171387, "val_acc": 72.0}
{"epoch": 37, "training_loss": 4699.224456787109, "training_acc": 50.0, "val_loss": 1866.9258117675781, "val_acc": 72.0}
{"epoch": 38, "training_loss": 6259.921249389648, "training_acc": 72.0, "val_loss": 239.89887237548828, "val_acc": 28.0}
{"epoch": 39, "training_loss": 2332.3211975097656, "training_acc": 52.0, "val_loss": 1510.305404663086, "val_acc": 72.0}
{"epoch": 40, "training_loss": 4396.352912902832, "training_acc": 72.0, "val_loss": 1890.6463623046875, "val_acc": 28.0}
{"epoch": 41, "training_loss": 8143.3551025390625, "training_acc": 40.0, "val_loss": 732.2676181793213, "val_acc": 72.0}
{"epoch": 42, "training_loss": 3354.2850189208984, "training_acc": 56.0, "val_loss": 2010.4669570922852, "val_acc": 72.0}
