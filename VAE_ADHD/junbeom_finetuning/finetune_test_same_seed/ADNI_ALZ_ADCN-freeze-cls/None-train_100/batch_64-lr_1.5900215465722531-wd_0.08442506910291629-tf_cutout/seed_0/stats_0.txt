"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 7533.518280029297, "training_acc": 42.0, "val_loss": 3462.1631622314453, "val_acc": 72.0}
{"epoch": 1, "training_loss": 9528.052032470703, "training_acc": 72.0, "val_loss": 5552.326965332031, "val_acc": 28.0}
{"epoch": 2, "training_loss": 19598.27069091797, "training_acc": 28.0, "val_loss": 567.6424980163574, "val_acc": 72.0}
{"epoch": 3, "training_loss": 3668.2315063476562, "training_acc": 72.0, "val_loss": 1726.2008666992188, "val_acc": 72.0}
{"epoch": 4, "training_loss": 6310.324264526367, "training_acc": 72.0, "val_loss": 507.8482151031494, "val_acc": 72.0}
{"epoch": 5, "training_loss": 4644.155670166016, "training_acc": 48.0, "val_loss": 775.6021499633789, "val_acc": 28.0}
{"epoch": 6, "training_loss": 3323.742477416992, "training_acc": 44.0, "val_loss": 1524.222755432129, "val_acc": 72.0}
{"epoch": 7, "training_loss": 6100.133590698242, "training_acc": 72.0, "val_loss": 1264.6612167358398, "val_acc": 72.0}
{"epoch": 8, "training_loss": 4168.780662536621, "training_acc": 72.0, "val_loss": 354.00683879852295, "val_acc": 28.0}
{"epoch": 9, "training_loss": 907.2378449440002, "training_acc": 50.0, "val_loss": 58.06229114532471, "val_acc": 28.0}
{"epoch": 10, "training_loss": 1165.3781356811523, "training_acc": 44.0, "val_loss": 949.8270988464355, "val_acc": 72.0}
{"epoch": 11, "training_loss": 3575.5263671875, "training_acc": 72.0, "val_loss": 336.4152669906616, "val_acc": 72.0}
{"epoch": 12, "training_loss": 2360.557159423828, "training_acc": 54.0, "val_loss": 136.18355989456177, "val_acc": 28.0}
{"epoch": 13, "training_loss": 1100.3661804199219, "training_acc": 52.0, "val_loss": 1475.3697395324707, "val_acc": 72.0}
{"epoch": 14, "training_loss": 6146.931976318359, "training_acc": 72.0, "val_loss": 1339.1899108886719, "val_acc": 72.0}
{"epoch": 15, "training_loss": 4464.920471191406, "training_acc": 72.0, "val_loss": 27.95942723751068, "val_acc": 72.0}
{"epoch": 16, "training_loss": 3005.5104064941406, "training_acc": 60.0, "val_loss": 1998.3055114746094, "val_acc": 28.0}
{"epoch": 17, "training_loss": 4801.768149852753, "training_acc": 50.0, "val_loss": 466.2376403808594, "val_acc": 72.0}
{"epoch": 18, "training_loss": 2009.1821060180664, "training_acc": 72.0, "val_loss": 198.16930294036865, "val_acc": 72.0}
{"epoch": 19, "training_loss": 2418.901809692383, "training_acc": 50.0, "val_loss": 116.56293869018555, "val_acc": 28.0}
{"epoch": 20, "training_loss": 1576.488296508789, "training_acc": 46.0, "val_loss": 1541.4533615112305, "val_acc": 72.0}
{"epoch": 21, "training_loss": 6450.1737060546875, "training_acc": 72.0, "val_loss": 1404.9779891967773, "val_acc": 72.0}
{"epoch": 22, "training_loss": 4831.033966064453, "training_acc": 72.0, "val_loss": 108.99581909179688, "val_acc": 72.0}
{"epoch": 23, "training_loss": 3385.069122314453, "training_acc": 54.0, "val_loss": 1743.4614181518555, "val_acc": 28.0}
{"epoch": 24, "training_loss": 4719.7012367248535, "training_acc": 44.0, "val_loss": 565.5703067779541, "val_acc": 72.0}
{"epoch": 25, "training_loss": 2271.5440979003906, "training_acc": 72.0, "val_loss": 242.8473949432373, "val_acc": 72.0}
{"epoch": 26, "training_loss": 1962.5211944580078, "training_acc": 54.0, "val_loss": 18.698851764202118, "val_acc": 72.0}
{"epoch": 27, "training_loss": 166.31204509735107, "training_acc": 72.0, "val_loss": 417.9673194885254, "val_acc": 28.0}
{"epoch": 28, "training_loss": 1554.0712242126465, "training_acc": 46.0, "val_loss": 560.1092338562012, "val_acc": 72.0}
{"epoch": 29, "training_loss": 1984.1497077941895, "training_acc": 72.0, "val_loss": 156.7021608352661, "val_acc": 28.0}
{"epoch": 30, "training_loss": 687.9620323181152, "training_acc": 50.0, "val_loss": 393.9905881881714, "val_acc": 72.0}
{"epoch": 31, "training_loss": 1221.007038116455, "training_acc": 72.0, "val_loss": 882.3698997497559, "val_acc": 28.0}
{"epoch": 32, "training_loss": 2301.553526878357, "training_acc": 28.0, "val_loss": 1011.8511199951172, "val_acc": 72.0}
{"epoch": 33, "training_loss": 4590.284164428711, "training_acc": 72.0, "val_loss": 1608.04443359375, "val_acc": 72.0}
{"epoch": 34, "training_loss": 6096.522186279297, "training_acc": 72.0, "val_loss": 887.1833801269531, "val_acc": 72.0}
{"epoch": 35, "training_loss": 2509.6718349456787, "training_acc": 72.0, "val_loss": 1822.2909927368164, "val_acc": 28.0}
{"epoch": 36, "training_loss": 6937.502899169922, "training_acc": 28.0, "val_loss": 253.16643714904785, "val_acc": 72.0}
{"epoch": 37, "training_loss": 1905.608413696289, "training_acc": 72.0, "val_loss": 626.7062187194824, "val_acc": 72.0}
{"epoch": 38, "training_loss": 1957.092399597168, "training_acc": 72.0, "val_loss": 835.9784126281738, "val_acc": 28.0}
{"epoch": 39, "training_loss": 2404.2773122787476, "training_acc": 36.0, "val_loss": 209.31954383850098, "val_acc": 28.0}
{"epoch": 40, "training_loss": 1603.954231262207, "training_acc": 40.0, "val_loss": 791.9157981872559, "val_acc": 72.0}
{"epoch": 41, "training_loss": 2934.243621826172, "training_acc": 72.0, "val_loss": 198.6026167869568, "val_acc": 72.0}
{"epoch": 42, "training_loss": 2342.461624145508, "training_acc": 54.0, "val_loss": 480.2262783050537, "val_acc": 28.0}
{"epoch": 43, "training_loss": 2029.0087966918945, "training_acc": 48.0, "val_loss": 1239.2274856567383, "val_acc": 72.0}
{"epoch": 44, "training_loss": 5058.592559814453, "training_acc": 72.0, "val_loss": 1058.3380699157715, "val_acc": 72.0}
{"epoch": 45, "training_loss": 3457.5499572753906, "training_acc": 72.0, "val_loss": 386.74187660217285, "val_acc": 28.0}
