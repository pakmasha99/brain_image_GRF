"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 4132.197151184082, "training_acc": 72.0, "val_loss": 3221.0262298583984, "val_acc": 72.0}
{"epoch": 1, "training_loss": 9159.459854125977, "training_acc": 54.0, "val_loss": 1369.3556785583496, "val_acc": 72.0}
{"epoch": 2, "training_loss": 5546.801956176758, "training_acc": 72.0, "val_loss": 836.7130279541016, "val_acc": 72.0}
{"epoch": 3, "training_loss": 2765.9418563842773, "training_acc": 60.0, "val_loss": 441.3022518157959, "val_acc": 72.0}
{"epoch": 4, "training_loss": 1747.5427169799805, "training_acc": 72.0, "val_loss": 414.19739723205566, "val_acc": 28.0}
{"epoch": 5, "training_loss": 2222.105484008789, "training_acc": 40.0, "val_loss": 737.949800491333, "val_acc": 72.0}
{"epoch": 6, "training_loss": 2440.2725143432617, "training_acc": 72.0, "val_loss": 903.6295890808105, "val_acc": 28.0}
{"epoch": 7, "training_loss": 2342.34663772583, "training_acc": 48.0, "val_loss": 170.840322971344, "val_acc": 72.0}
{"epoch": 8, "training_loss": 1958.2623596191406, "training_acc": 44.0, "val_loss": 463.25693130493164, "val_acc": 72.0}
{"epoch": 9, "training_loss": 2204.9515686035156, "training_acc": 72.0, "val_loss": 550.7181167602539, "val_acc": 72.0}
{"epoch": 10, "training_loss": 1619.4133677482605, "training_acc": 72.0, "val_loss": 1619.249153137207, "val_acc": 28.0}
{"epoch": 11, "training_loss": 5160.218170166016, "training_acc": 28.0, "val_loss": 802.7774810791016, "val_acc": 72.0}
{"epoch": 12, "training_loss": 3675.883316040039, "training_acc": 72.0, "val_loss": 1393.9985275268555, "val_acc": 72.0}
{"epoch": 13, "training_loss": 5251.148010253906, "training_acc": 72.0, "val_loss": 638.9054298400879, "val_acc": 72.0}
{"epoch": 14, "training_loss": 1952.9691467285156, "training_acc": 56.0, "val_loss": 174.8770236968994, "val_acc": 72.0}
{"epoch": 15, "training_loss": 684.2706470489502, "training_acc": 46.0, "val_loss": 712.6929759979248, "val_acc": 72.0}
{"epoch": 16, "training_loss": 3234.9246673583984, "training_acc": 72.0, "val_loss": 877.7026176452637, "val_acc": 72.0}
{"epoch": 17, "training_loss": 3020.0090103149414, "training_acc": 72.0, "val_loss": 358.6695194244385, "val_acc": 28.0}
{"epoch": 18, "training_loss": 890.2665853500366, "training_acc": 54.0, "val_loss": 139.38277959823608, "val_acc": 72.0}
{"epoch": 19, "training_loss": 983.1752548217773, "training_acc": 54.0, "val_loss": 419.7688102722168, "val_acc": 72.0}
{"epoch": 20, "training_loss": 1888.0494384765625, "training_acc": 72.0, "val_loss": 371.77019119262695, "val_acc": 72.0}
{"epoch": 21, "training_loss": 1554.2083206176758, "training_acc": 52.0, "val_loss": 325.63133239746094, "val_acc": 72.0}
{"epoch": 22, "training_loss": 1443.5582733154297, "training_acc": 72.0, "val_loss": 109.06258821487427, "val_acc": 72.0}
{"epoch": 23, "training_loss": 1736.6268157958984, "training_acc": 58.0, "val_loss": 177.48963832855225, "val_acc": 28.0}
{"epoch": 24, "training_loss": 1971.6709442138672, "training_acc": 42.0, "val_loss": 1432.2306632995605, "val_acc": 72.0}
{"epoch": 25, "training_loss": 5782.312545776367, "training_acc": 72.0, "val_loss": 1219.2152976989746, "val_acc": 72.0}
{"epoch": 26, "training_loss": 4145.600387573242, "training_acc": 72.0, "val_loss": 19.4714292883873, "val_acc": 28.0}
{"epoch": 27, "training_loss": 956.2475891113281, "training_acc": 28.0, "val_loss": 384.02671813964844, "val_acc": 72.0}
{"epoch": 28, "training_loss": 1748.391746520996, "training_acc": 72.0, "val_loss": 402.31895446777344, "val_acc": 72.0}
{"epoch": 29, "training_loss": 1231.6410055160522, "training_acc": 54.0, "val_loss": 415.65470695495605, "val_acc": 72.0}
{"epoch": 30, "training_loss": 1847.854995727539, "training_acc": 72.0, "val_loss": 199.95825290679932, "val_acc": 72.0}
{"epoch": 31, "training_loss": 1960.1540832519531, "training_acc": 54.0, "val_loss": 21.726399660110474, "val_acc": 28.0}
{"epoch": 32, "training_loss": 789.6877822875977, "training_acc": 48.0, "val_loss": 905.5399894714355, "val_acc": 72.0}
{"epoch": 33, "training_loss": 3465.4596557617188, "training_acc": 72.0, "val_loss": 404.8396587371826, "val_acc": 72.0}
{"epoch": 34, "training_loss": 1829.5860977172852, "training_acc": 54.0, "val_loss": 155.86625337600708, "val_acc": 72.0}
{"epoch": 35, "training_loss": 559.9813261032104, "training_acc": 72.0, "val_loss": 520.5872535705566, "val_acc": 28.0}
{"epoch": 36, "training_loss": 1898.2307891845703, "training_acc": 42.0, "val_loss": 438.8807773590088, "val_acc": 72.0}
{"epoch": 37, "training_loss": 1416.133487701416, "training_acc": 72.0, "val_loss": 709.1037273406982, "val_acc": 28.0}
{"epoch": 38, "training_loss": 1946.3795957565308, "training_acc": 44.0, "val_loss": 85.60572862625122, "val_acc": 72.0}
{"epoch": 39, "training_loss": 1048.4543991088867, "training_acc": 56.0, "val_loss": 307.47125148773193, "val_acc": 72.0}
{"epoch": 40, "training_loss": 1531.428092956543, "training_acc": 72.0, "val_loss": 203.64515781402588, "val_acc": 72.0}
{"epoch": 41, "training_loss": 2244.485137939453, "training_acc": 48.0, "val_loss": 66.01535081863403, "val_acc": 72.0}
{"epoch": 42, "training_loss": 363.27381896972656, "training_acc": 72.0, "val_loss": 380.503511428833, "val_acc": 28.0}
{"epoch": 43, "training_loss": 1172.1987609863281, "training_acc": 52.0, "val_loss": 526.26633644104, "val_acc": 72.0}
{"epoch": 44, "training_loss": 1913.0704727172852, "training_acc": 72.0, "val_loss": 127.19055414199829, "val_acc": 28.0}
{"epoch": 45, "training_loss": 742.0135650634766, "training_acc": 46.0, "val_loss": 355.8422088623047, "val_acc": 72.0}
