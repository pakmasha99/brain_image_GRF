"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 484.6835708618164, "training_acc": 72.0, "val_loss": 200.1314401626587, "val_acc": 72.0}
{"epoch": 1, "training_loss": 509.1813745498657, "training_acc": 72.0, "val_loss": 361.59684658050537, "val_acc": 28.0}
{"epoch": 2, "training_loss": 1353.7278213500977, "training_acc": 28.0, "val_loss": 28.569182753562927, "val_acc": 28.0}
{"epoch": 3, "training_loss": 235.27240371704102, "training_acc": 46.0, "val_loss": 211.36701107025146, "val_acc": 72.0}
{"epoch": 4, "training_loss": 886.6550273895264, "training_acc": 72.0, "val_loss": 253.01506519317627, "val_acc": 72.0}
{"epoch": 5, "training_loss": 976.2776069641113, "training_acc": 72.0, "val_loss": 180.77428340911865, "val_acc": 72.0}
{"epoch": 6, "training_loss": 614.3891410827637, "training_acc": 72.0, "val_loss": 24.63974356651306, "val_acc": 72.0}
{"epoch": 7, "training_loss": 301.12207412719727, "training_acc": 60.0, "val_loss": 229.22637462615967, "val_acc": 28.0}
{"epoch": 8, "training_loss": 755.7416954040527, "training_acc": 28.0, "val_loss": 40.912601351737976, "val_acc": 72.0}
{"epoch": 9, "training_loss": 224.17646503448486, "training_acc": 72.0, "val_loss": 122.82706499099731, "val_acc": 72.0}
{"epoch": 10, "training_loss": 489.7974410057068, "training_acc": 72.0, "val_loss": 108.72832536697388, "val_acc": 72.0}
{"epoch": 11, "training_loss": 391.3031940460205, "training_acc": 72.0, "val_loss": 31.17396831512451, "val_acc": 72.0}
{"epoch": 12, "training_loss": 217.0345058441162, "training_acc": 52.0, "val_loss": 75.95369815826416, "val_acc": 28.0}
{"epoch": 13, "training_loss": 224.71029806137085, "training_acc": 44.0, "val_loss": 47.65428900718689, "val_acc": 72.0}
{"epoch": 14, "training_loss": 192.1699299812317, "training_acc": 72.0, "val_loss": 37.009066343307495, "val_acc": 72.0}
{"epoch": 15, "training_loss": 120.83301424980164, "training_acc": 72.0, "val_loss": 47.29992151260376, "val_acc": 28.0}
{"epoch": 16, "training_loss": 142.08806252479553, "training_acc": 44.0, "val_loss": 28.30631136894226, "val_acc": 72.0}
{"epoch": 17, "training_loss": 110.55462670326233, "training_acc": 72.0, "val_loss": 15.937614440917969, "val_acc": 72.0}
{"epoch": 18, "training_loss": 78.10987758636475, "training_acc": 56.0, "val_loss": 14.993852376937866, "val_acc": 72.0}
{"epoch": 19, "training_loss": 62.77838444709778, "training_acc": 72.0, "val_loss": 16.542670130729675, "val_acc": 72.0}
{"epoch": 20, "training_loss": 90.38067483901978, "training_acc": 46.0, "val_loss": 19.62999999523163, "val_acc": 72.0}
{"epoch": 21, "training_loss": 82.94010138511658, "training_acc": 72.0, "val_loss": 15.568558871746063, "val_acc": 72.0}
{"epoch": 22, "training_loss": 76.6790099143982, "training_acc": 56.0, "val_loss": 15.501968562602997, "val_acc": 72.0}
{"epoch": 23, "training_loss": 64.54699373245239, "training_acc": 72.0, "val_loss": 15.664614737033844, "val_acc": 72.0}
{"epoch": 24, "training_loss": 61.3188099861145, "training_acc": 72.0, "val_loss": 15.844827890396118, "val_acc": 28.0}
{"epoch": 25, "training_loss": 67.90390348434448, "training_acc": 72.0, "val_loss": 15.603390336036682, "val_acc": 72.0}
{"epoch": 26, "training_loss": 63.27329707145691, "training_acc": 72.0, "val_loss": 14.831069111824036, "val_acc": 72.0}
{"epoch": 27, "training_loss": 59.85438323020935, "training_acc": 72.0, "val_loss": 15.224356949329376, "val_acc": 72.0}
{"epoch": 28, "training_loss": 62.226601123809814, "training_acc": 72.0, "val_loss": 14.80884850025177, "val_acc": 72.0}
{"epoch": 29, "training_loss": 59.45124578475952, "training_acc": 72.0, "val_loss": 14.7992342710495, "val_acc": 72.0}
{"epoch": 30, "training_loss": 61.150184631347656, "training_acc": 72.0, "val_loss": 15.753687918186188, "val_acc": 72.0}
{"epoch": 31, "training_loss": 62.269516468048096, "training_acc": 72.0, "val_loss": 14.991845190525055, "val_acc": 72.0}
{"epoch": 32, "training_loss": 63.5681471824646, "training_acc": 72.0, "val_loss": 16.878797113895416, "val_acc": 28.0}
{"epoch": 33, "training_loss": 63.46625733375549, "training_acc": 72.0, "val_loss": 17.15855747461319, "val_acc": 72.0}
{"epoch": 34, "training_loss": 69.39345169067383, "training_acc": 72.0, "val_loss": 14.864206314086914, "val_acc": 72.0}
{"epoch": 35, "training_loss": 61.24332785606384, "training_acc": 72.0, "val_loss": 17.341311275959015, "val_acc": 72.0}
{"epoch": 36, "training_loss": 66.9401924610138, "training_acc": 72.0, "val_loss": 15.731875598430634, "val_acc": 28.0}
{"epoch": 37, "training_loss": 61.00247287750244, "training_acc": 72.0, "val_loss": 15.884765982627869, "val_acc": 72.0}
{"epoch": 38, "training_loss": 61.505844831466675, "training_acc": 72.0, "val_loss": 15.745183825492859, "val_acc": 28.0}
{"epoch": 39, "training_loss": 70.55623126029968, "training_acc": 72.0, "val_loss": 14.799030125141144, "val_acc": 72.0}
{"epoch": 40, "training_loss": 63.889487981796265, "training_acc": 56.0, "val_loss": 18.99963468313217, "val_acc": 72.0}
{"epoch": 41, "training_loss": 73.16234707832336, "training_acc": 72.0, "val_loss": 20.78729271888733, "val_acc": 28.0}
{"epoch": 42, "training_loss": 90.32739353179932, "training_acc": 38.0, "val_loss": 15.081577003002167, "val_acc": 72.0}
{"epoch": 43, "training_loss": 96.45362377166748, "training_acc": 48.0, "val_loss": 32.48642683029175, "val_acc": 72.0}
{"epoch": 44, "training_loss": 150.82125282287598, "training_acc": 72.0, "val_loss": 29.543259739875793, "val_acc": 72.0}
{"epoch": 45, "training_loss": 161.04347276687622, "training_acc": 48.0, "val_loss": 16.443228721618652, "val_acc": 72.0}
{"epoch": 46, "training_loss": 89.72349834442139, "training_acc": 72.0, "val_loss": 15.47330766916275, "val_acc": 72.0}
{"epoch": 47, "training_loss": 103.39943408966064, "training_acc": 54.0, "val_loss": 21.583688259124756, "val_acc": 72.0}
{"epoch": 48, "training_loss": 99.50838327407837, "training_acc": 72.0, "val_loss": 18.078307807445526, "val_acc": 72.0}
{"epoch": 49, "training_loss": 119.87475490570068, "training_acc": 52.0, "val_loss": 18.895389139652252, "val_acc": 72.0}
{"epoch": 50, "training_loss": 92.9620270729065, "training_acc": 72.0, "val_loss": 17.51437336206436, "val_acc": 72.0}
{"epoch": 51, "training_loss": 125.74631977081299, "training_acc": 50.0, "val_loss": 20.060695707798004, "val_acc": 72.0}
{"epoch": 52, "training_loss": 99.57734775543213, "training_acc": 72.0, "val_loss": 19.766367971897125, "val_acc": 72.0}
{"epoch": 53, "training_loss": 110.78797340393066, "training_acc": 54.0, "val_loss": 16.512076556682587, "val_acc": 72.0}
{"epoch": 54, "training_loss": 74.10600686073303, "training_acc": 72.0, "val_loss": 15.96403419971466, "val_acc": 72.0}
{"epoch": 55, "training_loss": 69.35618925094604, "training_acc": 60.0, "val_loss": 15.250909328460693, "val_acc": 72.0}
{"epoch": 56, "training_loss": 63.122217893600464, "training_acc": 72.0, "val_loss": 14.745527505874634, "val_acc": 72.0}
{"epoch": 57, "training_loss": 60.287731885910034, "training_acc": 72.0, "val_loss": 15.128779411315918, "val_acc": 72.0}
{"epoch": 58, "training_loss": 64.71187710762024, "training_acc": 72.0, "val_loss": 21.105313301086426, "val_acc": 28.0}
{"epoch": 59, "training_loss": 84.83651685714722, "training_acc": 40.0, "val_loss": 15.598003566265106, "val_acc": 72.0}
{"epoch": 60, "training_loss": 64.31339192390442, "training_acc": 58.0, "val_loss": 15.008942782878876, "val_acc": 72.0}
{"epoch": 61, "training_loss": 61.372971534729004, "training_acc": 72.0, "val_loss": 15.342220664024353, "val_acc": 36.0}
{"epoch": 62, "training_loss": 60.663970947265625, "training_acc": 72.0, "val_loss": 15.504571795463562, "val_acc": 72.0}
{"epoch": 63, "training_loss": 61.08036518096924, "training_acc": 72.0, "val_loss": 15.144410729408264, "val_acc": 72.0}
{"epoch": 64, "training_loss": 62.39514088630676, "training_acc": 72.0, "val_loss": 14.891132712364197, "val_acc": 72.0}
{"epoch": 65, "training_loss": 59.61923670768738, "training_acc": 72.0, "val_loss": 14.736978709697723, "val_acc": 72.0}
{"epoch": 66, "training_loss": 60.818994998931885, "training_acc": 72.0, "val_loss": 17.448683083057404, "val_acc": 28.0}
{"epoch": 67, "training_loss": 67.69745683670044, "training_acc": 45.0, "val_loss": 15.546463429927826, "val_acc": 72.0}
{"epoch": 68, "training_loss": 98.66462087631226, "training_acc": 44.0, "val_loss": 36.301565170288086, "val_acc": 72.0}
{"epoch": 69, "training_loss": 201.914888381958, "training_acc": 72.0, "val_loss": 35.972580313682556, "val_acc": 72.0}
{"epoch": 70, "training_loss": 148.14939260482788, "training_acc": 54.0, "val_loss": 16.033226251602173, "val_acc": 28.0}
{"epoch": 71, "training_loss": 70.6374843120575, "training_acc": 72.0, "val_loss": 26.75771415233612, "val_acc": 72.0}
{"epoch": 72, "training_loss": 94.80316281318665, "training_acc": 72.0, "val_loss": 20.38749009370804, "val_acc": 28.0}
{"epoch": 73, "training_loss": 110.4096884727478, "training_acc": 36.0, "val_loss": 21.778006851673126, "val_acc": 72.0}
{"epoch": 74, "training_loss": 80.18646168708801, "training_acc": 60.0, "val_loss": 15.39779007434845, "val_acc": 28.0}
{"epoch": 75, "training_loss": 62.50712490081787, "training_acc": 72.0, "val_loss": 17.947858572006226, "val_acc": 72.0}
{"epoch": 76, "training_loss": 92.2294249534607, "training_acc": 50.0, "val_loss": 23.738466203212738, "val_acc": 72.0}
{"epoch": 77, "training_loss": 95.1228141784668, "training_acc": 72.0, "val_loss": 14.768041670322418, "val_acc": 72.0}
{"epoch": 78, "training_loss": 66.12462282180786, "training_acc": 60.0, "val_loss": 18.28925311565399, "val_acc": 72.0}
{"epoch": 79, "training_loss": 74.64000654220581, "training_acc": 72.0, "val_loss": 20.062489807605743, "val_acc": 28.0}
{"epoch": 80, "training_loss": 73.08682560920715, "training_acc": 44.0, "val_loss": 16.721296310424805, "val_acc": 72.0}
{"epoch": 81, "training_loss": 65.67505311965942, "training_acc": 72.0, "val_loss": 14.765918254852295, "val_acc": 72.0}
{"epoch": 82, "training_loss": 58.598915815353394, "training_acc": 72.0, "val_loss": 15.938469767570496, "val_acc": 72.0}
{"epoch": 83, "training_loss": 63.06761884689331, "training_acc": 72.0, "val_loss": 14.748074114322662, "val_acc": 72.0}
{"epoch": 84, "training_loss": 58.8859703540802, "training_acc": 72.0, "val_loss": 14.927253127098083, "val_acc": 72.0}
