"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 15830.64225769043, "training_acc": 72.0, "val_loss": 7380.861663818359, "val_acc": 72.0}
{"epoch": 1, "training_loss": 18271.840209960938, "training_acc": 72.0, "val_loss": 19909.00421142578, "val_acc": 28.0}
{"epoch": 2, "training_loss": 75987.37866210938, "training_acc": 28.0, "val_loss": 7706.591796875, "val_acc": 28.0}
{"epoch": 3, "training_loss": 23734.74380493164, "training_acc": 44.0, "val_loss": 7222.5494384765625, "val_acc": 72.0}
{"epoch": 4, "training_loss": 32856.91857910156, "training_acc": 72.0, "val_loss": 10853.642272949219, "val_acc": 72.0}
{"epoch": 5, "training_loss": 42449.05456542969, "training_acc": 72.0, "val_loss": 9498.804473876953, "val_acc": 72.0}
{"epoch": 6, "training_loss": 34808.15075683594, "training_acc": 72.0, "val_loss": 4518.999862670898, "val_acc": 72.0}
{"epoch": 7, "training_loss": 12952.826446533203, "training_acc": 72.0, "val_loss": 7468.115997314453, "val_acc": 28.0}
{"epoch": 8, "training_loss": 33991.37487792969, "training_acc": 28.0, "val_loss": 7302.6031494140625, "val_acc": 28.0}
{"epoch": 9, "training_loss": 17614.233093738556, "training_acc": 48.0, "val_loss": 2126.962661743164, "val_acc": 72.0}
{"epoch": 10, "training_loss": 9628.017608642578, "training_acc": 72.0, "val_loss": 3342.1463012695312, "val_acc": 72.0}
{"epoch": 11, "training_loss": 12231.174072265625, "training_acc": 72.0, "val_loss": 1366.6221618652344, "val_acc": 72.0}
{"epoch": 12, "training_loss": 5566.465377807617, "training_acc": 58.0, "val_loss": 1443.8426971435547, "val_acc": 28.0}
{"epoch": 13, "training_loss": 5218.117752075195, "training_acc": 46.0, "val_loss": 2398.756217956543, "val_acc": 72.0}
{"epoch": 14, "training_loss": 9579.53076171875, "training_acc": 72.0, "val_loss": 2028.609275817871, "val_acc": 72.0}
{"epoch": 15, "training_loss": 6814.921131134033, "training_acc": 72.0, "val_loss": 2585.995101928711, "val_acc": 28.0}
{"epoch": 16, "training_loss": 8793.334548950195, "training_acc": 28.0, "val_loss": 1470.4493522644043, "val_acc": 72.0}
{"epoch": 17, "training_loss": 9116.178894042969, "training_acc": 72.0, "val_loss": 3090.8573150634766, "val_acc": 72.0}
{"epoch": 18, "training_loss": 11324.37646484375, "training_acc": 72.0, "val_loss": 1189.9065971374512, "val_acc": 72.0}
{"epoch": 19, "training_loss": 5890.6619873046875, "training_acc": 54.0, "val_loss": 1300.4290580749512, "val_acc": 28.0}
{"epoch": 20, "training_loss": 4977.644775390625, "training_acc": 46.0, "val_loss": 2485.3187561035156, "val_acc": 72.0}
{"epoch": 21, "training_loss": 9829.085479736328, "training_acc": 72.0, "val_loss": 2268.014335632324, "val_acc": 72.0}
{"epoch": 22, "training_loss": 7507.692428588867, "training_acc": 72.0, "val_loss": 614.3632888793945, "val_acc": 28.0}
{"epoch": 23, "training_loss": 1691.5267028808594, "training_acc": 46.0, "val_loss": 309.50963497161865, "val_acc": 28.0}
{"epoch": 24, "training_loss": 2321.042678833008, "training_acc": 48.0, "val_loss": 1962.447738647461, "val_acc": 72.0}
{"epoch": 25, "training_loss": 7604.087585449219, "training_acc": 72.0, "val_loss": 1033.0131530761719, "val_acc": 72.0}
{"epoch": 26, "training_loss": 4037.623275756836, "training_acc": 56.0, "val_loss": 105.0480604171753, "val_acc": 68.0}
{"epoch": 27, "training_loss": 1223.1393585205078, "training_acc": 70.0, "val_loss": 347.30005264282227, "val_acc": 72.0}
{"epoch": 28, "training_loss": 4184.296173095703, "training_acc": 48.0, "val_loss": 259.44387912750244, "val_acc": 72.0}
{"epoch": 29, "training_loss": 1387.3022384643555, "training_acc": 72.0, "val_loss": 110.89026927947998, "val_acc": 64.0}
{"epoch": 30, "training_loss": 2498.248764038086, "training_acc": 50.0, "val_loss": 836.0933303833008, "val_acc": 72.0}
{"epoch": 31, "training_loss": 4646.833404541016, "training_acc": 72.0, "val_loss": 1339.3022537231445, "val_acc": 72.0}
{"epoch": 32, "training_loss": 4038.478729248047, "training_acc": 72.0, "val_loss": 2771.0433959960938, "val_acc": 28.0}
{"epoch": 33, "training_loss": 9274.399688720703, "training_acc": 28.0, "val_loss": 1413.652515411377, "val_acc": 72.0}
{"epoch": 34, "training_loss": 7369.594268798828, "training_acc": 72.0, "val_loss": 3082.2404861450195, "val_acc": 72.0}
{"epoch": 35, "training_loss": 11742.153564453125, "training_acc": 72.0, "val_loss": 1714.6574020385742, "val_acc": 72.0}
{"epoch": 36, "training_loss": 4241.164310455322, "training_acc": 72.0, "val_loss": 4035.4618072509766, "val_acc": 28.0}
{"epoch": 37, "training_loss": 14860.834167480469, "training_acc": 28.0, "val_loss": 617.7900314331055, "val_acc": 72.0}
{"epoch": 38, "training_loss": 3628.762008666992, "training_acc": 72.0, "val_loss": 2031.6532135009766, "val_acc": 72.0}
{"epoch": 39, "training_loss": 7477.591506958008, "training_acc": 72.0, "val_loss": 558.866548538208, "val_acc": 72.0}
{"epoch": 40, "training_loss": 3982.0709228515625, "training_acc": 62.0, "val_loss": 1694.3414688110352, "val_acc": 28.0}
{"epoch": 41, "training_loss": 4386.632255554199, "training_acc": 54.0, "val_loss": 2304.212188720703, "val_acc": 72.0}
{"epoch": 42, "training_loss": 9478.687591552734, "training_acc": 72.0, "val_loss": 2376.5703201293945, "val_acc": 72.0}
{"epoch": 43, "training_loss": 8150.005813598633, "training_acc": 72.0, "val_loss": 311.67383193969727, "val_acc": 36.0}
{"epoch": 44, "training_loss": 1468.1777877807617, "training_acc": 42.0, "val_loss": 1461.8968963623047, "val_acc": 72.0}
{"epoch": 45, "training_loss": 7586.708404541016, "training_acc": 72.0, "val_loss": 2399.4508743286133, "val_acc": 72.0}
