"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 68.12434339523315, "training_acc": 72.0, "val_loss": 17.00647920370102, "val_acc": 28.0}
{"epoch": 1, "training_loss": 68.06195831298828, "training_acc": 72.0, "val_loss": 16.99613928794861, "val_acc": 28.0}
{"epoch": 2, "training_loss": 68.0465247631073, "training_acc": 72.0, "val_loss": 16.985896229743958, "val_acc": 28.0}
{"epoch": 3, "training_loss": 68.0164864063263, "training_acc": 72.0, "val_loss": 16.975603997707367, "val_acc": 28.0}
{"epoch": 4, "training_loss": 67.97504138946533, "training_acc": 72.0, "val_loss": 16.965357959270477, "val_acc": 28.0}
{"epoch": 5, "training_loss": 67.88874626159668, "training_acc": 72.0, "val_loss": 16.95508509874344, "val_acc": 28.0}
{"epoch": 6, "training_loss": 67.90195822715759, "training_acc": 72.0, "val_loss": 16.944919526576996, "val_acc": 28.0}
{"epoch": 7, "training_loss": 67.83807396888733, "training_acc": 72.0, "val_loss": 16.934730112552643, "val_acc": 28.0}
{"epoch": 8, "training_loss": 67.76546049118042, "training_acc": 72.0, "val_loss": 16.924691200256348, "val_acc": 28.0}
{"epoch": 9, "training_loss": 67.76517176628113, "training_acc": 72.0, "val_loss": 16.91465973854065, "val_acc": 28.0}
{"epoch": 10, "training_loss": 67.72387266159058, "training_acc": 72.0, "val_loss": 16.90487563610077, "val_acc": 28.0}
{"epoch": 11, "training_loss": 67.65596389770508, "training_acc": 72.0, "val_loss": 16.8950155377388, "val_acc": 28.0}
{"epoch": 12, "training_loss": 67.62240314483643, "training_acc": 72.0, "val_loss": 16.885270178318024, "val_acc": 28.0}
{"epoch": 13, "training_loss": 67.59982347488403, "training_acc": 72.0, "val_loss": 16.875484585762024, "val_acc": 28.0}
{"epoch": 14, "training_loss": 67.55626940727234, "training_acc": 72.0, "val_loss": 16.865836083889008, "val_acc": 28.0}
{"epoch": 15, "training_loss": 67.48633694648743, "training_acc": 72.0, "val_loss": 16.85599982738495, "val_acc": 28.0}
{"epoch": 16, "training_loss": 67.4822039604187, "training_acc": 72.0, "val_loss": 16.84626340866089, "val_acc": 28.0}
{"epoch": 17, "training_loss": 67.40720748901367, "training_acc": 72.0, "val_loss": 16.83666706085205, "val_acc": 28.0}
{"epoch": 18, "training_loss": 67.38978266716003, "training_acc": 72.0, "val_loss": 16.82710200548172, "val_acc": 28.0}
{"epoch": 19, "training_loss": 67.37498927116394, "training_acc": 72.0, "val_loss": 16.817733645439148, "val_acc": 28.0}
{"epoch": 20, "training_loss": 67.33223915100098, "training_acc": 72.0, "val_loss": 16.80825799703598, "val_acc": 28.0}
{"epoch": 21, "training_loss": 67.29926347732544, "training_acc": 72.0, "val_loss": 16.79880917072296, "val_acc": 28.0}
{"epoch": 22, "training_loss": 67.25539469718933, "training_acc": 72.0, "val_loss": 16.789351403713226, "val_acc": 28.0}
{"epoch": 23, "training_loss": 67.21039271354675, "training_acc": 72.0, "val_loss": 16.779792308807373, "val_acc": 28.0}
{"epoch": 24, "training_loss": 67.17045569419861, "training_acc": 72.0, "val_loss": 16.770316660404205, "val_acc": 28.0}
{"epoch": 25, "training_loss": 67.11688137054443, "training_acc": 72.0, "val_loss": 16.760772466659546, "val_acc": 28.0}
{"epoch": 26, "training_loss": 67.0841212272644, "training_acc": 72.0, "val_loss": 16.751249134540558, "val_acc": 28.0}
{"epoch": 27, "training_loss": 67.07135343551636, "training_acc": 72.0, "val_loss": 16.741637885570526, "val_acc": 28.0}
{"epoch": 28, "training_loss": 67.03676390647888, "training_acc": 72.0, "val_loss": 16.732096672058105, "val_acc": 28.0}
{"epoch": 29, "training_loss": 66.9459924697876, "training_acc": 72.0, "val_loss": 16.72264039516449, "val_acc": 28.0}
{"epoch": 30, "training_loss": 66.95154595375061, "training_acc": 72.0, "val_loss": 16.71319007873535, "val_acc": 28.0}
{"epoch": 31, "training_loss": 66.90861463546753, "training_acc": 72.0, "val_loss": 16.703906655311584, "val_acc": 28.0}
{"epoch": 32, "training_loss": 66.88490271568298, "training_acc": 72.0, "val_loss": 16.694670915603638, "val_acc": 28.0}
{"epoch": 33, "training_loss": 66.82804584503174, "training_acc": 72.0, "val_loss": 16.685478389263153, "val_acc": 28.0}
{"epoch": 34, "training_loss": 66.7947907447815, "training_acc": 72.0, "val_loss": 16.676323115825653, "val_acc": 28.0}
{"epoch": 35, "training_loss": 66.74179029464722, "training_acc": 72.0, "val_loss": 16.667144000530243, "val_acc": 28.0}
{"epoch": 36, "training_loss": 66.77375817298889, "training_acc": 72.0, "val_loss": 16.65806472301483, "val_acc": 28.0}
{"epoch": 37, "training_loss": 66.70856547355652, "training_acc": 72.0, "val_loss": 16.649070382118225, "val_acc": 28.0}
{"epoch": 38, "training_loss": 66.6421308517456, "training_acc": 72.0, "val_loss": 16.64031893014908, "val_acc": 28.0}
{"epoch": 39, "training_loss": 66.62194538116455, "training_acc": 72.0, "val_loss": 16.631492972373962, "val_acc": 28.0}
{"epoch": 40, "training_loss": 66.58942151069641, "training_acc": 72.0, "val_loss": 16.622774302959442, "val_acc": 28.0}
{"epoch": 41, "training_loss": 66.54495596885681, "training_acc": 72.0, "val_loss": 16.61425232887268, "val_acc": 28.0}
{"epoch": 42, "training_loss": 66.52351069450378, "training_acc": 72.0, "val_loss": 16.605734825134277, "val_acc": 28.0}
