"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 936.3858337402344, "training_acc": 40.0, "val_loss": 463.33255767822266, "val_acc": 72.0}
{"epoch": 1, "training_loss": 1484.4937705993652, "training_acc": 72.0, "val_loss": 65.9644603729248, "val_acc": 28.0}
{"epoch": 2, "training_loss": 185.29193019866943, "training_acc": 46.0, "val_loss": 44.74114775657654, "val_acc": 72.0}
{"epoch": 3, "training_loss": 167.74114513397217, "training_acc": 52.0, "val_loss": 73.0119526386261, "val_acc": 72.0}
{"epoch": 4, "training_loss": 314.7554302215576, "training_acc": 72.0, "val_loss": 27.488896250724792, "val_acc": 72.0}
{"epoch": 5, "training_loss": 418.2284240722656, "training_acc": 52.0, "val_loss": 89.67441916465759, "val_acc": 28.0}
{"epoch": 6, "training_loss": 303.1476106643677, "training_acc": 52.0, "val_loss": 212.5314235687256, "val_acc": 72.0}
{"epoch": 7, "training_loss": 928.446949005127, "training_acc": 72.0, "val_loss": 260.4235887527466, "val_acc": 72.0}
{"epoch": 8, "training_loss": 972.6822509765625, "training_acc": 72.0, "val_loss": 131.65026903152466, "val_acc": 72.0}
{"epoch": 9, "training_loss": 331.74628496170044, "training_acc": 72.0, "val_loss": 320.6472158432007, "val_acc": 28.0}
{"epoch": 10, "training_loss": 1377.614028930664, "training_acc": 28.0, "val_loss": 151.5989899635315, "val_acc": 28.0}
{"epoch": 11, "training_loss": 542.428014755249, "training_acc": 44.0, "val_loss": 224.24466609954834, "val_acc": 72.0}
{"epoch": 12, "training_loss": 993.5406799316406, "training_acc": 72.0, "val_loss": 318.8976287841797, "val_acc": 72.0}
{"epoch": 13, "training_loss": 1259.0963020324707, "training_acc": 72.0, "val_loss": 259.20262336730957, "val_acc": 72.0}
{"epoch": 14, "training_loss": 917.4396820068359, "training_acc": 72.0, "val_loss": 65.38345217704773, "val_acc": 72.0}
{"epoch": 15, "training_loss": 546.0737838745117, "training_acc": 48.0, "val_loss": 257.1671724319458, "val_acc": 28.0}
{"epoch": 16, "training_loss": 786.070972442627, "training_acc": 28.0, "val_loss": 112.55338191986084, "val_acc": 72.0}
{"epoch": 17, "training_loss": 629.9165306091309, "training_acc": 72.0, "val_loss": 255.14721870422363, "val_acc": 72.0}
{"epoch": 18, "training_loss": 1032.805534362793, "training_acc": 72.0, "val_loss": 235.07726192474365, "val_acc": 72.0}
{"epoch": 19, "training_loss": 865.4855728149414, "training_acc": 72.0, "val_loss": 85.77756881713867, "val_acc": 72.0}
{"epoch": 20, "training_loss": 294.62810134887695, "training_acc": 58.0, "val_loss": 105.40441274642944, "val_acc": 28.0}
{"epoch": 21, "training_loss": 323.37622833251953, "training_acc": 42.0, "val_loss": 60.582154989242554, "val_acc": 72.0}
{"epoch": 22, "training_loss": 239.11567783355713, "training_acc": 72.0, "val_loss": 18.219730257987976, "val_acc": 72.0}
{"epoch": 23, "training_loss": 234.92910766601562, "training_acc": 54.0, "val_loss": 43.785521388053894, "val_acc": 28.0}
{"epoch": 24, "training_loss": 252.13168239593506, "training_acc": 42.0, "val_loss": 146.19488716125488, "val_acc": 72.0}
{"epoch": 25, "training_loss": 605.4321365356445, "training_acc": 72.0, "val_loss": 150.0974416732788, "val_acc": 72.0}
{"epoch": 26, "training_loss": 525.6354179382324, "training_acc": 72.0, "val_loss": 29.917150735855103, "val_acc": 72.0}
{"epoch": 27, "training_loss": 286.2565174102783, "training_acc": 62.0, "val_loss": 200.9068489074707, "val_acc": 28.0}
{"epoch": 28, "training_loss": 534.969731092453, "training_acc": 28.0, "val_loss": 110.00274419784546, "val_acc": 72.0}
{"epoch": 29, "training_loss": 513.8836326599121, "training_acc": 72.0, "val_loss": 210.3510856628418, "val_acc": 72.0}
{"epoch": 30, "training_loss": 840.1822738647461, "training_acc": 72.0, "val_loss": 171.53704166412354, "val_acc": 72.0}
{"epoch": 31, "training_loss": 584.3859825134277, "training_acc": 72.0, "val_loss": 14.898017048835754, "val_acc": 72.0}
{"epoch": 32, "training_loss": 323.0804748535156, "training_acc": 56.0, "val_loss": 161.54160499572754, "val_acc": 28.0}
{"epoch": 33, "training_loss": 446.0489263534546, "training_acc": 46.0, "val_loss": 91.43924713134766, "val_acc": 72.0}
{"epoch": 34, "training_loss": 390.03864097595215, "training_acc": 72.0, "val_loss": 93.78572702407837, "val_acc": 72.0}
{"epoch": 35, "training_loss": 312.4514045715332, "training_acc": 72.0, "val_loss": 85.56841611862183, "val_acc": 28.0}
{"epoch": 36, "training_loss": 272.4584975242615, "training_acc": 28.0, "val_loss": 72.30280041694641, "val_acc": 72.0}
{"epoch": 37, "training_loss": 338.69598960876465, "training_acc": 72.0, "val_loss": 119.60642337799072, "val_acc": 72.0}
{"epoch": 38, "training_loss": 433.8234748840332, "training_acc": 72.0, "val_loss": 33.22104215621948, "val_acc": 72.0}
{"epoch": 39, "training_loss": 293.74123191833496, "training_acc": 56.0, "val_loss": 126.1650800704956, "val_acc": 28.0}
{"epoch": 40, "training_loss": 338.99353313446045, "training_acc": 50.0, "val_loss": 93.1516945362091, "val_acc": 72.0}
{"epoch": 41, "training_loss": 380.52849292755127, "training_acc": 72.0, "val_loss": 90.11744260787964, "val_acc": 72.0}
{"epoch": 42, "training_loss": 281.53244495391846, "training_acc": 72.0, "val_loss": 80.97444772720337, "val_acc": 28.0}
{"epoch": 43, "training_loss": 264.6813361644745, "training_acc": 28.0, "val_loss": 55.89664578437805, "val_acc": 72.0}
{"epoch": 44, "training_loss": 269.0567150115967, "training_acc": 72.0, "val_loss": 69.53535079956055, "val_acc": 72.0}
{"epoch": 45, "training_loss": 232.16787767410278, "training_acc": 72.0, "val_loss": 107.04548358917236, "val_acc": 28.0}
{"epoch": 46, "training_loss": 306.7639241218567, "training_acc": 28.0, "val_loss": 83.15356969833374, "val_acc": 72.0}
{"epoch": 47, "training_loss": 407.3179340362549, "training_acc": 72.0, "val_loss": 140.66187143325806, "val_acc": 72.0}
{"epoch": 48, "training_loss": 526.2162618637085, "training_acc": 72.0, "val_loss": 54.55339550971985, "val_acc": 72.0}
{"epoch": 49, "training_loss": 189.43903350830078, "training_acc": 62.0, "val_loss": 47.8218674659729, "val_acc": 28.0}
{"epoch": 50, "training_loss": 137.0413134098053, "training_acc": 56.0, "val_loss": 104.34222221374512, "val_acc": 72.0}
