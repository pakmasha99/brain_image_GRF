"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 263.23047065734863, "training_acc": 46.0, "val_loss": 156.2297224998474, "val_acc": 72.0}
{"epoch": 1, "training_loss": 516.0754337310791, "training_acc": 72.0, "val_loss": 63.71967792510986, "val_acc": 28.0}
{"epoch": 2, "training_loss": 194.86882638931274, "training_acc": 28.0, "val_loss": 46.794408559799194, "val_acc": 72.0}
{"epoch": 3, "training_loss": 208.6961488723755, "training_acc": 72.0, "val_loss": 57.84304141998291, "val_acc": 72.0}
{"epoch": 4, "training_loss": 192.62426948547363, "training_acc": 72.0, "val_loss": 36.676791310310364, "val_acc": 28.0}
{"epoch": 5, "training_loss": 140.20239305496216, "training_acc": 28.0, "val_loss": 27.544641494750977, "val_acc": 72.0}
{"epoch": 6, "training_loss": 138.9333667755127, "training_acc": 72.0, "val_loss": 45.224130153656006, "val_acc": 72.0}
{"epoch": 7, "training_loss": 158.40058088302612, "training_acc": 72.0, "val_loss": 18.46662312746048, "val_acc": 28.0}
{"epoch": 8, "training_loss": 90.1705379486084, "training_acc": 28.0, "val_loss": 14.894865453243256, "val_acc": 72.0}
{"epoch": 9, "training_loss": 71.36740803718567, "training_acc": 72.0, "val_loss": 23.512841761112213, "val_acc": 72.0}
{"epoch": 10, "training_loss": 87.89538168907166, "training_acc": 72.0, "val_loss": 20.26073932647705, "val_acc": 28.0}
{"epoch": 11, "training_loss": 71.18608474731445, "training_acc": 50.0, "val_loss": 20.983392000198364, "val_acc": 72.0}
{"epoch": 12, "training_loss": 91.62214183807373, "training_acc": 72.0, "val_loss": 15.700910985469818, "val_acc": 72.0}
{"epoch": 13, "training_loss": 59.24232888221741, "training_acc": 66.0, "val_loss": 22.7897047996521, "val_acc": 28.0}
{"epoch": 14, "training_loss": 90.66559648513794, "training_acc": 40.0, "val_loss": 23.268325626850128, "val_acc": 72.0}
{"epoch": 15, "training_loss": 88.0252457857132, "training_acc": 72.0, "val_loss": 16.480226814746857, "val_acc": 28.0}
{"epoch": 16, "training_loss": 65.38282203674316, "training_acc": 72.0, "val_loss": 16.58778488636017, "val_acc": 72.0}
{"epoch": 17, "training_loss": 68.90562438964844, "training_acc": 72.0, "val_loss": 14.79661762714386, "val_acc": 72.0}
{"epoch": 18, "training_loss": 74.58137083053589, "training_acc": 50.0, "val_loss": 15.864652395248413, "val_acc": 72.0}
{"epoch": 19, "training_loss": 77.56171131134033, "training_acc": 72.0, "val_loss": 16.901874542236328, "val_acc": 72.0}
{"epoch": 20, "training_loss": 101.6179347038269, "training_acc": 44.0, "val_loss": 14.986379444599152, "val_acc": 72.0}
{"epoch": 21, "training_loss": 60.46815073490143, "training_acc": 72.0, "val_loss": 21.92646414041519, "val_acc": 72.0}
{"epoch": 22, "training_loss": 76.25454759597778, "training_acc": 72.0, "val_loss": 23.55394810438156, "val_acc": 28.0}
{"epoch": 23, "training_loss": 84.77128148078918, "training_acc": 40.0, "val_loss": 17.355598509311676, "val_acc": 72.0}
{"epoch": 24, "training_loss": 73.2737352848053, "training_acc": 72.0, "val_loss": 14.937061071395874, "val_acc": 72.0}
{"epoch": 25, "training_loss": 65.75424480438232, "training_acc": 58.0, "val_loss": 14.796262979507446, "val_acc": 72.0}
{"epoch": 26, "training_loss": 65.10139560699463, "training_acc": 72.0, "val_loss": 16.246314346790314, "val_acc": 72.0}
{"epoch": 27, "training_loss": 59.11863565444946, "training_acc": 72.0, "val_loss": 20.36387026309967, "val_acc": 28.0}
{"epoch": 28, "training_loss": 73.61589360237122, "training_acc": 44.0, "val_loss": 19.3658247590065, "val_acc": 72.0}
{"epoch": 29, "training_loss": 73.60417425632477, "training_acc": 72.0, "val_loss": 15.560023486614227, "val_acc": 32.0}
{"epoch": 30, "training_loss": 61.326324224472046, "training_acc": 72.0, "val_loss": 14.77779746055603, "val_acc": 72.0}
{"epoch": 31, "training_loss": 58.97495985031128, "training_acc": 72.0, "val_loss": 15.298107266426086, "val_acc": 72.0}
{"epoch": 32, "training_loss": 60.11285352706909, "training_acc": 72.0, "val_loss": 14.802846312522888, "val_acc": 72.0}
{"epoch": 33, "training_loss": 60.77846074104309, "training_acc": 72.0, "val_loss": 15.908421576023102, "val_acc": 72.0}
{"epoch": 34, "training_loss": 63.016002893447876, "training_acc": 72.0, "val_loss": 15.358087420463562, "val_acc": 52.0}
{"epoch": 35, "training_loss": 61.59691333770752, "training_acc": 72.0, "val_loss": 15.638633072376251, "val_acc": 72.0}
{"epoch": 36, "training_loss": 61.97757053375244, "training_acc": 72.0, "val_loss": 14.83282595872879, "val_acc": 72.0}
{"epoch": 37, "training_loss": 60.15208864212036, "training_acc": 72.0, "val_loss": 15.087538957595825, "val_acc": 72.0}
{"epoch": 38, "training_loss": 62.9067862033844, "training_acc": 72.0, "val_loss": 15.093439817428589, "val_acc": 68.0}
{"epoch": 39, "training_loss": 61.69292712211609, "training_acc": 75.0, "val_loss": 15.67257046699524, "val_acc": 72.0}
{"epoch": 40, "training_loss": 65.51996970176697, "training_acc": 72.0, "val_loss": 15.203101933002472, "val_acc": 56.0}
{"epoch": 41, "training_loss": 59.71585559844971, "training_acc": 72.0, "val_loss": 15.52150696516037, "val_acc": 44.0}
{"epoch": 42, "training_loss": 58.24854230880737, "training_acc": 72.0, "val_loss": 17.42548644542694, "val_acc": 72.0}
{"epoch": 43, "training_loss": 64.41081094741821, "training_acc": 72.0, "val_loss": 17.177921533584595, "val_acc": 28.0}
{"epoch": 44, "training_loss": 63.06791424751282, "training_acc": 68.0, "val_loss": 17.72221475839615, "val_acc": 72.0}
{"epoch": 45, "training_loss": 65.91986227035522, "training_acc": 72.0, "val_loss": 18.577808141708374, "val_acc": 28.0}
{"epoch": 46, "training_loss": 66.64966154098511, "training_acc": 48.0, "val_loss": 20.714181661605835, "val_acc": 72.0}
{"epoch": 47, "training_loss": 77.5426378250122, "training_acc": 72.0, "val_loss": 15.364694595336914, "val_acc": 52.0}
{"epoch": 48, "training_loss": 62.345606088638306, "training_acc": 77.0, "val_loss": 16.41472578048706, "val_acc": 72.0}
{"epoch": 49, "training_loss": 65.60470223426819, "training_acc": 72.0, "val_loss": 15.489757061004639, "val_acc": 44.0}
