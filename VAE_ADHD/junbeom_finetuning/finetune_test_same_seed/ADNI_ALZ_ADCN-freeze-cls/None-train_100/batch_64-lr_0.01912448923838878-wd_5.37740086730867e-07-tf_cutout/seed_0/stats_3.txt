"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 119.29490232467651, "training_acc": 46.0, "val_loss": 53.34274768829346, "val_acc": 72.0}
{"epoch": 1, "training_loss": 179.17467737197876, "training_acc": 72.0, "val_loss": 22.714917361736298, "val_acc": 28.0}
{"epoch": 2, "training_loss": 107.52281141281128, "training_acc": 28.0, "val_loss": 14.840470254421234, "val_acc": 72.0}
{"epoch": 3, "training_loss": 64.90598011016846, "training_acc": 72.0, "val_loss": 27.795737981796265, "val_acc": 72.0}
{"epoch": 4, "training_loss": 108.48764824867249, "training_acc": 72.0, "val_loss": 18.665218353271484, "val_acc": 72.0}
{"epoch": 5, "training_loss": 75.6057620048523, "training_acc": 72.0, "val_loss": 20.167705416679382, "val_acc": 28.0}
{"epoch": 6, "training_loss": 76.59054040908813, "training_acc": 28.0, "val_loss": 15.029875934123993, "val_acc": 72.0}
{"epoch": 7, "training_loss": 67.2311851978302, "training_acc": 72.0, "val_loss": 18.83501410484314, "val_acc": 72.0}
{"epoch": 8, "training_loss": 72.59934616088867, "training_acc": 72.0, "val_loss": 14.93685096502304, "val_acc": 72.0}
{"epoch": 9, "training_loss": 65.04697513580322, "training_acc": 52.0, "val_loss": 15.87730497121811, "val_acc": 28.0}
{"epoch": 10, "training_loss": 61.23617458343506, "training_acc": 72.0, "val_loss": 16.377750039100647, "val_acc": 72.0}
{"epoch": 11, "training_loss": 66.72320032119751, "training_acc": 72.0, "val_loss": 15.481697022914886, "val_acc": 72.0}
{"epoch": 12, "training_loss": 59.15779972076416, "training_acc": 72.0, "val_loss": 17.091120779514313, "val_acc": 28.0}
{"epoch": 13, "training_loss": 68.43631148338318, "training_acc": 72.0, "val_loss": 14.805126190185547, "val_acc": 72.0}
{"epoch": 14, "training_loss": 60.159749031066895, "training_acc": 72.0, "val_loss": 17.255061864852905, "val_acc": 72.0}
{"epoch": 15, "training_loss": 67.81145429611206, "training_acc": 72.0, "val_loss": 14.806842803955078, "val_acc": 72.0}
{"epoch": 16, "training_loss": 59.62132167816162, "training_acc": 72.0, "val_loss": 15.92637449502945, "val_acc": 28.0}
{"epoch": 17, "training_loss": 63.17410206794739, "training_acc": 72.0, "val_loss": 14.95918482542038, "val_acc": 72.0}
{"epoch": 18, "training_loss": 59.893205642700195, "training_acc": 72.0, "val_loss": 14.96409922838211, "val_acc": 72.0}
{"epoch": 19, "training_loss": 59.38798499107361, "training_acc": 72.0, "val_loss": 15.06858915090561, "val_acc": 72.0}
{"epoch": 20, "training_loss": 61.11234211921692, "training_acc": 72.0, "val_loss": 14.806339144706726, "val_acc": 72.0}
{"epoch": 21, "training_loss": 58.52711308002472, "training_acc": 72.0, "val_loss": 15.698166191577911, "val_acc": 72.0}
{"epoch": 22, "training_loss": 62.799633502960205, "training_acc": 72.0, "val_loss": 14.795204997062683, "val_acc": 72.0}
{"epoch": 23, "training_loss": 60.29934620857239, "training_acc": 72.0, "val_loss": 15.525135397911072, "val_acc": 28.0}
{"epoch": 24, "training_loss": 63.280293703079224, "training_acc": 72.0, "val_loss": 15.026898682117462, "val_acc": 72.0}
{"epoch": 25, "training_loss": 60.92523908615112, "training_acc": 72.0, "val_loss": 14.787937700748444, "val_acc": 72.0}
{"epoch": 26, "training_loss": 59.19872975349426, "training_acc": 72.0, "val_loss": 14.786039292812347, "val_acc": 72.0}
{"epoch": 27, "training_loss": 59.71688199043274, "training_acc": 72.0, "val_loss": 14.784219861030579, "val_acc": 72.0}
{"epoch": 28, "training_loss": 59.13336181640625, "training_acc": 72.0, "val_loss": 14.972960948944092, "val_acc": 72.0}
{"epoch": 29, "training_loss": 59.85808873176575, "training_acc": 72.0, "val_loss": 14.77549523115158, "val_acc": 72.0}
{"epoch": 30, "training_loss": 59.96783661842346, "training_acc": 72.0, "val_loss": 14.772333204746246, "val_acc": 72.0}
{"epoch": 31, "training_loss": 59.11049795150757, "training_acc": 72.0, "val_loss": 15.196758508682251, "val_acc": 72.0}
{"epoch": 32, "training_loss": 62.02374768257141, "training_acc": 72.0, "val_loss": 14.803382754325867, "val_acc": 72.0}
{"epoch": 33, "training_loss": 60.16865277290344, "training_acc": 72.0, "val_loss": 15.404033660888672, "val_acc": 28.0}
{"epoch": 34, "training_loss": 66.72100925445557, "training_acc": 72.0, "val_loss": 14.994804561138153, "val_acc": 72.0}
{"epoch": 35, "training_loss": 58.963287353515625, "training_acc": 72.0, "val_loss": 15.661659836769104, "val_acc": 28.0}
{"epoch": 36, "training_loss": 62.04191994667053, "training_acc": 72.0, "val_loss": 14.791591465473175, "val_acc": 72.0}
{"epoch": 37, "training_loss": 59.02928590774536, "training_acc": 72.0, "val_loss": 15.174742043018341, "val_acc": 72.0}
{"epoch": 38, "training_loss": 60.69856810569763, "training_acc": 72.0, "val_loss": 15.16803503036499, "val_acc": 72.0}
{"epoch": 39, "training_loss": 60.695348501205444, "training_acc": 72.0, "val_loss": 15.178635716438293, "val_acc": 76.0}
{"epoch": 40, "training_loss": 60.752047061920166, "training_acc": 72.0, "val_loss": 15.097375214099884, "val_acc": 72.0}
{"epoch": 41, "training_loss": 60.065014123916626, "training_acc": 72.0, "val_loss": 14.879904687404633, "val_acc": 72.0}
{"epoch": 42, "training_loss": 60.71341347694397, "training_acc": 72.0, "val_loss": 14.742346107959747, "val_acc": 72.0}
{"epoch": 43, "training_loss": 59.6861047744751, "training_acc": 72.0, "val_loss": 15.212024748325348, "val_acc": 72.0}
{"epoch": 44, "training_loss": 59.858642578125, "training_acc": 72.0, "val_loss": 15.364663302898407, "val_acc": 28.0}
{"epoch": 45, "training_loss": 61.16330075263977, "training_acc": 72.0, "val_loss": 14.803160727024078, "val_acc": 72.0}
{"epoch": 46, "training_loss": 63.95523715019226, "training_acc": 72.0, "val_loss": 14.758358895778656, "val_acc": 72.0}
{"epoch": 47, "training_loss": 60.89777112007141, "training_acc": 72.0, "val_loss": 15.457843244075775, "val_acc": 28.0}
{"epoch": 48, "training_loss": 60.94921827316284, "training_acc": 72.0, "val_loss": 15.825571119785309, "val_acc": 72.0}
{"epoch": 49, "training_loss": 62.49993848800659, "training_acc": 72.0, "val_loss": 14.943763613700867, "val_acc": 72.0}
{"epoch": 50, "training_loss": 60.07199788093567, "training_acc": 72.0, "val_loss": 15.090131759643555, "val_acc": 72.0}
{"epoch": 51, "training_loss": 59.96073055267334, "training_acc": 72.0, "val_loss": 15.350554883480072, "val_acc": 72.0}
{"epoch": 52, "training_loss": 61.53982090950012, "training_acc": 72.0, "val_loss": 15.219464898109436, "val_acc": 68.0}
{"epoch": 53, "training_loss": 60.997204303741455, "training_acc": 72.0, "val_loss": 15.129251778125763, "val_acc": 76.0}
{"epoch": 54, "training_loss": 59.44646334648132, "training_acc": 72.0, "val_loss": 15.59179276227951, "val_acc": 72.0}
{"epoch": 55, "training_loss": 62.44702959060669, "training_acc": 72.0, "val_loss": 14.91813212633133, "val_acc": 72.0}
{"epoch": 56, "training_loss": 65.99934101104736, "training_acc": 50.0, "val_loss": 14.766225218772888, "val_acc": 72.0}
{"epoch": 57, "training_loss": 65.41534447669983, "training_acc": 72.0, "val_loss": 16.42731875181198, "val_acc": 72.0}
{"epoch": 58, "training_loss": 61.39535140991211, "training_acc": 72.0, "val_loss": 18.386997282505035, "val_acc": 28.0}
{"epoch": 59, "training_loss": 69.9104654788971, "training_acc": 42.0, "val_loss": 15.379326045513153, "val_acc": 72.0}
{"epoch": 60, "training_loss": 62.0051691532135, "training_acc": 72.0, "val_loss": 15.488606691360474, "val_acc": 72.0}
{"epoch": 61, "training_loss": 59.31045961380005, "training_acc": 72.0, "val_loss": 17.00347363948822, "val_acc": 28.0}
