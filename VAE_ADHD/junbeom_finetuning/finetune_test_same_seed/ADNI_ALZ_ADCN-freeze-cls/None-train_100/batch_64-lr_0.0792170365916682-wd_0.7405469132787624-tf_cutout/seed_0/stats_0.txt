"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 417.9195785522461, "training_acc": 42.0, "val_loss": 189.9283766746521, "val_acc": 72.0}
{"epoch": 1, "training_loss": 565.0152292251587, "training_acc": 72.0, "val_loss": 196.14285230636597, "val_acc": 28.0}
{"epoch": 2, "training_loss": 695.904993057251, "training_acc": 28.0, "val_loss": 42.626696825027466, "val_acc": 72.0}
{"epoch": 3, "training_loss": 242.19519424438477, "training_acc": 72.0, "val_loss": 106.8840503692627, "val_acc": 72.0}
{"epoch": 4, "training_loss": 406.37411308288574, "training_acc": 72.0, "val_loss": 55.244892835617065, "val_acc": 72.0}
{"epoch": 5, "training_loss": 209.83671188354492, "training_acc": 48.0, "val_loss": 36.78102791309357, "val_acc": 28.0}
{"epoch": 6, "training_loss": 129.08904337882996, "training_acc": 44.0, "val_loss": 36.71972453594208, "val_acc": 72.0}
{"epoch": 7, "training_loss": 135.16667008399963, "training_acc": 72.0, "val_loss": 16.16414189338684, "val_acc": 28.0}
{"epoch": 8, "training_loss": 72.88375854492188, "training_acc": 58.0, "val_loss": 17.88179725408554, "val_acc": 72.0}
{"epoch": 9, "training_loss": 71.90254020690918, "training_acc": 72.0, "val_loss": 15.151140093803406, "val_acc": 72.0}
{"epoch": 10, "training_loss": 65.94997000694275, "training_acc": 56.0, "val_loss": 15.109330415725708, "val_acc": 72.0}
{"epoch": 11, "training_loss": 62.83030390739441, "training_acc": 72.0, "val_loss": 14.781361818313599, "val_acc": 72.0}
{"epoch": 12, "training_loss": 64.07007575035095, "training_acc": 54.0, "val_loss": 16.95488840341568, "val_acc": 72.0}
{"epoch": 13, "training_loss": 67.6542489528656, "training_acc": 72.0, "val_loss": 14.975108206272125, "val_acc": 72.0}
{"epoch": 14, "training_loss": 58.90795540809631, "training_acc": 72.0, "val_loss": 16.612128913402557, "val_acc": 28.0}
{"epoch": 15, "training_loss": 70.05681157112122, "training_acc": 72.0, "val_loss": 14.854052662849426, "val_acc": 72.0}
{"epoch": 16, "training_loss": 58.99369144439697, "training_acc": 72.0, "val_loss": 15.187802910804749, "val_acc": 72.0}
{"epoch": 17, "training_loss": 58.44509172439575, "training_acc": 72.0, "val_loss": 18.05102527141571, "val_acc": 72.0}
{"epoch": 18, "training_loss": 66.53171253204346, "training_acc": 72.0, "val_loss": 28.341320157051086, "val_acc": 28.0}
{"epoch": 19, "training_loss": 86.70875930786133, "training_acc": 50.0, "val_loss": 26.575645804405212, "val_acc": 72.0}
{"epoch": 20, "training_loss": 95.53468537330627, "training_acc": 72.0, "val_loss": 24.13094788789749, "val_acc": 28.0}
{"epoch": 21, "training_loss": 95.0064115524292, "training_acc": 38.0, "val_loss": 15.75470119714737, "val_acc": 72.0}
{"epoch": 22, "training_loss": 66.01513528823853, "training_acc": 56.0, "val_loss": 15.131427347660065, "val_acc": 72.0}
{"epoch": 23, "training_loss": 60.75446319580078, "training_acc": 72.0, "val_loss": 14.778351783752441, "val_acc": 72.0}
{"epoch": 24, "training_loss": 59.70979690551758, "training_acc": 72.0, "val_loss": 15.272995829582214, "val_acc": 72.0}
{"epoch": 25, "training_loss": 60.56317639350891, "training_acc": 72.0, "val_loss": 16.840705275535583, "val_acc": 28.0}
{"epoch": 26, "training_loss": 63.645694732666016, "training_acc": 72.0, "val_loss": 16.61270260810852, "val_acc": 72.0}
{"epoch": 27, "training_loss": 64.81718039512634, "training_acc": 72.0, "val_loss": 14.894059300422668, "val_acc": 72.0}
{"epoch": 28, "training_loss": 59.34446668624878, "training_acc": 72.0, "val_loss": 15.07105678319931, "val_acc": 72.0}
{"epoch": 29, "training_loss": 61.315147399902344, "training_acc": 72.0, "val_loss": 15.096031129360199, "val_acc": 72.0}
{"epoch": 30, "training_loss": 62.53912711143494, "training_acc": 72.0, "val_loss": 17.519868910312653, "val_acc": 72.0}
{"epoch": 31, "training_loss": 67.96091032028198, "training_acc": 72.0, "val_loss": 35.22334694862366, "val_acc": 28.0}
{"epoch": 32, "training_loss": 108.57803440093994, "training_acc": 48.0, "val_loss": 28.47897708415985, "val_acc": 72.0}
{"epoch": 33, "training_loss": 100.35859227180481, "training_acc": 72.0, "val_loss": 27.731183171272278, "val_acc": 28.0}
{"epoch": 34, "training_loss": 113.13729524612427, "training_acc": 40.0, "val_loss": 22.634167969226837, "val_acc": 72.0}
{"epoch": 35, "training_loss": 83.50116086006165, "training_acc": 56.0, "val_loss": 15.633752942085266, "val_acc": 28.0}
{"epoch": 36, "training_loss": 64.00563883781433, "training_acc": 72.0, "val_loss": 16.384638845920563, "val_acc": 72.0}
{"epoch": 37, "training_loss": 58.74686336517334, "training_acc": 72.0, "val_loss": 22.133353352546692, "val_acc": 28.0}
{"epoch": 38, "training_loss": 97.90344858169556, "training_acc": 42.0, "val_loss": 25.342416763305664, "val_acc": 72.0}
{"epoch": 39, "training_loss": 75.95989942550659, "training_acc": 72.0, "val_loss": 49.89469349384308, "val_acc": 28.0}
{"epoch": 40, "training_loss": 171.52503967285156, "training_acc": 40.0, "val_loss": 30.172815918922424, "val_acc": 72.0}
{"epoch": 41, "training_loss": 100.03822445869446, "training_acc": 72.0, "val_loss": 43.15950274467468, "val_acc": 28.0}
{"epoch": 42, "training_loss": 129.4848711490631, "training_acc": 46.0, "val_loss": 27.732712030410767, "val_acc": 72.0}
