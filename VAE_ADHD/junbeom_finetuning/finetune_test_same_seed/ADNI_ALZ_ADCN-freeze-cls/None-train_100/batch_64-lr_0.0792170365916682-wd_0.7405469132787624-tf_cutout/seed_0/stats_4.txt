"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 315.5686511993408, "training_acc": 72.0, "val_loss": 180.63390254974365, "val_acc": 72.0}
{"epoch": 1, "training_loss": 474.95044708251953, "training_acc": 72.0, "val_loss": 319.3957567214966, "val_acc": 28.0}
{"epoch": 2, "training_loss": 1169.6716499328613, "training_acc": 28.0, "val_loss": 15.142233669757843, "val_acc": 72.0}
{"epoch": 3, "training_loss": 144.3560037612915, "training_acc": 72.0, "val_loss": 139.20881748199463, "val_acc": 72.0}
{"epoch": 4, "training_loss": 563.1946506500244, "training_acc": 72.0, "val_loss": 131.10766410827637, "val_acc": 72.0}
{"epoch": 5, "training_loss": 473.97879123687744, "training_acc": 72.0, "val_loss": 36.1332893371582, "val_acc": 72.0}
{"epoch": 6, "training_loss": 192.60688591003418, "training_acc": 60.0, "val_loss": 81.90954327583313, "val_acc": 28.0}
{"epoch": 7, "training_loss": 247.16129755973816, "training_acc": 42.0, "val_loss": 45.64314782619476, "val_acc": 72.0}
{"epoch": 8, "training_loss": 180.5559630393982, "training_acc": 72.0, "val_loss": 29.681381583213806, "val_acc": 72.0}
{"epoch": 9, "training_loss": 114.88300013542175, "training_acc": 52.0, "val_loss": 21.105967462062836, "val_acc": 28.0}
{"epoch": 10, "training_loss": 83.89204859733582, "training_acc": 44.0, "val_loss": 24.873942136764526, "val_acc": 72.0}
{"epoch": 11, "training_loss": 84.29837012290955, "training_acc": 72.0, "val_loss": 35.116925835609436, "val_acc": 28.0}
{"epoch": 12, "training_loss": 110.57046961784363, "training_acc": 46.0, "val_loss": 26.462599635124207, "val_acc": 72.0}
{"epoch": 13, "training_loss": 116.22815370559692, "training_acc": 72.0, "val_loss": 16.58020317554474, "val_acc": 72.0}
{"epoch": 14, "training_loss": 102.97783899307251, "training_acc": 54.0, "val_loss": 14.870291948318481, "val_acc": 72.0}
{"epoch": 15, "training_loss": 78.20802068710327, "training_acc": 72.0, "val_loss": 24.723105132579803, "val_acc": 72.0}
{"epoch": 16, "training_loss": 80.29669570922852, "training_acc": 72.0, "val_loss": 34.74559783935547, "val_acc": 28.0}
{"epoch": 17, "training_loss": 113.8934051990509, "training_acc": 42.0, "val_loss": 22.295844554901123, "val_acc": 72.0}
{"epoch": 18, "training_loss": 83.3080894947052, "training_acc": 72.0, "val_loss": 17.231102287769318, "val_acc": 28.0}
{"epoch": 19, "training_loss": 68.21917700767517, "training_acc": 66.0, "val_loss": 14.751431345939636, "val_acc": 72.0}
{"epoch": 20, "training_loss": 61.60832214355469, "training_acc": 72.0, "val_loss": 16.935497522354126, "val_acc": 72.0}
{"epoch": 21, "training_loss": 67.0941390991211, "training_acc": 72.0, "val_loss": 14.75028246641159, "val_acc": 72.0}
{"epoch": 22, "training_loss": 59.95290398597717, "training_acc": 72.0, "val_loss": 15.308676660060883, "val_acc": 72.0}
{"epoch": 23, "training_loss": 61.15950655937195, "training_acc": 72.0, "val_loss": 16.13103449344635, "val_acc": 28.0}
{"epoch": 24, "training_loss": 63.42307925224304, "training_acc": 72.0, "val_loss": 14.955727756023407, "val_acc": 72.0}
{"epoch": 25, "training_loss": 59.21747589111328, "training_acc": 72.0, "val_loss": 15.439465641975403, "val_acc": 28.0}
{"epoch": 26, "training_loss": 66.78811693191528, "training_acc": 72.0, "val_loss": 15.208755433559418, "val_acc": 72.0}
{"epoch": 27, "training_loss": 60.64801645278931, "training_acc": 72.0, "val_loss": 14.875786006450653, "val_acc": 72.0}
{"epoch": 28, "training_loss": 59.37871336936951, "training_acc": 72.0, "val_loss": 14.88303542137146, "val_acc": 72.0}
{"epoch": 29, "training_loss": 60.124961137771606, "training_acc": 72.0, "val_loss": 17.292949557304382, "val_acc": 72.0}
{"epoch": 30, "training_loss": 66.04965901374817, "training_acc": 72.0, "val_loss": 17.824500799179077, "val_acc": 28.0}
{"epoch": 31, "training_loss": 63.29661560058594, "training_acc": 52.0, "val_loss": 23.732002079486847, "val_acc": 72.0}
{"epoch": 32, "training_loss": 88.89960837364197, "training_acc": 72.0, "val_loss": 17.521746456623077, "val_acc": 28.0}
{"epoch": 33, "training_loss": 67.60360288619995, "training_acc": 42.0, "val_loss": 15.159313380718231, "val_acc": 72.0}
{"epoch": 34, "training_loss": 59.0743453502655, "training_acc": 72.0, "val_loss": 17.365144193172455, "val_acc": 28.0}
{"epoch": 35, "training_loss": 68.79998707771301, "training_acc": 47.0, "val_loss": 16.14445149898529, "val_acc": 72.0}
{"epoch": 36, "training_loss": 70.9720094203949, "training_acc": 54.0, "val_loss": 16.943441331386566, "val_acc": 72.0}
{"epoch": 37, "training_loss": 68.6323013305664, "training_acc": 72.0, "val_loss": 18.65803897380829, "val_acc": 28.0}
{"epoch": 38, "training_loss": 69.22630858421326, "training_acc": 44.0, "val_loss": 16.386660933494568, "val_acc": 72.0}
{"epoch": 39, "training_loss": 66.22029614448547, "training_acc": 72.0, "val_loss": 14.87957090139389, "val_acc": 72.0}
{"epoch": 40, "training_loss": 62.095017194747925, "training_acc": 72.0, "val_loss": 18.099820613861084, "val_acc": 28.0}
