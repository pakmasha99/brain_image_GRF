"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 1356.35542678833, "training_acc": 40.0, "val_loss": 614.0617370605469, "val_acc": 72.0}
{"epoch": 1, "training_loss": 1969.4199562072754, "training_acc": 72.0, "val_loss": 571.5781211853027, "val_acc": 28.0}
{"epoch": 2, "training_loss": 1921.0954856872559, "training_acc": 28.0, "val_loss": 187.92086839675903, "val_acc": 72.0}
{"epoch": 3, "training_loss": 1045.9277648925781, "training_acc": 72.0, "val_loss": 393.1255102157593, "val_acc": 72.0}
{"epoch": 4, "training_loss": 1489.3134269714355, "training_acc": 72.0, "val_loss": 206.60183429718018, "val_acc": 72.0}
{"epoch": 5, "training_loss": 563.2339825630188, "training_acc": 72.0, "val_loss": 504.1853904724121, "val_acc": 28.0}
{"epoch": 6, "training_loss": 1975.613914489746, "training_acc": 28.0, "val_loss": 45.37384808063507, "val_acc": 28.0}
{"epoch": 7, "training_loss": 553.4711151123047, "training_acc": 40.0, "val_loss": 409.75003242492676, "val_acc": 72.0}
{"epoch": 8, "training_loss": 1775.7571105957031, "training_acc": 72.0, "val_loss": 487.61420249938965, "val_acc": 72.0}
{"epoch": 9, "training_loss": 1858.3051681518555, "training_acc": 72.0, "val_loss": 316.8238162994385, "val_acc": 72.0}
{"epoch": 10, "training_loss": 1124.7978239059448, "training_acc": 72.0, "val_loss": 62.63245940208435, "val_acc": 28.0}
{"epoch": 11, "training_loss": 237.21306657791138, "training_acc": 28.0, "val_loss": 82.24265575408936, "val_acc": 72.0}
{"epoch": 12, "training_loss": 375.81560134887695, "training_acc": 72.0, "val_loss": 91.42681956291199, "val_acc": 72.0}
{"epoch": 13, "training_loss": 261.8640720844269, "training_acc": 72.0, "val_loss": 215.91875553131104, "val_acc": 28.0}
{"epoch": 14, "training_loss": 689.5440998077393, "training_acc": 28.0, "val_loss": 132.00271129608154, "val_acc": 72.0}
{"epoch": 15, "training_loss": 701.9415855407715, "training_acc": 72.0, "val_loss": 259.9722146987915, "val_acc": 72.0}
{"epoch": 16, "training_loss": 1001.9834823608398, "training_acc": 72.0, "val_loss": 154.45467233657837, "val_acc": 72.0}
{"epoch": 17, "training_loss": 423.8619704246521, "training_acc": 72.0, "val_loss": 326.5077829360962, "val_acc": 28.0}
{"epoch": 18, "training_loss": 1287.7996406555176, "training_acc": 28.0, "val_loss": 22.866366803646088, "val_acc": 28.0}
{"epoch": 19, "training_loss": 317.06824111938477, "training_acc": 42.0, "val_loss": 254.85763549804688, "val_acc": 72.0}
{"epoch": 20, "training_loss": 1092.9016799926758, "training_acc": 72.0, "val_loss": 253.77051830291748, "val_acc": 72.0}
{"epoch": 21, "training_loss": 902.6999950408936, "training_acc": 72.0, "val_loss": 47.79981076717377, "val_acc": 72.0}
{"epoch": 22, "training_loss": 574.3378562927246, "training_acc": 52.0, "val_loss": 265.80162048339844, "val_acc": 28.0}
{"epoch": 23, "training_loss": 761.0694756507874, "training_acc": 40.0, "val_loss": 89.46382999420166, "val_acc": 72.0}
{"epoch": 24, "training_loss": 351.88310527801514, "training_acc": 72.0, "val_loss": 42.60646402835846, "val_acc": 72.0}
{"epoch": 25, "training_loss": 361.6063175201416, "training_acc": 48.0, "val_loss": 15.07452130317688, "val_acc": 72.0}
{"epoch": 26, "training_loss": 120.99788188934326, "training_acc": 72.0, "val_loss": 39.10118341445923, "val_acc": 72.0}
{"epoch": 27, "training_loss": 165.22421312332153, "training_acc": 58.0, "val_loss": 41.09399616718292, "val_acc": 72.0}
{"epoch": 28, "training_loss": 177.54788970947266, "training_acc": 72.0, "val_loss": 18.255598843097687, "val_acc": 28.0}
{"epoch": 29, "training_loss": 83.6704626083374, "training_acc": 28.0, "val_loss": 78.55499386787415, "val_acc": 72.0}
{"epoch": 30, "training_loss": 345.2243709564209, "training_acc": 72.0, "val_loss": 71.03601098060608, "val_acc": 72.0}
{"epoch": 31, "training_loss": 213.37621021270752, "training_acc": 56.0, "val_loss": 18.908412754535675, "val_acc": 28.0}
{"epoch": 32, "training_loss": 146.17948055267334, "training_acc": 40.0, "val_loss": 47.64896333217621, "val_acc": 72.0}
{"epoch": 33, "training_loss": 168.03996419906616, "training_acc": 56.0, "val_loss": 36.96361184120178, "val_acc": 72.0}
{"epoch": 34, "training_loss": 132.38752782344818, "training_acc": 72.0, "val_loss": 41.82826280593872, "val_acc": 28.0}
{"epoch": 35, "training_loss": 254.4144172668457, "training_acc": 38.0, "val_loss": 88.98784518241882, "val_acc": 72.0}
{"epoch": 36, "training_loss": 296.2824010848999, "training_acc": 72.0, "val_loss": 88.03983330726624, "val_acc": 28.0}
{"epoch": 37, "training_loss": 260.33356046676636, "training_acc": 42.0, "val_loss": 22.033420205116272, "val_acc": 72.0}
{"epoch": 38, "training_loss": 157.60217761993408, "training_acc": 52.0, "val_loss": 70.34458518028259, "val_acc": 72.0}
{"epoch": 39, "training_loss": 331.57047843933105, "training_acc": 72.0, "val_loss": 76.0607659816742, "val_acc": 72.0}
{"epoch": 40, "training_loss": 205.68052887916565, "training_acc": 72.0, "val_loss": 134.51159000396729, "val_acc": 28.0}
{"epoch": 41, "training_loss": 431.0261764526367, "training_acc": 38.0, "val_loss": 46.21577262878418, "val_acc": 72.0}
{"epoch": 42, "training_loss": 146.27766728401184, "training_acc": 72.0, "val_loss": 25.595125555992126, "val_acc": 28.0}
{"epoch": 43, "training_loss": 172.94549179077148, "training_acc": 42.0, "val_loss": 76.62510275840759, "val_acc": 72.0}
{"epoch": 44, "training_loss": 246.05520868301392, "training_acc": 72.0, "val_loss": 134.1741442680359, "val_acc": 28.0}
