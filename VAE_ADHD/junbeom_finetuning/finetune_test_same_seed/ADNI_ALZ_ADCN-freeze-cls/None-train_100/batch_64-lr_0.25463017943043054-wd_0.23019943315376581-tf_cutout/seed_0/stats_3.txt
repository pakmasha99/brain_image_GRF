"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 1461.3069877624512, "training_acc": 38.0, "val_loss": 585.9158039093018, "val_acc": 72.0}
{"epoch": 1, "training_loss": 1894.5675506591797, "training_acc": 72.0, "val_loss": 656.9483280181885, "val_acc": 28.0}
{"epoch": 2, "training_loss": 2281.147605895996, "training_acc": 28.0, "val_loss": 146.57284021377563, "val_acc": 72.0}
{"epoch": 3, "training_loss": 788.7228202819824, "training_acc": 72.0, "val_loss": 354.43098545074463, "val_acc": 72.0}
{"epoch": 4, "training_loss": 1362.726791381836, "training_acc": 72.0, "val_loss": 197.7968454360962, "val_acc": 72.0}
{"epoch": 5, "training_loss": 720.6278796195984, "training_acc": 44.0, "val_loss": 42.10459887981415, "val_acc": 28.0}
{"epoch": 6, "training_loss": 299.1696548461914, "training_acc": 42.0, "val_loss": 165.67262411117554, "val_acc": 72.0}
{"epoch": 7, "training_loss": 636.6919884681702, "training_acc": 72.0, "val_loss": 57.216423749923706, "val_acc": 72.0}
{"epoch": 8, "training_loss": 332.48225021362305, "training_acc": 58.0, "val_loss": 17.913147807121277, "val_acc": 28.0}
{"epoch": 9, "training_loss": 181.66378688812256, "training_acc": 44.0, "val_loss": 119.25113201141357, "val_acc": 72.0}
{"epoch": 10, "training_loss": 424.57340717315674, "training_acc": 72.0, "val_loss": 26.295197010040283, "val_acc": 28.0}
{"epoch": 11, "training_loss": 134.01473188400269, "training_acc": 34.0, "val_loss": 53.365558385849, "val_acc": 28.0}
{"epoch": 12, "training_loss": 265.5300636291504, "training_acc": 40.0, "val_loss": 86.81818842887878, "val_acc": 72.0}
{"epoch": 13, "training_loss": 290.3878927230835, "training_acc": 72.0, "val_loss": 109.12351608276367, "val_acc": 28.0}
{"epoch": 14, "training_loss": 350.08915615081787, "training_acc": 38.0, "val_loss": 25.68998634815216, "val_acc": 72.0}
{"epoch": 15, "training_loss": 125.42966890335083, "training_acc": 60.0, "val_loss": 53.76828908920288, "val_acc": 72.0}
{"epoch": 16, "training_loss": 224.83529472351074, "training_acc": 72.0, "val_loss": 22.938770055770874, "val_acc": 72.0}
{"epoch": 17, "training_loss": 225.42780876159668, "training_acc": 60.0, "val_loss": 15.218999981880188, "val_acc": 72.0}
{"epoch": 18, "training_loss": 87.59478330612183, "training_acc": 72.0, "val_loss": 25.561678409576416, "val_acc": 72.0}
{"epoch": 19, "training_loss": 214.7182788848877, "training_acc": 54.0, "val_loss": 44.74605917930603, "val_acc": 72.0}
{"epoch": 20, "training_loss": 206.80765533447266, "training_acc": 72.0, "val_loss": 30.442258715629578, "val_acc": 72.0}
{"epoch": 21, "training_loss": 256.8340358734131, "training_acc": 56.0, "val_loss": 18.46700757741928, "val_acc": 72.0}
{"epoch": 22, "training_loss": 99.59385824203491, "training_acc": 72.0, "val_loss": 14.960792660713196, "val_acc": 72.0}
{"epoch": 23, "training_loss": 106.14132356643677, "training_acc": 52.0, "val_loss": 90.32670259475708, "val_acc": 72.0}
{"epoch": 24, "training_loss": 427.2861804962158, "training_acc": 72.0, "val_loss": 111.88466548919678, "val_acc": 72.0}
{"epoch": 25, "training_loss": 348.29475688934326, "training_acc": 72.0, "val_loss": 172.78170585632324, "val_acc": 28.0}
{"epoch": 26, "training_loss": 548.8570432662964, "training_acc": 28.0, "val_loss": 128.3247947692871, "val_acc": 72.0}
{"epoch": 27, "training_loss": 616.3916683197021, "training_acc": 72.0, "val_loss": 231.24127388000488, "val_acc": 72.0}
{"epoch": 28, "training_loss": 877.8802070617676, "training_acc": 72.0, "val_loss": 119.70494985580444, "val_acc": 72.0}
{"epoch": 29, "training_loss": 334.40871810913086, "training_acc": 56.0, "val_loss": 139.96528387069702, "val_acc": 28.0}
{"epoch": 30, "training_loss": 371.60062193870544, "training_acc": 48.0, "val_loss": 57.242369651794434, "val_acc": 72.0}
{"epoch": 31, "training_loss": 200.54870700836182, "training_acc": 72.0, "val_loss": 93.62989664077759, "val_acc": 28.0}
{"epoch": 32, "training_loss": 290.8651747703552, "training_acc": 42.0, "val_loss": 32.400572299957275, "val_acc": 72.0}
{"epoch": 33, "training_loss": 191.71067237854004, "training_acc": 50.0, "val_loss": 67.93602705001831, "val_acc": 72.0}
{"epoch": 34, "training_loss": 295.6555013656616, "training_acc": 72.0, "val_loss": 66.56089425086975, "val_acc": 72.0}
{"epoch": 35, "training_loss": 234.3521203994751, "training_acc": 52.0, "val_loss": 27.608367800712585, "val_acc": 72.0}
{"epoch": 36, "training_loss": 93.17955827713013, "training_acc": 72.0, "val_loss": 56.65423274040222, "val_acc": 28.0}
{"epoch": 37, "training_loss": 214.72674369812012, "training_acc": 48.0, "val_loss": 101.12953186035156, "val_acc": 72.0}
{"epoch": 38, "training_loss": 374.118643283844, "training_acc": 72.0, "val_loss": 15.048235654830933, "val_acc": 72.0}
{"epoch": 39, "training_loss": 229.2223720550537, "training_acc": 58.0, "val_loss": 20.228129625320435, "val_acc": 72.0}
{"epoch": 40, "training_loss": 103.76329565048218, "training_acc": 72.0, "val_loss": 15.14936089515686, "val_acc": 72.0}
{"epoch": 41, "training_loss": 120.20923233032227, "training_acc": 60.0, "val_loss": 62.80561089515686, "val_acc": 72.0}
