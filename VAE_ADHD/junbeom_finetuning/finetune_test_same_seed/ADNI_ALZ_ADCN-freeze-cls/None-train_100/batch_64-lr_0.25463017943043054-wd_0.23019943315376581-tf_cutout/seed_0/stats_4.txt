"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 809.4641304016113, "training_acc": 50.0, "val_loss": 658.5399627685547, "val_acc": 72.0}
{"epoch": 1, "training_loss": 1980.3693542480469, "training_acc": 72.0, "val_loss": 751.5470027923584, "val_acc": 28.0}
{"epoch": 2, "training_loss": 2565.719493865967, "training_acc": 28.0, "val_loss": 165.83683490753174, "val_acc": 72.0}
{"epoch": 3, "training_loss": 986.8200912475586, "training_acc": 72.0, "val_loss": 358.316707611084, "val_acc": 72.0}
{"epoch": 4, "training_loss": 1332.5571594238281, "training_acc": 72.0, "val_loss": 138.08770179748535, "val_acc": 72.0}
{"epoch": 5, "training_loss": 713.8706817626953, "training_acc": 50.0, "val_loss": 69.28751468658447, "val_acc": 28.0}
{"epoch": 6, "training_loss": 378.69261741638184, "training_acc": 46.0, "val_loss": 260.8217716217041, "val_acc": 72.0}
{"epoch": 7, "training_loss": 1103.429069519043, "training_acc": 72.0, "val_loss": 240.1642084121704, "val_acc": 72.0}
{"epoch": 8, "training_loss": 819.6497173309326, "training_acc": 72.0, "val_loss": 44.781798124313354, "val_acc": 28.0}
{"epoch": 9, "training_loss": 164.48551225662231, "training_acc": 28.0, "val_loss": 114.80039358139038, "val_acc": 72.0}
{"epoch": 10, "training_loss": 501.7435054779053, "training_acc": 72.0, "val_loss": 154.45715188980103, "val_acc": 72.0}
{"epoch": 11, "training_loss": 538.3517370223999, "training_acc": 72.0, "val_loss": 30.77993392944336, "val_acc": 28.0}
{"epoch": 12, "training_loss": 108.32896327972412, "training_acc": 40.0, "val_loss": 22.87900447845459, "val_acc": 28.0}
{"epoch": 13, "training_loss": 153.0609712600708, "training_acc": 42.0, "val_loss": 55.0287663936615, "val_acc": 72.0}
{"epoch": 14, "training_loss": 243.7143406867981, "training_acc": 48.0, "val_loss": 64.20429348945618, "val_acc": 72.0}
{"epoch": 15, "training_loss": 267.2704973220825, "training_acc": 72.0, "val_loss": 39.164724946022034, "val_acc": 72.0}
{"epoch": 16, "training_loss": 260.2193202972412, "training_acc": 56.0, "val_loss": 23.07080775499344, "val_acc": 72.0}
{"epoch": 17, "training_loss": 135.75678539276123, "training_acc": 72.0, "val_loss": 27.814167737960815, "val_acc": 28.0}
{"epoch": 18, "training_loss": 113.46050119400024, "training_acc": 46.0, "val_loss": 24.96347576379776, "val_acc": 72.0}
{"epoch": 19, "training_loss": 127.18550872802734, "training_acc": 62.0, "val_loss": 49.57895576953888, "val_acc": 72.0}
{"epoch": 20, "training_loss": 205.75589227676392, "training_acc": 72.0, "val_loss": 16.88705086708069, "val_acc": 72.0}
{"epoch": 21, "training_loss": 215.8520565032959, "training_acc": 58.0, "val_loss": 27.75130271911621, "val_acc": 72.0}
{"epoch": 22, "training_loss": 148.13337421417236, "training_acc": 72.0, "val_loss": 14.690794050693512, "val_acc": 72.0}
{"epoch": 23, "training_loss": 130.34657382965088, "training_acc": 58.0, "val_loss": 70.91225385665894, "val_acc": 72.0}
{"epoch": 24, "training_loss": 311.171067237854, "training_acc": 72.0, "val_loss": 74.02011752128601, "val_acc": 72.0}
{"epoch": 25, "training_loss": 186.23423218727112, "training_acc": 72.0, "val_loss": 204.7037124633789, "val_acc": 28.0}
{"epoch": 26, "training_loss": 615.1913585662842, "training_acc": 28.0, "val_loss": 154.01148796081543, "val_acc": 72.0}
{"epoch": 27, "training_loss": 712.4294853210449, "training_acc": 72.0, "val_loss": 295.55580615997314, "val_acc": 72.0}
{"epoch": 28, "training_loss": 1159.7498207092285, "training_acc": 72.0, "val_loss": 214.9402379989624, "val_acc": 72.0}
{"epoch": 29, "training_loss": 696.0507202148438, "training_acc": 72.0, "val_loss": 90.0793969631195, "val_acc": 28.0}
{"epoch": 30, "training_loss": 319.03067111968994, "training_acc": 28.0, "val_loss": 106.15278482437134, "val_acc": 72.0}
{"epoch": 31, "training_loss": 520.6796836853027, "training_acc": 72.0, "val_loss": 162.79679536819458, "val_acc": 72.0}
{"epoch": 32, "training_loss": 562.3832054138184, "training_acc": 72.0, "val_loss": 14.727430045604706, "val_acc": 72.0}
{"epoch": 33, "training_loss": 344.826114654541, "training_acc": 58.0, "val_loss": 95.69945931434631, "val_acc": 28.0}
{"epoch": 34, "training_loss": 433.93161964416504, "training_acc": 44.0, "val_loss": 225.8950710296631, "val_acc": 72.0}
{"epoch": 35, "training_loss": 928.0772724151611, "training_acc": 72.0, "val_loss": 223.81088733673096, "val_acc": 72.0}
{"epoch": 36, "training_loss": 791.1724033355713, "training_acc": 72.0, "val_loss": 37.32917308807373, "val_acc": 72.0}
{"epoch": 37, "training_loss": 640.1035652160645, "training_acc": 48.0, "val_loss": 264.6913766860962, "val_acc": 28.0}
{"epoch": 38, "training_loss": 666.4029552936554, "training_acc": 50.0, "val_loss": 116.33576154708862, "val_acc": 72.0}
{"epoch": 39, "training_loss": 499.41100692749023, "training_acc": 72.0, "val_loss": 100.67511796951294, "val_acc": 72.0}
{"epoch": 40, "training_loss": 270.30031538009644, "training_acc": 72.0, "val_loss": 250.51710605621338, "val_acc": 28.0}
{"epoch": 41, "training_loss": 893.3972721099854, "training_acc": 28.0, "val_loss": 85.37721633911133, "val_acc": 72.0}
