"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 1025.2575912475586, "training_acc": 72.0, "val_loss": 550.598669052124, "val_acc": 72.0}
{"epoch": 1, "training_loss": 1496.0243282318115, "training_acc": 72.0, "val_loss": 1146.2251663208008, "val_acc": 28.0}
{"epoch": 2, "training_loss": 4212.013214111328, "training_acc": 28.0, "val_loss": 57.74015784263611, "val_acc": 28.0}
{"epoch": 3, "training_loss": 601.0590705871582, "training_acc": 48.0, "val_loss": 683.3568572998047, "val_acc": 72.0}
{"epoch": 4, "training_loss": 3004.0144958496094, "training_acc": 72.0, "val_loss": 873.5805511474609, "val_acc": 72.0}
{"epoch": 5, "training_loss": 3421.598129272461, "training_acc": 72.0, "val_loss": 703.7045955657959, "val_acc": 72.0}
{"epoch": 6, "training_loss": 2525.675392150879, "training_acc": 72.0, "val_loss": 308.4178686141968, "val_acc": 72.0}
{"epoch": 7, "training_loss": 896.9610977172852, "training_acc": 72.0, "val_loss": 603.9106845855713, "val_acc": 28.0}
{"epoch": 8, "training_loss": 2675.985321044922, "training_acc": 28.0, "val_loss": 487.76373863220215, "val_acc": 28.0}
{"epoch": 9, "training_loss": 1098.2481317520142, "training_acc": 56.0, "val_loss": 189.6972894668579, "val_acc": 72.0}
{"epoch": 10, "training_loss": 881.2157859802246, "training_acc": 72.0, "val_loss": 291.1865472793579, "val_acc": 72.0}
{"epoch": 11, "training_loss": 1108.9748001098633, "training_acc": 72.0, "val_loss": 154.24779653549194, "val_acc": 72.0}
{"epoch": 12, "training_loss": 431.58544516563416, "training_acc": 72.0, "val_loss": 351.91400051116943, "val_acc": 28.0}
{"epoch": 13, "training_loss": 1410.4767036437988, "training_acc": 28.0, "val_loss": 15.168540179729462, "val_acc": 72.0}
{"epoch": 14, "training_loss": 191.78831100463867, "training_acc": 72.0, "val_loss": 162.63750791549683, "val_acc": 72.0}
{"epoch": 15, "training_loss": 629.6092987060547, "training_acc": 72.0, "val_loss": 81.90690279006958, "val_acc": 72.0}
{"epoch": 16, "training_loss": 453.199254989624, "training_acc": 46.0, "val_loss": 16.195011138916016, "val_acc": 72.0}
{"epoch": 17, "training_loss": 90.17436742782593, "training_acc": 72.0, "val_loss": 15.46192616224289, "val_acc": 28.0}
{"epoch": 18, "training_loss": 81.49872016906738, "training_acc": 52.0, "val_loss": 79.38467264175415, "val_acc": 72.0}
{"epoch": 19, "training_loss": 323.59178829193115, "training_acc": 72.0, "val_loss": 68.3397889137268, "val_acc": 72.0}
{"epoch": 20, "training_loss": 215.30640029907227, "training_acc": 54.0, "val_loss": 15.547330677509308, "val_acc": 28.0}
{"epoch": 21, "training_loss": 84.25656795501709, "training_acc": 72.0, "val_loss": 14.860783517360687, "val_acc": 72.0}
{"epoch": 22, "training_loss": 97.26795864105225, "training_acc": 52.0, "val_loss": 92.57178902626038, "val_acc": 72.0}
{"epoch": 23, "training_loss": 444.0169734954834, "training_acc": 72.0, "val_loss": 108.05613994598389, "val_acc": 72.0}
{"epoch": 24, "training_loss": 316.15783166885376, "training_acc": 72.0, "val_loss": 221.15161418914795, "val_acc": 28.0}
{"epoch": 25, "training_loss": 749.456470489502, "training_acc": 28.0, "val_loss": 110.04564762115479, "val_acc": 72.0}
{"epoch": 26, "training_loss": 543.1663970947266, "training_acc": 72.0, "val_loss": 210.53719520568848, "val_acc": 72.0}
{"epoch": 27, "training_loss": 791.7234134674072, "training_acc": 72.0, "val_loss": 91.78675413131714, "val_acc": 72.0}
{"epoch": 28, "training_loss": 383.6301279067993, "training_acc": 54.0, "val_loss": 20.818322896957397, "val_acc": 28.0}
{"epoch": 29, "training_loss": 159.58362770080566, "training_acc": 46.0, "val_loss": 110.40844917297363, "val_acc": 72.0}
{"epoch": 30, "training_loss": 400.0963706970215, "training_acc": 72.0, "val_loss": 15.258856117725372, "val_acc": 48.0}
{"epoch": 31, "training_loss": 145.24401950836182, "training_acc": 62.0, "val_loss": 54.04560565948486, "val_acc": 72.0}
{"epoch": 32, "training_loss": 239.97862720489502, "training_acc": 72.0, "val_loss": 40.817540884017944, "val_acc": 72.0}
{"epoch": 33, "training_loss": 294.2628421783447, "training_acc": 52.0, "val_loss": 28.307339549064636, "val_acc": 72.0}
{"epoch": 34, "training_loss": 117.16672945022583, "training_acc": 72.0, "val_loss": 16.428185999393463, "val_acc": 28.0}
{"epoch": 35, "training_loss": 62.58920454978943, "training_acc": 72.0, "val_loss": 21.488817036151886, "val_acc": 72.0}
{"epoch": 36, "training_loss": 115.73463487625122, "training_acc": 54.0, "val_loss": 75.0711739063263, "val_acc": 72.0}
{"epoch": 37, "training_loss": 340.2664604187012, "training_acc": 72.0, "val_loss": 63.572144508361816, "val_acc": 72.0}
{"epoch": 38, "training_loss": 201.50405550003052, "training_acc": 60.0, "val_loss": 25.286021828651428, "val_acc": 72.0}
{"epoch": 39, "training_loss": 87.2306740283966, "training_acc": 72.0, "val_loss": 53.546202182769775, "val_acc": 28.0}
{"epoch": 40, "training_loss": 249.91851997375488, "training_acc": 44.0, "val_loss": 110.635507106781, "val_acc": 72.0}
{"epoch": 41, "training_loss": 409.31005859375, "training_acc": 72.0, "val_loss": 14.820098876953125, "val_acc": 72.0}
{"epoch": 42, "training_loss": 405.81690216064453, "training_acc": 48.0, "val_loss": 23.43522757291794, "val_acc": 28.0}
{"epoch": 43, "training_loss": 281.33644676208496, "training_acc": 42.0, "val_loss": 206.16800785064697, "val_acc": 72.0}
{"epoch": 44, "training_loss": 816.7898254394531, "training_acc": 72.0, "val_loss": 155.47735691070557, "val_acc": 72.0}
{"epoch": 45, "training_loss": 473.61139011383057, "training_acc": 72.0, "val_loss": 157.19972848892212, "val_acc": 28.0}
{"epoch": 46, "training_loss": 561.0793704986572, "training_acc": 28.0, "val_loss": 109.59328413009644, "val_acc": 72.0}
{"epoch": 47, "training_loss": 549.5183143615723, "training_acc": 72.0, "val_loss": 200.19946098327637, "val_acc": 72.0}
{"epoch": 48, "training_loss": 746.0211429595947, "training_acc": 72.0, "val_loss": 68.33300590515137, "val_acc": 72.0}
{"epoch": 49, "training_loss": 354.4270534515381, "training_acc": 58.0, "val_loss": 65.28204679489136, "val_acc": 28.0}
{"epoch": 50, "training_loss": 273.2893829345703, "training_acc": 50.0, "val_loss": 201.51138305664062, "val_acc": 72.0}
{"epoch": 51, "training_loss": 839.9735641479492, "training_acc": 72.0, "val_loss": 182.62723684310913, "val_acc": 72.0}
{"epoch": 52, "training_loss": 614.6151580810547, "training_acc": 72.0, "val_loss": 82.74833559989929, "val_acc": 28.0}
{"epoch": 53, "training_loss": 246.74295043945312, "training_acc": 28.0, "val_loss": 105.41518926620483, "val_acc": 72.0}
{"epoch": 54, "training_loss": 506.578218460083, "training_acc": 72.0, "val_loss": 133.57348442077637, "val_acc": 72.0}
{"epoch": 55, "training_loss": 439.47061347961426, "training_acc": 72.0, "val_loss": 129.06980514526367, "val_acc": 28.0}
{"epoch": 56, "training_loss": 371.77323842048645, "training_acc": 28.0, "val_loss": 105.74802160263062, "val_acc": 72.0}
{"epoch": 57, "training_loss": 518.6800727844238, "training_acc": 72.0, "val_loss": 132.50484466552734, "val_acc": 72.0}
{"epoch": 58, "training_loss": 413.4214754104614, "training_acc": 72.0, "val_loss": 143.88293027877808, "val_acc": 28.0}
{"epoch": 59, "training_loss": 452.446400642395, "training_acc": 28.0, "val_loss": 127.23358869552612, "val_acc": 72.0}
{"epoch": 60, "training_loss": 614.4769897460938, "training_acc": 72.0, "val_loss": 210.82088947296143, "val_acc": 72.0}
