"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 6117.632019042969, "training_acc": 72.0, "val_loss": 3111.3229751586914, "val_acc": 72.0}
{"epoch": 1, "training_loss": 8996.808959960938, "training_acc": 72.0, "val_loss": 2994.1179275512695, "val_acc": 28.0}
{"epoch": 2, "training_loss": 11153.3037109375, "training_acc": 28.0, "val_loss": 491.1994934082031, "val_acc": 72.0}
{"epoch": 3, "training_loss": 2800.241989135742, "training_acc": 72.0, "val_loss": 1511.3435745239258, "val_acc": 72.0}
{"epoch": 4, "training_loss": 5823.893264770508, "training_acc": 72.0, "val_loss": 803.8065910339355, "val_acc": 72.0}
{"epoch": 5, "training_loss": 1822.0438165664673, "training_acc": 72.0, "val_loss": 2797.614860534668, "val_acc": 28.0}
{"epoch": 6, "training_loss": 11366.232543945312, "training_acc": 28.0, "val_loss": 989.3429756164551, "val_acc": 28.0}
{"epoch": 7, "training_loss": 4122.013381958008, "training_acc": 42.0, "val_loss": 1892.4577713012695, "val_acc": 72.0}
{"epoch": 8, "training_loss": 8787.739074707031, "training_acc": 72.0, "val_loss": 2648.165702819824, "val_acc": 72.0}
{"epoch": 9, "training_loss": 10404.221099853516, "training_acc": 72.0, "val_loss": 2104.484748840332, "val_acc": 72.0}
{"epoch": 10, "training_loss": 7565.865280151367, "training_acc": 72.0, "val_loss": 583.2132816314697, "val_acc": 72.0}
{"epoch": 11, "training_loss": 3642.107147216797, "training_acc": 52.0, "val_loss": 1746.1524963378906, "val_acc": 28.0}
{"epoch": 12, "training_loss": 5220.302558898926, "training_acc": 28.0, "val_loss": 865.8275604248047, "val_acc": 72.0}
{"epoch": 13, "training_loss": 4571.148345947266, "training_acc": 72.0, "val_loss": 1931.1473846435547, "val_acc": 72.0}
{"epoch": 14, "training_loss": 7918.738433837891, "training_acc": 72.0, "val_loss": 1839.5172119140625, "val_acc": 72.0}
{"epoch": 15, "training_loss": 6792.653747558594, "training_acc": 72.0, "val_loss": 763.0198955535889, "val_acc": 72.0}
{"epoch": 16, "training_loss": 2531.604724884033, "training_acc": 52.0, "val_loss": 379.1867971420288, "val_acc": 28.0}
{"epoch": 17, "training_loss": 1037.3074855804443, "training_acc": 56.0, "val_loss": 679.8237323760986, "val_acc": 72.0}
{"epoch": 18, "training_loss": 2818.8432540893555, "training_acc": 72.0, "val_loss": 559.6774578094482, "val_acc": 72.0}
{"epoch": 19, "training_loss": 1735.0105333328247, "training_acc": 72.0, "val_loss": 1247.5032806396484, "val_acc": 28.0}
{"epoch": 20, "training_loss": 4623.825149536133, "training_acc": 28.0, "val_loss": 315.92180728912354, "val_acc": 72.0}
{"epoch": 21, "training_loss": 1624.6620864868164, "training_acc": 72.0, "val_loss": 825.2002716064453, "val_acc": 72.0}
{"epoch": 22, "training_loss": 3153.3178939819336, "training_acc": 72.0, "val_loss": 377.4996757507324, "val_acc": 72.0}
{"epoch": 23, "training_loss": 1583.8503341674805, "training_acc": 56.0, "val_loss": 51.323920488357544, "val_acc": 28.0}
{"epoch": 24, "training_loss": 555.9355278015137, "training_acc": 54.0, "val_loss": 1065.305519104004, "val_acc": 72.0}
{"epoch": 25, "training_loss": 4505.561996459961, "training_acc": 72.0, "val_loss": 1138.032054901123, "val_acc": 72.0}
{"epoch": 26, "training_loss": 4148.8248291015625, "training_acc": 72.0, "val_loss": 254.97374534606934, "val_acc": 72.0}
{"epoch": 27, "training_loss": 3203.46435546875, "training_acc": 46.0, "val_loss": 1236.4802360534668, "val_acc": 28.0}
{"epoch": 28, "training_loss": 3300.685214996338, "training_acc": 46.0, "val_loss": 535.322904586792, "val_acc": 72.0}
{"epoch": 29, "training_loss": 2281.995292663574, "training_acc": 72.0, "val_loss": 471.3583469390869, "val_acc": 72.0}
{"epoch": 30, "training_loss": 1418.404417514801, "training_acc": 72.0, "val_loss": 1310.2462768554688, "val_acc": 28.0}
{"epoch": 31, "training_loss": 4748.993927001953, "training_acc": 28.0, "val_loss": 327.7165412902832, "val_acc": 72.0}
{"epoch": 32, "training_loss": 1951.008804321289, "training_acc": 72.0, "val_loss": 831.3010215759277, "val_acc": 72.0}
{"epoch": 33, "training_loss": 3114.5321350097656, "training_acc": 72.0, "val_loss": 298.97027015686035, "val_acc": 72.0}
{"epoch": 34, "training_loss": 2013.9556121826172, "training_acc": 54.0, "val_loss": 535.8560085296631, "val_acc": 28.0}
{"epoch": 35, "training_loss": 2075.301055908203, "training_acc": 44.0, "val_loss": 889.1806602478027, "val_acc": 72.0}
{"epoch": 36, "training_loss": 3701.4529876708984, "training_acc": 72.0, "val_loss": 872.3660469055176, "val_acc": 72.0}
{"epoch": 37, "training_loss": 3026.595314025879, "training_acc": 72.0, "val_loss": 101.17619037628174, "val_acc": 28.0}
{"epoch": 38, "training_loss": 336.4150400161743, "training_acc": 46.0, "val_loss": 102.75025367736816, "val_acc": 28.0}
{"epoch": 39, "training_loss": 1225.6080322265625, "training_acc": 38.0, "val_loss": 692.8372859954834, "val_acc": 72.0}
{"epoch": 40, "training_loss": 2669.7272720336914, "training_acc": 72.0, "val_loss": 312.0178699493408, "val_acc": 72.0}
{"epoch": 41, "training_loss": 1773.4646301269531, "training_acc": 52.0, "val_loss": 74.38064217567444, "val_acc": 28.0}
{"epoch": 42, "training_loss": 870.688720703125, "training_acc": 50.0, "val_loss": 1182.125473022461, "val_acc": 72.0}
