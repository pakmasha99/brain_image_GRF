"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 1795.1128387451172, "training_acc": 46.0, "val_loss": 1146.653175354004, "val_acc": 72.0}
{"epoch": 1, "training_loss": 3668.711845397949, "training_acc": 72.0, "val_loss": 1194.4242477416992, "val_acc": 28.0}
{"epoch": 2, "training_loss": 4137.484878540039, "training_acc": 28.0, "val_loss": 275.0691890716553, "val_acc": 72.0}
{"epoch": 3, "training_loss": 1617.453857421875, "training_acc": 72.0, "val_loss": 712.8963947296143, "val_acc": 72.0}
{"epoch": 4, "training_loss": 2773.913345336914, "training_acc": 72.0, "val_loss": 463.09804916381836, "val_acc": 72.0}
{"epoch": 5, "training_loss": 1366.7228755950928, "training_acc": 72.0, "val_loss": 679.5788288116455, "val_acc": 28.0}
{"epoch": 6, "training_loss": 2791.301887512207, "training_acc": 28.0, "val_loss": 44.540923833847046, "val_acc": 28.0}
{"epoch": 7, "training_loss": 890.6195526123047, "training_acc": 40.0, "val_loss": 742.8083419799805, "val_acc": 72.0}
{"epoch": 8, "training_loss": 3229.7198333740234, "training_acc": 72.0, "val_loss": 943.8470840454102, "val_acc": 72.0}
{"epoch": 9, "training_loss": 3675.0795288085938, "training_acc": 72.0, "val_loss": 713.7260437011719, "val_acc": 72.0}
{"epoch": 10, "training_loss": 2550.722526550293, "training_acc": 72.0, "val_loss": 128.7355899810791, "val_acc": 72.0}
{"epoch": 11, "training_loss": 1143.5615921020508, "training_acc": 60.0, "val_loss": 874.5151519775391, "val_acc": 28.0}
{"epoch": 12, "training_loss": 2906.8870849609375, "training_acc": 28.0, "val_loss": 185.95507144927979, "val_acc": 72.0}
{"epoch": 13, "training_loss": 968.1486320495605, "training_acc": 72.0, "val_loss": 550.4730701446533, "val_acc": 72.0}
{"epoch": 14, "training_loss": 2251.12735748291, "training_acc": 72.0, "val_loss": 525.4238128662109, "val_acc": 72.0}
{"epoch": 15, "training_loss": 1907.6416282653809, "training_acc": 72.0, "val_loss": 156.1020016670227, "val_acc": 72.0}
{"epoch": 16, "training_loss": 835.2568244934082, "training_acc": 58.0, "val_loss": 390.06969928741455, "val_acc": 28.0}
{"epoch": 17, "training_loss": 1071.8873777389526, "training_acc": 44.0, "val_loss": 148.8153338432312, "val_acc": 72.0}
{"epoch": 18, "training_loss": 618.5956611633301, "training_acc": 72.0, "val_loss": 57.90404677391052, "val_acc": 72.0}
{"epoch": 19, "training_loss": 701.6985816955566, "training_acc": 52.0, "val_loss": 154.47176694869995, "val_acc": 28.0}
{"epoch": 20, "training_loss": 667.3636493682861, "training_acc": 46.0, "val_loss": 392.9105758666992, "val_acc": 72.0}
{"epoch": 21, "training_loss": 1626.7692604064941, "training_acc": 72.0, "val_loss": 438.5557174682617, "val_acc": 72.0}
{"epoch": 22, "training_loss": 1631.8275108337402, "training_acc": 72.0, "val_loss": 165.0803804397583, "val_acc": 72.0}
{"epoch": 23, "training_loss": 801.6450958251953, "training_acc": 52.0, "val_loss": 152.26370096206665, "val_acc": 28.0}
{"epoch": 24, "training_loss": 528.6749229431152, "training_acc": 50.0, "val_loss": 307.1810007095337, "val_acc": 72.0}
{"epoch": 25, "training_loss": 1256.111499786377, "training_acc": 72.0, "val_loss": 298.4635829925537, "val_acc": 72.0}
{"epoch": 26, "training_loss": 1003.7861213684082, "training_acc": 72.0, "val_loss": 40.10057747364044, "val_acc": 28.0}
{"epoch": 27, "training_loss": 135.71478390693665, "training_acc": 28.0, "val_loss": 123.00475835800171, "val_acc": 72.0}
{"epoch": 28, "training_loss": 570.9891662597656, "training_acc": 72.0, "val_loss": 90.31413793563843, "val_acc": 72.0}
{"epoch": 29, "training_loss": 513.7783737182617, "training_acc": 54.0, "val_loss": 21.064871549606323, "val_acc": 72.0}
{"epoch": 30, "training_loss": 88.66532588005066, "training_acc": 72.0, "val_loss": 101.72629356384277, "val_acc": 28.0}
{"epoch": 31, "training_loss": 407.9113597869873, "training_acc": 46.0, "val_loss": 182.74189233779907, "val_acc": 72.0}
{"epoch": 32, "training_loss": 686.4999771118164, "training_acc": 72.0, "val_loss": 39.006397128105164, "val_acc": 72.0}
{"epoch": 33, "training_loss": 639.9280700683594, "training_acc": 56.0, "val_loss": 217.86072254180908, "val_acc": 28.0}
{"epoch": 34, "training_loss": 740.9466190338135, "training_acc": 48.0, "val_loss": 348.9748954772949, "val_acc": 72.0}
{"epoch": 35, "training_loss": 1472.507381439209, "training_acc": 72.0, "val_loss": 385.2323532104492, "val_acc": 72.0}
{"epoch": 36, "training_loss": 1362.913745880127, "training_acc": 72.0, "val_loss": 90.79010486602783, "val_acc": 72.0}
{"epoch": 37, "training_loss": 830.7559967041016, "training_acc": 58.0, "val_loss": 478.0851364135742, "val_acc": 28.0}
{"epoch": 38, "training_loss": 1290.9637880325317, "training_acc": 42.0, "val_loss": 129.27584648132324, "val_acc": 72.0}
{"epoch": 39, "training_loss": 503.5452013015747, "training_acc": 72.0, "val_loss": 50.931018590927124, "val_acc": 72.0}
{"epoch": 40, "training_loss": 500.22948455810547, "training_acc": 54.0, "val_loss": 13.690024614334106, "val_acc": 72.0}
{"epoch": 41, "training_loss": 163.3615198135376, "training_acc": 72.0, "val_loss": 76.73926949501038, "val_acc": 72.0}
{"epoch": 42, "training_loss": 195.7678987979889, "training_acc": 62.0, "val_loss": 31.92603886127472, "val_acc": 72.0}
{"epoch": 43, "training_loss": 196.0063600540161, "training_acc": 52.0, "val_loss": 154.4499635696411, "val_acc": 72.0}
{"epoch": 44, "training_loss": 693.0847587585449, "training_acc": 72.0, "val_loss": 209.91814136505127, "val_acc": 72.0}
{"epoch": 45, "training_loss": 691.6681461334229, "training_acc": 72.0, "val_loss": 157.84509181976318, "val_acc": 28.0}
{"epoch": 46, "training_loss": 426.4672658443451, "training_acc": 44.0, "val_loss": 97.62969613075256, "val_acc": 72.0}
{"epoch": 47, "training_loss": 380.26838970184326, "training_acc": 72.0, "val_loss": 13.506188988685608, "val_acc": 80.0}
{"epoch": 48, "training_loss": 234.2708625793457, "training_acc": 58.0, "val_loss": 107.14006423950195, "val_acc": 72.0}
{"epoch": 49, "training_loss": 511.84274101257324, "training_acc": 72.0, "val_loss": 130.19890785217285, "val_acc": 72.0}
{"epoch": 50, "training_loss": 313.42435359954834, "training_acc": 72.0, "val_loss": 466.44129753112793, "val_acc": 28.0}
{"epoch": 51, "training_loss": 1707.1665229797363, "training_acc": 28.0, "val_loss": 110.47019958496094, "val_acc": 72.0}
{"epoch": 52, "training_loss": 627.9406623840332, "training_acc": 72.0, "val_loss": 271.22459411621094, "val_acc": 72.0}
{"epoch": 53, "training_loss": 999.4805297851562, "training_acc": 72.0, "val_loss": 67.24259853363037, "val_acc": 72.0}
{"epoch": 54, "training_loss": 947.4815521240234, "training_acc": 50.0, "val_loss": 376.9878149032593, "val_acc": 28.0}
{"epoch": 55, "training_loss": 956.0775804519653, "training_acc": 52.0, "val_loss": 242.6560640335083, "val_acc": 72.0}
{"epoch": 56, "training_loss": 1061.4072570800781, "training_acc": 72.0, "val_loss": 256.69360160827637, "val_acc": 72.0}
{"epoch": 57, "training_loss": 853.5665340423584, "training_acc": 72.0, "val_loss": 187.7239465713501, "val_acc": 28.0}
{"epoch": 58, "training_loss": 612.3965721130371, "training_acc": 28.0, "val_loss": 238.99269104003906, "val_acc": 72.0}
{"epoch": 59, "training_loss": 1187.7669143676758, "training_acc": 72.0, "val_loss": 461.80481910705566, "val_acc": 72.0}
{"epoch": 60, "training_loss": 1796.8087463378906, "training_acc": 72.0, "val_loss": 307.94310569763184, "val_acc": 72.0}
{"epoch": 61, "training_loss": 894.94114112854, "training_acc": 72.0, "val_loss": 376.263165473938, "val_acc": 28.0}
{"epoch": 62, "training_loss": 1583.1175689697266, "training_acc": 28.0, "val_loss": 38.61030638217926, "val_acc": 72.0}
{"epoch": 63, "training_loss": 316.3070316314697, "training_acc": 72.0, "val_loss": 140.39994478225708, "val_acc": 72.0}
{"epoch": 64, "training_loss": 431.3587656021118, "training_acc": 72.0, "val_loss": 295.62199115753174, "val_acc": 28.0}
{"epoch": 65, "training_loss": 900.663872718811, "training_acc": 28.0, "val_loss": 246.7869758605957, "val_acc": 72.0}
{"epoch": 66, "training_loss": 1288.733772277832, "training_acc": 72.0, "val_loss": 479.1933536529541, "val_acc": 72.0}
