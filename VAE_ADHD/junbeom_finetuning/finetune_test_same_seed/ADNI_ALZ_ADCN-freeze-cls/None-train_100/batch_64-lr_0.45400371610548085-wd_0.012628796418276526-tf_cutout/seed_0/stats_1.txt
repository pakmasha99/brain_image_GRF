"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 1598.3015174865723, "training_acc": 72.0, "val_loss": 1016.7306900024414, "val_acc": 72.0}
{"epoch": 1, "training_loss": 2714.9230766296387, "training_acc": 72.0, "val_loss": 2006.4361572265625, "val_acc": 28.0}
{"epoch": 2, "training_loss": 7552.680969238281, "training_acc": 28.0, "val_loss": 285.1588487625122, "val_acc": 28.0}
{"epoch": 3, "training_loss": 1441.7706604003906, "training_acc": 48.0, "val_loss": 1198.7353324890137, "val_acc": 72.0}
{"epoch": 4, "training_loss": 4960.418556213379, "training_acc": 72.0, "val_loss": 1685.682487487793, "val_acc": 72.0}
{"epoch": 5, "training_loss": 6807.654846191406, "training_acc": 72.0, "val_loss": 1644.413185119629, "val_acc": 72.0}
{"epoch": 6, "training_loss": 6334.501953125, "training_acc": 72.0, "val_loss": 1141.7607307434082, "val_acc": 72.0}
{"epoch": 7, "training_loss": 4029.4338455200195, "training_acc": 72.0, "val_loss": 310.90755462646484, "val_acc": 72.0}
{"epoch": 8, "training_loss": 1877.2160415649414, "training_acc": 50.0, "val_loss": 915.4909133911133, "val_acc": 28.0}
{"epoch": 9, "training_loss": 3106.8746185302734, "training_acc": 28.0, "val_loss": 160.08319854736328, "val_acc": 72.0}
{"epoch": 10, "training_loss": 982.6819343566895, "training_acc": 72.0, "val_loss": 520.454216003418, "val_acc": 72.0}
{"epoch": 11, "training_loss": 2073.077808380127, "training_acc": 72.0, "val_loss": 448.3612060546875, "val_acc": 72.0}
{"epoch": 12, "training_loss": 1581.506706237793, "training_acc": 72.0, "val_loss": 70.56195139884949, "val_acc": 72.0}
{"epoch": 13, "training_loss": 1165.6332702636719, "training_acc": 52.0, "val_loss": 651.7811298370361, "val_acc": 28.0}
{"epoch": 14, "training_loss": 1750.2362365722656, "training_acc": 28.0, "val_loss": 365.0378227233887, "val_acc": 72.0}
{"epoch": 15, "training_loss": 1836.4679107666016, "training_acc": 72.0, "val_loss": 843.9518928527832, "val_acc": 72.0}
{"epoch": 16, "training_loss": 3492.8165893554688, "training_acc": 72.0, "val_loss": 910.2560997009277, "val_acc": 72.0}
{"epoch": 17, "training_loss": 3515.2614135742188, "training_acc": 72.0, "val_loss": 628.8692951202393, "val_acc": 72.0}
{"epoch": 18, "training_loss": 2115.6788330078125, "training_acc": 72.0, "val_loss": 79.87524271011353, "val_acc": 72.0}
{"epoch": 19, "training_loss": 1226.365379333496, "training_acc": 58.0, "val_loss": 1015.0422096252441, "val_acc": 28.0}
{"epoch": 20, "training_loss": 3584.7670974731445, "training_acc": 28.0, "val_loss": 63.5952889919281, "val_acc": 72.0}
{"epoch": 21, "training_loss": 516.4404792785645, "training_acc": 72.0, "val_loss": 374.2358207702637, "val_acc": 72.0}
{"epoch": 22, "training_loss": 1512.1272087097168, "training_acc": 72.0, "val_loss": 307.3638916015625, "val_acc": 72.0}
{"epoch": 23, "training_loss": 1022.8524761199951, "training_acc": 72.0, "val_loss": 196.7643141746521, "val_acc": 28.0}
{"epoch": 24, "training_loss": 713.3517227172852, "training_acc": 28.0, "val_loss": 184.45123434066772, "val_acc": 72.0}
{"epoch": 25, "training_loss": 903.249439239502, "training_acc": 72.0, "val_loss": 352.1227836608887, "val_acc": 72.0}
{"epoch": 26, "training_loss": 1341.260269165039, "training_acc": 72.0, "val_loss": 171.31277322769165, "val_acc": 72.0}
{"epoch": 27, "training_loss": 655.795295715332, "training_acc": 54.0, "val_loss": 43.840742111206055, "val_acc": 28.0}
{"epoch": 28, "training_loss": 328.6164722442627, "training_acc": 48.0, "val_loss": 315.03257751464844, "val_acc": 72.0}
{"epoch": 29, "training_loss": 1270.3716583251953, "training_acc": 72.0, "val_loss": 273.1478214263916, "val_acc": 72.0}
{"epoch": 30, "training_loss": 916.240385055542, "training_acc": 72.0, "val_loss": 166.73147678375244, "val_acc": 28.0}
{"epoch": 31, "training_loss": 524.6801767349243, "training_acc": 28.0, "val_loss": 224.6718168258667, "val_acc": 72.0}
{"epoch": 32, "training_loss": 1131.4119873046875, "training_acc": 72.0, "val_loss": 411.33599281311035, "val_acc": 72.0}
{"epoch": 33, "training_loss": 1575.1783142089844, "training_acc": 72.0, "val_loss": 230.0398588180542, "val_acc": 72.0}
{"epoch": 34, "training_loss": 619.8928453922272, "training_acc": 72.0, "val_loss": 610.854959487915, "val_acc": 28.0}
{"epoch": 35, "training_loss": 2465.849494934082, "training_acc": 28.0, "val_loss": 77.04364061355591, "val_acc": 28.0}
{"epoch": 36, "training_loss": 921.9014511108398, "training_acc": 38.0, "val_loss": 645.1839447021484, "val_acc": 72.0}
{"epoch": 37, "training_loss": 2695.3357696533203, "training_acc": 72.0, "val_loss": 845.2923774719238, "val_acc": 72.0}
{"epoch": 38, "training_loss": 3351.6451416015625, "training_acc": 72.0, "val_loss": 713.70849609375, "val_acc": 72.0}
{"epoch": 39, "training_loss": 2680.5220527648926, "training_acc": 72.0, "val_loss": 271.61922454833984, "val_acc": 72.0}
{"epoch": 40, "training_loss": 628.0392370223999, "training_acc": 62.0, "val_loss": 292.3774003982544, "val_acc": 28.0}
{"epoch": 41, "training_loss": 751.5346043109894, "training_acc": 46.0, "val_loss": 100.08361339569092, "val_acc": 72.0}
{"epoch": 42, "training_loss": 394.0183582305908, "training_acc": 72.0, "val_loss": 13.842083513736725, "val_acc": 72.0}
{"epoch": 43, "training_loss": 326.0721321105957, "training_acc": 52.0, "val_loss": 96.4793860912323, "val_acc": 72.0}
{"epoch": 44, "training_loss": 498.5873966217041, "training_acc": 72.0, "val_loss": 136.5602970123291, "val_acc": 72.0}
{"epoch": 45, "training_loss": 427.1682242155075, "training_acc": 72.0, "val_loss": 248.77748489379883, "val_acc": 28.0}
{"epoch": 46, "training_loss": 704.6267285346985, "training_acc": 42.0, "val_loss": 62.0919406414032, "val_acc": 72.0}
{"epoch": 47, "training_loss": 200.30287909507751, "training_acc": 54.0, "val_loss": 58.304768800735474, "val_acc": 72.0}
{"epoch": 48, "training_loss": 190.4347207546234, "training_acc": 72.0, "val_loss": 194.1329002380371, "val_acc": 28.0}
{"epoch": 49, "training_loss": 632.3874826431274, "training_acc": 40.0, "val_loss": 90.9449577331543, "val_acc": 72.0}
{"epoch": 50, "training_loss": 276.277307510376, "training_acc": 72.0, "val_loss": 271.0488557815552, "val_acc": 28.0}
{"epoch": 51, "training_loss": 730.0528345108032, "training_acc": 42.0, "val_loss": 86.37765049934387, "val_acc": 72.0}
{"epoch": 52, "training_loss": 343.4725914001465, "training_acc": 72.0, "val_loss": 72.0842182636261, "val_acc": 28.0}
{"epoch": 53, "training_loss": 357.8269920349121, "training_acc": 40.0, "val_loss": 104.79623079299927, "val_acc": 72.0}
{"epoch": 54, "training_loss": 303.5503373146057, "training_acc": 72.0, "val_loss": 326.5547037124634, "val_acc": 28.0}
{"epoch": 55, "training_loss": 1002.0665245056152, "training_acc": 28.0, "val_loss": 247.17516899108887, "val_acc": 72.0}
{"epoch": 56, "training_loss": 1243.5608978271484, "training_acc": 72.0, "val_loss": 497.32351303100586, "val_acc": 72.0}
{"epoch": 57, "training_loss": 1960.1604385375977, "training_acc": 72.0, "val_loss": 368.82970333099365, "val_acc": 72.0}
{"epoch": 58, "training_loss": 1196.2270431518555, "training_acc": 72.0, "val_loss": 162.8814935684204, "val_acc": 28.0}
{"epoch": 59, "training_loss": 649.447732925415, "training_acc": 28.0, "val_loss": 149.7749924659729, "val_acc": 72.0}
{"epoch": 60, "training_loss": 798.5997772216797, "training_acc": 72.0, "val_loss": 265.0034189224243, "val_acc": 72.0}
{"epoch": 61, "training_loss": 936.6582908630371, "training_acc": 72.0, "val_loss": 15.865297615528107, "val_acc": 72.0}
