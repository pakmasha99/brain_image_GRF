"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 2183.0245971679688, "training_acc": 42.0, "val_loss": 1163.0297660827637, "val_acc": 72.0}
{"epoch": 1, "training_loss": 3634.2885208129883, "training_acc": 72.0, "val_loss": 727.3983478546143, "val_acc": 28.0}
{"epoch": 2, "training_loss": 2501.4488525390625, "training_acc": 28.0, "val_loss": 356.81731700897217, "val_acc": 72.0}
{"epoch": 3, "training_loss": 1854.2059173583984, "training_acc": 72.0, "val_loss": 762.879753112793, "val_acc": 72.0}
{"epoch": 4, "training_loss": 2978.432846069336, "training_acc": 72.0, "val_loss": 524.3848323822021, "val_acc": 72.0}
{"epoch": 5, "training_loss": 1811.992057800293, "training_acc": 72.0, "val_loss": 465.74859619140625, "val_acc": 28.0}
{"epoch": 6, "training_loss": 1835.6041107177734, "training_acc": 28.0, "val_loss": 110.78475713729858, "val_acc": 72.0}
{"epoch": 7, "training_loss": 569.0462436676025, "training_acc": 72.0, "val_loss": 276.72271728515625, "val_acc": 72.0}
{"epoch": 8, "training_loss": 1002.4337730407715, "training_acc": 72.0, "val_loss": 37.08521127700806, "val_acc": 72.0}
{"epoch": 9, "training_loss": 1098.6048431396484, "training_acc": 50.0, "val_loss": 481.11557960510254, "val_acc": 28.0}
{"epoch": 10, "training_loss": 1363.4386415481567, "training_acc": 44.0, "val_loss": 253.0383586883545, "val_acc": 72.0}
{"epoch": 11, "training_loss": 1082.825397491455, "training_acc": 72.0, "val_loss": 256.11140727996826, "val_acc": 72.0}
{"epoch": 12, "training_loss": 841.1191139221191, "training_acc": 72.0, "val_loss": 246.5689182281494, "val_acc": 28.0}
{"epoch": 13, "training_loss": 859.0007572174072, "training_acc": 28.0, "val_loss": 217.4271583557129, "val_acc": 72.0}
{"epoch": 14, "training_loss": 1172.8041229248047, "training_acc": 72.0, "val_loss": 450.20527839660645, "val_acc": 72.0}
{"epoch": 15, "training_loss": 1748.8854141235352, "training_acc": 72.0, "val_loss": 289.22622203826904, "val_acc": 72.0}
{"epoch": 16, "training_loss": 815.3399333953857, "training_acc": 72.0, "val_loss": 496.00372314453125, "val_acc": 28.0}
{"epoch": 17, "training_loss": 2149.369300842285, "training_acc": 28.0, "val_loss": 58.13835859298706, "val_acc": 28.0}
{"epoch": 18, "training_loss": 855.8459243774414, "training_acc": 38.0, "val_loss": 625.8560180664062, "val_acc": 72.0}
{"epoch": 19, "training_loss": 2628.403968811035, "training_acc": 72.0, "val_loss": 809.3844413757324, "val_acc": 72.0}
{"epoch": 20, "training_loss": 3183.6363830566406, "training_acc": 72.0, "val_loss": 657.7647686004639, "val_acc": 72.0}
{"epoch": 21, "training_loss": 2307.0892639160156, "training_acc": 72.0, "val_loss": 217.70155429840088, "val_acc": 72.0}
{"epoch": 22, "training_loss": 963.2928123474121, "training_acc": 56.0, "val_loss": 454.09488677978516, "val_acc": 28.0}
{"epoch": 23, "training_loss": 1240.8626070022583, "training_acc": 28.0, "val_loss": 319.09947395324707, "val_acc": 72.0}
{"epoch": 24, "training_loss": 1600.3318710327148, "training_acc": 72.0, "val_loss": 670.4555511474609, "val_acc": 72.0}
{"epoch": 25, "training_loss": 2727.4433364868164, "training_acc": 72.0, "val_loss": 636.6836547851562, "val_acc": 72.0}
{"epoch": 26, "training_loss": 2353.8484268188477, "training_acc": 72.0, "val_loss": 278.83214950561523, "val_acc": 72.0}
{"epoch": 27, "training_loss": 735.5725173950195, "training_acc": 72.0, "val_loss": 570.930814743042, "val_acc": 28.0}
{"epoch": 28, "training_loss": 2236.5292053222656, "training_acc": 28.0, "val_loss": 27.647647261619568, "val_acc": 72.0}
{"epoch": 29, "training_loss": 299.8110523223877, "training_acc": 72.0, "val_loss": 198.15032482147217, "val_acc": 72.0}
{"epoch": 30, "training_loss": 723.5574922561646, "training_acc": 72.0, "val_loss": 15.380395948886871, "val_acc": 72.0}
{"epoch": 31, "training_loss": 399.4140396118164, "training_acc": 64.0, "val_loss": 110.80397367477417, "val_acc": 28.0}
{"epoch": 32, "training_loss": 534.5019626617432, "training_acc": 48.0, "val_loss": 404.6030044555664, "val_acc": 72.0}
{"epoch": 33, "training_loss": 1688.0128211975098, "training_acc": 72.0, "val_loss": 456.4541816711426, "val_acc": 72.0}
{"epoch": 34, "training_loss": 1672.1826400756836, "training_acc": 72.0, "val_loss": 186.27381324768066, "val_acc": 72.0}
{"epoch": 35, "training_loss": 713.4564533233643, "training_acc": 56.0, "val_loss": 163.49247694015503, "val_acc": 28.0}
{"epoch": 36, "training_loss": 602.4035530090332, "training_acc": 46.0, "val_loss": 264.6580457687378, "val_acc": 72.0}
{"epoch": 37, "training_loss": 1129.9295806884766, "training_acc": 72.0, "val_loss": 214.85447883605957, "val_acc": 72.0}
{"epoch": 38, "training_loss": 605.317379951477, "training_acc": 72.0, "val_loss": 468.0419921875, "val_acc": 28.0}
{"epoch": 39, "training_loss": 1788.969165802002, "training_acc": 28.0, "val_loss": 56.45785331726074, "val_acc": 72.0}
{"epoch": 40, "training_loss": 418.51156997680664, "training_acc": 72.0, "val_loss": 170.28937339782715, "val_acc": 72.0}
{"epoch": 41, "training_loss": 549.8648958206177, "training_acc": 72.0, "val_loss": 198.32444190979004, "val_acc": 28.0}
{"epoch": 42, "training_loss": 550.6659569740295, "training_acc": 28.0, "val_loss": 228.73823642730713, "val_acc": 72.0}
{"epoch": 43, "training_loss": 1050.7800827026367, "training_acc": 72.0, "val_loss": 390.76151847839355, "val_acc": 72.0}
{"epoch": 44, "training_loss": 1488.488639831543, "training_acc": 72.0, "val_loss": 209.80534553527832, "val_acc": 72.0}
{"epoch": 45, "training_loss": 529.6374661922455, "training_acc": 72.0, "val_loss": 532.0957660675049, "val_acc": 28.0}
{"epoch": 46, "training_loss": 2062.7327194213867, "training_acc": 28.0, "val_loss": 63.18805813789368, "val_acc": 72.0}
{"epoch": 47, "training_loss": 508.88224029541016, "training_acc": 72.0, "val_loss": 247.92230129241943, "val_acc": 72.0}
{"epoch": 48, "training_loss": 912.8220348358154, "training_acc": 72.0, "val_loss": 53.06966304779053, "val_acc": 72.0}
{"epoch": 49, "training_loss": 735.4662170410156, "training_acc": 54.0, "val_loss": 270.5092668533325, "val_acc": 28.0}
