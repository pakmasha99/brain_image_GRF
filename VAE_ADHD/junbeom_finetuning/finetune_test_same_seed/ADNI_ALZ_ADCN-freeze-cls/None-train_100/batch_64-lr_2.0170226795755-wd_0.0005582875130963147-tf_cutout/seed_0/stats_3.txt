"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 6950.685104370117, "training_acc": 48.0, "val_loss": 5501.267623901367, "val_acc": 72.0}
{"epoch": 1, "training_loss": 17521.342346191406, "training_acc": 72.0, "val_loss": 3436.2171173095703, "val_acc": 28.0}
{"epoch": 2, "training_loss": 10924.214859008789, "training_acc": 28.0, "val_loss": 1931.494140625, "val_acc": 72.0}
{"epoch": 3, "training_loss": 9456.659088134766, "training_acc": 72.0, "val_loss": 3808.5189819335938, "val_acc": 72.0}
{"epoch": 4, "training_loss": 15179.281127929688, "training_acc": 72.0, "val_loss": 2809.4980239868164, "val_acc": 72.0}
{"epoch": 5, "training_loss": 8820.571044921875, "training_acc": 72.0, "val_loss": 1400.286865234375, "val_acc": 28.0}
{"epoch": 6, "training_loss": 6407.452484130859, "training_acc": 28.0, "val_loss": 367.48950481414795, "val_acc": 72.0}
{"epoch": 7, "training_loss": 2589.7689208984375, "training_acc": 72.0, "val_loss": 894.0842628479004, "val_acc": 72.0}
{"epoch": 8, "training_loss": 2735.457359313965, "training_acc": 72.0, "val_loss": 1511.9486808776855, "val_acc": 28.0}
{"epoch": 9, "training_loss": 5027.692581176758, "training_acc": 28.0, "val_loss": 1008.7471961975098, "val_acc": 72.0}
{"epoch": 10, "training_loss": 5461.166412353516, "training_acc": 72.0, "val_loss": 2093.7740325927734, "val_acc": 72.0}
{"epoch": 11, "training_loss": 8103.213882446289, "training_acc": 72.0, "val_loss": 1310.0011825561523, "val_acc": 72.0}
{"epoch": 12, "training_loss": 4034.4082565307617, "training_acc": 72.0, "val_loss": 2180.282211303711, "val_acc": 28.0}
{"epoch": 13, "training_loss": 8786.992797851562, "training_acc": 28.0, "val_loss": 125.76740980148315, "val_acc": 72.0}
{"epoch": 14, "training_loss": 1221.5321731567383, "training_acc": 72.0, "val_loss": 692.5460815429688, "val_acc": 72.0}
{"epoch": 15, "training_loss": 2279.2235832214355, "training_acc": 72.0, "val_loss": 906.7534446716309, "val_acc": 28.0}
{"epoch": 16, "training_loss": 2282.6300916671753, "training_acc": 28.0, "val_loss": 1166.7414665222168, "val_acc": 72.0}
{"epoch": 17, "training_loss": 5778.066345214844, "training_acc": 72.0, "val_loss": 2091.826629638672, "val_acc": 72.0}
{"epoch": 18, "training_loss": 8085.487121582031, "training_acc": 72.0, "val_loss": 1331.4570426940918, "val_acc": 72.0}
{"epoch": 19, "training_loss": 4026.3643646240234, "training_acc": 72.0, "val_loss": 2014.2833709716797, "val_acc": 28.0}
{"epoch": 20, "training_loss": 8420.546295166016, "training_acc": 28.0, "val_loss": 50.719231367111206, "val_acc": 72.0}
{"epoch": 21, "training_loss": 960.4081649780273, "training_acc": 72.0, "val_loss": 552.1700382232666, "val_acc": 72.0}
{"epoch": 22, "training_loss": 1673.956781387329, "training_acc": 72.0, "val_loss": 1369.678783416748, "val_acc": 28.0}
{"epoch": 23, "training_loss": 4281.143905639648, "training_acc": 28.0, "val_loss": 1061.9303703308105, "val_acc": 72.0}
{"epoch": 24, "training_loss": 5848.316467285156, "training_acc": 72.0, "val_loss": 2216.499900817871, "val_acc": 72.0}
{"epoch": 25, "training_loss": 8711.962310791016, "training_acc": 72.0, "val_loss": 1615.9406661987305, "val_acc": 72.0}
{"epoch": 26, "training_loss": 5558.263771057129, "training_acc": 72.0, "val_loss": 942.1773910522461, "val_acc": 28.0}
{"epoch": 27, "training_loss": 3883.82763671875, "training_acc": 28.0, "val_loss": 608.4015846252441, "val_acc": 72.0}
{"epoch": 28, "training_loss": 2996.6774291992188, "training_acc": 72.0, "val_loss": 1251.226806640625, "val_acc": 72.0}
{"epoch": 29, "training_loss": 4647.975784301758, "training_acc": 72.0, "val_loss": 400.9786128997803, "val_acc": 72.0}
{"epoch": 30, "training_loss": 3386.517837524414, "training_acc": 52.0, "val_loss": 961.4270210266113, "val_acc": 28.0}
{"epoch": 31, "training_loss": 2990.7822265625, "training_acc": 50.0, "val_loss": 1377.7814865112305, "val_acc": 72.0}
{"epoch": 32, "training_loss": 5938.400543212891, "training_acc": 72.0, "val_loss": 1486.3919258117676, "val_acc": 72.0}
{"epoch": 33, "training_loss": 5230.338577270508, "training_acc": 72.0, "val_loss": 92.36704111099243, "val_acc": 72.0}
{"epoch": 34, "training_loss": 3651.6210021972656, "training_acc": 60.0, "val_loss": 2767.8781509399414, "val_acc": 28.0}
{"epoch": 35, "training_loss": 7743.670341491699, "training_acc": 28.0, "val_loss": 1568.3544158935547, "val_acc": 72.0}
{"epoch": 36, "training_loss": 7234.210845947266, "training_acc": 72.0, "val_loss": 3489.822769165039, "val_acc": 72.0}
{"epoch": 37, "training_loss": 14443.742065429688, "training_acc": 72.0, "val_loss": 3758.817672729492, "val_acc": 72.0}
{"epoch": 38, "training_loss": 14434.152374267578, "training_acc": 72.0, "val_loss": 2530.5097579956055, "val_acc": 72.0}
{"epoch": 39, "training_loss": 8243.252960205078, "training_acc": 72.0, "val_loss": 115.61535596847534, "val_acc": 72.0}
