"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 9525.691528320312, "training_acc": 72.0, "val_loss": 4850.389099121094, "val_acc": 72.0}
{"epoch": 1, "training_loss": 14006.984313964844, "training_acc": 72.0, "val_loss": 4712.197494506836, "val_acc": 28.0}
{"epoch": 2, "training_loss": 17555.51934814453, "training_acc": 28.0, "val_loss": 755.8645248413086, "val_acc": 72.0}
{"epoch": 3, "training_loss": 4326.669921875, "training_acc": 72.0, "val_loss": 2345.4879760742188, "val_acc": 72.0}
{"epoch": 4, "training_loss": 9033.46598815918, "training_acc": 72.0, "val_loss": 1236.0443115234375, "val_acc": 72.0}
{"epoch": 5, "training_loss": 2766.518091201782, "training_acc": 72.0, "val_loss": 4421.471405029297, "val_acc": 28.0}
{"epoch": 6, "training_loss": 17955.49102783203, "training_acc": 28.0, "val_loss": 1591.4448738098145, "val_acc": 28.0}
{"epoch": 7, "training_loss": 6529.333740234375, "training_acc": 42.0, "val_loss": 2934.2430114746094, "val_acc": 72.0}
{"epoch": 8, "training_loss": 13630.8310546875, "training_acc": 72.0, "val_loss": 4105.659484863281, "val_acc": 72.0}
{"epoch": 9, "training_loss": 16120.623291015625, "training_acc": 72.0, "val_loss": 3247.3861694335938, "val_acc": 72.0}
{"epoch": 10, "training_loss": 11654.227966308594, "training_acc": 72.0, "val_loss": 865.9754753112793, "val_acc": 72.0}
{"epoch": 11, "training_loss": 5741.393249511719, "training_acc": 52.0, "val_loss": 2841.581153869629, "val_acc": 28.0}
{"epoch": 12, "training_loss": 8608.29955291748, "training_acc": 28.0, "val_loss": 1308.1215858459473, "val_acc": 72.0}
{"epoch": 13, "training_loss": 6959.263397216797, "training_acc": 72.0, "val_loss": 2966.8357849121094, "val_acc": 72.0}
{"epoch": 14, "training_loss": 12164.316711425781, "training_acc": 72.0, "val_loss": 2816.767692565918, "val_acc": 72.0}
{"epoch": 15, "training_loss": 10379.928741455078, "training_acc": 72.0, "val_loss": 1130.5460929870605, "val_acc": 72.0}
{"epoch": 16, "training_loss": 4027.1733169555664, "training_acc": 52.0, "val_loss": 751.214599609375, "val_acc": 28.0}
{"epoch": 17, "training_loss": 1937.9365196228027, "training_acc": 56.0, "val_loss": 999.259090423584, "val_acc": 72.0}
{"epoch": 18, "training_loss": 4150.438766479492, "training_acc": 72.0, "val_loss": 809.272289276123, "val_acc": 72.0}
{"epoch": 19, "training_loss": 2449.302035331726, "training_acc": 72.0, "val_loss": 2116.568374633789, "val_acc": 28.0}
{"epoch": 20, "training_loss": 7891.268463134766, "training_acc": 28.0, "val_loss": 429.2978286743164, "val_acc": 72.0}
{"epoch": 21, "training_loss": 2279.817024230957, "training_acc": 72.0, "val_loss": 1222.7252960205078, "val_acc": 72.0}
{"epoch": 22, "training_loss": 4658.500961303711, "training_acc": 72.0, "val_loss": 521.2428092956543, "val_acc": 72.0}
{"epoch": 23, "training_loss": 2553.360580444336, "training_acc": 56.0, "val_loss": 254.13882732391357, "val_acc": 28.0}
{"epoch": 24, "training_loss": 1264.7821197509766, "training_acc": 54.0, "val_loss": 1727.208137512207, "val_acc": 72.0}
{"epoch": 25, "training_loss": 7372.040191650391, "training_acc": 72.0, "val_loss": 1950.3326416015625, "val_acc": 72.0}
{"epoch": 26, "training_loss": 7227.774490356445, "training_acc": 72.0, "val_loss": 666.0250663757324, "val_acc": 72.0}
{"epoch": 27, "training_loss": 4502.118072509766, "training_acc": 46.0, "val_loss": 1037.5768661499023, "val_acc": 28.0}
{"epoch": 28, "training_loss": 3404.2481689453125, "training_acc": 46.0, "val_loss": 1237.2360229492188, "val_acc": 72.0}
{"epoch": 29, "training_loss": 5203.846878051758, "training_acc": 72.0, "val_loss": 1183.4099769592285, "val_acc": 72.0}
{"epoch": 30, "training_loss": 4030.30419921875, "training_acc": 72.0, "val_loss": 785.5364799499512, "val_acc": 28.0}
{"epoch": 31, "training_loss": 2318.6747245788574, "training_acc": 28.0, "val_loss": 1029.3269157409668, "val_acc": 72.0}
{"epoch": 32, "training_loss": 5134.751495361328, "training_acc": 72.0, "val_loss": 1835.623550415039, "val_acc": 72.0}
{"epoch": 33, "training_loss": 7029.974761962891, "training_acc": 72.0, "val_loss": 1023.3858108520508, "val_acc": 72.0}
{"epoch": 34, "training_loss": 2884.3585510253906, "training_acc": 72.0, "val_loss": 2878.626823425293, "val_acc": 28.0}
{"epoch": 35, "training_loss": 11859.392669677734, "training_acc": 28.0, "val_loss": 841.8493270874023, "val_acc": 28.0}
{"epoch": 36, "training_loss": 3885.2266387939453, "training_acc": 46.0, "val_loss": 2601.949119567871, "val_acc": 72.0}
{"epoch": 37, "training_loss": 11333.857940673828, "training_acc": 72.0, "val_loss": 3605.362319946289, "val_acc": 72.0}
{"epoch": 38, "training_loss": 14277.423736572266, "training_acc": 72.0, "val_loss": 3019.5314407348633, "val_acc": 72.0}
{"epoch": 39, "training_loss": 10739.190979003906, "training_acc": 72.0, "val_loss": 1138.2028579711914, "val_acc": 72.0}
{"epoch": 40, "training_loss": 4249.7026443481445, "training_acc": 54.0, "val_loss": 1455.8781623840332, "val_acc": 28.0}
{"epoch": 41, "training_loss": 3681.2714080810547, "training_acc": 48.0, "val_loss": 423.0395317077637, "val_acc": 72.0}
{"epoch": 42, "training_loss": 1548.510066986084, "training_acc": 72.0, "val_loss": 335.57324409484863, "val_acc": 28.0}
