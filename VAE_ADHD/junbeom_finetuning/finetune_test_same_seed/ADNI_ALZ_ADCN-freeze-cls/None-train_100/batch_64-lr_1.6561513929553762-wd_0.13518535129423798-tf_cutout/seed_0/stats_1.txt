"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 6430.721351623535, "training_acc": 72.0, "val_loss": 2399.256706237793, "val_acc": 72.0}
{"epoch": 1, "training_loss": 11762.109191894531, "training_acc": 52.0, "val_loss": 359.1958284378052, "val_acc": 72.0}
{"epoch": 2, "training_loss": 1386.317756652832, "training_acc": 72.0, "val_loss": 363.0772352218628, "val_acc": 28.0}
{"epoch": 3, "training_loss": 2337.8451232910156, "training_acc": 46.0, "val_loss": 1413.2110595703125, "val_acc": 72.0}
{"epoch": 4, "training_loss": 5101.955596923828, "training_acc": 72.0, "val_loss": 213.58084678649902, "val_acc": 72.0}
{"epoch": 5, "training_loss": 4248.681182861328, "training_acc": 52.0, "val_loss": 829.6586990356445, "val_acc": 28.0}
{"epoch": 6, "training_loss": 4121.033721923828, "training_acc": 40.0, "val_loss": 1611.5211486816406, "val_acc": 72.0}
{"epoch": 7, "training_loss": 6357.359558105469, "training_acc": 72.0, "val_loss": 1058.875846862793, "val_acc": 72.0}
{"epoch": 8, "training_loss": 3088.4066047668457, "training_acc": 72.0, "val_loss": 1544.0234184265137, "val_acc": 28.0}
{"epoch": 9, "training_loss": 4995.243186950684, "training_acc": 28.0, "val_loss": 818.9070701599121, "val_acc": 72.0}
{"epoch": 10, "training_loss": 3879.985122680664, "training_acc": 72.0, "val_loss": 1173.8455772399902, "val_acc": 72.0}
{"epoch": 11, "training_loss": 4139.939323425293, "training_acc": 72.0, "val_loss": 117.08416938781738, "val_acc": 72.0}
{"epoch": 12, "training_loss": 2958.7468872070312, "training_acc": 54.0, "val_loss": 831.9375038146973, "val_acc": 28.0}
{"epoch": 13, "training_loss": 3010.8889083862305, "training_acc": 46.0, "val_loss": 1162.4579429626465, "val_acc": 72.0}
{"epoch": 14, "training_loss": 4678.589370727539, "training_acc": 72.0, "val_loss": 746.1605072021484, "val_acc": 72.0}
{"epoch": 15, "training_loss": 2135.0365562438965, "training_acc": 72.0, "val_loss": 1730.1994323730469, "val_acc": 28.0}
{"epoch": 16, "training_loss": 5792.721145629883, "training_acc": 28.0, "val_loss": 665.466833114624, "val_acc": 72.0}
{"epoch": 17, "training_loss": 3027.4552001953125, "training_acc": 72.0, "val_loss": 1049.7719764709473, "val_acc": 72.0}
{"epoch": 18, "training_loss": 3760.75496673584, "training_acc": 72.0, "val_loss": 171.01529836654663, "val_acc": 72.0}
{"epoch": 19, "training_loss": 2573.1026153564453, "training_acc": 54.0, "val_loss": 490.3046131134033, "val_acc": 28.0}
{"epoch": 20, "training_loss": 2621.651596069336, "training_acc": 42.0, "val_loss": 1236.2475395202637, "val_acc": 72.0}
{"epoch": 21, "training_loss": 4892.730773925781, "training_acc": 72.0, "val_loss": 788.8357162475586, "val_acc": 72.0}
{"epoch": 22, "training_loss": 2268.3060932159424, "training_acc": 72.0, "val_loss": 1498.1560707092285, "val_acc": 28.0}
{"epoch": 23, "training_loss": 4920.2546310424805, "training_acc": 28.0, "val_loss": 690.6955242156982, "val_acc": 72.0}
{"epoch": 24, "training_loss": 3550.7552032470703, "training_acc": 72.0, "val_loss": 993.3201789855957, "val_acc": 72.0}
{"epoch": 25, "training_loss": 3560.8572425842285, "training_acc": 72.0, "val_loss": 62.747395038604736, "val_acc": 28.0}
{"epoch": 26, "training_loss": 751.1826705932617, "training_acc": 42.0, "val_loss": 310.7977628707886, "val_acc": 72.0}
{"epoch": 27, "training_loss": 1479.8198852539062, "training_acc": 48.0, "val_loss": 487.825345993042, "val_acc": 72.0}
{"epoch": 28, "training_loss": 2252.5735473632812, "training_acc": 72.0, "val_loss": 357.04660415649414, "val_acc": 72.0}
{"epoch": 29, "training_loss": 1660.7678604125977, "training_acc": 54.0, "val_loss": 261.0874652862549, "val_acc": 72.0}
{"epoch": 30, "training_loss": 1044.902660369873, "training_acc": 72.0, "val_loss": 117.17253923416138, "val_acc": 28.0}
{"epoch": 31, "training_loss": 835.1515350341797, "training_acc": 48.0, "val_loss": 598.9236831665039, "val_acc": 72.0}
{"epoch": 32, "training_loss": 2053.941993713379, "training_acc": 72.0, "val_loss": 291.56761169433594, "val_acc": 28.0}
{"epoch": 33, "training_loss": 970.0394344329834, "training_acc": 50.0, "val_loss": 327.63938903808594, "val_acc": 72.0}
{"epoch": 34, "training_loss": 903.4433126449585, "training_acc": 72.0, "val_loss": 1157.532787322998, "val_acc": 28.0}
{"epoch": 35, "training_loss": 3023.935987472534, "training_acc": 28.0, "val_loss": 1051.2665748596191, "val_acc": 72.0}
{"epoch": 36, "training_loss": 4744.566833496094, "training_acc": 72.0, "val_loss": 1460.7844352722168, "val_acc": 72.0}
{"epoch": 37, "training_loss": 5313.389114379883, "training_acc": 72.0, "val_loss": 544.2908763885498, "val_acc": 72.0}
{"epoch": 38, "training_loss": 2319.266700744629, "training_acc": 54.0, "val_loss": 73.21500778198242, "val_acc": 72.0}
{"epoch": 39, "training_loss": 228.01231479644775, "training_acc": 58.0, "val_loss": 527.8172492980957, "val_acc": 72.0}
{"epoch": 40, "training_loss": 2321.2220306396484, "training_acc": 72.0, "val_loss": 309.8694324493408, "val_acc": 72.0}
{"epoch": 41, "training_loss": 1403.5152206420898, "training_acc": 60.0, "val_loss": 172.533118724823, "val_acc": 72.0}
{"epoch": 42, "training_loss": 641.7021045684814, "training_acc": 72.0, "val_loss": 562.4069690704346, "val_acc": 28.0}
{"epoch": 43, "training_loss": 1893.2059020996094, "training_acc": 44.0, "val_loss": 385.6776714324951, "val_acc": 72.0}
{"epoch": 44, "training_loss": 1157.4609603881836, "training_acc": 72.0, "val_loss": 981.5150260925293, "val_acc": 28.0}
