"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 83.75757217407227, "training_acc": 44.0, "val_loss": 24.93920773267746, "val_acc": 72.0}
{"epoch": 1, "training_loss": 91.16340112686157, "training_acc": 72.0, "val_loss": 16.155576705932617, "val_acc": 28.0}
{"epoch": 2, "training_loss": 70.56457877159119, "training_acc": 56.0, "val_loss": 16.4859801530838, "val_acc": 28.0}
{"epoch": 3, "training_loss": 61.61813449859619, "training_acc": 72.0, "val_loss": 17.304208874702454, "val_acc": 72.0}
{"epoch": 4, "training_loss": 70.42147874832153, "training_acc": 72.0, "val_loss": 17.39696115255356, "val_acc": 72.0}
{"epoch": 5, "training_loss": 67.13374066352844, "training_acc": 72.0, "val_loss": 15.064562857151031, "val_acc": 72.0}
{"epoch": 6, "training_loss": 61.02906847000122, "training_acc": 72.0, "val_loss": 16.521450877189636, "val_acc": 28.0}
{"epoch": 7, "training_loss": 63.762813568115234, "training_acc": 72.0, "val_loss": 14.939562976360321, "val_acc": 72.0}
{"epoch": 8, "training_loss": 66.0058205127716, "training_acc": 72.0, "val_loss": 16.633333265781403, "val_acc": 72.0}
{"epoch": 9, "training_loss": 64.27039337158203, "training_acc": 72.0, "val_loss": 14.830555021762848, "val_acc": 72.0}
{"epoch": 10, "training_loss": 58.43603992462158, "training_acc": 72.0, "val_loss": 17.12147146463394, "val_acc": 28.0}
{"epoch": 11, "training_loss": 67.4531180858612, "training_acc": 72.0, "val_loss": 14.971555769443512, "val_acc": 72.0}
{"epoch": 12, "training_loss": 59.273443937301636, "training_acc": 72.0, "val_loss": 16.099514067173004, "val_acc": 72.0}
{"epoch": 13, "training_loss": 64.65525269508362, "training_acc": 72.0, "val_loss": 15.684525668621063, "val_acc": 72.0}
{"epoch": 14, "training_loss": 61.04134678840637, "training_acc": 72.0, "val_loss": 15.143245458602905, "val_acc": 72.0}
{"epoch": 15, "training_loss": 61.67765688896179, "training_acc": 72.0, "val_loss": 15.673786401748657, "val_acc": 28.0}
{"epoch": 16, "training_loss": 61.1035430431366, "training_acc": 72.0, "val_loss": 15.010958909988403, "val_acc": 72.0}
{"epoch": 17, "training_loss": 62.499637842178345, "training_acc": 72.0, "val_loss": 15.653800964355469, "val_acc": 72.0}
{"epoch": 18, "training_loss": 62.25736927986145, "training_acc": 72.0, "val_loss": 14.88611102104187, "val_acc": 72.0}
{"epoch": 19, "training_loss": 59.838921546936035, "training_acc": 72.0, "val_loss": 15.165556967258453, "val_acc": 72.0}
{"epoch": 20, "training_loss": 61.38827848434448, "training_acc": 72.0, "val_loss": 14.829206466674805, "val_acc": 72.0}
{"epoch": 21, "training_loss": 59.36724519729614, "training_acc": 72.0, "val_loss": 14.822669327259064, "val_acc": 72.0}
{"epoch": 22, "training_loss": 59.01512551307678, "training_acc": 72.0, "val_loss": 14.91679847240448, "val_acc": 72.0}
{"epoch": 23, "training_loss": 59.45702838897705, "training_acc": 72.0, "val_loss": 14.903923869132996, "val_acc": 72.0}
{"epoch": 24, "training_loss": 59.38787126541138, "training_acc": 72.0, "val_loss": 14.821061491966248, "val_acc": 72.0}
{"epoch": 25, "training_loss": 59.018996477127075, "training_acc": 72.0, "val_loss": 14.849695563316345, "val_acc": 72.0}
{"epoch": 26, "training_loss": 59.199039936065674, "training_acc": 72.0, "val_loss": 14.83967900276184, "val_acc": 72.0}
{"epoch": 27, "training_loss": 59.47173810005188, "training_acc": 72.0, "val_loss": 14.82996940612793, "val_acc": 72.0}
{"epoch": 28, "training_loss": 58.95111632347107, "training_acc": 72.0, "val_loss": 15.100164711475372, "val_acc": 72.0}
{"epoch": 29, "training_loss": 60.160388469696045, "training_acc": 72.0, "val_loss": 14.873982965946198, "val_acc": 72.0}
{"epoch": 30, "training_loss": 59.15824365615845, "training_acc": 72.0, "val_loss": 14.94089812040329, "val_acc": 72.0}
{"epoch": 31, "training_loss": 59.55215573310852, "training_acc": 72.0, "val_loss": 15.161412954330444, "val_acc": 72.0}
{"epoch": 32, "training_loss": 59.919041872024536, "training_acc": 72.0, "val_loss": 14.865413308143616, "val_acc": 72.0}
{"epoch": 33, "training_loss": 59.819740533828735, "training_acc": 72.0, "val_loss": 15.21480679512024, "val_acc": 72.0}
{"epoch": 34, "training_loss": 59.75399994850159, "training_acc": 72.0, "val_loss": 15.013286471366882, "val_acc": 72.0}
{"epoch": 35, "training_loss": 59.583694100379944, "training_acc": 72.0, "val_loss": 15.554171800613403, "val_acc": 72.0}
{"epoch": 36, "training_loss": 62.39424228668213, "training_acc": 72.0, "val_loss": 14.821702241897583, "val_acc": 72.0}
{"epoch": 37, "training_loss": 61.87943172454834, "training_acc": 72.0, "val_loss": 15.812671184539795, "val_acc": 28.0}
{"epoch": 38, "training_loss": 62.578526735305786, "training_acc": 72.0, "val_loss": 15.012307465076447, "val_acc": 72.0}
{"epoch": 39, "training_loss": 60.90591478347778, "training_acc": 72.0, "val_loss": 15.056313574314117, "val_acc": 72.0}
{"epoch": 40, "training_loss": 59.5820996761322, "training_acc": 72.0, "val_loss": 15.161414444446564, "val_acc": 72.0}
{"epoch": 41, "training_loss": 60.33096361160278, "training_acc": 72.0, "val_loss": 14.88150805234909, "val_acc": 72.0}
{"epoch": 42, "training_loss": 59.109636545181274, "training_acc": 72.0, "val_loss": 15.009354054927826, "val_acc": 72.0}
{"epoch": 43, "training_loss": 59.99599766731262, "training_acc": 72.0, "val_loss": 14.829069375991821, "val_acc": 72.0}
