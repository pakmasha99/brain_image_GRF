"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 13651.578357696533, "training_acc": 44.0, "val_loss": 14201.307678222656, "val_acc": 28.0}
{"epoch": 1, "training_loss": 52433.768310546875, "training_acc": 38.0, "val_loss": 4023.7060546875, "val_acc": 28.0}
{"epoch": 2, "training_loss": 17359.90509033203, "training_acc": 44.0, "val_loss": 307.56542682647705, "val_acc": 72.0}
{"epoch": 3, "training_loss": 1828.2762145996094, "training_acc": 72.0, "val_loss": 1254.395866394043, "val_acc": 28.0}
{"epoch": 4, "training_loss": 8080.7698974609375, "training_acc": 38.0, "val_loss": 472.5607395172119, "val_acc": 28.0}
{"epoch": 5, "training_loss": 5168.143096923828, "training_acc": 44.0, "val_loss": 448.1034278869629, "val_acc": 72.0}
{"epoch": 6, "training_loss": 1965.4326248168945, "training_acc": 72.0, "val_loss": 973.462963104248, "val_acc": 28.0}
{"epoch": 7, "training_loss": 4372.764541625977, "training_acc": 48.0, "val_loss": 124.41070079803467, "val_acc": 72.0}
{"epoch": 8, "training_loss": 362.79527735710144, "training_acc": 54.0, "val_loss": 685.5640888214111, "val_acc": 72.0}
{"epoch": 9, "training_loss": 2845.5078125, "training_acc": 58.0, "val_loss": 1736.4473342895508, "val_acc": 72.0}
{"epoch": 10, "training_loss": 4509.448464870453, "training_acc": 72.0, "val_loss": 61.32664084434509, "val_acc": 72.0}
{"epoch": 11, "training_loss": 2466.8367462158203, "training_acc": 56.0, "val_loss": 1385.0943565368652, "val_acc": 72.0}
{"epoch": 12, "training_loss": 4387.929725646973, "training_acc": 56.0, "val_loss": 1997.1464157104492, "val_acc": 72.0}
{"epoch": 13, "training_loss": 5482.071392059326, "training_acc": 72.0, "val_loss": 279.33788299560547, "val_acc": 72.0}
{"epoch": 14, "training_loss": 1761.5894775390625, "training_acc": 60.0, "val_loss": 1404.7541618347168, "val_acc": 72.0}
{"epoch": 15, "training_loss": 4228.324073791504, "training_acc": 54.0, "val_loss": 2065.679168701172, "val_acc": 72.0}
{"epoch": 16, "training_loss": 5607.366584777832, "training_acc": 72.0, "val_loss": 327.82952785491943, "val_acc": 72.0}
{"epoch": 17, "training_loss": 2188.780029296875, "training_acc": 54.0, "val_loss": 1545.3510284423828, "val_acc": 72.0}
{"epoch": 18, "training_loss": 3994.767964363098, "training_acc": 72.0, "val_loss": 383.59506130218506, "val_acc": 72.0}
{"epoch": 19, "training_loss": 2077.0884552001953, "training_acc": 60.0, "val_loss": 1395.6073760986328, "val_acc": 72.0}
{"epoch": 20, "training_loss": 3520.1481046676636, "training_acc": 58.0, "val_loss": 1956.0720443725586, "val_acc": 72.0}
{"epoch": 21, "training_loss": 5880.217670440674, "training_acc": 72.0, "val_loss": 414.3589973449707, "val_acc": 72.0}
{"epoch": 22, "training_loss": 2188.5954971313477, "training_acc": 56.0, "val_loss": 1532.87935256958, "val_acc": 72.0}
{"epoch": 23, "training_loss": 4570.738899230957, "training_acc": 52.0, "val_loss": 2161.4709854125977, "val_acc": 72.0}
{"epoch": 24, "training_loss": 6584.407642364502, "training_acc": 72.0, "val_loss": 549.724006652832, "val_acc": 72.0}
{"epoch": 25, "training_loss": 2347.654251098633, "training_acc": 54.0, "val_loss": 1656.075668334961, "val_acc": 72.0}
{"epoch": 26, "training_loss": 4097.213817596436, "training_acc": 72.0, "val_loss": 34.628164768218994, "val_acc": 72.0}
{"epoch": 27, "training_loss": 1708.7599639892578, "training_acc": 58.0, "val_loss": 1244.2423820495605, "val_acc": 72.0}
{"epoch": 28, "training_loss": 4165.693420410156, "training_acc": 54.0, "val_loss": 1920.234489440918, "val_acc": 72.0}
{"epoch": 29, "training_loss": 5697.6503257751465, "training_acc": 72.0, "val_loss": 375.3146171569824, "val_acc": 72.0}
{"epoch": 30, "training_loss": 3174.680450439453, "training_acc": 48.0, "val_loss": 1634.7082138061523, "val_acc": 72.0}
{"epoch": 31, "training_loss": 4230.083687543869, "training_acc": 72.0, "val_loss": 230.97829818725586, "val_acc": 72.0}
{"epoch": 32, "training_loss": 1176.9967803955078, "training_acc": 68.0, "val_loss": 1113.0663871765137, "val_acc": 72.0}
{"epoch": 33, "training_loss": 3470.6441955566406, "training_acc": 56.0, "val_loss": 1837.723159790039, "val_acc": 72.0}
{"epoch": 34, "training_loss": 5236.409763336182, "training_acc": 72.0, "val_loss": 286.3546133041382, "val_acc": 72.0}
{"epoch": 35, "training_loss": 2354.1351928710938, "training_acc": 54.0, "val_loss": 1468.6708450317383, "val_acc": 72.0}
{"epoch": 36, "training_loss": 6132.805702209473, "training_acc": 44.0, "val_loss": 2252.7856826782227, "val_acc": 72.0}
{"epoch": 37, "training_loss": 6151.741943359375, "training_acc": 72.0, "val_loss": 454.71506118774414, "val_acc": 72.0}
{"epoch": 38, "training_loss": 1889.335838317871, "training_acc": 56.0, "val_loss": 1555.3621292114258, "val_acc": 72.0}
{"epoch": 39, "training_loss": 4290.776711463928, "training_acc": 72.0, "val_loss": 117.71427392959595, "val_acc": 72.0}
{"epoch": 40, "training_loss": 1967.347427368164, "training_acc": 58.0, "val_loss": 1247.9628562927246, "val_acc": 72.0}
{"epoch": 41, "training_loss": 3837.2801666259766, "training_acc": 56.0, "val_loss": 1871.1084365844727, "val_acc": 72.0}
{"epoch": 42, "training_loss": 4807.5908279418945, "training_acc": 72.0, "val_loss": 188.31310272216797, "val_acc": 72.0}
{"epoch": 43, "training_loss": 2725.1564178466797, "training_acc": 48.0, "val_loss": 1535.6325149536133, "val_acc": 72.0}
{"epoch": 44, "training_loss": 4094.3426303863525, "training_acc": 56.0, "val_loss": 2024.3194580078125, "val_acc": 72.0}
{"epoch": 45, "training_loss": 6572.457576751709, "training_acc": 72.0, "val_loss": 563.3864402770996, "val_acc": 72.0}
