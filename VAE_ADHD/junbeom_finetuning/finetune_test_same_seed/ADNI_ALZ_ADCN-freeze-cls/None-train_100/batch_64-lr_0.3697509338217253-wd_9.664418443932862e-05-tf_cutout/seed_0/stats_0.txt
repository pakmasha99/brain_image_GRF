"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 1684.7217559814453, "training_acc": 42.0, "val_loss": 1017.5219535827637, "val_acc": 72.0}
{"epoch": 1, "training_loss": 3378.3645401000977, "training_acc": 72.0, "val_loss": 67.67057180404663, "val_acc": 72.0}
{"epoch": 2, "training_loss": 2757.826690673828, "training_acc": 50.0, "val_loss": 1553.4252166748047, "val_acc": 28.0}
{"epoch": 3, "training_loss": 5109.6339111328125, "training_acc": 28.0, "val_loss": 175.40501356124878, "val_acc": 72.0}
{"epoch": 4, "training_loss": 1214.9042205810547, "training_acc": 72.0, "val_loss": 706.1363220214844, "val_acc": 72.0}
{"epoch": 5, "training_loss": 2809.721260070801, "training_acc": 72.0, "val_loss": 750.3292560577393, "val_acc": 72.0}
{"epoch": 6, "training_loss": 2802.34228515625, "training_acc": 72.0, "val_loss": 474.59068298339844, "val_acc": 72.0}
{"epoch": 7, "training_loss": 1606.8040657043457, "training_acc": 72.0, "val_loss": 203.65209579467773, "val_acc": 28.0}
{"epoch": 8, "training_loss": 941.333251953125, "training_acc": 28.0, "val_loss": 35.972437262535095, "val_acc": 72.0}
{"epoch": 9, "training_loss": 187.90234088897705, "training_acc": 72.0, "val_loss": 79.33853268623352, "val_acc": 72.0}
{"epoch": 10, "training_loss": 225.9642789363861, "training_acc": 61.0, "val_loss": 13.692930340766907, "val_acc": 72.0}
{"epoch": 11, "training_loss": 55.62217736244202, "training_acc": 71.0, "val_loss": 15.401540696620941, "val_acc": 48.0}
{"epoch": 12, "training_loss": 104.28590202331543, "training_acc": 71.0, "val_loss": 27.973467111587524, "val_acc": 72.0}
{"epoch": 13, "training_loss": 428.7942695617676, "training_acc": 48.0, "val_loss": 37.69730031490326, "val_acc": 72.0}
{"epoch": 14, "training_loss": 223.62788677215576, "training_acc": 72.0, "val_loss": 33.862486481666565, "val_acc": 72.0}
{"epoch": 15, "training_loss": 321.46226501464844, "training_acc": 60.0, "val_loss": 15.184307098388672, "val_acc": 52.0}
{"epoch": 16, "training_loss": 275.7528896331787, "training_acc": 64.0, "val_loss": 175.24935007095337, "val_acc": 72.0}
{"epoch": 17, "training_loss": 631.1772575378418, "training_acc": 72.0, "val_loss": 21.220572292804718, "val_acc": 72.0}
{"epoch": 18, "training_loss": 429.10787200927734, "training_acc": 62.0, "val_loss": 189.01777267456055, "val_acc": 28.0}
{"epoch": 19, "training_loss": 577.8338499069214, "training_acc": 50.0, "val_loss": 261.73384189605713, "val_acc": 72.0}
{"epoch": 20, "training_loss": 1079.146556854248, "training_acc": 72.0, "val_loss": 281.6780090332031, "val_acc": 72.0}
{"epoch": 21, "training_loss": 945.9232940673828, "training_acc": 72.0, "val_loss": 23.25979173183441, "val_acc": 72.0}
{"epoch": 22, "training_loss": 749.4383239746094, "training_acc": 56.0, "val_loss": 499.64756965637207, "val_acc": 28.0}
{"epoch": 23, "training_loss": 1394.6051607131958, "training_acc": 28.0, "val_loss": 290.0639057159424, "val_acc": 72.0}
{"epoch": 24, "training_loss": 1454.1759872436523, "training_acc": 72.0, "val_loss": 650.9997844696045, "val_acc": 72.0}
{"epoch": 25, "training_loss": 2632.485023498535, "training_acc": 72.0, "val_loss": 674.6778011322021, "val_acc": 72.0}
{"epoch": 26, "training_loss": 2510.1285400390625, "training_acc": 72.0, "val_loss": 411.8582248687744, "val_acc": 72.0}
{"epoch": 27, "training_loss": 1306.5116271972656, "training_acc": 72.0, "val_loss": 163.39930295944214, "val_acc": 28.0}
{"epoch": 28, "training_loss": 898.756275177002, "training_acc": 28.0, "val_loss": 13.098368048667908, "val_acc": 64.0}
{"epoch": 29, "training_loss": 271.7533664703369, "training_acc": 70.0, "val_loss": 248.4102964401245, "val_acc": 72.0}
{"epoch": 30, "training_loss": 960.0945682525635, "training_acc": 72.0, "val_loss": 179.31054830551147, "val_acc": 72.0}
{"epoch": 31, "training_loss": 481.9842834472656, "training_acc": 72.0, "val_loss": 305.87189197540283, "val_acc": 28.0}
{"epoch": 32, "training_loss": 1271.6697578430176, "training_acc": 28.0, "val_loss": 47.9730099439621, "val_acc": 72.0}
{"epoch": 33, "training_loss": 284.28466987609863, "training_acc": 72.0, "val_loss": 161.5541934967041, "val_acc": 72.0}
{"epoch": 34, "training_loss": 557.4638004302979, "training_acc": 72.0, "val_loss": 13.328257203102112, "val_acc": 64.0}
{"epoch": 35, "training_loss": 209.3288402557373, "training_acc": 48.0, "val_loss": 73.24969172477722, "val_acc": 72.0}
{"epoch": 36, "training_loss": 332.8536081314087, "training_acc": 72.0, "val_loss": 87.50121593475342, "val_acc": 72.0}
{"epoch": 37, "training_loss": 190.862975358963, "training_acc": 75.0, "val_loss": 223.17264080047607, "val_acc": 28.0}
{"epoch": 38, "training_loss": 641.8406324386597, "training_acc": 39.0, "val_loss": 144.29364204406738, "val_acc": 72.0}
{"epoch": 39, "training_loss": 730.3752632141113, "training_acc": 72.0, "val_loss": 204.60617542266846, "val_acc": 72.0}
{"epoch": 40, "training_loss": 651.2868003845215, "training_acc": 72.0, "val_loss": 89.90670442581177, "val_acc": 28.0}
{"epoch": 41, "training_loss": 286.3245072364807, "training_acc": 34.0, "val_loss": 121.17875814437866, "val_acc": 72.0}
{"epoch": 42, "training_loss": 527.9582843780518, "training_acc": 72.0, "val_loss": 141.88382625579834, "val_acc": 72.0}
{"epoch": 43, "training_loss": 435.3305015563965, "training_acc": 72.0, "val_loss": 243.20194721221924, "val_acc": 28.0}
{"epoch": 44, "training_loss": 838.3248672485352, "training_acc": 28.0, "val_loss": 158.89092683792114, "val_acc": 72.0}
{"epoch": 45, "training_loss": 816.3295783996582, "training_acc": 72.0, "val_loss": 330.72242736816406, "val_acc": 72.0}
{"epoch": 46, "training_loss": 1243.1458625793457, "training_acc": 72.0, "val_loss": 191.44490957260132, "val_acc": 72.0}
{"epoch": 47, "training_loss": 487.48907995224, "training_acc": 72.0, "val_loss": 412.63389587402344, "val_acc": 28.0}
