"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 4565.571151733398, "training_acc": 40.0, "val_loss": 2272.566032409668, "val_acc": 72.0}
{"epoch": 1, "training_loss": 8340.515090942383, "training_acc": 72.0, "val_loss": 987.9111289978027, "val_acc": 28.0}
{"epoch": 2, "training_loss": 2631.2227668762207, "training_acc": 42.0, "val_loss": 100.42080879211426, "val_acc": 28.0}
{"epoch": 3, "training_loss": 578.0282745361328, "training_acc": 56.0, "val_loss": 1075.9870529174805, "val_acc": 72.0}
{"epoch": 4, "training_loss": 4473.214691162109, "training_acc": 72.0, "val_loss": 961.7545127868652, "val_acc": 72.0}
{"epoch": 5, "training_loss": 3165.263198852539, "training_acc": 72.0, "val_loss": 526.9278526306152, "val_acc": 28.0}
{"epoch": 6, "training_loss": 1879.9279174804688, "training_acc": 28.0, "val_loss": 480.18665313720703, "val_acc": 72.0}
{"epoch": 7, "training_loss": 2790.136917114258, "training_acc": 72.0, "val_loss": 930.179500579834, "val_acc": 72.0}
{"epoch": 8, "training_loss": 3466.119369506836, "training_acc": 72.0, "val_loss": 402.21686363220215, "val_acc": 72.0}
{"epoch": 9, "training_loss": 1696.976661682129, "training_acc": 54.0, "val_loss": 281.4460277557373, "val_acc": 28.0}
{"epoch": 10, "training_loss": 1558.4682922363281, "training_acc": 38.0, "val_loss": 674.2767333984375, "val_acc": 72.0}
{"epoch": 11, "training_loss": 2801.9859771728516, "training_acc": 72.0, "val_loss": 575.914192199707, "val_acc": 72.0}
{"epoch": 12, "training_loss": 1868.48286819458, "training_acc": 72.0, "val_loss": 591.45188331604, "val_acc": 28.0}
{"epoch": 13, "training_loss": 2142.856830596924, "training_acc": 28.0, "val_loss": 297.6999521255493, "val_acc": 72.0}
{"epoch": 14, "training_loss": 1449.7777557373047, "training_acc": 72.0, "val_loss": 595.0166702270508, "val_acc": 72.0}
{"epoch": 15, "training_loss": 2223.9426803588867, "training_acc": 72.0, "val_loss": 208.72535705566406, "val_acc": 72.0}
{"epoch": 16, "training_loss": 1301.5526809692383, "training_acc": 56.0, "val_loss": 335.75737476348877, "val_acc": 28.0}
{"epoch": 17, "training_loss": 1371.2050323486328, "training_acc": 44.0, "val_loss": 642.1723365783691, "val_acc": 72.0}
{"epoch": 18, "training_loss": 2689.37735748291, "training_acc": 72.0, "val_loss": 636.711597442627, "val_acc": 72.0}
{"epoch": 19, "training_loss": 2234.9797897338867, "training_acc": 72.0, "val_loss": 44.38709914684296, "val_acc": 28.0}
{"epoch": 20, "training_loss": 165.81038761138916, "training_acc": 42.0, "val_loss": 254.4449806213379, "val_acc": 28.0}
{"epoch": 21, "training_loss": 945.387321472168, "training_acc": 44.0, "val_loss": 321.860671043396, "val_acc": 72.0}
{"epoch": 22, "training_loss": 1191.853717803955, "training_acc": 72.0, "val_loss": 28.698688745498657, "val_acc": 72.0}
{"epoch": 23, "training_loss": 1413.4662475585938, "training_acc": 54.0, "val_loss": 597.0019817352295, "val_acc": 28.0}
{"epoch": 24, "training_loss": 2021.2774772644043, "training_acc": 42.0, "val_loss": 596.2815284729004, "val_acc": 72.0}
{"epoch": 25, "training_loss": 2534.9107971191406, "training_acc": 72.0, "val_loss": 632.5942039489746, "val_acc": 72.0}
{"epoch": 26, "training_loss": 2245.0153884887695, "training_acc": 72.0, "val_loss": 24.72119778394699, "val_acc": 72.0}
{"epoch": 27, "training_loss": 1452.1425704956055, "training_acc": 62.0, "val_loss": 1145.0160026550293, "val_acc": 28.0}
{"epoch": 28, "training_loss": 3097.9692249298096, "training_acc": 28.0, "val_loss": 732.2176456451416, "val_acc": 72.0}
{"epoch": 29, "training_loss": 3426.504440307617, "training_acc": 72.0, "val_loss": 1569.6882247924805, "val_acc": 72.0}
{"epoch": 30, "training_loss": 6416.0693359375, "training_acc": 72.0, "val_loss": 1667.6944732666016, "val_acc": 72.0}
{"epoch": 31, "training_loss": 6432.772720336914, "training_acc": 72.0, "val_loss": 1139.3646240234375, "val_acc": 72.0}
{"epoch": 32, "training_loss": 3899.178726196289, "training_acc": 72.0, "val_loss": 94.89228129386902, "val_acc": 72.0}
{"epoch": 33, "training_loss": 2747.304443359375, "training_acc": 52.0, "val_loss": 2031.2652587890625, "val_acc": 28.0}
{"epoch": 34, "training_loss": 7102.235137939453, "training_acc": 28.0, "val_loss": 141.85165166854858, "val_acc": 72.0}
{"epoch": 35, "training_loss": 1299.7482070922852, "training_acc": 72.0, "val_loss": 850.483512878418, "val_acc": 72.0}
{"epoch": 36, "training_loss": 3455.7427139282227, "training_acc": 72.0, "val_loss": 784.9302291870117, "val_acc": 72.0}
{"epoch": 37, "training_loss": 2714.3661880493164, "training_acc": 72.0, "val_loss": 114.8889422416687, "val_acc": 72.0}
{"epoch": 38, "training_loss": 1901.8182983398438, "training_acc": 56.0, "val_loss": 1269.045639038086, "val_acc": 28.0}
{"epoch": 39, "training_loss": 3658.8919677734375, "training_acc": 28.0, "val_loss": 611.0245227813721, "val_acc": 72.0}
{"epoch": 40, "training_loss": 3211.5325469970703, "training_acc": 72.0, "val_loss": 1473.2698440551758, "val_acc": 72.0}
{"epoch": 41, "training_loss": 6042.066665649414, "training_acc": 72.0, "val_loss": 1552.1159172058105, "val_acc": 72.0}
{"epoch": 42, "training_loss": 5881.936584472656, "training_acc": 72.0, "val_loss": 1006.1055183410645, "val_acc": 72.0}
{"epoch": 43, "training_loss": 3363.0782928466797, "training_acc": 72.0, "val_loss": 291.422700881958, "val_acc": 28.0}
{"epoch": 44, "training_loss": 1561.8155517578125, "training_acc": 28.0, "val_loss": 79.52717542648315, "val_acc": 72.0}
{"epoch": 45, "training_loss": 448.5915927886963, "training_acc": 72.0, "val_loss": 129.12837266921997, "val_acc": 72.0}
