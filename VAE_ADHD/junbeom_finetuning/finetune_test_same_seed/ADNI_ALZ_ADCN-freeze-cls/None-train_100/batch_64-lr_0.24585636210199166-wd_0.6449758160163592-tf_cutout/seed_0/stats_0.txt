"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 1202.8458251953125, "training_acc": 42.0, "val_loss": 517.039155960083, "val_acc": 72.0}
{"epoch": 1, "training_loss": 1382.7400493621826, "training_acc": 72.0, "val_loss": 929.2116165161133, "val_acc": 28.0}
{"epoch": 2, "training_loss": 3261.310905456543, "training_acc": 28.0, "val_loss": 80.91378808021545, "val_acc": 72.0}
{"epoch": 3, "training_loss": 538.2836074829102, "training_acc": 72.0, "val_loss": 254.44388389587402, "val_acc": 72.0}
{"epoch": 4, "training_loss": 918.7802200317383, "training_acc": 72.0, "val_loss": 59.165650606155396, "val_acc": 72.0}
{"epoch": 5, "training_loss": 735.1785278320312, "training_acc": 48.0, "val_loss": 152.26250886917114, "val_acc": 28.0}
{"epoch": 6, "training_loss": 583.0736484527588, "training_acc": 44.0, "val_loss": 224.22292232513428, "val_acc": 72.0}
{"epoch": 7, "training_loss": 893.5037975311279, "training_acc": 72.0, "val_loss": 176.84234380722046, "val_acc": 72.0}
{"epoch": 8, "training_loss": 566.9842529296875, "training_acc": 72.0, "val_loss": 106.04928731918335, "val_acc": 28.0}
{"epoch": 9, "training_loss": 295.02102518081665, "training_acc": 28.0, "val_loss": 117.84857511520386, "val_acc": 72.0}
{"epoch": 10, "training_loss": 530.7493648529053, "training_acc": 72.0, "val_loss": 137.96513080596924, "val_acc": 72.0}
{"epoch": 11, "training_loss": 451.17054080963135, "training_acc": 72.0, "val_loss": 97.49035239219666, "val_acc": 28.0}
{"epoch": 12, "training_loss": 264.33798909187317, "training_acc": 46.0, "val_loss": 36.71596944332123, "val_acc": 72.0}
{"epoch": 13, "training_loss": 149.29732537269592, "training_acc": 48.0, "val_loss": 51.18262767791748, "val_acc": 72.0}
{"epoch": 14, "training_loss": 199.50855541229248, "training_acc": 72.0, "val_loss": 39.48781490325928, "val_acc": 28.0}
{"epoch": 15, "training_loss": 192.15048599243164, "training_acc": 40.0, "val_loss": 41.09368920326233, "val_acc": 72.0}
{"epoch": 16, "training_loss": 137.28550004959106, "training_acc": 60.0, "val_loss": 36.262813210487366, "val_acc": 72.0}
{"epoch": 17, "training_loss": 127.21778178215027, "training_acc": 72.0, "val_loss": 45.495033264160156, "val_acc": 28.0}
{"epoch": 18, "training_loss": 270.4192295074463, "training_acc": 38.0, "val_loss": 87.86588311195374, "val_acc": 72.0}
{"epoch": 19, "training_loss": 299.3626780509949, "training_acc": 72.0, "val_loss": 96.77248001098633, "val_acc": 28.0}
{"epoch": 20, "training_loss": 279.95449686050415, "training_acc": 46.0, "val_loss": 39.00522291660309, "val_acc": 72.0}
{"epoch": 21, "training_loss": 107.08710169792175, "training_acc": 72.0, "val_loss": 73.98706078529358, "val_acc": 28.0}
{"epoch": 22, "training_loss": 286.71731662750244, "training_acc": 44.0, "val_loss": 92.59224534034729, "val_acc": 72.0}
{"epoch": 23, "training_loss": 325.3600721359253, "training_acc": 72.0, "val_loss": 37.946707010269165, "val_acc": 28.0}
{"epoch": 24, "training_loss": 149.2168526649475, "training_acc": 44.0, "val_loss": 26.7514705657959, "val_acc": 72.0}
{"epoch": 25, "training_loss": 154.33153915405273, "training_acc": 58.0, "val_loss": 53.41216325759888, "val_acc": 72.0}
{"epoch": 26, "training_loss": 222.38248348236084, "training_acc": 72.0, "val_loss": 22.139038145542145, "val_acc": 72.0}
{"epoch": 27, "training_loss": 244.81451034545898, "training_acc": 56.0, "val_loss": 23.2327401638031, "val_acc": 72.0}
{"epoch": 28, "training_loss": 105.01771879196167, "training_acc": 72.0, "val_loss": 19.19485777616501, "val_acc": 28.0}
{"epoch": 29, "training_loss": 83.87159085273743, "training_acc": 44.0, "val_loss": 17.74638295173645, "val_acc": 28.0}
{"epoch": 30, "training_loss": 64.50562167167664, "training_acc": 50.0, "val_loss": 15.375608205795288, "val_acc": 72.0}
{"epoch": 31, "training_loss": 58.363505125045776, "training_acc": 72.0, "val_loss": 15.810367465019226, "val_acc": 72.0}
{"epoch": 32, "training_loss": 84.23655366897583, "training_acc": 52.0, "val_loss": 73.73949885368347, "val_acc": 72.0}
{"epoch": 33, "training_loss": 298.5652666091919, "training_acc": 72.0, "val_loss": 37.67065107822418, "val_acc": 72.0}
{"epoch": 34, "training_loss": 217.80281257629395, "training_acc": 60.0, "val_loss": 27.012735605239868, "val_acc": 72.0}
{"epoch": 35, "training_loss": 109.84176635742188, "training_acc": 72.0, "val_loss": 50.68420767784119, "val_acc": 28.0}
{"epoch": 36, "training_loss": 207.81817531585693, "training_acc": 46.0, "val_loss": 78.67623567581177, "val_acc": 72.0}
{"epoch": 37, "training_loss": 255.75017166137695, "training_acc": 72.0, "val_loss": 115.86751937866211, "val_acc": 28.0}
{"epoch": 38, "training_loss": 333.64290475845337, "training_acc": 42.0, "val_loss": 23.872970044612885, "val_acc": 72.0}
{"epoch": 39, "training_loss": 81.41460514068604, "training_acc": 64.0, "val_loss": 33.314049243927, "val_acc": 72.0}
{"epoch": 40, "training_loss": 110.57009482383728, "training_acc": 72.0, "val_loss": 108.25544595718384, "val_acc": 28.0}
{"epoch": 41, "training_loss": 355.5196838378906, "training_acc": 42.0, "val_loss": 57.35129714012146, "val_acc": 72.0}
{"epoch": 42, "training_loss": 176.30502080917358, "training_acc": 72.0, "val_loss": 97.5422739982605, "val_acc": 28.0}
{"epoch": 43, "training_loss": 296.36097717285156, "training_acc": 48.0, "val_loss": 76.86795592308044, "val_acc": 72.0}
{"epoch": 44, "training_loss": 260.4848861694336, "training_acc": 72.0, "val_loss": 82.93294906616211, "val_acc": 28.0}
{"epoch": 45, "training_loss": 283.9549341201782, "training_acc": 42.0, "val_loss": 43.91394853591919, "val_acc": 72.0}
{"epoch": 46, "training_loss": 180.94616651535034, "training_acc": 52.0, "val_loss": 60.08840203285217, "val_acc": 72.0}
{"epoch": 47, "training_loss": 255.498685836792, "training_acc": 72.0, "val_loss": 17.26495772600174, "val_acc": 72.0}
{"epoch": 48, "training_loss": 302.9240474700928, "training_acc": 52.0, "val_loss": 20.807701349258423, "val_acc": 72.0}
{"epoch": 49, "training_loss": 106.56453895568848, "training_acc": 72.0, "val_loss": 15.039065480232239, "val_acc": 72.0}
{"epoch": 50, "training_loss": 168.09567737579346, "training_acc": 52.0, "val_loss": 76.33855938911438, "val_acc": 72.0}
{"epoch": 51, "training_loss": 340.2821979522705, "training_acc": 72.0, "val_loss": 77.7429461479187, "val_acc": 72.0}
{"epoch": 52, "training_loss": 220.95184445381165, "training_acc": 72.0, "val_loss": 114.54181671142578, "val_acc": 28.0}
{"epoch": 53, "training_loss": 313.1543073654175, "training_acc": 50.0, "val_loss": 67.88924932479858, "val_acc": 72.0}
{"epoch": 54, "training_loss": 228.06206893920898, "training_acc": 72.0, "val_loss": 92.39597916603088, "val_acc": 28.0}
{"epoch": 55, "training_loss": 263.71967363357544, "training_acc": 48.0, "val_loss": 45.77537178993225, "val_acc": 72.0}
{"epoch": 56, "training_loss": 157.7746081352234, "training_acc": 52.0, "val_loss": 23.624244332313538, "val_acc": 72.0}
{"epoch": 57, "training_loss": 112.80351257324219, "training_acc": 52.0, "val_loss": 70.03337144851685, "val_acc": 72.0}
{"epoch": 58, "training_loss": 318.7943801879883, "training_acc": 72.0, "val_loss": 32.23934769630432, "val_acc": 72.0}
{"epoch": 59, "training_loss": 335.900146484375, "training_acc": 54.0, "val_loss": 17.267489433288574, "val_acc": 28.0}
{"epoch": 60, "training_loss": 139.56777667999268, "training_acc": 62.0, "val_loss": 105.10084629058838, "val_acc": 72.0}
{"epoch": 61, "training_loss": 369.76061725616455, "training_acc": 72.0, "val_loss": 31.926938891410828, "val_acc": 28.0}
{"epoch": 62, "training_loss": 134.48314094543457, "training_acc": 42.0, "val_loss": 15.53795039653778, "val_acc": 72.0}
{"epoch": 63, "training_loss": 80.93217897415161, "training_acc": 64.0, "val_loss": 56.92518949508667, "val_acc": 72.0}
{"epoch": 64, "training_loss": 229.13001918792725, "training_acc": 72.0, "val_loss": 14.750716090202332, "val_acc": 72.0}
{"epoch": 65, "training_loss": 107.46018505096436, "training_acc": 64.0, "val_loss": 60.17724871635437, "val_acc": 72.0}
{"epoch": 66, "training_loss": 242.46549606323242, "training_acc": 72.0, "val_loss": 27.362605929374695, "val_acc": 72.0}
{"epoch": 67, "training_loss": 294.38013648986816, "training_acc": 52.0, "val_loss": 28.588613867759705, "val_acc": 72.0}
{"epoch": 68, "training_loss": 115.97772908210754, "training_acc": 72.0, "val_loss": 15.231925249099731, "val_acc": 76.0}
{"epoch": 69, "training_loss": 63.45024847984314, "training_acc": 72.0, "val_loss": 36.35168373584747, "val_acc": 72.0}
{"epoch": 70, "training_loss": 129.66411137580872, "training_acc": 72.0, "val_loss": 16.95462316274643, "val_acc": 72.0}
{"epoch": 71, "training_loss": 66.6317024230957, "training_acc": 60.0, "val_loss": 31.006744503974915, "val_acc": 72.0}
{"epoch": 72, "training_loss": 137.7153172492981, "training_acc": 48.0, "val_loss": 68.85033845901489, "val_acc": 72.0}
{"epoch": 73, "training_loss": 314.82812118530273, "training_acc": 72.0, "val_loss": 23.28626960515976, "val_acc": 72.0}
{"epoch": 74, "training_loss": 379.6720771789551, "training_acc": 52.0, "val_loss": 28.379249572753906, "val_acc": 28.0}
{"epoch": 75, "training_loss": 290.6129341125488, "training_acc": 42.0, "val_loss": 190.5276656150818, "val_acc": 72.0}
{"epoch": 76, "training_loss": 737.782205581665, "training_acc": 72.0, "val_loss": 109.85997915267944, "val_acc": 72.0}
{"epoch": 77, "training_loss": 282.2459707260132, "training_acc": 72.0, "val_loss": 258.49478244781494, "val_acc": 28.0}
{"epoch": 78, "training_loss": 856.7239933013916, "training_acc": 28.0, "val_loss": 114.5988941192627, "val_acc": 72.0}
{"epoch": 79, "training_loss": 547.0453453063965, "training_acc": 72.0, "val_loss": 187.47164011001587, "val_acc": 72.0}
{"epoch": 80, "training_loss": 670.2597789764404, "training_acc": 72.0, "val_loss": 42.416343092918396, "val_acc": 72.0}
{"epoch": 81, "training_loss": 341.2413101196289, "training_acc": 62.0, "val_loss": 119.75914239883423, "val_acc": 28.0}
{"epoch": 82, "training_loss": 434.35576343536377, "training_acc": 46.0, "val_loss": 172.38116264343262, "val_acc": 72.0}
{"epoch": 83, "training_loss": 693.7132034301758, "training_acc": 72.0, "val_loss": 113.00863027572632, "val_acc": 72.0}
