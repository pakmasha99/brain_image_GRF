"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 4201.723384857178, "training_acc": 72.0, "val_loss": 627.5899410247803, "val_acc": 72.0}
{"epoch": 1, "training_loss": 6815.615173339844, "training_acc": 52.0, "val_loss": 2553.5837173461914, "val_acc": 28.0}
{"epoch": 2, "training_loss": 6576.586330413818, "training_acc": 28.0, "val_loss": 1305.2360534667969, "val_acc": 72.0}
{"epoch": 3, "training_loss": 6108.334442138672, "training_acc": 72.0, "val_loss": 2773.283576965332, "val_acc": 72.0}
{"epoch": 4, "training_loss": 11174.300231933594, "training_acc": 72.0, "val_loss": 3092.3315048217773, "val_acc": 72.0}
{"epoch": 5, "training_loss": 11938.731079101562, "training_acc": 72.0, "val_loss": 2577.6342391967773, "val_acc": 72.0}
{"epoch": 6, "training_loss": 9313.347747802734, "training_acc": 72.0, "val_loss": 1420.486068725586, "val_acc": 72.0}
{"epoch": 7, "training_loss": 4368.296058654785, "training_acc": 72.0, "val_loss": 707.8790187835693, "val_acc": 28.0}
{"epoch": 8, "training_loss": 3934.021926879883, "training_acc": 28.0, "val_loss": 1035.254955291748, "val_acc": 28.0}
{"epoch": 9, "training_loss": 2629.6619663238525, "training_acc": 48.0, "val_loss": 494.6547508239746, "val_acc": 72.0}
{"epoch": 10, "training_loss": 2058.0424728393555, "training_acc": 72.0, "val_loss": 638.7325286865234, "val_acc": 72.0}
{"epoch": 11, "training_loss": 2298.5006561279297, "training_acc": 72.0, "val_loss": 190.4234766960144, "val_acc": 72.0}
{"epoch": 12, "training_loss": 848.5324249267578, "training_acc": 68.0, "val_loss": 510.68716049194336, "val_acc": 28.0}
{"epoch": 13, "training_loss": 1514.0553226470947, "training_acc": 44.0, "val_loss": 356.9265604019165, "val_acc": 72.0}
{"epoch": 14, "training_loss": 1403.898307800293, "training_acc": 72.0, "val_loss": 217.27194786071777, "val_acc": 72.0}
{"epoch": 15, "training_loss": 830.6432132720947, "training_acc": 54.0, "val_loss": 64.67171907424927, "val_acc": 72.0}
{"epoch": 16, "training_loss": 265.538685798645, "training_acc": 50.0, "val_loss": 234.20507907867432, "val_acc": 72.0}
{"epoch": 17, "training_loss": 1038.3067665100098, "training_acc": 72.0, "val_loss": 219.22054290771484, "val_acc": 72.0}
{"epoch": 18, "training_loss": 595.0202584266663, "training_acc": 58.0, "val_loss": 43.47633123397827, "val_acc": 68.0}
{"epoch": 19, "training_loss": 320.60215950012207, "training_acc": 54.0, "val_loss": 239.6409034729004, "val_acc": 72.0}
{"epoch": 20, "training_loss": 1137.9998207092285, "training_acc": 72.0, "val_loss": 324.7506618499756, "val_acc": 72.0}
{"epoch": 21, "training_loss": 998.8629837036133, "training_acc": 72.0, "val_loss": 482.5166702270508, "val_acc": 28.0}
{"epoch": 22, "training_loss": 1530.379098892212, "training_acc": 28.0, "val_loss": 347.0311164855957, "val_acc": 72.0}
{"epoch": 23, "training_loss": 1541.7127151489258, "training_acc": 72.0, "val_loss": 667.2396183013916, "val_acc": 72.0}
{"epoch": 24, "training_loss": 2548.9768447875977, "training_acc": 72.0, "val_loss": 411.3428592681885, "val_acc": 72.0}
{"epoch": 25, "training_loss": 1302.795273065567, "training_acc": 72.0, "val_loss": 923.9829063415527, "val_acc": 28.0}
{"epoch": 26, "training_loss": 3513.349220275879, "training_acc": 28.0, "val_loss": 27.94192135334015, "val_acc": 72.0}
{"epoch": 27, "training_loss": 380.9325065612793, "training_acc": 71.0, "val_loss": 352.6790380477905, "val_acc": 72.0}
{"epoch": 28, "training_loss": 1281.7171249389648, "training_acc": 72.0, "val_loss": 71.19912505149841, "val_acc": 68.0}
{"epoch": 29, "training_loss": 1135.2024459838867, "training_acc": 56.0, "val_loss": 486.39307022094727, "val_acc": 28.0}
{"epoch": 30, "training_loss": 1675.7037467956543, "training_acc": 40.0, "val_loss": 482.3117256164551, "val_acc": 72.0}
{"epoch": 31, "training_loss": 1997.2900161743164, "training_acc": 72.0, "val_loss": 445.6876754760742, "val_acc": 72.0}
{"epoch": 32, "training_loss": 1407.0108604431152, "training_acc": 72.0, "val_loss": 401.1294364929199, "val_acc": 28.0}
{"epoch": 33, "training_loss": 1430.1475448608398, "training_acc": 28.0, "val_loss": 286.965274810791, "val_acc": 72.0}
{"epoch": 34, "training_loss": 1408.5732116699219, "training_acc": 72.0, "val_loss": 589.1288757324219, "val_acc": 72.0}
{"epoch": 35, "training_loss": 2204.670478820801, "training_acc": 72.0, "val_loss": 286.11717224121094, "val_acc": 72.0}
{"epoch": 36, "training_loss": 1411.83939743042, "training_acc": 46.0, "val_loss": 99.00352954864502, "val_acc": 28.0}
{"epoch": 37, "training_loss": 580.4443740844727, "training_acc": 49.0, "val_loss": 584.9878311157227, "val_acc": 72.0}
{"epoch": 38, "training_loss": 2353.9704971313477, "training_acc": 72.0, "val_loss": 566.7023658752441, "val_acc": 72.0}
{"epoch": 39, "training_loss": 1951.3675079345703, "training_acc": 72.0, "val_loss": 35.32806038856506, "val_acc": 68.0}
{"epoch": 40, "training_loss": 937.7385482788086, "training_acc": 65.0, "val_loss": 611.2385272979736, "val_acc": 28.0}
{"epoch": 41, "training_loss": 1391.4505829811096, "training_acc": 56.0, "val_loss": 389.08305168151855, "val_acc": 72.0}
{"epoch": 42, "training_loss": 1690.5925598144531, "training_acc": 72.0, "val_loss": 426.6202449798584, "val_acc": 72.0}
{"epoch": 43, "training_loss": 1529.49329662323, "training_acc": 72.0, "val_loss": 306.5378427505493, "val_acc": 28.0}
{"epoch": 44, "training_loss": 807.2606883049011, "training_acc": 38.0, "val_loss": 222.60184288024902, "val_acc": 72.0}
{"epoch": 45, "training_loss": 965.6495971679688, "training_acc": 72.0, "val_loss": 192.54025220870972, "val_acc": 72.0}
