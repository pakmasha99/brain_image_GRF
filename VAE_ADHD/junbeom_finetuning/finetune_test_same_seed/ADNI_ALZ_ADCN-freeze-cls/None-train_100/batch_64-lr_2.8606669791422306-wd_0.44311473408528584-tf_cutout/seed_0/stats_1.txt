"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 12357.841079711914, "training_acc": 72.0, "val_loss": 10379.974365234375, "val_acc": 28.0}
{"epoch": 1, "training_loss": 32928.852600097656, "training_acc": 46.0, "val_loss": 419.4559097290039, "val_acc": 72.0}
{"epoch": 2, "training_loss": 1589.179256439209, "training_acc": 56.0, "val_loss": 2896.8910217285156, "val_acc": 72.0}
{"epoch": 3, "training_loss": 7492.823547363281, "training_acc": 72.0, "val_loss": 620.1209545135498, "val_acc": 28.0}
{"epoch": 4, "training_loss": 4729.734802246094, "training_acc": 44.0, "val_loss": 474.69401359558105, "val_acc": 72.0}
{"epoch": 5, "training_loss": 1286.217628479004, "training_acc": 72.0, "val_loss": 2063.0416870117188, "val_acc": 28.0}
{"epoch": 6, "training_loss": 8390.031707763672, "training_acc": 38.0, "val_loss": 374.5151996612549, "val_acc": 28.0}
{"epoch": 7, "training_loss": 3958.8348999023438, "training_acc": 42.0, "val_loss": 472.9794979095459, "val_acc": 72.0}
{"epoch": 8, "training_loss": 1419.940987110138, "training_acc": 72.0, "val_loss": 878.0237197875977, "val_acc": 28.0}
{"epoch": 9, "training_loss": 3331.816650390625, "training_acc": 50.0, "val_loss": 290.49978256225586, "val_acc": 72.0}
{"epoch": 10, "training_loss": 1372.8258056640625, "training_acc": 54.0, "val_loss": 1496.58784866333, "val_acc": 72.0}
{"epoch": 11, "training_loss": 4123.3276290893555, "training_acc": 72.0, "val_loss": 155.05095720291138, "val_acc": 28.0}
{"epoch": 12, "training_loss": 1996.0015258789062, "training_acc": 48.0, "val_loss": 447.7583408355713, "val_acc": 72.0}
{"epoch": 13, "training_loss": 1246.7861833572388, "training_acc": 72.0, "val_loss": 1047.0584869384766, "val_acc": 28.0}
{"epoch": 14, "training_loss": 4343.439743041992, "training_acc": 44.0, "val_loss": 102.84284353256226, "val_acc": 72.0}
{"epoch": 15, "training_loss": 464.98850440979004, "training_acc": 64.0, "val_loss": 1159.7260475158691, "val_acc": 72.0}
{"epoch": 16, "training_loss": 3440.6215105056763, "training_acc": 72.0, "val_loss": 219.48766708374023, "val_acc": 28.0}
{"epoch": 17, "training_loss": 2019.4883575439453, "training_acc": 48.0, "val_loss": 377.29923725128174, "val_acc": 72.0}
{"epoch": 18, "training_loss": 1487.6549377441406, "training_acc": 50.0, "val_loss": 1558.556842803955, "val_acc": 72.0}
{"epoch": 19, "training_loss": 4326.640655517578, "training_acc": 72.0, "val_loss": 23.833632469177246, "val_acc": 72.0}
{"epoch": 20, "training_loss": 1283.9361190795898, "training_acc": 56.0, "val_loss": 1166.6752815246582, "val_acc": 72.0}
{"epoch": 21, "training_loss": 3087.637414932251, "training_acc": 72.0, "val_loss": 512.5367164611816, "val_acc": 28.0}
{"epoch": 22, "training_loss": 4176.626739501953, "training_acc": 36.0, "val_loss": 96.78636193275452, "val_acc": 72.0}
{"epoch": 23, "training_loss": 611.9394645690918, "training_acc": 58.0, "val_loss": 1249.0788459777832, "val_acc": 72.0}
{"epoch": 24, "training_loss": 3671.3877906799316, "training_acc": 72.0, "val_loss": 76.81124806404114, "val_acc": 28.0}
{"epoch": 25, "training_loss": 1657.9500732421875, "training_acc": 48.0, "val_loss": 397.5198268890381, "val_acc": 72.0}
{"epoch": 26, "training_loss": 1070.5761528015137, "training_acc": 72.0, "val_loss": 1222.4841117858887, "val_acc": 28.0}
{"epoch": 27, "training_loss": 4918.144607543945, "training_acc": 42.0, "val_loss": 15.842315554618835, "val_acc": 72.0}
{"epoch": 28, "training_loss": 222.28225898742676, "training_acc": 72.0, "val_loss": 1444.9604988098145, "val_acc": 28.0}
{"epoch": 29, "training_loss": 4892.125549316406, "training_acc": 46.0, "val_loss": 27.524995803833008, "val_acc": 72.0}
{"epoch": 30, "training_loss": 1261.2107849121094, "training_acc": 52.0, "val_loss": 1274.45650100708, "val_acc": 72.0}
{"epoch": 31, "training_loss": 4394.6011662483215, "training_acc": 72.0, "val_loss": 133.3048939704895, "val_acc": 72.0}
{"epoch": 32, "training_loss": 1973.1999206542969, "training_acc": 56.0, "val_loss": 1109.8405838012695, "val_acc": 72.0}
{"epoch": 33, "training_loss": 3193.06187748909, "training_acc": 72.0, "val_loss": 115.05874395370483, "val_acc": 72.0}
{"epoch": 34, "training_loss": 2000.2806549072266, "training_acc": 56.0, "val_loss": 1100.2272605895996, "val_acc": 72.0}
{"epoch": 35, "training_loss": 3429.244291305542, "training_acc": 50.0, "val_loss": 1716.3890838623047, "val_acc": 72.0}
{"epoch": 36, "training_loss": 5345.211578369141, "training_acc": 72.0, "val_loss": 283.9268684387207, "val_acc": 72.0}
{"epoch": 37, "training_loss": 1552.6080017089844, "training_acc": 56.0, "val_loss": 1277.6419639587402, "val_acc": 72.0}
{"epoch": 38, "training_loss": 4262.186309814453, "training_acc": 72.0, "val_loss": 102.72994041442871, "val_acc": 72.0}
{"epoch": 39, "training_loss": 1920.0279693603516, "training_acc": 56.0, "val_loss": 1097.7954864501953, "val_acc": 72.0}
{"epoch": 40, "training_loss": 3160.6142842769623, "training_acc": 72.0, "val_loss": 576.9319534301758, "val_acc": 72.0}
{"epoch": 41, "training_loss": 3286.001983642578, "training_acc": 46.0, "val_loss": 1547.6149559020996, "val_acc": 72.0}
{"epoch": 42, "training_loss": 4659.160469055176, "training_acc": 72.0, "val_loss": 129.35246229171753, "val_acc": 72.0}
{"epoch": 43, "training_loss": 1679.2385711669922, "training_acc": 56.0, "val_loss": 1153.3571243286133, "val_acc": 72.0}
{"epoch": 44, "training_loss": 3366.082549571991, "training_acc": 72.0, "val_loss": 260.22722721099854, "val_acc": 28.0}
{"epoch": 45, "training_loss": 3009.5318298339844, "training_acc": 40.0, "val_loss": 209.3083381652832, "val_acc": 72.0}
{"epoch": 46, "training_loss": 648.2757606506348, "training_acc": 58.0, "val_loss": 1312.760829925537, "val_acc": 72.0}
