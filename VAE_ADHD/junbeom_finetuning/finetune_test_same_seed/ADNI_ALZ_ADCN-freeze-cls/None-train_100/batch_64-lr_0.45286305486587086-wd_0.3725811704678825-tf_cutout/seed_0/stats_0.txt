"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 2177.807601928711, "training_acc": 42.0, "val_loss": 938.9552116394043, "val_acc": 72.0}
{"epoch": 1, "training_loss": 2480.6191215515137, "training_acc": 72.0, "val_loss": 1761.8022918701172, "val_acc": 28.0}
{"epoch": 2, "training_loss": 6166.39567565918, "training_acc": 28.0, "val_loss": 145.8790898323059, "val_acc": 72.0}
{"epoch": 3, "training_loss": 977.4660339355469, "training_acc": 72.0, "val_loss": 460.8998775482178, "val_acc": 72.0}
{"epoch": 4, "training_loss": 1655.8504028320312, "training_acc": 72.0, "val_loss": 96.04727625846863, "val_acc": 72.0}
{"epoch": 5, "training_loss": 1364.8079452514648, "training_acc": 48.0, "val_loss": 300.0591278076172, "val_acc": 28.0}
{"epoch": 6, "training_loss": 1116.581314086914, "training_acc": 44.0, "val_loss": 406.12049102783203, "val_acc": 72.0}
{"epoch": 7, "training_loss": 1615.2371940612793, "training_acc": 72.0, "val_loss": 313.36512565612793, "val_acc": 72.0}
{"epoch": 8, "training_loss": 993.0082664489746, "training_acc": 72.0, "val_loss": 228.4576177597046, "val_acc": 28.0}
{"epoch": 9, "training_loss": 641.6324949264526, "training_acc": 28.0, "val_loss": 274.2624521255493, "val_acc": 72.0}
{"epoch": 10, "training_loss": 1277.158863067627, "training_acc": 72.0, "val_loss": 398.4525680541992, "val_acc": 72.0}
{"epoch": 11, "training_loss": 1449.5149116516113, "training_acc": 72.0, "val_loss": 126.2014627456665, "val_acc": 72.0}
{"epoch": 12, "training_loss": 737.9220085144043, "training_acc": 54.0, "val_loss": 69.7905957698822, "val_acc": 28.0}
{"epoch": 13, "training_loss": 356.1538543701172, "training_acc": 52.0, "val_loss": 360.1691722869873, "val_acc": 72.0}
{"epoch": 14, "training_loss": 1470.1772003173828, "training_acc": 72.0, "val_loss": 279.81743812561035, "val_acc": 72.0}
{"epoch": 15, "training_loss": 848.7821083068848, "training_acc": 72.0, "val_loss": 266.94653034210205, "val_acc": 28.0}
{"epoch": 16, "training_loss": 829.8426790237427, "training_acc": 28.0, "val_loss": 216.67795181274414, "val_acc": 72.0}
{"epoch": 17, "training_loss": 951.8516159057617, "training_acc": 72.0, "val_loss": 302.5296926498413, "val_acc": 72.0}
{"epoch": 18, "training_loss": 1066.8921661376953, "training_acc": 72.0, "val_loss": 52.48941779136658, "val_acc": 72.0}
{"epoch": 19, "training_loss": 904.0766372680664, "training_acc": 50.0, "val_loss": 262.85388469696045, "val_acc": 28.0}
{"epoch": 20, "training_loss": 869.6638164520264, "training_acc": 46.0, "val_loss": 290.01381397247314, "val_acc": 72.0}
{"epoch": 21, "training_loss": 1194.7203369140625, "training_acc": 72.0, "val_loss": 209.08632278442383, "val_acc": 72.0}
{"epoch": 22, "training_loss": 601.9021244049072, "training_acc": 72.0, "val_loss": 419.52877044677734, "val_acc": 28.0}
{"epoch": 23, "training_loss": 1442.243366241455, "training_acc": 28.0, "val_loss": 166.0573959350586, "val_acc": 72.0}
{"epoch": 24, "training_loss": 829.0322418212891, "training_acc": 72.0, "val_loss": 281.49468898773193, "val_acc": 72.0}
{"epoch": 25, "training_loss": 992.7605018615723, "training_acc": 72.0, "val_loss": 38.30086290836334, "val_acc": 72.0}
{"epoch": 26, "training_loss": 761.7242202758789, "training_acc": 54.0, "val_loss": 249.5603322982788, "val_acc": 28.0}
{"epoch": 27, "training_loss": 881.1898670196533, "training_acc": 44.0, "val_loss": 286.79683208465576, "val_acc": 72.0}
{"epoch": 28, "training_loss": 1139.9023628234863, "training_acc": 72.0, "val_loss": 198.2298493385315, "val_acc": 72.0}
{"epoch": 29, "training_loss": 577.7934947013855, "training_acc": 72.0, "val_loss": 384.3520402908325, "val_acc": 28.0}
{"epoch": 30, "training_loss": 1298.9962882995605, "training_acc": 28.0, "val_loss": 188.92648220062256, "val_acc": 72.0}
{"epoch": 31, "training_loss": 1042.8679428100586, "training_acc": 72.0, "val_loss": 319.83532905578613, "val_acc": 72.0}
{"epoch": 32, "training_loss": 1155.4649391174316, "training_acc": 72.0, "val_loss": 60.9793484210968, "val_acc": 72.0}
{"epoch": 33, "training_loss": 763.0801773071289, "training_acc": 52.0, "val_loss": 173.5862374305725, "val_acc": 28.0}
{"epoch": 34, "training_loss": 826.3743476867676, "training_acc": 40.0, "val_loss": 320.4490900039673, "val_acc": 72.0}
{"epoch": 35, "training_loss": 1271.8264846801758, "training_acc": 72.0, "val_loss": 221.82245254516602, "val_acc": 72.0}
{"epoch": 36, "training_loss": 679.7523584365845, "training_acc": 72.0, "val_loss": 349.54748153686523, "val_acc": 28.0}
{"epoch": 37, "training_loss": 1122.3384799957275, "training_acc": 28.0, "val_loss": 196.92914485931396, "val_acc": 72.0}
{"epoch": 38, "training_loss": 948.9163284301758, "training_acc": 72.0, "val_loss": 281.70549869537354, "val_acc": 72.0}
{"epoch": 39, "training_loss": 960.3634796142578, "training_acc": 72.0, "val_loss": 19.20211762189865, "val_acc": 72.0}
{"epoch": 40, "training_loss": 630.1968307495117, "training_acc": 60.0, "val_loss": 280.96280097961426, "val_acc": 28.0}
{"epoch": 41, "training_loss": 976.25608253479, "training_acc": 42.0, "val_loss": 259.8634958267212, "val_acc": 72.0}
{"epoch": 42, "training_loss": 1022.1314582824707, "training_acc": 72.0, "val_loss": 158.49707126617432, "val_acc": 72.0}
{"epoch": 43, "training_loss": 483.76150822639465, "training_acc": 52.0, "val_loss": 21.80078774690628, "val_acc": 28.0}
{"epoch": 44, "training_loss": 207.80649757385254, "training_acc": 44.0, "val_loss": 104.14026975631714, "val_acc": 72.0}
{"epoch": 45, "training_loss": 287.0088312625885, "training_acc": 72.0, "val_loss": 308.46989154815674, "val_acc": 28.0}
{"epoch": 46, "training_loss": 819.7093191146851, "training_acc": 28.0, "val_loss": 265.21618366241455, "val_acc": 72.0}
{"epoch": 47, "training_loss": 1285.8507461547852, "training_acc": 72.0, "val_loss": 388.89763355255127, "val_acc": 72.0}
{"epoch": 48, "training_loss": 1433.1413612365723, "training_acc": 72.0, "val_loss": 130.58730363845825, "val_acc": 72.0}
{"epoch": 49, "training_loss": 645.7218208312988, "training_acc": 54.0, "val_loss": 18.411093950271606, "val_acc": 28.0}
{"epoch": 50, "training_loss": 191.01015663146973, "training_acc": 48.0, "val_loss": 152.6999831199646, "val_acc": 72.0}
{"epoch": 51, "training_loss": 516.8556642532349, "training_acc": 72.0, "val_loss": 134.05028581619263, "val_acc": 28.0}
{"epoch": 52, "training_loss": 439.60682582855225, "training_acc": 44.0, "val_loss": 72.8479266166687, "val_acc": 72.0}
{"epoch": 53, "training_loss": 320.35014247894287, "training_acc": 50.0, "val_loss": 137.55837678909302, "val_acc": 72.0}
{"epoch": 54, "training_loss": 599.0189762115479, "training_acc": 72.0, "val_loss": 110.77824831008911, "val_acc": 72.0}
{"epoch": 55, "training_loss": 443.3849515914917, "training_acc": 52.0, "val_loss": 94.03416514396667, "val_acc": 72.0}
{"epoch": 56, "training_loss": 374.2156095504761, "training_acc": 72.0, "val_loss": 23.657982051372528, "val_acc": 72.0}
{"epoch": 57, "training_loss": 521.2757606506348, "training_acc": 52.0, "val_loss": 22.59158045053482, "val_acc": 72.0}
{"epoch": 58, "training_loss": 185.18284225463867, "training_acc": 72.0, "val_loss": 36.029601097106934, "val_acc": 28.0}
{"epoch": 59, "training_loss": 213.10542106628418, "training_acc": 46.0, "val_loss": 99.29748177528381, "val_acc": 72.0}
{"epoch": 60, "training_loss": 302.8730854988098, "training_acc": 72.0, "val_loss": 105.10444641113281, "val_acc": 28.0}
{"epoch": 61, "training_loss": 438.56109619140625, "training_acc": 46.0, "val_loss": 192.24867820739746, "val_acc": 72.0}
{"epoch": 62, "training_loss": 701.8505325317383, "training_acc": 72.0, "val_loss": 23.37714284658432, "val_acc": 72.0}
{"epoch": 63, "training_loss": 451.45418548583984, "training_acc": 64.0, "val_loss": 134.94229316711426, "val_acc": 28.0}
{"epoch": 64, "training_loss": 662.8194770812988, "training_acc": 44.0, "val_loss": 340.84925651550293, "val_acc": 72.0}
{"epoch": 65, "training_loss": 1395.9274291992188, "training_acc": 72.0, "val_loss": 245.36383152008057, "val_acc": 72.0}
{"epoch": 66, "training_loss": 792.7266039848328, "training_acc": 72.0, "val_loss": 371.96526527404785, "val_acc": 28.0}
{"epoch": 67, "training_loss": 1239.5924644470215, "training_acc": 28.0, "val_loss": 200.187349319458, "val_acc": 72.0}
{"epoch": 68, "training_loss": 889.7476768493652, "training_acc": 72.0, "val_loss": 327.58965492248535, "val_acc": 72.0}
