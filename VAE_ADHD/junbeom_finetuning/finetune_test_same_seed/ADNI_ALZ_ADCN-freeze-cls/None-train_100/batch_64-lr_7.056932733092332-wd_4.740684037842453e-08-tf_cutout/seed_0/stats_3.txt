"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 21231.812133789062, "training_acc": 50.0, "val_loss": 18928.34930419922, "val_acc": 72.0}
{"epoch": 1, "training_loss": 65579.97192382812, "training_acc": 72.0, "val_loss": 16365.040588378906, "val_acc": 28.0}
{"epoch": 2, "training_loss": 52238.595458984375, "training_acc": 28.0, "val_loss": 6684.0576171875, "val_acc": 72.0}
{"epoch": 3, "training_loss": 41357.660888671875, "training_acc": 72.0, "val_loss": 14496.635437011719, "val_acc": 72.0}
{"epoch": 4, "training_loss": 56410.998291015625, "training_acc": 72.0, "val_loss": 10201.45492553711, "val_acc": 72.0}
{"epoch": 5, "training_loss": 34257.16436767578, "training_acc": 72.0, "val_loss": 3342.9588317871094, "val_acc": 28.0}
{"epoch": 6, "training_loss": 14732.343933105469, "training_acc": 28.0, "val_loss": 2626.7162322998047, "val_acc": 72.0}
{"epoch": 7, "training_loss": 11387.082550048828, "training_acc": 72.0, "val_loss": 4897.55744934082, "val_acc": 72.0}
{"epoch": 8, "training_loss": 18119.603637695312, "training_acc": 72.0, "val_loss": 1314.0209197998047, "val_acc": 72.0}
{"epoch": 9, "training_loss": 13494.390014648438, "training_acc": 56.0, "val_loss": 5466.143417358398, "val_acc": 28.0}
{"epoch": 10, "training_loss": 16351.138580322266, "training_acc": 46.0, "val_loss": 4585.1318359375, "val_acc": 72.0}
{"epoch": 11, "training_loss": 19656.36328125, "training_acc": 72.0, "val_loss": 4556.302642822266, "val_acc": 72.0}
{"epoch": 12, "training_loss": 14954.11767578125, "training_acc": 72.0, "val_loss": 3275.9769439697266, "val_acc": 28.0}
{"epoch": 13, "training_loss": 10415.24853515625, "training_acc": 28.0, "val_loss": 3506.5223693847656, "val_acc": 72.0}
{"epoch": 14, "training_loss": 15647.036376953125, "training_acc": 72.0, "val_loss": 6288.542938232422, "val_acc": 72.0}
{"epoch": 15, "training_loss": 24258.83465576172, "training_acc": 72.0, "val_loss": 3696.018600463867, "val_acc": 72.0}
{"epoch": 16, "training_loss": 11748.168942451477, "training_acc": 72.0, "val_loss": 10009.251403808594, "val_acc": 28.0}
{"epoch": 17, "training_loss": 39155.22033691406, "training_acc": 28.0, "val_loss": 670.2739238739014, "val_acc": 28.0}
{"epoch": 18, "training_loss": 9435.1171875, "training_acc": 46.0, "val_loss": 10557.244110107422, "val_acc": 72.0}
{"epoch": 19, "training_loss": 46119.566650390625, "training_acc": 72.0, "val_loss": 14415.167236328125, "val_acc": 72.0}
{"epoch": 20, "training_loss": 57152.571533203125, "training_acc": 72.0, "val_loss": 12367.394256591797, "val_acc": 72.0}
{"epoch": 21, "training_loss": 44717.488037109375, "training_acc": 72.0, "val_loss": 5437.556838989258, "val_acc": 72.0}
{"epoch": 22, "training_loss": 13731.379287719727, "training_acc": 72.0, "val_loss": 14895.176696777344, "val_acc": 28.0}
{"epoch": 23, "training_loss": 66580.37182617188, "training_acc": 28.0, "val_loss": 14996.511840820312, "val_acc": 28.0}
{"epoch": 24, "training_loss": 40410.55679321289, "training_acc": 28.0, "val_loss": 6996.513366699219, "val_acc": 72.0}
{"epoch": 25, "training_loss": 35416.663818359375, "training_acc": 72.0, "val_loss": 16861.952209472656, "val_acc": 72.0}
{"epoch": 26, "training_loss": 72221.35595703125, "training_acc": 72.0, "val_loss": 20046.800231933594, "val_acc": 72.0}
{"epoch": 27, "training_loss": 79339.02124023438, "training_acc": 72.0, "val_loss": 17155.833435058594, "val_acc": 72.0}
{"epoch": 28, "training_loss": 65115.14404296875, "training_acc": 72.0, "val_loss": 10226.513671875, "val_acc": 72.0}
{"epoch": 29, "training_loss": 35160.048095703125, "training_acc": 72.0, "val_loss": 1040.8002853393555, "val_acc": 28.0}
{"epoch": 30, "training_loss": 10302.431091308594, "training_acc": 28.0, "val_loss": 160.25784015655518, "val_acc": 72.0}
{"epoch": 31, "training_loss": 2170.2772827148438, "training_acc": 72.0, "val_loss": 25.578242540359497, "val_acc": 68.0}
{"epoch": 32, "training_loss": 8294.25180053711, "training_acc": 44.0, "val_loss": 773.3664989471436, "val_acc": 72.0}
{"epoch": 33, "training_loss": 4769.565155029297, "training_acc": 72.0, "val_loss": 1373.3001708984375, "val_acc": 72.0}
{"epoch": 34, "training_loss": 8501.245635986328, "training_acc": 44.0, "val_loss": 1347.3222732543945, "val_acc": 72.0}
{"epoch": 35, "training_loss": 6520.250885009766, "training_acc": 72.0, "val_loss": 1316.8785095214844, "val_acc": 72.0}
{"epoch": 36, "training_loss": 5965.976623535156, "training_acc": 56.0, "val_loss": 867.0245170593262, "val_acc": 72.0}
{"epoch": 37, "training_loss": 3468.5131607055664, "training_acc": 72.0, "val_loss": 609.2272758483887, "val_acc": 28.0}
{"epoch": 38, "training_loss": 6912.112731933594, "training_acc": 34.0, "val_loss": 2915.4558181762695, "val_acc": 72.0}
{"epoch": 39, "training_loss": 10375.70327758789, "training_acc": 72.0, "val_loss": 76.12932324409485, "val_acc": 32.0}
{"epoch": 40, "training_loss": 804.5316467285156, "training_acc": 56.0, "val_loss": 518.9494132995605, "val_acc": 28.0}
{"epoch": 41, "training_loss": 6479.503173828125, "training_acc": 38.0, "val_loss": 3683.2870483398438, "val_acc": 72.0}
{"epoch": 42, "training_loss": 14185.636779785156, "training_acc": 72.0, "val_loss": 1560.0642204284668, "val_acc": 72.0}
{"epoch": 43, "training_loss": 9848.068786621094, "training_acc": 54.0, "val_loss": 1771.8393325805664, "val_acc": 28.0}
{"epoch": 44, "training_loss": 10624.986755371094, "training_acc": 40.0, "val_loss": 5653.460311889648, "val_acc": 72.0}
{"epoch": 45, "training_loss": 23213.258422851562, "training_acc": 72.0, "val_loss": 5654.474639892578, "val_acc": 72.0}
{"epoch": 46, "training_loss": 20165.33184814453, "training_acc": 72.0, "val_loss": 908.0243110656738, "val_acc": 72.0}
{"epoch": 47, "training_loss": 13366.235595703125, "training_acc": 56.0, "val_loss": 7983.928680419922, "val_acc": 28.0}
{"epoch": 48, "training_loss": 20513.348927497864, "training_acc": 44.0, "val_loss": 1721.0853576660156, "val_acc": 72.0}
{"epoch": 49, "training_loss": 7074.68212890625, "training_acc": 72.0, "val_loss": 719.5441246032715, "val_acc": 72.0}
{"epoch": 50, "training_loss": 9114.570678710938, "training_acc": 52.0, "val_loss": 1006.208610534668, "val_acc": 28.0}
