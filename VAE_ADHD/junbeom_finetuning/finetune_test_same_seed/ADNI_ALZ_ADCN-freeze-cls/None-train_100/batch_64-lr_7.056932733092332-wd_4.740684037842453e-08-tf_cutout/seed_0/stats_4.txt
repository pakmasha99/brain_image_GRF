"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 24231.487274169922, "training_acc": 48.0, "val_loss": 19636.17401123047, "val_acc": 72.0}
{"epoch": 1, "training_loss": 69235.01293945312, "training_acc": 72.0, "val_loss": 9727.253723144531, "val_acc": 28.0}
{"epoch": 2, "training_loss": 26153.378692626953, "training_acc": 28.0, "val_loss": 8892.711639404297, "val_acc": 72.0}
{"epoch": 3, "training_loss": 42587.235107421875, "training_acc": 72.0, "val_loss": 16904.708862304688, "val_acc": 72.0}
{"epoch": 4, "training_loss": 67072.98779296875, "training_acc": 72.0, "val_loss": 14392.781066894531, "val_acc": 72.0}
{"epoch": 5, "training_loss": 52059.390380859375, "training_acc": 72.0, "val_loss": 5039.365005493164, "val_acc": 72.0}
{"epoch": 6, "training_loss": 17389.733459472656, "training_acc": 60.0, "val_loss": 6882.074737548828, "val_acc": 28.0}
{"epoch": 7, "training_loss": 18945.702362060547, "training_acc": 44.0, "val_loss": 2502.9239654541016, "val_acc": 72.0}
{"epoch": 8, "training_loss": 9788.123596191406, "training_acc": 72.0, "val_loss": 248.5541582107544, "val_acc": 72.0}
{"epoch": 9, "training_loss": 9876.268829345703, "training_acc": 64.0, "val_loss": 5763.372039794922, "val_acc": 28.0}
{"epoch": 10, "training_loss": 16550.519104003906, "training_acc": 48.0, "val_loss": 4899.849319458008, "val_acc": 72.0}
{"epoch": 11, "training_loss": 20933.985229492188, "training_acc": 72.0, "val_loss": 5142.927551269531, "val_acc": 72.0}
{"epoch": 12, "training_loss": 17652.48159790039, "training_acc": 72.0, "val_loss": 1401.207447052002, "val_acc": 28.0}
{"epoch": 13, "training_loss": 3608.6696434020996, "training_acc": 48.0, "val_loss": 1041.534423828125, "val_acc": 28.0}
{"epoch": 14, "training_loss": 3669.651168823242, "training_acc": 56.0, "val_loss": 4303.944396972656, "val_acc": 72.0}
{"epoch": 15, "training_loss": 17751.136169433594, "training_acc": 72.0, "val_loss": 3329.954147338867, "val_acc": 72.0}
{"epoch": 16, "training_loss": 9201.949935913086, "training_acc": 72.0, "val_loss": 8448.455810546875, "val_acc": 28.0}
{"epoch": 17, "training_loss": 32398.783813476562, "training_acc": 28.0, "val_loss": 856.9589614868164, "val_acc": 72.0}
{"epoch": 18, "training_loss": 6212.234680175781, "training_acc": 72.0, "val_loss": 3155.5654525756836, "val_acc": 72.0}
{"epoch": 19, "training_loss": 10955.865753173828, "training_acc": 72.0, "val_loss": 1082.0802688598633, "val_acc": 28.0}
{"epoch": 20, "training_loss": 5545.703125, "training_acc": 36.0, "val_loss": 857.7467918395996, "val_acc": 72.0}
{"epoch": 21, "training_loss": 4092.255813598633, "training_acc": 62.0, "val_loss": 1097.3844528198242, "val_acc": 72.0}
{"epoch": 22, "training_loss": 4566.999954223633, "training_acc": 72.0, "val_loss": 112.41521835327148, "val_acc": 72.0}
{"epoch": 23, "training_loss": 10890.433410644531, "training_acc": 50.0, "val_loss": 2396.65470123291, "val_acc": 28.0}
{"epoch": 24, "training_loss": 10965.079345703125, "training_acc": 46.0, "val_loss": 7022.0733642578125, "val_acc": 72.0}
{"epoch": 25, "training_loss": 31264.009399414062, "training_acc": 72.0, "val_loss": 8505.860900878906, "val_acc": 72.0}
{"epoch": 26, "training_loss": 31869.77911376953, "training_acc": 72.0, "val_loss": 4092.6036834716797, "val_acc": 72.0}
{"epoch": 27, "training_loss": 11744.377168178558, "training_acc": 72.0, "val_loss": 12055.685424804688, "val_acc": 28.0}
{"epoch": 28, "training_loss": 52040.77392578125, "training_acc": 28.0, "val_loss": 6450.502777099609, "val_acc": 28.0}
{"epoch": 29, "training_loss": 20729.574157714844, "training_acc": 46.0, "val_loss": 7880.992126464844, "val_acc": 72.0}
{"epoch": 30, "training_loss": 36141.49670410156, "training_acc": 72.0, "val_loss": 11413.605499267578, "val_acc": 72.0}
{"epoch": 31, "training_loss": 44793.71325683594, "training_acc": 72.0, "val_loss": 8894.509887695312, "val_acc": 72.0}
{"epoch": 32, "training_loss": 32585.554260253906, "training_acc": 72.0, "val_loss": 1464.8420333862305, "val_acc": 72.0}
{"epoch": 33, "training_loss": 18721.739379882812, "training_acc": 52.0, "val_loss": 11280.46646118164, "val_acc": 28.0}
{"epoch": 34, "training_loss": 34843.2080078125, "training_acc": 28.0, "val_loss": 4264.08576965332, "val_acc": 72.0}
{"epoch": 35, "training_loss": 25623.783447265625, "training_acc": 72.0, "val_loss": 10570.15609741211, "val_acc": 72.0}
{"epoch": 36, "training_loss": 42776.689697265625, "training_acc": 72.0, "val_loss": 10124.272155761719, "val_acc": 72.0}
{"epoch": 37, "training_loss": 36960.05065917969, "training_acc": 72.0, "val_loss": 4766.320037841797, "val_acc": 72.0}
{"epoch": 38, "training_loss": 13172.256841659546, "training_acc": 72.0, "val_loss": 13194.837951660156, "val_acc": 28.0}
{"epoch": 39, "training_loss": 57176.4443359375, "training_acc": 28.0, "val_loss": 10557.135772705078, "val_acc": 28.0}
{"epoch": 40, "training_loss": 30617.49853515625, "training_acc": 40.0, "val_loss": 4818.231964111328, "val_acc": 72.0}
{"epoch": 41, "training_loss": 22117.914916992188, "training_acc": 72.0, "val_loss": 6831.645202636719, "val_acc": 72.0}
