"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 17438.97573852539, "training_acc": 42.0, "val_loss": 5504.293441772461, "val_acc": 28.0}
{"epoch": 1, "training_loss": 24618.064819335938, "training_acc": 40.0, "val_loss": 1673.647117614746, "val_acc": 72.0}
{"epoch": 2, "training_loss": 7326.407653808594, "training_acc": 50.0, "val_loss": 3418.7057495117188, "val_acc": 72.0}
{"epoch": 3, "training_loss": 10906.6005859375, "training_acc": 72.0, "val_loss": 200.71179866790771, "val_acc": 72.0}
{"epoch": 4, "training_loss": 3474.5486450195312, "training_acc": 58.0, "val_loss": 1854.3327331542969, "val_acc": 72.0}
{"epoch": 5, "training_loss": 6081.900308609009, "training_acc": 72.0, "val_loss": 589.6384716033936, "val_acc": 28.0}
{"epoch": 6, "training_loss": 4466.829284667969, "training_acc": 44.0, "val_loss": 871.367073059082, "val_acc": 72.0}
{"epoch": 7, "training_loss": 3322.6877670288086, "training_acc": 50.0, "val_loss": 2154.2741775512695, "val_acc": 72.0}
{"epoch": 8, "training_loss": 6915.530471801758, "training_acc": 72.0, "val_loss": 160.17733812332153, "val_acc": 72.0}
{"epoch": 9, "training_loss": 3468.1039123535156, "training_acc": 50.0, "val_loss": 1528.4120559692383, "val_acc": 72.0}
{"epoch": 10, "training_loss": 4686.4836349487305, "training_acc": 72.0, "val_loss": 513.8676643371582, "val_acc": 28.0}
{"epoch": 11, "training_loss": 3693.1566467285156, "training_acc": 44.0, "val_loss": 707.5191497802734, "val_acc": 72.0}
{"epoch": 12, "training_loss": 2184.6085243225098, "training_acc": 54.0, "val_loss": 1807.0268630981445, "val_acc": 72.0}
{"epoch": 13, "training_loss": 6267.550888061523, "training_acc": 72.0, "val_loss": 266.5266990661621, "val_acc": 72.0}
{"epoch": 14, "training_loss": 1845.7098541259766, "training_acc": 60.0, "val_loss": 1336.9316101074219, "val_acc": 72.0}
{"epoch": 15, "training_loss": 3928.956039428711, "training_acc": 72.0, "val_loss": 828.0526161193848, "val_acc": 28.0}
{"epoch": 16, "training_loss": 4876.939392089844, "training_acc": 40.0, "val_loss": 505.80334663391113, "val_acc": 72.0}
{"epoch": 17, "training_loss": 2707.001663208008, "training_acc": 50.0, "val_loss": 1680.3226470947266, "val_acc": 72.0}
{"epoch": 18, "training_loss": 5207.56803894043, "training_acc": 72.0, "val_loss": 103.78910303115845, "val_acc": 28.0}
{"epoch": 19, "training_loss": 1822.578598022461, "training_acc": 50.0, "val_loss": 883.9993476867676, "val_acc": 72.0}
{"epoch": 20, "training_loss": 2557.6112518310547, "training_acc": 72.0, "val_loss": 1156.9520950317383, "val_acc": 28.0}
{"epoch": 21, "training_loss": 5805.249572753906, "training_acc": 38.0, "val_loss": 342.9494619369507, "val_acc": 72.0}
{"epoch": 22, "training_loss": 2012.6881790161133, "training_acc": 56.0, "val_loss": 1448.4838485717773, "val_acc": 72.0}
{"epoch": 23, "training_loss": 4600.910774230957, "training_acc": 72.0, "val_loss": 193.2431936264038, "val_acc": 28.0}
{"epoch": 24, "training_loss": 2832.4207153320312, "training_acc": 44.0, "val_loss": 740.632438659668, "val_acc": 72.0}
{"epoch": 25, "training_loss": 1929.7144813537598, "training_acc": 72.0, "val_loss": 1588.1914138793945, "val_acc": 28.0}
{"epoch": 26, "training_loss": 5431.011795043945, "training_acc": 46.0, "val_loss": 349.22289848327637, "val_acc": 72.0}
{"epoch": 27, "training_loss": 2065.823959350586, "training_acc": 56.0, "val_loss": 1446.7774391174316, "val_acc": 72.0}
{"epoch": 28, "training_loss": 4590.583587646484, "training_acc": 72.0, "val_loss": 209.0191125869751, "val_acc": 28.0}
{"epoch": 29, "training_loss": 2877.304229736328, "training_acc": 44.0, "val_loss": 737.9438400268555, "val_acc": 72.0}
{"epoch": 30, "training_loss": 2498.074043273926, "training_acc": 50.0, "val_loss": 1852.4900436401367, "val_acc": 72.0}
{"epoch": 31, "training_loss": 5829.768585205078, "training_acc": 72.0, "val_loss": 79.31431531906128, "val_acc": 72.0}
{"epoch": 32, "training_loss": 2695.393829345703, "training_acc": 52.0, "val_loss": 1298.1093406677246, "val_acc": 72.0}
{"epoch": 33, "training_loss": 4100.4958572387695, "training_acc": 72.0, "val_loss": 426.1995792388916, "val_acc": 28.0}
{"epoch": 34, "training_loss": 3913.762664794922, "training_acc": 40.0, "val_loss": 591.715669631958, "val_acc": 72.0}
{"epoch": 35, "training_loss": 1805.1087703704834, "training_acc": 56.0, "val_loss": 1641.1319732666016, "val_acc": 72.0}
{"epoch": 36, "training_loss": 5352.6140060424805, "training_acc": 72.0, "val_loss": 89.8322582244873, "val_acc": 72.0}
{"epoch": 37, "training_loss": 1450.0362701416016, "training_acc": 64.0, "val_loss": 1096.2575912475586, "val_acc": 72.0}
{"epoch": 38, "training_loss": 3112.9739837646484, "training_acc": 72.0, "val_loss": 1092.116928100586, "val_acc": 28.0}
{"epoch": 39, "training_loss": 6002.321350097656, "training_acc": 36.0, "val_loss": 328.1687021255493, "val_acc": 72.0}
{"epoch": 40, "training_loss": 1560.8264465332031, "training_acc": 60.0, "val_loss": 1375.2280235290527, "val_acc": 72.0}
{"epoch": 41, "training_loss": 4177.649635314941, "training_acc": 72.0, "val_loss": 510.53266525268555, "val_acc": 28.0}
{"epoch": 42, "training_loss": 3207.1797637939453, "training_acc": 46.0, "val_loss": 682.107400894165, "val_acc": 72.0}
{"epoch": 43, "training_loss": 2283.0727462768555, "training_acc": 52.0, "val_loss": 1776.1528015136719, "val_acc": 72.0}
{"epoch": 44, "training_loss": 5775.764846801758, "training_acc": 72.0, "val_loss": 153.00112962722778, "val_acc": 72.0}
{"epoch": 45, "training_loss": 1998.2950592041016, "training_acc": 58.0, "val_loss": 1252.7851104736328, "val_acc": 72.0}
{"epoch": 46, "training_loss": 3929.0124130249023, "training_acc": 72.0, "val_loss": 520.4816341400146, "val_acc": 28.0}
{"epoch": 47, "training_loss": 4131.313049316406, "training_acc": 40.0, "val_loss": 567.0408725738525, "val_acc": 72.0}
{"epoch": 48, "training_loss": 2323.930633544922, "training_acc": 52.0, "val_loss": 1694.523811340332, "val_acc": 72.0}
{"epoch": 49, "training_loss": 5544.021392822266, "training_acc": 72.0, "val_loss": 120.14707326889038, "val_acc": 72.0}
{"epoch": 50, "training_loss": 2737.5145874023438, "training_acc": 52.0, "val_loss": 1321.4696884155273, "val_acc": 72.0}
