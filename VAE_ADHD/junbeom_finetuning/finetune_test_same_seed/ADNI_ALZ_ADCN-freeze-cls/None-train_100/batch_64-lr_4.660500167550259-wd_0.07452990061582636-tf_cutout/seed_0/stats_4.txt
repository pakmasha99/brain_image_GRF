"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 15993.91149520874, "training_acc": 48.0, "val_loss": 7859.348297119141, "val_acc": 72.0}
{"epoch": 1, "training_loss": 26303.474487304688, "training_acc": 56.0, "val_loss": 3213.1832122802734, "val_acc": 72.0}
{"epoch": 2, "training_loss": 13127.100646972656, "training_acc": 72.0, "val_loss": 617.6718711853027, "val_acc": 72.0}
{"epoch": 3, "training_loss": 12611.644409179688, "training_acc": 56.0, "val_loss": 491.20373725891113, "val_acc": 28.0}
{"epoch": 4, "training_loss": 9519.414489746094, "training_acc": 40.0, "val_loss": 5847.881317138672, "val_acc": 72.0}
{"epoch": 5, "training_loss": 22163.875732421875, "training_acc": 72.0, "val_loss": 2937.056541442871, "val_acc": 72.0}
{"epoch": 6, "training_loss": 8557.124307632446, "training_acc": 72.0, "val_loss": 6290.3656005859375, "val_acc": 28.0}
{"epoch": 7, "training_loss": 19289.75210571289, "training_acc": 28.0, "val_loss": 2679.7548294067383, "val_acc": 72.0}
{"epoch": 8, "training_loss": 11872.671417236328, "training_acc": 72.0, "val_loss": 3075.661087036133, "val_acc": 72.0}
{"epoch": 9, "training_loss": 10549.435348510742, "training_acc": 72.0, "val_loss": 576.8204689025879, "val_acc": 28.0}
{"epoch": 10, "training_loss": 1838.1364860534668, "training_acc": 56.0, "val_loss": 1261.376953125, "val_acc": 72.0}
{"epoch": 11, "training_loss": 3766.084243774414, "training_acc": 72.0, "val_loss": 2904.560661315918, "val_acc": 28.0}
{"epoch": 12, "training_loss": 8412.14575958252, "training_acc": 42.0, "val_loss": 448.0708122253418, "val_acc": 72.0}
{"epoch": 13, "training_loss": 2623.041519165039, "training_acc": 60.0, "val_loss": 1241.1206245422363, "val_acc": 72.0}
{"epoch": 14, "training_loss": 5007.835662841797, "training_acc": 72.0, "val_loss": 318.52128505706787, "val_acc": 72.0}
{"epoch": 15, "training_loss": 4879.8193359375, "training_acc": 58.0, "val_loss": 334.65611934661865, "val_acc": 72.0}
{"epoch": 16, "training_loss": 1632.268424987793, "training_acc": 72.0, "val_loss": 1880.2902221679688, "val_acc": 28.0}
{"epoch": 17, "training_loss": 7080.274642944336, "training_acc": 38.0, "val_loss": 916.4026260375977, "val_acc": 72.0}
{"epoch": 18, "training_loss": 3071.0801010131836, "training_acc": 54.0, "val_loss": 1594.903564453125, "val_acc": 72.0}
{"epoch": 19, "training_loss": 6323.748977661133, "training_acc": 72.0, "val_loss": 840.0389671325684, "val_acc": 72.0}
{"epoch": 20, "training_loss": 4394.706817626953, "training_acc": 54.0, "val_loss": 1016.9533729553223, "val_acc": 72.0}
{"epoch": 21, "training_loss": 4360.068893432617, "training_acc": 72.0, "val_loss": 88.70765566825867, "val_acc": 72.0}
{"epoch": 22, "training_loss": 6011.311050415039, "training_acc": 54.0, "val_loss": 46.710410714149475, "val_acc": 28.0}
{"epoch": 23, "training_loss": 3232.75, "training_acc": 48.0, "val_loss": 3575.0320434570312, "val_acc": 72.0}
{"epoch": 24, "training_loss": 13659.861907958984, "training_acc": 72.0, "val_loss": 1853.421401977539, "val_acc": 72.0}
{"epoch": 25, "training_loss": 5311.120615005493, "training_acc": 72.0, "val_loss": 3980.826187133789, "val_acc": 28.0}
{"epoch": 26, "training_loss": 11196.49146270752, "training_acc": 28.0, "val_loss": 2688.209342956543, "val_acc": 72.0}
{"epoch": 27, "training_loss": 11975.804077148438, "training_acc": 72.0, "val_loss": 2860.091209411621, "val_acc": 72.0}
{"epoch": 28, "training_loss": 10133.308097839355, "training_acc": 72.0, "val_loss": 349.6415853500366, "val_acc": 28.0}
{"epoch": 29, "training_loss": 2169.955780029297, "training_acc": 48.0, "val_loss": 1179.563808441162, "val_acc": 72.0}
{"epoch": 30, "training_loss": 3425.433864593506, "training_acc": 72.0, "val_loss": 2946.9961166381836, "val_acc": 28.0}
{"epoch": 31, "training_loss": 9195.923217773438, "training_acc": 36.0, "val_loss": 248.91080856323242, "val_acc": 72.0}
{"epoch": 32, "training_loss": 3902.369903564453, "training_acc": 54.0, "val_loss": 876.6468048095703, "val_acc": 72.0}
{"epoch": 33, "training_loss": 3985.303909301758, "training_acc": 72.0, "val_loss": 126.49966478347778, "val_acc": 72.0}
{"epoch": 34, "training_loss": 5733.369354248047, "training_acc": 54.0, "val_loss": 81.23946785926819, "val_acc": 72.0}
{"epoch": 35, "training_loss": 402.29434967041016, "training_acc": 72.0, "val_loss": 2037.5301361083984, "val_acc": 28.0}
{"epoch": 36, "training_loss": 7148.262756347656, "training_acc": 40.0, "val_loss": 998.2184410095215, "val_acc": 72.0}
{"epoch": 37, "training_loss": 2650.7976779937744, "training_acc": 72.0, "val_loss": 3373.2452392578125, "val_acc": 28.0}
{"epoch": 38, "training_loss": 8268.023383378983, "training_acc": 48.0, "val_loss": 1438.2625579833984, "val_acc": 72.0}
{"epoch": 39, "training_loss": 5889.923828125, "training_acc": 72.0, "val_loss": 380.08666038513184, "val_acc": 72.0}
{"epoch": 40, "training_loss": 6617.302490234375, "training_acc": 50.0, "val_loss": 220.2237844467163, "val_acc": 72.0}
{"epoch": 41, "training_loss": 770.1429557800293, "training_acc": 72.0, "val_loss": 1445.606803894043, "val_acc": 28.0}
