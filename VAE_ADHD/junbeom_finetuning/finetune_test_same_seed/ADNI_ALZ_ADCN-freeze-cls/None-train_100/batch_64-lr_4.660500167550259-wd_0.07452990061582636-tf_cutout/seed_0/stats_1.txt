"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 23968.41005706787, "training_acc": 72.0, "val_loss": 5098.911666870117, "val_acc": 72.0}
{"epoch": 1, "training_loss": 31330.046875, "training_acc": 54.0, "val_loss": 308.074951171875, "val_acc": 72.0}
{"epoch": 2, "training_loss": 2414.624954223633, "training_acc": 72.0, "val_loss": 3691.098403930664, "val_acc": 28.0}
{"epoch": 3, "training_loss": 11392.763656616211, "training_acc": 46.0, "val_loss": 1969.0067291259766, "val_acc": 72.0}
{"epoch": 4, "training_loss": 6053.722034454346, "training_acc": 72.0, "val_loss": 4424.470520019531, "val_acc": 28.0}
{"epoch": 5, "training_loss": 10980.963821411133, "training_acc": 50.0, "val_loss": 746.1057186126709, "val_acc": 72.0}
{"epoch": 6, "training_loss": 3636.6092987060547, "training_acc": 54.0, "val_loss": 2009.0154647827148, "val_acc": 72.0}
{"epoch": 7, "training_loss": 8241.631225585938, "training_acc": 72.0, "val_loss": 1185.6098175048828, "val_acc": 72.0}
{"epoch": 8, "training_loss": 4586.89599609375, "training_acc": 58.0, "val_loss": 1201.8689155578613, "val_acc": 72.0}
{"epoch": 9, "training_loss": 4845.76318359375, "training_acc": 72.0, "val_loss": 95.2724814414978, "val_acc": 28.0}
{"epoch": 10, "training_loss": 2753.995834350586, "training_acc": 44.0, "val_loss": 1778.395652770996, "val_acc": 72.0}
{"epoch": 11, "training_loss": 5642.941650390625, "training_acc": 72.0, "val_loss": 2226.5005111694336, "val_acc": 28.0}
{"epoch": 12, "training_loss": 7619.608169555664, "training_acc": 38.0, "val_loss": 602.8643608093262, "val_acc": 72.0}
{"epoch": 13, "training_loss": 4919.570709228516, "training_acc": 48.0, "val_loss": 1249.4510650634766, "val_acc": 72.0}
{"epoch": 14, "training_loss": 5418.2174072265625, "training_acc": 72.0, "val_loss": 637.3452186584473, "val_acc": 72.0}
{"epoch": 15, "training_loss": 6326.4066162109375, "training_acc": 48.0, "val_loss": 689.6331787109375, "val_acc": 72.0}
{"epoch": 16, "training_loss": 3230.7847442626953, "training_acc": 72.0, "val_loss": 177.74395942687988, "val_acc": 28.0}
{"epoch": 17, "training_loss": 2377.7884216308594, "training_acc": 46.0, "val_loss": 1613.113784790039, "val_acc": 72.0}
{"epoch": 18, "training_loss": 5228.22599029541, "training_acc": 72.0, "val_loss": 1660.9975814819336, "val_acc": 28.0}
{"epoch": 19, "training_loss": 4837.748764038086, "training_acc": 48.0, "val_loss": 757.1717262268066, "val_acc": 72.0}
{"epoch": 20, "training_loss": 2009.5753269195557, "training_acc": 58.0, "val_loss": 1609.6660614013672, "val_acc": 72.0}
{"epoch": 21, "training_loss": 6360.217193603516, "training_acc": 72.0, "val_loss": 825.3326416015625, "val_acc": 72.0}
{"epoch": 22, "training_loss": 3593.190200805664, "training_acc": 58.0, "val_loss": 971.151065826416, "val_acc": 72.0}
{"epoch": 23, "training_loss": 3743.4013671875, "training_acc": 72.0, "val_loss": 34.88193154335022, "val_acc": 28.0}
{"epoch": 24, "training_loss": 1621.3098220825195, "training_acc": 46.0, "val_loss": 1087.7863883972168, "val_acc": 72.0}
{"epoch": 25, "training_loss": 2845.3501739501953, "training_acc": 72.0, "val_loss": 3513.246536254883, "val_acc": 28.0}
{"epoch": 26, "training_loss": 8952.337491989136, "training_acc": 28.0, "val_loss": 2999.3270874023438, "val_acc": 72.0}
{"epoch": 27, "training_loss": 13400.970336914062, "training_acc": 72.0, "val_loss": 3338.9312744140625, "val_acc": 72.0}
{"epoch": 28, "training_loss": 11214.7548828125, "training_acc": 72.0, "val_loss": 300.7584810256958, "val_acc": 72.0}
{"epoch": 29, "training_loss": 6177.1219482421875, "training_acc": 60.0, "val_loss": 1498.1037139892578, "val_acc": 28.0}
{"epoch": 30, "training_loss": 7482.243133544922, "training_acc": 42.0, "val_loss": 2941.1983489990234, "val_acc": 72.0}
{"epoch": 31, "training_loss": 10963.374755859375, "training_acc": 72.0, "val_loss": 1100.1294136047363, "val_acc": 72.0}
{"epoch": 32, "training_loss": 4920.274932861328, "training_acc": 56.0, "val_loss": 712.5919818878174, "val_acc": 72.0}
{"epoch": 33, "training_loss": 2654.3460693359375, "training_acc": 72.0, "val_loss": 997.2244262695312, "val_acc": 28.0}
{"epoch": 34, "training_loss": 4673.194625854492, "training_acc": 42.0, "val_loss": 1339.8369789123535, "val_acc": 72.0}
{"epoch": 35, "training_loss": 4024.6381759643555, "training_acc": 72.0, "val_loss": 2513.687515258789, "val_acc": 28.0}
{"epoch": 36, "training_loss": 7573.782341003418, "training_acc": 40.0, "val_loss": 341.2410259246826, "val_acc": 72.0}
{"epoch": 37, "training_loss": 3238.077392578125, "training_acc": 56.0, "val_loss": 1025.606346130371, "val_acc": 72.0}
{"epoch": 38, "training_loss": 3989.917495727539, "training_acc": 72.0, "val_loss": 331.6718101501465, "val_acc": 72.0}
{"epoch": 39, "training_loss": 1880.9498901367188, "training_acc": 70.0, "val_loss": 659.2397212982178, "val_acc": 72.0}
{"epoch": 40, "training_loss": 2307.6546173095703, "training_acc": 72.0, "val_loss": 1397.2606658935547, "val_acc": 28.0}
{"epoch": 41, "training_loss": 4615.09294128418, "training_acc": 48.0, "val_loss": 1265.1857376098633, "val_acc": 72.0}
{"epoch": 42, "training_loss": 3985.774959564209, "training_acc": 72.0, "val_loss": 2222.971534729004, "val_acc": 28.0}
