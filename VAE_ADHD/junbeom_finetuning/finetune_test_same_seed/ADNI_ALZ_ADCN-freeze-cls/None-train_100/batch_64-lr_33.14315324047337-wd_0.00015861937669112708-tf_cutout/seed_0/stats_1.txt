"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 176513.46266937256, "training_acc": 72.0, "val_loss": 66016.552734375, "val_acc": 72.0}
{"epoch": 1, "training_loss": 316904.560546875, "training_acc": 54.0, "val_loss": 17211.062622070312, "val_acc": 28.0}
{"epoch": 2, "training_loss": 150638.111328125, "training_acc": 40.0, "val_loss": 98974.24926757812, "val_acc": 72.0}
{"epoch": 3, "training_loss": 403805.298828125, "training_acc": 72.0, "val_loss": 108704.08935546875, "val_acc": 72.0}
{"epoch": 4, "training_loss": 404617.107421875, "training_acc": 72.0, "val_loss": 61966.0888671875, "val_acc": 72.0}
{"epoch": 5, "training_loss": 188521.677734375, "training_acc": 72.0, "val_loss": 68895.51391601562, "val_acc": 28.0}
{"epoch": 6, "training_loss": 285210.234375, "training_acc": 28.0, "val_loss": 30794.607543945312, "val_acc": 28.0}
{"epoch": 7, "training_loss": 107170.29760742188, "training_acc": 48.0, "val_loss": 58936.95068359375, "val_acc": 72.0}
{"epoch": 8, "training_loss": 248186.6962890625, "training_acc": 72.0, "val_loss": 77518.12133789062, "val_acc": 72.0}
{"epoch": 9, "training_loss": 296291.07861328125, "training_acc": 72.0, "val_loss": 54135.296630859375, "val_acc": 72.0}
{"epoch": 10, "training_loss": 173595.1337890625, "training_acc": 72.0, "val_loss": 3384.2430114746094, "val_acc": 28.0}
{"epoch": 11, "training_loss": 26201.418212890625, "training_acc": 28.0, "val_loss": 16244.401550292969, "val_acc": 72.0}
{"epoch": 12, "training_loss": 79876.21533203125, "training_acc": 72.0, "val_loss": 24953.482055664062, "val_acc": 72.0}
{"epoch": 13, "training_loss": 80933.12133789062, "training_acc": 72.0, "val_loss": 12563.06381225586, "val_acc": 28.0}
{"epoch": 14, "training_loss": 32777.25844192505, "training_acc": 44.0, "val_loss": 2330.4534912109375, "val_acc": 28.0}
{"epoch": 15, "training_loss": 31524.76953125, "training_acc": 46.0, "val_loss": 30655.133056640625, "val_acc": 72.0}
{"epoch": 16, "training_loss": 120470.98852539062, "training_acc": 72.0, "val_loss": 21936.07177734375, "val_acc": 72.0}
{"epoch": 17, "training_loss": 60290.71789550781, "training_acc": 72.0, "val_loss": 46903.38134765625, "val_acc": 28.0}
{"epoch": 18, "training_loss": 179222.20166015625, "training_acc": 28.0, "val_loss": 5620.381546020508, "val_acc": 72.0}
{"epoch": 19, "training_loss": 43685.0849609375, "training_acc": 72.0, "val_loss": 18857.052612304688, "val_acc": 72.0}
{"epoch": 20, "training_loss": 62648.29284667969, "training_acc": 72.0, "val_loss": 15735.231018066406, "val_acc": 28.0}
{"epoch": 21, "training_loss": 38730.498306274414, "training_acc": 48.0, "val_loss": 251.62243843078613, "val_acc": 64.0}
{"epoch": 22, "training_loss": 2764.9195861816406, "training_acc": 50.0, "val_loss": 23422.987365722656, "val_acc": 72.0}
{"epoch": 23, "training_loss": 110551.6865234375, "training_acc": 72.0, "val_loss": 38933.26416015625, "val_acc": 72.0}
{"epoch": 24, "training_loss": 145501.41821289062, "training_acc": 72.0, "val_loss": 16914.173889160156, "val_acc": 72.0}
{"epoch": 25, "training_loss": 59994.5390625, "training_acc": 56.0, "val_loss": 3480.289077758789, "val_acc": 28.0}
{"epoch": 26, "training_loss": 30441.217529296875, "training_acc": 50.0, "val_loss": 37292.29736328125, "val_acc": 72.0}
{"epoch": 27, "training_loss": 152810.8828125, "training_acc": 72.0, "val_loss": 37086.71875, "val_acc": 72.0}
{"epoch": 28, "training_loss": 128392.09716796875, "training_acc": 72.0, "val_loss": 3167.2183990478516, "val_acc": 72.0}
{"epoch": 29, "training_loss": 124381.9150390625, "training_acc": 46.0, "val_loss": 67378.7109375, "val_acc": 28.0}
{"epoch": 30, "training_loss": 184494.1629638672, "training_acc": 28.0, "val_loss": 36818.32275390625, "val_acc": 72.0}
{"epoch": 31, "training_loss": 176318.03369140625, "training_acc": 72.0, "val_loss": 82382.26928710938, "val_acc": 72.0}
{"epoch": 32, "training_loss": 329610.95166015625, "training_acc": 72.0, "val_loss": 86960.72998046875, "val_acc": 72.0}
{"epoch": 33, "training_loss": 331466.2685546875, "training_acc": 72.0, "val_loss": 60683.673095703125, "val_acc": 72.0}
{"epoch": 34, "training_loss": 205314.01220703125, "training_acc": 72.0, "val_loss": 6052.584457397461, "val_acc": 72.0}
{"epoch": 35, "training_loss": 136444.48828125, "training_acc": 52.0, "val_loss": 100942.66967773438, "val_acc": 28.0}
{"epoch": 36, "training_loss": 345845.12109375, "training_acc": 28.0, "val_loss": 8560.41030883789, "val_acc": 72.0}
{"epoch": 37, "training_loss": 68010.50390625, "training_acc": 72.0, "val_loss": 40826.025390625, "val_acc": 72.0}
{"epoch": 38, "training_loss": 162354.91259765625, "training_acc": 72.0, "val_loss": 32659.249877929688, "val_acc": 72.0}
{"epoch": 39, "training_loss": 106848.83154296875, "training_acc": 72.0, "val_loss": 20226.74560546875, "val_acc": 28.0}
{"epoch": 40, "training_loss": 75480.10205078125, "training_acc": 28.0, "val_loss": 15704.501342773438, "val_acc": 72.0}
