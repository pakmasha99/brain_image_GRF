"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 96864.10984039307, "training_acc": 50.0, "val_loss": 115389.2578125, "val_acc": 72.0}
{"epoch": 1, "training_loss": 427936.849609375, "training_acc": 72.0, "val_loss": 50653.045654296875, "val_acc": 72.0}
{"epoch": 2, "training_loss": 183926.29931640625, "training_acc": 52.0, "val_loss": 17147.581481933594, "val_acc": 28.0}
{"epoch": 3, "training_loss": 71861.61499023438, "training_acc": 46.0, "val_loss": 38618.780517578125, "val_acc": 72.0}
{"epoch": 4, "training_loss": 159608.88134765625, "training_acc": 72.0, "val_loss": 34828.89099121094, "val_acc": 72.0}
{"epoch": 5, "training_loss": 124284.14477539062, "training_acc": 72.0, "val_loss": 11819.989013671875, "val_acc": 28.0}
{"epoch": 6, "training_loss": 35835.59893798828, "training_acc": 28.0, "val_loss": 21731.956481933594, "val_acc": 72.0}
{"epoch": 7, "training_loss": 103661.00341796875, "training_acc": 72.0, "val_loss": 39472.943115234375, "val_acc": 72.0}
{"epoch": 8, "training_loss": 153866.7978515625, "training_acc": 72.0, "val_loss": 25546.08612060547, "val_acc": 72.0}
{"epoch": 9, "training_loss": 79841.72644042969, "training_acc": 72.0, "val_loss": 28262.765502929688, "val_acc": 28.0}
{"epoch": 10, "training_loss": 111256.10888671875, "training_acc": 28.0, "val_loss": 5338.834762573242, "val_acc": 72.0}
{"epoch": 11, "training_loss": 32304.911376953125, "training_acc": 72.0, "val_loss": 13234.246826171875, "val_acc": 72.0}
{"epoch": 12, "training_loss": 42535.677490234375, "training_acc": 72.0, "val_loss": 15096.867370605469, "val_acc": 28.0}
{"epoch": 13, "training_loss": 42424.16147232056, "training_acc": 37.0, "val_loss": 6136.783599853516, "val_acc": 72.0}
{"epoch": 14, "training_loss": 22039.024475097656, "training_acc": 72.0, "val_loss": 6347.1160888671875, "val_acc": 28.0}
{"epoch": 15, "training_loss": 26759.9931640625, "training_acc": 44.0, "val_loss": 10095.640563964844, "val_acc": 72.0}
{"epoch": 16, "training_loss": 34317.742614746094, "training_acc": 72.0, "val_loss": 10058.600616455078, "val_acc": 28.0}
{"epoch": 17, "training_loss": 28113.324310302734, "training_acc": 48.0, "val_loss": 3560.833740234375, "val_acc": 72.0}
{"epoch": 18, "training_loss": 29798.334106445312, "training_acc": 44.0, "val_loss": 8450.62484741211, "val_acc": 72.0}
{"epoch": 19, "training_loss": 41994.388427734375, "training_acc": 72.0, "val_loss": 12039.632415771484, "val_acc": 72.0}
{"epoch": 20, "training_loss": 35671.632080078125, "training_acc": 72.0, "val_loss": 22983.164978027344, "val_acc": 28.0}
{"epoch": 21, "training_loss": 78416.24597167969, "training_acc": 28.0, "val_loss": 12484.464263916016, "val_acc": 72.0}
{"epoch": 22, "training_loss": 63514.27294921875, "training_acc": 72.0, "val_loss": 25014.51873779297, "val_acc": 72.0}
{"epoch": 23, "training_loss": 94360.60180664062, "training_acc": 72.0, "val_loss": 11635.684967041016, "val_acc": 72.0}
{"epoch": 24, "training_loss": 28848.38311767578, "training_acc": 64.0, "val_loss": 1418.3481216430664, "val_acc": 32.0}
{"epoch": 25, "training_loss": 32793.681640625, "training_acc": 42.0, "val_loss": 21609.930419921875, "val_acc": 72.0}
{"epoch": 26, "training_loss": 85202.43383789062, "training_acc": 72.0, "val_loss": 14649.925231933594, "val_acc": 72.0}
{"epoch": 27, "training_loss": 45037.78372192383, "training_acc": 72.0, "val_loss": 34932.95593261719, "val_acc": 28.0}
{"epoch": 28, "training_loss": 136415.30517578125, "training_acc": 28.0, "val_loss": 3672.329330444336, "val_acc": 72.0}
{"epoch": 29, "training_loss": 32543.04248046875, "training_acc": 72.0, "val_loss": 14923.959350585938, "val_acc": 72.0}
{"epoch": 30, "training_loss": 52188.22265625, "training_acc": 72.0, "val_loss": 1474.2704391479492, "val_acc": 36.0}
{"epoch": 31, "training_loss": 7823.953033447266, "training_acc": 53.0, "val_loss": 1384.069538116455, "val_acc": 72.0}
{"epoch": 32, "training_loss": 28875.72314453125, "training_acc": 50.0, "val_loss": 3832.503128051758, "val_acc": 72.0}
{"epoch": 33, "training_loss": 18212.24591064453, "training_acc": 72.0, "val_loss": 3590.231704711914, "val_acc": 72.0}
{"epoch": 34, "training_loss": 23101.181640625, "training_acc": 60.0, "val_loss": 2479.586410522461, "val_acc": 72.0}
{"epoch": 35, "training_loss": 10166.51333618164, "training_acc": 72.0, "val_loss": 6858.233642578125, "val_acc": 28.0}
{"epoch": 36, "training_loss": 21766.193298339844, "training_acc": 52.0, "val_loss": 9251.612091064453, "val_acc": 72.0}
{"epoch": 37, "training_loss": 32514.0546875, "training_acc": 72.0, "val_loss": 3859.607696533203, "val_acc": 28.0}
{"epoch": 38, "training_loss": 18878.00018310547, "training_acc": 44.0, "val_loss": 7816.641998291016, "val_acc": 72.0}
{"epoch": 39, "training_loss": 23401.614196777344, "training_acc": 72.0, "val_loss": 17734.01336669922, "val_acc": 28.0}
{"epoch": 40, "training_loss": 54363.03869628906, "training_acc": 28.0, "val_loss": 17221.00372314453, "val_acc": 72.0}
{"epoch": 41, "training_loss": 92084.80322265625, "training_acc": 72.0, "val_loss": 33942.437744140625, "val_acc": 72.0}
{"epoch": 42, "training_loss": 131104.595703125, "training_acc": 72.0, "val_loss": 22422.93243408203, "val_acc": 72.0}
{"epoch": 43, "training_loss": 67847.96850585938, "training_acc": 72.0, "val_loss": 23347.999572753906, "val_acc": 28.0}
{"epoch": 44, "training_loss": 96245.87841796875, "training_acc": 28.0, "val_loss": 3375.1724243164062, "val_acc": 72.0}
{"epoch": 45, "training_loss": 22831.368774414062, "training_acc": 72.0, "val_loss": 8966.917419433594, "val_acc": 72.0}
{"epoch": 46, "training_loss": 26318.63655090332, "training_acc": 72.0, "val_loss": 25102.951049804688, "val_acc": 28.0}
{"epoch": 47, "training_loss": 82393.55627441406, "training_acc": 28.0, "val_loss": 14311.630249023438, "val_acc": 72.0}
{"epoch": 48, "training_loss": 66107.5517578125, "training_acc": 72.0, "val_loss": 28710.894775390625, "val_acc": 72.0}
{"epoch": 49, "training_loss": 111288.61645507812, "training_acc": 72.0, "val_loss": 18492.86651611328, "val_acc": 72.0}
{"epoch": 50, "training_loss": 56713.791015625, "training_acc": 72.0, "val_loss": 27742.019653320312, "val_acc": 28.0}
