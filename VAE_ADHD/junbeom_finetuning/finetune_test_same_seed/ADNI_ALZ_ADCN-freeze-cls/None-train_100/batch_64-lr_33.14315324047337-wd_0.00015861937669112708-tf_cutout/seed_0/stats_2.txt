"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 211859.9257965088, "training_acc": 40.0, "val_loss": 117558.82568359375, "val_acc": 72.0}
{"epoch": 1, "training_loss": 410672.1171875, "training_acc": 72.0, "val_loss": 7352.503967285156, "val_acc": 72.0}
{"epoch": 2, "training_loss": 270294.138671875, "training_acc": 56.0, "val_loss": 172071.00830078125, "val_acc": 28.0}
{"epoch": 3, "training_loss": 567763.2314453125, "training_acc": 28.0, "val_loss": 29305.459594726562, "val_acc": 72.0}
{"epoch": 4, "training_loss": 176996.6796875, "training_acc": 72.0, "val_loss": 87883.01391601562, "val_acc": 72.0}
{"epoch": 5, "training_loss": 366067.8515625, "training_acc": 72.0, "val_loss": 88522.16186523438, "val_acc": 72.0}
{"epoch": 6, "training_loss": 322609.9208984375, "training_acc": 72.0, "val_loss": 44650.72021484375, "val_acc": 72.0}
{"epoch": 7, "training_loss": 121622.21142578125, "training_acc": 72.0, "val_loss": 84424.5849609375, "val_acc": 28.0}
{"epoch": 8, "training_loss": 387268.498046875, "training_acc": 28.0, "val_loss": 78661.34033203125, "val_acc": 28.0}
{"epoch": 9, "training_loss": 230657.44018554688, "training_acc": 34.0, "val_loss": 24191.60919189453, "val_acc": 72.0}
{"epoch": 10, "training_loss": 110532.796875, "training_acc": 72.0, "val_loss": 32997.283935546875, "val_acc": 72.0}
{"epoch": 11, "training_loss": 117361.60595703125, "training_acc": 72.0, "val_loss": 7204.344940185547, "val_acc": 72.0}
{"epoch": 12, "training_loss": 77877.955078125, "training_acc": 56.0, "val_loss": 41428.57971191406, "val_acc": 28.0}
{"epoch": 13, "training_loss": 113591.70819091797, "training_acc": 42.0, "val_loss": 13285.731506347656, "val_acc": 72.0}
{"epoch": 14, "training_loss": 57853.79052734375, "training_acc": 72.0, "val_loss": 6624.903106689453, "val_acc": 72.0}
{"epoch": 15, "training_loss": 55632.48876953125, "training_acc": 54.0, "val_loss": 12203.311157226562, "val_acc": 28.0}
{"epoch": 16, "training_loss": 53694.659423828125, "training_acc": 46.0, "val_loss": 32602.670288085938, "val_acc": 72.0}
{"epoch": 17, "training_loss": 136073.31884765625, "training_acc": 72.0, "val_loss": 34326.6357421875, "val_acc": 72.0}
{"epoch": 18, "training_loss": 120956.70092773438, "training_acc": 72.0, "val_loss": 5786.94953918457, "val_acc": 72.0}
{"epoch": 19, "training_loss": 72572.279296875, "training_acc": 58.0, "val_loss": 46573.95324707031, "val_acc": 28.0}
{"epoch": 20, "training_loss": 120777.8203125, "training_acc": 32.0, "val_loss": 31537.362670898438, "val_acc": 72.0}
{"epoch": 21, "training_loss": 154522.880859375, "training_acc": 72.0, "val_loss": 65164.2822265625, "val_acc": 72.0}
{"epoch": 22, "training_loss": 260063.5, "training_acc": 72.0, "val_loss": 62959.65576171875, "val_acc": 72.0}
{"epoch": 23, "training_loss": 232360.0966796875, "training_acc": 72.0, "val_loss": 33445.44982910156, "val_acc": 72.0}
{"epoch": 24, "training_loss": 102386.31829833984, "training_acc": 72.0, "val_loss": 53171.76513671875, "val_acc": 28.0}
{"epoch": 25, "training_loss": 235702.533203125, "training_acc": 28.0, "val_loss": 33484.61608886719, "val_acc": 28.0}
{"epoch": 26, "training_loss": 100403.82861328125, "training_acc": 48.0, "val_loss": 38464.849853515625, "val_acc": 72.0}
{"epoch": 27, "training_loss": 166157.87841796875, "training_acc": 72.0, "val_loss": 52526.64794921875, "val_acc": 72.0}
{"epoch": 28, "training_loss": 200925.6953125, "training_acc": 72.0, "val_loss": 34708.013916015625, "val_acc": 72.0}
{"epoch": 29, "training_loss": 117020.87744140625, "training_acc": 72.0, "val_loss": 18895.57647705078, "val_acc": 28.0}
{"epoch": 30, "training_loss": 79394.94946289062, "training_acc": 28.0, "val_loss": 10346.381378173828, "val_acc": 72.0}
{"epoch": 31, "training_loss": 55605.4765625, "training_acc": 72.0, "val_loss": 19987.14599609375, "val_acc": 72.0}
{"epoch": 32, "training_loss": 66326.61547851562, "training_acc": 72.0, "val_loss": 7895.624542236328, "val_acc": 28.0}
{"epoch": 33, "training_loss": 21748.529251098633, "training_acc": 46.0, "val_loss": 2550.895881652832, "val_acc": 72.0}
{"epoch": 34, "training_loss": 25427.806640625, "training_acc": 54.0, "val_loss": 7929.918670654297, "val_acc": 72.0}
{"epoch": 35, "training_loss": 33317.64794921875, "training_acc": 72.0, "val_loss": 8053.3294677734375, "val_acc": 72.0}
{"epoch": 36, "training_loss": 23067.17498779297, "training_acc": 61.0, "val_loss": 6262.657928466797, "val_acc": 72.0}
{"epoch": 37, "training_loss": 21432.55206298828, "training_acc": 72.0, "val_loss": 4283.667755126953, "val_acc": 32.0}
{"epoch": 38, "training_loss": 25741.413696289062, "training_acc": 45.0, "val_loss": 13201.483154296875, "val_acc": 72.0}
{"epoch": 39, "training_loss": 42426.88610839844, "training_acc": 72.0, "val_loss": 9871.637725830078, "val_acc": 28.0}
{"epoch": 40, "training_loss": 34556.616271972656, "training_acc": 40.0, "val_loss": 4382.792282104492, "val_acc": 72.0}
{"epoch": 41, "training_loss": 20206.095825195312, "training_acc": 60.0, "val_loss": 8051.1444091796875, "val_acc": 72.0}
{"epoch": 42, "training_loss": 33321.71105957031, "training_acc": 72.0, "val_loss": 4744.313812255859, "val_acc": 72.0}
{"epoch": 43, "training_loss": 55014.60302734375, "training_acc": 48.0, "val_loss": 2550.0839233398438, "val_acc": 36.0}
{"epoch": 44, "training_loss": 59211.97802734375, "training_acc": 43.0, "val_loss": 36324.3896484375, "val_acc": 72.0}
{"epoch": 45, "training_loss": 145698.892578125, "training_acc": 72.0, "val_loss": 33710.546875, "val_acc": 72.0}
{"epoch": 46, "training_loss": 113245.71997070312, "training_acc": 72.0, "val_loss": 1875.4985809326172, "val_acc": 72.0}
{"epoch": 47, "training_loss": 87390.3408203125, "training_acc": 53.0, "val_loss": 53488.427734375, "val_acc": 28.0}
{"epoch": 48, "training_loss": 147500.75, "training_acc": 35.0, "val_loss": 29698.703002929688, "val_acc": 72.0}
{"epoch": 49, "training_loss": 158199.8955078125, "training_acc": 72.0, "val_loss": 60413.0859375, "val_acc": 72.0}
{"epoch": 50, "training_loss": 238783.31640625, "training_acc": 72.0, "val_loss": 52897.86376953125, "val_acc": 72.0}
{"epoch": 51, "training_loss": 181714.595703125, "training_acc": 72.0, "val_loss": 16505.60302734375, "val_acc": 72.0}
{"epoch": 52, "training_loss": 76761.03076171875, "training_acc": 58.0, "val_loss": 41258.21838378906, "val_acc": 28.0}
{"epoch": 53, "training_loss": 118008.5474243164, "training_acc": 36.0, "val_loss": 22857.90252685547, "val_acc": 72.0}
{"epoch": 54, "training_loss": 103707.0205078125, "training_acc": 72.0, "val_loss": 40319.27185058594, "val_acc": 72.0}
{"epoch": 55, "training_loss": 151895.61669921875, "training_acc": 72.0, "val_loss": 25290.80810546875, "val_acc": 72.0}
{"epoch": 56, "training_loss": 68764.47668457031, "training_acc": 72.0, "val_loss": 36961.48376464844, "val_acc": 28.0}
{"epoch": 57, "training_loss": 159959.2099609375, "training_acc": 28.0, "val_loss": 1384.9515914916992, "val_acc": 64.0}
{"epoch": 58, "training_loss": 17229.6708984375, "training_acc": 75.0, "val_loss": 29111.24267578125, "val_acc": 72.0}
{"epoch": 59, "training_loss": 116129.75146484375, "training_acc": 72.0, "val_loss": 24876.573181152344, "val_acc": 72.0}
{"epoch": 60, "training_loss": 74002.90112304688, "training_acc": 72.0, "val_loss": 22227.589416503906, "val_acc": 28.0}
{"epoch": 61, "training_loss": 83764.95825195312, "training_acc": 28.0, "val_loss": 12918.202209472656, "val_acc": 72.0}
{"epoch": 62, "training_loss": 56029.890869140625, "training_acc": 72.0, "val_loss": 24260.430908203125, "val_acc": 72.0}
{"epoch": 63, "training_loss": 84247.0166015625, "training_acc": 72.0, "val_loss": 5687.399673461914, "val_acc": 72.0}
{"epoch": 64, "training_loss": 58861.11181640625, "training_acc": 56.0, "val_loss": 28449.014282226562, "val_acc": 28.0}
{"epoch": 65, "training_loss": 93647.375, "training_acc": 40.0, "val_loss": 21354.86297607422, "val_acc": 72.0}
{"epoch": 66, "training_loss": 83269.79516601562, "training_acc": 72.0, "val_loss": 16880.747985839844, "val_acc": 72.0}
{"epoch": 67, "training_loss": 50501.41323852539, "training_acc": 69.0, "val_loss": 31099.4140625, "val_acc": 28.0}
{"epoch": 68, "training_loss": 100204.94055175781, "training_acc": 28.0, "val_loss": 19992.605590820312, "val_acc": 72.0}
{"epoch": 69, "training_loss": 103936.4140625, "training_acc": 72.0, "val_loss": 39421.417236328125, "val_acc": 72.0}
{"epoch": 70, "training_loss": 146684.84228515625, "training_acc": 72.0, "val_loss": 23063.546752929688, "val_acc": 72.0}
{"epoch": 71, "training_loss": 64774.05465698242, "training_acc": 73.0, "val_loss": 49686.03515625, "val_acc": 28.0}
{"epoch": 72, "training_loss": 202488.22705078125, "training_acc": 28.0, "val_loss": 5293.7713623046875, "val_acc": 32.0}
{"epoch": 73, "training_loss": 64453.75732421875, "training_acc": 45.0, "val_loss": 55439.703369140625, "val_acc": 72.0}
{"epoch": 74, "training_loss": 242211.4541015625, "training_acc": 72.0, "val_loss": 71800.64697265625, "val_acc": 72.0}
{"epoch": 75, "training_loss": 274070.5966796875, "training_acc": 72.0, "val_loss": 52926.397705078125, "val_acc": 72.0}
{"epoch": 76, "training_loss": 176464.6025390625, "training_acc": 72.0, "val_loss": 8478.23486328125, "val_acc": 72.0}
