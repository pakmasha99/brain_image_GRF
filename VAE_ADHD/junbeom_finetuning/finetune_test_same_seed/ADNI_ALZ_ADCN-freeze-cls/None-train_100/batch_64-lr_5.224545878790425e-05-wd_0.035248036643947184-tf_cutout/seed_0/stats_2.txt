"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 71.39011216163635, "training_acc": 28.0, "val_loss": 17.62307733297348, "val_acc": 28.0}
{"epoch": 1, "training_loss": 70.3511917591095, "training_acc": 28.0, "val_loss": 17.36833155155182, "val_acc": 28.0}
{"epoch": 2, "training_loss": 69.37800669670105, "training_acc": 41.0, "val_loss": 17.127707600593567, "val_acc": 28.0}
{"epoch": 3, "training_loss": 68.35048961639404, "training_acc": 72.0, "val_loss": 16.90172106027603, "val_acc": 28.0}
{"epoch": 4, "training_loss": 67.43255257606506, "training_acc": 72.0, "val_loss": 16.68989360332489, "val_acc": 28.0}
{"epoch": 5, "training_loss": 66.63456058502197, "training_acc": 72.0, "val_loss": 16.49092137813568, "val_acc": 28.0}
{"epoch": 6, "training_loss": 65.89592862129211, "training_acc": 72.0, "val_loss": 16.30537509918213, "val_acc": 28.0}
{"epoch": 7, "training_loss": 65.10047650337219, "training_acc": 72.0, "val_loss": 16.135407984256744, "val_acc": 28.0}
{"epoch": 8, "training_loss": 64.4853937625885, "training_acc": 72.0, "val_loss": 15.977713465690613, "val_acc": 28.0}
{"epoch": 9, "training_loss": 63.89558482170105, "training_acc": 72.0, "val_loss": 15.834523737430573, "val_acc": 28.0}
{"epoch": 10, "training_loss": 63.342644691467285, "training_acc": 72.0, "val_loss": 15.706737339496613, "val_acc": 28.0}
{"epoch": 11, "training_loss": 62.70322918891907, "training_acc": 72.0, "val_loss": 15.594257414340973, "val_acc": 28.0}
{"epoch": 12, "training_loss": 62.35453915596008, "training_acc": 72.0, "val_loss": 15.48997014760971, "val_acc": 28.0}
{"epoch": 13, "training_loss": 61.966564655303955, "training_acc": 72.0, "val_loss": 15.396802127361298, "val_acc": 28.0}
{"epoch": 14, "training_loss": 61.57211351394653, "training_acc": 72.0, "val_loss": 15.315288305282593, "val_acc": 72.0}
{"epoch": 15, "training_loss": 61.23303246498108, "training_acc": 72.0, "val_loss": 15.243595838546753, "val_acc": 72.0}
{"epoch": 16, "training_loss": 60.98971915245056, "training_acc": 72.0, "val_loss": 15.179583430290222, "val_acc": 72.0}
{"epoch": 17, "training_loss": 60.79269790649414, "training_acc": 72.0, "val_loss": 15.124258399009705, "val_acc": 72.0}
{"epoch": 18, "training_loss": 60.52062392234802, "training_acc": 72.0, "val_loss": 15.078730881214142, "val_acc": 72.0}
{"epoch": 19, "training_loss": 60.28428292274475, "training_acc": 72.0, "val_loss": 15.038931369781494, "val_acc": 72.0}
{"epoch": 20, "training_loss": 60.18769383430481, "training_acc": 72.0, "val_loss": 15.002337098121643, "val_acc": 72.0}
{"epoch": 21, "training_loss": 60.038572788238525, "training_acc": 72.0, "val_loss": 14.97098058462143, "val_acc": 72.0}
{"epoch": 22, "training_loss": 59.90206480026245, "training_acc": 72.0, "val_loss": 14.943955838680267, "val_acc": 72.0}
{"epoch": 23, "training_loss": 59.80944871902466, "training_acc": 72.0, "val_loss": 14.920437335968018, "val_acc": 72.0}
{"epoch": 24, "training_loss": 59.68947148323059, "training_acc": 72.0, "val_loss": 14.901348948478699, "val_acc": 72.0}
{"epoch": 25, "training_loss": 59.66211819648743, "training_acc": 72.0, "val_loss": 14.884637296199799, "val_acc": 72.0}
{"epoch": 26, "training_loss": 59.590789556503296, "training_acc": 72.0, "val_loss": 14.871610701084137, "val_acc": 72.0}
{"epoch": 27, "training_loss": 59.53068518638611, "training_acc": 72.0, "val_loss": 14.86198902130127, "val_acc": 72.0}
{"epoch": 28, "training_loss": 59.47533369064331, "training_acc": 72.0, "val_loss": 14.854475855827332, "val_acc": 72.0}
{"epoch": 29, "training_loss": 59.46802568435669, "training_acc": 72.0, "val_loss": 14.848221838474274, "val_acc": 72.0}
{"epoch": 30, "training_loss": 59.44439935684204, "training_acc": 72.0, "val_loss": 14.843228459358215, "val_acc": 72.0}
{"epoch": 31, "training_loss": 59.403491258621216, "training_acc": 72.0, "val_loss": 14.83944058418274, "val_acc": 72.0}
{"epoch": 32, "training_loss": 59.391441822052, "training_acc": 72.0, "val_loss": 14.836430549621582, "val_acc": 72.0}
{"epoch": 33, "training_loss": 59.38760828971863, "training_acc": 72.0, "val_loss": 14.833898842334747, "val_acc": 72.0}
{"epoch": 34, "training_loss": 59.36814212799072, "training_acc": 72.0, "val_loss": 14.832153916358948, "val_acc": 72.0}
{"epoch": 35, "training_loss": 59.38449239730835, "training_acc": 72.0, "val_loss": 14.830884337425232, "val_acc": 72.0}
{"epoch": 36, "training_loss": 59.36856198310852, "training_acc": 72.0, "val_loss": 14.829787611961365, "val_acc": 72.0}
{"epoch": 37, "training_loss": 59.38097882270813, "training_acc": 72.0, "val_loss": 14.828576147556305, "val_acc": 72.0}
{"epoch": 38, "training_loss": 59.36443328857422, "training_acc": 72.0, "val_loss": 14.827361702919006, "val_acc": 72.0}
{"epoch": 39, "training_loss": 59.35361361503601, "training_acc": 72.0, "val_loss": 14.826591312885284, "val_acc": 72.0}
{"epoch": 40, "training_loss": 59.35058903694153, "training_acc": 72.0, "val_loss": 14.826007187366486, "val_acc": 72.0}
{"epoch": 41, "training_loss": 59.339635372161865, "training_acc": 72.0, "val_loss": 14.825527369976044, "val_acc": 72.0}
{"epoch": 42, "training_loss": 59.33957076072693, "training_acc": 72.0, "val_loss": 14.825186133384705, "val_acc": 72.0}
{"epoch": 43, "training_loss": 59.3291015625, "training_acc": 72.0, "val_loss": 14.82497751712799, "val_acc": 72.0}
{"epoch": 44, "training_loss": 59.34805703163147, "training_acc": 72.0, "val_loss": 14.824885129928589, "val_acc": 72.0}
{"epoch": 45, "training_loss": 59.34176230430603, "training_acc": 72.0, "val_loss": 14.82475996017456, "val_acc": 72.0}
{"epoch": 46, "training_loss": 59.335052132606506, "training_acc": 72.0, "val_loss": 14.82471376657486, "val_acc": 72.0}
{"epoch": 47, "training_loss": 59.36158847808838, "training_acc": 72.0, "val_loss": 14.824658632278442, "val_acc": 72.0}
{"epoch": 48, "training_loss": 59.356526374816895, "training_acc": 72.0, "val_loss": 14.824658632278442, "val_acc": 72.0}
{"epoch": 49, "training_loss": 59.355175495147705, "training_acc": 72.0, "val_loss": 14.824685454368591, "val_acc": 72.0}
{"epoch": 50, "training_loss": 59.33563041687012, "training_acc": 72.0, "val_loss": 14.824727177619934, "val_acc": 72.0}
{"epoch": 51, "training_loss": 59.3527307510376, "training_acc": 72.0, "val_loss": 14.824788272380829, "val_acc": 72.0}
{"epoch": 52, "training_loss": 59.329800605773926, "training_acc": 72.0, "val_loss": 14.824923872947693, "val_acc": 72.0}
{"epoch": 53, "training_loss": 59.34516906738281, "training_acc": 72.0, "val_loss": 14.825128018856049, "val_acc": 72.0}
{"epoch": 54, "training_loss": 59.34735941886902, "training_acc": 72.0, "val_loss": 14.82531726360321, "val_acc": 72.0}
{"epoch": 55, "training_loss": 59.34085702896118, "training_acc": 72.0, "val_loss": 14.825575053691864, "val_acc": 72.0}
{"epoch": 56, "training_loss": 59.33406138420105, "training_acc": 72.0, "val_loss": 14.825783669948578, "val_acc": 72.0}
{"epoch": 57, "training_loss": 59.34597587585449, "training_acc": 72.0, "val_loss": 14.825993776321411, "val_acc": 72.0}
{"epoch": 58, "training_loss": 59.34874153137207, "training_acc": 72.0, "val_loss": 14.826203882694244, "val_acc": 72.0}
{"epoch": 59, "training_loss": 59.36067295074463, "training_acc": 72.0, "val_loss": 14.826339483261108, "val_acc": 72.0}
{"epoch": 60, "training_loss": 59.366002559661865, "training_acc": 72.0, "val_loss": 14.826388657093048, "val_acc": 72.0}
{"epoch": 61, "training_loss": 59.34260296821594, "training_acc": 72.0, "val_loss": 14.826543629169464, "val_acc": 72.0}
{"epoch": 62, "training_loss": 59.35325050354004, "training_acc": 72.0, "val_loss": 14.82638269662857, "val_acc": 72.0}
{"epoch": 63, "training_loss": 59.36577272415161, "training_acc": 72.0, "val_loss": 14.826209843158722, "val_acc": 72.0}
{"epoch": 64, "training_loss": 59.33611035346985, "training_acc": 72.0, "val_loss": 14.82597440481186, "val_acc": 72.0}
{"epoch": 65, "training_loss": 59.33016085624695, "training_acc": 72.0, "val_loss": 14.825578033924103, "val_acc": 72.0}
{"epoch": 66, "training_loss": 59.351810693740845, "training_acc": 72.0, "val_loss": 14.825265109539032, "val_acc": 72.0}
{"epoch": 67, "training_loss": 59.34972667694092, "training_acc": 72.0, "val_loss": 14.825162291526794, "val_acc": 72.0}
