"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 4948.502136230469, "training_acc": 72.0, "val_loss": 2512.4122619628906, "val_acc": 72.0}
{"epoch": 1, "training_loss": 7265.013259887695, "training_acc": 72.0, "val_loss": 2417.6801681518555, "val_acc": 28.0}
{"epoch": 2, "training_loss": 9005.98794555664, "training_acc": 28.0, "val_loss": 396.6675281524658, "val_acc": 72.0}
{"epoch": 3, "training_loss": 2261.2974700927734, "training_acc": 72.0, "val_loss": 1220.4303741455078, "val_acc": 72.0}
{"epoch": 4, "training_loss": 4702.8815841674805, "training_acc": 72.0, "val_loss": 649.0943431854248, "val_acc": 72.0}
{"epoch": 5, "training_loss": 1471.4505252838135, "training_acc": 72.0, "val_loss": 2255.069351196289, "val_acc": 28.0}
{"epoch": 6, "training_loss": 9156.822540283203, "training_acc": 28.0, "val_loss": 787.7784729003906, "val_acc": 28.0}
{"epoch": 7, "training_loss": 3308.119369506836, "training_acc": 42.0, "val_loss": 1534.6616744995117, "val_acc": 72.0}
{"epoch": 8, "training_loss": 7123.893127441406, "training_acc": 72.0, "val_loss": 2146.5011596679688, "val_acc": 72.0}
{"epoch": 9, "training_loss": 8434.850982666016, "training_acc": 72.0, "val_loss": 1708.6240768432617, "val_acc": 72.0}
{"epoch": 10, "training_loss": 6147.04150390625, "training_acc": 72.0, "val_loss": 481.00123405456543, "val_acc": 72.0}
{"epoch": 11, "training_loss": 2928.285354614258, "training_acc": 52.0, "val_loss": 1382.4799537658691, "val_acc": 28.0}
{"epoch": 12, "training_loss": 4104.005630493164, "training_acc": 28.0, "val_loss": 710.5552673339844, "val_acc": 72.0}
{"epoch": 13, "training_loss": 3737.313507080078, "training_acc": 72.0, "val_loss": 1571.3483810424805, "val_acc": 72.0}
{"epoch": 14, "training_loss": 6442.572357177734, "training_acc": 72.0, "val_loss": 1497.7277755737305, "val_acc": 72.0}
{"epoch": 15, "training_loss": 5534.561935424805, "training_acc": 72.0, "val_loss": 628.6848545074463, "val_acc": 72.0}
{"epoch": 16, "training_loss": 2029.6165561676025, "training_acc": 52.0, "val_loss": 273.8896369934082, "val_acc": 28.0}
{"epoch": 17, "training_loss": 773.0166606903076, "training_acc": 56.0, "val_loss": 561.4574909210205, "val_acc": 72.0}
{"epoch": 18, "training_loss": 2326.2065811157227, "training_acc": 72.0, "val_loss": 464.3343925476074, "val_acc": 72.0}
{"epoch": 19, "training_loss": 1450.566487312317, "training_acc": 72.0, "val_loss": 975.8599281311035, "val_acc": 28.0}
{"epoch": 20, "training_loss": 3607.8161392211914, "training_acc": 28.0, "val_loss": 267.33596324920654, "val_acc": 72.0}
{"epoch": 21, "training_loss": 1360.8833694458008, "training_acc": 72.0, "val_loss": 678.5645484924316, "val_acc": 72.0}
{"epoch": 22, "training_loss": 2595.1717834472656, "training_acc": 72.0, "val_loss": 316.97425842285156, "val_acc": 72.0}
{"epoch": 23, "training_loss": 1265.23775100708, "training_acc": 56.0, "val_loss": 20.28820812702179, "val_acc": 28.0}
{"epoch": 24, "training_loss": 268.02044105529785, "training_acc": 56.0, "val_loss": 487.58554458618164, "val_acc": 72.0}
{"epoch": 25, "training_loss": 1904.7056732177734, "training_acc": 72.0, "val_loss": 221.67487144470215, "val_acc": 72.0}
{"epoch": 26, "training_loss": 1473.2057495117188, "training_acc": 52.0, "val_loss": 108.69992971420288, "val_acc": 28.0}
{"epoch": 27, "training_loss": 641.977352142334, "training_acc": 54.0, "val_loss": 991.4703369140625, "val_acc": 72.0}
{"epoch": 28, "training_loss": 4216.609024047852, "training_acc": 72.0, "val_loss": 1174.3945121765137, "val_acc": 72.0}
{"epoch": 29, "training_loss": 4410.34814453125, "training_acc": 72.0, "val_loss": 580.1368713378906, "val_acc": 72.0}
{"epoch": 30, "training_loss": 1679.347062945366, "training_acc": 72.0, "val_loss": 1270.7390785217285, "val_acc": 28.0}
{"epoch": 31, "training_loss": 4764.510711669922, "training_acc": 28.0, "val_loss": 149.99032020568848, "val_acc": 72.0}
{"epoch": 32, "training_loss": 1096.0591354370117, "training_acc": 72.0, "val_loss": 526.9263744354248, "val_acc": 72.0}
{"epoch": 33, "training_loss": 1905.848617553711, "training_acc": 72.0, "val_loss": 54.388427734375, "val_acc": 72.0}
{"epoch": 34, "training_loss": 1939.1574096679688, "training_acc": 54.0, "val_loss": 1006.441593170166, "val_acc": 28.0}
{"epoch": 35, "training_loss": 2825.8897819519043, "training_acc": 44.0, "val_loss": 494.2026138305664, "val_acc": 72.0}
{"epoch": 36, "training_loss": 2083.7851791381836, "training_acc": 72.0, "val_loss": 463.58609199523926, "val_acc": 72.0}
{"epoch": 37, "training_loss": 1461.9412174224854, "training_acc": 72.0, "val_loss": 777.9199600219727, "val_acc": 28.0}
{"epoch": 38, "training_loss": 2762.10196685791, "training_acc": 28.0, "val_loss": 391.2799835205078, "val_acc": 72.0}
{"epoch": 39, "training_loss": 2285.1102447509766, "training_acc": 72.0, "val_loss": 866.2478446960449, "val_acc": 72.0}
{"epoch": 40, "training_loss": 3308.2630920410156, "training_acc": 72.0, "val_loss": 443.4943675994873, "val_acc": 72.0}
{"epoch": 41, "training_loss": 1434.5389423370361, "training_acc": 52.0, "val_loss": 66.80578589439392, "val_acc": 72.0}
{"epoch": 42, "training_loss": 449.66968727111816, "training_acc": 50.0, "val_loss": 382.0734739303589, "val_acc": 72.0}
