"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 77.71609354019165, "training_acc": 28.0, "val_loss": 19.00714635848999, "val_acc": 28.0}
{"epoch": 1, "training_loss": 75.62962794303894, "training_acc": 28.0, "val_loss": 18.499384820461273, "val_acc": 28.0}
{"epoch": 2, "training_loss": 73.51976704597473, "training_acc": 28.0, "val_loss": 18.02944540977478, "val_acc": 28.0}
{"epoch": 3, "training_loss": 71.70830035209656, "training_acc": 28.0, "val_loss": 17.593678832054138, "val_acc": 28.0}
{"epoch": 4, "training_loss": 69.86912608146667, "training_acc": 30.0, "val_loss": 17.197677493095398, "val_acc": 28.0}
{"epoch": 5, "training_loss": 68.4498016834259, "training_acc": 72.0, "val_loss": 16.83523654937744, "val_acc": 28.0}
{"epoch": 6, "training_loss": 67.16191697120667, "training_acc": 72.0, "val_loss": 16.512233018875122, "val_acc": 28.0}
{"epoch": 7, "training_loss": 65.74202036857605, "training_acc": 72.0, "val_loss": 16.230103373527527, "val_acc": 28.0}
{"epoch": 8, "training_loss": 64.65271091461182, "training_acc": 72.0, "val_loss": 15.98045527935028, "val_acc": 28.0}
{"epoch": 9, "training_loss": 63.764015436172485, "training_acc": 72.0, "val_loss": 15.762273967266083, "val_acc": 28.0}
{"epoch": 10, "training_loss": 62.81305146217346, "training_acc": 72.0, "val_loss": 15.576277673244476, "val_acc": 28.0}
{"epoch": 11, "training_loss": 62.14048433303833, "training_acc": 72.0, "val_loss": 15.417493879795074, "val_acc": 28.0}
{"epoch": 12, "training_loss": 61.5663640499115, "training_acc": 72.0, "val_loss": 15.2848482131958, "val_acc": 68.0}
{"epoch": 13, "training_loss": 61.113911390304565, "training_acc": 72.0, "val_loss": 15.176646411418915, "val_acc": 72.0}
{"epoch": 14, "training_loss": 60.59452033042908, "training_acc": 72.0, "val_loss": 15.091222524642944, "val_acc": 72.0}
{"epoch": 15, "training_loss": 60.27342414855957, "training_acc": 72.0, "val_loss": 15.021824836730957, "val_acc": 72.0}
{"epoch": 16, "training_loss": 59.993576765060425, "training_acc": 72.0, "val_loss": 14.966626465320587, "val_acc": 72.0}
{"epoch": 17, "training_loss": 59.753395795822144, "training_acc": 72.0, "val_loss": 14.924001693725586, "val_acc": 72.0}
{"epoch": 18, "training_loss": 59.661391258239746, "training_acc": 72.0, "val_loss": 14.890541136264801, "val_acc": 72.0}
{"epoch": 19, "training_loss": 59.58183789253235, "training_acc": 72.0, "val_loss": 14.866328239440918, "val_acc": 72.0}
{"epoch": 20, "training_loss": 59.446765661239624, "training_acc": 72.0, "val_loss": 14.850644767284393, "val_acc": 72.0}
{"epoch": 21, "training_loss": 59.309966683387756, "training_acc": 72.0, "val_loss": 14.840233325958252, "val_acc": 72.0}
{"epoch": 22, "training_loss": 59.31542611122131, "training_acc": 72.0, "val_loss": 14.833095669746399, "val_acc": 72.0}
{"epoch": 23, "training_loss": 59.317681074142456, "training_acc": 72.0, "val_loss": 14.829346537590027, "val_acc": 72.0}
{"epoch": 24, "training_loss": 59.331398487091064, "training_acc": 72.0, "val_loss": 14.82827216386795, "val_acc": 72.0}
{"epoch": 25, "training_loss": 59.387577056884766, "training_acc": 72.0, "val_loss": 14.828860759735107, "val_acc": 72.0}
{"epoch": 26, "training_loss": 59.31603693962097, "training_acc": 72.0, "val_loss": 14.829856157302856, "val_acc": 72.0}
{"epoch": 27, "training_loss": 59.34183096885681, "training_acc": 72.0, "val_loss": 14.830824732780457, "val_acc": 72.0}
{"epoch": 28, "training_loss": 59.354244232177734, "training_acc": 72.0, "val_loss": 14.832162857055664, "val_acc": 72.0}
{"epoch": 29, "training_loss": 59.38159966468811, "training_acc": 72.0, "val_loss": 14.833022654056549, "val_acc": 72.0}
{"epoch": 30, "training_loss": 59.34525203704834, "training_acc": 72.0, "val_loss": 14.833441376686096, "val_acc": 72.0}
{"epoch": 31, "training_loss": 59.35140609741211, "training_acc": 72.0, "val_loss": 14.833538234233856, "val_acc": 72.0}
{"epoch": 32, "training_loss": 59.33737659454346, "training_acc": 72.0, "val_loss": 14.834001660346985, "val_acc": 72.0}
{"epoch": 33, "training_loss": 59.33802807331085, "training_acc": 72.0, "val_loss": 14.834265410900116, "val_acc": 72.0}
{"epoch": 34, "training_loss": 59.347312211990356, "training_acc": 72.0, "val_loss": 14.834925532341003, "val_acc": 72.0}
{"epoch": 35, "training_loss": 59.3546187877655, "training_acc": 72.0, "val_loss": 14.834854006767273, "val_acc": 72.0}
{"epoch": 36, "training_loss": 59.34627890586853, "training_acc": 72.0, "val_loss": 14.83432948589325, "val_acc": 72.0}
{"epoch": 37, "training_loss": 59.348013401031494, "training_acc": 72.0, "val_loss": 14.83364850282669, "val_acc": 72.0}
{"epoch": 38, "training_loss": 59.34648609161377, "training_acc": 72.0, "val_loss": 14.832891523838043, "val_acc": 72.0}
{"epoch": 39, "training_loss": 59.36104154586792, "training_acc": 72.0, "val_loss": 14.831945300102234, "val_acc": 72.0}
{"epoch": 40, "training_loss": 59.33348107337952, "training_acc": 72.0, "val_loss": 14.831183850765228, "val_acc": 72.0}
{"epoch": 41, "training_loss": 59.33440327644348, "training_acc": 72.0, "val_loss": 14.830721914768219, "val_acc": 72.0}
{"epoch": 42, "training_loss": 59.33687901496887, "training_acc": 72.0, "val_loss": 14.830119907855988, "val_acc": 72.0}
{"epoch": 43, "training_loss": 59.32331156730652, "training_acc": 72.0, "val_loss": 14.829662442207336, "val_acc": 72.0}
