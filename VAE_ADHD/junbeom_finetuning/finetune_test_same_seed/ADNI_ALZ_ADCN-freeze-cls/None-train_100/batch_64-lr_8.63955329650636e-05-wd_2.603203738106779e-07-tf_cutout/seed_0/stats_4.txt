"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 73.21100211143494, "training_acc": 28.0, "val_loss": 17.899678647518158, "val_acc": 28.0}
{"epoch": 1, "training_loss": 71.28806614875793, "training_acc": 28.0, "val_loss": 17.466264963150024, "val_acc": 28.0}
{"epoch": 2, "training_loss": 69.51754021644592, "training_acc": 50.0, "val_loss": 17.07041561603546, "val_acc": 28.0}
{"epoch": 3, "training_loss": 67.97869420051575, "training_acc": 72.0, "val_loss": 16.711507737636566, "val_acc": 28.0}
{"epoch": 4, "training_loss": 66.74056458473206, "training_acc": 72.0, "val_loss": 16.388194262981415, "val_acc": 28.0}
{"epoch": 5, "training_loss": 65.38045907020569, "training_acc": 72.0, "val_loss": 16.10448658466339, "val_acc": 28.0}
{"epoch": 6, "training_loss": 64.31187057495117, "training_acc": 72.0, "val_loss": 15.855561196804047, "val_acc": 28.0}
{"epoch": 7, "training_loss": 63.26179385185242, "training_acc": 72.0, "val_loss": 15.642441809177399, "val_acc": 28.0}
{"epoch": 8, "training_loss": 62.5523099899292, "training_acc": 72.0, "val_loss": 15.460357069969177, "val_acc": 28.0}
{"epoch": 9, "training_loss": 61.760005712509155, "training_acc": 72.0, "val_loss": 15.31020849943161, "val_acc": 72.0}
{"epoch": 10, "training_loss": 61.207441568374634, "training_acc": 72.0, "val_loss": 15.186227858066559, "val_acc": 72.0}
{"epoch": 11, "training_loss": 60.76632761955261, "training_acc": 72.0, "val_loss": 15.086080133914948, "val_acc": 72.0}
{"epoch": 12, "training_loss": 60.29593563079834, "training_acc": 72.0, "val_loss": 15.009269118309021, "val_acc": 72.0}
{"epoch": 13, "training_loss": 59.98600721359253, "training_acc": 72.0, "val_loss": 14.948056638240814, "val_acc": 72.0}
{"epoch": 14, "training_loss": 59.72640800476074, "training_acc": 72.0, "val_loss": 14.901398122310638, "val_acc": 72.0}
{"epoch": 15, "training_loss": 59.664653062820435, "training_acc": 72.0, "val_loss": 14.867107570171356, "val_acc": 72.0}
{"epoch": 16, "training_loss": 59.4612352848053, "training_acc": 72.0, "val_loss": 14.844459295272827, "val_acc": 72.0}
{"epoch": 17, "training_loss": 59.36224174499512, "training_acc": 72.0, "val_loss": 14.829063415527344, "val_acc": 72.0}
{"epoch": 18, "training_loss": 59.327868700027466, "training_acc": 72.0, "val_loss": 14.819437265396118, "val_acc": 72.0}
{"epoch": 19, "training_loss": 59.30073094367981, "training_acc": 72.0, "val_loss": 14.814208447933197, "val_acc": 72.0}
{"epoch": 20, "training_loss": 59.28820466995239, "training_acc": 72.0, "val_loss": 14.81226533651352, "val_acc": 72.0}
{"epoch": 21, "training_loss": 59.25401782989502, "training_acc": 72.0, "val_loss": 14.812211692333221, "val_acc": 72.0}
{"epoch": 22, "training_loss": 59.28842282295227, "training_acc": 72.0, "val_loss": 14.812985062599182, "val_acc": 72.0}
{"epoch": 23, "training_loss": 59.27368950843811, "training_acc": 72.0, "val_loss": 14.813992381095886, "val_acc": 72.0}
{"epoch": 24, "training_loss": 59.247169733047485, "training_acc": 72.0, "val_loss": 14.815258979797363, "val_acc": 72.0}
{"epoch": 25, "training_loss": 59.27064394950867, "training_acc": 72.0, "val_loss": 14.81655240058899, "val_acc": 72.0}
{"epoch": 26, "training_loss": 59.284045457839966, "training_acc": 72.0, "val_loss": 14.817389845848083, "val_acc": 72.0}
{"epoch": 27, "training_loss": 59.304832220077515, "training_acc": 72.0, "val_loss": 14.81807678937912, "val_acc": 72.0}
{"epoch": 28, "training_loss": 59.2928364276886, "training_acc": 72.0, "val_loss": 14.818370342254639, "val_acc": 72.0}
{"epoch": 29, "training_loss": 59.29479217529297, "training_acc": 72.0, "val_loss": 14.818745851516724, "val_acc": 72.0}
{"epoch": 30, "training_loss": 59.29433274269104, "training_acc": 72.0, "val_loss": 14.8185595870018, "val_acc": 72.0}
{"epoch": 31, "training_loss": 59.29732966423035, "training_acc": 72.0, "val_loss": 14.817886054515839, "val_acc": 72.0}
{"epoch": 32, "training_loss": 59.309836864471436, "training_acc": 72.0, "val_loss": 14.817462861537933, "val_acc": 72.0}
{"epoch": 33, "training_loss": 59.27497124671936, "training_acc": 72.0, "val_loss": 14.817056059837341, "val_acc": 72.0}
{"epoch": 34, "training_loss": 59.30490446090698, "training_acc": 72.0, "val_loss": 14.816460013389587, "val_acc": 72.0}
{"epoch": 35, "training_loss": 59.31941246986389, "training_acc": 72.0, "val_loss": 14.816327393054962, "val_acc": 72.0}
{"epoch": 36, "training_loss": 59.292635440826416, "training_acc": 72.0, "val_loss": 14.815863966941833, "val_acc": 72.0}
{"epoch": 37, "training_loss": 59.295250415802, "training_acc": 72.0, "val_loss": 14.814826846122742, "val_acc": 72.0}
{"epoch": 38, "training_loss": 59.28761005401611, "training_acc": 72.0, "val_loss": 14.81408178806305, "val_acc": 72.0}
{"epoch": 39, "training_loss": 59.30235147476196, "training_acc": 72.0, "val_loss": 14.813560247421265, "val_acc": 72.0}
{"epoch": 40, "training_loss": 59.25418448448181, "training_acc": 72.0, "val_loss": 14.813074469566345, "val_acc": 72.0}
