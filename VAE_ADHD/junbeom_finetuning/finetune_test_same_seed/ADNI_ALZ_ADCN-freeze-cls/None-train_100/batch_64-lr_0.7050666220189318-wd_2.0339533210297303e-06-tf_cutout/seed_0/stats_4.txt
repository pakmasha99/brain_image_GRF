"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 2758.99751663208, "training_acc": 46.0, "val_loss": 1894.6435928344727, "val_acc": 72.0}
{"epoch": 1, "training_loss": 6522.767608642578, "training_acc": 72.0, "val_loss": 1117.960262298584, "val_acc": 28.0}
{"epoch": 2, "training_loss": 3299.751392364502, "training_acc": 28.0, "val_loss": 787.7043724060059, "val_acc": 72.0}
{"epoch": 3, "training_loss": 3547.8312759399414, "training_acc": 72.0, "val_loss": 1507.918643951416, "val_acc": 72.0}
{"epoch": 4, "training_loss": 6019.881790161133, "training_acc": 72.0, "val_loss": 1264.958381652832, "val_acc": 72.0}
{"epoch": 5, "training_loss": 4374.898391723633, "training_acc": 72.0, "val_loss": 260.84160804748535, "val_acc": 72.0}
{"epoch": 6, "training_loss": 2457.288589477539, "training_acc": 56.0, "val_loss": 1529.5595169067383, "val_acc": 28.0}
{"epoch": 7, "training_loss": 4898.471633911133, "training_acc": 28.0, "val_loss": 435.67638397216797, "val_acc": 72.0}
{"epoch": 8, "training_loss": 2713.8766021728516, "training_acc": 72.0, "val_loss": 1136.0292434692383, "val_acc": 72.0}
{"epoch": 9, "training_loss": 4569.345245361328, "training_acc": 72.0, "val_loss": 1051.554012298584, "val_acc": 72.0}
{"epoch": 10, "training_loss": 3850.3337173461914, "training_acc": 72.0, "val_loss": 421.01597785949707, "val_acc": 72.0}
{"epoch": 11, "training_loss": 1471.9795417785645, "training_acc": 54.0, "val_loss": 315.102744102478, "val_acc": 28.0}
{"epoch": 12, "training_loss": 992.3727207183838, "training_acc": 48.0, "val_loss": 365.7735824584961, "val_acc": 72.0}
{"epoch": 13, "training_loss": 1443.3604373931885, "training_acc": 72.0, "val_loss": 240.8630609512329, "val_acc": 72.0}
{"epoch": 14, "training_loss": 621.4635834693909, "training_acc": 72.0, "val_loss": 810.4719161987305, "val_acc": 28.0}
{"epoch": 15, "training_loss": 2976.5109481811523, "training_acc": 28.0, "val_loss": 163.80622386932373, "val_acc": 72.0}
{"epoch": 16, "training_loss": 1073.7358856201172, "training_acc": 72.0, "val_loss": 463.7126922607422, "val_acc": 72.0}
{"epoch": 17, "training_loss": 1727.5702476501465, "training_acc": 72.0, "val_loss": 155.45644760131836, "val_acc": 72.0}
{"epoch": 18, "training_loss": 1004.0808563232422, "training_acc": 58.0, "val_loss": 324.2178201675415, "val_acc": 28.0}
{"epoch": 19, "training_loss": 1477.60986328125, "training_acc": 36.0, "val_loss": 476.53393745422363, "val_acc": 72.0}
{"epoch": 20, "training_loss": 2014.996681213379, "training_acc": 72.0, "val_loss": 413.5835647583008, "val_acc": 72.0}
{"epoch": 21, "training_loss": 1328.059061050415, "training_acc": 72.0, "val_loss": 489.98098373413086, "val_acc": 28.0}
{"epoch": 22, "training_loss": 1918.8224792480469, "training_acc": 28.0, "val_loss": 205.74204921722412, "val_acc": 72.0}
{"epoch": 23, "training_loss": 1093.6057014465332, "training_acc": 72.0, "val_loss": 488.3248805999756, "val_acc": 72.0}
{"epoch": 24, "training_loss": 1868.2313537597656, "training_acc": 72.0, "val_loss": 227.13871002197266, "val_acc": 72.0}
{"epoch": 25, "training_loss": 1034.136375427246, "training_acc": 54.0, "val_loss": 131.7456603050232, "val_acc": 28.0}
{"epoch": 26, "training_loss": 688.7996101379395, "training_acc": 48.0, "val_loss": 547.8999137878418, "val_acc": 72.0}
{"epoch": 27, "training_loss": 2245.778793334961, "training_acc": 72.0, "val_loss": 559.4740867614746, "val_acc": 72.0}
{"epoch": 28, "training_loss": 2006.5994453430176, "training_acc": 72.0, "val_loss": 96.23744487762451, "val_acc": 72.0}
{"epoch": 29, "training_loss": 1177.4595413208008, "training_acc": 60.0, "val_loss": 759.3203544616699, "val_acc": 28.0}
{"epoch": 30, "training_loss": 2037.4957065582275, "training_acc": 42.0, "val_loss": 190.3567671775818, "val_acc": 72.0}
{"epoch": 31, "training_loss": 743.8795833587646, "training_acc": 72.0, "val_loss": 72.86877036094666, "val_acc": 72.0}
{"epoch": 32, "training_loss": 858.2254791259766, "training_acc": 52.0, "val_loss": 23.868542909622192, "val_acc": 28.0}
{"epoch": 33, "training_loss": 552.2323951721191, "training_acc": 44.0, "val_loss": 530.2333831787109, "val_acc": 72.0}
{"epoch": 34, "training_loss": 2251.3950805664062, "training_acc": 72.0, "val_loss": 469.0401077270508, "val_acc": 72.0}
{"epoch": 35, "training_loss": 1546.6288433074951, "training_acc": 72.0, "val_loss": 349.7441053390503, "val_acc": 28.0}
{"epoch": 36, "training_loss": 1239.3296279907227, "training_acc": 28.0, "val_loss": 257.60350227355957, "val_acc": 72.0}
{"epoch": 37, "training_loss": 1251.4891357421875, "training_acc": 72.0, "val_loss": 467.20895767211914, "val_acc": 72.0}
{"epoch": 38, "training_loss": 1722.3178787231445, "training_acc": 72.0, "val_loss": 144.4752812385559, "val_acc": 72.0}
{"epoch": 39, "training_loss": 946.8127212524414, "training_acc": 60.0, "val_loss": 398.7725019454956, "val_acc": 28.0}
{"epoch": 40, "training_loss": 1378.868953704834, "training_acc": 42.0, "val_loss": 397.68786430358887, "val_acc": 72.0}
{"epoch": 41, "training_loss": 1616.0367584228516, "training_acc": 72.0, "val_loss": 333.0399513244629, "val_acc": 72.0}
{"epoch": 42, "training_loss": 978.5215721130371, "training_acc": 72.0, "val_loss": 508.9973449707031, "val_acc": 28.0}
{"epoch": 43, "training_loss": 1996.5478515625, "training_acc": 28.0, "val_loss": 199.25142526626587, "val_acc": 72.0}
{"epoch": 44, "training_loss": 993.2352485656738, "training_acc": 72.0, "val_loss": 481.5895080566406, "val_acc": 72.0}
{"epoch": 45, "training_loss": 1843.8879470825195, "training_acc": 72.0, "val_loss": 242.16415882110596, "val_acc": 72.0}
{"epoch": 46, "training_loss": 731.3285427093506, "training_acc": 58.0, "val_loss": 28.897979855537415, "val_acc": 72.0}
{"epoch": 47, "training_loss": 148.53393173217773, "training_acc": 58.0, "val_loss": 234.13922786712646, "val_acc": 72.0}
{"epoch": 48, "training_loss": 1106.8104858398438, "training_acc": 72.0, "val_loss": 295.4453706741333, "val_acc": 72.0}
{"epoch": 49, "training_loss": 894.0606365203857, "training_acc": 72.0, "val_loss": 429.64935302734375, "val_acc": 28.0}
{"epoch": 50, "training_loss": 1479.891284942627, "training_acc": 28.0, "val_loss": 287.64331340789795, "val_acc": 72.0}
{"epoch": 51, "training_loss": 1463.1580047607422, "training_acc": 72.0, "val_loss": 572.197675704956, "val_acc": 72.0}
