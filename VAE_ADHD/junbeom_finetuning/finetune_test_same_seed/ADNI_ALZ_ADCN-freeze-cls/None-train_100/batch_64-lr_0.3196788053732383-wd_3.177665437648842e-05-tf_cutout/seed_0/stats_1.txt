"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 728.8343124389648, "training_acc": 72.0, "val_loss": 765.6775951385498, "val_acc": 72.0}
{"epoch": 1, "training_loss": 2127.3123874664307, "training_acc": 72.0, "val_loss": 1460.8129501342773, "val_acc": 28.0}
{"epoch": 2, "training_loss": 5376.885559082031, "training_acc": 28.0, "val_loss": 88.43632340431213, "val_acc": 28.0}
{"epoch": 3, "training_loss": 889.0099792480469, "training_acc": 46.0, "val_loss": 908.1448554992676, "val_acc": 72.0}
{"epoch": 4, "training_loss": 3895.231658935547, "training_acc": 72.0, "val_loss": 1261.991786956787, "val_acc": 72.0}
{"epoch": 5, "training_loss": 5042.860572814941, "training_acc": 72.0, "val_loss": 1201.225471496582, "val_acc": 72.0}
{"epoch": 6, "training_loss": 4618.874298095703, "training_acc": 72.0, "val_loss": 865.8317565917969, "val_acc": 72.0}
{"epoch": 7, "training_loss": 3149.214874267578, "training_acc": 72.0, "val_loss": 272.49510288238525, "val_acc": 72.0}
{"epoch": 8, "training_loss": 1054.7859992980957, "training_acc": 56.0, "val_loss": 507.03229904174805, "val_acc": 28.0}
{"epoch": 9, "training_loss": 1655.4555702209473, "training_acc": 28.0, "val_loss": 162.3274803161621, "val_acc": 72.0}
{"epoch": 10, "training_loss": 869.9696426391602, "training_acc": 72.0, "val_loss": 427.02226638793945, "val_acc": 72.0}
{"epoch": 11, "training_loss": 1732.0879592895508, "training_acc": 72.0, "val_loss": 389.47832584381104, "val_acc": 72.0}
{"epoch": 12, "training_loss": 1462.2024307250977, "training_acc": 72.0, "val_loss": 96.39005661010742, "val_acc": 72.0}
{"epoch": 13, "training_loss": 675.1470146179199, "training_acc": 56.0, "val_loss": 324.97503757476807, "val_acc": 28.0}
{"epoch": 14, "training_loss": 819.7909481525421, "training_acc": 48.0, "val_loss": 113.96801471710205, "val_acc": 72.0}
{"epoch": 15, "training_loss": 457.34541034698486, "training_acc": 72.0, "val_loss": 82.99636840820312, "val_acc": 72.0}
{"epoch": 16, "training_loss": 287.482186794281, "training_acc": 54.0, "val_loss": 46.072760224342346, "val_acc": 72.0}
{"epoch": 17, "training_loss": 172.52679586410522, "training_acc": 72.0, "val_loss": 87.10401654243469, "val_acc": 28.0}
{"epoch": 18, "training_loss": 277.92508602142334, "training_acc": 48.0, "val_loss": 88.883376121521, "val_acc": 72.0}
{"epoch": 19, "training_loss": 307.8098793029785, "training_acc": 72.0, "val_loss": 106.88502788543701, "val_acc": 28.0}
{"epoch": 20, "training_loss": 330.2644944190979, "training_acc": 42.0, "val_loss": 32.91553556919098, "val_acc": 72.0}
{"epoch": 21, "training_loss": 170.00295639038086, "training_acc": 58.0, "val_loss": 72.12100028991699, "val_acc": 72.0}
{"epoch": 22, "training_loss": 317.49449634552, "training_acc": 72.0, "val_loss": 56.661397218704224, "val_acc": 72.0}
{"epoch": 23, "training_loss": 294.25212478637695, "training_acc": 56.0, "val_loss": 33.771926164627075, "val_acc": 72.0}
{"epoch": 24, "training_loss": 139.48082733154297, "training_acc": 72.0, "val_loss": 43.798357248306274, "val_acc": 28.0}
{"epoch": 25, "training_loss": 203.00629138946533, "training_acc": 46.0, "val_loss": 97.61443734169006, "val_acc": 72.0}
{"epoch": 26, "training_loss": 334.8202757835388, "training_acc": 72.0, "val_loss": 108.84063243865967, "val_acc": 28.0}
{"epoch": 27, "training_loss": 372.68263721466064, "training_acc": 38.0, "val_loss": 37.24376857280731, "val_acc": 72.0}
{"epoch": 28, "training_loss": 221.19359302520752, "training_acc": 54.0, "val_loss": 69.5886492729187, "val_acc": 72.0}
{"epoch": 29, "training_loss": 305.731915473938, "training_acc": 72.0, "val_loss": 63.30190300941467, "val_acc": 72.0}
{"epoch": 30, "training_loss": 335.73859214782715, "training_acc": 50.0, "val_loss": 51.07506513595581, "val_acc": 72.0}
{"epoch": 31, "training_loss": 223.48594856262207, "training_acc": 72.0, "val_loss": 22.824233770370483, "val_acc": 72.0}
{"epoch": 32, "training_loss": 346.1904411315918, "training_acc": 54.0, "val_loss": 14.17793482542038, "val_acc": 72.0}
{"epoch": 33, "training_loss": 178.91021347045898, "training_acc": 72.0, "val_loss": 100.83334445953369, "val_acc": 72.0}
{"epoch": 34, "training_loss": 326.7594165802002, "training_acc": 72.0, "val_loss": 152.07678079605103, "val_acc": 28.0}
{"epoch": 35, "training_loss": 418.6630985736847, "training_acc": 28.0, "val_loss": 125.2402663230896, "val_acc": 72.0}
{"epoch": 36, "training_loss": 582.7384643554688, "training_acc": 72.0, "val_loss": 169.62385177612305, "val_acc": 72.0}
{"epoch": 37, "training_loss": 564.2446174621582, "training_acc": 72.0, "val_loss": 73.74783158302307, "val_acc": 28.0}
{"epoch": 38, "training_loss": 219.06255197525024, "training_acc": 28.0, "val_loss": 142.82822608947754, "val_acc": 72.0}
{"epoch": 39, "training_loss": 699.136302947998, "training_acc": 72.0, "val_loss": 226.88913345336914, "val_acc": 72.0}
{"epoch": 40, "training_loss": 847.7437734603882, "training_acc": 72.0, "val_loss": 51.40025019645691, "val_acc": 72.0}
{"epoch": 41, "training_loss": 416.13232421875, "training_acc": 62.0, "val_loss": 206.2243938446045, "val_acc": 28.0}
{"epoch": 42, "training_loss": 625.1494569778442, "training_acc": 46.0, "val_loss": 179.03437614440918, "val_acc": 72.0}
{"epoch": 43, "training_loss": 751.6811790466309, "training_acc": 72.0, "val_loss": 160.99321842193604, "val_acc": 72.0}
{"epoch": 44, "training_loss": 524.2499485015869, "training_acc": 72.0, "val_loss": 233.47704410552979, "val_acc": 28.0}
{"epoch": 45, "training_loss": 816.3100528717041, "training_acc": 28.0, "val_loss": 122.175133228302, "val_acc": 72.0}
{"epoch": 46, "training_loss": 582.9624691009521, "training_acc": 72.0, "val_loss": 253.76460552215576, "val_acc": 72.0}
{"epoch": 47, "training_loss": 981.210132598877, "training_acc": 72.0, "val_loss": 141.7096972465515, "val_acc": 72.0}
{"epoch": 48, "training_loss": 415.24320816993713, "training_acc": 54.0, "val_loss": 137.35620975494385, "val_acc": 28.0}
{"epoch": 49, "training_loss": 415.3867406845093, "training_acc": 46.0, "val_loss": 100.57733058929443, "val_acc": 72.0}
{"epoch": 50, "training_loss": 378.3265037536621, "training_acc": 72.0, "val_loss": 18.123511970043182, "val_acc": 28.0}
{"epoch": 51, "training_loss": 130.3443374633789, "training_acc": 38.0, "val_loss": 116.47334098815918, "val_acc": 72.0}
