"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 4850.229209899902, "training_acc": 44.0, "val_loss": 2926.9994735717773, "val_acc": 72.0}
{"epoch": 1, "training_loss": 8920.323364257812, "training_acc": 72.0, "val_loss": 1617.1415328979492, "val_acc": 28.0}
{"epoch": 2, "training_loss": 5336.005630493164, "training_acc": 28.0, "val_loss": 955.3762435913086, "val_acc": 72.0}
{"epoch": 3, "training_loss": 4658.936721801758, "training_acc": 72.0, "val_loss": 1828.3935546875, "val_acc": 72.0}
{"epoch": 4, "training_loss": 7062.128021240234, "training_acc": 72.0, "val_loss": 1146.4674949645996, "val_acc": 72.0}
{"epoch": 5, "training_loss": 3216.834297180176, "training_acc": 72.0, "val_loss": 1696.910285949707, "val_acc": 28.0}
{"epoch": 6, "training_loss": 7206.072692871094, "training_acc": 28.0, "val_loss": 355.42657375335693, "val_acc": 28.0}
{"epoch": 7, "training_loss": 2922.2317504882812, "training_acc": 38.0, "val_loss": 1799.3637084960938, "val_acc": 72.0}
{"epoch": 8, "training_loss": 7890.020660400391, "training_acc": 72.0, "val_loss": 2372.366142272949, "val_acc": 72.0}
{"epoch": 9, "training_loss": 9311.769439697266, "training_acc": 72.0, "val_loss": 1895.672607421875, "val_acc": 72.0}
{"epoch": 10, "training_loss": 6798.12190246582, "training_acc": 72.0, "val_loss": 611.7343425750732, "val_acc": 72.0}
{"epoch": 11, "training_loss": 2315.87003326416, "training_acc": 60.0, "val_loss": 1218.411922454834, "val_acc": 28.0}
{"epoch": 12, "training_loss": 3373.897819519043, "training_acc": 28.0, "val_loss": 821.3394165039062, "val_acc": 72.0}
{"epoch": 13, "training_loss": 4272.764205932617, "training_acc": 72.0, "val_loss": 1780.5143356323242, "val_acc": 72.0}
{"epoch": 14, "training_loss": 7422.600341796875, "training_acc": 72.0, "val_loss": 1722.237777709961, "val_acc": 72.0}
{"epoch": 15, "training_loss": 6401.656814575195, "training_acc": 72.0, "val_loss": 735.2219581604004, "val_acc": 72.0}
{"epoch": 16, "training_loss": 2034.0375390052795, "training_acc": 54.0, "val_loss": 706.7400455474854, "val_acc": 28.0}
{"epoch": 17, "training_loss": 1866.1455473899841, "training_acc": 44.0, "val_loss": 115.6913161277771, "val_acc": 72.0}
{"epoch": 18, "training_loss": 713.1124153137207, "training_acc": 48.0, "val_loss": 368.1473970413208, "val_acc": 72.0}
{"epoch": 19, "training_loss": 1632.0, "training_acc": 72.0, "val_loss": 528.1156539916992, "val_acc": 72.0}
{"epoch": 20, "training_loss": 1817.78853225708, "training_acc": 72.0, "val_loss": 245.06232738494873, "val_acc": 28.0}
{"epoch": 21, "training_loss": 618.1105256080627, "training_acc": 52.0, "val_loss": 49.7256338596344, "val_acc": 72.0}
{"epoch": 22, "training_loss": 721.8510971069336, "training_acc": 56.0, "val_loss": 195.36837339401245, "val_acc": 72.0}
{"epoch": 23, "training_loss": 1009.6143264770508, "training_acc": 72.0, "val_loss": 196.92165851593018, "val_acc": 72.0}
{"epoch": 24, "training_loss": 1013.7075729370117, "training_acc": 56.0, "val_loss": 108.39310884475708, "val_acc": 72.0}
{"epoch": 25, "training_loss": 490.16668128967285, "training_acc": 72.0, "val_loss": 196.69301509857178, "val_acc": 28.0}
{"epoch": 26, "training_loss": 1163.0566711425781, "training_acc": 36.0, "val_loss": 364.8564577102661, "val_acc": 72.0}
{"epoch": 27, "training_loss": 1248.624843597412, "training_acc": 72.0, "val_loss": 377.22322940826416, "val_acc": 28.0}
{"epoch": 28, "training_loss": 1094.401888847351, "training_acc": 44.0, "val_loss": 99.8846709728241, "val_acc": 72.0}
{"epoch": 29, "training_loss": 695.7923126220703, "training_acc": 54.0, "val_loss": 250.8373737335205, "val_acc": 72.0}
{"epoch": 30, "training_loss": 1112.653148651123, "training_acc": 72.0, "val_loss": 271.0688352584839, "val_acc": 72.0}
{"epoch": 31, "training_loss": 1281.4735374450684, "training_acc": 44.0, "val_loss": 262.40649223327637, "val_acc": 72.0}
{"epoch": 32, "training_loss": 1226.8464736938477, "training_acc": 72.0, "val_loss": 253.8930892944336, "val_acc": 72.0}
{"epoch": 33, "training_loss": 803.2175750732422, "training_acc": 58.0, "val_loss": 181.0766577720642, "val_acc": 72.0}
{"epoch": 34, "training_loss": 769.1455497741699, "training_acc": 72.0, "val_loss": 27.457523345947266, "val_acc": 28.0}
{"epoch": 35, "training_loss": 311.88017082214355, "training_acc": 45.0, "val_loss": 123.3138918876648, "val_acc": 72.0}
{"epoch": 36, "training_loss": 694.8521118164062, "training_acc": 58.0, "val_loss": 186.70486211776733, "val_acc": 72.0}
{"epoch": 37, "training_loss": 964.0387191772461, "training_acc": 72.0, "val_loss": 106.344735622406, "val_acc": 72.0}
{"epoch": 38, "training_loss": 1008.7625274658203, "training_acc": 62.0, "val_loss": 185.85584163665771, "val_acc": 28.0}
{"epoch": 39, "training_loss": 1227.3640365600586, "training_acc": 46.0, "val_loss": 1005.5576324462891, "val_acc": 72.0}
{"epoch": 40, "training_loss": 4151.214378356934, "training_acc": 72.0, "val_loss": 1112.1943473815918, "val_acc": 72.0}
{"epoch": 41, "training_loss": 4126.7568359375, "training_acc": 72.0, "val_loss": 456.2371253967285, "val_acc": 72.0}
{"epoch": 42, "training_loss": 1258.1557025909424, "training_acc": 62.0, "val_loss": 248.4821319580078, "val_acc": 28.0}
{"epoch": 43, "training_loss": 1161.2542877197266, "training_acc": 46.0, "val_loss": 692.1708106994629, "val_acc": 72.0}
{"epoch": 44, "training_loss": 2783.940155029297, "training_acc": 72.0, "val_loss": 557.1663856506348, "val_acc": 72.0}
{"epoch": 45, "training_loss": 1838.6774291992188, "training_acc": 72.0, "val_loss": 819.1370964050293, "val_acc": 28.0}
{"epoch": 46, "training_loss": 2952.8296966552734, "training_acc": 28.0, "val_loss": 399.82402324676514, "val_acc": 72.0}
{"epoch": 47, "training_loss": 2259.8335571289062, "training_acc": 72.0, "val_loss": 888.2660865783691, "val_acc": 72.0}
{"epoch": 48, "training_loss": 3377.629348754883, "training_acc": 72.0, "val_loss": 449.67031478881836, "val_acc": 72.0}
{"epoch": 49, "training_loss": 1843.8805198669434, "training_acc": 48.0, "val_loss": 21.494115889072418, "val_acc": 72.0}
{"epoch": 50, "training_loss": 270.20825004577637, "training_acc": 52.0, "val_loss": 446.0111141204834, "val_acc": 72.0}
{"epoch": 51, "training_loss": 2131.0893630981445, "training_acc": 72.0, "val_loss": 708.8303089141846, "val_acc": 72.0}
{"epoch": 52, "training_loss": 2531.2137298583984, "training_acc": 72.0, "val_loss": 132.81404972076416, "val_acc": 72.0}
{"epoch": 53, "training_loss": 2177.124984741211, "training_acc": 54.0, "val_loss": 1163.2560729980469, "val_acc": 28.0}
{"epoch": 54, "training_loss": 3404.68625831604, "training_acc": 38.0, "val_loss": 378.2684326171875, "val_acc": 72.0}
{"epoch": 55, "training_loss": 1519.8849487304688, "training_acc": 72.0, "val_loss": 212.83040046691895, "val_acc": 72.0}
{"epoch": 56, "training_loss": 1015.0448722839355, "training_acc": 58.0, "val_loss": 51.19089484214783, "val_acc": 72.0}
{"epoch": 57, "training_loss": 182.60211944580078, "training_acc": 72.0, "val_loss": 527.6716709136963, "val_acc": 28.0}
{"epoch": 58, "training_loss": 1555.3898735046387, "training_acc": 42.0, "val_loss": 163.1425380706787, "val_acc": 72.0}
{"epoch": 59, "training_loss": 503.253182888031, "training_acc": 56.0, "val_loss": 219.68653202056885, "val_acc": 72.0}
{"epoch": 60, "training_loss": 944.1153984069824, "training_acc": 72.0, "val_loss": 24.001558125019073, "val_acc": 72.0}
{"epoch": 61, "training_loss": 1723.6501541137695, "training_acc": 50.0, "val_loss": 472.261381149292, "val_acc": 28.0}
{"epoch": 62, "training_loss": 1639.0802192687988, "training_acc": 50.0, "val_loss": 974.2579460144043, "val_acc": 72.0}
{"epoch": 63, "training_loss": 4158.681976318359, "training_acc": 72.0, "val_loss": 1188.5214805603027, "val_acc": 72.0}
{"epoch": 64, "training_loss": 4440.503707885742, "training_acc": 72.0, "val_loss": 589.708423614502, "val_acc": 72.0}
{"epoch": 65, "training_loss": 1465.427836894989, "training_acc": 72.0, "val_loss": 1764.193344116211, "val_acc": 28.0}
{"epoch": 66, "training_loss": 7410.929229736328, "training_acc": 28.0, "val_loss": 898.3513832092285, "val_acc": 28.0}
{"epoch": 67, "training_loss": 3558.4423828125, "training_acc": 38.0, "val_loss": 1181.783390045166, "val_acc": 72.0}
{"epoch": 68, "training_loss": 5246.8558349609375, "training_acc": 72.0, "val_loss": 1592.6862716674805, "val_acc": 72.0}
