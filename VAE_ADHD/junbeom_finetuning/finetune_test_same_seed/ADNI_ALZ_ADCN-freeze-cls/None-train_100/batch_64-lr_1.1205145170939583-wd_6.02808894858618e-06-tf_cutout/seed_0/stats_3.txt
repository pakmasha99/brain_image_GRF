"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 6277.808181762695, "training_acc": 38.0, "val_loss": 2760.0082397460938, "val_acc": 72.0}
{"epoch": 1, "training_loss": 9223.764236450195, "training_acc": 72.0, "val_loss": 1967.0759201049805, "val_acc": 28.0}
{"epoch": 2, "training_loss": 6511.428283691406, "training_acc": 28.0, "val_loss": 921.9527244567871, "val_acc": 72.0}
{"epoch": 3, "training_loss": 4465.176788330078, "training_acc": 72.0, "val_loss": 1894.4866180419922, "val_acc": 72.0}
{"epoch": 4, "training_loss": 7408.083236694336, "training_acc": 72.0, "val_loss": 1334.843921661377, "val_acc": 72.0}
{"epoch": 5, "training_loss": 4414.348129272461, "training_acc": 72.0, "val_loss": 819.6293830871582, "val_acc": 28.0}
{"epoch": 6, "training_loss": 3113.8300552368164, "training_acc": 28.0, "val_loss": 412.7762794494629, "val_acc": 72.0}
{"epoch": 7, "training_loss": 2211.1658630371094, "training_acc": 72.0, "val_loss": 797.1292495727539, "val_acc": 72.0}
{"epoch": 8, "training_loss": 2860.2531661987305, "training_acc": 72.0, "val_loss": 77.50298976898193, "val_acc": 72.0}
{"epoch": 9, "training_loss": 2744.0550842285156, "training_acc": 52.0, "val_loss": 1404.0327072143555, "val_acc": 28.0}
{"epoch": 10, "training_loss": 3340.243615627289, "training_acc": 52.0, "val_loss": 495.2253818511963, "val_acc": 72.0}
{"epoch": 11, "training_loss": 2186.760223388672, "training_acc": 72.0, "val_loss": 535.3359699249268, "val_acc": 72.0}
{"epoch": 12, "training_loss": 1560.1803436279297, "training_acc": 72.0, "val_loss": 820.0066566467285, "val_acc": 28.0}
{"epoch": 13, "training_loss": 3049.0927505493164, "training_acc": 28.0, "val_loss": 361.1285209655762, "val_acc": 72.0}
{"epoch": 14, "training_loss": 1862.9549255371094, "training_acc": 72.0, "val_loss": 785.7102870941162, "val_acc": 72.0}
{"epoch": 15, "training_loss": 2953.7574768066406, "training_acc": 72.0, "val_loss": 306.46753311157227, "val_acc": 72.0}
{"epoch": 16, "training_loss": 2384.3722076416016, "training_acc": 44.0, "val_loss": 330.46820163726807, "val_acc": 28.0}
{"epoch": 17, "training_loss": 1740.1245498657227, "training_acc": 42.0, "val_loss": 953.0044555664062, "val_acc": 72.0}
{"epoch": 18, "training_loss": 4020.344192504883, "training_acc": 72.0, "val_loss": 1022.496223449707, "val_acc": 72.0}
{"epoch": 19, "training_loss": 3710.420341491699, "training_acc": 72.0, "val_loss": 268.3603525161743, "val_acc": 72.0}
{"epoch": 20, "training_loss": 1904.6096801757812, "training_acc": 58.0, "val_loss": 960.3821754455566, "val_acc": 28.0}
{"epoch": 21, "training_loss": 2653.7476749420166, "training_acc": 44.0, "val_loss": 395.0261116027832, "val_acc": 72.0}
{"epoch": 22, "training_loss": 1656.5034942626953, "training_acc": 72.0, "val_loss": 229.7640562057495, "val_acc": 72.0}
{"epoch": 23, "training_loss": 1661.9802551269531, "training_acc": 48.0, "val_loss": 16.18351936340332, "val_acc": 28.0}
{"epoch": 24, "training_loss": 556.4224700927734, "training_acc": 67.0, "val_loss": 480.014705657959, "val_acc": 72.0}
{"epoch": 25, "training_loss": 1790.7071628570557, "training_acc": 72.0, "val_loss": 68.42843294143677, "val_acc": 72.0}
{"epoch": 26, "training_loss": 1282.2434005737305, "training_acc": 62.0, "val_loss": 602.6038646697998, "val_acc": 28.0}
{"epoch": 27, "training_loss": 2002.4945678710938, "training_acc": 46.0, "val_loss": 760.4719638824463, "val_acc": 72.0}
{"epoch": 28, "training_loss": 3224.5225830078125, "training_acc": 72.0, "val_loss": 795.4012393951416, "val_acc": 72.0}
{"epoch": 29, "training_loss": 2754.628547668457, "training_acc": 72.0, "val_loss": 18.104730546474457, "val_acc": 72.0}
{"epoch": 30, "training_loss": 1927.1173248291016, "training_acc": 60.0, "val_loss": 1408.1535339355469, "val_acc": 28.0}
{"epoch": 31, "training_loss": 3698.6440320014954, "training_acc": 42.0, "val_loss": 563.9330387115479, "val_acc": 72.0}
{"epoch": 32, "training_loss": 2674.7943572998047, "training_acc": 72.0, "val_loss": 913.4879112243652, "val_acc": 72.0}
{"epoch": 33, "training_loss": 3435.539863586426, "training_acc": 72.0, "val_loss": 381.9179058074951, "val_acc": 72.0}
{"epoch": 34, "training_loss": 1629.6510696411133, "training_acc": 54.0, "val_loss": 156.5917730331421, "val_acc": 28.0}
{"epoch": 35, "training_loss": 1528.3529434204102, "training_acc": 38.0, "val_loss": 915.291690826416, "val_acc": 72.0}
{"epoch": 36, "training_loss": 3672.632522583008, "training_acc": 72.0, "val_loss": 846.4704513549805, "val_acc": 72.0}
{"epoch": 37, "training_loss": 3021.747024536133, "training_acc": 72.0, "val_loss": 72.28025794029236, "val_acc": 72.0}
{"epoch": 38, "training_loss": 2518.0596923828125, "training_acc": 52.0, "val_loss": 1460.430908203125, "val_acc": 28.0}
{"epoch": 39, "training_loss": 3776.772657394409, "training_acc": 28.0, "val_loss": 991.4664268493652, "val_acc": 72.0}
{"epoch": 40, "training_loss": 5363.644744873047, "training_acc": 72.0, "val_loss": 2154.8370361328125, "val_acc": 72.0}
{"epoch": 41, "training_loss": 8669.934158325195, "training_acc": 72.0, "val_loss": 2241.810417175293, "val_acc": 72.0}
{"epoch": 42, "training_loss": 8646.662719726562, "training_acc": 72.0, "val_loss": 1624.761962890625, "val_acc": 72.0}
