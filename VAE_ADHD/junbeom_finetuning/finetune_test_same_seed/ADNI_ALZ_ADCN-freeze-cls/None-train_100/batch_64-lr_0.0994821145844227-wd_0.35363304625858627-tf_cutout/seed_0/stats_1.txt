"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 343.1250801086426, "training_acc": 72.0, "val_loss": 219.82555389404297, "val_acc": 72.0}
{"epoch": 1, "training_loss": 659.7299387454987, "training_acc": 72.0, "val_loss": 484.8806381225586, "val_acc": 28.0}
{"epoch": 2, "training_loss": 1775.1074447631836, "training_acc": 28.0, "val_loss": 35.87410748004913, "val_acc": 28.0}
{"epoch": 3, "training_loss": 321.02860832214355, "training_acc": 44.0, "val_loss": 272.5545644760132, "val_acc": 72.0}
{"epoch": 4, "training_loss": 1157.7662544250488, "training_acc": 72.0, "val_loss": 356.68835639953613, "val_acc": 72.0}
{"epoch": 5, "training_loss": 1414.90185546875, "training_acc": 72.0, "val_loss": 311.215877532959, "val_acc": 72.0}
{"epoch": 6, "training_loss": 1180.5091228485107, "training_acc": 72.0, "val_loss": 171.17562294006348, "val_acc": 72.0}
{"epoch": 7, "training_loss": 564.2086362838745, "training_acc": 72.0, "val_loss": 71.64127230644226, "val_acc": 28.0}
{"epoch": 8, "training_loss": 368.87409019470215, "training_acc": 28.0, "val_loss": 29.599612951278687, "val_acc": 28.0}
{"epoch": 9, "training_loss": 111.83102798461914, "training_acc": 54.0, "val_loss": 108.96360874176025, "val_acc": 72.0}
{"epoch": 10, "training_loss": 464.02847480773926, "training_acc": 72.0, "val_loss": 122.97197580337524, "val_acc": 72.0}
{"epoch": 11, "training_loss": 457.234414100647, "training_acc": 72.0, "val_loss": 47.727084159851074, "val_acc": 72.0}
{"epoch": 12, "training_loss": 247.08488941192627, "training_acc": 46.0, "val_loss": 39.57954943180084, "val_acc": 28.0}
{"epoch": 13, "training_loss": 142.8556718826294, "training_acc": 46.0, "val_loss": 57.92997479438782, "val_acc": 72.0}
{"epoch": 14, "training_loss": 231.5310230255127, "training_acc": 72.0, "val_loss": 36.82197034358978, "val_acc": 72.0}
{"epoch": 15, "training_loss": 138.96759128570557, "training_acc": 52.0, "val_loss": 26.758715510368347, "val_acc": 28.0}
{"epoch": 16, "training_loss": 117.15479516983032, "training_acc": 40.0, "val_loss": 33.625102043151855, "val_acc": 72.0}
{"epoch": 17, "training_loss": 115.12643003463745, "training_acc": 72.0, "val_loss": 32.17422366142273, "val_acc": 28.0}
{"epoch": 18, "training_loss": 105.9252519607544, "training_acc": 42.0, "val_loss": 22.035764157772064, "val_acc": 72.0}
{"epoch": 19, "training_loss": 84.12342429161072, "training_acc": 72.0, "val_loss": 16.96685254573822, "val_acc": 28.0}
{"epoch": 20, "training_loss": 66.44595456123352, "training_acc": 72.0, "val_loss": 17.845679819583893, "val_acc": 72.0}
{"epoch": 21, "training_loss": 69.11905860900879, "training_acc": 72.0, "val_loss": 16.29195362329483, "val_acc": 28.0}
{"epoch": 22, "training_loss": 62.74246287345886, "training_acc": 72.0, "val_loss": 17.552123963832855, "val_acc": 72.0}
{"epoch": 23, "training_loss": 67.38609433174133, "training_acc": 72.0, "val_loss": 16.307595372200012, "val_acc": 28.0}
{"epoch": 24, "training_loss": 70.47935962677002, "training_acc": 72.0, "val_loss": 14.963313937187195, "val_acc": 72.0}
{"epoch": 25, "training_loss": 62.725703716278076, "training_acc": 72.0, "val_loss": 18.338438868522644, "val_acc": 72.0}
{"epoch": 26, "training_loss": 70.80338644981384, "training_acc": 72.0, "val_loss": 15.774807333946228, "val_acc": 28.0}
{"epoch": 27, "training_loss": 62.48533010482788, "training_acc": 72.0, "val_loss": 22.139757871627808, "val_acc": 72.0}
{"epoch": 28, "training_loss": 83.11600589752197, "training_acc": 72.0, "val_loss": 19.9671670794487, "val_acc": 28.0}
{"epoch": 29, "training_loss": 70.81234407424927, "training_acc": 46.0, "val_loss": 18.8061460852623, "val_acc": 72.0}
{"epoch": 30, "training_loss": 68.36561632156372, "training_acc": 72.0, "val_loss": 23.30968827009201, "val_acc": 28.0}
{"epoch": 31, "training_loss": 99.46522378921509, "training_acc": 40.0, "val_loss": 20.572230219841003, "val_acc": 72.0}
{"epoch": 32, "training_loss": 81.48583507537842, "training_acc": 56.0, "val_loss": 14.713309705257416, "val_acc": 72.0}
{"epoch": 33, "training_loss": 62.56651496887207, "training_acc": 72.0, "val_loss": 14.721383154392242, "val_acc": 72.0}
{"epoch": 34, "training_loss": 58.525158405303955, "training_acc": 72.0, "val_loss": 16.002289950847626, "val_acc": 28.0}
{"epoch": 35, "training_loss": 58.59763157367706, "training_acc": 72.0, "val_loss": 21.413075923919678, "val_acc": 72.0}
{"epoch": 36, "training_loss": 75.79551672935486, "training_acc": 72.0, "val_loss": 30.197817087173462, "val_acc": 28.0}
{"epoch": 37, "training_loss": 88.69214296340942, "training_acc": 52.0, "val_loss": 31.111392378807068, "val_acc": 72.0}
{"epoch": 38, "training_loss": 111.02276849746704, "training_acc": 72.0, "val_loss": 39.6779328584671, "val_acc": 28.0}
{"epoch": 39, "training_loss": 119.53641557693481, "training_acc": 46.0, "val_loss": 26.93704664707184, "val_acc": 72.0}
{"epoch": 40, "training_loss": 97.38577723503113, "training_acc": 72.0, "val_loss": 29.137659072875977, "val_acc": 28.0}
{"epoch": 41, "training_loss": 91.44750261306763, "training_acc": 48.0, "val_loss": 24.450281262397766, "val_acc": 72.0}
{"epoch": 42, "training_loss": 84.19012498855591, "training_acc": 72.0, "val_loss": 36.44188642501831, "val_acc": 28.0}
{"epoch": 43, "training_loss": 107.06669211387634, "training_acc": 50.0, "val_loss": 31.473124027252197, "val_acc": 72.0}
{"epoch": 44, "training_loss": 110.55902051925659, "training_acc": 72.0, "val_loss": 36.60605847835541, "val_acc": 28.0}
{"epoch": 45, "training_loss": 112.60597252845764, "training_acc": 46.0, "val_loss": 25.095871090888977, "val_acc": 72.0}
{"epoch": 46, "training_loss": 88.6203203201294, "training_acc": 72.0, "val_loss": 27.595427632331848, "val_acc": 28.0}
{"epoch": 47, "training_loss": 114.30182695388794, "training_acc": 40.0, "val_loss": 23.72414320707321, "val_acc": 72.0}
{"epoch": 48, "training_loss": 92.39489793777466, "training_acc": 54.0, "val_loss": 14.701464772224426, "val_acc": 72.0}
{"epoch": 49, "training_loss": 64.59740352630615, "training_acc": 72.0, "val_loss": 15.31655341386795, "val_acc": 44.0}
{"epoch": 50, "training_loss": 61.97862267494202, "training_acc": 72.0, "val_loss": 16.178084909915924, "val_acc": 28.0}
{"epoch": 51, "training_loss": 71.33302736282349, "training_acc": 72.0, "val_loss": 14.805099368095398, "val_acc": 72.0}
{"epoch": 52, "training_loss": 60.579269886016846, "training_acc": 72.0, "val_loss": 15.929612517356873, "val_acc": 72.0}
{"epoch": 53, "training_loss": 62.210339307785034, "training_acc": 72.0, "val_loss": 16.610728204250336, "val_acc": 28.0}
{"epoch": 54, "training_loss": 66.03679037094116, "training_acc": 72.0, "val_loss": 15.08183628320694, "val_acc": 72.0}
{"epoch": 55, "training_loss": 64.6916732788086, "training_acc": 56.0, "val_loss": 17.899151146411896, "val_acc": 72.0}
{"epoch": 56, "training_loss": 69.36851620674133, "training_acc": 72.0, "val_loss": 24.198104441165924, "val_acc": 28.0}
{"epoch": 57, "training_loss": 96.75405836105347, "training_acc": 42.0, "val_loss": 20.113809406757355, "val_acc": 72.0}
{"epoch": 58, "training_loss": 73.01969575881958, "training_acc": 60.0, "val_loss": 15.224586427211761, "val_acc": 60.0}
{"epoch": 59, "training_loss": 66.95361065864563, "training_acc": 72.0, "val_loss": 15.120798349380493, "val_acc": 72.0}
{"epoch": 60, "training_loss": 118.47632026672363, "training_acc": 44.0, "val_loss": 41.09331667423248, "val_acc": 72.0}
{"epoch": 61, "training_loss": 241.84344291687012, "training_acc": 72.0, "val_loss": 54.1412353515625, "val_acc": 72.0}
{"epoch": 62, "training_loss": 167.12202405929565, "training_acc": 72.0, "val_loss": 86.50801181793213, "val_acc": 28.0}
{"epoch": 63, "training_loss": 243.23620915412903, "training_acc": 28.0, "val_loss": 61.51326894760132, "val_acc": 72.0}
{"epoch": 64, "training_loss": 282.20688343048096, "training_acc": 72.0, "val_loss": 89.72702026367188, "val_acc": 72.0}
{"epoch": 65, "training_loss": 316.1067237854004, "training_acc": 72.0, "val_loss": 16.070596873760223, "val_acc": 72.0}
{"epoch": 66, "training_loss": 197.66901016235352, "training_acc": 60.0, "val_loss": 85.719895362854, "val_acc": 28.0}
{"epoch": 67, "training_loss": 257.8420491218567, "training_acc": 48.0, "val_loss": 86.83782815933228, "val_acc": 72.0}
