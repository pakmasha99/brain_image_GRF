"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 5331.354804992676, "training_acc": 42.0, "val_loss": 2814.31884765625, "val_acc": 72.0}
{"epoch": 1, "training_loss": 8803.874008178711, "training_acc": 72.0, "val_loss": 2265.304183959961, "val_acc": 28.0}
{"epoch": 2, "training_loss": 7820.894073486328, "training_acc": 28.0, "val_loss": 769.3109035491943, "val_acc": 72.0}
{"epoch": 3, "training_loss": 3966.073257446289, "training_acc": 72.0, "val_loss": 1719.4217681884766, "val_acc": 72.0}
{"epoch": 4, "training_loss": 6679.190490722656, "training_acc": 72.0, "val_loss": 1099.5495796203613, "val_acc": 72.0}
{"epoch": 5, "training_loss": 3284.171516418457, "training_acc": 72.0, "val_loss": 1796.1553573608398, "val_acc": 28.0}
{"epoch": 6, "training_loss": 7232.086334228516, "training_acc": 28.0, "val_loss": 197.73205518722534, "val_acc": 28.0}
{"epoch": 7, "training_loss": 1364.3275909423828, "training_acc": 52.0, "val_loss": 1922.4977493286133, "val_acc": 72.0}
{"epoch": 8, "training_loss": 8434.306243896484, "training_acc": 72.0, "val_loss": 2665.984535217285, "val_acc": 72.0}
{"epoch": 9, "training_loss": 10563.705642700195, "training_acc": 72.0, "val_loss": 2300.990104675293, "val_acc": 72.0}
{"epoch": 10, "training_loss": 8382.957946777344, "training_acc": 72.0, "val_loss": 1141.5496826171875, "val_acc": 72.0}
{"epoch": 11, "training_loss": 3389.3101921081543, "training_acc": 72.0, "val_loss": 2119.166374206543, "val_acc": 28.0}
{"epoch": 12, "training_loss": 9524.33447265625, "training_acc": 28.0, "val_loss": 1941.8586730957031, "val_acc": 28.0}
{"epoch": 13, "training_loss": 5128.139923095703, "training_acc": 44.0, "val_loss": 686.7223739624023, "val_acc": 72.0}
{"epoch": 14, "training_loss": 3284.921905517578, "training_acc": 72.0, "val_loss": 1018.9260482788086, "val_acc": 72.0}
{"epoch": 15, "training_loss": 3760.016326904297, "training_acc": 72.0, "val_loss": 391.41829013824463, "val_acc": 72.0}
{"epoch": 16, "training_loss": 1837.6186141967773, "training_acc": 56.0, "val_loss": 490.75613021850586, "val_acc": 28.0}
{"epoch": 17, "training_loss": 1620.149642944336, "training_acc": 48.0, "val_loss": 703.8907527923584, "val_acc": 72.0}
{"epoch": 18, "training_loss": 2904.57266998291, "training_acc": 72.0, "val_loss": 642.156982421875, "val_acc": 72.0}
{"epoch": 19, "training_loss": 1997.282569885254, "training_acc": 72.0, "val_loss": 584.10325050354, "val_acc": 28.0}
{"epoch": 20, "training_loss": 2324.0860900878906, "training_acc": 28.0, "val_loss": 427.4433135986328, "val_acc": 72.0}
{"epoch": 21, "training_loss": 2111.46142578125, "training_acc": 72.0, "val_loss": 931.4846992492676, "val_acc": 72.0}
{"epoch": 22, "training_loss": 3604.018180847168, "training_acc": 72.0, "val_loss": 560.5875492095947, "val_acc": 72.0}
{"epoch": 23, "training_loss": 1632.930006980896, "training_acc": 72.0, "val_loss": 1373.7936973571777, "val_acc": 28.0}
{"epoch": 24, "training_loss": 5370.465927124023, "training_acc": 28.0, "val_loss": 21.27925008535385, "val_acc": 72.0}
{"epoch": 25, "training_loss": 208.67145252227783, "training_acc": 72.0, "val_loss": 357.1871280670166, "val_acc": 72.0}
{"epoch": 26, "training_loss": 1268.5891494750977, "training_acc": 72.0, "val_loss": 177.1488904953003, "val_acc": 28.0}
{"epoch": 27, "training_loss": 461.1308846473694, "training_acc": 56.0, "val_loss": 201.77273750305176, "val_acc": 72.0}
{"epoch": 28, "training_loss": 601.2270345687866, "training_acc": 72.0, "val_loss": 727.5269031524658, "val_acc": 28.0}
{"epoch": 29, "training_loss": 2027.3095970153809, "training_acc": 28.0, "val_loss": 709.9676132202148, "val_acc": 72.0}
{"epoch": 30, "training_loss": 3426.717071533203, "training_acc": 72.0, "val_loss": 1392.0259475708008, "val_acc": 72.0}
{"epoch": 31, "training_loss": 5662.284454345703, "training_acc": 72.0, "val_loss": 1157.7407836914062, "val_acc": 72.0}
{"epoch": 32, "training_loss": 3860.946060180664, "training_acc": 72.0, "val_loss": 47.29369282722473, "val_acc": 72.0}
{"epoch": 33, "training_loss": 3139.8106689453125, "training_acc": 54.0, "val_loss": 2416.3509368896484, "val_acc": 28.0}
{"epoch": 34, "training_loss": 8221.110778808594, "training_acc": 28.0, "val_loss": 370.0242757797241, "val_acc": 72.0}
{"epoch": 35, "training_loss": 2506.116897583008, "training_acc": 72.0, "val_loss": 1259.947681427002, "val_acc": 72.0}
{"epoch": 36, "training_loss": 5082.923889160156, "training_acc": 72.0, "val_loss": 1136.7733001708984, "val_acc": 72.0}
{"epoch": 37, "training_loss": 3968.92236328125, "training_acc": 72.0, "val_loss": 241.47372245788574, "val_acc": 72.0}
{"epoch": 38, "training_loss": 2696.9498443603516, "training_acc": 52.0, "val_loss": 1472.1407890319824, "val_acc": 28.0}
{"epoch": 39, "training_loss": 4085.932548522949, "training_acc": 28.0, "val_loss": 868.4270858764648, "val_acc": 72.0}
{"epoch": 40, "training_loss": 4428.502716064453, "training_acc": 72.0, "val_loss": 1936.8169784545898, "val_acc": 72.0}
{"epoch": 41, "training_loss": 7952.342193603516, "training_acc": 72.0, "val_loss": 2006.5393447875977, "val_acc": 72.0}
{"epoch": 42, "training_loss": 7616.676788330078, "training_acc": 72.0, "val_loss": 1255.2931785583496, "val_acc": 72.0}
{"epoch": 43, "training_loss": 4263.901268005371, "training_acc": 72.0, "val_loss": 440.89999198913574, "val_acc": 28.0}
