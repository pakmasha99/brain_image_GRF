"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 3886.8963623046875, "training_acc": 48.0, "val_loss": 3118.5121536254883, "val_acc": 72.0}
{"epoch": 1, "training_loss": 10995.189758300781, "training_acc": 72.0, "val_loss": 1547.1267700195312, "val_acc": 28.0}
{"epoch": 2, "training_loss": 4162.182502746582, "training_acc": 28.0, "val_loss": 1411.8491172790527, "val_acc": 72.0}
{"epoch": 3, "training_loss": 6761.9332275390625, "training_acc": 72.0, "val_loss": 2684.4804763793945, "val_acc": 72.0}
{"epoch": 4, "training_loss": 10651.189239501953, "training_acc": 72.0, "val_loss": 2285.36319732666, "val_acc": 72.0}
{"epoch": 5, "training_loss": 8265.856842041016, "training_acc": 72.0, "val_loss": 799.5049953460693, "val_acc": 72.0}
{"epoch": 6, "training_loss": 2763.545570373535, "training_acc": 60.0, "val_loss": 1095.7879066467285, "val_acc": 28.0}
{"epoch": 7, "training_loss": 3014.676643371582, "training_acc": 44.0, "val_loss": 396.58796787261963, "val_acc": 72.0}
{"epoch": 8, "training_loss": 1550.8532333374023, "training_acc": 72.0, "val_loss": 38.5610431432724, "val_acc": 72.0}
{"epoch": 9, "training_loss": 1559.292724609375, "training_acc": 64.0, "val_loss": 901.0473251342773, "val_acc": 28.0}
{"epoch": 10, "training_loss": 2603.307548522949, "training_acc": 48.0, "val_loss": 789.2463684082031, "val_acc": 72.0}
{"epoch": 11, "training_loss": 3372.122314453125, "training_acc": 72.0, "val_loss": 831.9527626037598, "val_acc": 72.0}
{"epoch": 12, "training_loss": 2866.241485595703, "training_acc": 72.0, "val_loss": 175.86989402770996, "val_acc": 28.0}
{"epoch": 13, "training_loss": 481.43555307388306, "training_acc": 48.0, "val_loss": 107.94976949691772, "val_acc": 28.0}
{"epoch": 14, "training_loss": 468.4087142944336, "training_acc": 56.0, "val_loss": 707.6274394989014, "val_acc": 72.0}
{"epoch": 15, "training_loss": 2916.392532348633, "training_acc": 72.0, "val_loss": 553.7798881530762, "val_acc": 72.0}
{"epoch": 16, "training_loss": 1561.3622417449951, "training_acc": 72.0, "val_loss": 1277.50825881958, "val_acc": 28.0}
{"epoch": 17, "training_loss": 4887.114440917969, "training_acc": 28.0, "val_loss": 162.11880445480347, "val_acc": 72.0}
{"epoch": 18, "training_loss": 1091.361915588379, "training_acc": 72.0, "val_loss": 527.8861999511719, "val_acc": 72.0}
{"epoch": 19, "training_loss": 1847.0455436706543, "training_acc": 72.0, "val_loss": 103.04557085037231, "val_acc": 28.0}
{"epoch": 20, "training_loss": 738.9045486450195, "training_acc": 36.0, "val_loss": 158.6563229560852, "val_acc": 72.0}
{"epoch": 21, "training_loss": 631.361572265625, "training_acc": 62.0, "val_loss": 193.06670427322388, "val_acc": 72.0}
{"epoch": 22, "training_loss": 798.485237121582, "training_acc": 72.0, "val_loss": 33.69118571281433, "val_acc": 72.0}
{"epoch": 23, "training_loss": 1695.3626403808594, "training_acc": 50.0, "val_loss": 319.0129280090332, "val_acc": 28.0}
{"epoch": 24, "training_loss": 1623.6692962646484, "training_acc": 46.0, "val_loss": 1147.1092224121094, "val_acc": 72.0}
{"epoch": 25, "training_loss": 5098.644256591797, "training_acc": 72.0, "val_loss": 1388.4124755859375, "val_acc": 72.0}
{"epoch": 26, "training_loss": 5214.279510498047, "training_acc": 72.0, "val_loss": 691.2478446960449, "val_acc": 72.0}
{"epoch": 27, "training_loss": 2031.8237128257751, "training_acc": 72.0, "val_loss": 1803.4442901611328, "val_acc": 28.0}
{"epoch": 28, "training_loss": 7815.479522705078, "training_acc": 28.0, "val_loss": 906.4299583435059, "val_acc": 28.0}
{"epoch": 29, "training_loss": 3058.5075912475586, "training_acc": 46.0, "val_loss": 1300.7482528686523, "val_acc": 72.0}
{"epoch": 30, "training_loss": 5938.2724609375, "training_acc": 72.0, "val_loss": 1863.7067794799805, "val_acc": 72.0}
{"epoch": 31, "training_loss": 7319.020538330078, "training_acc": 72.0, "val_loss": 1464.4631385803223, "val_acc": 72.0}
{"epoch": 32, "training_loss": 5382.602729797363, "training_acc": 72.0, "val_loss": 284.4998598098755, "val_acc": 72.0}
{"epoch": 33, "training_loss": 2914.8941497802734, "training_acc": 52.0, "val_loss": 1657.9696655273438, "val_acc": 28.0}
{"epoch": 34, "training_loss": 4996.844520568848, "training_acc": 28.0, "val_loss": 730.8136463165283, "val_acc": 72.0}
{"epoch": 35, "training_loss": 4285.439392089844, "training_acc": 72.0, "val_loss": 1733.5393905639648, "val_acc": 72.0}
{"epoch": 36, "training_loss": 7013.258056640625, "training_acc": 72.0, "val_loss": 1663.0311965942383, "val_acc": 72.0}
{"epoch": 37, "training_loss": 6090.288848876953, "training_acc": 72.0, "val_loss": 811.7733955383301, "val_acc": 72.0}
{"epoch": 38, "training_loss": 2310.682231903076, "training_acc": 72.0, "val_loss": 1956.8010330200195, "val_acc": 28.0}
{"epoch": 39, "training_loss": 8525.479888916016, "training_acc": 28.0, "val_loss": 1536.937141418457, "val_acc": 28.0}
{"epoch": 40, "training_loss": 4584.279113769531, "training_acc": 40.0, "val_loss": 820.6595420837402, "val_acc": 72.0}
{"epoch": 41, "training_loss": 3734.8694610595703, "training_acc": 72.0, "val_loss": 1140.7474517822266, "val_acc": 72.0}
