"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 433301.56674194336, "training_acc": 40.0, "val_loss": 205967.3095703125, "val_acc": 72.0}
{"epoch": 1, "training_loss": 639243.6591796875, "training_acc": 72.0, "val_loss": 180354.3212890625, "val_acc": 28.0}
{"epoch": 2, "training_loss": 636974.501953125, "training_acc": 28.0, "val_loss": 49405.609130859375, "val_acc": 72.0}
{"epoch": 3, "training_loss": 282460.884765625, "training_acc": 72.0, "val_loss": 118902.03857421875, "val_acc": 72.0}
{"epoch": 4, "training_loss": 461224.447265625, "training_acc": 72.0, "val_loss": 68218.20678710938, "val_acc": 72.0}
{"epoch": 5, "training_loss": 188637.38369750977, "training_acc": 72.0, "val_loss": 197863.623046875, "val_acc": 28.0}
{"epoch": 6, "training_loss": 824974.0625, "training_acc": 28.0, "val_loss": 100705.94482421875, "val_acc": 28.0}
{"epoch": 7, "training_loss": 324580.9296875, "training_acc": 44.0, "val_loss": 106078.96728515625, "val_acc": 72.0}
{"epoch": 8, "training_loss": 466832.86328125, "training_acc": 72.0, "val_loss": 153239.6240234375, "val_acc": 72.0}
{"epoch": 9, "training_loss": 604511.35546875, "training_acc": 72.0, "val_loss": 123633.8134765625, "val_acc": 72.0}
{"epoch": 10, "training_loss": 440576.357421875, "training_acc": 72.0, "val_loss": 28265.902709960938, "val_acc": 72.0}
{"epoch": 11, "training_loss": 216477.9775390625, "training_acc": 56.0, "val_loss": 131163.00048828125, "val_acc": 28.0}
{"epoch": 12, "training_loss": 400572.8310546875, "training_acc": 28.0, "val_loss": 51051.82800292969, "val_acc": 72.0}
{"epoch": 13, "training_loss": 281996.962890625, "training_acc": 72.0, "val_loss": 128263.916015625, "val_acc": 72.0}
{"epoch": 14, "training_loss": 519653.416015625, "training_acc": 72.0, "val_loss": 127445.1416015625, "val_acc": 72.0}
{"epoch": 15, "training_loss": 475881.01953125, "training_acc": 72.0, "val_loss": 66675.81787109375, "val_acc": 72.0}
{"epoch": 16, "training_loss": 205877.66552734375, "training_acc": 72.0, "val_loss": 121004.04052734375, "val_acc": 28.0}
{"epoch": 17, "training_loss": 531485.447265625, "training_acc": 28.0, "val_loss": 70235.97412109375, "val_acc": 28.0}
{"epoch": 18, "training_loss": 206505.58447265625, "training_acc": 50.0, "val_loss": 87525.64697265625, "val_acc": 72.0}
{"epoch": 19, "training_loss": 374837.16015625, "training_acc": 72.0, "val_loss": 125186.5966796875, "val_acc": 72.0}
{"epoch": 20, "training_loss": 492268.744140625, "training_acc": 72.0, "val_loss": 99042.1142578125, "val_acc": 72.0}
{"epoch": 21, "training_loss": 337751.501953125, "training_acc": 72.0, "val_loss": 15706.645202636719, "val_acc": 72.0}
{"epoch": 22, "training_loss": 222576.974609375, "training_acc": 54.0, "val_loss": 150799.0478515625, "val_acc": 28.0}
{"epoch": 23, "training_loss": 489360.2998046875, "training_acc": 28.0, "val_loss": 38091.23840332031, "val_acc": 72.0}
{"epoch": 24, "training_loss": 209559.64453125, "training_acc": 72.0, "val_loss": 110233.6669921875, "val_acc": 72.0}
{"epoch": 25, "training_loss": 449375.146484375, "training_acc": 72.0, "val_loss": 110526.13525390625, "val_acc": 72.0}
{"epoch": 26, "training_loss": 404991.994140625, "training_acc": 72.0, "val_loss": 51592.999267578125, "val_acc": 72.0}
{"epoch": 27, "training_loss": 132994.0721282959, "training_acc": 72.0, "val_loss": 156575.69580078125, "val_acc": 28.0}
{"epoch": 28, "training_loss": 677795.94140625, "training_acc": 28.0, "val_loss": 119021.3623046875, "val_acc": 28.0}
{"epoch": 29, "training_loss": 330842.2939453125, "training_acc": 44.0, "val_loss": 62258.526611328125, "val_acc": 72.0}
{"epoch": 30, "training_loss": 291043.259765625, "training_acc": 72.0, "val_loss": 91603.3447265625, "val_acc": 72.0}
{"epoch": 31, "training_loss": 347074.001953125, "training_acc": 72.0, "val_loss": 52694.586181640625, "val_acc": 72.0}
{"epoch": 32, "training_loss": 145247.3497314453, "training_acc": 72.0, "val_loss": 109578.3447265625, "val_acc": 28.0}
{"epoch": 33, "training_loss": 486743.53125, "training_acc": 28.0, "val_loss": 44555.499267578125, "val_acc": 28.0}
{"epoch": 34, "training_loss": 251647.5556640625, "training_acc": 34.0, "val_loss": 104524.13330078125, "val_acc": 72.0}
{"epoch": 35, "training_loss": 453370.568359375, "training_acc": 72.0, "val_loss": 138244.091796875, "val_acc": 72.0}
{"epoch": 36, "training_loss": 539118.98828125, "training_acc": 72.0, "val_loss": 106474.70703125, "val_acc": 72.0}
{"epoch": 37, "training_loss": 380193.68359375, "training_acc": 72.0, "val_loss": 20500.616455078125, "val_acc": 72.0}
{"epoch": 38, "training_loss": 171177.291015625, "training_acc": 60.0, "val_loss": 128412.80517578125, "val_acc": 28.0}
{"epoch": 39, "training_loss": 414973.91943359375, "training_acc": 28.0, "val_loss": 43430.92346191406, "val_acc": 72.0}
{"epoch": 40, "training_loss": 236876.47265625, "training_acc": 72.0, "val_loss": 104925.64697265625, "val_acc": 72.0}
