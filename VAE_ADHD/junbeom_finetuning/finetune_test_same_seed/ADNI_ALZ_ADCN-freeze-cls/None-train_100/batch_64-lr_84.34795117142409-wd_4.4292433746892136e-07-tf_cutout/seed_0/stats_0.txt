"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 397303.366607666, "training_acc": 42.0, "val_loss": 217626.611328125, "val_acc": 72.0}
{"epoch": 1, "training_loss": 683518.228515625, "training_acc": 72.0, "val_loss": 126428.3935546875, "val_acc": 28.0}
{"epoch": 2, "training_loss": 431138.6025390625, "training_acc": 28.0, "val_loss": 69182.01293945312, "val_acc": 72.0}
{"epoch": 3, "training_loss": 356586.451171875, "training_acc": 72.0, "val_loss": 145694.970703125, "val_acc": 72.0}
{"epoch": 4, "training_loss": 570467.65625, "training_acc": 72.0, "val_loss": 102951.47705078125, "val_acc": 72.0}
{"epoch": 5, "training_loss": 359224.6064453125, "training_acc": 72.0, "val_loss": 70347.00927734375, "val_acc": 28.0}
{"epoch": 6, "training_loss": 277041.8798828125, "training_acc": 28.0, "val_loss": 26492.156982421875, "val_acc": 72.0}
{"epoch": 7, "training_loss": 129467.29638671875, "training_acc": 72.0, "val_loss": 57689.312744140625, "val_acc": 72.0}
{"epoch": 8, "training_loss": 211800.82080078125, "training_acc": 72.0, "val_loss": 13660.884094238281, "val_acc": 72.0}
{"epoch": 9, "training_loss": 197425.185546875, "training_acc": 50.0, "val_loss": 74026.48315429688, "val_acc": 28.0}
{"epoch": 10, "training_loss": 221950.70849609375, "training_acc": 44.0, "val_loss": 52371.234130859375, "val_acc": 72.0}
{"epoch": 11, "training_loss": 222691.8017578125, "training_acc": 72.0, "val_loss": 53145.2392578125, "val_acc": 72.0}
{"epoch": 12, "training_loss": 178654.21069335938, "training_acc": 72.0, "val_loss": 31210.90087890625, "val_acc": 28.0}
{"epoch": 13, "training_loss": 102253.99169921875, "training_acc": 28.0, "val_loss": 45631.71691894531, "val_acc": 72.0}
{"epoch": 14, "training_loss": 239031.9482421875, "training_acc": 72.0, "val_loss": 89330.19409179688, "val_acc": 72.0}
{"epoch": 15, "training_loss": 348327.5361328125, "training_acc": 72.0, "val_loss": 60200.860595703125, "val_acc": 72.0}
{"epoch": 16, "training_loss": 177749.103515625, "training_acc": 72.0, "val_loss": 74691.88842773438, "val_acc": 28.0}
{"epoch": 17, "training_loss": 330637.826171875, "training_acc": 28.0, "val_loss": 1915.4550552368164, "val_acc": 72.0}
{"epoch": 18, "training_loss": 47849.618408203125, "training_acc": 72.0, "val_loss": 22950.79803466797, "val_acc": 72.0}
{"epoch": 19, "training_loss": 72052.94546508789, "training_acc": 72.0, "val_loss": 64306.915283203125, "val_acc": 28.0}
{"epoch": 20, "training_loss": 203059.44189453125, "training_acc": 28.0, "val_loss": 42939.361572265625, "val_acc": 72.0}
{"epoch": 21, "training_loss": 238010.6455078125, "training_acc": 72.0, "val_loss": 91064.91088867188, "val_acc": 72.0}
{"epoch": 22, "training_loss": 357177.5859375, "training_acc": 72.0, "val_loss": 66492.4072265625, "val_acc": 72.0}
{"epoch": 23, "training_loss": 220666.8447265625, "training_acc": 72.0, "val_loss": 35911.737060546875, "val_acc": 28.0}
{"epoch": 24, "training_loss": 145227.68505859375, "training_acc": 28.0, "val_loss": 25348.27117919922, "val_acc": 72.0}
{"epoch": 25, "training_loss": 134308.47314453125, "training_acc": 72.0, "val_loss": 48711.72180175781, "val_acc": 72.0}
{"epoch": 26, "training_loss": 175458.01171875, "training_acc": 72.0, "val_loss": 6497.150421142578, "val_acc": 72.0}
{"epoch": 27, "training_loss": 143268.75, "training_acc": 56.0, "val_loss": 76684.99755859375, "val_acc": 28.0}
{"epoch": 28, "training_loss": 208691.85021972656, "training_acc": 46.0, "val_loss": 37266.82434082031, "val_acc": 72.0}
{"epoch": 29, "training_loss": 158086.1923828125, "training_acc": 72.0, "val_loss": 34313.24157714844, "val_acc": 72.0}
{"epoch": 30, "training_loss": 109647.19274902344, "training_acc": 72.0, "val_loss": 72527.42309570312, "val_acc": 28.0}
{"epoch": 31, "training_loss": 253254.35009765625, "training_acc": 28.0, "val_loss": 28270.806884765625, "val_acc": 72.0}
{"epoch": 32, "training_loss": 138808.1806640625, "training_acc": 72.0, "val_loss": 59641.90673828125, "val_acc": 72.0}
{"epoch": 33, "training_loss": 226564.8447265625, "training_acc": 72.0, "val_loss": 28021.517944335938, "val_acc": 72.0}
{"epoch": 34, "training_loss": 82212.97241210938, "training_acc": 60.0, "val_loss": 1724.947166442871, "val_acc": 72.0}
{"epoch": 35, "training_loss": 30305.8330078125, "training_acc": 56.0, "val_loss": 24842.28057861328, "val_acc": 72.0}
{"epoch": 36, "training_loss": 114884.43701171875, "training_acc": 72.0, "val_loss": 34542.28515625, "val_acc": 72.0}
{"epoch": 37, "training_loss": 107735.583984375, "training_acc": 72.0, "val_loss": 39990.875244140625, "val_acc": 28.0}
{"epoch": 38, "training_loss": 127127.69921875, "training_acc": 28.0, "val_loss": 40904.59289550781, "val_acc": 72.0}
{"epoch": 39, "training_loss": 224129.169921875, "training_acc": 72.0, "val_loss": 77106.33544921875, "val_acc": 72.0}
{"epoch": 40, "training_loss": 292542.3212890625, "training_acc": 72.0, "val_loss": 41346.81396484375, "val_acc": 72.0}
{"epoch": 41, "training_loss": 102964.57208251953, "training_acc": 72.0, "val_loss": 129578.62548828125, "val_acc": 28.0}
{"epoch": 42, "training_loss": 548868.24609375, "training_acc": 28.0, "val_loss": 58485.906982421875, "val_acc": 28.0}
{"epoch": 43, "training_loss": 195888.041015625, "training_acc": 48.0, "val_loss": 96622.55859375, "val_acc": 72.0}
{"epoch": 44, "training_loss": 428428.197265625, "training_acc": 72.0, "val_loss": 137900.69580078125, "val_acc": 72.0}
{"epoch": 45, "training_loss": 543659.3671875, "training_acc": 72.0, "val_loss": 111930.50537109375, "val_acc": 72.0}
{"epoch": 46, "training_loss": 404680.0517578125, "training_acc": 72.0, "val_loss": 28996.78955078125, "val_acc": 72.0}
{"epoch": 47, "training_loss": 156516.806640625, "training_acc": 60.0, "val_loss": 98786.30981445312, "val_acc": 28.0}
{"epoch": 48, "training_loss": 285789.16552734375, "training_acc": 28.0, "val_loss": 56854.730224609375, "val_acc": 72.0}
{"epoch": 49, "training_loss": 284314.3681640625, "training_acc": 72.0, "val_loss": 128186.8896484375, "val_acc": 72.0}
{"epoch": 50, "training_loss": 519243.12890625, "training_acc": 72.0, "val_loss": 127841.32080078125, "val_acc": 72.0}
{"epoch": 51, "training_loss": 481146.3427734375, "training_acc": 72.0, "val_loss": 70195.42846679688, "val_acc": 72.0}
{"epoch": 52, "training_loss": 212123.99731445312, "training_acc": 72.0, "val_loss": 90661.0595703125, "val_acc": 28.0}
{"epoch": 53, "training_loss": 424171.20703125, "training_acc": 28.0, "val_loss": 44138.03405761719, "val_acc": 28.0}
