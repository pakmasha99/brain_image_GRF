"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 289135.31857299805, "training_acc": 48.0, "val_loss": 234699.90234375, "val_acc": 72.0}
{"epoch": 1, "training_loss": 827518.0478515625, "training_acc": 72.0, "val_loss": 116300.830078125, "val_acc": 28.0}
{"epoch": 2, "training_loss": 312735.4143066406, "training_acc": 28.0, "val_loss": 106278.173828125, "val_acc": 72.0}
{"epoch": 3, "training_loss": 508970.70703125, "training_acc": 72.0, "val_loss": 202031.25, "val_acc": 72.0}
{"epoch": 4, "training_loss": 801594.091796875, "training_acc": 72.0, "val_loss": 171992.7734375, "val_acc": 72.0}
{"epoch": 5, "training_loss": 622083.0625, "training_acc": 72.0, "val_loss": 60185.076904296875, "val_acc": 72.0}
{"epoch": 6, "training_loss": 207913.0634765625, "training_acc": 60.0, "val_loss": 82385.13793945312, "val_acc": 28.0}
{"epoch": 7, "training_loss": 226704.58654785156, "training_acc": 44.0, "val_loss": 29867.578125, "val_acc": 72.0}
{"epoch": 8, "training_loss": 116795.82421875, "training_acc": 72.0, "val_loss": 2920.0756072998047, "val_acc": 72.0}
{"epoch": 9, "training_loss": 118104.4130859375, "training_acc": 64.0, "val_loss": 69014.34326171875, "val_acc": 28.0}
{"epoch": 10, "training_loss": 198076.671875, "training_acc": 48.0, "val_loss": 58515.93017578125, "val_acc": 72.0}
{"epoch": 11, "training_loss": 250012.013671875, "training_acc": 72.0, "val_loss": 61416.56494140625, "val_acc": 72.0}
{"epoch": 12, "training_loss": 210771.181640625, "training_acc": 72.0, "val_loss": 16896.839904785156, "val_acc": 28.0}
{"epoch": 13, "training_loss": 43430.65353393555, "training_acc": 48.0, "val_loss": 12597.394561767578, "val_acc": 28.0}
{"epoch": 14, "training_loss": 44158.95422363281, "training_acc": 56.0, "val_loss": 51384.417724609375, "val_acc": 72.0}
{"epoch": 15, "training_loss": 211933.4033203125, "training_acc": 72.0, "val_loss": 39738.8427734375, "val_acc": 72.0}
{"epoch": 16, "training_loss": 109733.80346679688, "training_acc": 72.0, "val_loss": 101145.63598632812, "val_acc": 28.0}
{"epoch": 17, "training_loss": 387903.92578125, "training_acc": 28.0, "val_loss": 10181.33544921875, "val_acc": 72.0}
{"epoch": 18, "training_loss": 74004.95849609375, "training_acc": 72.0, "val_loss": 37653.79943847656, "val_acc": 72.0}
{"epoch": 19, "training_loss": 130695.59716796875, "training_acc": 72.0, "val_loss": 13102.243041992188, "val_acc": 28.0}
{"epoch": 20, "training_loss": 66623.0556640625, "training_acc": 36.0, "val_loss": 10186.448669433594, "val_acc": 72.0}
{"epoch": 21, "training_loss": 48989.5703125, "training_acc": 62.0, "val_loss": 13050.651550292969, "val_acc": 72.0}
{"epoch": 22, "training_loss": 54322.777099609375, "training_acc": 72.0, "val_loss": 1276.7822265625, "val_acc": 72.0}
{"epoch": 23, "training_loss": 130245.21337890625, "training_acc": 50.0, "val_loss": 28815.850830078125, "val_acc": 28.0}
{"epoch": 24, "training_loss": 131400.765625, "training_acc": 46.0, "val_loss": 83864.1845703125, "val_acc": 72.0}
{"epoch": 25, "training_loss": 373408.38671875, "training_acc": 72.0, "val_loss": 101592.16918945312, "val_acc": 72.0}
{"epoch": 26, "training_loss": 380621.99609375, "training_acc": 72.0, "val_loss": 48835.92834472656, "val_acc": 72.0}
{"epoch": 27, "training_loss": 140048.18045043945, "training_acc": 72.0, "val_loss": 144451.9775390625, "val_acc": 28.0}
{"epoch": 28, "training_loss": 623642.197265625, "training_acc": 28.0, "val_loss": 77689.38598632812, "val_acc": 28.0}
{"epoch": 29, "training_loss": 248898.6318359375, "training_acc": 46.0, "val_loss": 93890.75317382812, "val_acc": 72.0}
{"epoch": 30, "training_loss": 430691.6875, "training_acc": 72.0, "val_loss": 136043.93310546875, "val_acc": 72.0}
{"epoch": 31, "training_loss": 533838.65234375, "training_acc": 72.0, "val_loss": 105874.98779296875, "val_acc": 72.0}
{"epoch": 32, "training_loss": 387711.54150390625, "training_acc": 72.0, "val_loss": 17026.206970214844, "val_acc": 72.0}
{"epoch": 33, "training_loss": 224393.916015625, "training_acc": 52.0, "val_loss": 136150.9033203125, "val_acc": 28.0}
{"epoch": 34, "training_loss": 421794.2021484375, "training_acc": 28.0, "val_loss": 50428.1982421875, "val_acc": 72.0}
{"epoch": 35, "training_loss": 304087.568359375, "training_acc": 72.0, "val_loss": 125773.52294921875, "val_acc": 72.0}
{"epoch": 36, "training_loss": 509005.478515625, "training_acc": 72.0, "val_loss": 120417.4560546875, "val_acc": 72.0}
{"epoch": 37, "training_loss": 439371.685546875, "training_acc": 72.0, "val_loss": 56355.316162109375, "val_acc": 72.0}
{"epoch": 38, "training_loss": 154975.71435546875, "training_acc": 72.0, "val_loss": 159322.74169921875, "val_acc": 28.0}
{"epoch": 39, "training_loss": 689855.46484375, "training_acc": 28.0, "val_loss": 127806.48193359375, "val_acc": 28.0}
{"epoch": 40, "training_loss": 369195.8359375, "training_acc": 40.0, "val_loss": 56952.227783203125, "val_acc": 72.0}
{"epoch": 41, "training_loss": 261805.142578125, "training_acc": 72.0, "val_loss": 81006.23168945312, "val_acc": 72.0}
