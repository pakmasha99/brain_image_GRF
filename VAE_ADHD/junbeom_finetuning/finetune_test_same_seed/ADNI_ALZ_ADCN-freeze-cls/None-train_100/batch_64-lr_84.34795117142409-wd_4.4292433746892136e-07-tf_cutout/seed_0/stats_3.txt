"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 253286.30920410156, "training_acc": 50.0, "val_loss": 226232.1533203125, "val_acc": 72.0}
{"epoch": 1, "training_loss": 783802.4541015625, "training_acc": 72.0, "val_loss": 195654.74853515625, "val_acc": 28.0}
{"epoch": 2, "training_loss": 624577.498046875, "training_acc": 28.0, "val_loss": 79874.85961914062, "val_acc": 72.0}
{"epoch": 3, "training_loss": 494255.96875, "training_acc": 72.0, "val_loss": 173245.88623046875, "val_acc": 72.0}
{"epoch": 4, "training_loss": 674143.849609375, "training_acc": 72.0, "val_loss": 121895.44677734375, "val_acc": 72.0}
{"epoch": 5, "training_loss": 409304.1787109375, "training_acc": 72.0, "val_loss": 40069.65637207031, "val_acc": 28.0}
{"epoch": 6, "training_loss": 176537.4189453125, "training_acc": 28.0, "val_loss": 31352.951049804688, "val_acc": 72.0}
{"epoch": 7, "training_loss": 135932.52124023438, "training_acc": 72.0, "val_loss": 58492.169189453125, "val_acc": 72.0}
{"epoch": 8, "training_loss": 216388.265625, "training_acc": 72.0, "val_loss": 15656.047058105469, "val_acc": 72.0}
{"epoch": 9, "training_loss": 161350.1064453125, "training_acc": 56.0, "val_loss": 65459.85107421875, "val_acc": 28.0}
{"epoch": 10, "training_loss": 195689.66064453125, "training_acc": 46.0, "val_loss": 54754.638671875, "val_acc": 72.0}
{"epoch": 11, "training_loss": 234743.857421875, "training_acc": 72.0, "val_loss": 54405.633544921875, "val_acc": 72.0}
{"epoch": 12, "training_loss": 178522.52978515625, "training_acc": 72.0, "val_loss": 39301.031494140625, "val_acc": 28.0}
{"epoch": 13, "training_loss": 125065.36669921875, "training_acc": 28.0, "val_loss": 41855.92041015625, "val_acc": 72.0}
{"epoch": 14, "training_loss": 186797.26904296875, "training_acc": 72.0, "val_loss": 75104.05883789062, "val_acc": 72.0}
{"epoch": 15, "training_loss": 289711.6904296875, "training_acc": 72.0, "val_loss": 44111.64245605469, "val_acc": 72.0}
{"epoch": 16, "training_loss": 140159.51663208008, "training_acc": 72.0, "val_loss": 119806.60400390625, "val_acc": 28.0}
{"epoch": 17, "training_loss": 468680.8447265625, "training_acc": 28.0, "val_loss": 8173.9654541015625, "val_acc": 28.0}
{"epoch": 18, "training_loss": 113098.228515625, "training_acc": 46.0, "val_loss": 126119.49462890625, "val_acc": 72.0}
{"epoch": 19, "training_loss": 550974.314453125, "training_acc": 72.0, "val_loss": 172220.76416015625, "val_acc": 72.0}
{"epoch": 20, "training_loss": 682801.4765625, "training_acc": 72.0, "val_loss": 147732.1533203125, "val_acc": 72.0}
{"epoch": 21, "training_loss": 534120.8515625, "training_acc": 72.0, "val_loss": 64893.597412109375, "val_acc": 72.0}
{"epoch": 22, "training_loss": 163726.28399658203, "training_acc": 72.0, "val_loss": 178294.81201171875, "val_acc": 28.0}
{"epoch": 23, "training_loss": 796832.70703125, "training_acc": 28.0, "val_loss": 179490.673828125, "val_acc": 28.0}
{"epoch": 24, "training_loss": 483976.37451171875, "training_acc": 28.0, "val_loss": 83533.43505859375, "val_acc": 72.0}
{"epoch": 25, "training_loss": 422945.46484375, "training_acc": 72.0, "val_loss": 201441.30859375, "val_acc": 72.0}
{"epoch": 26, "training_loss": 862807.78515625, "training_acc": 72.0, "val_loss": 239492.138671875, "val_acc": 72.0}
{"epoch": 27, "training_loss": 947825.587890625, "training_acc": 72.0, "val_loss": 204920.654296875, "val_acc": 72.0}
{"epoch": 28, "training_loss": 777743.2421875, "training_acc": 72.0, "val_loss": 122084.1796875, "val_acc": 72.0}
{"epoch": 29, "training_loss": 419653.12451171875, "training_acc": 72.0, "val_loss": 12838.719177246094, "val_acc": 28.0}
{"epoch": 30, "training_loss": 124732.734375, "training_acc": 28.0, "val_loss": 1761.5692138671875, "val_acc": 72.0}
{"epoch": 31, "training_loss": 25325.318115234375, "training_acc": 72.0, "val_loss": 220.9078073501587, "val_acc": 68.0}
{"epoch": 32, "training_loss": 62107.85479736328, "training_acc": 49.0, "val_loss": 24528.22723388672, "val_acc": 72.0}
{"epoch": 33, "training_loss": 128455.0, "training_acc": 72.0, "val_loss": 44129.443359375, "val_acc": 72.0}
{"epoch": 34, "training_loss": 163996.45483398438, "training_acc": 72.0, "val_loss": 6841.849517822266, "val_acc": 28.0}
{"epoch": 35, "training_loss": 42948.919677734375, "training_acc": 42.0, "val_loss": 15730.16357421875, "val_acc": 72.0}
{"epoch": 36, "training_loss": 42248.69161987305, "training_acc": 58.0, "val_loss": 22245.396423339844, "val_acc": 72.0}
{"epoch": 37, "training_loss": 93405.75122070312, "training_acc": 72.0, "val_loss": 15150.759887695312, "val_acc": 72.0}
{"epoch": 38, "training_loss": 46871.55224609375, "training_acc": 66.0, "val_loss": 7745.417785644531, "val_acc": 72.0}
{"epoch": 39, "training_loss": 27912.594360351562, "training_acc": 72.0, "val_loss": 36826.83410644531, "val_acc": 28.0}
{"epoch": 40, "training_loss": 101231.775390625, "training_acc": 46.0, "val_loss": 12561.248016357422, "val_acc": 72.0}
{"epoch": 41, "training_loss": 33780.780517578125, "training_acc": 72.0, "val_loss": 63227.55126953125, "val_acc": 28.0}
{"epoch": 42, "training_loss": 193614.32080078125, "training_acc": 28.0, "val_loss": 45747.064208984375, "val_acc": 72.0}
{"epoch": 43, "training_loss": 222789.2041015625, "training_acc": 72.0, "val_loss": 90876.171875, "val_acc": 72.0}
{"epoch": 44, "training_loss": 360179.712890625, "training_acc": 72.0, "val_loss": 68439.50805664062, "val_acc": 72.0}
{"epoch": 45, "training_loss": 231541.93090820312, "training_acc": 72.0, "val_loss": 36040.155029296875, "val_acc": 28.0}
{"epoch": 46, "training_loss": 147039.61767578125, "training_acc": 28.0, "val_loss": 26968.142700195312, "val_acc": 72.0}
{"epoch": 47, "training_loss": 138581.3583984375, "training_acc": 72.0, "val_loss": 52713.238525390625, "val_acc": 72.0}
{"epoch": 48, "training_loss": 192360.61083984375, "training_acc": 72.0, "val_loss": 12750.997161865234, "val_acc": 72.0}
{"epoch": 49, "training_loss": 136001.3642578125, "training_acc": 56.0, "val_loss": 59359.051513671875, "val_acc": 28.0}
{"epoch": 50, "training_loss": 167838.17944335938, "training_acc": 48.0, "val_loss": 46489.80712890625, "val_acc": 72.0}
