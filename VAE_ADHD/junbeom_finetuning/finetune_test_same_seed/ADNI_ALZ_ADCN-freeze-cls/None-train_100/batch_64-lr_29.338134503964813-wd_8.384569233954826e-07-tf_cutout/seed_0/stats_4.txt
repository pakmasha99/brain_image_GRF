"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 137800.9856185913, "training_acc": 40.0, "val_loss": 75292.08984375, "val_acc": 72.0}
{"epoch": 1, "training_loss": 256775.478515625, "training_acc": 72.0, "val_loss": 4578.221893310547, "val_acc": 72.0}
{"epoch": 2, "training_loss": 189791.689453125, "training_acc": 54.0, "val_loss": 118218.1884765625, "val_acc": 28.0}
{"epoch": 3, "training_loss": 387102.7021484375, "training_acc": 28.0, "val_loss": 14286.222839355469, "val_acc": 72.0}
{"epoch": 4, "training_loss": 88592.98291015625, "training_acc": 72.0, "val_loss": 53395.721435546875, "val_acc": 72.0}
{"epoch": 5, "training_loss": 221582.83203125, "training_acc": 72.0, "val_loss": 57115.966796875, "val_acc": 72.0}
{"epoch": 6, "training_loss": 215000.59375, "training_acc": 72.0, "val_loss": 33716.741943359375, "val_acc": 72.0}
{"epoch": 7, "training_loss": 105631.65063476562, "training_acc": 72.0, "val_loss": 28097.409057617188, "val_acc": 28.0}
{"epoch": 8, "training_loss": 129253.017578125, "training_acc": 28.0, "val_loss": 13002.975463867188, "val_acc": 28.0}
{"epoch": 9, "training_loss": 69567.06201171875, "training_acc": 38.0, "val_loss": 32750.070190429688, "val_acc": 72.0}
{"epoch": 10, "training_loss": 139542.548828125, "training_acc": 72.0, "val_loss": 41203.41796875, "val_acc": 72.0}
{"epoch": 11, "training_loss": 158358.625, "training_acc": 72.0, "val_loss": 27657.479858398438, "val_acc": 72.0}
{"epoch": 12, "training_loss": 91453.96496582031, "training_acc": 72.0, "val_loss": 10894.058227539062, "val_acc": 28.0}
{"epoch": 13, "training_loss": 46419.11730957031, "training_acc": 28.0, "val_loss": 5083.247756958008, "val_acc": 72.0}
{"epoch": 14, "training_loss": 26960.693603515625, "training_acc": 72.0, "val_loss": 7858.106231689453, "val_acc": 72.0}
{"epoch": 15, "training_loss": 21899.669647216797, "training_acc": 72.0, "val_loss": 26076.144409179688, "val_acc": 28.0}
{"epoch": 16, "training_loss": 92536.80834960938, "training_acc": 28.0, "val_loss": 8276.661682128906, "val_acc": 72.0}
{"epoch": 17, "training_loss": 50858.03271484375, "training_acc": 72.0, "val_loss": 20573.426818847656, "val_acc": 72.0}
{"epoch": 18, "training_loss": 77371.47680664062, "training_acc": 72.0, "val_loss": 9735.519409179688, "val_acc": 72.0}
{"epoch": 19, "training_loss": 24559.91519165039, "training_acc": 62.0, "val_loss": 184.15230512619019, "val_acc": 80.0}
{"epoch": 20, "training_loss": 4823.562744140625, "training_acc": 71.0, "val_loss": 840.3267860412598, "val_acc": 52.0}
{"epoch": 21, "training_loss": 11083.49560546875, "training_acc": 55.0, "val_loss": 8764.405822753906, "val_acc": 72.0}
{"epoch": 22, "training_loss": 29565.67315673828, "training_acc": 72.0, "val_loss": 6494.800567626953, "val_acc": 28.0}
{"epoch": 23, "training_loss": 24184.989196777344, "training_acc": 40.0, "val_loss": 2916.5788650512695, "val_acc": 72.0}
{"epoch": 24, "training_loss": 19935.074096679688, "training_acc": 52.0, "val_loss": 5069.144058227539, "val_acc": 72.0}
{"epoch": 25, "training_loss": 23670.318115234375, "training_acc": 72.0, "val_loss": 4700.913619995117, "val_acc": 72.0}
{"epoch": 26, "training_loss": 12845.991302490234, "training_acc": 66.0, "val_loss": 2803.9093017578125, "val_acc": 72.0}
{"epoch": 27, "training_loss": 8686.164596557617, "training_acc": 72.0, "val_loss": 9697.300720214844, "val_acc": 28.0}
{"epoch": 28, "training_loss": 28739.654754638672, "training_acc": 46.0, "val_loss": 5834.169387817383, "val_acc": 72.0}
{"epoch": 29, "training_loss": 18228.82241821289, "training_acc": 72.0, "val_loss": 11885.222625732422, "val_acc": 28.0}
{"epoch": 30, "training_loss": 29454.327423095703, "training_acc": 40.0, "val_loss": 10890.872955322266, "val_acc": 72.0}
{"epoch": 31, "training_loss": 46675.58349609375, "training_acc": 72.0, "val_loss": 15148.565673828125, "val_acc": 72.0}
{"epoch": 32, "training_loss": 52798.72229003906, "training_acc": 72.0, "val_loss": 280.4378271102905, "val_acc": 84.0}
{"epoch": 33, "training_loss": 35267.248779296875, "training_acc": 49.0, "val_loss": 2465.1708602905273, "val_acc": 32.0}
{"epoch": 34, "training_loss": 32112.588623046875, "training_acc": 45.0, "val_loss": 29676.388549804688, "val_acc": 72.0}
{"epoch": 35, "training_loss": 128276.52392578125, "training_acc": 72.0, "val_loss": 35496.17004394531, "val_acc": 72.0}
{"epoch": 36, "training_loss": 132578.00146484375, "training_acc": 72.0, "val_loss": 19564.195251464844, "val_acc": 72.0}
{"epoch": 37, "training_loss": 53264.41516113281, "training_acc": 72.0, "val_loss": 32546.389770507812, "val_acc": 28.0}
{"epoch": 38, "training_loss": 139034.666015625, "training_acc": 28.0, "val_loss": 16870.79315185547, "val_acc": 28.0}
