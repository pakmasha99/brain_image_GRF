"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 235711.26663208008, "training_acc": 72.0, "val_loss": 78059.46655273438, "val_acc": 72.0}
{"epoch": 1, "training_loss": 233223.24487304688, "training_acc": 72.0, "val_loss": 125959.70458984375, "val_acc": 28.0}
{"epoch": 2, "training_loss": 468406.123046875, "training_acc": 28.0, "val_loss": 9473.6328125, "val_acc": 28.0}
{"epoch": 3, "training_loss": 99661.888671875, "training_acc": 44.0, "val_loss": 91757.73315429688, "val_acc": 72.0}
{"epoch": 4, "training_loss": 396715.408203125, "training_acc": 72.0, "val_loss": 123646.484375, "val_acc": 72.0}
{"epoch": 5, "training_loss": 483592.0009765625, "training_acc": 72.0, "val_loss": 108675.732421875, "val_acc": 72.0}
{"epoch": 6, "training_loss": 403452.9404296875, "training_acc": 72.0, "val_loss": 61095.648193359375, "val_acc": 72.0}
{"epoch": 7, "training_loss": 188796.65966796875, "training_acc": 72.0, "val_loss": 30406.01806640625, "val_acc": 28.0}
{"epoch": 8, "training_loss": 158437.0205078125, "training_acc": 28.0, "val_loss": 29352.578735351562, "val_acc": 28.0}
{"epoch": 9, "training_loss": 98124.0009765625, "training_acc": 44.0, "val_loss": 36096.11511230469, "val_acc": 72.0}
{"epoch": 10, "training_loss": 154816.36181640625, "training_acc": 72.0, "val_loss": 45256.40563964844, "val_acc": 72.0}
{"epoch": 11, "training_loss": 167208.0908203125, "training_acc": 72.0, "val_loss": 22824.554443359375, "val_acc": 72.0}
{"epoch": 12, "training_loss": 54939.47756958008, "training_acc": 72.0, "val_loss": 63457.818603515625, "val_acc": 28.0}
{"epoch": 13, "training_loss": 269554.9677734375, "training_acc": 28.0, "val_loss": 41109.70153808594, "val_acc": 28.0}
{"epoch": 14, "training_loss": 126089.83715820312, "training_acc": 42.0, "val_loss": 34103.89099121094, "val_acc": 72.0}
{"epoch": 15, "training_loss": 147246.9814453125, "training_acc": 72.0, "val_loss": 47568.09387207031, "val_acc": 72.0}
{"epoch": 16, "training_loss": 179854.66650390625, "training_acc": 72.0, "val_loss": 31735.763549804688, "val_acc": 72.0}
{"epoch": 17, "training_loss": 96090.79833984375, "training_acc": 72.0, "val_loss": 22343.820190429688, "val_acc": 28.0}
{"epoch": 18, "training_loss": 96340.99853515625, "training_acc": 28.0, "val_loss": 2857.5958251953125, "val_acc": 72.0}
{"epoch": 19, "training_loss": 16964.283813476562, "training_acc": 72.0, "val_loss": 7504.169464111328, "val_acc": 72.0}
{"epoch": 20, "training_loss": 19444.77744293213, "training_acc": 64.0, "val_loss": 1437.0017051696777, "val_acc": 68.0}
{"epoch": 21, "training_loss": 20809.309936523438, "training_acc": 51.0, "val_loss": 8675.19302368164, "val_acc": 72.0}
{"epoch": 22, "training_loss": 38933.97131347656, "training_acc": 72.0, "val_loss": 11505.811309814453, "val_acc": 72.0}
{"epoch": 23, "training_loss": 33294.95095825195, "training_acc": 72.0, "val_loss": 30164.154052734375, "val_acc": 28.0}
{"epoch": 24, "training_loss": 99055.21948242188, "training_acc": 28.0, "val_loss": 14687.313842773438, "val_acc": 72.0}
{"epoch": 25, "training_loss": 76800.47314453125, "training_acc": 72.0, "val_loss": 31030.242919921875, "val_acc": 72.0}
{"epoch": 26, "training_loss": 116664.78979492188, "training_acc": 72.0, "val_loss": 16587.70294189453, "val_acc": 72.0}
{"epoch": 27, "training_loss": 47853.94468688965, "training_acc": 64.0, "val_loss": 14782.58056640625, "val_acc": 28.0}
{"epoch": 28, "training_loss": 38061.181640625, "training_acc": 52.0, "val_loss": 11236.515045166016, "val_acc": 72.0}
{"epoch": 29, "training_loss": 41314.380615234375, "training_acc": 72.0, "val_loss": 1316.315746307373, "val_acc": 68.0}
{"epoch": 30, "training_loss": 33576.8662109375, "training_acc": 69.0, "val_loss": 20382.05108642578, "val_acc": 28.0}
{"epoch": 31, "training_loss": 67557.3671875, "training_acc": 44.0, "val_loss": 23882.899475097656, "val_acc": 72.0}
{"epoch": 32, "training_loss": 97197.99267578125, "training_acc": 72.0, "val_loss": 22904.953002929688, "val_acc": 72.0}
{"epoch": 33, "training_loss": 75242.67858886719, "training_acc": 72.0, "val_loss": 11378.136444091797, "val_acc": 28.0}
{"epoch": 34, "training_loss": 39565.261962890625, "training_acc": 28.0, "val_loss": 19002.194213867188, "val_acc": 72.0}
{"epoch": 35, "training_loss": 96128.02099609375, "training_acc": 72.0, "val_loss": 37969.00634765625, "val_acc": 72.0}
{"epoch": 36, "training_loss": 144617.25927734375, "training_acc": 72.0, "val_loss": 25544.288635253906, "val_acc": 72.0}
{"epoch": 37, "training_loss": 79699.48889160156, "training_acc": 72.0, "val_loss": 29508.847045898438, "val_acc": 28.0}
{"epoch": 38, "training_loss": 116751.267578125, "training_acc": 28.0, "val_loss": 4116.872024536133, "val_acc": 68.0}
{"epoch": 39, "training_loss": 26400.23681640625, "training_acc": 72.0, "val_loss": 12336.45248413086, "val_acc": 72.0}
{"epoch": 40, "training_loss": 37519.521728515625, "training_acc": 72.0, "val_loss": 18835.44921875, "val_acc": 28.0}
{"epoch": 41, "training_loss": 57340.31921386719, "training_acc": 28.0, "val_loss": 19591.905212402344, "val_acc": 72.0}
{"epoch": 42, "training_loss": 96613.26806640625, "training_acc": 72.0, "val_loss": 40190.46936035156, "val_acc": 72.0}
{"epoch": 43, "training_loss": 154173.6201171875, "training_acc": 72.0, "val_loss": 29874.139404296875, "val_acc": 72.0}
{"epoch": 44, "training_loss": 94665.01123046875, "training_acc": 72.0, "val_loss": 6376.606369018555, "val_acc": 28.0}
{"epoch": 45, "training_loss": 26208.350708007812, "training_acc": 28.0, "val_loss": 13930.001831054688, "val_acc": 72.0}
{"epoch": 46, "training_loss": 66845.984375, "training_acc": 72.0, "val_loss": 23968.885803222656, "val_acc": 72.0}
{"epoch": 47, "training_loss": 84879.26953125, "training_acc": 72.0, "val_loss": 5157.819747924805, "val_acc": 68.0}
{"epoch": 48, "training_loss": 51120.82421875, "training_acc": 60.0, "val_loss": 29098.489379882812, "val_acc": 28.0}
