"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 168.37643337249756, "training_acc": 40.0, "val_loss": 60.924309492111206, "val_acc": 72.0}
{"epoch": 1, "training_loss": 202.30453538894653, "training_acc": 72.0, "val_loss": 37.47239708900452, "val_acc": 28.0}
{"epoch": 2, "training_loss": 144.95922946929932, "training_acc": 28.0, "val_loss": 16.71481281518936, "val_acc": 72.0}
{"epoch": 3, "training_loss": 92.00594854354858, "training_acc": 72.0, "val_loss": 34.742602705955505, "val_acc": 72.0}
{"epoch": 4, "training_loss": 131.6871156692505, "training_acc": 72.0, "val_loss": 18.859755992889404, "val_acc": 72.0}
{"epoch": 5, "training_loss": 83.46406149864197, "training_acc": 50.0, "val_loss": 25.84575116634369, "val_acc": 28.0}
{"epoch": 6, "training_loss": 90.42346692085266, "training_acc": 44.0, "val_loss": 16.460125148296356, "val_acc": 72.0}
{"epoch": 7, "training_loss": 72.64581656455994, "training_acc": 72.0, "val_loss": 21.748550236225128, "val_acc": 72.0}
{"epoch": 8, "training_loss": 83.04376292228699, "training_acc": 72.0, "val_loss": 14.855696260929108, "val_acc": 72.0}
{"epoch": 9, "training_loss": 65.52486062049866, "training_acc": 54.0, "val_loss": 17.16274917125702, "val_acc": 28.0}
{"epoch": 10, "training_loss": 68.71192121505737, "training_acc": 72.0, "val_loss": 16.227947175502777, "val_acc": 72.0}
{"epoch": 11, "training_loss": 65.7560453414917, "training_acc": 72.0, "val_loss": 15.039356052875519, "val_acc": 72.0}
{"epoch": 12, "training_loss": 59.79445838928223, "training_acc": 72.0, "val_loss": 16.749246418476105, "val_acc": 28.0}
{"epoch": 13, "training_loss": 65.13322925567627, "training_acc": 72.0, "val_loss": 14.968302845954895, "val_acc": 72.0}
{"epoch": 14, "training_loss": 59.83642935752869, "training_acc": 72.0, "val_loss": 15.702944993972778, "val_acc": 72.0}
{"epoch": 15, "training_loss": 61.672385931015015, "training_acc": 72.0, "val_loss": 14.957717061042786, "val_acc": 72.0}
{"epoch": 16, "training_loss": 60.48037314414978, "training_acc": 72.0, "val_loss": 15.005005896091461, "val_acc": 72.0}
{"epoch": 17, "training_loss": 58.79685854911804, "training_acc": 72.0, "val_loss": 15.628889203071594, "val_acc": 72.0}
{"epoch": 18, "training_loss": 63.79713463783264, "training_acc": 72.0, "val_loss": 14.94983285665512, "val_acc": 72.0}
{"epoch": 19, "training_loss": 58.793562173843384, "training_acc": 72.0, "val_loss": 16.5913924574852, "val_acc": 28.0}
{"epoch": 20, "training_loss": 65.58899664878845, "training_acc": 72.0, "val_loss": 14.917255938053131, "val_acc": 72.0}
{"epoch": 21, "training_loss": 59.50824284553528, "training_acc": 72.0, "val_loss": 14.972926676273346, "val_acc": 72.0}
{"epoch": 22, "training_loss": 59.42265772819519, "training_acc": 72.0, "val_loss": 15.210480988025665, "val_acc": 72.0}
{"epoch": 23, "training_loss": 62.524200201034546, "training_acc": 72.0, "val_loss": 14.812590181827545, "val_acc": 72.0}
{"epoch": 24, "training_loss": 61.472182750701904, "training_acc": 72.0, "val_loss": 15.813630819320679, "val_acc": 72.0}
{"epoch": 25, "training_loss": 61.054145097732544, "training_acc": 72.0, "val_loss": 15.81389456987381, "val_acc": 28.0}
{"epoch": 26, "training_loss": 62.712894439697266, "training_acc": 72.0, "val_loss": 14.804159104824066, "val_acc": 72.0}
{"epoch": 27, "training_loss": 61.41405153274536, "training_acc": 72.0, "val_loss": 15.192848443984985, "val_acc": 72.0}
{"epoch": 28, "training_loss": 59.98531770706177, "training_acc": 72.0, "val_loss": 15.460540354251862, "val_acc": 28.0}
{"epoch": 29, "training_loss": 60.793004274368286, "training_acc": 72.0, "val_loss": 14.988480508327484, "val_acc": 72.0}
{"epoch": 30, "training_loss": 59.64285695552826, "training_acc": 72.0, "val_loss": 15.19196629524231, "val_acc": 72.0}
{"epoch": 31, "training_loss": 60.20378351211548, "training_acc": 72.0, "val_loss": 14.835125207901001, "val_acc": 72.0}
{"epoch": 32, "training_loss": 59.10615515708923, "training_acc": 72.0, "val_loss": 14.799007773399353, "val_acc": 72.0}
{"epoch": 33, "training_loss": 59.21808338165283, "training_acc": 72.0, "val_loss": 14.818921685218811, "val_acc": 72.0}
{"epoch": 34, "training_loss": 58.72988510131836, "training_acc": 72.0, "val_loss": 14.967025816440582, "val_acc": 72.0}
{"epoch": 35, "training_loss": 60.270511865615845, "training_acc": 72.0, "val_loss": 15.244394540786743, "val_acc": 72.0}
{"epoch": 36, "training_loss": 60.61260652542114, "training_acc": 72.0, "val_loss": 15.316985547542572, "val_acc": 72.0}
{"epoch": 37, "training_loss": 59.351240158081055, "training_acc": 72.0, "val_loss": 16.665448248386383, "val_acc": 28.0}
{"epoch": 38, "training_loss": 65.6246600151062, "training_acc": 72.0, "val_loss": 15.039677917957306, "val_acc": 72.0}
{"epoch": 39, "training_loss": 60.874894857406616, "training_acc": 72.0, "val_loss": 15.871885418891907, "val_acc": 72.0}
{"epoch": 40, "training_loss": 63.60916495323181, "training_acc": 72.0, "val_loss": 15.404888987541199, "val_acc": 48.0}
{"epoch": 41, "training_loss": 60.8244788646698, "training_acc": 72.0, "val_loss": 15.039396286010742, "val_acc": 72.0}
{"epoch": 42, "training_loss": 59.53099775314331, "training_acc": 72.0, "val_loss": 14.856584370136261, "val_acc": 72.0}
{"epoch": 43, "training_loss": 59.91745042800903, "training_acc": 72.0, "val_loss": 15.72924256324768, "val_acc": 28.0}
{"epoch": 44, "training_loss": 61.93773078918457, "training_acc": 72.0, "val_loss": 15.239237248897552, "val_acc": 72.0}
{"epoch": 45, "training_loss": 60.49741053581238, "training_acc": 72.0, "val_loss": 15.35545438528061, "val_acc": 72.0}
{"epoch": 46, "training_loss": 60.11777138710022, "training_acc": 72.0, "val_loss": 15.469545125961304, "val_acc": 32.0}
{"epoch": 47, "training_loss": 60.467835664749146, "training_acc": 72.0, "val_loss": 15.68610817193985, "val_acc": 72.0}
{"epoch": 48, "training_loss": 62.066717743873596, "training_acc": 72.0, "val_loss": 15.39178192615509, "val_acc": 72.0}
{"epoch": 49, "training_loss": 61.67191290855408, "training_acc": 72.0, "val_loss": 14.882704615592957, "val_acc": 72.0}
{"epoch": 50, "training_loss": 59.103206157684326, "training_acc": 72.0, "val_loss": 14.833399653434753, "val_acc": 72.0}
{"epoch": 51, "training_loss": 60.35966420173645, "training_acc": 72.0, "val_loss": 14.890937507152557, "val_acc": 72.0}
