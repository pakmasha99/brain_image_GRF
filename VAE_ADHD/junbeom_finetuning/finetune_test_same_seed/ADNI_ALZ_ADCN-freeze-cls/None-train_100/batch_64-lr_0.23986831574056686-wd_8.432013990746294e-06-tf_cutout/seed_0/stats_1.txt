"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 1584.2475051879883, "training_acc": 72.0, "val_loss": 508.9733123779297, "val_acc": 72.0}
{"epoch": 1, "training_loss": 1550.9956302642822, "training_acc": 72.0, "val_loss": 826.255989074707, "val_acc": 28.0}
{"epoch": 2, "training_loss": 3130.168067932129, "training_acc": 28.0, "val_loss": 40.76606035232544, "val_acc": 28.0}
{"epoch": 3, "training_loss": 497.784725189209, "training_acc": 48.0, "val_loss": 604.1284084320068, "val_acc": 72.0}
{"epoch": 4, "training_loss": 2692.8345489501953, "training_acc": 72.0, "val_loss": 819.2404747009277, "val_acc": 72.0}
{"epoch": 5, "training_loss": 3255.9356231689453, "training_acc": 72.0, "val_loss": 718.0764198303223, "val_acc": 72.0}
{"epoch": 6, "training_loss": 2657.642364501953, "training_acc": 72.0, "val_loss": 387.7392530441284, "val_acc": 72.0}
{"epoch": 7, "training_loss": 1246.1987991333008, "training_acc": 72.0, "val_loss": 306.557297706604, "val_acc": 28.0}
{"epoch": 8, "training_loss": 1500.2851943969727, "training_acc": 28.0, "val_loss": 298.6521005630493, "val_acc": 28.0}
{"epoch": 9, "training_loss": 844.8363695144653, "training_acc": 46.0, "val_loss": 204.64918613433838, "val_acc": 72.0}
{"epoch": 10, "training_loss": 898.7338256835938, "training_acc": 72.0, "val_loss": 276.43730640411377, "val_acc": 72.0}
{"epoch": 11, "training_loss": 1048.9572982788086, "training_acc": 72.0, "val_loss": 144.95192766189575, "val_acc": 72.0}
{"epoch": 12, "training_loss": 355.86192560195923, "training_acc": 72.0, "val_loss": 375.94306468963623, "val_acc": 28.0}
{"epoch": 13, "training_loss": 1590.2879638671875, "training_acc": 28.0, "val_loss": 155.49263954162598, "val_acc": 28.0}
{"epoch": 14, "training_loss": 548.3146228790283, "training_acc": 48.0, "val_loss": 298.67029190063477, "val_acc": 72.0}
{"epoch": 15, "training_loss": 1278.113655090332, "training_acc": 72.0, "val_loss": 422.7243900299072, "val_acc": 72.0}
{"epoch": 16, "training_loss": 1676.7214241027832, "training_acc": 72.0, "val_loss": 356.6244840621948, "val_acc": 72.0}
{"epoch": 17, "training_loss": 1299.8430061340332, "training_acc": 72.0, "val_loss": 123.96245002746582, "val_acc": 72.0}
{"epoch": 18, "training_loss": 453.0142021179199, "training_acc": 58.0, "val_loss": 184.22123193740845, "val_acc": 28.0}
{"epoch": 19, "training_loss": 536.6287174224854, "training_acc": 40.0, "val_loss": 63.175880908966064, "val_acc": 72.0}
{"epoch": 20, "training_loss": 234.43141317367554, "training_acc": 72.0, "val_loss": 27.051472663879395, "val_acc": 28.0}
{"epoch": 21, "training_loss": 98.82816576957703, "training_acc": 44.0, "val_loss": 14.495328068733215, "val_acc": 72.0}
{"epoch": 22, "training_loss": 57.86123180389404, "training_acc": 72.0, "val_loss": 15.244254469871521, "val_acc": 72.0}
{"epoch": 23, "training_loss": 93.80578327178955, "training_acc": 48.0, "val_loss": 79.27398681640625, "val_acc": 72.0}
{"epoch": 24, "training_loss": 372.8376274108887, "training_acc": 72.0, "val_loss": 93.28793287277222, "val_acc": 72.0}
{"epoch": 25, "training_loss": 310.4018099308014, "training_acc": 72.0, "val_loss": 173.85891675949097, "val_acc": 28.0}
{"epoch": 26, "training_loss": 513.488450050354, "training_acc": 28.0, "val_loss": 117.73476600646973, "val_acc": 72.0}
{"epoch": 27, "training_loss": 555.5172653198242, "training_acc": 72.0, "val_loss": 198.8468885421753, "val_acc": 72.0}
{"epoch": 28, "training_loss": 747.6327972412109, "training_acc": 72.0, "val_loss": 85.57129502296448, "val_acc": 72.0}
{"epoch": 29, "training_loss": 404.0506782531738, "training_acc": 52.0, "val_loss": 49.368613958358765, "val_acc": 28.0}
{"epoch": 30, "training_loss": 306.2023639678955, "training_acc": 42.0, "val_loss": 179.67690229415894, "val_acc": 72.0}
{"epoch": 31, "training_loss": 724.5820713043213, "training_acc": 72.0, "val_loss": 156.1921238899231, "val_acc": 72.0}
{"epoch": 32, "training_loss": 513.4200754165649, "training_acc": 72.0, "val_loss": 81.39834403991699, "val_acc": 28.0}
{"epoch": 33, "training_loss": 275.15281772613525, "training_acc": 28.0, "val_loss": 107.54624605178833, "val_acc": 72.0}
{"epoch": 34, "training_loss": 513.1327610015869, "training_acc": 72.0, "val_loss": 187.06804513931274, "val_acc": 72.0}
{"epoch": 35, "training_loss": 700.6451168060303, "training_acc": 72.0, "val_loss": 76.12226605415344, "val_acc": 72.0}
{"epoch": 36, "training_loss": 394.0054130554199, "training_acc": 54.0, "val_loss": 85.74129939079285, "val_acc": 28.0}
{"epoch": 37, "training_loss": 382.30985260009766, "training_acc": 42.0, "val_loss": 170.49221992492676, "val_acc": 72.0}
{"epoch": 38, "training_loss": 727.7045097351074, "training_acc": 72.0, "val_loss": 155.90053796768188, "val_acc": 72.0}
{"epoch": 39, "training_loss": 492.09567165374756, "training_acc": 72.0, "val_loss": 119.98072862625122, "val_acc": 28.0}
{"epoch": 40, "training_loss": 443.1164264678955, "training_acc": 28.0, "val_loss": 86.24567985534668, "val_acc": 72.0}
