"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 92766.40673828125, "training_acc": 42.0, "val_loss": 28514.95361328125, "val_acc": 72.0}
{"epoch": 1, "training_loss": 112387.53540039062, "training_acc": 54.0, "val_loss": 10176.866912841797, "val_acc": 72.0}
{"epoch": 2, "training_loss": 42054.948486328125, "training_acc": 72.0, "val_loss": 2386.370277404785, "val_acc": 28.0}
{"epoch": 3, "training_loss": 24400.016845703125, "training_acc": 42.0, "val_loss": 10551.769256591797, "val_acc": 72.0}
{"epoch": 4, "training_loss": 32185.432250976562, "training_acc": 72.0, "val_loss": 17602.72216796875, "val_acc": 28.0}
{"epoch": 5, "training_loss": 48032.47897338867, "training_acc": 44.0, "val_loss": 2139.9200439453125, "val_acc": 72.0}
{"epoch": 6, "training_loss": 14110.004211425781, "training_acc": 60.0, "val_loss": 5999.22981262207, "val_acc": 72.0}
{"epoch": 7, "training_loss": 22983.592712402344, "training_acc": 72.0, "val_loss": 706.5471172332764, "val_acc": 72.0}
{"epoch": 8, "training_loss": 23336.828369140625, "training_acc": 58.0, "val_loss": 1375.0947952270508, "val_acc": 72.0}
{"epoch": 9, "training_loss": 7324.578918457031, "training_acc": 72.0, "val_loss": 9925.359344482422, "val_acc": 28.0}
{"epoch": 10, "training_loss": 27584.277801513672, "training_acc": 48.0, "val_loss": 3518.8980102539062, "val_acc": 72.0}
{"epoch": 11, "training_loss": 12804.033020019531, "training_acc": 54.0, "val_loss": 7488.724517822266, "val_acc": 72.0}
{"epoch": 12, "training_loss": 30248.991577148438, "training_acc": 72.0, "val_loss": 3527.7557373046875, "val_acc": 72.0}
{"epoch": 13, "training_loss": 27254.508422851562, "training_acc": 48.0, "val_loss": 3775.7205963134766, "val_acc": 72.0}
{"epoch": 14, "training_loss": 14558.833557128906, "training_acc": 72.0, "val_loss": 175.93380212783813, "val_acc": 72.0}
{"epoch": 15, "training_loss": 21869.877075195312, "training_acc": 54.0, "val_loss": 1494.1519737243652, "val_acc": 72.0}
{"epoch": 16, "training_loss": 5261.648536682129, "training_acc": 72.0, "val_loss": 5221.967315673828, "val_acc": 28.0}
{"epoch": 17, "training_loss": 21731.181701660156, "training_acc": 44.0, "val_loss": 6366.289901733398, "val_acc": 72.0}
{"epoch": 18, "training_loss": 19554.72833251953, "training_acc": 72.0, "val_loss": 8833.140563964844, "val_acc": 28.0}
{"epoch": 19, "training_loss": 26056.16552734375, "training_acc": 44.0, "val_loss": 2038.6909484863281, "val_acc": 72.0}
{"epoch": 20, "training_loss": 10328.095764160156, "training_acc": 60.0, "val_loss": 5095.259475708008, "val_acc": 72.0}
{"epoch": 21, "training_loss": 21602.72412109375, "training_acc": 72.0, "val_loss": 563.4303569793701, "val_acc": 72.0}
{"epoch": 22, "training_loss": 26836.249389648438, "training_acc": 52.0, "val_loss": 73.65092039108276, "val_acc": 72.0}
{"epoch": 23, "training_loss": 383.89511489868164, "training_acc": 56.0, "val_loss": 9002.982330322266, "val_acc": 72.0}
{"epoch": 24, "training_loss": 36799.42041015625, "training_acc": 72.0, "val_loss": 6445.684814453125, "val_acc": 72.0}
{"epoch": 25, "training_loss": 18644.513229370117, "training_acc": 72.0, "val_loss": 15329.1015625, "val_acc": 28.0}
{"epoch": 26, "training_loss": 38845.21130371094, "training_acc": 28.0, "val_loss": 12773.216247558594, "val_acc": 72.0}
{"epoch": 27, "training_loss": 56800.39404296875, "training_acc": 72.0, "val_loss": 13833.970642089844, "val_acc": 72.0}
{"epoch": 28, "training_loss": 46675.177490234375, "training_acc": 72.0, "val_loss": 1083.7891578674316, "val_acc": 72.0}
{"epoch": 29, "training_loss": 23087.10009765625, "training_acc": 60.0, "val_loss": 3589.722442626953, "val_acc": 28.0}
{"epoch": 30, "training_loss": 23921.553100585938, "training_acc": 44.0, "val_loss": 12766.973114013672, "val_acc": 72.0}
{"epoch": 31, "training_loss": 46682.4375, "training_acc": 72.0, "val_loss": 4301.521301269531, "val_acc": 72.0}
{"epoch": 32, "training_loss": 14325.00048828125, "training_acc": 64.0, "val_loss": 2733.3641052246094, "val_acc": 72.0}
{"epoch": 33, "training_loss": 9681.258422851562, "training_acc": 72.0, "val_loss": 8003.264617919922, "val_acc": 28.0}
{"epoch": 34, "training_loss": 24005.463409423828, "training_acc": 46.0, "val_loss": 3210.4232788085938, "val_acc": 72.0}
{"epoch": 35, "training_loss": 14362.975891113281, "training_acc": 50.0, "val_loss": 6739.003753662109, "val_acc": 72.0}
{"epoch": 36, "training_loss": 27175.451232910156, "training_acc": 72.0, "val_loss": 3463.728713989258, "val_acc": 72.0}
{"epoch": 37, "training_loss": 12856.140930175781, "training_acc": 62.0, "val_loss": 3754.307174682617, "val_acc": 72.0}
{"epoch": 38, "training_loss": 13550.126495361328, "training_acc": 72.0, "val_loss": 3278.460693359375, "val_acc": 28.0}
{"epoch": 39, "training_loss": 14267.326965332031, "training_acc": 48.0, "val_loss": 6008.356857299805, "val_acc": 72.0}
{"epoch": 40, "training_loss": 19662.200729370117, "training_acc": 72.0, "val_loss": 8159.799957275391, "val_acc": 28.0}
{"epoch": 41, "training_loss": 24630.813507080078, "training_acc": 46.0, "val_loss": 3450.6851196289062, "val_acc": 72.0}
