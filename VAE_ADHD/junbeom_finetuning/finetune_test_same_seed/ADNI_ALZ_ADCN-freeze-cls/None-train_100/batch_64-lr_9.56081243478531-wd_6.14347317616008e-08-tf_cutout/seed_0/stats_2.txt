"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 49153.94271850586, "training_acc": 40.0, "val_loss": 23347.276306152344, "val_acc": 72.0}
{"epoch": 1, "training_loss": 72463.19323730469, "training_acc": 72.0, "val_loss": 20437.48779296875, "val_acc": 28.0}
{"epoch": 2, "training_loss": 72179.48486328125, "training_acc": 28.0, "val_loss": 5601.797866821289, "val_acc": 72.0}
{"epoch": 3, "training_loss": 32023.919921875, "training_acc": 72.0, "val_loss": 13479.833984375, "val_acc": 72.0}
{"epoch": 4, "training_loss": 52289.98583984375, "training_acc": 72.0, "val_loss": 7735.800933837891, "val_acc": 72.0}
{"epoch": 5, "training_loss": 21395.524642944336, "training_acc": 72.0, "val_loss": 22418.540954589844, "val_acc": 28.0}
{"epoch": 6, "training_loss": 93474.82763671875, "training_acc": 28.0, "val_loss": 11407.489776611328, "val_acc": 28.0}
{"epoch": 7, "training_loss": 36775.900146484375, "training_acc": 44.0, "val_loss": 12026.958465576172, "val_acc": 72.0}
{"epoch": 8, "training_loss": 52927.66796875, "training_acc": 72.0, "val_loss": 17373.62823486328, "val_acc": 72.0}
{"epoch": 9, "training_loss": 68537.99926757812, "training_acc": 72.0, "val_loss": 14019.076538085938, "val_acc": 72.0}
{"epoch": 10, "training_loss": 49960.71838378906, "training_acc": 72.0, "val_loss": 3210.010528564453, "val_acc": 72.0}
{"epoch": 11, "training_loss": 24530.265991210938, "training_acc": 56.0, "val_loss": 14851.744079589844, "val_acc": 28.0}
{"epoch": 12, "training_loss": 45343.51330566406, "training_acc": 28.0, "val_loss": 5792.44384765625, "val_acc": 72.0}
{"epoch": 13, "training_loss": 31987.5, "training_acc": 72.0, "val_loss": 14545.042419433594, "val_acc": 72.0}
{"epoch": 14, "training_loss": 58928.597412109375, "training_acc": 72.0, "val_loss": 14453.330993652344, "val_acc": 72.0}
{"epoch": 15, "training_loss": 53971.51611328125, "training_acc": 72.0, "val_loss": 7566.114044189453, "val_acc": 72.0}
{"epoch": 16, "training_loss": 23370.182693481445, "training_acc": 72.0, "val_loss": 13693.208312988281, "val_acc": 28.0}
{"epoch": 17, "training_loss": 60154.198486328125, "training_acc": 28.0, "val_loss": 7939.80712890625, "val_acc": 28.0}
{"epoch": 18, "training_loss": 23364.374969482422, "training_acc": 50.0, "val_loss": 9929.374694824219, "val_acc": 72.0}
{"epoch": 19, "training_loss": 42521.457763671875, "training_acc": 72.0, "val_loss": 14199.066162109375, "val_acc": 72.0}
{"epoch": 20, "training_loss": 55836.0068359375, "training_acc": 72.0, "val_loss": 11236.61880493164, "val_acc": 72.0}
{"epoch": 21, "training_loss": 38325.56359863281, "training_acc": 72.0, "val_loss": 1791.2574768066406, "val_acc": 72.0}
{"epoch": 22, "training_loss": 25216.19482421875, "training_acc": 54.0, "val_loss": 17065.277099609375, "val_acc": 28.0}
{"epoch": 23, "training_loss": 55358.79650878906, "training_acc": 28.0, "val_loss": 4328.042984008789, "val_acc": 72.0}
{"epoch": 24, "training_loss": 23795.293090820312, "training_acc": 72.0, "val_loss": 12505.868530273438, "val_acc": 72.0}
{"epoch": 25, "training_loss": 50980.66223144531, "training_acc": 72.0, "val_loss": 12539.971160888672, "val_acc": 72.0}
{"epoch": 26, "training_loss": 45953.849609375, "training_acc": 72.0, "val_loss": 5860.754776000977, "val_acc": 72.0}
{"epoch": 27, "training_loss": 15125.837079048157, "training_acc": 72.0, "val_loss": 17714.508056640625, "val_acc": 28.0}
{"epoch": 28, "training_loss": 76695.56494140625, "training_acc": 28.0, "val_loss": 13459.097290039062, "val_acc": 28.0}
{"epoch": 29, "training_loss": 37436.68667602539, "training_acc": 44.0, "val_loss": 7069.309234619141, "val_acc": 72.0}
{"epoch": 30, "training_loss": 33039.25744628906, "training_acc": 72.0, "val_loss": 10396.183013916016, "val_acc": 72.0}
{"epoch": 31, "training_loss": 39393.12438964844, "training_acc": 72.0, "val_loss": 5986.626815795898, "val_acc": 72.0}
{"epoch": 32, "training_loss": 16518.789108276367, "training_acc": 72.0, "val_loss": 12384.66567993164, "val_acc": 28.0}
{"epoch": 33, "training_loss": 55028.890625, "training_acc": 28.0, "val_loss": 5015.348052978516, "val_acc": 28.0}
{"epoch": 34, "training_loss": 28453.882568359375, "training_acc": 34.0, "val_loss": 11861.563873291016, "val_acc": 72.0}
{"epoch": 35, "training_loss": 51444.966796875, "training_acc": 72.0, "val_loss": 15684.695434570312, "val_acc": 72.0}
{"epoch": 36, "training_loss": 61168.69482421875, "training_acc": 72.0, "val_loss": 12084.770965576172, "val_acc": 72.0}
{"epoch": 37, "training_loss": 43158.70300292969, "training_acc": 72.0, "val_loss": 2340.367889404297, "val_acc": 72.0}
{"epoch": 38, "training_loss": 19383.435791015625, "training_acc": 60.0, "val_loss": 14512.930297851562, "val_acc": 28.0}
{"epoch": 39, "training_loss": 46867.336975097656, "training_acc": 28.0, "val_loss": 4939.165496826172, "val_acc": 72.0}
{"epoch": 40, "training_loss": 26915.047973632812, "training_acc": 72.0, "val_loss": 11910.108947753906, "val_acc": 72.0}
