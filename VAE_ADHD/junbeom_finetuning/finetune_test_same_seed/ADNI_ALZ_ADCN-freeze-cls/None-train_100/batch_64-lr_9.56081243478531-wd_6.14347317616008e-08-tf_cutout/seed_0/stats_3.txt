"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 28749.328002929688, "training_acc": 50.0, "val_loss": 25644.351196289062, "val_acc": 72.0}
{"epoch": 1, "training_loss": 88848.54516601562, "training_acc": 72.0, "val_loss": 22171.519470214844, "val_acc": 28.0}
{"epoch": 2, "training_loss": 70773.32348632812, "training_acc": 28.0, "val_loss": 9055.641174316406, "val_acc": 72.0}
{"epoch": 3, "training_loss": 56031.865234375, "training_acc": 72.0, "val_loss": 19640.20233154297, "val_acc": 72.0}
{"epoch": 4, "training_loss": 76426.28515625, "training_acc": 72.0, "val_loss": 13821.037292480469, "val_acc": 72.0}
{"epoch": 5, "training_loss": 46411.97882080078, "training_acc": 72.0, "val_loss": 4529.0985107421875, "val_acc": 28.0}
{"epoch": 6, "training_loss": 19959.606567382812, "training_acc": 28.0, "val_loss": 3558.6891174316406, "val_acc": 72.0}
{"epoch": 7, "training_loss": 15427.30435180664, "training_acc": 72.0, "val_loss": 6635.248565673828, "val_acc": 72.0}
{"epoch": 8, "training_loss": 24548.5908203125, "training_acc": 72.0, "val_loss": 1780.2299499511719, "val_acc": 72.0}
{"epoch": 9, "training_loss": 18282.349487304688, "training_acc": 56.0, "val_loss": 7405.622100830078, "val_acc": 28.0}
{"epoch": 10, "training_loss": 22152.759887695312, "training_acc": 46.0, "val_loss": 6211.968994140625, "val_acc": 72.0}
{"epoch": 11, "training_loss": 26630.609985351562, "training_acc": 72.0, "val_loss": 6172.907257080078, "val_acc": 72.0}
{"epoch": 12, "training_loss": 20259.931549072266, "training_acc": 72.0, "val_loss": 4438.372802734375, "val_acc": 28.0}
{"epoch": 13, "training_loss": 14110.841842651367, "training_acc": 28.0, "val_loss": 4750.652313232422, "val_acc": 72.0}
{"epoch": 14, "training_loss": 21198.701293945312, "training_acc": 72.0, "val_loss": 8519.762420654297, "val_acc": 72.0}
{"epoch": 15, "training_loss": 32866.0615234375, "training_acc": 72.0, "val_loss": 5007.376861572266, "val_acc": 72.0}
{"epoch": 16, "training_loss": 15916.456819534302, "training_acc": 72.0, "val_loss": 13560.708618164062, "val_acc": 28.0}
{"epoch": 17, "training_loss": 53048.15783691406, "training_acc": 28.0, "val_loss": 908.1499099731445, "val_acc": 28.0}
{"epoch": 18, "training_loss": 12782.899291992188, "training_acc": 46.0, "val_loss": 14303.042602539062, "val_acc": 72.0}
{"epoch": 19, "training_loss": 62483.19970703125, "training_acc": 72.0, "val_loss": 19529.79278564453, "val_acc": 72.0}
{"epoch": 20, "training_loss": 77430.8017578125, "training_acc": 72.0, "val_loss": 16755.435180664062, "val_acc": 72.0}
{"epoch": 21, "training_loss": 60583.5771484375, "training_acc": 72.0, "val_loss": 7366.805267333984, "val_acc": 72.0}
{"epoch": 22, "training_loss": 18603.221366882324, "training_acc": 72.0, "val_loss": 20180.284118652344, "val_acc": 28.0}
{"epoch": 23, "training_loss": 90204.337890625, "training_acc": 28.0, "val_loss": 20317.56591796875, "val_acc": 28.0}
{"epoch": 24, "training_loss": 54749.14697265625, "training_acc": 28.0, "val_loss": 9478.902435302734, "val_acc": 72.0}
{"epoch": 25, "training_loss": 47982.71826171875, "training_acc": 72.0, "val_loss": 22844.7021484375, "val_acc": 72.0}
{"epoch": 26, "training_loss": 97846.072265625, "training_acc": 72.0, "val_loss": 27159.561157226562, "val_acc": 72.0}
{"epoch": 27, "training_loss": 107489.15075683594, "training_acc": 72.0, "val_loss": 23242.832946777344, "val_acc": 72.0}
{"epoch": 28, "training_loss": 88218.4208984375, "training_acc": 72.0, "val_loss": 13854.902648925781, "val_acc": 72.0}
{"epoch": 29, "training_loss": 47634.865295410156, "training_acc": 72.0, "val_loss": 1410.3206634521484, "val_acc": 28.0}
{"epoch": 30, "training_loss": 13958.749389648438, "training_acc": 28.0, "val_loss": 217.02072620391846, "val_acc": 72.0}
{"epoch": 31, "training_loss": 2939.942626953125, "training_acc": 72.0, "val_loss": 33.052581548690796, "val_acc": 68.0}
{"epoch": 32, "training_loss": 11676.690475463867, "training_acc": 44.0, "val_loss": 864.1522407531738, "val_acc": 72.0}
{"epoch": 33, "training_loss": 5604.752990722656, "training_acc": 72.0, "val_loss": 1529.1369438171387, "val_acc": 72.0}
{"epoch": 34, "training_loss": 12183.492919921875, "training_acc": 44.0, "val_loss": 1375.1449584960938, "val_acc": 72.0}
{"epoch": 35, "training_loss": 6953.155700683594, "training_acc": 72.0, "val_loss": 1238.1217956542969, "val_acc": 72.0}
{"epoch": 36, "training_loss": 8858.880676269531, "training_acc": 56.0, "val_loss": 551.1340141296387, "val_acc": 72.0}
{"epoch": 37, "training_loss": 2162.3666915893555, "training_acc": 72.0, "val_loss": 2590.659523010254, "val_acc": 28.0}
{"epoch": 38, "training_loss": 12839.166809082031, "training_acc": 34.0, "val_loss": 3212.525177001953, "val_acc": 72.0}
{"epoch": 39, "training_loss": 11070.626953125, "training_acc": 72.0, "val_loss": 2097.510528564453, "val_acc": 28.0}
{"epoch": 40, "training_loss": 6362.3093185424805, "training_acc": 46.0, "val_loss": 860.4830741882324, "val_acc": 72.0}
{"epoch": 41, "training_loss": 3835.300491333008, "training_acc": 62.0, "val_loss": 2037.2474670410156, "val_acc": 72.0}
{"epoch": 42, "training_loss": 9535.711364746094, "training_acc": 72.0, "val_loss": 1521.4394569396973, "val_acc": 72.0}
{"epoch": 43, "training_loss": 10127.907775878906, "training_acc": 54.0, "val_loss": 358.4747314453125, "val_acc": 72.0}
{"epoch": 44, "training_loss": 1704.129409790039, "training_acc": 72.0, "val_loss": 3549.3648529052734, "val_acc": 28.0}
{"epoch": 45, "training_loss": 10074.611145019531, "training_acc": 48.0, "val_loss": 2127.559471130371, "val_acc": 72.0}
{"epoch": 46, "training_loss": 7115.237106323242, "training_acc": 72.0, "val_loss": 3651.1688232421875, "val_acc": 28.0}
{"epoch": 47, "training_loss": 10578.542449951172, "training_acc": 44.0, "val_loss": 1143.7386512756348, "val_acc": 72.0}
{"epoch": 48, "training_loss": 4221.162780761719, "training_acc": 56.0, "val_loss": 2835.736846923828, "val_acc": 72.0}
{"epoch": 49, "training_loss": 12893.13671875, "training_acc": 72.0, "val_loss": 3193.313407897949, "val_acc": 72.0}
{"epoch": 50, "training_loss": 9672.84772491455, "training_acc": 72.0, "val_loss": 8479.031372070312, "val_acc": 28.0}
