"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 6371.328956604004, "training_acc": 36.0, "val_loss": 3596.460723876953, "val_acc": 72.0}
{"epoch": 1, "training_loss": 13401.583099365234, "training_acc": 72.0, "val_loss": 1927.1717071533203, "val_acc": 72.0}
{"epoch": 2, "training_loss": 5870.052116394043, "training_acc": 72.0, "val_loss": 3324.505615234375, "val_acc": 28.0}
{"epoch": 3, "training_loss": 13705.467102050781, "training_acc": 28.0, "val_loss": 1750.0152587890625, "val_acc": 28.0}
{"epoch": 4, "training_loss": 5147.917297363281, "training_acc": 46.0, "val_loss": 1599.5929718017578, "val_acc": 72.0}
{"epoch": 5, "training_loss": 7248.139556884766, "training_acc": 72.0, "val_loss": 2349.622917175293, "val_acc": 72.0}
{"epoch": 6, "training_loss": 9101.220275878906, "training_acc": 72.0, "val_loss": 1851.2699127197266, "val_acc": 72.0}
{"epoch": 7, "training_loss": 6520.151351928711, "training_acc": 72.0, "val_loss": 408.0808639526367, "val_acc": 72.0}
{"epoch": 8, "training_loss": 3484.1207885742188, "training_acc": 52.0, "val_loss": 1973.8092422485352, "val_acc": 28.0}
{"epoch": 9, "training_loss": 6214.556945800781, "training_acc": 28.0, "val_loss": 538.4804725646973, "val_acc": 72.0}
{"epoch": 10, "training_loss": 3014.885284423828, "training_acc": 72.0, "val_loss": 1533.3507537841797, "val_acc": 72.0}
{"epoch": 11, "training_loss": 6448.463195800781, "training_acc": 72.0, "val_loss": 1523.1821060180664, "val_acc": 72.0}
{"epoch": 12, "training_loss": 5440.568344116211, "training_acc": 72.0, "val_loss": 564.0137672424316, "val_acc": 72.0}
{"epoch": 13, "training_loss": 2170.4427642822266, "training_acc": 54.0, "val_loss": 670.0461387634277, "val_acc": 28.0}
{"epoch": 14, "training_loss": 1853.1996822357178, "training_acc": 46.0, "val_loss": 364.1571044921875, "val_acc": 72.0}
{"epoch": 15, "training_loss": 1381.803409576416, "training_acc": 72.0, "val_loss": 101.41710042953491, "val_acc": 72.0}
{"epoch": 16, "training_loss": 1083.9206161499023, "training_acc": 58.0, "val_loss": 134.3180537223816, "val_acc": 28.0}
{"epoch": 17, "training_loss": 1207.9832382202148, "training_acc": 44.0, "val_loss": 1031.6286087036133, "val_acc": 72.0}
{"epoch": 18, "training_loss": 4265.860656738281, "training_acc": 72.0, "val_loss": 1146.0667610168457, "val_acc": 72.0}
{"epoch": 19, "training_loss": 4144.098937988281, "training_acc": 72.0, "val_loss": 471.0484027862549, "val_acc": 72.0}
{"epoch": 20, "training_loss": 1651.692310333252, "training_acc": 56.0, "val_loss": 383.573055267334, "val_acc": 28.0}
{"epoch": 21, "training_loss": 1591.4298400878906, "training_acc": 40.0, "val_loss": 568.1760787963867, "val_acc": 72.0}
{"epoch": 22, "training_loss": 2192.840564727783, "training_acc": 72.0, "val_loss": 349.1459846496582, "val_acc": 72.0}
{"epoch": 23, "training_loss": 914.7867975234985, "training_acc": 70.0, "val_loss": 385.9870195388794, "val_acc": 28.0}
{"epoch": 24, "training_loss": 1021.225549697876, "training_acc": 52.0, "val_loss": 355.63323497772217, "val_acc": 72.0}
{"epoch": 25, "training_loss": 1385.9787979125977, "training_acc": 72.0, "val_loss": 69.38872337341309, "val_acc": 72.0}
{"epoch": 26, "training_loss": 1652.2774963378906, "training_acc": 54.0, "val_loss": 684.8837852478027, "val_acc": 28.0}
{"epoch": 27, "training_loss": 2185.166156768799, "training_acc": 44.0, "val_loss": 695.8035945892334, "val_acc": 72.0}
{"epoch": 28, "training_loss": 2922.400161743164, "training_acc": 72.0, "val_loss": 715.984582901001, "val_acc": 72.0}
{"epoch": 29, "training_loss": 2356.5140228271484, "training_acc": 72.0, "val_loss": 112.50337362289429, "val_acc": 28.0}
{"epoch": 30, "training_loss": 304.1415550708771, "training_acc": 43.0, "val_loss": 178.6600947380066, "val_acc": 28.0}
{"epoch": 31, "training_loss": 910.2782096862793, "training_acc": 44.0, "val_loss": 514.9959087371826, "val_acc": 72.0}
{"epoch": 32, "training_loss": 1941.3421821594238, "training_acc": 72.0, "val_loss": 219.22109127044678, "val_acc": 72.0}
{"epoch": 33, "training_loss": 1281.2495651245117, "training_acc": 54.0, "val_loss": 100.29308795928955, "val_acc": 28.0}
{"epoch": 34, "training_loss": 1359.7060470581055, "training_acc": 38.0, "val_loss": 944.0617561340332, "val_acc": 72.0}
{"epoch": 35, "training_loss": 3736.754570007324, "training_acc": 72.0, "val_loss": 918.9278602600098, "val_acc": 72.0}
{"epoch": 36, "training_loss": 3288.7114334106445, "training_acc": 72.0, "val_loss": 205.13381958007812, "val_acc": 72.0}
{"epoch": 37, "training_loss": 2103.2689514160156, "training_acc": 52.0, "val_loss": 980.6873321533203, "val_acc": 28.0}
{"epoch": 38, "training_loss": 2492.8736743927, "training_acc": 46.0, "val_loss": 372.84092903137207, "val_acc": 72.0}
{"epoch": 39, "training_loss": 1609.7787322998047, "training_acc": 72.0, "val_loss": 251.58143043518066, "val_acc": 72.0}
{"epoch": 40, "training_loss": 1643.2983093261719, "training_acc": 44.0, "val_loss": 33.043742179870605, "val_acc": 72.0}
{"epoch": 41, "training_loss": 184.50019359588623, "training_acc": 72.0, "val_loss": 198.9607334136963, "val_acc": 28.0}
{"epoch": 42, "training_loss": 522.2499589920044, "training_acc": 56.0, "val_loss": 377.01072692871094, "val_acc": 72.0}
{"epoch": 43, "training_loss": 1444.6557693481445, "training_acc": 72.0, "val_loss": 95.33117413520813, "val_acc": 72.0}
{"epoch": 44, "training_loss": 1401.0696411132812, "training_acc": 56.0, "val_loss": 486.02938652038574, "val_acc": 28.0}
{"epoch": 45, "training_loss": 1675.2172355651855, "training_acc": 46.0, "val_loss": 773.2243061065674, "val_acc": 72.0}
{"epoch": 46, "training_loss": 3456.280227661133, "training_acc": 72.0, "val_loss": 825.7890701293945, "val_acc": 72.0}
{"epoch": 47, "training_loss": 2749.4422302246094, "training_acc": 72.0, "val_loss": 32.30704367160797, "val_acc": 64.0}
{"epoch": 48, "training_loss": 1777.3903121948242, "training_acc": 56.0, "val_loss": 896.9181060791016, "val_acc": 28.0}
{"epoch": 49, "training_loss": 2877.8910331726074, "training_acc": 38.0, "val_loss": 581.0123920440674, "val_acc": 72.0}
{"epoch": 50, "training_loss": 2464.352813720703, "training_acc": 72.0, "val_loss": 536.3603591918945, "val_acc": 72.0}
{"epoch": 51, "training_loss": 1671.824363708496, "training_acc": 72.0, "val_loss": 749.3510723114014, "val_acc": 28.0}
{"epoch": 52, "training_loss": 2626.655746459961, "training_acc": 28.0, "val_loss": 315.76225757598877, "val_acc": 72.0}
{"epoch": 53, "training_loss": 1546.5878372192383, "training_acc": 72.0, "val_loss": 662.1540546417236, "val_acc": 72.0}
{"epoch": 54, "training_loss": 2431.445297241211, "training_acc": 72.0, "val_loss": 204.02541160583496, "val_acc": 72.0}
{"epoch": 55, "training_loss": 1443.4761962890625, "training_acc": 56.0, "val_loss": 482.1848392486572, "val_acc": 28.0}
{"epoch": 56, "training_loss": 1669.015338897705, "training_acc": 44.0, "val_loss": 679.5115947723389, "val_acc": 72.0}
{"epoch": 57, "training_loss": 2787.4654388427734, "training_acc": 72.0, "val_loss": 625.8376121520996, "val_acc": 72.0}
{"epoch": 58, "training_loss": 2076.7219467163086, "training_acc": 72.0, "val_loss": 452.3334503173828, "val_acc": 28.0}
{"epoch": 59, "training_loss": 1397.2027015686035, "training_acc": 28.0, "val_loss": 475.6001949310303, "val_acc": 72.0}
{"epoch": 60, "training_loss": 2325.963233947754, "training_acc": 72.0, "val_loss": 881.4864158630371, "val_acc": 72.0}
{"epoch": 61, "training_loss": 3296.04776763916, "training_acc": 72.0, "val_loss": 432.7138900756836, "val_acc": 72.0}
{"epoch": 62, "training_loss": 1304.3656339645386, "training_acc": 57.0, "val_loss": 83.59580636024475, "val_acc": 40.0}
{"epoch": 63, "training_loss": 672.2991828918457, "training_acc": 56.0, "val_loss": 449.92027282714844, "val_acc": 72.0}
{"epoch": 64, "training_loss": 1636.7418594360352, "training_acc": 72.0, "val_loss": 70.38270235061646, "val_acc": 68.0}
{"epoch": 65, "training_loss": 1715.9842681884766, "training_acc": 53.0, "val_loss": 691.947603225708, "val_acc": 28.0}
{"epoch": 66, "training_loss": 2459.987632751465, "training_acc": 38.0, "val_loss": 684.17649269104, "val_acc": 72.0}
