"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 59089.576080322266, "training_acc": 42.0, "val_loss": 32346.8505859375, "val_acc": 72.0}
{"epoch": 1, "training_loss": 101597.21875, "training_acc": 72.0, "val_loss": 18784.368896484375, "val_acc": 28.0}
{"epoch": 2, "training_loss": 64054.471923828125, "training_acc": 28.0, "val_loss": 10284.765625, "val_acc": 72.0}
{"epoch": 3, "training_loss": 53008.989501953125, "training_acc": 72.0, "val_loss": 21657.99560546875, "val_acc": 72.0}
{"epoch": 4, "training_loss": 84803.2255859375, "training_acc": 72.0, "val_loss": 15306.544494628906, "val_acc": 72.0}
{"epoch": 5, "training_loss": 53411.428283691406, "training_acc": 72.0, "val_loss": 10441.353607177734, "val_acc": 28.0}
{"epoch": 6, "training_loss": 41120.04479980469, "training_acc": 28.0, "val_loss": 3942.8409576416016, "val_acc": 72.0}
{"epoch": 7, "training_loss": 19264.114196777344, "training_acc": 72.0, "val_loss": 8580.11245727539, "val_acc": 72.0}
{"epoch": 8, "training_loss": 31503.410766601562, "training_acc": 72.0, "val_loss": 2036.7006301879883, "val_acc": 72.0}
{"epoch": 9, "training_loss": 29335.92431640625, "training_acc": 50.0, "val_loss": 10986.795043945312, "val_acc": 28.0}
{"epoch": 10, "training_loss": 32956.816650390625, "training_acc": 44.0, "val_loss": 7790.081024169922, "val_acc": 72.0}
{"epoch": 11, "training_loss": 33123.628662109375, "training_acc": 72.0, "val_loss": 7905.742645263672, "val_acc": 72.0}
{"epoch": 12, "training_loss": 26580.72589111328, "training_acc": 72.0, "val_loss": 4620.4437255859375, "val_acc": 28.0}
{"epoch": 13, "training_loss": 15124.625061035156, "training_acc": 28.0, "val_loss": 6789.3341064453125, "val_acc": 72.0}
{"epoch": 14, "training_loss": 35556.08203125, "training_acc": 72.0, "val_loss": 13284.944152832031, "val_acc": 72.0}
{"epoch": 15, "training_loss": 51803.907470703125, "training_acc": 72.0, "val_loss": 8956.375885009766, "val_acc": 72.0}
{"epoch": 16, "training_loss": 26454.22100830078, "training_acc": 72.0, "val_loss": 11077.957916259766, "val_acc": 28.0}
{"epoch": 17, "training_loss": 49049.4951171875, "training_acc": 28.0, "val_loss": 293.4758186340332, "val_acc": 72.0}
{"epoch": 18, "training_loss": 7147.022521972656, "training_acc": 72.0, "val_loss": 3420.1126098632812, "val_acc": 72.0}
{"epoch": 19, "training_loss": 10745.037349700928, "training_acc": 72.0, "val_loss": 9534.60693359375, "val_acc": 28.0}
{"epoch": 20, "training_loss": 30088.04803466797, "training_acc": 28.0, "val_loss": 6390.973663330078, "val_acc": 72.0}
{"epoch": 21, "training_loss": 35411.510009765625, "training_acc": 72.0, "val_loss": 13544.56787109375, "val_acc": 72.0}
{"epoch": 22, "training_loss": 53126.28515625, "training_acc": 72.0, "val_loss": 9893.304443359375, "val_acc": 72.0}
{"epoch": 23, "training_loss": 32840.201110839844, "training_acc": 72.0, "val_loss": 5309.132385253906, "val_acc": 28.0}
{"epoch": 24, "training_loss": 21471.91729736328, "training_acc": 28.0, "val_loss": 3778.4034729003906, "val_acc": 72.0}
{"epoch": 25, "training_loss": 20005.962890625, "training_acc": 72.0, "val_loss": 7251.319885253906, "val_acc": 72.0}
{"epoch": 26, "training_loss": 26123.753967285156, "training_acc": 72.0, "val_loss": 977.3903846740723, "val_acc": 72.0}
{"epoch": 27, "training_loss": 21280.42529296875, "training_acc": 56.0, "val_loss": 11367.882537841797, "val_acc": 28.0}
{"epoch": 28, "training_loss": 30958.188262939453, "training_acc": 46.0, "val_loss": 5550.459671020508, "val_acc": 72.0}
{"epoch": 29, "training_loss": 23542.502563476562, "training_acc": 72.0, "val_loss": 5111.909103393555, "val_acc": 72.0}
{"epoch": 30, "training_loss": 16344.661399841309, "training_acc": 72.0, "val_loss": 10748.442840576172, "val_acc": 28.0}
{"epoch": 31, "training_loss": 37516.69073486328, "training_acc": 28.0, "val_loss": 4213.798904418945, "val_acc": 72.0}
{"epoch": 32, "training_loss": 20678.74041748047, "training_acc": 72.0, "val_loss": 8876.939392089844, "val_acc": 72.0}
{"epoch": 33, "training_loss": 33724.018798828125, "training_acc": 72.0, "val_loss": 4177.773666381836, "val_acc": 72.0}
{"epoch": 34, "training_loss": 12203.78775024414, "training_acc": 60.0, "val_loss": 269.41845417022705, "val_acc": 72.0}
{"epoch": 35, "training_loss": 4489.051208496094, "training_acc": 56.0, "val_loss": 3705.331802368164, "val_acc": 72.0}
{"epoch": 36, "training_loss": 17127.406188964844, "training_acc": 72.0, "val_loss": 5147.3876953125, "val_acc": 72.0}
{"epoch": 37, "training_loss": 16066.415893554688, "training_acc": 72.0, "val_loss": 5908.571243286133, "val_acc": 28.0}
{"epoch": 38, "training_loss": 18754.607177734375, "training_acc": 28.0, "val_loss": 6093.196105957031, "val_acc": 72.0}
{"epoch": 39, "training_loss": 33366.861572265625, "training_acc": 72.0, "val_loss": 11474.495697021484, "val_acc": 72.0}
{"epoch": 40, "training_loss": 43537.83776855469, "training_acc": 72.0, "val_loss": 6160.324478149414, "val_acc": 72.0}
{"epoch": 41, "training_loss": 15363.547485351562, "training_acc": 72.0, "val_loss": 19220.10955810547, "val_acc": 28.0}
{"epoch": 42, "training_loss": 81423.19458007812, "training_acc": 28.0, "val_loss": 8655.064392089844, "val_acc": 28.0}
{"epoch": 43, "training_loss": 29039.251831054688, "training_acc": 48.0, "val_loss": 14375.761413574219, "val_acc": 72.0}
{"epoch": 44, "training_loss": 63736.863037109375, "training_acc": 72.0, "val_loss": 20512.173461914062, "val_acc": 72.0}
{"epoch": 45, "training_loss": 80869.04663085938, "training_acc": 72.0, "val_loss": 16653.68194580078, "val_acc": 72.0}
{"epoch": 46, "training_loss": 60217.84045410156, "training_acc": 72.0, "val_loss": 4328.204345703125, "val_acc": 72.0}
{"epoch": 47, "training_loss": 23241.195190429688, "training_acc": 60.0, "val_loss": 14634.980773925781, "val_acc": 28.0}
{"epoch": 48, "training_loss": 42287.66632080078, "training_acc": 28.0, "val_loss": 8468.489074707031, "val_acc": 72.0}
{"epoch": 49, "training_loss": 42330.573486328125, "training_acc": 72.0, "val_loss": 19071.495056152344, "val_acc": 72.0}
{"epoch": 50, "training_loss": 77252.00341796875, "training_acc": 72.0, "val_loss": 19021.56524658203, "val_acc": 72.0}
{"epoch": 51, "training_loss": 71595.46215820312, "training_acc": 72.0, "val_loss": 10454.8828125, "val_acc": 72.0}
{"epoch": 52, "training_loss": 31615.11456298828, "training_acc": 72.0, "val_loss": 13417.486572265625, "val_acc": 28.0}
{"epoch": 53, "training_loss": 62816.338134765625, "training_acc": 28.0, "val_loss": 6503.924560546875, "val_acc": 28.0}
