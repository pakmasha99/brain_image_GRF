"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 2141.859046936035, "training_acc": 72.0, "val_loss": 289.2582416534424, "val_acc": 72.0}
{"epoch": 1, "training_loss": 3047.082794189453, "training_acc": 62.0, "val_loss": 187.71116733551025, "val_acc": 28.0}
{"epoch": 2, "training_loss": 1516.1573867797852, "training_acc": 44.0, "val_loss": 850.8315086364746, "val_acc": 72.0}
{"epoch": 3, "training_loss": 3121.083770751953, "training_acc": 72.0, "val_loss": 349.50952529907227, "val_acc": 72.0}
{"epoch": 4, "training_loss": 1031.6238899230957, "training_acc": 54.0, "val_loss": 181.0232162475586, "val_acc": 72.0}
{"epoch": 5, "training_loss": 649.4922847747803, "training_acc": 72.0, "val_loss": 91.51570200920105, "val_acc": 28.0}
{"epoch": 6, "training_loss": 533.8974876403809, "training_acc": 44.0, "val_loss": 202.45187282562256, "val_acc": 72.0}
{"epoch": 7, "training_loss": 671.3006637096405, "training_acc": 72.0, "val_loss": 293.9366102218628, "val_acc": 28.0}
{"epoch": 8, "training_loss": 1076.0077476501465, "training_acc": 36.0, "val_loss": 95.45133113861084, "val_acc": 72.0}
{"epoch": 9, "training_loss": 337.0991792678833, "training_acc": 58.0, "val_loss": 162.9909634590149, "val_acc": 72.0}
{"epoch": 10, "training_loss": 624.1746158599854, "training_acc": 72.0, "val_loss": 38.03114295005798, "val_acc": 72.0}
{"epoch": 11, "training_loss": 413.8140411376953, "training_acc": 62.0, "val_loss": 66.39559864997864, "val_acc": 72.0}
{"epoch": 12, "training_loss": 231.4620761871338, "training_acc": 72.0, "val_loss": 152.52872705459595, "val_acc": 28.0}
{"epoch": 13, "training_loss": 639.8510723114014, "training_acc": 42.0, "val_loss": 149.2424726486206, "val_acc": 72.0}
{"epoch": 14, "training_loss": 431.52727222442627, "training_acc": 72.0, "val_loss": 299.578595161438, "val_acc": 28.0}
{"epoch": 15, "training_loss": 780.526116847992, "training_acc": 48.0, "val_loss": 45.869943499565125, "val_acc": 72.0}
{"epoch": 16, "training_loss": 276.1331253051758, "training_acc": 58.0, "val_loss": 146.02124691009521, "val_acc": 72.0}
{"epoch": 17, "training_loss": 596.7288208007812, "training_acc": 72.0, "val_loss": 36.42169237136841, "val_acc": 72.0}
{"epoch": 18, "training_loss": 628.9035720825195, "training_acc": 52.0, "val_loss": 53.35959792137146, "val_acc": 72.0}
{"epoch": 19, "training_loss": 210.74469089508057, "training_acc": 72.0, "val_loss": 116.86662435531616, "val_acc": 28.0}
{"epoch": 20, "training_loss": 579.0892009735107, "training_acc": 40.0, "val_loss": 142.28347539901733, "val_acc": 72.0}
{"epoch": 21, "training_loss": 403.9780731201172, "training_acc": 72.0, "val_loss": 300.56450366973877, "val_acc": 28.0}
{"epoch": 22, "training_loss": 747.7800152301788, "training_acc": 50.0, "val_loss": 43.406230211257935, "val_acc": 72.0}
{"epoch": 23, "training_loss": 310.07679176330566, "training_acc": 54.0, "val_loss": 152.0180344581604, "val_acc": 72.0}
{"epoch": 24, "training_loss": 597.0931377410889, "training_acc": 72.0, "val_loss": 65.09053111076355, "val_acc": 72.0}
{"epoch": 25, "training_loss": 509.07763290405273, "training_acc": 52.0, "val_loss": 104.62925434112549, "val_acc": 72.0}
{"epoch": 26, "training_loss": 416.2525043487549, "training_acc": 72.0, "val_loss": 14.791026711463928, "val_acc": 72.0}
{"epoch": 27, "training_loss": 164.32796955108643, "training_acc": 58.0, "val_loss": 171.72131538391113, "val_acc": 72.0}
{"epoch": 28, "training_loss": 682.1798839569092, "training_acc": 72.0, "val_loss": 84.61844325065613, "val_acc": 72.0}
{"epoch": 29, "training_loss": 423.664363861084, "training_acc": 56.0, "val_loss": 114.68958854675293, "val_acc": 72.0}
{"epoch": 30, "training_loss": 443.685604095459, "training_acc": 72.0, "val_loss": 16.04803502559662, "val_acc": 28.0}
{"epoch": 31, "training_loss": 74.53767776489258, "training_acc": 54.0, "val_loss": 131.34759664535522, "val_acc": 72.0}
{"epoch": 32, "training_loss": 478.5616178512573, "training_acc": 72.0, "val_loss": 30.38962483406067, "val_acc": 28.0}
{"epoch": 33, "training_loss": 194.1890459060669, "training_acc": 48.0, "val_loss": 82.89255499839783, "val_acc": 72.0}
{"epoch": 34, "training_loss": 282.18781757354736, "training_acc": 58.0, "val_loss": 147.06847667694092, "val_acc": 72.0}
{"epoch": 35, "training_loss": 581.1197814941406, "training_acc": 72.0, "val_loss": 22.229057550430298, "val_acc": 72.0}
{"epoch": 36, "training_loss": 363.4778861999512, "training_acc": 64.0, "val_loss": 49.448204040527344, "val_acc": 72.0}
{"epoch": 37, "training_loss": 180.80635833740234, "training_acc": 72.0, "val_loss": 206.7005157470703, "val_acc": 28.0}
{"epoch": 38, "training_loss": 680.5087871551514, "training_acc": 44.0, "val_loss": 103.45962047576904, "val_acc": 72.0}
{"epoch": 39, "training_loss": 273.48970460891724, "training_acc": 72.0, "val_loss": 139.04917240142822, "val_acc": 28.0}
{"epoch": 40, "training_loss": 549.2732334136963, "training_acc": 46.0, "val_loss": 172.94013500213623, "val_acc": 72.0}
{"epoch": 41, "training_loss": 550.3527870178223, "training_acc": 72.0, "val_loss": 184.45254564285278, "val_acc": 28.0}
{"epoch": 42, "training_loss": 580.6114511489868, "training_acc": 46.0, "val_loss": 87.8035843372345, "val_acc": 72.0}
{"epoch": 43, "training_loss": 285.0815477371216, "training_acc": 56.0, "val_loss": 143.45954656600952, "val_acc": 72.0}
{"epoch": 44, "training_loss": 549.1835613250732, "training_acc": 72.0, "val_loss": 15.199211239814758, "val_acc": 72.0}
{"epoch": 45, "training_loss": 329.73105239868164, "training_acc": 56.0, "val_loss": 127.66988277435303, "val_acc": 72.0}
