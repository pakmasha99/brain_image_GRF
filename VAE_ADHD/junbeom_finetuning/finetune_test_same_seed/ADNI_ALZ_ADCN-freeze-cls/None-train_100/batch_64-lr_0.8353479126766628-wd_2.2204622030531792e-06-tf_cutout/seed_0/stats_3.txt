"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 2547.225372314453, "training_acc": 50.0, "val_loss": 2277.8173446655273, "val_acc": 72.0}
{"epoch": 1, "training_loss": 7015.818832397461, "training_acc": 72.0, "val_loss": 1710.8861923217773, "val_acc": 28.0}
{"epoch": 2, "training_loss": 5692.876510620117, "training_acc": 28.0, "val_loss": 682.059383392334, "val_acc": 72.0}
{"epoch": 3, "training_loss": 3326.6465911865234, "training_acc": 72.0, "val_loss": 1426.455307006836, "val_acc": 72.0}
{"epoch": 4, "training_loss": 5575.351776123047, "training_acc": 72.0, "val_loss": 986.6634368896484, "val_acc": 72.0}
{"epoch": 5, "training_loss": 3266.7203941345215, "training_acc": 72.0, "val_loss": 878.9506912231445, "val_acc": 28.0}
{"epoch": 6, "training_loss": 3605.682815551758, "training_acc": 28.0, "val_loss": 181.91074132919312, "val_acc": 72.0}
{"epoch": 7, "training_loss": 1079.6731033325195, "training_acc": 72.0, "val_loss": 522.1889495849609, "val_acc": 72.0}
{"epoch": 8, "training_loss": 1884.8655166625977, "training_acc": 72.0, "val_loss": 61.90319061279297, "val_acc": 72.0}
{"epoch": 9, "training_loss": 1692.859115600586, "training_acc": 58.0, "val_loss": 1000.2606391906738, "val_acc": 28.0}
{"epoch": 10, "training_loss": 2616.5279655456543, "training_acc": 46.0, "val_loss": 365.8647060394287, "val_acc": 72.0}
{"epoch": 11, "training_loss": 1520.7127571105957, "training_acc": 72.0, "val_loss": 320.1448678970337, "val_acc": 72.0}
{"epoch": 12, "training_loss": 1021.9283720254898, "training_acc": 72.0, "val_loss": 798.2630252838135, "val_acc": 28.0}
{"epoch": 13, "training_loss": 2592.7664794921875, "training_acc": 28.0, "val_loss": 418.40295791625977, "val_acc": 72.0}
{"epoch": 14, "training_loss": 2597.043258666992, "training_acc": 72.0, "val_loss": 919.2901611328125, "val_acc": 72.0}
{"epoch": 15, "training_loss": 3561.4148712158203, "training_acc": 72.0, "val_loss": 598.4673976898193, "val_acc": 72.0}
{"epoch": 16, "training_loss": 1883.3617916107178, "training_acc": 72.0, "val_loss": 793.5999393463135, "val_acc": 28.0}
{"epoch": 17, "training_loss": 3294.3091583251953, "training_acc": 28.0, "val_loss": 62.46880888938904, "val_acc": 72.0}
{"epoch": 18, "training_loss": 333.71446990966797, "training_acc": 72.0, "val_loss": 280.5497646331787, "val_acc": 72.0}
{"epoch": 19, "training_loss": 976.2391929626465, "training_acc": 72.0, "val_loss": 188.1387233734131, "val_acc": 28.0}
{"epoch": 20, "training_loss": 635.9614887237549, "training_acc": 44.0, "val_loss": 111.2611174583435, "val_acc": 72.0}
{"epoch": 21, "training_loss": 518.5055656433105, "training_acc": 54.0, "val_loss": 219.55008506774902, "val_acc": 72.0}
{"epoch": 22, "training_loss": 1033.6963500976562, "training_acc": 72.0, "val_loss": 233.1270933151245, "val_acc": 72.0}
{"epoch": 23, "training_loss": 628.7105405330658, "training_acc": 56.0, "val_loss": 13.849043846130371, "val_acc": 68.0}
{"epoch": 24, "training_loss": 98.41175413131714, "training_acc": 72.0, "val_loss": 318.49777698516846, "val_acc": 28.0}
{"epoch": 25, "training_loss": 834.484034538269, "training_acc": 52.0, "val_loss": 223.9018440246582, "val_acc": 72.0}
{"epoch": 26, "training_loss": 807.8343105316162, "training_acc": 72.0, "val_loss": 135.81335544586182, "val_acc": 28.0}
{"epoch": 27, "training_loss": 428.4078893661499, "training_acc": 52.0, "val_loss": 205.16490936279297, "val_acc": 72.0}
{"epoch": 28, "training_loss": 683.6954479217529, "training_acc": 72.0, "val_loss": 369.9328422546387, "val_acc": 28.0}
{"epoch": 29, "training_loss": 947.2040407657623, "training_acc": 46.0, "val_loss": 61.95042133331299, "val_acc": 72.0}
{"epoch": 30, "training_loss": 381.0886344909668, "training_acc": 56.0, "val_loss": 246.3871717453003, "val_acc": 72.0}
{"epoch": 31, "training_loss": 1148.411247253418, "training_acc": 72.0, "val_loss": 303.47795486450195, "val_acc": 72.0}
{"epoch": 32, "training_loss": 870.5873374938965, "training_acc": 72.0, "val_loss": 695.7084655761719, "val_acc": 28.0}
{"epoch": 33, "training_loss": 2451.7807540893555, "training_acc": 28.0, "val_loss": 300.4544496536255, "val_acc": 72.0}
{"epoch": 34, "training_loss": 1589.7300872802734, "training_acc": 72.0, "val_loss": 658.4918022155762, "val_acc": 72.0}
{"epoch": 35, "training_loss": 2512.8729705810547, "training_acc": 72.0, "val_loss": 330.1804542541504, "val_acc": 72.0}
{"epoch": 36, "training_loss": 1059.3400115966797, "training_acc": 54.0, "val_loss": 52.143192291259766, "val_acc": 72.0}
{"epoch": 37, "training_loss": 132.10225248336792, "training_acc": 71.0, "val_loss": 180.24569749832153, "val_acc": 28.0}
{"epoch": 38, "training_loss": 570.4112777709961, "training_acc": 54.0, "val_loss": 420.6977844238281, "val_acc": 72.0}
{"epoch": 39, "training_loss": 1670.0121955871582, "training_acc": 72.0, "val_loss": 272.36688137054443, "val_acc": 72.0}
{"epoch": 40, "training_loss": 913.1200885772705, "training_acc": 52.0, "val_loss": 135.42768955230713, "val_acc": 72.0}
{"epoch": 41, "training_loss": 517.7315311431885, "training_acc": 72.0, "val_loss": 103.94974946975708, "val_acc": 28.0}
{"epoch": 42, "training_loss": 757.6692314147949, "training_acc": 38.0, "val_loss": 311.88225746154785, "val_acc": 72.0}
