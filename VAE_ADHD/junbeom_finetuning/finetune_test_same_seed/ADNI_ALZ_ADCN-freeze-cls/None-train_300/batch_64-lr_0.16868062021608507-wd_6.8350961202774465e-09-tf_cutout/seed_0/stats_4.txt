"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 300 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 3622.81404876709, "training_acc": 67.33333333333333, "val_loss": 965.0037879943848, "val_acc": 28.0}
{"epoch": 1, "training_loss": 2573.210304260254, "training_acc": 63.0, "val_loss": 464.2723388671875, "val_acc": 72.0}
{"epoch": 2, "training_loss": 1931.8194468021393, "training_acc": 53.0, "val_loss": 245.47236013412476, "val_acc": 72.0}
{"epoch": 3, "training_loss": 988.5469665527344, "training_acc": 66.33333333333333, "val_loss": 296.48476457595825, "val_acc": 28.0}
{"epoch": 4, "training_loss": 1490.5549392700195, "training_acc": 59.666666666666664, "val_loss": 240.81494092941284, "val_acc": 72.0}
{"epoch": 5, "training_loss": 625.5023536682129, "training_acc": 53.666666666666664, "val_loss": 53.45186936855316, "val_acc": 72.0}
{"epoch": 6, "training_loss": 400.4961771965027, "training_acc": 63.0, "val_loss": 263.15543842315674, "val_acc": 28.0}
{"epoch": 7, "training_loss": 489.2499508857727, "training_acc": 55.0, "val_loss": 74.5939998626709, "val_acc": 72.0}
{"epoch": 8, "training_loss": 243.16084480285645, "training_acc": 66.33333333333333, "val_loss": 67.85091459751129, "val_acc": 72.0}
{"epoch": 9, "training_loss": 213.84118819236755, "training_acc": 57.666666666666664, "val_loss": 100.55505788326263, "val_acc": 72.0}
{"epoch": 10, "training_loss": 442.1273875236511, "training_acc": 60.333333333333336, "val_loss": 47.68131738901138, "val_acc": 72.0}
{"epoch": 11, "training_loss": 320.22128772735596, "training_acc": 64.33333333333333, "val_loss": 191.33542442321777, "val_acc": 28.0}
{"epoch": 12, "training_loss": 657.7824053764343, "training_acc": 53.666666666666664, "val_loss": 78.09239184856415, "val_acc": 72.0}
{"epoch": 13, "training_loss": 264.1911311149597, "training_acc": 73.66666666666667, "val_loss": 48.37598729133606, "val_acc": 28.0}
{"epoch": 14, "training_loss": 261.6632413864136, "training_acc": 65.33333333333333, "val_loss": 111.88032507896423, "val_acc": 72.0}
{"epoch": 15, "training_loss": 402.75194025039673, "training_acc": 60.333333333333336, "val_loss": 171.2136298418045, "val_acc": 72.0}
{"epoch": 16, "training_loss": 849.2162590026855, "training_acc": 65.0, "val_loss": 205.20665955543518, "val_acc": 28.0}
{"epoch": 17, "training_loss": 1782.09428024292, "training_acc": 61.0, "val_loss": 419.81292057037354, "val_acc": 72.0}
{"epoch": 18, "training_loss": 983.27232837677, "training_acc": 57.666666666666664, "val_loss": 307.80561339855194, "val_acc": 72.0}
{"epoch": 19, "training_loss": 973.3706130981445, "training_acc": 69.0, "val_loss": 343.2615313529968, "val_acc": 28.0}
{"epoch": 20, "training_loss": 1155.2560539245605, "training_acc": 64.33333333333333, "val_loss": 143.17182970046997, "val_acc": 72.0}
{"epoch": 21, "training_loss": 1006.0422477722168, "training_acc": 52.333333333333336, "val_loss": 449.44902443885803, "val_acc": 72.0}
{"epoch": 22, "training_loss": 1140.0531024932861, "training_acc": 56.333333333333336, "val_loss": 128.98632311820984, "val_acc": 72.0}
{"epoch": 23, "training_loss": 634.6803946495056, "training_acc": 68.33333333333333, "val_loss": 82.52166748046875, "val_acc": 28.0}
{"epoch": 24, "training_loss": 564.6579134464264, "training_acc": 65.0, "val_loss": 376.936146736145, "val_acc": 28.0}
{"epoch": 25, "training_loss": 1540.8411865234375, "training_acc": 51.0, "val_loss": 548.5779728889465, "val_acc": 72.0}
{"epoch": 26, "training_loss": 1945.8522262573242, "training_acc": 53.666666666666664, "val_loss": 41.97756099700928, "val_acc": 72.0}
{"epoch": 27, "training_loss": 826.8072309494019, "training_acc": 72.33333333333333, "val_loss": 42.06362038850784, "val_acc": 80.0}
{"epoch": 28, "training_loss": 680.6152048110962, "training_acc": 62.666666666666664, "val_loss": 260.6196572780609, "val_acc": 72.0}
{"epoch": 29, "training_loss": 531.9800639152527, "training_acc": 62.666666666666664, "val_loss": 104.60193824768066, "val_acc": 72.0}
{"epoch": 30, "training_loss": 252.23859906196594, "training_acc": 58.0, "val_loss": 47.29603862762451, "val_acc": 72.0}
{"epoch": 31, "training_loss": 319.16592025756836, "training_acc": 65.0, "val_loss": 140.13459074497223, "val_acc": 72.0}
{"epoch": 32, "training_loss": 528.3613061904907, "training_acc": 57.666666666666664, "val_loss": 211.34185004234314, "val_acc": 72.0}
{"epoch": 33, "training_loss": 818.7180051803589, "training_acc": 53.666666666666664, "val_loss": 473.9508800506592, "val_acc": 72.0}
{"epoch": 34, "training_loss": 1736.4945917129517, "training_acc": 72.33333333333333, "val_loss": 458.6393451690674, "val_acc": 28.0}
{"epoch": 35, "training_loss": 1083.3872385025024, "training_acc": 53.666666666666664, "val_loss": 102.93468880653381, "val_acc": 72.0}
{"epoch": 36, "training_loss": 282.4297556877136, "training_acc": 67.0, "val_loss": 67.0615296959877, "val_acc": 28.0}
{"epoch": 37, "training_loss": 470.12066745758057, "training_acc": 58.333333333333336, "val_loss": 53.11031964421272, "val_acc": 72.0}
{"epoch": 38, "training_loss": 391.22216844558716, "training_acc": 61.666666666666664, "val_loss": 112.72439277172089, "val_acc": 72.0}
{"epoch": 39, "training_loss": 510.38094758987427, "training_acc": 61.0, "val_loss": 150.95762372016907, "val_acc": 72.0}
{"epoch": 40, "training_loss": 511.36169815063477, "training_acc": 62.333333333333336, "val_loss": 60.29811108112335, "val_acc": 72.0}
{"epoch": 41, "training_loss": 415.4563901424408, "training_acc": 61.666666666666664, "val_loss": 90.49211156368256, "val_acc": 28.0}
{"epoch": 42, "training_loss": 477.7644271850586, "training_acc": 62.333333333333336, "val_loss": 228.80506992340088, "val_acc": 28.0}
{"epoch": 43, "training_loss": 483.7111339569092, "training_acc": 57.666666666666664, "val_loss": 102.28386688232422, "val_acc": 72.0}
{"epoch": 44, "training_loss": 380.7637872695923, "training_acc": 68.0, "val_loss": 123.70965003967285, "val_acc": 72.0}
{"epoch": 45, "training_loss": 535.9536848068237, "training_acc": 63.666666666666664, "val_loss": 143.51545786857605, "val_acc": 72.0}
