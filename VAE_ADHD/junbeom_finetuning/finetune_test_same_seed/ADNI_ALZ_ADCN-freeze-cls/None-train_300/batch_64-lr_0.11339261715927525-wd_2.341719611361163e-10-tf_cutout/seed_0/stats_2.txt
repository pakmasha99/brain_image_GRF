"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 300 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 2055.762596130371, "training_acc": 63.0, "val_loss": 1723.2004680633545, "val_acc": 28.0}
{"epoch": 1, "training_loss": 3918.21160697937, "training_acc": 48.333333333333336, "val_loss": 768.2576460838318, "val_acc": 72.0}
{"epoch": 2, "training_loss": 3383.1207847595215, "training_acc": 72.33333333333333, "val_loss": 442.95867681503296, "val_acc": 72.0}
{"epoch": 3, "training_loss": 1814.951530456543, "training_acc": 48.333333333333336, "val_loss": 131.28031730651855, "val_acc": 72.0}
{"epoch": 4, "training_loss": 1454.939805984497, "training_acc": 72.33333333333333, "val_loss": 303.71366691589355, "val_acc": 72.0}
{"epoch": 5, "training_loss": 1176.643711090088, "training_acc": 45.666666666666664, "val_loss": 244.7317135334015, "val_acc": 72.0}
{"epoch": 6, "training_loss": 1817.5524730682373, "training_acc": 72.33333333333333, "val_loss": 400.0727243423462, "val_acc": 72.0}
{"epoch": 7, "training_loss": 1125.6315517425537, "training_acc": 57.0, "val_loss": 60.795221507549286, "val_acc": 72.0}
{"epoch": 8, "training_loss": 614.7943601608276, "training_acc": 72.33333333333333, "val_loss": 70.33516836166382, "val_acc": 28.0}
{"epoch": 9, "training_loss": 430.07055139541626, "training_acc": 51.0, "val_loss": 48.85544145107269, "val_acc": 28.0}
{"epoch": 10, "training_loss": 366.3402009010315, "training_acc": 63.333333333333336, "val_loss": 128.7352603673935, "val_acc": 72.0}
{"epoch": 11, "training_loss": 375.6656246185303, "training_acc": 63.666666666666664, "val_loss": 89.50941848754883, "val_acc": 72.0}
{"epoch": 12, "training_loss": 253.85436010360718, "training_acc": 57.666666666666664, "val_loss": 44.074744284152985, "val_acc": 72.0}
{"epoch": 13, "training_loss": 208.3714621067047, "training_acc": 63.0, "val_loss": 44.26211380958557, "val_acc": 72.0}
{"epoch": 14, "training_loss": 250.09406065940857, "training_acc": 62.333333333333336, "val_loss": 93.584468126297, "val_acc": 28.0}
{"epoch": 15, "training_loss": 248.57353699207306, "training_acc": 62.333333333333336, "val_loss": 73.05923080444336, "val_acc": 72.0}
{"epoch": 16, "training_loss": 288.5994596481323, "training_acc": 63.666666666666664, "val_loss": 74.64741480350494, "val_acc": 72.0}
{"epoch": 17, "training_loss": 204.19420075416565, "training_acc": 70.66666666666667, "val_loss": 55.088307067751884, "val_acc": 72.0}
{"epoch": 18, "training_loss": 301.65787172317505, "training_acc": 61.0, "val_loss": 79.26991760730743, "val_acc": 72.0}
{"epoch": 19, "training_loss": 267.57474637031555, "training_acc": 60.0, "val_loss": 51.675363063812256, "val_acc": 28.0}
{"epoch": 20, "training_loss": 215.2511305809021, "training_acc": 61.333333333333336, "val_loss": 65.22832515835762, "val_acc": 72.0}
{"epoch": 21, "training_loss": 215.04224002361298, "training_acc": 69.33333333333333, "val_loss": 43.87057965993881, "val_acc": 68.0}
{"epoch": 22, "training_loss": 190.22303915023804, "training_acc": 67.33333333333333, "val_loss": 51.04235589504242, "val_acc": 72.0}
{"epoch": 23, "training_loss": 215.45288562774658, "training_acc": 62.333333333333336, "val_loss": 50.56368565559387, "val_acc": 28.0}
{"epoch": 24, "training_loss": 236.24098563194275, "training_acc": 63.0, "val_loss": 106.84268188476562, "val_acc": 72.0}
{"epoch": 25, "training_loss": 405.6622505187988, "training_acc": 61.666666666666664, "val_loss": 84.57002019882202, "val_acc": 72.0}
{"epoch": 26, "training_loss": 249.36727046966553, "training_acc": 64.33333333333333, "val_loss": 63.192800760269165, "val_acc": 28.0}
{"epoch": 27, "training_loss": 277.93357133865356, "training_acc": 49.666666666666664, "val_loss": 56.1356126666069, "val_acc": 72.0}
{"epoch": 28, "training_loss": 215.22903156280518, "training_acc": 59.333333333333336, "val_loss": 45.82953596115112, "val_acc": 52.0}
{"epoch": 29, "training_loss": 224.9298815727234, "training_acc": 58.666666666666664, "val_loss": 127.58674621582031, "val_acc": 72.0}
{"epoch": 30, "training_loss": 418.38570499420166, "training_acc": 63.666666666666664, "val_loss": 133.02204251289368, "val_acc": 72.0}
{"epoch": 31, "training_loss": 379.38651275634766, "training_acc": 65.33333333333333, "val_loss": 106.33248209953308, "val_acc": 72.0}
{"epoch": 32, "training_loss": 304.62578570842743, "training_acc": 65.0, "val_loss": 46.58618539571762, "val_acc": 72.0}
{"epoch": 33, "training_loss": 311.0304112434387, "training_acc": 61.0, "val_loss": 163.5756859779358, "val_acc": 28.0}
{"epoch": 34, "training_loss": 363.10949659347534, "training_acc": 57.0, "val_loss": 45.781525015830994, "val_acc": 72.0}
{"epoch": 35, "training_loss": 192.571368932724, "training_acc": 74.66666666666667, "val_loss": 46.783312141895294, "val_acc": 72.0}
{"epoch": 36, "training_loss": 309.07316613197327, "training_acc": 60.333333333333336, "val_loss": 211.56916093826294, "val_acc": 28.0}
{"epoch": 37, "training_loss": 431.5484867095947, "training_acc": 64.66666666666667, "val_loss": 135.4294319152832, "val_acc": 28.0}
{"epoch": 38, "training_loss": 265.007435798645, "training_acc": 59.333333333333336, "val_loss": 50.61712074279785, "val_acc": 72.0}
{"epoch": 39, "training_loss": 222.58825016021729, "training_acc": 68.66666666666667, "val_loss": 81.81562542915344, "val_acc": 72.0}
{"epoch": 40, "training_loss": 218.52512335777283, "training_acc": 63.666666666666664, "val_loss": 70.50740432739258, "val_acc": 28.0}
