"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 300 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 13047.748938560486, "training_acc": 61.0, "val_loss": 10396.184356689453, "val_acc": 28.0}
{"epoch": 1, "training_loss": 25719.573501586914, "training_acc": 44.333333333333336, "val_loss": 3584.037965774536, "val_acc": 72.0}
{"epoch": 2, "training_loss": 13883.65412902832, "training_acc": 72.33333333333333, "val_loss": 791.0067911148071, "val_acc": 72.0}
{"epoch": 3, "training_loss": 7826.4158935546875, "training_acc": 44.333333333333336, "val_loss": 2533.4672050476074, "val_acc": 72.0}
{"epoch": 4, "training_loss": 11296.819122314453, "training_acc": 72.33333333333333, "val_loss": 1323.5763521194458, "val_acc": 72.0}
{"epoch": 5, "training_loss": 2698.649772644043, "training_acc": 55.0, "val_loss": 905.5900435447693, "val_acc": 72.0}
{"epoch": 6, "training_loss": 2992.370319366455, "training_acc": 52.333333333333336, "val_loss": 1012.7203760147095, "val_acc": 72.0}
{"epoch": 7, "training_loss": 3469.1778297424316, "training_acc": 56.333333333333336, "val_loss": 894.5540685653687, "val_acc": 72.0}
{"epoch": 8, "training_loss": 6109.0082931518555, "training_acc": 72.33333333333333, "val_loss": 234.78346967697144, "val_acc": 72.0}
{"epoch": 9, "training_loss": 6763.225028991699, "training_acc": 43.0, "val_loss": 2710.527416229248, "val_acc": 72.0}
{"epoch": 10, "training_loss": 13326.922225952148, "training_acc": 72.33333333333333, "val_loss": 2401.1123447418213, "val_acc": 72.0}
{"epoch": 11, "training_loss": 4705.060010910034, "training_acc": 51.666666666666664, "val_loss": 772.8702964782715, "val_acc": 72.0}
{"epoch": 12, "training_loss": 4551.510597229004, "training_acc": 51.666666666666664, "val_loss": 633.6579828262329, "val_acc": 72.0}
{"epoch": 13, "training_loss": 4817.928466796875, "training_acc": 72.33333333333333, "val_loss": 187.60375261306763, "val_acc": 72.0}
{"epoch": 14, "training_loss": 8527.505897521973, "training_acc": 37.0, "val_loss": 2575.872444152832, "val_acc": 72.0}
{"epoch": 15, "training_loss": 13069.098434448242, "training_acc": 72.33333333333333, "val_loss": 2575.6943950653076, "val_acc": 72.0}
{"epoch": 16, "training_loss": 9171.151611328125, "training_acc": 57.0, "val_loss": 3372.6235542297363, "val_acc": 28.0}
{"epoch": 17, "training_loss": 6357.7746505737305, "training_acc": 62.333333333333336, "val_loss": 1394.0554237365723, "val_acc": 72.0}
{"epoch": 18, "training_loss": 2253.0877962112427, "training_acc": 64.0, "val_loss": 96.12882649898529, "val_acc": 72.0}
{"epoch": 19, "training_loss": 1466.16273021698, "training_acc": 63.666666666666664, "val_loss": 762.9149551391602, "val_acc": 28.0}
{"epoch": 20, "training_loss": 1704.810962677002, "training_acc": 50.333333333333336, "val_loss": 881.9215698242188, "val_acc": 72.0}
{"epoch": 21, "training_loss": 2976.7958755493164, "training_acc": 59.666666666666664, "val_loss": 853.7336797714233, "val_acc": 72.0}
{"epoch": 22, "training_loss": 5360.366069793701, "training_acc": 72.33333333333333, "val_loss": 116.22405672073364, "val_acc": 72.0}
{"epoch": 23, "training_loss": 6261.850639343262, "training_acc": 41.666666666666664, "val_loss": 3062.7750282287598, "val_acc": 72.0}
{"epoch": 24, "training_loss": 15580.629043579102, "training_acc": 72.33333333333333, "val_loss": 3084.3004150390625, "val_acc": 72.0}
{"epoch": 25, "training_loss": 10483.892501831055, "training_acc": 52.333333333333336, "val_loss": 2522.6769733428955, "val_acc": 28.0}
{"epoch": 26, "training_loss": 6815.896354675293, "training_acc": 63.0, "val_loss": 1862.8143272399902, "val_acc": 72.0}
{"epoch": 27, "training_loss": 5622.964874267578, "training_acc": 57.666666666666664, "val_loss": 47.98853302001953, "val_acc": 68.0}
{"epoch": 28, "training_loss": 1736.705542564392, "training_acc": 63.0, "val_loss": 475.0650963783264, "val_acc": 72.0}
{"epoch": 29, "training_loss": 2279.370225906372, "training_acc": 53.666666666666664, "val_loss": 1300.7308177947998, "val_acc": 72.0}
{"epoch": 30, "training_loss": 8792.661178588867, "training_acc": 72.33333333333333, "val_loss": 1686.6883850097656, "val_acc": 72.0}
{"epoch": 31, "training_loss": 5388.980187416077, "training_acc": 48.333333333333336, "val_loss": 1625.5844268798828, "val_acc": 72.0}
{"epoch": 32, "training_loss": 13346.333618164062, "training_acc": 72.33333333333333, "val_loss": 3376.8608722686768, "val_acc": 72.0}
{"epoch": 33, "training_loss": 8233.66088104248, "training_acc": 55.0, "val_loss": 97.31701201200485, "val_acc": 28.0}
{"epoch": 34, "training_loss": 7515.645660400391, "training_acc": 60.0, "val_loss": 2098.4986610412598, "val_acc": 72.0}
{"epoch": 35, "training_loss": 7348.174911499023, "training_acc": 57.666666666666664, "val_loss": 1612.9270324707031, "val_acc": 28.0}
{"epoch": 36, "training_loss": 8401.822654724121, "training_acc": 62.333333333333336, "val_loss": 2866.3390007019043, "val_acc": 72.0}
{"epoch": 37, "training_loss": 7067.696197509766, "training_acc": 67.66666666666667, "val_loss": 2964.6354026794434, "val_acc": 28.0}
{"epoch": 38, "training_loss": 9400.977279663086, "training_acc": 49.666666666666664, "val_loss": 3300.405300140381, "val_acc": 72.0}
{"epoch": 39, "training_loss": 8590.358986139297, "training_acc": 73.0, "val_loss": 2587.768865585327, "val_acc": 28.0}
{"epoch": 40, "training_loss": 6720.247978210449, "training_acc": 53.333333333333336, "val_loss": 2043.332447052002, "val_acc": 72.0}
{"epoch": 41, "training_loss": 5882.113334655762, "training_acc": 66.33333333333333, "val_loss": 1883.9501781463623, "val_acc": 28.0}
{"epoch": 42, "training_loss": 3435.997386932373, "training_acc": 60.333333333333336, "val_loss": 870.5579710006714, "val_acc": 28.0}
{"epoch": 43, "training_loss": 1646.3774337768555, "training_acc": 53.0, "val_loss": 598.2369194030762, "val_acc": 72.0}
{"epoch": 44, "training_loss": 1762.7229130268097, "training_acc": 66.33333333333333, "val_loss": 753.8138914108276, "val_acc": 72.0}
{"epoch": 45, "training_loss": 2868.7896728515625, "training_acc": 65.66666666666667, "val_loss": 416.7568025588989, "val_acc": 28.0}
{"epoch": 46, "training_loss": 6189.9340744018555, "training_acc": 61.0, "val_loss": 1514.2585754394531, "val_acc": 72.0}
