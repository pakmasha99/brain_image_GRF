"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 300 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 6448.450401306152, "training_acc": 57.0, "val_loss": 2497.9886779785156, "val_acc": 28.0}
{"epoch": 1, "training_loss": 4034.6522331237793, "training_acc": 63.0, "val_loss": 611.3961038589478, "val_acc": 72.0}
{"epoch": 2, "training_loss": 3170.239094734192, "training_acc": 51.666666666666664, "val_loss": 376.26937913894653, "val_acc": 72.0}
{"epoch": 3, "training_loss": 1397.9363851547241, "training_acc": 58.333333333333336, "val_loss": 116.72930550575256, "val_acc": 72.0}
{"epoch": 4, "training_loss": 677.2623996734619, "training_acc": 63.666666666666664, "val_loss": 309.11075019836426, "val_acc": 28.0}
{"epoch": 5, "training_loss": 1357.810302734375, "training_acc": 60.333333333333336, "val_loss": 740.5411901473999, "val_acc": 72.0}
{"epoch": 6, "training_loss": 2401.487907409668, "training_acc": 57.0, "val_loss": 110.07130768895149, "val_acc": 72.0}
{"epoch": 7, "training_loss": 1026.4990234375, "training_acc": 65.0, "val_loss": 73.70033720135689, "val_acc": 72.0}
{"epoch": 8, "training_loss": 627.605094909668, "training_acc": 63.0, "val_loss": 218.6031675338745, "val_acc": 72.0}
{"epoch": 9, "training_loss": 1221.0905437469482, "training_acc": 53.0, "val_loss": 812.07066822052, "val_acc": 72.0}
{"epoch": 10, "training_loss": 3040.3768787384033, "training_acc": 72.33333333333333, "val_loss": 205.03689622879028, "val_acc": 28.0}
{"epoch": 11, "training_loss": 1330.1318817138672, "training_acc": 53.0, "val_loss": 381.76584792137146, "val_acc": 72.0}
{"epoch": 12, "training_loss": 1743.0093774795532, "training_acc": 53.666666666666664, "val_loss": 523.8440728187561, "val_acc": 72.0}
{"epoch": 13, "training_loss": 1910.844741821289, "training_acc": 63.0, "val_loss": 239.57929301261902, "val_acc": 28.0}
{"epoch": 14, "training_loss": 1933.8010787963867, "training_acc": 65.66666666666667, "val_loss": 450.587185382843, "val_acc": 72.0}
{"epoch": 15, "training_loss": 1987.4633719921112, "training_acc": 53.666666666666664, "val_loss": 303.3241457939148, "val_acc": 72.0}
{"epoch": 16, "training_loss": 1034.4955749511719, "training_acc": 59.0, "val_loss": 113.08361577987671, "val_acc": 72.0}
{"epoch": 17, "training_loss": 528.9815447330475, "training_acc": 63.0, "val_loss": 313.65385341644287, "val_acc": 28.0}
{"epoch": 18, "training_loss": 1119.5166239738464, "training_acc": 49.666666666666664, "val_loss": 72.10007929801941, "val_acc": 72.0}
{"epoch": 19, "training_loss": 468.3838596343994, "training_acc": 62.333333333333336, "val_loss": 333.533784866333, "val_acc": 28.0}
{"epoch": 20, "training_loss": 776.8338451385498, "training_acc": 55.0, "val_loss": 173.87373208999634, "val_acc": 72.0}
{"epoch": 21, "training_loss": 953.7313995361328, "training_acc": 68.33333333333333, "val_loss": 150.8270869255066, "val_acc": 28.0}
{"epoch": 22, "training_loss": 2213.4125175476074, "training_acc": 65.0, "val_loss": 555.3328771591187, "val_acc": 72.0}
{"epoch": 23, "training_loss": 1414.4441509246826, "training_acc": 54.333333333333336, "val_loss": 490.01541924476624, "val_acc": 72.0}
{"epoch": 24, "training_loss": 1330.918655872345, "training_acc": 59.0, "val_loss": 43.74842655658722, "val_acc": 68.0}
{"epoch": 25, "training_loss": 478.5384283065796, "training_acc": 59.666666666666664, "val_loss": 104.97197961807251, "val_acc": 72.0}
{"epoch": 26, "training_loss": 542.1009171009064, "training_acc": 66.33333333333333, "val_loss": 163.1312735080719, "val_acc": 72.0}
{"epoch": 27, "training_loss": 559.8642709255219, "training_acc": 64.33333333333333, "val_loss": 43.569148540496826, "val_acc": 65.33333333333333}
{"epoch": 28, "training_loss": 902.0071601867676, "training_acc": 63.0, "val_loss": 133.59918439388275, "val_acc": 72.0}
{"epoch": 29, "training_loss": 1829.4573783874512, "training_acc": 51.0, "val_loss": 830.9994659423828, "val_acc": 72.0}
{"epoch": 30, "training_loss": 2287.755603790283, "training_acc": 65.33333333333333, "val_loss": 492.56779861450195, "val_acc": 28.0}
{"epoch": 31, "training_loss": 2211.7953453063965, "training_acc": 65.66666666666667, "val_loss": 551.8885087966919, "val_acc": 72.0}
{"epoch": 32, "training_loss": 1513.6449794769287, "training_acc": 55.666666666666664, "val_loss": 439.65925550460815, "val_acc": 72.0}
{"epoch": 33, "training_loss": 1686.7422218322754, "training_acc": 65.0, "val_loss": 702.4174575805664, "val_acc": 28.0}
{"epoch": 34, "training_loss": 1992.967451095581, "training_acc": 61.0, "val_loss": 202.7973611354828, "val_acc": 72.0}
{"epoch": 35, "training_loss": 1319.4409065246582, "training_acc": 52.333333333333336, "val_loss": 666.3708200454712, "val_acc": 72.0}
{"epoch": 36, "training_loss": 1555.7551555633545, "training_acc": 58.333333333333336, "val_loss": 392.8587112426758, "val_acc": 72.0}
{"epoch": 37, "training_loss": 2349.3792114257812, "training_acc": 72.33333333333333, "val_loss": 69.24912017583847, "val_acc": 29.333333333333332}
{"epoch": 38, "training_loss": 765.6510400772095, "training_acc": 58.666666666666664, "val_loss": 61.07731795310974, "val_acc": 36.0}
{"epoch": 39, "training_loss": 336.386775970459, "training_acc": 61.666666666666664, "val_loss": 203.39497542381287, "val_acc": 72.0}
{"epoch": 40, "training_loss": 745.7975177764893, "training_acc": 62.333333333333336, "val_loss": 117.37403512001038, "val_acc": 72.0}
{"epoch": 41, "training_loss": 500.5973126888275, "training_acc": 62.0, "val_loss": 419.47345304489136, "val_acc": 28.0}
{"epoch": 42, "training_loss": 689.3300857543945, "training_acc": 59.666666666666664, "val_loss": 102.45113921165466, "val_acc": 72.0}
{"epoch": 43, "training_loss": 412.04778385162354, "training_acc": 62.333333333333336, "val_loss": 358.8624448776245, "val_acc": 28.0}
{"epoch": 44, "training_loss": 698.8145751953125, "training_acc": 49.666666666666664, "val_loss": 243.488299369812, "val_acc": 72.0}
{"epoch": 45, "training_loss": 1129.895887374878, "training_acc": 51.666666666666664, "val_loss": 789.2170715332031, "val_acc": 72.0}
{"epoch": 46, "training_loss": 2938.193619251251, "training_acc": 72.33333333333333, "val_loss": 495.83437061309814, "val_acc": 28.0}
