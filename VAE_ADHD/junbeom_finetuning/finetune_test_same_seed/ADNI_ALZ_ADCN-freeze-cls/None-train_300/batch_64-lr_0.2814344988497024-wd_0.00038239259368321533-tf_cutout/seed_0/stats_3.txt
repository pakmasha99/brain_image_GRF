"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 300 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 5813.850799560547, "training_acc": 57.0, "val_loss": 993.4864511489868, "val_acc": 28.0}
{"epoch": 1, "training_loss": 3941.071979522705, "training_acc": 63.0, "val_loss": 862.7412109375, "val_acc": 72.0}
{"epoch": 2, "training_loss": 3182.985470533371, "training_acc": 53.666666666666664, "val_loss": 496.90929222106934, "val_acc": 72.0}
{"epoch": 3, "training_loss": 2051.5841188430786, "training_acc": 72.33333333333333, "val_loss": 688.917239189148, "val_acc": 28.0}
{"epoch": 4, "training_loss": 1773.5564613342285, "training_acc": 61.0, "val_loss": 123.70210552215576, "val_acc": 72.0}
{"epoch": 5, "training_loss": 1748.0782585144043, "training_acc": 55.0, "val_loss": 831.556676864624, "val_acc": 72.0}
{"epoch": 6, "training_loss": 1694.2504444122314, "training_acc": 58.333333333333336, "val_loss": 273.8980293273926, "val_acc": 72.0}
{"epoch": 7, "training_loss": 1156.2574081420898, "training_acc": 63.0, "val_loss": 63.91527199745178, "val_acc": 72.0}
{"epoch": 8, "training_loss": 788.9074172973633, "training_acc": 67.66666666666667, "val_loss": 357.0411982536316, "val_acc": 28.0}
{"epoch": 9, "training_loss": 2297.256576538086, "training_acc": 65.66666666666667, "val_loss": 580.4665794372559, "val_acc": 72.0}
{"epoch": 10, "training_loss": 1947.8208351135254, "training_acc": 53.0, "val_loss": 376.90834045410156, "val_acc": 72.0}
{"epoch": 11, "training_loss": 1712.1549186706543, "training_acc": 64.33333333333333, "val_loss": 742.757896900177, "val_acc": 28.0}
{"epoch": 12, "training_loss": 2110.3405590057373, "training_acc": 61.666666666666664, "val_loss": 193.74854612350464, "val_acc": 72.0}
{"epoch": 13, "training_loss": 1403.986873626709, "training_acc": 52.333333333333336, "val_loss": 656.1781516075134, "val_acc": 72.0}
{"epoch": 14, "training_loss": 2271.189464569092, "training_acc": 54.333333333333336, "val_loss": 214.8322789669037, "val_acc": 72.0}
{"epoch": 15, "training_loss": 1763.8607511520386, "training_acc": 72.33333333333333, "val_loss": 368.102077960968, "val_acc": 28.0}
{"epoch": 16, "training_loss": 1594.1625061035156, "training_acc": 55.666666666666664, "val_loss": 563.4562993049622, "val_acc": 72.0}
{"epoch": 17, "training_loss": 1229.5071849822998, "training_acc": 53.0, "val_loss": 371.35961532592773, "val_acc": 72.0}
{"epoch": 18, "training_loss": 1349.253300666809, "training_acc": 56.333333333333336, "val_loss": 400.67422580718994, "val_acc": 72.0}
{"epoch": 19, "training_loss": 2230.9380855560303, "training_acc": 72.33333333333333, "val_loss": 51.16127896308899, "val_acc": 72.0}
{"epoch": 20, "training_loss": 1887.68887424469, "training_acc": 54.333333333333336, "val_loss": 429.3930826187134, "val_acc": 72.0}
{"epoch": 21, "training_loss": 1703.3333568572998, "training_acc": 52.333333333333336, "val_loss": 355.68359327316284, "val_acc": 72.0}
{"epoch": 22, "training_loss": 2108.4267024993896, "training_acc": 72.33333333333333, "val_loss": 61.90942108631134, "val_acc": 72.0}
{"epoch": 23, "training_loss": 1977.6324396133423, "training_acc": 54.333333333333336, "val_loss": 682.4251656532288, "val_acc": 72.0}
{"epoch": 24, "training_loss": 2216.2671818733215, "training_acc": 72.33333333333333, "val_loss": 1051.3142833709717, "val_acc": 28.0}
{"epoch": 25, "training_loss": 3124.4703063964844, "training_acc": 41.666666666666664, "val_loss": 1036.3958377838135, "val_acc": 72.0}
{"epoch": 26, "training_loss": 2920.679754257202, "training_acc": 65.66666666666667, "val_loss": 318.3863067626953, "val_acc": 28.0}
{"epoch": 27, "training_loss": 1513.3850059509277, "training_acc": 65.0, "val_loss": 91.15217792987823, "val_acc": 72.0}
{"epoch": 28, "training_loss": 1809.2446250915527, "training_acc": 52.333333333333336, "val_loss": 645.0631704330444, "val_acc": 72.0}
{"epoch": 29, "training_loss": 1697.5316982269287, "training_acc": 55.0, "val_loss": 407.9213423728943, "val_acc": 72.0}
{"epoch": 30, "training_loss": 2533.468210220337, "training_acc": 72.33333333333333, "val_loss": 121.96928381919861, "val_acc": 72.0}
{"epoch": 31, "training_loss": 2143.0090503692627, "training_acc": 51.333333333333336, "val_loss": 736.5476627349854, "val_acc": 72.0}
{"epoch": 32, "training_loss": 2514.9752898216248, "training_acc": 62.333333333333336, "val_loss": 124.56787967681885, "val_acc": 28.0}
{"epoch": 33, "training_loss": 1486.9259490966797, "training_acc": 61.0, "val_loss": 119.04893696308136, "val_acc": 28.0}
{"epoch": 34, "training_loss": 422.97070503234863, "training_acc": 57.333333333333336, "val_loss": 173.57589709758759, "val_acc": 72.0}
{"epoch": 35, "training_loss": 759.9246783256531, "training_acc": 63.666666666666664, "val_loss": 132.51076912879944, "val_acc": 72.0}
{"epoch": 36, "training_loss": 320.0936622619629, "training_acc": 58.666666666666664, "val_loss": 296.0565195083618, "val_acc": 72.0}
{"epoch": 37, "training_loss": 1246.8373193740845, "training_acc": 67.66666666666667, "val_loss": 99.97496902942657, "val_acc": 28.0}
{"epoch": 38, "training_loss": 1079.9448022842407, "training_acc": 63.0, "val_loss": 421.79859018325806, "val_acc": 28.0}
