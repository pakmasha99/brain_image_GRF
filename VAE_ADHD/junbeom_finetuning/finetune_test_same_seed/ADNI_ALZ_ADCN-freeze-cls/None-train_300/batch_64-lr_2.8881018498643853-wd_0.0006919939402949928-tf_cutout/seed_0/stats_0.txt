"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 300 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 58480.24005126953, "training_acc": 54.666666666666664, "val_loss": 4809.137348175049, "val_acc": 26.666666666666668}
{"epoch": 1, "training_loss": 43026.16864013672, "training_acc": 64.0, "val_loss": 10602.429962158203, "val_acc": 73.33333333333333}
{"epoch": 2, "training_loss": 24587.631729125977, "training_acc": 54.0, "val_loss": 5728.749198913574, "val_acc": 73.33333333333333}
{"epoch": 3, "training_loss": 17706.928895950317, "training_acc": 59.333333333333336, "val_loss": 428.12832975387573, "val_acc": 73.33333333333333}
{"epoch": 4, "training_loss": 10023.271026611328, "training_acc": 60.666666666666664, "val_loss": 1216.9194955825806, "val_acc": 26.666666666666668}
{"epoch": 5, "training_loss": 5651.858940124512, "training_acc": 49.333333333333336, "val_loss": 1492.0544052124023, "val_acc": 73.33333333333333}
{"epoch": 6, "training_loss": 5110.323133468628, "training_acc": 57.333333333333336, "val_loss": 2951.4871912002563, "val_acc": 73.33333333333333}
{"epoch": 7, "training_loss": 14117.886291503906, "training_acc": 64.0, "val_loss": 1789.8746700286865, "val_acc": 26.666666666666668}
{"epoch": 8, "training_loss": 25668.410522460938, "training_acc": 62.0, "val_loss": 5996.018203735352, "val_acc": 73.33333333333333}
{"epoch": 9, "training_loss": 14955.93441772461, "training_acc": 57.333333333333336, "val_loss": 3496.3893461227417, "val_acc": 73.33333333333333}
{"epoch": 10, "training_loss": 10940.866466522217, "training_acc": 63.333333333333336, "val_loss": 399.1591520309448, "val_acc": 73.33333333333333}
{"epoch": 11, "training_loss": 9505.016906738281, "training_acc": 60.666666666666664, "val_loss": 536.5681371688843, "val_acc": 26.666666666666668}
{"epoch": 12, "training_loss": 5287.430526733398, "training_acc": 55.333333333333336, "val_loss": 2274.8247470855713, "val_acc": 73.33333333333333}
{"epoch": 13, "training_loss": 6512.415977478027, "training_acc": 63.333333333333336, "val_loss": 659.3783903121948, "val_acc": 26.666666666666668}
{"epoch": 14, "training_loss": 6694.012519836426, "training_acc": 60.0, "val_loss": 1622.3626708984375, "val_acc": 73.33333333333333}
{"epoch": 15, "training_loss": 5705.609001159668, "training_acc": 67.33333333333333, "val_loss": 643.3573884963989, "val_acc": 73.33333333333333}
{"epoch": 16, "training_loss": 5814.017108917236, "training_acc": 54.666666666666664, "val_loss": 2772.9936180114746, "val_acc": 73.33333333333333}
{"epoch": 17, "training_loss": 12632.645095825195, "training_acc": 66.0, "val_loss": 1567.2709426879883, "val_acc": 26.666666666666668}
{"epoch": 18, "training_loss": 22619.994384765625, "training_acc": 64.0, "val_loss": 4930.0376052856445, "val_acc": 73.33333333333333}
{"epoch": 19, "training_loss": 14581.07894897461, "training_acc": 56.666666666666664, "val_loss": 3851.0748920440674, "val_acc": 73.33333333333333}
{"epoch": 20, "training_loss": 11064.054253339767, "training_acc": 60.0, "val_loss": 2444.9344520568848, "val_acc": 73.33333333333333}
{"epoch": 21, "training_loss": 9418.462215423584, "training_acc": 62.0, "val_loss": 954.1542663574219, "val_acc": 73.33333333333333}
{"epoch": 22, "training_loss": 9050.648162841797, "training_acc": 62.0, "val_loss": 448.7809524536133, "val_acc": 73.33333333333333}
{"epoch": 23, "training_loss": 19017.070220947266, "training_acc": 52.666666666666664, "val_loss": 5353.65185546875, "val_acc": 73.33333333333333}
{"epoch": 24, "training_loss": 17063.128814697266, "training_acc": 56.666666666666664, "val_loss": 3497.0368118286133, "val_acc": 73.33333333333333}
{"epoch": 25, "training_loss": 32179.262268066406, "training_acc": 72.0, "val_loss": 2538.502040863037, "val_acc": 73.33333333333333}
{"epoch": 26, "training_loss": 13105.008094787598, "training_acc": 56.666666666666664, "val_loss": 2269.9131240844727, "val_acc": 73.33333333333333}
{"epoch": 27, "training_loss": 13351.156387329102, "training_acc": 50.666666666666664, "val_loss": 7628.222930908203, "val_acc": 73.33333333333333}
{"epoch": 28, "training_loss": 31469.023559570312, "training_acc": 72.0, "val_loss": 2278.2911834716797, "val_acc": 26.666666666666668}
{"epoch": 29, "training_loss": 15222.17172241211, "training_acc": 52.0, "val_loss": 4034.8711471557617, "val_acc": 73.33333333333333}
