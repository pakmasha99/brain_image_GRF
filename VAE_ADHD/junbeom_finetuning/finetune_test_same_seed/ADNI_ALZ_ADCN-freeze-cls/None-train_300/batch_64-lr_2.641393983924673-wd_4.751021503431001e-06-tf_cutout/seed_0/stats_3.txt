"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 300 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 58959.447608947754, "training_acc": 54.333333333333336, "val_loss": 14412.906311035156, "val_acc": 28.0}
{"epoch": 1, "training_loss": 33137.886169433594, "training_acc": 65.0, "val_loss": 6737.326549530029, "val_acc": 72.0}
{"epoch": 2, "training_loss": 31229.5958404541, "training_acc": 45.0, "val_loss": 8360.868576049805, "val_acc": 72.0}
{"epoch": 3, "training_loss": 66259.61389160156, "training_acc": 72.33333333333333, "val_loss": 18362.188751220703, "val_acc": 72.0}
{"epoch": 4, "training_loss": 47068.98635864258, "training_acc": 65.0, "val_loss": 11908.603515625, "val_acc": 28.0}
{"epoch": 5, "training_loss": 28396.830505371094, "training_acc": 54.333333333333336, "val_loss": 10533.810302734375, "val_acc": 72.0}
{"epoch": 6, "training_loss": 26498.61420440674, "training_acc": 67.66666666666667, "val_loss": 1664.9540481567383, "val_acc": 28.0}
{"epoch": 7, "training_loss": 16050.376159667969, "training_acc": 61.0, "val_loss": 983.1983728408813, "val_acc": 72.0}
{"epoch": 8, "training_loss": 16757.45139312744, "training_acc": 55.666666666666664, "val_loss": 2829.183307647705, "val_acc": 72.0}
{"epoch": 9, "training_loss": 6361.109537124634, "training_acc": 65.0, "val_loss": 1893.4445514678955, "val_acc": 28.0}
{"epoch": 10, "training_loss": 9177.80318069458, "training_acc": 61.0, "val_loss": 7275.523193359375, "val_acc": 28.0}
{"epoch": 11, "training_loss": 18878.1912689209, "training_acc": 55.0, "val_loss": 4847.619773864746, "val_acc": 72.0}
{"epoch": 12, "training_loss": 8119.4091148376465, "training_acc": 61.0, "val_loss": 4842.171268463135, "val_acc": 28.0}
{"epoch": 13, "training_loss": 7163.404178619385, "training_acc": 54.666666666666664, "val_loss": 2163.6645545959473, "val_acc": 72.0}
{"epoch": 14, "training_loss": 9360.642093658447, "training_acc": 61.666666666666664, "val_loss": 940.6063899993896, "val_acc": 72.0}
{"epoch": 15, "training_loss": 5305.344711303711, "training_acc": 64.66666666666667, "val_loss": 1230.1702880859375, "val_acc": 72.0}
{"epoch": 16, "training_loss": 8959.053131103516, "training_acc": 59.666666666666664, "val_loss": 341.88081908226013, "val_acc": 72.0}
{"epoch": 17, "training_loss": 16579.093658447266, "training_acc": 54.333333333333336, "val_loss": 4835.626953125, "val_acc": 72.0}
{"epoch": 18, "training_loss": 15809.966903686523, "training_acc": 55.0, "val_loss": 3512.806282043457, "val_acc": 72.0}
{"epoch": 19, "training_loss": 25703.89501953125, "training_acc": 72.33333333333333, "val_loss": 2773.552116394043, "val_acc": 72.0}
{"epoch": 20, "training_loss": 14445.466400146484, "training_acc": 53.0, "val_loss": 3794.204315185547, "val_acc": 72.0}
{"epoch": 21, "training_loss": 16188.378631591797, "training_acc": 58.333333333333336, "val_loss": 1030.2747678756714, "val_acc": 72.0}
{"epoch": 22, "training_loss": 8279.350332260132, "training_acc": 72.33333333333333, "val_loss": 9278.7359085083, "val_acc": 28.0}
{"epoch": 23, "training_loss": 25149.447052001953, "training_acc": 48.333333333333336, "val_loss": 3796.362594604492, "val_acc": 72.0}
{"epoch": 24, "training_loss": 18922.288639068604, "training_acc": 51.0, "val_loss": 3966.3919372558594, "val_acc": 72.0}
{"epoch": 25, "training_loss": 14679.198791503906, "training_acc": 65.0, "val_loss": 2977.339729309082, "val_acc": 28.0}
{"epoch": 26, "training_loss": 22808.29766845703, "training_acc": 59.666666666666664, "val_loss": 5004.264717102051, "val_acc": 72.0}
{"epoch": 27, "training_loss": 19481.635787963867, "training_acc": 53.333333333333336, "val_loss": 3582.469079017639, "val_acc": 72.0}
{"epoch": 28, "training_loss": 17085.871812820435, "training_acc": 72.33333333333333, "val_loss": 7001.817291259766, "val_acc": 28.0}
{"epoch": 29, "training_loss": 21469.020141601562, "training_acc": 52.333333333333336, "val_loss": 5301.419738769531, "val_acc": 72.0}
{"epoch": 30, "training_loss": 12398.482772827148, "training_acc": 53.666666666666664, "val_loss": 6097.8712158203125, "val_acc": 72.0}
{"epoch": 31, "training_loss": 21344.827434539795, "training_acc": 72.33333333333333, "val_loss": 8756.785446166992, "val_acc": 28.0}
{"epoch": 32, "training_loss": 21924.161491394043, "training_acc": 53.666666666666664, "val_loss": 6648.694709777832, "val_acc": 72.0}
{"epoch": 33, "training_loss": 17824.024322509766, "training_acc": 68.33333333333333, "val_loss": 10141.183059692383, "val_acc": 28.0}
{"epoch": 34, "training_loss": 33208.04309082031, "training_acc": 51.0, "val_loss": 14313.548217773438, "val_acc": 72.0}
{"epoch": 35, "training_loss": 43297.379066467285, "training_acc": 72.33333333333333, "val_loss": 5801.035194396973, "val_acc": 28.0}
