"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 300 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 258595.20600128174, "training_acc": 60.333333333333336, "val_loss": 14805.865264892578, "val_acc": 72.0}
{"epoch": 1, "training_loss": 95975.87353515625, "training_acc": 61.666666666666664, "val_loss": 15810.48226928711, "val_acc": 72.0}
{"epoch": 2, "training_loss": 107623.52124023438, "training_acc": 52.333333333333336, "val_loss": 51858.47204589844, "val_acc": 72.0}
{"epoch": 3, "training_loss": 109175.92316055298, "training_acc": 63.0, "val_loss": 5150.38444519043, "val_acc": 28.0}
{"epoch": 4, "training_loss": 59544.06982421875, "training_acc": 65.66666666666667, "val_loss": 15061.207565307617, "val_acc": 28.0}
{"epoch": 5, "training_loss": 89518.7861328125, "training_acc": 55.666666666666664, "val_loss": 39754.830963134766, "val_acc": 72.0}
{"epoch": 6, "training_loss": 103375.59265136719, "training_acc": 59.666666666666664, "val_loss": 8963.134902954102, "val_acc": 72.0}
{"epoch": 7, "training_loss": 66650.869140625, "training_acc": 61.666666666666664, "val_loss": 9647.896766662598, "val_acc": 28.0}
{"epoch": 8, "training_loss": 109414.16931152344, "training_acc": 65.0, "val_loss": 28715.125518798828, "val_acc": 72.0}
{"epoch": 9, "training_loss": 89883.1943359375, "training_acc": 52.333333333333336, "val_loss": 25826.13787841797, "val_acc": 72.0}
{"epoch": 10, "training_loss": 91437.67407226562, "training_acc": 65.66666666666667, "val_loss": 32790.7294921875, "val_acc": 28.0}
{"epoch": 11, "training_loss": 89550.96508789062, "training_acc": 64.33333333333333, "val_loss": 8117.152671813965, "val_acc": 72.0}
{"epoch": 12, "training_loss": 88089.248046875, "training_acc": 51.666666666666664, "val_loss": 37409.72903442383, "val_acc": 72.0}
{"epoch": 13, "training_loss": 72944.97128295898, "training_acc": 61.666666666666664, "val_loss": 1718.490306854248, "val_acc": 28.0}
{"epoch": 14, "training_loss": 49821.52261352539, "training_acc": 63.0, "val_loss": 31958.106018066406, "val_acc": 28.0}
{"epoch": 15, "training_loss": 103140.79760742188, "training_acc": 54.333333333333336, "val_loss": 29663.38198852539, "val_acc": 72.0}
{"epoch": 16, "training_loss": 57030.18801879883, "training_acc": 55.666666666666664, "val_loss": 16380.159240722656, "val_acc": 28.0}
{"epoch": 17, "training_loss": 52200.14587402344, "training_acc": 52.333333333333336, "val_loss": 6966.025680541992, "val_acc": 28.0}
{"epoch": 18, "training_loss": 128121.12622070312, "training_acc": 63.0, "val_loss": 28821.60009765625, "val_acc": 72.0}
{"epoch": 19, "training_loss": 94922.75073242188, "training_acc": 53.666666666666664, "val_loss": 18493.45021057129, "val_acc": 72.0}
{"epoch": 20, "training_loss": 58167.78273010254, "training_acc": 61.0, "val_loss": 1505.9391841888428, "val_acc": 72.0}
{"epoch": 21, "training_loss": 32336.235748291016, "training_acc": 64.33333333333333, "val_loss": 19001.886306762695, "val_acc": 28.0}
{"epoch": 22, "training_loss": 34851.53625488281, "training_acc": 53.666666666666664, "val_loss": 17166.863250732422, "val_acc": 72.0}
{"epoch": 23, "training_loss": 46119.100830078125, "training_acc": 61.0, "val_loss": 22122.099822998047, "val_acc": 72.0}
{"epoch": 24, "training_loss": 107911.92205810547, "training_acc": 72.33333333333333, "val_loss": 16642.466079711914, "val_acc": 28.0}
{"epoch": 25, "training_loss": 98348.15515136719, "training_acc": 51.0, "val_loss": 33310.023498535156, "val_acc": 72.0}
{"epoch": 26, "training_loss": 130574.91458129883, "training_acc": 57.0, "val_loss": 5269.640892028809, "val_acc": 28.0}
{"epoch": 27, "training_loss": 239311.84814453125, "training_acc": 59.666666666666664, "val_loss": 74351.28942871094, "val_acc": 72.0}
{"epoch": 28, "training_loss": 198586.99951171875, "training_acc": 62.333333333333336, "val_loss": 80271.63708496094, "val_acc": 28.0}
{"epoch": 29, "training_loss": 185550.2744140625, "training_acc": 54.333333333333336, "val_loss": 55685.81756591797, "val_acc": 72.0}
{"epoch": 30, "training_loss": 141861.39453125, "training_acc": 66.33333333333333, "val_loss": 35256.132415771484, "val_acc": 28.0}
{"epoch": 31, "training_loss": 68380.91806030273, "training_acc": 61.0, "val_loss": 27091.22885131836, "val_acc": 28.0}
{"epoch": 32, "training_loss": 96057.298828125, "training_acc": 55.666666666666664, "val_loss": 36706.86865234375, "val_acc": 72.0}
{"epoch": 33, "training_loss": 103049.27124023438, "training_acc": 57.0, "val_loss": 11862.805809020996, "val_acc": 72.0}
{"epoch": 34, "training_loss": 60685.28125, "training_acc": 72.33333333333333, "val_loss": 45176.045654296875, "val_acc": 28.0}
{"epoch": 35, "training_loss": 104685.74310302734, "training_acc": 53.666666666666664, "val_loss": 12977.656784057617, "val_acc": 72.0}
{"epoch": 36, "training_loss": 83472.48321533203, "training_acc": 53.666666666666664, "val_loss": 26101.296752929688, "val_acc": 72.0}
{"epoch": 37, "training_loss": 101931.3046875, "training_acc": 62.333333333333336, "val_loss": 22758.846466064453, "val_acc": 28.0}
{"epoch": 38, "training_loss": 124712.82653808594, "training_acc": 61.0, "val_loss": 19920.281845092773, "val_acc": 72.0}
{"epoch": 39, "training_loss": 104189.15173721313, "training_acc": 55.0, "val_loss": 10870.73843383789, "val_acc": 72.0}
