"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 300 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 51205.16786956787, "training_acc": 66.33333333333333, "val_loss": 12585.101852416992, "val_acc": 28.0}
{"epoch": 1, "training_loss": 40836.124938964844, "training_acc": 60.666666666666664, "val_loss": 12957.475616455078, "val_acc": 72.0}
{"epoch": 2, "training_loss": 28219.330078125, "training_acc": 65.66666666666667, "val_loss": 3777.08744430542, "val_acc": 74.66666666666667}
{"epoch": 3, "training_loss": 17899.55519104004, "training_acc": 69.66666666666667, "val_loss": 4303.135070800781, "val_acc": 70.66666666666667}
{"epoch": 4, "training_loss": 11715.894035339355, "training_acc": 64.66666666666667, "val_loss": 2656.944007873535, "val_acc": 72.0}
{"epoch": 5, "training_loss": 8532.428100585938, "training_acc": 61.666666666666664, "val_loss": 2406.659740447998, "val_acc": 72.0}
{"epoch": 6, "training_loss": 7596.6749267578125, "training_acc": 62.0, "val_loss": 2267.9633255004883, "val_acc": 72.0}
{"epoch": 7, "training_loss": 7318.770614624023, "training_acc": 65.33333333333333, "val_loss": 3252.3503789901733, "val_acc": 70.66666666666667}
{"epoch": 8, "training_loss": 8421.109329223633, "training_acc": 67.66666666666667, "val_loss": 2387.4571113586426, "val_acc": 70.66666666666667}
{"epoch": 9, "training_loss": 8527.467548370361, "training_acc": 60.666666666666664, "val_loss": 1738.8571643829346, "val_acc": 72.0}
{"epoch": 10, "training_loss": 4270.507186889648, "training_acc": 69.33333333333333, "val_loss": 637.5425500869751, "val_acc": 70.66666666666667}
{"epoch": 11, "training_loss": 5346.9855880737305, "training_acc": 58.666666666666664, "val_loss": 903.7108669281006, "val_acc": 72.0}
{"epoch": 12, "training_loss": 2831.581456184387, "training_acc": 70.66666666666667, "val_loss": 662.9855360984802, "val_acc": 64.0}
{"epoch": 13, "training_loss": 2359.1172647476196, "training_acc": 70.0, "val_loss": 880.9829635620117, "val_acc": 72.0}
{"epoch": 14, "training_loss": 3731.55224609375, "training_acc": 71.66666666666667, "val_loss": 1270.8395099639893, "val_acc": 72.0}
{"epoch": 15, "training_loss": 8019.688888549805, "training_acc": 62.0, "val_loss": 4576.38618850708, "val_acc": 72.0}
{"epoch": 16, "training_loss": 10052.227188110352, "training_acc": 62.333333333333336, "val_loss": 2790.496047973633, "val_acc": 70.66666666666667}
{"epoch": 17, "training_loss": 11715.749671936035, "training_acc": 74.66666666666667, "val_loss": 2909.927547454834, "val_acc": 53.333333333333336}
{"epoch": 18, "training_loss": 12538.412902832031, "training_acc": 61.333333333333336, "val_loss": 2117.1873168945312, "val_acc": 72.0}
{"epoch": 19, "training_loss": 7909.920951843262, "training_acc": 68.0, "val_loss": 1942.2395858764648, "val_acc": 72.0}
{"epoch": 20, "training_loss": 6147.8638916015625, "training_acc": 65.66666666666667, "val_loss": 3226.6901206970215, "val_acc": 72.0}
{"epoch": 21, "training_loss": 10586.811576843262, "training_acc": 60.0, "val_loss": 3330.3847999572754, "val_acc": 72.0}
{"epoch": 22, "training_loss": 16266.119234085083, "training_acc": 72.0, "val_loss": 3273.3599643707275, "val_acc": 42.666666666666664}
{"epoch": 23, "training_loss": 11403.752227783203, "training_acc": 59.666666666666664, "val_loss": 1480.1128273010254, "val_acc": 77.33333333333333}
{"epoch": 24, "training_loss": 10296.645156860352, "training_acc": 64.0, "val_loss": 5922.827724456787, "val_acc": 72.0}
{"epoch": 25, "training_loss": 16064.351211547852, "training_acc": 67.33333333333333, "val_loss": 1794.876859664917, "val_acc": 68.0}
{"epoch": 26, "training_loss": 8834.382949829102, "training_acc": 71.0, "val_loss": 1438.5943660736084, "val_acc": 66.66666666666667}
{"epoch": 27, "training_loss": 5827.372329711914, "training_acc": 69.33333333333333, "val_loss": 815.62229347229, "val_acc": 69.33333333333333}
{"epoch": 28, "training_loss": 5147.377214431763, "training_acc": 68.0, "val_loss": 2212.8548679351807, "val_acc": 45.333333333333336}
{"epoch": 29, "training_loss": 3851.7967224121094, "training_acc": 71.33333333333333, "val_loss": 1188.5253067016602, "val_acc": 60.0}
