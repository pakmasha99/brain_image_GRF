"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 300 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 15055.989002227783, "training_acc": 68.0, "val_loss": 5221.108028411865, "val_acc": 28.0}
{"epoch": 1, "training_loss": 16138.355407714844, "training_acc": 61.0, "val_loss": 5143.113662719727, "val_acc": 72.0}
{"epoch": 2, "training_loss": 12943.454467773438, "training_acc": 68.0, "val_loss": 2246.754056930542, "val_acc": 53.333333333333336}
{"epoch": 3, "training_loss": 7007.3167724609375, "training_acc": 66.0, "val_loss": 2386.555690765381, "val_acc": 72.0}
{"epoch": 4, "training_loss": 6151.238456726074, "training_acc": 66.66666666666667, "val_loss": 996.7334578037262, "val_acc": 62.666666666666664}
{"epoch": 5, "training_loss": 5977.307991027832, "training_acc": 66.0, "val_loss": 474.8442487716675, "val_acc": 70.66666666666667}
{"epoch": 6, "training_loss": 3955.449451446533, "training_acc": 55.333333333333336, "val_loss": 1499.58589553833, "val_acc": 72.0}
{"epoch": 7, "training_loss": 4412.069046020508, "training_acc": 62.666666666666664, "val_loss": 371.8785262107849, "val_acc": 74.66666666666667}
{"epoch": 8, "training_loss": 2579.5665321350098, "training_acc": 72.33333333333333, "val_loss": 1334.9137077331543, "val_acc": 42.666666666666664}
{"epoch": 9, "training_loss": 3416.444065093994, "training_acc": 65.66666666666667, "val_loss": 477.5871195793152, "val_acc": 70.66666666666667}
{"epoch": 10, "training_loss": 3293.2913131713867, "training_acc": 63.0, "val_loss": 556.3937063217163, "val_acc": 72.0}
{"epoch": 11, "training_loss": 2897.729347229004, "training_acc": 63.0, "val_loss": 551.5461578369141, "val_acc": 72.0}
{"epoch": 12, "training_loss": 2081.7136878967285, "training_acc": 66.0, "val_loss": 414.65377616882324, "val_acc": 72.0}
{"epoch": 13, "training_loss": 1621.541416168213, "training_acc": 67.66666666666667, "val_loss": 435.05561113357544, "val_acc": 72.0}
{"epoch": 14, "training_loss": 2107.687334060669, "training_acc": 65.33333333333333, "val_loss": 352.06443578004837, "val_acc": 72.0}
{"epoch": 15, "training_loss": 1959.902198791504, "training_acc": 66.33333333333333, "val_loss": 484.96281719207764, "val_acc": 72.0}
{"epoch": 16, "training_loss": 2014.8784866333008, "training_acc": 67.66666666666667, "val_loss": 633.7472505569458, "val_acc": 73.33333333333333}
{"epoch": 17, "training_loss": 1868.6809644699097, "training_acc": 72.66666666666667, "val_loss": 412.13351917266846, "val_acc": 77.33333333333333}
{"epoch": 18, "training_loss": 1277.1059265136719, "training_acc": 72.0, "val_loss": 103.61377704143524, "val_acc": 76.0}
{"epoch": 19, "training_loss": 1133.0846843719482, "training_acc": 68.0, "val_loss": 206.76614379882812, "val_acc": 74.66666666666667}
{"epoch": 20, "training_loss": 695.9860033988953, "training_acc": 75.66666666666667, "val_loss": 388.7291898727417, "val_acc": 40.0}
{"epoch": 21, "training_loss": 1041.3137292861938, "training_acc": 70.0, "val_loss": 160.6319785118103, "val_acc": 78.66666666666667}
{"epoch": 22, "training_loss": 600.2542147636414, "training_acc": 74.66666666666667, "val_loss": 116.8464183807373, "val_acc": 66.66666666666667}
{"epoch": 23, "training_loss": 979.2949743270874, "training_acc": 70.0, "val_loss": 375.45858240127563, "val_acc": 72.0}
{"epoch": 24, "training_loss": 1096.2875747680664, "training_acc": 72.33333333333333, "val_loss": 146.5206332206726, "val_acc": 78.66666666666667}
{"epoch": 25, "training_loss": 1472.034336090088, "training_acc": 61.333333333333336, "val_loss": 291.9664442539215, "val_acc": 72.0}
{"epoch": 26, "training_loss": 754.1985988616943, "training_acc": 74.33333333333333, "val_loss": 121.52021753787994, "val_acc": 78.66666666666667}
{"epoch": 27, "training_loss": 746.9946718215942, "training_acc": 74.0, "val_loss": 96.37792146205902, "val_acc": 80.0}
{"epoch": 28, "training_loss": 840.451057434082, "training_acc": 72.66666666666667, "val_loss": 345.09109830856323, "val_acc": 74.66666666666667}
{"epoch": 29, "training_loss": 1365.0506858825684, "training_acc": 75.0, "val_loss": 204.2085886001587, "val_acc": 77.33333333333333}
{"epoch": 30, "training_loss": 961.4430112838745, "training_acc": 74.33333333333333, "val_loss": 657.1418461799622, "val_acc": 37.333333333333336}
{"epoch": 31, "training_loss": 1974.5266151428223, "training_acc": 60.666666666666664, "val_loss": 169.5837823152542, "val_acc": 76.0}
{"epoch": 32, "training_loss": 685.3253601789474, "training_acc": 78.66666666666667, "val_loss": 303.13604068756104, "val_acc": 72.0}
{"epoch": 33, "training_loss": 1205.3952684402466, "training_acc": 71.0, "val_loss": 201.86179113388062, "val_acc": 77.33333333333333}
{"epoch": 34, "training_loss": 875.6620006561279, "training_acc": 75.66666666666667, "val_loss": 233.90564012527466, "val_acc": 66.66666666666667}
{"epoch": 35, "training_loss": 1043.2504863739014, "training_acc": 63.333333333333336, "val_loss": 190.6346561908722, "val_acc": 80.0}
{"epoch": 36, "training_loss": 1366.9245624542236, "training_acc": 66.66666666666667, "val_loss": 768.0390663146973, "val_acc": 33.333333333333336}
{"epoch": 37, "training_loss": 1746.6859092712402, "training_acc": 69.0, "val_loss": 1413.157325744629, "val_acc": 30.666666666666668}
{"epoch": 38, "training_loss": 3763.9326782226562, "training_acc": 63.666666666666664, "val_loss": 371.3172297477722, "val_acc": 76.0}
{"epoch": 39, "training_loss": 1921.6885833740234, "training_acc": 68.0, "val_loss": 442.83933568000793, "val_acc": 76.0}
{"epoch": 40, "training_loss": 2018.8249397277832, "training_acc": 72.0, "val_loss": 929.1292133331299, "val_acc": 72.0}
{"epoch": 41, "training_loss": 1722.8360214233398, "training_acc": 69.66666666666667, "val_loss": 308.9254631996155, "val_acc": 77.33333333333333}
{"epoch": 42, "training_loss": 669.5163507461548, "training_acc": 79.0, "val_loss": 148.61411345005035, "val_acc": 78.66666666666667}
{"epoch": 43, "training_loss": 468.7740750312805, "training_acc": 82.0, "val_loss": 156.5300394296646, "val_acc": 77.33333333333333}
{"epoch": 44, "training_loss": 355.47457790374756, "training_acc": 80.0, "val_loss": 251.81977713108063, "val_acc": 64.0}
{"epoch": 45, "training_loss": 684.5598134994507, "training_acc": 74.66666666666667, "val_loss": 266.8359098434448, "val_acc": 77.33333333333333}
{"epoch": 46, "training_loss": 1019.1175403594971, "training_acc": 69.33333333333333, "val_loss": 127.63481879234314, "val_acc": 80.0}
