"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 300 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 3118.5234699249268, "training_acc": 60.333333333333336, "val_loss": 587.442512512207, "val_acc": 72.0}
{"epoch": 1, "training_loss": 2110.3462867736816, "training_acc": 66.0, "val_loss": 179.97480034828186, "val_acc": 73.33333333333333}
{"epoch": 2, "training_loss": 869.3857426643372, "training_acc": 70.66666666666667, "val_loss": 135.60406184196472, "val_acc": 72.0}
{"epoch": 3, "training_loss": 634.7852802276611, "training_acc": 64.0, "val_loss": 79.186039686203, "val_acc": 70.66666666666667}
{"epoch": 4, "training_loss": 332.2322678565979, "training_acc": 68.33333333333333, "val_loss": 171.2173023223877, "val_acc": 37.333333333333336}
{"epoch": 5, "training_loss": 292.79655265808105, "training_acc": 69.0, "val_loss": 83.09073805809021, "val_acc": 72.0}
{"epoch": 6, "training_loss": 310.7173066139221, "training_acc": 67.33333333333333, "val_loss": 58.403505086898804, "val_acc": 72.0}
{"epoch": 7, "training_loss": 549.8005881309509, "training_acc": 64.0, "val_loss": 248.3620948791504, "val_acc": 30.666666666666668}
{"epoch": 8, "training_loss": 641.5461466312408, "training_acc": 65.33333333333333, "val_loss": 191.25405526161194, "val_acc": 48.0}
{"epoch": 9, "training_loss": 972.8231115341187, "training_acc": 62.333333333333336, "val_loss": 87.48932293057442, "val_acc": 72.0}
{"epoch": 10, "training_loss": 427.6580238342285, "training_acc": 66.0, "val_loss": 94.89451229572296, "val_acc": 50.666666666666664}
{"epoch": 11, "training_loss": 546.4027843475342, "training_acc": 61.0, "val_loss": 37.76492241024971, "val_acc": 74.66666666666667}
{"epoch": 12, "training_loss": 638.7844505310059, "training_acc": 68.0, "val_loss": 76.71712028980255, "val_acc": 62.666666666666664}
{"epoch": 13, "training_loss": 860.6069355010986, "training_acc": 69.66666666666667, "val_loss": 162.51408088207245, "val_acc": 57.333333333333336}
{"epoch": 14, "training_loss": 904.9291381835938, "training_acc": 64.66666666666667, "val_loss": 176.4536337852478, "val_acc": 72.0}
{"epoch": 15, "training_loss": 619.1211361885071, "training_acc": 69.66666666666667, "val_loss": 319.40991163253784, "val_acc": 72.0}
{"epoch": 16, "training_loss": 798.2823114395142, "training_acc": 65.66666666666667, "val_loss": 106.9189475774765, "val_acc": 73.33333333333333}
{"epoch": 17, "training_loss": 592.2646462917328, "training_acc": 66.66666666666667, "val_loss": 67.18965256214142, "val_acc": 76.0}
{"epoch": 18, "training_loss": 174.0445818901062, "training_acc": 79.66666666666667, "val_loss": 82.67142534255981, "val_acc": 73.33333333333333}
{"epoch": 19, "training_loss": 202.17753410339355, "training_acc": 75.33333333333333, "val_loss": 109.87896597385406, "val_acc": 53.333333333333336}
{"epoch": 20, "training_loss": 329.47351837158203, "training_acc": 69.0, "val_loss": 57.10311099886894, "val_acc": 73.33333333333333}
{"epoch": 21, "training_loss": 254.85270535945892, "training_acc": 73.0, "val_loss": 135.08876049518585, "val_acc": 72.0}
{"epoch": 22, "training_loss": 260.8396670818329, "training_acc": 77.33333333333333, "val_loss": 79.18353652954102, "val_acc": 74.66666666666667}
{"epoch": 23, "training_loss": 200.12835550308228, "training_acc": 75.66666666666667, "val_loss": 68.65501362085342, "val_acc": 60.0}
{"epoch": 24, "training_loss": 391.2107472419739, "training_acc": 63.333333333333336, "val_loss": 64.22600431367755, "val_acc": 81.33333333333333}
{"epoch": 25, "training_loss": 229.14114212989807, "training_acc": 76.0, "val_loss": 67.63700078427792, "val_acc": 81.33333333333333}
{"epoch": 26, "training_loss": 138.29723489284515, "training_acc": 82.33333333333333, "val_loss": 54.49311828613281, "val_acc": 66.66666666666667}
{"epoch": 27, "training_loss": 179.6294730901718, "training_acc": 74.33333333333333, "val_loss": 87.6107776761055, "val_acc": 73.33333333333333}
{"epoch": 28, "training_loss": 198.83966010808945, "training_acc": 79.0, "val_loss": 68.30180335044861, "val_acc": 78.66666666666667}
{"epoch": 29, "training_loss": 134.88205242156982, "training_acc": 79.0, "val_loss": 53.95540392398834, "val_acc": 61.333333333333336}
{"epoch": 30, "training_loss": 226.04014563560486, "training_acc": 71.66666666666667, "val_loss": 101.90865921974182, "val_acc": 73.33333333333333}
