"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 300 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 17773191.036907196, "training_acc": 67.66666666666667, "val_loss": 7701076.3046875, "val_acc": 28.0}
{"epoch": 1, "training_loss": 19626316.6875, "training_acc": 45.0, "val_loss": 4525036.0625, "val_acc": 72.0}
{"epoch": 2, "training_loss": 13639643.8671875, "training_acc": 73.0, "val_loss": 1815121.294921875, "val_acc": 65.33333333333333}
{"epoch": 3, "training_loss": 9528895.34375, "training_acc": 57.333333333333336, "val_loss": 1964358.197265625, "val_acc": 66.66666666666667}
{"epoch": 4, "training_loss": 7143174.41015625, "training_acc": 74.0, "val_loss": 1602795.37890625, "val_acc": 69.33333333333333}
{"epoch": 5, "training_loss": 5325320.26953125, "training_acc": 69.66666666666667, "val_loss": 1334725.705078125, "val_acc": 69.33333333333333}
{"epoch": 6, "training_loss": 4019125.328125, "training_acc": 71.66666666666667, "val_loss": 1619417.556640625, "val_acc": 40.0}
{"epoch": 7, "training_loss": 4208024.6953125, "training_acc": 64.0, "val_loss": 780651.505859375, "val_acc": 72.0}
{"epoch": 8, "training_loss": 2054648.5341796875, "training_acc": 63.666666666666664, "val_loss": 379736.1940917969, "val_acc": 68.0}
{"epoch": 9, "training_loss": 1324850.53125, "training_acc": 70.33333333333333, "val_loss": 582168.140625, "val_acc": 56.0}
{"epoch": 10, "training_loss": 1190000.34375, "training_acc": 72.0, "val_loss": 452510.5947265625, "val_acc": 72.0}
{"epoch": 11, "training_loss": 1050187.6496582031, "training_acc": 70.66666666666667, "val_loss": 148749.876953125, "val_acc": 64.0}
{"epoch": 12, "training_loss": 1514909.3203125, "training_acc": 65.66666666666667, "val_loss": 511431.5234375, "val_acc": 72.0}
{"epoch": 13, "training_loss": 2035723.806640625, "training_acc": 64.33333333333333, "val_loss": 406142.42626953125, "val_acc": 69.33333333333333}
{"epoch": 14, "training_loss": 946371.32421875, "training_acc": 75.0, "val_loss": 285923.56640625, "val_acc": 61.333333333333336}
{"epoch": 15, "training_loss": 609006.392578125, "training_acc": 75.33333333333333, "val_loss": 922424.0263671875, "val_acc": 29.333333333333332}
{"epoch": 16, "training_loss": 2832464.5546875, "training_acc": 56.0, "val_loss": 671336.9233398438, "val_acc": 46.666666666666664}
{"epoch": 17, "training_loss": 4857320.58203125, "training_acc": 66.66666666666667, "val_loss": 584169.16015625, "val_acc": 66.66666666666667}
{"epoch": 18, "training_loss": 2971256.88671875, "training_acc": 67.0, "val_loss": 1548325.10546875, "val_acc": 72.0}
{"epoch": 19, "training_loss": 3334286.384765625, "training_acc": 72.33333333333333, "val_loss": 551770.9775390625, "val_acc": 68.0}
{"epoch": 20, "training_loss": 1388424.4501953125, "training_acc": 75.33333333333333, "val_loss": 701594.1118164062, "val_acc": 46.666666666666664}
{"epoch": 21, "training_loss": 2242421.78515625, "training_acc": 64.33333333333333, "val_loss": 303728.35400390625, "val_acc": 62.666666666666664}
{"epoch": 22, "training_loss": 911735.84765625, "training_acc": 76.66666666666667, "val_loss": 274533.4228515625, "val_acc": 74.66666666666667}
{"epoch": 23, "training_loss": 800569.3298339844, "training_acc": 74.0, "val_loss": 423082.392578125, "val_acc": 73.33333333333333}
{"epoch": 24, "training_loss": 933943.8720703125, "training_acc": 73.66666666666667, "val_loss": 261291.66479492188, "val_acc": 76.0}
{"epoch": 25, "training_loss": 625358.8771972656, "training_acc": 75.33333333333333, "val_loss": 302353.8605957031, "val_acc": 52.0}
{"epoch": 26, "training_loss": 517435.4755859375, "training_acc": 76.33333333333333, "val_loss": 166004.23181152344, "val_acc": 74.66666666666667}
{"epoch": 27, "training_loss": 1584383.529296875, "training_acc": 67.66666666666667, "val_loss": 1082387.6025390625, "val_acc": 36.0}
{"epoch": 28, "training_loss": 2006888.7548828125, "training_acc": 67.66666666666667, "val_loss": 684796.2939453125, "val_acc": 48.0}
{"epoch": 29, "training_loss": 1520698.7412109375, "training_acc": 73.33333333333333, "val_loss": 692753.9345703125, "val_acc": 45.333333333333336}
{"epoch": 30, "training_loss": 1170493.548828125, "training_acc": 72.66666666666667, "val_loss": 556823.4721679688, "val_acc": 44.0}
