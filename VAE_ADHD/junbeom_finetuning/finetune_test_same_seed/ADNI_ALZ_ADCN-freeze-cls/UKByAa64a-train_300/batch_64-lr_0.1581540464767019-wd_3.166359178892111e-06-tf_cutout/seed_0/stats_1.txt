"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 300 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 1152.7766795158386, "training_acc": 70.33333333333333, "val_loss": 492.4464168548584, "val_acc": 28.0}
{"epoch": 1, "training_loss": 997.7316341400146, "training_acc": 53.666666666666664, "val_loss": 327.7854480743408, "val_acc": 72.0}
{"epoch": 2, "training_loss": 809.9597296714783, "training_acc": 71.33333333333333, "val_loss": 194.13518953323364, "val_acc": 48.0}
{"epoch": 3, "training_loss": 440.6714897155762, "training_acc": 65.0, "val_loss": 141.52598881721497, "val_acc": 73.33333333333333}
{"epoch": 4, "training_loss": 321.3586449623108, "training_acc": 66.33333333333333, "val_loss": 82.880126953125, "val_acc": 62.666666666666664}
{"epoch": 5, "training_loss": 200.3159487247467, "training_acc": 73.66666666666667, "val_loss": 82.57467687129974, "val_acc": 50.666666666666664}
{"epoch": 6, "training_loss": 218.04904437065125, "training_acc": 66.0, "val_loss": 73.15774649381638, "val_acc": 46.666666666666664}
{"epoch": 7, "training_loss": 192.99085974693298, "training_acc": 67.0, "val_loss": 52.671057283878326, "val_acc": 61.333333333333336}
{"epoch": 8, "training_loss": 157.1549515724182, "training_acc": 75.66666666666667, "val_loss": 48.956486999988556, "val_acc": 72.0}
{"epoch": 9, "training_loss": 150.60827112197876, "training_acc": 74.33333333333333, "val_loss": 42.965852320194244, "val_acc": 64.0}
{"epoch": 10, "training_loss": 145.20379090309143, "training_acc": 77.66666666666667, "val_loss": 41.08131206035614, "val_acc": 66.66666666666667}
{"epoch": 11, "training_loss": 139.4067497253418, "training_acc": 80.0, "val_loss": 45.456880658864975, "val_acc": 65.33333333333333}
{"epoch": 12, "training_loss": 152.78681993484497, "training_acc": 76.33333333333333, "val_loss": 46.26064822077751, "val_acc": 69.33333333333333}
{"epoch": 13, "training_loss": 137.564755320549, "training_acc": 80.0, "val_loss": 44.755367428064346, "val_acc": 74.66666666666667}
{"epoch": 14, "training_loss": 140.2822666168213, "training_acc": 78.0, "val_loss": 44.57455587387085, "val_acc": 68.0}
{"epoch": 15, "training_loss": 137.0085608959198, "training_acc": 80.66666666666667, "val_loss": 52.96829700469971, "val_acc": 62.666666666666664}
{"epoch": 16, "training_loss": 147.5351333618164, "training_acc": 77.0, "val_loss": 48.603451043367386, "val_acc": 66.66666666666667}
{"epoch": 17, "training_loss": 156.43343448638916, "training_acc": 74.33333333333333, "val_loss": 47.27461612224579, "val_acc": 64.0}
{"epoch": 18, "training_loss": 167.8582170009613, "training_acc": 70.33333333333333, "val_loss": 57.583064913749695, "val_acc": 72.0}
{"epoch": 19, "training_loss": 154.89136481285095, "training_acc": 77.33333333333333, "val_loss": 62.223740577697754, "val_acc": 72.0}
{"epoch": 20, "training_loss": 149.73956394195557, "training_acc": 77.33333333333333, "val_loss": 50.37977531552315, "val_acc": 74.66666666666667}
{"epoch": 21, "training_loss": 127.34396362304688, "training_acc": 80.33333333333333, "val_loss": 45.8297443985939, "val_acc": 68.0}
{"epoch": 22, "training_loss": 122.90431427955627, "training_acc": 82.33333333333333, "val_loss": 45.34769940376282, "val_acc": 70.66666666666667}
{"epoch": 23, "training_loss": 126.52444851398468, "training_acc": 82.33333333333333, "val_loss": 46.62070977687836, "val_acc": 68.0}
{"epoch": 24, "training_loss": 143.67146635055542, "training_acc": 79.33333333333333, "val_loss": 67.41425585746765, "val_acc": 50.666666666666664}
{"epoch": 25, "training_loss": 159.91481125354767, "training_acc": 74.0, "val_loss": 63.05070585012436, "val_acc": 60.0}
{"epoch": 26, "training_loss": 138.63573372364044, "training_acc": 78.66666666666667, "val_loss": 58.69765159487724, "val_acc": 61.333333333333336}
{"epoch": 27, "training_loss": 136.50365948677063, "training_acc": 79.66666666666667, "val_loss": 48.17747211456299, "val_acc": 74.66666666666667}
{"epoch": 28, "training_loss": 142.08417630195618, "training_acc": 76.66666666666667, "val_loss": 46.3640798330307, "val_acc": 72.0}
{"epoch": 29, "training_loss": 117.2923150062561, "training_acc": 82.33333333333333, "val_loss": 53.725666999816895, "val_acc": 66.66666666666667}
