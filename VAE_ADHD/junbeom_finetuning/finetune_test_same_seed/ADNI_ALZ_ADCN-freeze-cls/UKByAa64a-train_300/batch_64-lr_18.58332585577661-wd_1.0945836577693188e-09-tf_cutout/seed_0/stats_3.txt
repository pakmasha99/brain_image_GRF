"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 300 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 122074.95240783691, "training_acc": 61.333333333333336, "val_loss": 57053.47479248047, "val_acc": 28.0}
{"epoch": 1, "training_loss": 133281.56982421875, "training_acc": 50.666666666666664, "val_loss": 26528.38787841797, "val_acc": 72.0}
{"epoch": 2, "training_loss": 73681.68249511719, "training_acc": 66.33333333333333, "val_loss": 13501.505722045898, "val_acc": 54.666666666666664}
{"epoch": 3, "training_loss": 56830.24816894531, "training_acc": 65.0, "val_loss": 10376.46664428711, "val_acc": 73.33333333333333}
{"epoch": 4, "training_loss": 43795.08595275879, "training_acc": 59.666666666666664, "val_loss": 7227.850303649902, "val_acc": 72.0}
{"epoch": 5, "training_loss": 34502.15475463867, "training_acc": 71.33333333333333, "val_loss": 10776.507888793945, "val_acc": 42.666666666666664}
{"epoch": 6, "training_loss": 27704.618621826172, "training_acc": 65.0, "val_loss": 4044.3257331848145, "val_acc": 70.66666666666667}
{"epoch": 7, "training_loss": 23532.310577392578, "training_acc": 60.0, "val_loss": 6476.209602355957, "val_acc": 72.0}
{"epoch": 8, "training_loss": 17903.651584625244, "training_acc": 67.33333333333333, "val_loss": 3967.5208892822266, "val_acc": 72.0}
{"epoch": 9, "training_loss": 17921.98487854004, "training_acc": 61.333333333333336, "val_loss": 5461.895469665527, "val_acc": 72.0}
{"epoch": 10, "training_loss": 15103.209579467773, "training_acc": 65.66666666666667, "val_loss": 2862.0309295654297, "val_acc": 72.0}
{"epoch": 11, "training_loss": 14518.022109985352, "training_acc": 64.66666666666667, "val_loss": 8081.37092590332, "val_acc": 72.0}
{"epoch": 12, "training_loss": 24307.63916015625, "training_acc": 63.333333333333336, "val_loss": 4563.805370330811, "val_acc": 72.0}
{"epoch": 13, "training_loss": 14687.191146850586, "training_acc": 68.0, "val_loss": 3182.063522338867, "val_acc": 72.0}
{"epoch": 14, "training_loss": 14443.034561157227, "training_acc": 69.33333333333333, "val_loss": 2017.7697143554688, "val_acc": 73.33333333333333}
{"epoch": 15, "training_loss": 7555.9670333862305, "training_acc": 71.66666666666667, "val_loss": 1728.189715385437, "val_acc": 74.66666666666667}
{"epoch": 16, "training_loss": 7681.438583374023, "training_acc": 73.0, "val_loss": 1139.736982345581, "val_acc": 64.0}
{"epoch": 17, "training_loss": 8795.617080688477, "training_acc": 66.33333333333333, "val_loss": 756.5749545097351, "val_acc": 78.66666666666667}
{"epoch": 18, "training_loss": 6492.445388793945, "training_acc": 71.66666666666667, "val_loss": 1524.4290256500244, "val_acc": 78.66666666666667}
{"epoch": 19, "training_loss": 5833.382835388184, "training_acc": 71.33333333333333, "val_loss": 1598.9233894348145, "val_acc": 74.66666666666667}
{"epoch": 20, "training_loss": 11568.59439086914, "training_acc": 66.0, "val_loss": 2343.2013092041016, "val_acc": 72.0}
{"epoch": 21, "training_loss": 16308.187881469727, "training_acc": 65.66666666666667, "val_loss": 804.7086074352264, "val_acc": 78.66666666666667}
{"epoch": 22, "training_loss": 5731.5766677856445, "training_acc": 77.66666666666667, "val_loss": 879.9369170665741, "val_acc": 81.33333333333333}
{"epoch": 23, "training_loss": 2950.529733657837, "training_acc": 79.66666666666667, "val_loss": 1242.3098001480103, "val_acc": 60.0}
{"epoch": 24, "training_loss": 9202.542137145996, "training_acc": 64.66666666666667, "val_loss": 1528.1522197723389, "val_acc": 74.66666666666667}
{"epoch": 25, "training_loss": 7294.5435218811035, "training_acc": 70.66666666666667, "val_loss": 1312.6517461537505, "val_acc": 74.66666666666667}
{"epoch": 26, "training_loss": 5017.417343139648, "training_acc": 75.0, "val_loss": 1707.9822082519531, "val_acc": 76.0}
{"epoch": 27, "training_loss": 7073.4580078125, "training_acc": 72.33333333333333, "val_loss": 1181.5776681900024, "val_acc": 78.66666666666667}
{"epoch": 28, "training_loss": 5882.778099060059, "training_acc": 74.33333333333333, "val_loss": 3938.4381408691406, "val_acc": 50.666666666666664}
{"epoch": 29, "training_loss": 10960.261962890625, "training_acc": 67.66666666666667, "val_loss": 1413.0306445360184, "val_acc": 80.0}
{"epoch": 30, "training_loss": 7427.109893798828, "training_acc": 71.0, "val_loss": 1322.1349334716797, "val_acc": 78.66666666666667}
{"epoch": 31, "training_loss": 7451.688888549805, "training_acc": 70.0, "val_loss": 1043.9814825057983, "val_acc": 76.0}
{"epoch": 32, "training_loss": 4551.900505065918, "training_acc": 74.33333333333333, "val_loss": 3065.3701133728027, "val_acc": 72.0}
{"epoch": 33, "training_loss": 7389.580795288086, "training_acc": 72.33333333333333, "val_loss": 800.0475578308105, "val_acc": 78.66666666666667}
{"epoch": 34, "training_loss": 4189.771934509277, "training_acc": 76.0, "val_loss": 860.3037738800049, "val_acc": 77.33333333333333}
{"epoch": 35, "training_loss": 2236.588592529297, "training_acc": 78.66666666666667, "val_loss": 3366.5011014938354, "val_acc": 72.0}
{"epoch": 36, "training_loss": 8293.609862327576, "training_acc": 69.33333333333333, "val_loss": 1384.0947666168213, "val_acc": 61.333333333333336}
