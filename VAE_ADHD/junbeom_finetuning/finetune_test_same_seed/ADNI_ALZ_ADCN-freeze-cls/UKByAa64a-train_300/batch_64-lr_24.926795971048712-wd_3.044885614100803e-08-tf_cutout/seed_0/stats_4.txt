"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 300 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 169533.76959991455, "training_acc": 57.0, "val_loss": 11960.748294830322, "val_acc": 52.0}
{"epoch": 1, "training_loss": 68511.4849243164, "training_acc": 62.666666666666664, "val_loss": 9695.918245315552, "val_acc": 73.33333333333333}
{"epoch": 2, "training_loss": 52128.599609375, "training_acc": 63.333333333333336, "val_loss": 14871.163528442383, "val_acc": 70.66666666666667}
{"epoch": 3, "training_loss": 40786.2950592041, "training_acc": 64.66666666666667, "val_loss": 10801.63687133789, "val_acc": 72.0}
{"epoch": 4, "training_loss": 27931.840301513672, "training_acc": 63.0, "val_loss": 11668.701461791992, "val_acc": 72.0}
{"epoch": 5, "training_loss": 34196.42576599121, "training_acc": 63.333333333333336, "val_loss": 11543.764694213867, "val_acc": 72.0}
{"epoch": 6, "training_loss": 42078.49444580078, "training_acc": 71.66666666666667, "val_loss": 8952.691665649414, "val_acc": 40.0}
{"epoch": 7, "training_loss": 25669.595321655273, "training_acc": 65.33333333333333, "val_loss": 3577.0550537109375, "val_acc": 76.0}
{"epoch": 8, "training_loss": 11618.22395324707, "training_acc": 69.0, "val_loss": 3797.8868103027344, "val_acc": 72.0}
{"epoch": 9, "training_loss": 18912.926483154297, "training_acc": 63.666666666666664, "val_loss": 1726.4156303405762, "val_acc": 65.33333333333333}
{"epoch": 10, "training_loss": 13020.770877838135, "training_acc": 69.66666666666667, "val_loss": 3076.718536376953, "val_acc": 57.333333333333336}
{"epoch": 11, "training_loss": 11844.700988769531, "training_acc": 64.0, "val_loss": 3926.1439514160156, "val_acc": 72.0}
{"epoch": 12, "training_loss": 11327.531509399414, "training_acc": 70.33333333333333, "val_loss": 1220.2318515777588, "val_acc": 73.33333333333333}
{"epoch": 13, "training_loss": 13366.525573730469, "training_acc": 70.0, "val_loss": 6714.884536743164, "val_acc": 40.0}
{"epoch": 14, "training_loss": 14944.664108276367, "training_acc": 66.0, "val_loss": 2347.968101501465, "val_acc": 70.66666666666667}
{"epoch": 15, "training_loss": 10420.279279708862, "training_acc": 72.0, "val_loss": 2949.422149658203, "val_acc": 72.0}
{"epoch": 16, "training_loss": 8941.978973388672, "training_acc": 67.0, "val_loss": 4904.744495391846, "val_acc": 72.0}
{"epoch": 17, "training_loss": 21500.419815063477, "training_acc": 65.66666666666667, "val_loss": 9064.924896240234, "val_acc": 72.0}
{"epoch": 18, "training_loss": 25476.138244628906, "training_acc": 67.0, "val_loss": 7761.366455078125, "val_acc": 72.0}
{"epoch": 19, "training_loss": 23153.70428466797, "training_acc": 65.33333333333333, "val_loss": 7450.169715881348, "val_acc": 72.0}
{"epoch": 20, "training_loss": 21244.7529296875, "training_acc": 66.0, "val_loss": 10857.725234985352, "val_acc": 72.0}
{"epoch": 21, "training_loss": 47961.87168884277, "training_acc": 75.0, "val_loss": 12690.735649108887, "val_acc": 41.333333333333336}
{"epoch": 22, "training_loss": 49658.26501464844, "training_acc": 57.666666666666664, "val_loss": 10239.458862304688, "val_acc": 72.0}
{"epoch": 23, "training_loss": 35718.80155944824, "training_acc": 66.0, "val_loss": 10397.006332397461, "val_acc": 70.66666666666667}
{"epoch": 24, "training_loss": 31158.302291870117, "training_acc": 64.66666666666667, "val_loss": 2684.1454524993896, "val_acc": 78.66666666666667}
{"epoch": 25, "training_loss": 10197.861755371094, "training_acc": 75.66666666666667, "val_loss": 3653.309280395508, "val_acc": 66.66666666666667}
{"epoch": 26, "training_loss": 10294.841468811035, "training_acc": 72.0, "val_loss": 2681.1920471191406, "val_acc": 74.66666666666667}
{"epoch": 27, "training_loss": 5559.751895904541, "training_acc": 78.33333333333333, "val_loss": 2172.893440246582, "val_acc": 82.66666666666667}
{"epoch": 28, "training_loss": 5390.718505859375, "training_acc": 78.33333333333333, "val_loss": 5745.949722290039, "val_acc": 46.666666666666664}
{"epoch": 29, "training_loss": 14005.839813232422, "training_acc": 60.0, "val_loss": 7345.213684082031, "val_acc": 72.0}
{"epoch": 30, "training_loss": 19005.939254760742, "training_acc": 68.0, "val_loss": 8983.435546875, "val_acc": 70.66666666666667}
{"epoch": 31, "training_loss": 18678.884452819824, "training_acc": 71.66666666666667, "val_loss": 5037.056900024414, "val_acc": 72.0}
