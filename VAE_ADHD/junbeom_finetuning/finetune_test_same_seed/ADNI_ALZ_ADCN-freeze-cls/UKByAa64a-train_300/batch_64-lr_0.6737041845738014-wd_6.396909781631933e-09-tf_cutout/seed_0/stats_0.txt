"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 300 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 4623.8705978393555, "training_acc": 64.33333333333333, "val_loss": 1648.129550933838, "val_acc": 26.666666666666668}
{"epoch": 1, "training_loss": 3911.585868835449, "training_acc": 61.666666666666664, "val_loss": 1320.0859155654907, "val_acc": 73.33333333333333}
{"epoch": 2, "training_loss": 3420.4370155334473, "training_acc": 67.0, "val_loss": 689.7540740966797, "val_acc": 49.333333333333336}
{"epoch": 3, "training_loss": 1887.7524242401123, "training_acc": 64.66666666666667, "val_loss": 529.5919675827026, "val_acc": 74.66666666666667}
{"epoch": 4, "training_loss": 1834.1636981964111, "training_acc": 73.66666666666667, "val_loss": 464.0515031814575, "val_acc": 41.333333333333336}
{"epoch": 5, "training_loss": 1370.3868522644043, "training_acc": 63.333333333333336, "val_loss": 112.23641443252563, "val_acc": 66.66666666666667}
{"epoch": 6, "training_loss": 954.0208263397217, "training_acc": 60.333333333333336, "val_loss": 388.4110417366028, "val_acc": 73.33333333333333}
{"epoch": 7, "training_loss": 913.9754357337952, "training_acc": 66.33333333333333, "val_loss": 228.8614206314087, "val_acc": 73.33333333333333}
{"epoch": 8, "training_loss": 915.2531890869141, "training_acc": 61.0, "val_loss": 132.48081946372986, "val_acc": 73.33333333333333}
{"epoch": 9, "training_loss": 962.9273071289062, "training_acc": 65.0, "val_loss": 85.49207210540771, "val_acc": 72.0}
{"epoch": 10, "training_loss": 649.5903453826904, "training_acc": 68.0, "val_loss": 125.60858345031738, "val_acc": 62.666666666666664}
{"epoch": 11, "training_loss": 515.3096504211426, "training_acc": 68.0, "val_loss": 254.95568084716797, "val_acc": 40.0}
{"epoch": 12, "training_loss": 632.6813998222351, "training_acc": 68.0, "val_loss": 251.73370361328125, "val_acc": 36.0}
{"epoch": 13, "training_loss": 763.5324974060059, "training_acc": 61.0, "val_loss": 121.07621264457703, "val_acc": 64.0}
{"epoch": 14, "training_loss": 712.8813695907593, "training_acc": 65.66666666666667, "val_loss": 51.47456635534763, "val_acc": 80.0}
{"epoch": 15, "training_loss": 598.9814879894257, "training_acc": 66.66666666666667, "val_loss": 198.71224188804626, "val_acc": 37.333333333333336}
{"epoch": 16, "training_loss": 410.5070581436157, "training_acc": 63.0, "val_loss": 150.38922452926636, "val_acc": 73.33333333333333}
{"epoch": 17, "training_loss": 477.0938358306885, "training_acc": 68.33333333333333, "val_loss": 80.00516918301582, "val_acc": 74.66666666666667}
{"epoch": 18, "training_loss": 296.3947329521179, "training_acc": 74.0, "val_loss": 146.02235889434814, "val_acc": 73.33333333333333}
{"epoch": 19, "training_loss": 370.6635684967041, "training_acc": 68.0, "val_loss": 71.35012257099152, "val_acc": 56.0}
{"epoch": 20, "training_loss": 240.28960990905762, "training_acc": 71.33333333333333, "val_loss": 122.41735100746155, "val_acc": 73.33333333333333}
{"epoch": 21, "training_loss": 427.7867031097412, "training_acc": 66.66666666666667, "val_loss": 31.90483808517456, "val_acc": 85.33333333333333}
{"epoch": 22, "training_loss": 242.46252858638763, "training_acc": 74.66666666666667, "val_loss": 63.80769920349121, "val_acc": 60.0}
{"epoch": 23, "training_loss": 250.3885054588318, "training_acc": 67.33333333333333, "val_loss": 55.64261996746063, "val_acc": 74.66666666666667}
{"epoch": 24, "training_loss": 330.8123435974121, "training_acc": 72.0, "val_loss": 28.87094995379448, "val_acc": 82.66666666666667}
{"epoch": 25, "training_loss": 383.1763906478882, "training_acc": 64.0, "val_loss": 32.04711651802063, "val_acc": 82.66666666666667}
{"epoch": 26, "training_loss": 700.3685779571533, "training_acc": 63.0, "val_loss": 51.394678592681885, "val_acc": 84.0}
{"epoch": 27, "training_loss": 649.5129690170288, "training_acc": 72.66666666666667, "val_loss": 143.44324707984924, "val_acc": 62.666666666666664}
{"epoch": 28, "training_loss": 635.1445791721344, "training_acc": 70.33333333333333, "val_loss": 129.22205752134323, "val_acc": 62.666666666666664}
{"epoch": 29, "training_loss": 431.5115592479706, "training_acc": 71.66666666666667, "val_loss": 123.8464241027832, "val_acc": 50.666666666666664}
{"epoch": 30, "training_loss": 323.21324634552, "training_acc": 67.0, "val_loss": 32.891263633966446, "val_acc": 74.66666666666667}
{"epoch": 31, "training_loss": 187.35288286209106, "training_acc": 73.66666666666667, "val_loss": 36.67319416999817, "val_acc": 76.0}
{"epoch": 32, "training_loss": 185.67027807235718, "training_acc": 78.66666666666667, "val_loss": 64.41095352172852, "val_acc": 78.66666666666667}
{"epoch": 33, "training_loss": 160.2954638004303, "training_acc": 80.33333333333333, "val_loss": 43.102015018463135, "val_acc": 82.66666666666667}
{"epoch": 34, "training_loss": 144.3704776763916, "training_acc": 78.66666666666667, "val_loss": 91.0215514600277, "val_acc": 73.33333333333333}
{"epoch": 35, "training_loss": 362.42498111724854, "training_acc": 70.66666666666667, "val_loss": 139.419935464859, "val_acc": 73.33333333333333}
{"epoch": 36, "training_loss": 379.931734085083, "training_acc": 70.66666666666667, "val_loss": 39.99910944700241, "val_acc": 82.66666666666667}
{"epoch": 37, "training_loss": 278.30854177474976, "training_acc": 73.33333333333333, "val_loss": 92.3649069070816, "val_acc": 62.666666666666664}
{"epoch": 38, "training_loss": 587.9410593509674, "training_acc": 67.0, "val_loss": 244.57347857952118, "val_acc": 36.0}
{"epoch": 39, "training_loss": 701.5150303840637, "training_acc": 67.0, "val_loss": 61.50816035270691, "val_acc": 76.0}
{"epoch": 40, "training_loss": 632.5528688430786, "training_acc": 64.66666666666667, "val_loss": 62.289599895477295, "val_acc": 82.66666666666667}
{"epoch": 41, "training_loss": 644.3664865493774, "training_acc": 69.33333333333333, "val_loss": 145.4057658314705, "val_acc": 73.33333333333333}
{"epoch": 42, "training_loss": 398.9567222595215, "training_acc": 70.66666666666667, "val_loss": 32.715044379234314, "val_acc": 78.66666666666667}
{"epoch": 43, "training_loss": 151.89703345298767, "training_acc": 79.0, "val_loss": 41.10889941453934, "val_acc": 76.0}
