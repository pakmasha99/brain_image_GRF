"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 300 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 2377421.173538208, "training_acc": 54.333333333333336, "val_loss": 548225.90625, "val_acc": 72.0}
{"epoch": 1, "training_loss": 2223729.390625, "training_acc": 57.666666666666664, "val_loss": 105924.51373291016, "val_acc": 72.0}
{"epoch": 2, "training_loss": 1122107.9926757812, "training_acc": 72.33333333333333, "val_loss": 542317.3481445312, "val_acc": 28.0}
{"epoch": 3, "training_loss": 1870503.43359375, "training_acc": 49.0, "val_loss": 500837.3603515625, "val_acc": 72.0}
{"epoch": 4, "training_loss": 1777540.10546875, "training_acc": 57.0, "val_loss": 96111.29565429688, "val_acc": 28.0}
{"epoch": 5, "training_loss": 2378637.30859375, "training_acc": 61.666666666666664, "val_loss": 933628.111328125, "val_acc": 72.0}
{"epoch": 6, "training_loss": 2301363.369140625, "training_acc": 65.66666666666667, "val_loss": 530971.87890625, "val_acc": 28.0}
{"epoch": 7, "training_loss": 1165742.4741210938, "training_acc": 60.0, "val_loss": 142245.84790039062, "val_acc": 72.0}
{"epoch": 8, "training_loss": 756869.455078125, "training_acc": 55.333333333333336, "val_loss": 344448.4150390625, "val_acc": 72.0}
{"epoch": 9, "training_loss": 1155265.84765625, "training_acc": 58.333333333333336, "val_loss": 88685.82165527344, "val_acc": 72.0}
{"epoch": 10, "training_loss": 764933.16796875, "training_acc": 62.333333333333336, "val_loss": 6335.4160232543945, "val_acc": 81.33333333333333}
{"epoch": 11, "training_loss": 270785.865234375, "training_acc": 64.0, "val_loss": 245154.07299804688, "val_acc": 72.0}
{"epoch": 12, "training_loss": 724570.2919921875, "training_acc": 61.666666666666664, "val_loss": 6163.708614349365, "val_acc": 76.0}
{"epoch": 13, "training_loss": 526349.294921875, "training_acc": 63.333333333333336, "val_loss": 62273.22509765625, "val_acc": 72.0}
{"epoch": 14, "training_loss": 1045576.625, "training_acc": 51.0, "val_loss": 522971.9599609375, "val_acc": 72.0}
{"epoch": 15, "training_loss": 1395512.12890625, "training_acc": 68.33333333333333, "val_loss": 307643.4658203125, "val_acc": 28.0}
{"epoch": 16, "training_loss": 1178163.203125, "training_acc": 59.0, "val_loss": 29727.25518798828, "val_acc": 72.0}
{"epoch": 17, "training_loss": 1180364.267578125, "training_acc": 53.666666666666664, "val_loss": 237467.63061523438, "val_acc": 72.0}
{"epoch": 18, "training_loss": 702233.9296264648, "training_acc": 60.0, "val_loss": 227104.369140625, "val_acc": 72.0}
{"epoch": 19, "training_loss": 1063924.6760253906, "training_acc": 72.33333333333333, "val_loss": 524621.50390625, "val_acc": 28.0}
{"epoch": 20, "training_loss": 1248904.1572265625, "training_acc": 54.333333333333336, "val_loss": 175468.5828857422, "val_acc": 72.0}
{"epoch": 21, "training_loss": 783613.2260742188, "training_acc": 55.666666666666664, "val_loss": 479360.58251953125, "val_acc": 72.0}
{"epoch": 22, "training_loss": 1690727.9162597656, "training_acc": 72.33333333333333, "val_loss": 366516.4519042969, "val_acc": 28.0}
{"epoch": 23, "training_loss": 1295642.34375, "training_acc": 51.666666666666664, "val_loss": 285330.52685546875, "val_acc": 72.0}
{"epoch": 24, "training_loss": 1115932.7543945312, "training_acc": 51.0, "val_loss": 309637.4375, "val_acc": 72.0}
{"epoch": 25, "training_loss": 1124917.3056640625, "training_acc": 63.0, "val_loss": 57671.047424316406, "val_acc": 38.666666666666664}
{"epoch": 26, "training_loss": 1211632.8369140625, "training_acc": 64.0, "val_loss": 275749.8913574219, "val_acc": 72.0}
{"epoch": 27, "training_loss": 999070.1118164062, "training_acc": 57.0, "val_loss": 198725.98461914062, "val_acc": 72.0}
{"epoch": 28, "training_loss": 824715.435546875, "training_acc": 68.33333333333333, "val_loss": 440958.2822265625, "val_acc": 28.0}
{"epoch": 29, "training_loss": 825760.912109375, "training_acc": 66.33333333333333, "val_loss": 18524.67822265625, "val_acc": 72.0}
{"epoch": 30, "training_loss": 774783.708984375, "training_acc": 59.666666666666664, "val_loss": 414974.03564453125, "val_acc": 72.0}
{"epoch": 31, "training_loss": 1129933.81640625, "training_acc": 67.66666666666667, "val_loss": 760237.2666015625, "val_acc": 28.0}
