"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 300 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 1965692.704322815, "training_acc": 55.666666666666664, "val_loss": 103410.61053466797, "val_acc": 56.0}
{"epoch": 1, "training_loss": 805620.5317382812, "training_acc": 66.66666666666667, "val_loss": 56790.0009765625, "val_acc": 76.0}
{"epoch": 2, "training_loss": 396949.5646972656, "training_acc": 66.0, "val_loss": 42216.53146362305, "val_acc": 72.0}
{"epoch": 3, "training_loss": 236570.35546875, "training_acc": 62.0, "val_loss": 55486.23681640625, "val_acc": 62.666666666666664}
{"epoch": 4, "training_loss": 171867.53295898438, "training_acc": 62.0, "val_loss": 73152.24282836914, "val_acc": 72.0}
{"epoch": 5, "training_loss": 294337.23876953125, "training_acc": 66.0, "val_loss": 65610.81706237793, "val_acc": 72.0}
{"epoch": 6, "training_loss": 295579.34521484375, "training_acc": 65.33333333333333, "val_loss": 56882.59698486328, "val_acc": 72.0}
{"epoch": 7, "training_loss": 195369.23266601562, "training_acc": 68.33333333333333, "val_loss": 34870.40087890625, "val_acc": 72.0}
{"epoch": 8, "training_loss": 84962.9794921875, "training_acc": 72.66666666666667, "val_loss": 38540.92626953125, "val_acc": 49.333333333333336}
{"epoch": 9, "training_loss": 156468.9345703125, "training_acc": 64.33333333333333, "val_loss": 53971.27966308594, "val_acc": 72.0}
{"epoch": 10, "training_loss": 228941.58056640625, "training_acc": 66.0, "val_loss": 25522.585968017578, "val_acc": 76.0}
{"epoch": 11, "training_loss": 115330.14758300781, "training_acc": 71.66666666666667, "val_loss": 30580.985202789307, "val_acc": 56.0}
{"epoch": 12, "training_loss": 163819.62573242188, "training_acc": 63.333333333333336, "val_loss": 89600.20751953125, "val_acc": 72.0}
{"epoch": 13, "training_loss": 443747.27099609375, "training_acc": 57.666666666666664, "val_loss": 117884.99682617188, "val_acc": 72.0}
{"epoch": 14, "training_loss": 502166.681640625, "training_acc": 72.0, "val_loss": 130072.85754394531, "val_acc": 52.0}
{"epoch": 15, "training_loss": 456676.09326171875, "training_acc": 66.66666666666667, "val_loss": 51751.388916015625, "val_acc": 76.0}
{"epoch": 16, "training_loss": 344243.92626953125, "training_acc": 62.666666666666664, "val_loss": 102074.63537597656, "val_acc": 72.0}
{"epoch": 17, "training_loss": 236782.107421875, "training_acc": 66.0, "val_loss": 47589.664123535156, "val_acc": 70.66666666666667}
{"epoch": 18, "training_loss": 176478.27661132812, "training_acc": 68.0, "val_loss": 21247.377807617188, "val_acc": 77.33333333333333}
{"epoch": 19, "training_loss": 92252.45190429688, "training_acc": 72.33333333333333, "val_loss": 16342.941390991211, "val_acc": 78.66666666666667}
{"epoch": 20, "training_loss": 63625.190185546875, "training_acc": 75.33333333333333, "val_loss": 22875.792053222656, "val_acc": 54.666666666666664}
{"epoch": 21, "training_loss": 256832.4970703125, "training_acc": 65.33333333333333, "val_loss": 235253.81665039062, "val_acc": 28.0}
{"epoch": 22, "training_loss": 621329.19921875, "training_acc": 57.333333333333336, "val_loss": 146448.87731933594, "val_acc": 72.0}
{"epoch": 23, "training_loss": 364149.27380371094, "training_acc": 66.0, "val_loss": 114895.58508300781, "val_acc": 72.0}
{"epoch": 24, "training_loss": 355512.171875, "training_acc": 70.66666666666667, "val_loss": 32397.24757385254, "val_acc": 76.0}
{"epoch": 25, "training_loss": 260713.11279296875, "training_acc": 66.66666666666667, "val_loss": 76108.13165283203, "val_acc": 46.666666666666664}
{"epoch": 26, "training_loss": 457649.4260253906, "training_acc": 63.0, "val_loss": 136398.3653564453, "val_acc": 37.333333333333336}
{"epoch": 27, "training_loss": 500801.0439453125, "training_acc": 59.666666666666664, "val_loss": 85023.5142211914, "val_acc": 73.33333333333333}
{"epoch": 28, "training_loss": 333739.5895996094, "training_acc": 71.66666666666667, "val_loss": 112048.29345703125, "val_acc": 72.0}
{"epoch": 29, "training_loss": 338452.01416015625, "training_acc": 67.33333333333333, "val_loss": 55813.23957824707, "val_acc": 70.66666666666667}
{"epoch": 30, "training_loss": 239724.52514648438, "training_acc": 71.0, "val_loss": 28165.212997436523, "val_acc": 69.33333333333333}
{"epoch": 31, "training_loss": 152572.45135498047, "training_acc": 68.0, "val_loss": 52488.840087890625, "val_acc": 72.0}
{"epoch": 32, "training_loss": 140345.59393310547, "training_acc": 73.33333333333333, "val_loss": 23130.03987121582, "val_acc": 77.33333333333333}
{"epoch": 33, "training_loss": 79112.40869140625, "training_acc": 77.33333333333333, "val_loss": 15976.628936767578, "val_acc": 80.0}
{"epoch": 34, "training_loss": 61211.0478515625, "training_acc": 79.0, "val_loss": 16753.226623535156, "val_acc": 72.0}
{"epoch": 35, "training_loss": 86185.76086425781, "training_acc": 75.33333333333333, "val_loss": 14433.479400634766, "val_acc": 80.0}
{"epoch": 36, "training_loss": 113713.43475341797, "training_acc": 70.66666666666667, "val_loss": 13908.53515625, "val_acc": 80.0}
{"epoch": 37, "training_loss": 52031.993225097656, "training_acc": 82.33333333333333, "val_loss": 14632.976599334994, "val_acc": 80.0}
{"epoch": 38, "training_loss": 40396.64520263672, "training_acc": 82.33333333333333, "val_loss": 16778.44366455078, "val_acc": 76.0}
{"epoch": 39, "training_loss": 79091.0986328125, "training_acc": 73.66666666666667, "val_loss": 14322.781753540039, "val_acc": 82.66666666666667}
{"epoch": 40, "training_loss": 95827.07946777344, "training_acc": 75.33333333333333, "val_loss": 42925.58251953125, "val_acc": 76.0}
{"epoch": 41, "training_loss": 117752.20483398438, "training_acc": 72.33333333333333, "val_loss": 11892.060119628906, "val_acc": 81.33333333333333}
{"epoch": 42, "training_loss": 75391.1396484375, "training_acc": 73.33333333333333, "val_loss": 16662.81069946289, "val_acc": 77.33333333333333}
{"epoch": 43, "training_loss": 74065.083984375, "training_acc": 77.66666666666667, "val_loss": 21357.966915130615, "val_acc": 80.0}
{"epoch": 44, "training_loss": 40591.404235839844, "training_acc": 84.33333333333333, "val_loss": 27623.289123535156, "val_acc": 64.0}
{"epoch": 45, "training_loss": 103067.3344116211, "training_acc": 71.0, "val_loss": 46517.65075683594, "val_acc": 73.33333333333333}
{"epoch": 46, "training_loss": 152062.43005371094, "training_acc": 69.33333333333333, "val_loss": 48704.623596191406, "val_acc": 74.66666666666667}
{"epoch": 47, "training_loss": 116847.1025390625, "training_acc": 77.0, "val_loss": 20359.795013427734, "val_acc": 77.33333333333333}
{"epoch": 48, "training_loss": 100279.36636352539, "training_acc": 75.66666666666667, "val_loss": 20072.834594726562, "val_acc": 68.0}
{"epoch": 49, "training_loss": 67971.98181152344, "training_acc": 75.0, "val_loss": 20823.22166442871, "val_acc": 76.0}
{"epoch": 50, "training_loss": 97225.7451171875, "training_acc": 72.33333333333333, "val_loss": 23568.042358398438, "val_acc": 78.66666666666667}
{"epoch": 51, "training_loss": 62924.42395019531, "training_acc": 78.33333333333333, "val_loss": 12668.933044433594, "val_acc": 81.33333333333333}
{"epoch": 52, "training_loss": 76404.80737304688, "training_acc": 78.33333333333333, "val_loss": 11922.420379638672, "val_acc": 82.66666666666667}
{"epoch": 53, "training_loss": 68643.21459960938, "training_acc": 76.66666666666667, "val_loss": 12200.017578125, "val_acc": 82.66666666666667}
{"epoch": 54, "training_loss": 140887.61224365234, "training_acc": 71.66666666666667, "val_loss": 72563.58374023438, "val_acc": 38.666666666666664}
{"epoch": 55, "training_loss": 185975.07287597656, "training_acc": 72.0, "val_loss": 86575.4439086914, "val_acc": 38.666666666666664}
{"epoch": 56, "training_loss": 173990.10229492188, "training_acc": 69.66666666666667, "val_loss": 23018.763244628906, "val_acc": 70.66666666666667}
{"epoch": 57, "training_loss": 268246.11767578125, "training_acc": 64.66666666666667, "val_loss": 13069.345180511475, "val_acc": 76.0}
{"epoch": 58, "training_loss": 109051.05731201172, "training_acc": 71.66666666666667, "val_loss": 59563.671142578125, "val_acc": 72.0}
{"epoch": 59, "training_loss": 143629.7647705078, "training_acc": 72.0, "val_loss": 53232.49743652344, "val_acc": 76.0}
{"epoch": 60, "training_loss": 160884.01947021484, "training_acc": 73.33333333333333, "val_loss": 68835.48840332031, "val_acc": 74.66666666666667}
