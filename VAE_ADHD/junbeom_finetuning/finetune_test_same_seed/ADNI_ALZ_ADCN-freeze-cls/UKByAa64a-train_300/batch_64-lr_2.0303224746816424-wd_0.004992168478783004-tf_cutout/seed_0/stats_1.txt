"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 300 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 39050.232902526855, "training_acc": 67.66666666666667, "val_loss": 13084.944885253906, "val_acc": 28.0}
{"epoch": 1, "training_loss": 30866.53253173828, "training_acc": 63.666666666666664, "val_loss": 4853.267555236816, "val_acc": 72.0}
{"epoch": 2, "training_loss": 10860.395462036133, "training_acc": 49.666666666666664, "val_loss": 1164.196967124939, "val_acc": 28.0}
{"epoch": 3, "training_loss": 20953.340576171875, "training_acc": 63.666666666666664, "val_loss": 6926.809944152832, "val_acc": 72.0}
{"epoch": 4, "training_loss": 15083.557266235352, "training_acc": 63.666666666666664, "val_loss": 397.3178176879883, "val_acc": 72.0}
{"epoch": 5, "training_loss": 5644.775436401367, "training_acc": 59.0, "val_loss": 914.1725196838379, "val_acc": 72.0}
{"epoch": 6, "training_loss": 6531.066848754883, "training_acc": 55.0, "val_loss": 6264.562469482422, "val_acc": 72.0}
{"epoch": 7, "training_loss": 25462.560638427734, "training_acc": 72.33333333333333, "val_loss": 1075.2484245300293, "val_acc": 72.0}
{"epoch": 8, "training_loss": 18575.49658203125, "training_acc": 45.0, "val_loss": 7498.720153808594, "val_acc": 72.0}
{"epoch": 9, "training_loss": 33064.022705078125, "training_acc": 72.33333333333333, "val_loss": 3406.9778633117676, "val_acc": 72.0}
{"epoch": 10, "training_loss": 6322.171646118164, "training_acc": 58.666666666666664, "val_loss": 1721.9506015777588, "val_acc": 72.0}
{"epoch": 11, "training_loss": 4249.411895751953, "training_acc": 54.0, "val_loss": 2799.8275985717773, "val_acc": 72.0}
{"epoch": 12, "training_loss": 10737.101486206055, "training_acc": 55.666666666666664, "val_loss": 2545.6081981658936, "val_acc": 72.0}
{"epoch": 13, "training_loss": 17494.71923828125, "training_acc": 72.33333333333333, "val_loss": 1457.4690704345703, "val_acc": 72.0}
{"epoch": 14, "training_loss": 14237.936935424805, "training_acc": 49.0, "val_loss": 2558.4513549804688, "val_acc": 72.0}
{"epoch": 15, "training_loss": 4269.517331123352, "training_acc": 63.0, "val_loss": 801.0366249084473, "val_acc": 72.0}
{"epoch": 16, "training_loss": 4412.471466064453, "training_acc": 62.333333333333336, "val_loss": 307.93854808807373, "val_acc": 72.0}
{"epoch": 17, "training_loss": 4190.787090301514, "training_acc": 57.666666666666664, "val_loss": 1415.2096405029297, "val_acc": 72.0}
{"epoch": 18, "training_loss": 6088.518642425537, "training_acc": 60.333333333333336, "val_loss": 725.111560344696, "val_acc": 72.0}
{"epoch": 19, "training_loss": 5706.618110656738, "training_acc": 61.666666666666664, "val_loss": 179.67232656478882, "val_acc": 58.666666666666664}
{"epoch": 20, "training_loss": 7213.5319900512695, "training_acc": 63.0, "val_loss": 1846.0479221343994, "val_acc": 72.0}
{"epoch": 21, "training_loss": 2735.4668045043945, "training_acc": 62.333333333333336, "val_loss": 1180.5755653381348, "val_acc": 72.0}
{"epoch": 22, "training_loss": 6703.89697265625, "training_acc": 69.0, "val_loss": 3592.211311340332, "val_acc": 28.0}
{"epoch": 23, "training_loss": 13005.871551513672, "training_acc": 63.0, "val_loss": 1584.9546298980713, "val_acc": 72.0}
{"epoch": 24, "training_loss": 12081.553924560547, "training_acc": 51.0, "val_loss": 5382.294586181641, "val_acc": 72.0}
{"epoch": 25, "training_loss": 13929.334045410156, "training_acc": 68.33333333333333, "val_loss": 7252.701431274414, "val_acc": 28.0}
{"epoch": 26, "training_loss": 23195.442413330078, "training_acc": 52.666666666666664, "val_loss": 10549.337615966797, "val_acc": 72.0}
{"epoch": 27, "training_loss": 29116.72657775879, "training_acc": 72.33333333333333, "val_loss": 6191.425422668457, "val_acc": 28.0}
{"epoch": 28, "training_loss": 21753.801818847656, "training_acc": 43.666666666666664, "val_loss": 9102.617431640625, "val_acc": 72.0}
{"epoch": 29, "training_loss": 31983.02424621582, "training_acc": 72.33333333333333, "val_loss": 364.99396562576294, "val_acc": 72.0}
{"epoch": 30, "training_loss": 28447.06526184082, "training_acc": 41.0, "val_loss": 5482.472091674805, "val_acc": 72.0}
{"epoch": 31, "training_loss": 27625.1551361084, "training_acc": 72.33333333333333, "val_loss": 2664.665397644043, "val_acc": 72.0}
{"epoch": 32, "training_loss": 8706.278701782227, "training_acc": 54.333333333333336, "val_loss": 3618.523886680603, "val_acc": 72.0}
{"epoch": 33, "training_loss": 11613.24299621582, "training_acc": 57.666666666666664, "val_loss": 1561.7027769088745, "val_acc": 72.0}
{"epoch": 34, "training_loss": 9469.789037704468, "training_acc": 72.33333333333333, "val_loss": 5279.418472290039, "val_acc": 28.0}
{"epoch": 35, "training_loss": 13691.891387939453, "training_acc": 55.0, "val_loss": 3242.508125305176, "val_acc": 72.0}
{"epoch": 36, "training_loss": 9469.52643585205, "training_acc": 55.0, "val_loss": 4535.188438415527, "val_acc": 72.0}
{"epoch": 37, "training_loss": 15025.263137817383, "training_acc": 63.666666666666664, "val_loss": 355.0248417854309, "val_acc": 74.66666666666667}
{"epoch": 38, "training_loss": 2493.5809774398804, "training_acc": 66.66666666666667, "val_loss": 196.14308404922485, "val_acc": 60.0}
