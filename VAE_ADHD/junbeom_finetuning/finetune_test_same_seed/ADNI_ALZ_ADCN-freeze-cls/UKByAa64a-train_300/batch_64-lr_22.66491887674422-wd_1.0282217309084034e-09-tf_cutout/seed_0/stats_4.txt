"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 300 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 123242.93521118164, "training_acc": 62.666666666666664, "val_loss": 16082.239761352539, "val_acc": 36.0}
{"epoch": 1, "training_loss": 86986.72241210938, "training_acc": 64.0, "val_loss": 12610.592346191406, "val_acc": 72.0}
{"epoch": 2, "training_loss": 66077.21542358398, "training_acc": 58.0, "val_loss": 23959.11720275879, "val_acc": 72.0}
{"epoch": 3, "training_loss": 75762.49426269531, "training_acc": 71.66666666666667, "val_loss": 9508.852233886719, "val_acc": 64.0}
{"epoch": 4, "training_loss": 48606.31234741211, "training_acc": 60.0, "val_loss": 9117.050888061523, "val_acc": 72.0}
{"epoch": 5, "training_loss": 28520.66229248047, "training_acc": 59.666666666666664, "val_loss": 4543.870700836182, "val_acc": 72.0}
{"epoch": 6, "training_loss": 18481.553802490234, "training_acc": 66.0, "val_loss": 2751.8228302001953, "val_acc": 70.66666666666667}
{"epoch": 7, "training_loss": 27106.868713378906, "training_acc": 64.33333333333333, "val_loss": 18118.630889892578, "val_acc": 72.0}
{"epoch": 8, "training_loss": 48526.62121582031, "training_acc": 60.0, "val_loss": 7697.54182434082, "val_acc": 70.66666666666667}
{"epoch": 9, "training_loss": 35280.653869628906, "training_acc": 70.66666666666667, "val_loss": 6085.621452331543, "val_acc": 61.333333333333336}
{"epoch": 10, "training_loss": 33368.95959472656, "training_acc": 65.0, "val_loss": 3161.2700233459473, "val_acc": 76.0}
{"epoch": 11, "training_loss": 23107.36376953125, "training_acc": 66.33333333333333, "val_loss": 5558.763694763184, "val_acc": 72.0}
{"epoch": 12, "training_loss": 17570.945388793945, "training_acc": 68.33333333333333, "val_loss": 3581.7142333984375, "val_acc": 72.0}
{"epoch": 13, "training_loss": 13496.962371826172, "training_acc": 71.33333333333333, "val_loss": 6325.324615478516, "val_acc": 72.0}
{"epoch": 14, "training_loss": 13539.30598449707, "training_acc": 67.66666666666667, "val_loss": 2126.279878616333, "val_acc": 80.0}
{"epoch": 15, "training_loss": 10165.401000976562, "training_acc": 69.33333333333333, "val_loss": 1834.4015522003174, "val_acc": 76.0}
{"epoch": 16, "training_loss": 4027.7198944091797, "training_acc": 74.66666666666667, "val_loss": 2789.6943283081055, "val_acc": 56.0}
{"epoch": 17, "training_loss": 5394.413024902344, "training_acc": 75.66666666666667, "val_loss": 1738.4809188168292, "val_acc": 81.33333333333333}
{"epoch": 18, "training_loss": 8532.01821899414, "training_acc": 68.33333333333333, "val_loss": 8602.612564086914, "val_acc": 72.0}
{"epoch": 19, "training_loss": 17609.168487548828, "training_acc": 68.66666666666667, "val_loss": 1569.672773361206, "val_acc": 80.0}
{"epoch": 20, "training_loss": 7180.112205505371, "training_acc": 73.0, "val_loss": 2016.9183568954468, "val_acc": 66.66666666666667}
{"epoch": 21, "training_loss": 8938.473403930664, "training_acc": 71.66666666666667, "val_loss": 3853.3196868896484, "val_acc": 72.0}
{"epoch": 22, "training_loss": 12595.33512878418, "training_acc": 73.0, "val_loss": 6536.436786651611, "val_acc": 72.0}
{"epoch": 23, "training_loss": 11485.24979019165, "training_acc": 73.0, "val_loss": 1399.00803565979, "val_acc": 81.33333333333333}
{"epoch": 24, "training_loss": 3510.9105682373047, "training_acc": 79.0, "val_loss": 1736.7644262313843, "val_acc": 61.333333333333336}
{"epoch": 25, "training_loss": 5165.227577209473, "training_acc": 73.33333333333333, "val_loss": 3181.8526611328125, "val_acc": 72.0}
{"epoch": 26, "training_loss": 11099.193798065186, "training_acc": 72.0, "val_loss": 1491.9859943389893, "val_acc": 82.66666666666667}
{"epoch": 27, "training_loss": 3968.4671716690063, "training_acc": 76.66666666666667, "val_loss": 873.4322475194931, "val_acc": 69.33333333333333}
{"epoch": 28, "training_loss": 2873.864013671875, "training_acc": 79.0, "val_loss": 1306.3079190254211, "val_acc": 65.33333333333333}
{"epoch": 29, "training_loss": 19473.146362304688, "training_acc": 57.666666666666664, "val_loss": 3314.3830337524414, "val_acc": 73.33333333333333}
{"epoch": 30, "training_loss": 8786.403121948242, "training_acc": 71.0, "val_loss": 2879.2525787353516, "val_acc": 73.33333333333333}
{"epoch": 31, "training_loss": 10502.959213256836, "training_acc": 70.33333333333333, "val_loss": 8731.62769317627, "val_acc": 29.333333333333332}
{"epoch": 32, "training_loss": 15764.841217041016, "training_acc": 64.33333333333333, "val_loss": 1177.7123737335205, "val_acc": 80.0}
{"epoch": 33, "training_loss": 6387.861274719238, "training_acc": 79.66666666666667, "val_loss": 2961.952907562256, "val_acc": 76.0}
{"epoch": 34, "training_loss": 7996.671962738037, "training_acc": 74.66666666666667, "val_loss": 5200.494464874268, "val_acc": 44.0}
{"epoch": 35, "training_loss": 25563.60760498047, "training_acc": 57.0, "val_loss": 1732.4581775665283, "val_acc": 80.0}
{"epoch": 36, "training_loss": 9118.771789550781, "training_acc": 73.33333333333333, "val_loss": 5954.606700897217, "val_acc": 72.0}
{"epoch": 37, "training_loss": 16411.269538879395, "training_acc": 70.66666666666667, "val_loss": 3783.7823753356934, "val_acc": 73.33333333333333}
{"epoch": 38, "training_loss": 6907.427743911743, "training_acc": 79.66666666666667, "val_loss": 1368.7100791931152, "val_acc": 80.0}
{"epoch": 39, "training_loss": 9115.29086303711, "training_acc": 70.66666666666667, "val_loss": 4123.218074798584, "val_acc": 72.0}
{"epoch": 40, "training_loss": 14894.988014221191, "training_acc": 71.33333333333333, "val_loss": 5085.909568786621, "val_acc": 70.66666666666667}
{"epoch": 41, "training_loss": 12836.321716308594, "training_acc": 72.33333333333333, "val_loss": 4752.026420593262, "val_acc": 70.66666666666667}
{"epoch": 42, "training_loss": 9718.17741394043, "training_acc": 74.33333333333333, "val_loss": 4118.66491317749, "val_acc": 48.0}
{"epoch": 43, "training_loss": 8537.568435668945, "training_acc": 69.66666666666667, "val_loss": 4023.694175720215, "val_acc": 72.0}
{"epoch": 44, "training_loss": 9596.177352905273, "training_acc": 72.0, "val_loss": 4624.376499176025, "val_acc": 72.0}
{"epoch": 45, "training_loss": 21567.132202148438, "training_acc": 65.66666666666667, "val_loss": 8765.246154785156, "val_acc": 72.0}
{"epoch": 46, "training_loss": 20792.363357543945, "training_acc": 69.33333333333333, "val_loss": 10547.40813446045, "val_acc": 72.0}
