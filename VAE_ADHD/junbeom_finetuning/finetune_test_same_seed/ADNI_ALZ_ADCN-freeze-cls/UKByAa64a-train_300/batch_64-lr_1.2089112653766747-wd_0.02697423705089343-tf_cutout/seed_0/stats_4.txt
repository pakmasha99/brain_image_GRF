"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 300 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 28191.239364624023, "training_acc": 65.0, "val_loss": 10559.957412719727, "val_acc": 28.0}
{"epoch": 1, "training_loss": 18259.094482421875, "training_acc": 61.0, "val_loss": 1513.9027156829834, "val_acc": 72.0}
{"epoch": 2, "training_loss": 9409.959281921387, "training_acc": 50.333333333333336, "val_loss": 3628.7565269470215, "val_acc": 72.0}
{"epoch": 3, "training_loss": 10995.05525970459, "training_acc": 65.66666666666667, "val_loss": 5359.611949920654, "val_acc": 28.0}
{"epoch": 4, "training_loss": 13989.380645751953, "training_acc": 57.0, "val_loss": 7014.481338500977, "val_acc": 72.0}
{"epoch": 5, "training_loss": 20946.668914794922, "training_acc": 72.33333333333333, "val_loss": 1715.880506515503, "val_acc": 28.0}
{"epoch": 6, "training_loss": 6067.435707092285, "training_acc": 49.666666666666664, "val_loss": 184.65743446350098, "val_acc": 72.0}
{"epoch": 7, "training_loss": 8771.145950317383, "training_acc": 49.0, "val_loss": 2963.560329437256, "val_acc": 72.0}
{"epoch": 8, "training_loss": 8264.226303100586, "training_acc": 67.66666666666667, "val_loss": 4608.458766937256, "val_acc": 28.0}
{"epoch": 9, "training_loss": 13906.475402832031, "training_acc": 51.666666666666664, "val_loss": 6121.244766235352, "val_acc": 72.0}
{"epoch": 10, "training_loss": 17184.196350097656, "training_acc": 72.33333333333333, "val_loss": 2901.672508239746, "val_acc": 28.0}
{"epoch": 11, "training_loss": 11543.607452392578, "training_acc": 45.666666666666664, "val_loss": 5309.219951629639, "val_acc": 72.0}
{"epoch": 12, "training_loss": 18130.757583618164, "training_acc": 72.33333333333333, "val_loss": 385.05015087127686, "val_acc": 72.0}
{"epoch": 13, "training_loss": 16358.344789505005, "training_acc": 45.666666666666664, "val_loss": 2633.566864013672, "val_acc": 72.0}
{"epoch": 14, "training_loss": 11782.808853149414, "training_acc": 72.33333333333333, "val_loss": 325.9304189682007, "val_acc": 72.0}
{"epoch": 15, "training_loss": 8613.21061706543, "training_acc": 53.333333333333336, "val_loss": 2921.096035003662, "val_acc": 72.0}
{"epoch": 16, "training_loss": 8972.725246429443, "training_acc": 65.66666666666667, "val_loss": 198.03170132637024, "val_acc": 28.0}
{"epoch": 17, "training_loss": 7917.653244018555, "training_acc": 64.33333333333333, "val_loss": 1601.7157292366028, "val_acc": 72.0}
{"epoch": 18, "training_loss": 4323.060768127441, "training_acc": 55.0, "val_loss": 1134.3489236831665, "val_acc": 72.0}
{"epoch": 19, "training_loss": 6239.156074523926, "training_acc": 53.666666666666664, "val_loss": 2662.7153606414795, "val_acc": 72.0}
{"epoch": 20, "training_loss": 8129.937517166138, "training_acc": 66.33333333333333, "val_loss": 66.59437775611877, "val_acc": 72.0}
{"epoch": 21, "training_loss": 4403.957370758057, "training_acc": 63.0, "val_loss": 1387.7777214050293, "val_acc": 72.0}
{"epoch": 22, "training_loss": 7891.989429473877, "training_acc": 51.666666666666664, "val_loss": 1897.6903228759766, "val_acc": 72.0}
{"epoch": 23, "training_loss": 6959.018730163574, "training_acc": 66.33333333333333, "val_loss": 1767.7933406829834, "val_acc": 28.0}
{"epoch": 24, "training_loss": 8293.089584350586, "training_acc": 63.0, "val_loss": 1022.5676469802856, "val_acc": 72.0}
{"epoch": 25, "training_loss": 6295.224182128906, "training_acc": 52.333333333333336, "val_loss": 2925.1296615600586, "val_acc": 72.0}
{"epoch": 26, "training_loss": 7166.614912033081, "training_acc": 54.333333333333336, "val_loss": 1864.5053825378418, "val_acc": 72.0}
{"epoch": 27, "training_loss": 10843.931381225586, "training_acc": 72.33333333333333, "val_loss": 142.7107014656067, "val_acc": 72.0}
{"epoch": 28, "training_loss": 10063.851509094238, "training_acc": 45.0, "val_loss": 5211.907138824463, "val_acc": 72.0}
{"epoch": 29, "training_loss": 25775.726287841797, "training_acc": 72.33333333333333, "val_loss": 4361.242744445801, "val_acc": 72.0}
{"epoch": 30, "training_loss": 8571.310224533081, "training_acc": 56.333333333333336, "val_loss": 706.8239231109619, "val_acc": 72.0}
{"epoch": 31, "training_loss": 2588.5117263793945, "training_acc": 64.33333333333333, "val_loss": 419.0280771255493, "val_acc": 72.0}
{"epoch": 32, "training_loss": 3469.110855102539, "training_acc": 51.0, "val_loss": 699.0303964614868, "val_acc": 72.0}
{"epoch": 33, "training_loss": 3640.5246505737305, "training_acc": 71.0, "val_loss": 1333.674020767212, "val_acc": 28.0}
{"epoch": 34, "training_loss": 7565.719795227051, "training_acc": 62.333333333333336, "val_loss": 743.3325958251953, "val_acc": 72.0}
{"epoch": 35, "training_loss": 6897.89094543457, "training_acc": 51.666666666666664, "val_loss": 3207.6188468933105, "val_acc": 72.0}
{"epoch": 36, "training_loss": 9307.760299682617, "training_acc": 66.33333333333333, "val_loss": 4403.127216339111, "val_acc": 28.0}
{"epoch": 37, "training_loss": 7218.005790710449, "training_acc": 62.333333333333336, "val_loss": 255.8105866909027, "val_acc": 72.0}
{"epoch": 38, "training_loss": 6863.93278503418, "training_acc": 54.333333333333336, "val_loss": 2629.9399032592773, "val_acc": 72.0}
{"epoch": 39, "training_loss": 6952.045333862305, "training_acc": 67.0, "val_loss": 1884.0916328430176, "val_acc": 28.0}
