"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 300 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 16787.705825805664, "training_acc": 54.333333333333336, "val_loss": 3062.9375076293945, "val_acc": 72.0}
{"epoch": 1, "training_loss": 6939.053520202637, "training_acc": 61.666666666666664, "val_loss": 173.75865077972412, "val_acc": 72.0}
{"epoch": 2, "training_loss": 9328.94091796875, "training_acc": 55.0, "val_loss": 4244.33918762207, "val_acc": 72.0}
{"epoch": 3, "training_loss": 11519.96109008789, "training_acc": 67.66666666666667, "val_loss": 1913.8863906860352, "val_acc": 28.0}
{"epoch": 4, "training_loss": 7190.204620361328, "training_acc": 63.0, "val_loss": 236.28052687644958, "val_acc": 72.0}
{"epoch": 5, "training_loss": 8560.236385345459, "training_acc": 55.0, "val_loss": 1317.698865890503, "val_acc": 72.0}
{"epoch": 6, "training_loss": 3653.9181385040283, "training_acc": 61.0, "val_loss": 519.8713359832764, "val_acc": 28.0}
{"epoch": 7, "training_loss": 4191.970699310303, "training_acc": 61.0, "val_loss": 3459.282932281494, "val_acc": 28.0}
{"epoch": 8, "training_loss": 10066.422943115234, "training_acc": 52.333333333333336, "val_loss": 2271.7009048461914, "val_acc": 72.0}
{"epoch": 9, "training_loss": 3370.9616622924805, "training_acc": 57.0, "val_loss": 1252.0033702850342, "val_acc": 72.0}
{"epoch": 10, "training_loss": 4861.889591217041, "training_acc": 58.333333333333336, "val_loss": 1116.2948360443115, "val_acc": 72.0}
{"epoch": 11, "training_loss": 3363.1451420783997, "training_acc": 61.666666666666664, "val_loss": 384.848867893219, "val_acc": 28.0}
{"epoch": 12, "training_loss": 3768.5852756500244, "training_acc": 64.33333333333333, "val_loss": 2153.7238121032715, "val_acc": 28.0}
{"epoch": 13, "training_loss": 8937.620697021484, "training_acc": 51.666666666666664, "val_loss": 3225.163646697998, "val_acc": 72.0}
{"epoch": 14, "training_loss": 8401.28547668457, "training_acc": 58.333333333333336, "val_loss": 890.6682925224304, "val_acc": 72.0}
{"epoch": 15, "training_loss": 5345.525905609131, "training_acc": 72.33333333333333, "val_loss": 3286.324977874756, "val_acc": 28.0}
{"epoch": 16, "training_loss": 9900.079696655273, "training_acc": 50.333333333333336, "val_loss": 1840.3062591552734, "val_acc": 72.0}
{"epoch": 17, "training_loss": 6883.76545715332, "training_acc": 53.0, "val_loss": 2322.0630283355713, "val_acc": 72.0}
{"epoch": 18, "training_loss": 8100.060935974121, "training_acc": 64.33333333333333, "val_loss": 411.57566356658936, "val_acc": 28.0}
{"epoch": 19, "training_loss": 9674.4990234375, "training_acc": 60.333333333333336, "val_loss": 2082.465940475464, "val_acc": 72.0}
{"epoch": 20, "training_loss": 9377.717255592346, "training_acc": 51.666666666666664, "val_loss": 956.2070474624634, "val_acc": 72.0}
{"epoch": 21, "training_loss": 3076.4779024124146, "training_acc": 65.66666666666667, "val_loss": 116.5790559053421, "val_acc": 72.0}
{"epoch": 22, "training_loss": 2505.1945390701294, "training_acc": 64.33333333333333, "val_loss": 1121.0494542121887, "val_acc": 28.0}
{"epoch": 23, "training_loss": 3483.308868408203, "training_acc": 55.666666666666664, "val_loss": 254.69921398162842, "val_acc": 72.0}
{"epoch": 24, "training_loss": 2427.4728260040283, "training_acc": 62.333333333333336, "val_loss": 436.7008056640625, "val_acc": 72.0}
{"epoch": 25, "training_loss": 2643.86141204834, "training_acc": 53.666666666666664, "val_loss": 1108.3403091430664, "val_acc": 72.0}
{"epoch": 26, "training_loss": 5793.8570556640625, "training_acc": 63.666666666666664, "val_loss": 1191.5791130065918, "val_acc": 28.0}
{"epoch": 27, "training_loss": 11590.285919189453, "training_acc": 61.0, "val_loss": 2631.2095527648926, "val_acc": 72.0}
{"epoch": 28, "training_loss": 4430.852355957031, "training_acc": 55.666666666666664, "val_loss": 2242.428789138794, "val_acc": 72.0}
{"epoch": 29, "training_loss": 6071.600318908691, "training_acc": 67.0, "val_loss": 1278.7961254119873, "val_acc": 28.0}
{"epoch": 30, "training_loss": 9412.751770019531, "training_acc": 64.33333333333333, "val_loss": 2225.7117805480957, "val_acc": 72.0}
{"epoch": 31, "training_loss": 5262.351699829102, "training_acc": 56.333333333333336, "val_loss": 2346.699806213379, "val_acc": 72.0}
{"epoch": 32, "training_loss": 7462.544418334961, "training_acc": 65.66666666666667, "val_loss": 2717.9417037963867, "val_acc": 28.0}
{"epoch": 33, "training_loss": 8039.871154785156, "training_acc": 60.333333333333336, "val_loss": 610.035514831543, "val_acc": 72.0}
{"epoch": 34, "training_loss": 5497.536552429199, "training_acc": 56.333333333333336, "val_loss": 3099.3745651245117, "val_acc": 72.0}
{"epoch": 35, "training_loss": 9924.703079223633, "training_acc": 63.666666666666664, "val_loss": 5175.056983947754, "val_acc": 28.0}
{"epoch": 36, "training_loss": 17429.081665039062, "training_acc": 51.0, "val_loss": 6958.386232376099, "val_acc": 72.0}
{"epoch": 37, "training_loss": 19981.149963378906, "training_acc": 72.33333333333333, "val_loss": 2267.0804748535156, "val_acc": 28.0}
{"epoch": 38, "training_loss": 6768.764652252197, "training_acc": 49.0, "val_loss": 291.79157638549805, "val_acc": 28.0}
{"epoch": 39, "training_loss": 3362.063674926758, "training_acc": 55.0, "val_loss": 479.6875100135803, "val_acc": 28.0}
{"epoch": 40, "training_loss": 10228.440811157227, "training_acc": 65.0, "val_loss": 3137.208869934082, "val_acc": 72.0}
