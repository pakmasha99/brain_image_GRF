"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 300 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 50146104.60378647, "training_acc": 58.0, "val_loss": 2279848.93359375, "val_acc": 54.666666666666664}
{"epoch": 1, "training_loss": 28975614.34375, "training_acc": 66.33333333333333, "val_loss": 1955060.314453125, "val_acc": 68.0}
{"epoch": 2, "training_loss": 22652679.75, "training_acc": 57.666666666666664, "val_loss": 9363821.6796875, "val_acc": 72.0}
{"epoch": 3, "training_loss": 27952833.21875, "training_acc": 70.0, "val_loss": 5017089.6953125, "val_acc": 48.0}
{"epoch": 4, "training_loss": 15581853.046875, "training_acc": 63.666666666666664, "val_loss": 1621288.9140625, "val_acc": 74.66666666666667}
{"epoch": 5, "training_loss": 11969871.953125, "training_acc": 63.0, "val_loss": 6438809.37890625, "val_acc": 72.0}
{"epoch": 6, "training_loss": 20178806.53125, "training_acc": 63.333333333333336, "val_loss": 2043426.46875, "val_acc": 60.0}
{"epoch": 7, "training_loss": 12700800.3671875, "training_acc": 68.66666666666667, "val_loss": 1880515.19921875, "val_acc": 60.0}
{"epoch": 8, "training_loss": 5906777.103515625, "training_acc": 69.33333333333333, "val_loss": 2712988.640625, "val_acc": 36.0}
{"epoch": 9, "training_loss": 7348410.46875, "training_acc": 57.333333333333336, "val_loss": 425867.58740234375, "val_acc": 78.66666666666667}
{"epoch": 10, "training_loss": 5083767.875, "training_acc": 68.66666666666667, "val_loss": 557377.3134765625, "val_acc": 82.66666666666667}
{"epoch": 11, "training_loss": 3153204.701171875, "training_acc": 71.33333333333333, "val_loss": 578693.2749023438, "val_acc": 76.0}
{"epoch": 12, "training_loss": 2423287.033203125, "training_acc": 69.0, "val_loss": 360003.7255859375, "val_acc": 76.0}
{"epoch": 13, "training_loss": 2761785.0, "training_acc": 69.66666666666667, "val_loss": 510471.76599121094, "val_acc": 82.66666666666667}
{"epoch": 14, "training_loss": 2386151.72265625, "training_acc": 77.66666666666667, "val_loss": 545849.7119140625, "val_acc": 76.0}
{"epoch": 15, "training_loss": 4824127.03125, "training_acc": 63.333333333333336, "val_loss": 1870053.37109375, "val_acc": 72.0}
{"epoch": 16, "training_loss": 7418662.423828125, "training_acc": 66.66666666666667, "val_loss": 543274.392578125, "val_acc": 77.33333333333333}
{"epoch": 17, "training_loss": 2581000.671875, "training_acc": 73.33333333333333, "val_loss": 507505.171875, "val_acc": 78.66666666666667}
{"epoch": 18, "training_loss": 1804386.078125, "training_acc": 71.33333333333333, "val_loss": 1399832.634765625, "val_acc": 72.0}
{"epoch": 19, "training_loss": 4353442.47265625, "training_acc": 67.0, "val_loss": 849832.6030273438, "val_acc": 66.66666666666667}
{"epoch": 20, "training_loss": 3222474.484375, "training_acc": 71.33333333333333, "val_loss": 706829.4296875, "val_acc": 74.66666666666667}
{"epoch": 21, "training_loss": 5647405.1640625, "training_acc": 69.33333333333333, "val_loss": 1614145.0, "val_acc": 72.0}
{"epoch": 22, "training_loss": 8110511.9375, "training_acc": 65.0, "val_loss": 2393861.22265625, "val_acc": 72.0}
{"epoch": 23, "training_loss": 5659984.984375, "training_acc": 68.0, "val_loss": 965837.892578125, "val_acc": 65.33333333333333}
{"epoch": 24, "training_loss": 3534556.15625, "training_acc": 69.66666666666667, "val_loss": 783395.58984375, "val_acc": 68.0}
{"epoch": 25, "training_loss": 8239463.953125, "training_acc": 67.0, "val_loss": 1067874.44921875, "val_acc": 54.666666666666664}
{"epoch": 26, "training_loss": 7070578.28125, "training_acc": 68.0, "val_loss": 1106850.3310546875, "val_acc": 66.66666666666667}
{"epoch": 27, "training_loss": 7884197.640625, "training_acc": 65.66666666666667, "val_loss": 1172456.66015625, "val_acc": 61.333333333333336}
{"epoch": 28, "training_loss": 7461068.658691406, "training_acc": 73.33333333333333, "val_loss": 4411086.71484375, "val_acc": 32.0}
{"epoch": 29, "training_loss": 7547118.88671875, "training_acc": 70.66666666666667, "val_loss": 1506897.2802734375, "val_acc": 56.0}
{"epoch": 30, "training_loss": 2781267.7685546875, "training_acc": 73.33333333333333, "val_loss": 1264319.51171875, "val_acc": 54.666666666666664}
{"epoch": 31, "training_loss": 4143500.626953125, "training_acc": 67.0, "val_loss": 654681.3059082031, "val_acc": 77.33333333333333}
