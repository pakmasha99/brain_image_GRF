"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 300 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 226.94880867004395, "training_acc": 69.0, "val_loss": 53.34434509277344, "val_acc": 29.333333333333332}
{"epoch": 1, "training_loss": 197.15951657295227, "training_acc": 65.66666666666667, "val_loss": 51.12138777971268, "val_acc": 72.0}
{"epoch": 2, "training_loss": 182.08226919174194, "training_acc": 71.0, "val_loss": 39.680803030729294, "val_acc": 74.66666666666667}
{"epoch": 3, "training_loss": 172.6273467540741, "training_acc": 72.0, "val_loss": 42.599378764629364, "val_acc": 73.33333333333333}
{"epoch": 4, "training_loss": 164.89743101596832, "training_acc": 74.0, "val_loss": 38.91437727212906, "val_acc": 74.66666666666667}
{"epoch": 5, "training_loss": 160.89060592651367, "training_acc": 72.66666666666667, "val_loss": 38.9031725525856, "val_acc": 74.66666666666667}
{"epoch": 6, "training_loss": 165.98777437210083, "training_acc": 73.0, "val_loss": 40.029606997966766, "val_acc": 70.66666666666667}
{"epoch": 7, "training_loss": 162.94490110874176, "training_acc": 73.66666666666667, "val_loss": 38.073809653520584, "val_acc": 77.33333333333333}
{"epoch": 8, "training_loss": 162.63465332984924, "training_acc": 73.33333333333333, "val_loss": 40.36451143026352, "val_acc": 70.66666666666667}
{"epoch": 9, "training_loss": 158.38450241088867, "training_acc": 75.33333333333333, "val_loss": 37.77523285150528, "val_acc": 77.33333333333333}
{"epoch": 10, "training_loss": 163.27345967292786, "training_acc": 73.33333333333333, "val_loss": 37.46473950147629, "val_acc": 78.66666666666667}
{"epoch": 11, "training_loss": 161.9182631969452, "training_acc": 73.33333333333333, "val_loss": 42.86975559592247, "val_acc": 72.0}
{"epoch": 12, "training_loss": 160.8661766052246, "training_acc": 74.33333333333333, "val_loss": 38.55278807878494, "val_acc": 76.0}
{"epoch": 13, "training_loss": 172.3579981327057, "training_acc": 68.66666666666667, "val_loss": 42.231403946876526, "val_acc": 72.0}
{"epoch": 14, "training_loss": 162.5010702610016, "training_acc": 74.33333333333333, "val_loss": 37.688300013542175, "val_acc": 78.66666666666667}
{"epoch": 15, "training_loss": 158.29884338378906, "training_acc": 72.0, "val_loss": 38.11440563201904, "val_acc": 77.33333333333333}
{"epoch": 16, "training_loss": 159.53723168373108, "training_acc": 73.66666666666667, "val_loss": 40.99961233139038, "val_acc": 72.0}
{"epoch": 17, "training_loss": 160.1349549293518, "training_acc": 75.66666666666667, "val_loss": 37.57812052965164, "val_acc": 76.0}
{"epoch": 18, "training_loss": 155.57460117340088, "training_acc": 74.66666666666667, "val_loss": 41.33164045214653, "val_acc": 69.33333333333333}
{"epoch": 19, "training_loss": 150.15149116516113, "training_acc": 76.66666666666667, "val_loss": 37.485596776008606, "val_acc": 76.0}
{"epoch": 20, "training_loss": 150.3990616798401, "training_acc": 76.66666666666667, "val_loss": 40.02766475081444, "val_acc": 72.0}
{"epoch": 21, "training_loss": 157.86172723770142, "training_acc": 75.66666666666667, "val_loss": 37.74054527282715, "val_acc": 78.66666666666667}
{"epoch": 22, "training_loss": 148.58022165298462, "training_acc": 76.66666666666667, "val_loss": 37.86051523685455, "val_acc": 80.0}
{"epoch": 23, "training_loss": 148.1487741470337, "training_acc": 78.0, "val_loss": 37.9660667181015, "val_acc": 78.66666666666667}
{"epoch": 24, "training_loss": 149.1669797897339, "training_acc": 74.66666666666667, "val_loss": 37.495527505874634, "val_acc": 80.0}
{"epoch": 25, "training_loss": 151.0651547908783, "training_acc": 75.33333333333333, "val_loss": 38.775132179260254, "val_acc": 74.66666666666667}
{"epoch": 26, "training_loss": 148.05584180355072, "training_acc": 75.0, "val_loss": 38.70674026012421, "val_acc": 74.66666666666667}
{"epoch": 27, "training_loss": 148.87521278858185, "training_acc": 75.66666666666667, "val_loss": 37.49617248773575, "val_acc": 80.0}
{"epoch": 28, "training_loss": 145.32958936691284, "training_acc": 76.66666666666667, "val_loss": 38.83282285928726, "val_acc": 74.66666666666667}
{"epoch": 29, "training_loss": 147.4932998418808, "training_acc": 77.66666666666667, "val_loss": 39.356657564640045, "val_acc": 72.0}
