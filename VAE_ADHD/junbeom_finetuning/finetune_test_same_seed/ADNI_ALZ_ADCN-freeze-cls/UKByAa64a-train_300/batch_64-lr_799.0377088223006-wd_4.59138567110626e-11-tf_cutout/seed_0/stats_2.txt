"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 300 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 5276499.212902069, "training_acc": 57.0, "val_loss": 670849.427734375, "val_acc": 41.333333333333336}
{"epoch": 1, "training_loss": 3299069.98828125, "training_acc": 65.33333333333333, "val_loss": 650604.9467773438, "val_acc": 73.33333333333333}
{"epoch": 2, "training_loss": 2591506.822265625, "training_acc": 57.333333333333336, "val_loss": 589138.419921875, "val_acc": 72.0}
{"epoch": 3, "training_loss": 1969537.34375, "training_acc": 72.33333333333333, "val_loss": 417144.74627685547, "val_acc": 61.333333333333336}
{"epoch": 4, "training_loss": 1429942.56640625, "training_acc": 64.0, "val_loss": 563661.1372070312, "val_acc": 72.0}
{"epoch": 5, "training_loss": 1381057.4643554688, "training_acc": 62.0, "val_loss": 340319.5217285156, "val_acc": 72.0}
{"epoch": 6, "training_loss": 956229.2412109375, "training_acc": 69.0, "val_loss": 155412.70532226562, "val_acc": 69.33333333333333}
{"epoch": 7, "training_loss": 585200.5576171875, "training_acc": 71.66666666666667, "val_loss": 111501.02172851562, "val_acc": 70.66666666666667}
{"epoch": 8, "training_loss": 275152.1005859375, "training_acc": 75.66666666666667, "val_loss": 45687.781311035156, "val_acc": 72.0}
{"epoch": 9, "training_loss": 783420.74609375, "training_acc": 67.33333333333333, "val_loss": 258364.859375, "val_acc": 32.0}
{"epoch": 10, "training_loss": 306333.6677246094, "training_acc": 68.0, "val_loss": 96181.78018188477, "val_acc": 73.33333333333333}
{"epoch": 11, "training_loss": 340466.42236328125, "training_acc": 70.66666666666667, "val_loss": 74505.23876953125, "val_acc": 72.0}
{"epoch": 12, "training_loss": 166333.1895751953, "training_acc": 76.66666666666667, "val_loss": 50620.27917480469, "val_acc": 70.66666666666667}
{"epoch": 13, "training_loss": 595147.521484375, "training_acc": 63.666666666666664, "val_loss": 229040.72265625, "val_acc": 41.333333333333336}
{"epoch": 14, "training_loss": 573727.2158203125, "training_acc": 68.0, "val_loss": 143724.50048828125, "val_acc": 56.0}
{"epoch": 15, "training_loss": 407691.13623046875, "training_acc": 73.0, "val_loss": 110756.20739746094, "val_acc": 64.0}
{"epoch": 16, "training_loss": 325195.76611328125, "training_acc": 71.33333333333333, "val_loss": 45295.51593017578, "val_acc": 70.66666666666667}
{"epoch": 17, "training_loss": 455818.626953125, "training_acc": 67.66666666666667, "val_loss": 327596.31396484375, "val_acc": 40.0}
{"epoch": 18, "training_loss": 765007.859375, "training_acc": 66.0, "val_loss": 168953.44958496094, "val_acc": 56.0}
{"epoch": 19, "training_loss": 318434.05615234375, "training_acc": 75.33333333333333, "val_loss": 43556.741271972656, "val_acc": 73.33333333333333}
{"epoch": 20, "training_loss": 179574.44787597656, "training_acc": 73.33333333333333, "val_loss": 53819.76403808594, "val_acc": 60.0}
{"epoch": 21, "training_loss": 253572.52185058594, "training_acc": 77.0, "val_loss": 185014.77685546875, "val_acc": 72.0}
{"epoch": 22, "training_loss": 415865.56494140625, "training_acc": 69.33333333333333, "val_loss": 72723.1537475586, "val_acc": 74.66666666666667}
{"epoch": 23, "training_loss": 154571.45373535156, "training_acc": 83.0, "val_loss": 292487.18896484375, "val_acc": 30.666666666666668}
{"epoch": 24, "training_loss": 1052199.8056640625, "training_acc": 58.666666666666664, "val_loss": 62500.021728515625, "val_acc": 68.0}
{"epoch": 25, "training_loss": 568906.6376953125, "training_acc": 72.0, "val_loss": 212407.673828125, "val_acc": 57.333333333333336}
{"epoch": 26, "training_loss": 737552.6875, "training_acc": 71.66666666666667, "val_loss": 345181.35986328125, "val_acc": 52.0}
{"epoch": 27, "training_loss": 812830.646484375, "training_acc": 68.66666666666667, "val_loss": 377851.9765625, "val_acc": 41.333333333333336}
{"epoch": 28, "training_loss": 900805.73046875, "training_acc": 68.66666666666667, "val_loss": 214004.28857421875, "val_acc": 72.0}
{"epoch": 29, "training_loss": 667251.9228515625, "training_acc": 66.0, "val_loss": 144934.01599121094, "val_acc": 68.0}
{"epoch": 30, "training_loss": 448924.1884765625, "training_acc": 72.33333333333333, "val_loss": 165035.70349121094, "val_acc": 72.0}
{"epoch": 31, "training_loss": 449850.712890625, "training_acc": 74.33333333333333, "val_loss": 213165.79150390625, "val_acc": 73.33333333333333}
{"epoch": 32, "training_loss": 530068.306640625, "training_acc": 67.33333333333333, "val_loss": 180616.13037109375, "val_acc": 74.66666666666667}
{"epoch": 33, "training_loss": 342568.375, "training_acc": 75.66666666666667, "val_loss": 115556.26013183594, "val_acc": 72.0}
{"epoch": 34, "training_loss": 295649.33154296875, "training_acc": 76.66666666666667, "val_loss": 70893.1904296875, "val_acc": 60.0}
{"epoch": 35, "training_loss": 414180.64318847656, "training_acc": 72.0, "val_loss": 466303.3173828125, "val_acc": 32.0}
{"epoch": 36, "training_loss": 617110.9958496094, "training_acc": 70.66666666666667, "val_loss": 150827.34838867188, "val_acc": 61.333333333333336}
{"epoch": 37, "training_loss": 263870.79357910156, "training_acc": 78.66666666666667, "val_loss": 128023.7292175293, "val_acc": 56.0}
{"epoch": 38, "training_loss": 267854.5771484375, "training_acc": 71.66666666666667, "val_loss": 156592.7666015625, "val_acc": 73.33333333333333}
