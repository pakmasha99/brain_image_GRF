"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 3985.9556427001953, "training_acc": 57.0, "val_loss": 1877.424430847168, "val_acc": 72.0}
{"epoch": 1, "training_loss": 6060.375091552734, "training_acc": 72.0, "val_loss": 2184.1936111450195, "val_acc": 28.0}
{"epoch": 2, "training_loss": 8836.997131347656, "training_acc": 28.0, "val_loss": 229.3997049331665, "val_acc": 80.0}
{"epoch": 3, "training_loss": 1784.7136993408203, "training_acc": 71.0, "val_loss": 1510.9402656555176, "val_acc": 72.0}
{"epoch": 4, "training_loss": 6067.594970703125, "training_acc": 72.0, "val_loss": 1538.076400756836, "val_acc": 72.0}
{"epoch": 5, "training_loss": 5103.426498413086, "training_acc": 72.0, "val_loss": 538.5563850402832, "val_acc": 76.0}
{"epoch": 6, "training_loss": 1587.1160202026367, "training_acc": 69.0, "val_loss": 869.3359375, "val_acc": 44.0}
{"epoch": 7, "training_loss": 4080.4728393554688, "training_acc": 46.0, "val_loss": 367.47307777404785, "val_acc": 76.0}
{"epoch": 8, "training_loss": 1684.649185180664, "training_acc": 63.0, "val_loss": 1005.60302734375, "val_acc": 72.0}
{"epoch": 9, "training_loss": 3657.056671142578, "training_acc": 72.0, "val_loss": 964.6357536315918, "val_acc": 72.0}
{"epoch": 10, "training_loss": 2540.0336875915527, "training_acc": 73.0, "val_loss": 349.69482421875, "val_acc": 80.0}
{"epoch": 11, "training_loss": 1811.068588256836, "training_acc": 62.0, "val_loss": 521.7010498046875, "val_acc": 52.0}
{"epoch": 12, "training_loss": 2523.218158721924, "training_acc": 57.0, "val_loss": 504.7424793243408, "val_acc": 72.0}
{"epoch": 13, "training_loss": 1562.936782836914, "training_acc": 73.0, "val_loss": 671.548318862915, "val_acc": 72.0}
{"epoch": 14, "training_loss": 1659.793701171875, "training_acc": 71.0, "val_loss": 286.6727590560913, "val_acc": 76.0}
{"epoch": 15, "training_loss": 1389.7701263427734, "training_acc": 61.0, "val_loss": 361.8032693862915, "val_acc": 52.0}
{"epoch": 16, "training_loss": 1293.930751800537, "training_acc": 56.0, "val_loss": 503.03759574890137, "val_acc": 72.0}
{"epoch": 17, "training_loss": 1652.8686981201172, "training_acc": 72.0, "val_loss": 459.12179946899414, "val_acc": 72.0}
{"epoch": 18, "training_loss": 920.7114305496216, "training_acc": 72.0, "val_loss": 330.9486389160156, "val_acc": 52.0}
{"epoch": 19, "training_loss": 961.8779029846191, "training_acc": 56.0, "val_loss": 326.1287212371826, "val_acc": 72.0}
{"epoch": 20, "training_loss": 981.8100166320801, "training_acc": 72.0, "val_loss": 221.77023887634277, "val_acc": 64.0}
{"epoch": 21, "training_loss": 420.4986686706543, "training_acc": 77.0, "val_loss": 233.12630653381348, "val_acc": 48.0}
{"epoch": 22, "training_loss": 716.1559219360352, "training_acc": 57.0, "val_loss": 319.0591335296631, "val_acc": 72.0}
{"epoch": 23, "training_loss": 620.5555391311646, "training_acc": 74.0, "val_loss": 166.57357215881348, "val_acc": 76.0}
{"epoch": 24, "training_loss": 499.3151922225952, "training_acc": 73.0, "val_loss": 159.07306671142578, "val_acc": 72.0}
{"epoch": 25, "training_loss": 337.56210231781006, "training_acc": 78.0, "val_loss": 317.8906202316284, "val_acc": 72.0}
{"epoch": 26, "training_loss": 640.8830432891846, "training_acc": 73.0, "val_loss": 144.2265272140503, "val_acc": 64.0}
{"epoch": 27, "training_loss": 457.78604316711426, "training_acc": 62.0, "val_loss": 216.34185314178467, "val_acc": 72.0}
{"epoch": 28, "training_loss": 454.16610050201416, "training_acc": 74.0, "val_loss": 135.96138954162598, "val_acc": 48.0}
{"epoch": 29, "training_loss": 316.3074150085449, "training_acc": 67.0, "val_loss": 181.59730434417725, "val_acc": 72.0}
{"epoch": 30, "training_loss": 255.40347933769226, "training_acc": 76.0, "val_loss": 110.30995845794678, "val_acc": 56.0}
{"epoch": 31, "training_loss": 241.42110300064087, "training_acc": 71.0, "val_loss": 91.75296425819397, "val_acc": 72.0}
{"epoch": 32, "training_loss": 107.19533634185791, "training_acc": 86.0, "val_loss": 67.80595183372498, "val_acc": 64.0}
{"epoch": 33, "training_loss": 513.4272212982178, "training_acc": 58.0, "val_loss": 202.2144317626953, "val_acc": 72.0}
{"epoch": 34, "training_loss": 364.00581455230713, "training_acc": 72.0, "val_loss": 86.92271709442139, "val_acc": 64.0}
{"epoch": 35, "training_loss": 525.2962779998779, "training_acc": 63.0, "val_loss": 365.1400566101074, "val_acc": 72.0}
{"epoch": 36, "training_loss": 1038.5965576171875, "training_acc": 72.0, "val_loss": 79.15762662887573, "val_acc": 80.0}
{"epoch": 37, "training_loss": 586.5694541931152, "training_acc": 64.0, "val_loss": 89.17919993400574, "val_acc": 72.0}
{"epoch": 38, "training_loss": 412.0045871734619, "training_acc": 71.0, "val_loss": 474.8292922973633, "val_acc": 72.0}
{"epoch": 39, "training_loss": 1312.6805400848389, "training_acc": 73.0, "val_loss": 148.94604682922363, "val_acc": 68.0}
{"epoch": 40, "training_loss": 507.16907119750977, "training_acc": 76.0, "val_loss": 135.73867082595825, "val_acc": 68.0}
{"epoch": 41, "training_loss": 721.9250373840332, "training_acc": 65.0, "val_loss": 415.30137062072754, "val_acc": 72.0}
{"epoch": 42, "training_loss": 1089.5775756835938, "training_acc": 74.0, "val_loss": 150.71481466293335, "val_acc": 68.0}
{"epoch": 43, "training_loss": 888.0399169921875, "training_acc": 61.0, "val_loss": 104.58558797836304, "val_acc": 72.0}
{"epoch": 44, "training_loss": 549.2545547485352, "training_acc": 75.0, "val_loss": 413.2194519042969, "val_acc": 72.0}
{"epoch": 45, "training_loss": 954.8790111541748, "training_acc": 72.0, "val_loss": 213.56103420257568, "val_acc": 60.0}
{"epoch": 46, "training_loss": 597.8547720909119, "training_acc": 58.0, "val_loss": 169.2366600036621, "val_acc": 72.0}
{"epoch": 47, "training_loss": 349.0823612213135, "training_acc": 77.0, "val_loss": 69.78853344917297, "val_acc": 64.0}
{"epoch": 48, "training_loss": 297.6457347869873, "training_acc": 74.0, "val_loss": 137.60730028152466, "val_acc": 72.0}
{"epoch": 49, "training_loss": 315.3716697692871, "training_acc": 79.0, "val_loss": 125.41117668151855, "val_acc": 76.0}
{"epoch": 50, "training_loss": 148.89108562469482, "training_acc": 81.0, "val_loss": 71.62623405456543, "val_acc": 64.0}
{"epoch": 51, "training_loss": 130.19655561447144, "training_acc": 83.0, "val_loss": 184.12271738052368, "val_acc": 72.0}
