"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 4549.385681152344, "training_acc": 49.0, "val_loss": 2036.9081497192383, "val_acc": 72.0}
{"epoch": 1, "training_loss": 6059.704231262207, "training_acc": 72.0, "val_loss": 1925.0389099121094, "val_acc": 28.0}
{"epoch": 2, "training_loss": 6960.835876464844, "training_acc": 28.0, "val_loss": 192.25376844406128, "val_acc": 80.0}
{"epoch": 3, "training_loss": 1975.7922058105469, "training_acc": 72.0, "val_loss": 1170.526123046875, "val_acc": 72.0}
{"epoch": 4, "training_loss": 4287.927291870117, "training_acc": 72.0, "val_loss": 741.6245937347412, "val_acc": 72.0}
{"epoch": 5, "training_loss": 2169.0511360168457, "training_acc": 75.0, "val_loss": 689.5055294036865, "val_acc": 60.0}
{"epoch": 6, "training_loss": 3175.8058853149414, "training_acc": 48.0, "val_loss": 632.6691627502441, "val_acc": 60.0}
{"epoch": 7, "training_loss": 1940.1452407836914, "training_acc": 62.0, "val_loss": 470.4176902770996, "val_acc": 76.0}
{"epoch": 8, "training_loss": 2023.1052131652832, "training_acc": 73.0, "val_loss": 555.8924198150635, "val_acc": 72.0}
{"epoch": 9, "training_loss": 1643.6408786773682, "training_acc": 71.0, "val_loss": 272.80938625335693, "val_acc": 52.0}
{"epoch": 10, "training_loss": 1139.6981773376465, "training_acc": 62.0, "val_loss": 319.99096870422363, "val_acc": 56.0}
{"epoch": 11, "training_loss": 1217.0253143310547, "training_acc": 63.0, "val_loss": 378.00633907318115, "val_acc": 72.0}
{"epoch": 12, "training_loss": 1382.2662734985352, "training_acc": 72.0, "val_loss": 218.21227073669434, "val_acc": 72.0}
{"epoch": 13, "training_loss": 933.3765182495117, "training_acc": 62.0, "val_loss": 299.6849060058594, "val_acc": 52.0}
{"epoch": 14, "training_loss": 902.9200382232666, "training_acc": 57.0, "val_loss": 422.7320194244385, "val_acc": 72.0}
{"epoch": 15, "training_loss": 1288.695164680481, "training_acc": 72.0, "val_loss": 99.20344948768616, "val_acc": 68.0}
{"epoch": 16, "training_loss": 777.4862823486328, "training_acc": 59.0, "val_loss": 130.87459802627563, "val_acc": 52.0}
{"epoch": 17, "training_loss": 568.4923286437988, "training_acc": 69.0, "val_loss": 611.2193584442139, "val_acc": 72.0}
{"epoch": 18, "training_loss": 2024.3156509399414, "training_acc": 72.0, "val_loss": 314.5087003707886, "val_acc": 72.0}
{"epoch": 19, "training_loss": 1004.3471832275391, "training_acc": 60.0, "val_loss": 423.91228675842285, "val_acc": 44.0}
{"epoch": 20, "training_loss": 1229.3522691726685, "training_acc": 54.0, "val_loss": 339.9945020675659, "val_acc": 72.0}
{"epoch": 21, "training_loss": 1049.8981246948242, "training_acc": 72.0, "val_loss": 179.77981567382812, "val_acc": 72.0}
{"epoch": 22, "training_loss": 765.9695777893066, "training_acc": 70.0, "val_loss": 259.6010684967041, "val_acc": 56.0}
{"epoch": 23, "training_loss": 861.7943773269653, "training_acc": 59.0, "val_loss": 254.11465167999268, "val_acc": 72.0}
{"epoch": 24, "training_loss": 740.5230445861816, "training_acc": 75.0, "val_loss": 135.65210103988647, "val_acc": 72.0}
{"epoch": 25, "training_loss": 671.5538597106934, "training_acc": 68.0, "val_loss": 119.97272968292236, "val_acc": 64.0}
{"epoch": 26, "training_loss": 227.48584461212158, "training_acc": 83.0, "val_loss": 180.10540008544922, "val_acc": 72.0}
{"epoch": 27, "training_loss": 315.91569447517395, "training_acc": 75.0, "val_loss": 288.1049633026123, "val_acc": 40.0}
{"epoch": 28, "training_loss": 516.2415256500244, "training_acc": 60.0, "val_loss": 244.7909116744995, "val_acc": 72.0}
{"epoch": 29, "training_loss": 452.4839515686035, "training_acc": 74.0, "val_loss": 410.32538414001465, "val_acc": 40.0}
{"epoch": 30, "training_loss": 934.8587665557861, "training_acc": 53.0, "val_loss": 272.629451751709, "val_acc": 72.0}
{"epoch": 31, "training_loss": 773.3998413085938, "training_acc": 72.0, "val_loss": 93.45755577087402, "val_acc": 72.0}
{"epoch": 32, "training_loss": 666.294620513916, "training_acc": 72.0, "val_loss": 120.67445516586304, "val_acc": 60.0}
{"epoch": 33, "training_loss": 272.843225479126, "training_acc": 80.0, "val_loss": 279.679012298584, "val_acc": 72.0}
{"epoch": 34, "training_loss": 656.4036140441895, "training_acc": 74.0, "val_loss": 190.53361415863037, "val_acc": 64.0}
{"epoch": 35, "training_loss": 574.6205158233643, "training_acc": 65.0, "val_loss": 134.30455923080444, "val_acc": 72.0}
{"epoch": 36, "training_loss": 321.1528878211975, "training_acc": 80.0, "val_loss": 162.38372325897217, "val_acc": 76.0}
{"epoch": 37, "training_loss": 221.6095061302185, "training_acc": 80.0, "val_loss": 233.00490379333496, "val_acc": 48.0}
{"epoch": 38, "training_loss": 481.67019605636597, "training_acc": 64.0, "val_loss": 223.90954494476318, "val_acc": 72.0}
{"epoch": 39, "training_loss": 391.2061902284622, "training_acc": 76.0, "val_loss": 199.27185773849487, "val_acc": 48.0}
{"epoch": 40, "training_loss": 297.817400932312, "training_acc": 76.0, "val_loss": 200.5314826965332, "val_acc": 72.0}
{"epoch": 41, "training_loss": 335.9419574737549, "training_acc": 75.0, "val_loss": 201.35154724121094, "val_acc": 48.0}
{"epoch": 42, "training_loss": 334.80632066726685, "training_acc": 69.0, "val_loss": 160.70181131362915, "val_acc": 72.0}
{"epoch": 43, "training_loss": 185.56105399131775, "training_acc": 79.0, "val_loss": 146.10575437545776, "val_acc": 48.0}
{"epoch": 44, "training_loss": 345.00098037719727, "training_acc": 67.0, "val_loss": 118.06107759475708, "val_acc": 76.0}
{"epoch": 45, "training_loss": 210.60217666625977, "training_acc": 72.0, "val_loss": 91.7195975780487, "val_acc": 68.0}
{"epoch": 46, "training_loss": 189.78246784210205, "training_acc": 83.0, "val_loss": 83.75672698020935, "val_acc": 68.0}
{"epoch": 47, "training_loss": 126.72898387908936, "training_acc": 82.0, "val_loss": 155.2014708518982, "val_acc": 60.0}
{"epoch": 48, "training_loss": 285.3183307647705, "training_acc": 74.0, "val_loss": 238.014817237854, "val_acc": 72.0}
{"epoch": 49, "training_loss": 439.54161739349365, "training_acc": 76.0, "val_loss": 335.5180025100708, "val_acc": 40.0}
{"epoch": 50, "training_loss": 739.6226634979248, "training_acc": 59.0, "val_loss": 229.16674613952637, "val_acc": 72.0}
{"epoch": 51, "training_loss": 628.4176044464111, "training_acc": 72.0, "val_loss": 121.53750658035278, "val_acc": 60.0}
{"epoch": 52, "training_loss": 424.22596168518066, "training_acc": 66.0, "val_loss": 104.01499271392822, "val_acc": 72.0}
{"epoch": 53, "training_loss": 302.51166915893555, "training_acc": 84.0, "val_loss": 128.60409021377563, "val_acc": 76.0}
{"epoch": 54, "training_loss": 351.2577066421509, "training_acc": 72.0, "val_loss": 120.25840282440186, "val_acc": 64.0}
{"epoch": 55, "training_loss": 314.3514108657837, "training_acc": 74.0, "val_loss": 123.80011081695557, "val_acc": 80.0}
{"epoch": 56, "training_loss": 281.3750886917114, "training_acc": 77.0, "val_loss": 93.7343955039978, "val_acc": 60.0}
{"epoch": 57, "training_loss": 193.03753471374512, "training_acc": 80.0, "val_loss": 117.46823787689209, "val_acc": 76.0}
{"epoch": 58, "training_loss": 249.23698711395264, "training_acc": 76.0, "val_loss": 94.61275935173035, "val_acc": 68.0}
{"epoch": 59, "training_loss": 154.10842609405518, "training_acc": 80.0, "val_loss": 93.06793212890625, "val_acc": 60.0}
{"epoch": 60, "training_loss": 141.64030265808105, "training_acc": 78.0, "val_loss": 102.3232102394104, "val_acc": 76.0}
{"epoch": 61, "training_loss": 177.65489673614502, "training_acc": 86.0, "val_loss": 104.31196689605713, "val_acc": 56.0}
{"epoch": 62, "training_loss": 177.0966820716858, "training_acc": 78.0, "val_loss": 90.8086895942688, "val_acc": 68.0}
{"epoch": 63, "training_loss": 100.340580701828, "training_acc": 84.0, "val_loss": 90.81833362579346, "val_acc": 72.0}
{"epoch": 64, "training_loss": 73.62814784049988, "training_acc": 93.0, "val_loss": 99.36690330505371, "val_acc": 56.0}
{"epoch": 65, "training_loss": 113.96990132331848, "training_acc": 81.0, "val_loss": 153.09655666351318, "val_acc": 76.0}
