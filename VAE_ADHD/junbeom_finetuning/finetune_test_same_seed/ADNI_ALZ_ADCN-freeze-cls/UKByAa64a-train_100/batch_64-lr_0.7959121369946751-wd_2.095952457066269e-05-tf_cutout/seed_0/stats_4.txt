"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 1228.580867767334, "training_acc": 44.0, "val_loss": 692.5251960754395, "val_acc": 72.0}
{"epoch": 1, "training_loss": 2375.1306343078613, "training_acc": 72.0, "val_loss": 151.04976892471313, "val_acc": 36.0}
{"epoch": 2, "training_loss": 772.1215190887451, "training_acc": 41.0, "val_loss": 170.51550149917603, "val_acc": 72.0}
{"epoch": 3, "training_loss": 611.6191387176514, "training_acc": 72.0, "val_loss": 163.72708082199097, "val_acc": 72.0}
{"epoch": 4, "training_loss": 404.3630094528198, "training_acc": 69.0, "val_loss": 110.57870388031006, "val_acc": 60.0}
{"epoch": 5, "training_loss": 555.7176370620728, "training_acc": 59.0, "val_loss": 145.38642168045044, "val_acc": 72.0}
{"epoch": 6, "training_loss": 456.1249027252197, "training_acc": 75.0, "val_loss": 121.08680009841919, "val_acc": 72.0}
{"epoch": 7, "training_loss": 312.34526348114014, "training_acc": 75.0, "val_loss": 65.94065427780151, "val_acc": 56.0}
{"epoch": 8, "training_loss": 256.9553756713867, "training_acc": 61.0, "val_loss": 98.97673726081848, "val_acc": 72.0}
{"epoch": 9, "training_loss": 352.51738262176514, "training_acc": 72.0, "val_loss": 30.265283584594727, "val_acc": 56.0}
{"epoch": 10, "training_loss": 98.73978519439697, "training_acc": 63.0, "val_loss": 40.76342582702637, "val_acc": 72.0}
{"epoch": 11, "training_loss": 143.34881567955017, "training_acc": 73.0, "val_loss": 128.52764129638672, "val_acc": 28.0}
{"epoch": 12, "training_loss": 372.62581634521484, "training_acc": 43.0, "val_loss": 53.899085521698, "val_acc": 72.0}
{"epoch": 13, "training_loss": 148.5477683544159, "training_acc": 70.0, "val_loss": 46.40402793884277, "val_acc": 44.0}
{"epoch": 14, "training_loss": 214.3464183807373, "training_acc": 51.0, "val_loss": 74.13241267204285, "val_acc": 72.0}
{"epoch": 15, "training_loss": 201.38483095169067, "training_acc": 69.0, "val_loss": 27.708563208580017, "val_acc": 64.0}
{"epoch": 16, "training_loss": 92.11623859405518, "training_acc": 74.0, "val_loss": 38.36529850959778, "val_acc": 72.0}
{"epoch": 17, "training_loss": 84.92000937461853, "training_acc": 77.0, "val_loss": 16.55244082212448, "val_acc": 80.0}
{"epoch": 18, "training_loss": 43.63887965679169, "training_acc": 81.0, "val_loss": 23.822513222694397, "val_acc": 72.0}
{"epoch": 19, "training_loss": 96.94100427627563, "training_acc": 65.0, "val_loss": 34.54441726207733, "val_acc": 72.0}
{"epoch": 20, "training_loss": 111.45754015445709, "training_acc": 73.0, "val_loss": 72.59344458580017, "val_acc": 28.0}
{"epoch": 21, "training_loss": 276.72704124450684, "training_acc": 49.0, "val_loss": 80.09440302848816, "val_acc": 72.0}
{"epoch": 22, "training_loss": 196.2506983280182, "training_acc": 76.0, "val_loss": 86.48645877838135, "val_acc": 40.0}
{"epoch": 23, "training_loss": 304.3849492073059, "training_acc": 52.0, "val_loss": 84.42480564117432, "val_acc": 72.0}
{"epoch": 24, "training_loss": 213.0071678161621, "training_acc": 77.0, "val_loss": 36.1868292093277, "val_acc": 72.0}
{"epoch": 25, "training_loss": 212.88965892791748, "training_acc": 58.0, "val_loss": 64.13679122924805, "val_acc": 76.0}
{"epoch": 26, "training_loss": 184.42900896072388, "training_acc": 77.0, "val_loss": 65.11410474777222, "val_acc": 72.0}
{"epoch": 27, "training_loss": 175.52815914154053, "training_acc": 71.0, "val_loss": 28.542548418045044, "val_acc": 64.0}
{"epoch": 28, "training_loss": 88.53902888298035, "training_acc": 77.0, "val_loss": 60.13883352279663, "val_acc": 72.0}
{"epoch": 29, "training_loss": 152.79200315475464, "training_acc": 71.0, "val_loss": 16.001731157302856, "val_acc": 68.0}
{"epoch": 30, "training_loss": 49.76400411128998, "training_acc": 77.0, "val_loss": 22.290709614753723, "val_acc": 76.0}
{"epoch": 31, "training_loss": 50.49575710296631, "training_acc": 76.0, "val_loss": 14.11275863647461, "val_acc": 64.0}
{"epoch": 32, "training_loss": 59.47697377204895, "training_acc": 82.0, "val_loss": 13.311535120010376, "val_acc": 68.0}
{"epoch": 33, "training_loss": 75.58363914489746, "training_acc": 72.0, "val_loss": 41.70961678028107, "val_acc": 72.0}
{"epoch": 34, "training_loss": 141.6218273639679, "training_acc": 72.0, "val_loss": 49.2754340171814, "val_acc": 40.0}
{"epoch": 35, "training_loss": 121.58021473884583, "training_acc": 63.0, "val_loss": 108.49446058273315, "val_acc": 72.0}
{"epoch": 36, "training_loss": 354.0686206817627, "training_acc": 72.0, "val_loss": 77.98704504966736, "val_acc": 72.0}
{"epoch": 37, "training_loss": 196.1323618888855, "training_acc": 70.0, "val_loss": 51.49986743927002, "val_acc": 60.0}
{"epoch": 38, "training_loss": 246.34696435928345, "training_acc": 63.0, "val_loss": 123.7966775894165, "val_acc": 72.0}
{"epoch": 39, "training_loss": 315.72340869903564, "training_acc": 74.0, "val_loss": 34.43392217159271, "val_acc": 76.0}
{"epoch": 40, "training_loss": 186.5959825515747, "training_acc": 67.0, "val_loss": 49.629226326942444, "val_acc": 80.0}
{"epoch": 41, "training_loss": 160.14274501800537, "training_acc": 76.0, "val_loss": 44.13479566574097, "val_acc": 76.0}
{"epoch": 42, "training_loss": 126.69707870483398, "training_acc": 69.0, "val_loss": 26.39916241168976, "val_acc": 80.0}
{"epoch": 43, "training_loss": 67.54791116714478, "training_acc": 83.0, "val_loss": 16.957174241542816, "val_acc": 84.0}
{"epoch": 44, "training_loss": 83.9648790359497, "training_acc": 74.0, "val_loss": 58.734285831451416, "val_acc": 72.0}
{"epoch": 45, "training_loss": 164.39760541915894, "training_acc": 73.0, "val_loss": 24.031485617160797, "val_acc": 80.0}
{"epoch": 46, "training_loss": 102.39057731628418, "training_acc": 72.0, "val_loss": 24.698670208454132, "val_acc": 76.0}
{"epoch": 47, "training_loss": 111.27011108398438, "training_acc": 80.0, "val_loss": 41.45009517669678, "val_acc": 76.0}
{"epoch": 48, "training_loss": 124.22269678115845, "training_acc": 70.0, "val_loss": 24.534684419631958, "val_acc": 80.0}
{"epoch": 49, "training_loss": 41.464026927948, "training_acc": 87.0, "val_loss": 58.06320905685425, "val_acc": 76.0}
{"epoch": 50, "training_loss": 88.15949964523315, "training_acc": 83.0, "val_loss": 47.54104018211365, "val_acc": 40.0}
{"epoch": 51, "training_loss": 178.62324142456055, "training_acc": 65.0, "val_loss": 43.61189305782318, "val_acc": 76.0}
