"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 32 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 12992.078701019287, "training_acc": 61.0, "val_loss": 6062.087249755859, "val_acc": 28.0}
{"epoch": 1, "training_loss": 19660.60986328125, "training_acc": 27.0, "val_loss": 3124.732780456543, "val_acc": 72.0}
{"epoch": 2, "training_loss": 10361.812927246094, "training_acc": 72.0, "val_loss": 1067.8410530090332, "val_acc": 76.0}
{"epoch": 3, "training_loss": 11968.246948242188, "training_acc": 51.0, "val_loss": 1406.8181037902832, "val_acc": 72.0}
{"epoch": 4, "training_loss": 6233.039855957031, "training_acc": 74.0, "val_loss": 3623.9830017089844, "val_acc": 72.0}
{"epoch": 5, "training_loss": 9643.98648071289, "training_acc": 71.0, "val_loss": 1551.314640045166, "val_acc": 76.0}
{"epoch": 6, "training_loss": 7143.41162109375, "training_acc": 68.0, "val_loss": 1752.2125244140625, "val_acc": 60.0}
{"epoch": 7, "training_loss": 6908.320159912109, "training_acc": 67.0, "val_loss": 1705.8088302612305, "val_acc": 72.0}
{"epoch": 8, "training_loss": 4037.1142578125, "training_acc": 72.0, "val_loss": 853.3573150634766, "val_acc": 72.0}
{"epoch": 9, "training_loss": 2652.145565032959, "training_acc": 67.0, "val_loss": 698.4842300415039, "val_acc": 64.0}
{"epoch": 10, "training_loss": 1092.1244869232178, "training_acc": 69.0, "val_loss": 586.707878112793, "val_acc": 52.0}
{"epoch": 11, "training_loss": 2004.5750207901, "training_acc": 67.0, "val_loss": 308.7547540664673, "val_acc": 52.0}
{"epoch": 12, "training_loss": 791.7557201385498, "training_acc": 74.0, "val_loss": 771.3765621185303, "val_acc": 36.0}
{"epoch": 13, "training_loss": 2930.7455444335938, "training_acc": 54.0, "val_loss": 850.4673004150391, "val_acc": 72.0}
{"epoch": 14, "training_loss": 3375.2163696289062, "training_acc": 56.0, "val_loss": 1381.595230102539, "val_acc": 72.0}
{"epoch": 15, "training_loss": 8105.75830078125, "training_acc": 72.0, "val_loss": 2460.097122192383, "val_acc": 72.0}
{"epoch": 16, "training_loss": 6482.075103759766, "training_acc": 70.0, "val_loss": 334.99951362609863, "val_acc": 56.0}
{"epoch": 17, "training_loss": 1927.9696855545044, "training_acc": 63.0, "val_loss": 221.31469249725342, "val_acc": 56.0}
{"epoch": 18, "training_loss": 1219.6422710418701, "training_acc": 62.0, "val_loss": 581.890869140625, "val_acc": 72.0}
{"epoch": 19, "training_loss": 1885.6680145263672, "training_acc": 69.0, "val_loss": 452.908992767334, "val_acc": 72.0}
{"epoch": 20, "training_loss": 1791.1366577148438, "training_acc": 77.0, "val_loss": 458.62603187561035, "val_acc": 52.0}
{"epoch": 21, "training_loss": 1547.4913787841797, "training_acc": 65.0, "val_loss": 739.5639419555664, "val_acc": 72.0}
{"epoch": 22, "training_loss": 1641.285774230957, "training_acc": 70.0, "val_loss": 167.95203685760498, "val_acc": 64.0}
{"epoch": 23, "training_loss": 291.8077383041382, "training_acc": 89.0, "val_loss": 376.84342861175537, "val_acc": 72.0}
{"epoch": 24, "training_loss": 529.098030090332, "training_acc": 82.0, "val_loss": 305.8964252471924, "val_acc": 76.0}
{"epoch": 25, "training_loss": 1041.9648780822754, "training_acc": 76.0, "val_loss": 315.09361267089844, "val_acc": 76.0}
{"epoch": 26, "training_loss": 1150.595516204834, "training_acc": 77.0, "val_loss": 170.70598602294922, "val_acc": 64.0}
{"epoch": 27, "training_loss": 1729.0226669311523, "training_acc": 65.0, "val_loss": 128.58437299728394, "val_acc": 80.0}
{"epoch": 28, "training_loss": 909.4993629455566, "training_acc": 71.0, "val_loss": 1277.9983520507812, "val_acc": 72.0}
{"epoch": 29, "training_loss": 4612.2036056518555, "training_acc": 72.0, "val_loss": 2285.987091064453, "val_acc": 28.0}
{"epoch": 30, "training_loss": 7991.018966674805, "training_acc": 34.0, "val_loss": 1949.5447158813477, "val_acc": 72.0}
{"epoch": 31, "training_loss": 5564.245048522949, "training_acc": 73.0, "val_loss": 744.0468788146973, "val_acc": 68.0}
{"epoch": 32, "training_loss": 4479.1853103637695, "training_acc": 59.0, "val_loss": 1295.4832077026367, "val_acc": 76.0}
{"epoch": 33, "training_loss": 4650.187973022461, "training_acc": 76.0, "val_loss": 1252.5638580322266, "val_acc": 76.0}
{"epoch": 34, "training_loss": 3397.201427459717, "training_acc": 70.0, "val_loss": 749.0739345550537, "val_acc": 68.0}
{"epoch": 35, "training_loss": 3137.688217163086, "training_acc": 75.0, "val_loss": 683.4904670715332, "val_acc": 68.0}
{"epoch": 36, "training_loss": 3174.764404296875, "training_acc": 63.0, "val_loss": 890.8324241638184, "val_acc": 72.0}
{"epoch": 37, "training_loss": 5422.500183105469, "training_acc": 73.0, "val_loss": 432.59992599487305, "val_acc": 72.0}
{"epoch": 38, "training_loss": 3748.1625442504883, "training_acc": 51.0, "val_loss": 1705.5414199829102, "val_acc": 72.0}
{"epoch": 39, "training_loss": 8625.020385742188, "training_acc": 72.0, "val_loss": 3021.675491333008, "val_acc": 72.0}
{"epoch": 40, "training_loss": 6941.67236328125, "training_acc": 74.0, "val_loss": 1017.3091888427734, "val_acc": 48.0}
{"epoch": 41, "training_loss": 5228.298309326172, "training_acc": 55.0, "val_loss": 770.7047939300537, "val_acc": 68.0}
{"epoch": 42, "training_loss": 1386.382080078125, "training_acc": 83.0, "val_loss": 412.21771240234375, "val_acc": 72.0}
{"epoch": 43, "training_loss": 409.92409718036447, "training_acc": 87.0, "val_loss": 293.10338497161865, "val_acc": 64.0}
{"epoch": 44, "training_loss": 901.2001342773438, "training_acc": 69.0, "val_loss": 356.93516731262207, "val_acc": 68.0}
{"epoch": 45, "training_loss": 376.7965316772461, "training_acc": 83.0, "val_loss": 506.953763961792, "val_acc": 72.0}
{"epoch": 46, "training_loss": 1124.466007232666, "training_acc": 81.0, "val_loss": 335.3503227233887, "val_acc": 76.0}
