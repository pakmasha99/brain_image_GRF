"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 32 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 12959.612594604492, "training_acc": 62.0, "val_loss": 6292.660903930664, "val_acc": 28.0}
{"epoch": 1, "training_loss": 19503.149169921875, "training_acc": 36.0, "val_loss": 5205.177688598633, "val_acc": 72.0}
{"epoch": 2, "training_loss": 22107.4404296875, "training_acc": 72.0, "val_loss": 6210.262680053711, "val_acc": 72.0}
{"epoch": 3, "training_loss": 16997.224884033203, "training_acc": 72.0, "val_loss": 1857.1331024169922, "val_acc": 64.0}
{"epoch": 4, "training_loss": 15851.553192138672, "training_acc": 52.0, "val_loss": 1760.1818084716797, "val_acc": 64.0}
{"epoch": 5, "training_loss": 7127.7611083984375, "training_acc": 69.0, "val_loss": 4356.621551513672, "val_acc": 72.0}
{"epoch": 6, "training_loss": 13159.478515625, "training_acc": 73.0, "val_loss": 1863.631820678711, "val_acc": 72.0}
{"epoch": 7, "training_loss": 6666.109405517578, "training_acc": 61.0, "val_loss": 1077.6189804077148, "val_acc": 68.0}
{"epoch": 8, "training_loss": 3903.6002502441406, "training_acc": 76.0, "val_loss": 707.3870182037354, "val_acc": 72.0}
{"epoch": 9, "training_loss": 2533.3787231445312, "training_acc": 63.0, "val_loss": 360.302472114563, "val_acc": 72.0}
{"epoch": 10, "training_loss": 2130.0415802001953, "training_acc": 63.0, "val_loss": 976.8153190612793, "val_acc": 40.0}
{"epoch": 11, "training_loss": 1844.0278854370117, "training_acc": 56.0, "val_loss": 254.44910526275635, "val_acc": 76.0}
{"epoch": 12, "training_loss": 1065.3592376708984, "training_acc": 76.0, "val_loss": 565.2814865112305, "val_acc": 60.0}
{"epoch": 13, "training_loss": 1879.6299285888672, "training_acc": 68.0, "val_loss": 559.4257831573486, "val_acc": 56.0}
{"epoch": 14, "training_loss": 3197.9547119140625, "training_acc": 52.0, "val_loss": 845.4245567321777, "val_acc": 72.0}
{"epoch": 15, "training_loss": 1655.272476196289, "training_acc": 64.0, "val_loss": 165.69443941116333, "val_acc": 76.0}
{"epoch": 16, "training_loss": 588.349796295166, "training_acc": 70.0, "val_loss": 245.2277421951294, "val_acc": 60.0}
{"epoch": 17, "training_loss": 825.4327260404825, "training_acc": 73.0, "val_loss": 512.3539924621582, "val_acc": 76.0}
{"epoch": 18, "training_loss": 1225.2694702148438, "training_acc": 72.0, "val_loss": 373.36230278015137, "val_acc": 76.0}
{"epoch": 19, "training_loss": 833.7066955566406, "training_acc": 79.0, "val_loss": 175.46361684799194, "val_acc": 56.0}
{"epoch": 20, "training_loss": 1061.9377666248474, "training_acc": 79.0, "val_loss": 287.74468898773193, "val_acc": 68.0}
{"epoch": 21, "training_loss": 1650.9459075927734, "training_acc": 62.0, "val_loss": 342.9762601852417, "val_acc": 64.0}
{"epoch": 22, "training_loss": 1716.1027069091797, "training_acc": 64.0, "val_loss": 630.5757999420166, "val_acc": 72.0}
{"epoch": 23, "training_loss": 1529.8217468261719, "training_acc": 61.0, "val_loss": 702.4997234344482, "val_acc": 72.0}
{"epoch": 24, "training_loss": 1317.1553344726562, "training_acc": 67.0, "val_loss": 1382.561206817627, "val_acc": 72.0}
{"epoch": 25, "training_loss": 8413.780151367188, "training_acc": 72.0, "val_loss": 605.4574966430664, "val_acc": 72.0}
{"epoch": 26, "training_loss": 11277.6650390625, "training_acc": 40.0, "val_loss": 489.3402099609375, "val_acc": 72.0}
{"epoch": 27, "training_loss": 4863.037536621094, "training_acc": 75.0, "val_loss": 2054.783821105957, "val_acc": 72.0}
{"epoch": 28, "training_loss": 4381.419738769531, "training_acc": 73.0, "val_loss": 1706.5532684326172, "val_acc": 36.0}
{"epoch": 29, "training_loss": 6110.178527832031, "training_acc": 49.0, "val_loss": 1828.056526184082, "val_acc": 72.0}
{"epoch": 30, "training_loss": 3750.63037109375, "training_acc": 67.0, "val_loss": 549.5785236358643, "val_acc": 76.0}
{"epoch": 31, "training_loss": 4122.924057006836, "training_acc": 71.0, "val_loss": 1533.0881118774414, "val_acc": 72.0}
{"epoch": 32, "training_loss": 3316.2444610595703, "training_acc": 72.0, "val_loss": 961.8839263916016, "val_acc": 76.0}
{"epoch": 33, "training_loss": 3388.6921997070312, "training_acc": 75.0, "val_loss": 1467.1270370483398, "val_acc": 72.0}
{"epoch": 34, "training_loss": 2593.73291015625, "training_acc": 71.0, "val_loss": 614.018440246582, "val_acc": 68.0}
