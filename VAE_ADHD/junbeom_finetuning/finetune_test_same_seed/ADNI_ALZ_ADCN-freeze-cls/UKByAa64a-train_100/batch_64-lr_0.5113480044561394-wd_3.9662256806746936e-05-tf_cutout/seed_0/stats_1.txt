"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 665.7872543334961, "training_acc": 70.0, "val_loss": 407.51261711120605, "val_acc": 72.0}
{"epoch": 1, "training_loss": 1227.2623176574707, "training_acc": 72.0, "val_loss": 601.3959884643555, "val_acc": 28.0}
{"epoch": 2, "training_loss": 2098.3160362243652, "training_acc": 28.0, "val_loss": 42.81090497970581, "val_acc": 56.0}
{"epoch": 3, "training_loss": 304.2162628173828, "training_acc": 72.0, "val_loss": 255.84955215454102, "val_acc": 72.0}
{"epoch": 4, "training_loss": 976.8597259521484, "training_acc": 72.0, "val_loss": 193.8201904296875, "val_acc": 72.0}
{"epoch": 5, "training_loss": 586.9643535614014, "training_acc": 74.0, "val_loss": 121.34753465652466, "val_acc": 56.0}
{"epoch": 6, "training_loss": 583.5490970611572, "training_acc": 55.0, "val_loss": 190.10415077209473, "val_acc": 60.0}
{"epoch": 7, "training_loss": 668.8131761550903, "training_acc": 50.0, "val_loss": 104.97983694076538, "val_acc": 76.0}
{"epoch": 8, "training_loss": 526.3145332336426, "training_acc": 73.0, "val_loss": 205.13458251953125, "val_acc": 72.0}
{"epoch": 9, "training_loss": 775.7799186706543, "training_acc": 72.0, "val_loss": 115.45077562332153, "val_acc": 80.0}
{"epoch": 10, "training_loss": 350.7112617492676, "training_acc": 73.0, "val_loss": 143.03313493728638, "val_acc": 60.0}
{"epoch": 11, "training_loss": 642.8457660675049, "training_acc": 50.0, "val_loss": 93.1779146194458, "val_acc": 56.0}
{"epoch": 12, "training_loss": 265.94274044036865, "training_acc": 64.0, "val_loss": 96.56335115432739, "val_acc": 76.0}
{"epoch": 13, "training_loss": 422.3213996887207, "training_acc": 72.0, "val_loss": 91.75961017608643, "val_acc": 72.0}
{"epoch": 14, "training_loss": 336.167916059494, "training_acc": 72.0, "val_loss": 68.31713914871216, "val_acc": 52.0}
{"epoch": 15, "training_loss": 258.2450180053711, "training_acc": 56.0, "val_loss": 21.803638339042664, "val_acc": 68.0}
{"epoch": 16, "training_loss": 132.03315591812134, "training_acc": 78.0, "val_loss": 69.10633444786072, "val_acc": 72.0}
{"epoch": 17, "training_loss": 206.74834299087524, "training_acc": 74.0, "val_loss": 47.1936821937561, "val_acc": 48.0}
{"epoch": 18, "training_loss": 169.83397722244263, "training_acc": 52.0, "val_loss": 47.26279377937317, "val_acc": 72.0}
{"epoch": 19, "training_loss": 176.84856605529785, "training_acc": 72.0, "val_loss": 44.39015984535217, "val_acc": 72.0}
{"epoch": 20, "training_loss": 127.80353093147278, "training_acc": 68.0, "val_loss": 38.54916989803314, "val_acc": 44.0}
{"epoch": 21, "training_loss": 104.10137152671814, "training_acc": 68.0, "val_loss": 60.16669273376465, "val_acc": 72.0}
{"epoch": 22, "training_loss": 163.87480115890503, "training_acc": 74.0, "val_loss": 24.611283838748932, "val_acc": 60.0}
{"epoch": 23, "training_loss": 116.61647939682007, "training_acc": 64.0, "val_loss": 24.860943853855133, "val_acc": 68.0}
{"epoch": 24, "training_loss": 78.23569440841675, "training_acc": 80.0, "val_loss": 63.34756016731262, "val_acc": 72.0}
{"epoch": 25, "training_loss": 143.543053150177, "training_acc": 73.0, "val_loss": 38.05056810379028, "val_acc": 52.0}
{"epoch": 26, "training_loss": 96.77778470516205, "training_acc": 62.0, "val_loss": 34.588465094566345, "val_acc": 76.0}
{"epoch": 27, "training_loss": 68.37361776828766, "training_acc": 78.0, "val_loss": 25.425660610198975, "val_acc": 52.0}
{"epoch": 28, "training_loss": 43.10303473472595, "training_acc": 77.0, "val_loss": 26.05311870574951, "val_acc": 68.0}
{"epoch": 29, "training_loss": 34.85187005996704, "training_acc": 83.0, "val_loss": 29.264196753501892, "val_acc": 36.0}
{"epoch": 30, "training_loss": 43.375898361206055, "training_acc": 79.0, "val_loss": 25.819015502929688, "val_acc": 64.0}
{"epoch": 31, "training_loss": 59.83819317817688, "training_acc": 76.0, "val_loss": 33.062419295310974, "val_acc": 72.0}
{"epoch": 32, "training_loss": 55.09077703952789, "training_acc": 78.0, "val_loss": 28.95079255104065, "val_acc": 52.0}
{"epoch": 33, "training_loss": 56.30437254905701, "training_acc": 75.0, "val_loss": 30.486658215522766, "val_acc": 76.0}
{"epoch": 34, "training_loss": 53.780240416526794, "training_acc": 78.0, "val_loss": 24.491789937019348, "val_acc": 56.0}
