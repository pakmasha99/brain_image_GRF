"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 5480.2988357543945, "training_acc": 48.0, "val_loss": 3857.6290130615234, "val_acc": 72.0}
{"epoch": 1, "training_loss": 11337.614196777344, "training_acc": 72.0, "val_loss": 3044.203567504883, "val_acc": 28.0}
{"epoch": 2, "training_loss": 12397.468200683594, "training_acc": 31.0, "val_loss": 611.615514755249, "val_acc": 68.0}
{"epoch": 3, "training_loss": 3613.544143676758, "training_acc": 65.0, "val_loss": 1999.208641052246, "val_acc": 72.0}
{"epoch": 4, "training_loss": 6218.831405639648, "training_acc": 72.0, "val_loss": 985.0334167480469, "val_acc": 68.0}
{"epoch": 5, "training_loss": 3540.1118774414062, "training_acc": 65.0, "val_loss": 1059.890079498291, "val_acc": 56.0}
{"epoch": 6, "training_loss": 5675.517868041992, "training_acc": 54.0, "val_loss": 808.0322265625, "val_acc": 68.0}
{"epoch": 7, "training_loss": 3480.7163848876953, "training_acc": 71.0, "val_loss": 1515.5609130859375, "val_acc": 72.0}
{"epoch": 8, "training_loss": 4201.7770919799805, "training_acc": 73.0, "val_loss": 732.7744483947754, "val_acc": 68.0}
{"epoch": 9, "training_loss": 1873.656364440918, "training_acc": 73.0, "val_loss": 671.616268157959, "val_acc": 52.0}
{"epoch": 10, "training_loss": 2892.441452026367, "training_acc": 62.0, "val_loss": 669.0785884857178, "val_acc": 72.0}
{"epoch": 11, "training_loss": 2024.8311920166016, "training_acc": 72.0, "val_loss": 411.4530563354492, "val_acc": 60.0}
{"epoch": 12, "training_loss": 1313.8899993896484, "training_acc": 64.0, "val_loss": 573.0597496032715, "val_acc": 52.0}
{"epoch": 13, "training_loss": 1706.6979217529297, "training_acc": 58.0, "val_loss": 1033.6935997009277, "val_acc": 72.0}
{"epoch": 14, "training_loss": 3423.1579818725586, "training_acc": 72.0, "val_loss": 437.85338401794434, "val_acc": 60.0}
{"epoch": 15, "training_loss": 2057.1795196533203, "training_acc": 59.0, "val_loss": 626.7927646636963, "val_acc": 52.0}
{"epoch": 16, "training_loss": 2157.002899169922, "training_acc": 53.0, "val_loss": 912.5632286071777, "val_acc": 72.0}
{"epoch": 17, "training_loss": 2720.319793701172, "training_acc": 72.0, "val_loss": 330.59685230255127, "val_acc": 68.0}
{"epoch": 18, "training_loss": 1104.9147644042969, "training_acc": 74.0, "val_loss": 420.40815353393555, "val_acc": 56.0}
{"epoch": 19, "training_loss": 1898.2664527893066, "training_acc": 60.0, "val_loss": 571.5453147888184, "val_acc": 72.0}
{"epoch": 20, "training_loss": 1085.670841217041, "training_acc": 75.0, "val_loss": 290.61710834503174, "val_acc": 76.0}
{"epoch": 21, "training_loss": 1184.2446517944336, "training_acc": 65.0, "val_loss": 411.6641044616699, "val_acc": 72.0}
{"epoch": 22, "training_loss": 1094.5651664733887, "training_acc": 75.0, "val_loss": 290.72866439819336, "val_acc": 72.0}
{"epoch": 23, "training_loss": 534.2046508789062, "training_acc": 79.0, "val_loss": 340.8510684967041, "val_acc": 56.0}
{"epoch": 24, "training_loss": 1158.3951377868652, "training_acc": 60.0, "val_loss": 542.2077655792236, "val_acc": 72.0}
{"epoch": 25, "training_loss": 1272.2363033294678, "training_acc": 74.0, "val_loss": 264.46385383605957, "val_acc": 56.0}
{"epoch": 26, "training_loss": 720.6010761260986, "training_acc": 65.0, "val_loss": 512.0140075683594, "val_acc": 72.0}
{"epoch": 27, "training_loss": 1504.0748672485352, "training_acc": 72.0, "val_loss": 233.59851837158203, "val_acc": 68.0}
{"epoch": 28, "training_loss": 694.0312881469727, "training_acc": 76.0, "val_loss": 192.21919775009155, "val_acc": 76.0}
{"epoch": 29, "training_loss": 710.5493087768555, "training_acc": 71.0, "val_loss": 396.8648672103882, "val_acc": 72.0}
{"epoch": 30, "training_loss": 871.5774612426758, "training_acc": 73.0, "val_loss": 204.08036708831787, "val_acc": 72.0}
{"epoch": 31, "training_loss": 429.1129198074341, "training_acc": 80.0, "val_loss": 267.4161434173584, "val_acc": 76.0}
{"epoch": 32, "training_loss": 398.8729238510132, "training_acc": 82.0, "val_loss": 132.60287046432495, "val_acc": 72.0}
{"epoch": 33, "training_loss": 374.8870530128479, "training_acc": 78.0, "val_loss": 280.16204833984375, "val_acc": 72.0}
{"epoch": 34, "training_loss": 392.22471618652344, "training_acc": 79.0, "val_loss": 427.7991771697998, "val_acc": 44.0}
{"epoch": 35, "training_loss": 968.3755340576172, "training_acc": 63.0, "val_loss": 605.9298038482666, "val_acc": 72.0}
{"epoch": 36, "training_loss": 1926.8108100891113, "training_acc": 72.0, "val_loss": 297.1296548843384, "val_acc": 72.0}
{"epoch": 37, "training_loss": 1392.4692077636719, "training_acc": 64.0, "val_loss": 171.0834503173828, "val_acc": 60.0}
{"epoch": 38, "training_loss": 1437.5114822387695, "training_acc": 59.0, "val_loss": 948.9212989807129, "val_acc": 72.0}
{"epoch": 39, "training_loss": 2731.6833572387695, "training_acc": 72.0, "val_loss": 251.2007713317871, "val_acc": 72.0}
{"epoch": 40, "training_loss": 1641.8329315185547, "training_acc": 69.0, "val_loss": 376.10294818878174, "val_acc": 52.0}
{"epoch": 41, "training_loss": 1715.7790412902832, "training_acc": 64.0, "val_loss": 903.3684730529785, "val_acc": 72.0}
{"epoch": 42, "training_loss": 2546.250732421875, "training_acc": 72.0, "val_loss": 430.7513236999512, "val_acc": 72.0}
{"epoch": 43, "training_loss": 1422.781478881836, "training_acc": 73.0, "val_loss": 473.1575012207031, "val_acc": 52.0}
{"epoch": 44, "training_loss": 1640.2732238769531, "training_acc": 68.0, "val_loss": 737.5802040100098, "val_acc": 72.0}
{"epoch": 45, "training_loss": 2126.474807739258, "training_acc": 72.0, "val_loss": 487.39285469055176, "val_acc": 72.0}
{"epoch": 46, "training_loss": 1518.2757949829102, "training_acc": 64.0, "val_loss": 210.46714782714844, "val_acc": 64.0}
{"epoch": 47, "training_loss": 753.2670574188232, "training_acc": 76.0, "val_loss": 504.8985481262207, "val_acc": 72.0}
{"epoch": 48, "training_loss": 988.932674407959, "training_acc": 74.0, "val_loss": 389.2742156982422, "val_acc": 52.0}
{"epoch": 49, "training_loss": 945.4025082588196, "training_acc": 60.0, "val_loss": 445.6489562988281, "val_acc": 72.0}
{"epoch": 50, "training_loss": 879.0413627624512, "training_acc": 76.0, "val_loss": 155.85689544677734, "val_acc": 72.0}
{"epoch": 51, "training_loss": 572.8463172912598, "training_acc": 73.0, "val_loss": 275.0370979309082, "val_acc": 72.0}
