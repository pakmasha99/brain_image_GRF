"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 1288.664077758789, "training_acc": 72.0, "val_loss": 831.8002700805664, "val_acc": 72.0}
{"epoch": 1, "training_loss": 2183.3155422210693, "training_acc": 69.0, "val_loss": 726.73020362854, "val_acc": 28.0}
{"epoch": 2, "training_loss": 2072.921417236328, "training_acc": 40.0, "val_loss": 235.28358936309814, "val_acc": 72.0}
{"epoch": 3, "training_loss": 719.2101993560791, "training_acc": 74.0, "val_loss": 207.4754238128662, "val_acc": 64.0}
{"epoch": 4, "training_loss": 643.4200706481934, "training_acc": 53.0, "val_loss": 153.19790840148926, "val_acc": 72.0}
{"epoch": 5, "training_loss": 506.1307945251465, "training_acc": 72.0, "val_loss": 142.28062629699707, "val_acc": 72.0}
{"epoch": 6, "training_loss": 495.60008239746094, "training_acc": 65.0, "val_loss": 156.34784698486328, "val_acc": 64.0}
{"epoch": 7, "training_loss": 354.8526830673218, "training_acc": 67.0, "val_loss": 126.46851539611816, "val_acc": 72.0}
{"epoch": 8, "training_loss": 350.3621129989624, "training_acc": 72.0, "val_loss": 260.00030040740967, "val_acc": 24.0}
{"epoch": 9, "training_loss": 608.8266460895538, "training_acc": 49.0, "val_loss": 150.19177198410034, "val_acc": 72.0}
{"epoch": 10, "training_loss": 621.5133361816406, "training_acc": 72.0, "val_loss": 62.45846748352051, "val_acc": 76.0}
{"epoch": 11, "training_loss": 511.5800018310547, "training_acc": 56.0, "val_loss": 115.28545618057251, "val_acc": 48.0}
{"epoch": 12, "training_loss": 418.55994415283203, "training_acc": 60.0, "val_loss": 279.7980070114136, "val_acc": 72.0}
{"epoch": 13, "training_loss": 1079.6913814544678, "training_acc": 72.0, "val_loss": 165.16305208206177, "val_acc": 72.0}
{"epoch": 14, "training_loss": 431.4653630256653, "training_acc": 77.0, "val_loss": 265.2212142944336, "val_acc": 52.0}
{"epoch": 15, "training_loss": 799.1673831939697, "training_acc": 50.0, "val_loss": 110.06721258163452, "val_acc": 80.0}
{"epoch": 16, "training_loss": 434.78915214538574, "training_acc": 73.0, "val_loss": 191.75527095794678, "val_acc": 72.0}
{"epoch": 17, "training_loss": 584.0006017684937, "training_acc": 71.0, "val_loss": 151.4398217201233, "val_acc": 52.0}
{"epoch": 18, "training_loss": 415.88744163513184, "training_acc": 59.0, "val_loss": 87.23810315132141, "val_acc": 72.0}
{"epoch": 19, "training_loss": 172.64209270477295, "training_acc": 83.0, "val_loss": 90.78920483589172, "val_acc": 80.0}
{"epoch": 20, "training_loss": 277.3550100326538, "training_acc": 75.0, "val_loss": 82.64275789260864, "val_acc": 60.0}
{"epoch": 21, "training_loss": 223.52102327346802, "training_acc": 67.0, "val_loss": 26.792597770690918, "val_acc": 88.0}
{"epoch": 22, "training_loss": 125.78880834579468, "training_acc": 71.0, "val_loss": 22.915390133857727, "val_acc": 72.0}
{"epoch": 23, "training_loss": 128.09845638275146, "training_acc": 70.0, "val_loss": 72.64711856842041, "val_acc": 36.0}
{"epoch": 24, "training_loss": 213.81780338287354, "training_acc": 63.0, "val_loss": 23.13077449798584, "val_acc": 60.0}
{"epoch": 25, "training_loss": 64.12440156936646, "training_acc": 77.0, "val_loss": 25.10818839073181, "val_acc": 88.0}
{"epoch": 26, "training_loss": 137.7255620956421, "training_acc": 74.0, "val_loss": 63.24530243873596, "val_acc": 76.0}
{"epoch": 27, "training_loss": 222.9309310913086, "training_acc": 74.0, "val_loss": 68.32044124603271, "val_acc": 52.0}
{"epoch": 28, "training_loss": 163.84858536720276, "training_acc": 69.0, "val_loss": 49.11447763442993, "val_acc": 84.0}
{"epoch": 29, "training_loss": 152.94306087493896, "training_acc": 77.0, "val_loss": 70.10135650634766, "val_acc": 48.0}
{"epoch": 30, "training_loss": 143.39304852485657, "training_acc": 69.0, "val_loss": 86.75155639648438, "val_acc": 72.0}
{"epoch": 31, "training_loss": 319.0923614501953, "training_acc": 72.0, "val_loss": 69.53974962234497, "val_acc": 44.0}
{"epoch": 32, "training_loss": 122.10832977294922, "training_acc": 67.0, "val_loss": 71.0375726222992, "val_acc": 72.0}
{"epoch": 33, "training_loss": 297.28445625305176, "training_acc": 73.0, "val_loss": 105.68386316299438, "val_acc": 40.0}
{"epoch": 34, "training_loss": 236.3402009010315, "training_acc": 60.0, "val_loss": 34.16337966918945, "val_acc": 76.0}
{"epoch": 35, "training_loss": 89.17656064033508, "training_acc": 81.0, "val_loss": 64.10931944847107, "val_acc": 60.0}
{"epoch": 36, "training_loss": 128.10383439064026, "training_acc": 74.0, "val_loss": 40.12526571750641, "val_acc": 80.0}
{"epoch": 37, "training_loss": 123.18665814399719, "training_acc": 77.0, "val_loss": 71.01408243179321, "val_acc": 44.0}
{"epoch": 38, "training_loss": 251.81581592559814, "training_acc": 60.0, "val_loss": 60.27929186820984, "val_acc": 72.0}
{"epoch": 39, "training_loss": 320.96124935150146, "training_acc": 57.0, "val_loss": 31.611844897270203, "val_acc": 76.0}
{"epoch": 40, "training_loss": 116.2165150642395, "training_acc": 82.0, "val_loss": 32.041701674461365, "val_acc": 80.0}
{"epoch": 41, "training_loss": 97.72765684127808, "training_acc": 76.0, "val_loss": 36.438074707984924, "val_acc": 76.0}
