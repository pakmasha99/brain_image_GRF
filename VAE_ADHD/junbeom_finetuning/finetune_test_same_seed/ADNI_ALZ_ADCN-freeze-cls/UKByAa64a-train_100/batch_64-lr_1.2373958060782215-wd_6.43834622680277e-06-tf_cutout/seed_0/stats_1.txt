"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 2301.978317260742, "training_acc": 49.0, "val_loss": 1020.7477569580078, "val_acc": 72.0}
{"epoch": 1, "training_loss": 3036.5897483825684, "training_acc": 72.0, "val_loss": 964.757251739502, "val_acc": 28.0}
{"epoch": 2, "training_loss": 3488.6575317382812, "training_acc": 28.0, "val_loss": 96.66659235954285, "val_acc": 80.0}
{"epoch": 3, "training_loss": 987.507080078125, "training_acc": 72.0, "val_loss": 582.4184417724609, "val_acc": 72.0}
{"epoch": 4, "training_loss": 2131.324821472168, "training_acc": 72.0, "val_loss": 364.52856063842773, "val_acc": 72.0}
{"epoch": 5, "training_loss": 1068.2063102722168, "training_acc": 76.0, "val_loss": 344.5563316345215, "val_acc": 60.0}
{"epoch": 6, "training_loss": 1578.0818901062012, "training_acc": 48.0, "val_loss": 307.0765256881714, "val_acc": 56.0}
{"epoch": 7, "training_loss": 943.572925567627, "training_acc": 62.0, "val_loss": 240.7442569732666, "val_acc": 76.0}
{"epoch": 8, "training_loss": 1030.3027973175049, "training_acc": 73.0, "val_loss": 274.36981201171875, "val_acc": 72.0}
{"epoch": 9, "training_loss": 814.9242095947266, "training_acc": 70.0, "val_loss": 133.7479829788208, "val_acc": 52.0}
{"epoch": 10, "training_loss": 554.1043739318848, "training_acc": 62.0, "val_loss": 148.75844717025757, "val_acc": 56.0}
{"epoch": 11, "training_loss": 575.8253440856934, "training_acc": 64.0, "val_loss": 181.9486379623413, "val_acc": 72.0}
{"epoch": 12, "training_loss": 649.688720703125, "training_acc": 72.0, "val_loss": 81.38201832771301, "val_acc": 72.0}
{"epoch": 13, "training_loss": 401.9917850494385, "training_acc": 67.0, "val_loss": 100.17348527908325, "val_acc": 52.0}
{"epoch": 14, "training_loss": 347.5362539291382, "training_acc": 62.0, "val_loss": 245.4822301864624, "val_acc": 72.0}
{"epoch": 15, "training_loss": 744.3376207351685, "training_acc": 72.0, "val_loss": 71.02214694023132, "val_acc": 72.0}
{"epoch": 16, "training_loss": 476.47019958496094, "training_acc": 57.0, "val_loss": 93.74091029167175, "val_acc": 32.0}
{"epoch": 17, "training_loss": 330.2096347808838, "training_acc": 65.0, "val_loss": 337.50975131988525, "val_acc": 72.0}
{"epoch": 18, "training_loss": 1116.8335723876953, "training_acc": 72.0, "val_loss": 185.74001789093018, "val_acc": 72.0}
{"epoch": 19, "training_loss": 537.3104677200317, "training_acc": 64.0, "val_loss": 256.003475189209, "val_acc": 44.0}
{"epoch": 20, "training_loss": 760.808403968811, "training_acc": 49.0, "val_loss": 152.9380202293396, "val_acc": 72.0}
{"epoch": 21, "training_loss": 488.8997974395752, "training_acc": 72.0, "val_loss": 116.3652777671814, "val_acc": 76.0}
{"epoch": 22, "training_loss": 422.98792266845703, "training_acc": 71.0, "val_loss": 133.55172872543335, "val_acc": 60.0}
{"epoch": 23, "training_loss": 474.65275526046753, "training_acc": 59.0, "val_loss": 101.28931999206543, "val_acc": 80.0}
{"epoch": 24, "training_loss": 309.1827049255371, "training_acc": 80.0, "val_loss": 78.10356020927429, "val_acc": 76.0}
{"epoch": 25, "training_loss": 346.8182964324951, "training_acc": 70.0, "val_loss": 58.89338254928589, "val_acc": 64.0}
{"epoch": 26, "training_loss": 118.85991430282593, "training_acc": 80.0, "val_loss": 76.19770765304565, "val_acc": 72.0}
{"epoch": 27, "training_loss": 132.768230676651, "training_acc": 75.0, "val_loss": 149.16491508483887, "val_acc": 40.0}
{"epoch": 28, "training_loss": 286.52357006073, "training_acc": 58.0, "val_loss": 138.8307809829712, "val_acc": 72.0}
{"epoch": 29, "training_loss": 299.2583017349243, "training_acc": 73.0, "val_loss": 200.8737087249756, "val_acc": 40.0}
{"epoch": 30, "training_loss": 447.64075922966003, "training_acc": 51.0, "val_loss": 147.47930765151978, "val_acc": 72.0}
{"epoch": 31, "training_loss": 443.3254051208496, "training_acc": 72.0, "val_loss": 44.22424137592316, "val_acc": 68.0}
{"epoch": 32, "training_loss": 350.27087783813477, "training_acc": 66.0, "val_loss": 53.75133156776428, "val_acc": 64.0}
{"epoch": 33, "training_loss": 135.13901710510254, "training_acc": 82.0, "val_loss": 133.92598628997803, "val_acc": 72.0}
{"epoch": 34, "training_loss": 320.70416736602783, "training_acc": 75.0, "val_loss": 98.24549555778503, "val_acc": 64.0}
{"epoch": 35, "training_loss": 309.58458518981934, "training_acc": 63.0, "val_loss": 76.35850310325623, "val_acc": 80.0}
{"epoch": 36, "training_loss": 203.71675515174866, "training_acc": 79.0, "val_loss": 78.37642431259155, "val_acc": 76.0}
{"epoch": 37, "training_loss": 124.52374649047852, "training_acc": 82.0, "val_loss": 98.2697069644928, "val_acc": 60.0}
{"epoch": 38, "training_loss": 196.60342454910278, "training_acc": 72.0, "val_loss": 84.395831823349, "val_acc": 76.0}
{"epoch": 39, "training_loss": 144.08942079544067, "training_acc": 80.0, "val_loss": 70.76409459114075, "val_acc": 52.0}
{"epoch": 40, "training_loss": 85.19632053375244, "training_acc": 78.0, "val_loss": 96.09735608100891, "val_acc": 72.0}
{"epoch": 41, "training_loss": 155.45331501960754, "training_acc": 74.0, "val_loss": 105.09440898895264, "val_acc": 40.0}
{"epoch": 42, "training_loss": 164.60976099967957, "training_acc": 67.0, "val_loss": 83.21139812469482, "val_acc": 72.0}
{"epoch": 43, "training_loss": 111.86378860473633, "training_acc": 77.0, "val_loss": 68.98617148399353, "val_acc": 48.0}
{"epoch": 44, "training_loss": 178.75067043304443, "training_acc": 67.0, "val_loss": 63.313525915145874, "val_acc": 76.0}
{"epoch": 45, "training_loss": 108.9703049659729, "training_acc": 75.0, "val_loss": 45.86119055747986, "val_acc": 68.0}
{"epoch": 46, "training_loss": 101.7055082321167, "training_acc": 82.0, "val_loss": 50.747132301330566, "val_acc": 76.0}
{"epoch": 47, "training_loss": 72.66002988815308, "training_acc": 85.0, "val_loss": 85.53652763366699, "val_acc": 56.0}
{"epoch": 48, "training_loss": 159.60738277435303, "training_acc": 71.0, "val_loss": 122.1356987953186, "val_acc": 72.0}
{"epoch": 49, "training_loss": 216.89282178878784, "training_acc": 77.0, "val_loss": 169.3351149559021, "val_acc": 40.0}
{"epoch": 50, "training_loss": 380.54317158460617, "training_acc": 59.0, "val_loss": 121.60803079605103, "val_acc": 72.0}
