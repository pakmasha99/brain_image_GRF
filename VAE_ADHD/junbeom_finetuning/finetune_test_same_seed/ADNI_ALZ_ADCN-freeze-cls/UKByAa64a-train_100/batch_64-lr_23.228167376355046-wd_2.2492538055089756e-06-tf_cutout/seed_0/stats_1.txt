"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 15956.14344406128, "training_acc": 65.0, "val_loss": 20260.494995117188, "val_acc": 72.0}
{"epoch": 1, "training_loss": 54797.885986328125, "training_acc": 72.0, "val_loss": 25594.7265625, "val_acc": 28.0}
{"epoch": 2, "training_loss": 90991.53930664062, "training_acc": 28.0, "val_loss": 2364.338493347168, "val_acc": 68.0}
{"epoch": 3, "training_loss": 16127.476318359375, "training_acc": 75.0, "val_loss": 14015.802001953125, "val_acc": 72.0}
{"epoch": 4, "training_loss": 55146.5791015625, "training_acc": 72.0, "val_loss": 12983.930969238281, "val_acc": 72.0}
{"epoch": 5, "training_loss": 46615.41796875, "training_acc": 72.0, "val_loss": 3751.9893646240234, "val_acc": 72.0}
{"epoch": 6, "training_loss": 19854.363525390625, "training_acc": 66.0, "val_loss": 7627.721405029297, "val_acc": 56.0}
{"epoch": 7, "training_loss": 28184.068481445312, "training_acc": 52.0, "val_loss": 3371.441650390625, "val_acc": 72.0}
{"epoch": 8, "training_loss": 15683.463012695312, "training_acc": 75.0, "val_loss": 7425.823211669922, "val_acc": 72.0}
{"epoch": 9, "training_loss": 26144.03924560547, "training_acc": 72.0, "val_loss": 3135.29052734375, "val_acc": 80.0}
{"epoch": 10, "training_loss": 13217.986938476562, "training_acc": 68.0, "val_loss": 3370.139694213867, "val_acc": 56.0}
{"epoch": 11, "training_loss": 11943.496383666992, "training_acc": 65.0, "val_loss": 1782.9021453857422, "val_acc": 76.0}
{"epoch": 12, "training_loss": 5400.159133911133, "training_acc": 76.0, "val_loss": 1123.1268882751465, "val_acc": 76.0}
{"epoch": 13, "training_loss": 3894.8673248291016, "training_acc": 66.0, "val_loss": 1478.976058959961, "val_acc": 72.0}
{"epoch": 14, "training_loss": 2393.820369720459, "training_acc": 73.0, "val_loss": 1965.601921081543, "val_acc": 36.0}
{"epoch": 15, "training_loss": 4041.9352798461914, "training_acc": 63.0, "val_loss": 3857.158660888672, "val_acc": 72.0}
{"epoch": 16, "training_loss": 9357.209747314453, "training_acc": 72.0, "val_loss": 3075.347137451172, "val_acc": 32.0}
{"epoch": 17, "training_loss": 7809.432388305664, "training_acc": 46.0, "val_loss": 2127.79541015625, "val_acc": 72.0}
{"epoch": 18, "training_loss": 3684.327247619629, "training_acc": 66.0, "val_loss": 1030.9389114379883, "val_acc": 60.0}
{"epoch": 19, "training_loss": 1713.8606872558594, "training_acc": 73.0, "val_loss": 1064.0777587890625, "val_acc": 60.0}
{"epoch": 20, "training_loss": 1679.4317932128906, "training_acc": 74.0, "val_loss": 957.2910308837891, "val_acc": 60.0}
{"epoch": 21, "training_loss": 1111.4143104553223, "training_acc": 81.0, "val_loss": 925.0476837158203, "val_acc": 56.0}
{"epoch": 22, "training_loss": 1007.7576179504395, "training_acc": 82.0, "val_loss": 1625.1028060913086, "val_acc": 72.0}
{"epoch": 23, "training_loss": 2176.0718479156494, "training_acc": 70.0, "val_loss": 1067.6258087158203, "val_acc": 68.0}
{"epoch": 24, "training_loss": 1692.414192199707, "training_acc": 71.0, "val_loss": 1738.1717681884766, "val_acc": 72.0}
{"epoch": 25, "training_loss": 2810.5520725250244, "training_acc": 76.0, "val_loss": 1532.3643684387207, "val_acc": 52.0}
{"epoch": 26, "training_loss": 3032.209888458252, "training_acc": 64.0, "val_loss": 2485.116195678711, "val_acc": 72.0}
{"epoch": 27, "training_loss": 6259.106201171875, "training_acc": 75.0, "val_loss": 1770.4191207885742, "val_acc": 56.0}
{"epoch": 28, "training_loss": 7373.531188964844, "training_acc": 55.0, "val_loss": 2810.5236053466797, "val_acc": 72.0}
{"epoch": 29, "training_loss": 10079.022583007812, "training_acc": 72.0, "val_loss": 3785.5819702148438, "val_acc": 72.0}
{"epoch": 30, "training_loss": 10063.917701721191, "training_acc": 72.0, "val_loss": 3501.2718200683594, "val_acc": 36.0}
{"epoch": 31, "training_loss": 11256.563430786133, "training_acc": 50.0, "val_loss": 2163.0538940429688, "val_acc": 76.0}
{"epoch": 32, "training_loss": 7375.792022705078, "training_acc": 75.0, "val_loss": 2144.9134826660156, "val_acc": 76.0}
{"epoch": 33, "training_loss": 5147.946044921875, "training_acc": 75.0, "val_loss": 1022.2246170043945, "val_acc": 60.0}
{"epoch": 34, "training_loss": 3246.046417236328, "training_acc": 71.0, "val_loss": 1389.504337310791, "val_acc": 76.0}
{"epoch": 35, "training_loss": 2310.4302253723145, "training_acc": 78.0, "val_loss": 2293.8167572021484, "val_acc": 40.0}
{"epoch": 36, "training_loss": 4903.530990600586, "training_acc": 58.0, "val_loss": 1701.2521743774414, "val_acc": 72.0}
{"epoch": 37, "training_loss": 2794.6416244506836, "training_acc": 79.0, "val_loss": 1578.530502319336, "val_acc": 40.0}
{"epoch": 38, "training_loss": 3708.016159057617, "training_acc": 66.0, "val_loss": 3178.1898498535156, "val_acc": 72.0}
{"epoch": 39, "training_loss": 7086.955429077148, "training_acc": 73.0, "val_loss": 2109.168815612793, "val_acc": 44.0}
{"epoch": 40, "training_loss": 5980.603012084961, "training_acc": 55.0, "val_loss": 2035.5106353759766, "val_acc": 76.0}
