"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 32 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 1714.3836116790771, "training_acc": 66.0, "val_loss": 1192.6620483398438, "val_acc": 28.0}
{"epoch": 1, "training_loss": 2644.3169555664062, "training_acc": 44.0, "val_loss": 651.4082908630371, "val_acc": 72.0}
{"epoch": 2, "training_loss": 2309.3319702148438, "training_acc": 72.0, "val_loss": 148.19809198379517, "val_acc": 72.0}
{"epoch": 3, "training_loss": 996.9510955810547, "training_acc": 49.0, "val_loss": 194.5527195930481, "val_acc": 72.0}
{"epoch": 4, "training_loss": 1320.3619689941406, "training_acc": 72.0, "val_loss": 233.50820541381836, "val_acc": 72.0}
{"epoch": 5, "training_loss": 927.0175666809082, "training_acc": 59.0, "val_loss": 82.99768567085266, "val_acc": 76.0}
{"epoch": 6, "training_loss": 736.8599071502686, "training_acc": 74.0, "val_loss": 205.26721477508545, "val_acc": 72.0}
{"epoch": 7, "training_loss": 614.8543949127197, "training_acc": 64.0, "val_loss": 110.60973405838013, "val_acc": 60.0}
{"epoch": 8, "training_loss": 588.2185974121094, "training_acc": 65.0, "val_loss": 87.71255016326904, "val_acc": 68.0}
{"epoch": 9, "training_loss": 548.1188135147095, "training_acc": 54.0, "val_loss": 186.15940809249878, "val_acc": 72.0}
{"epoch": 10, "training_loss": 937.8806781768799, "training_acc": 72.0, "val_loss": 137.67797946929932, "val_acc": 72.0}
{"epoch": 11, "training_loss": 462.6811714172363, "training_acc": 56.0, "val_loss": 179.7431468963623, "val_acc": 72.0}
{"epoch": 12, "training_loss": 922.69873046875, "training_acc": 72.0, "val_loss": 218.58818531036377, "val_acc": 72.0}
{"epoch": 13, "training_loss": 534.4845676422119, "training_acc": 67.0, "val_loss": 54.86217141151428, "val_acc": 68.0}
{"epoch": 14, "training_loss": 489.88919258117676, "training_acc": 72.0, "val_loss": 55.231159925460815, "val_acc": 56.0}
{"epoch": 15, "training_loss": 168.68737936019897, "training_acc": 67.0, "val_loss": 50.400155782699585, "val_acc": 64.0}
{"epoch": 16, "training_loss": 101.78097248077393, "training_acc": 75.0, "val_loss": 68.12173128128052, "val_acc": 72.0}
{"epoch": 17, "training_loss": 114.83908081054688, "training_acc": 74.0, "val_loss": 44.02677118778229, "val_acc": 48.0}
{"epoch": 18, "training_loss": 113.29631805419922, "training_acc": 70.0, "val_loss": 120.85113525390625, "val_acc": 72.0}
{"epoch": 19, "training_loss": 304.60693359375, "training_acc": 72.0, "val_loss": 58.1537663936615, "val_acc": 60.0}
{"epoch": 20, "training_loss": 87.5044249817729, "training_acc": 80.0, "val_loss": 51.55985355377197, "val_acc": 68.0}
{"epoch": 21, "training_loss": 116.70759010314941, "training_acc": 77.0, "val_loss": 157.35970735549927, "val_acc": 72.0}
{"epoch": 22, "training_loss": 542.0641279220581, "training_acc": 69.0, "val_loss": 45.76355516910553, "val_acc": 60.0}
{"epoch": 23, "training_loss": 199.61531298048794, "training_acc": 75.0, "val_loss": 60.87421178817749, "val_acc": 60.0}
{"epoch": 24, "training_loss": 188.73170852661133, "training_acc": 73.0, "val_loss": 58.7557852268219, "val_acc": 60.0}
{"epoch": 25, "training_loss": 172.27015376091003, "training_acc": 73.0, "val_loss": 59.719860553741455, "val_acc": 72.0}
{"epoch": 26, "training_loss": 190.8284454345703, "training_acc": 68.0, "val_loss": 117.58147478103638, "val_acc": 32.0}
{"epoch": 27, "training_loss": 160.36866886168718, "training_acc": 66.0, "val_loss": 90.32694101333618, "val_acc": 72.0}
{"epoch": 28, "training_loss": 282.3956787586212, "training_acc": 62.0, "val_loss": 82.20913410186768, "val_acc": 76.0}
{"epoch": 29, "training_loss": 226.41570472717285, "training_acc": 76.0, "val_loss": 45.07507085800171, "val_acc": 76.0}
{"epoch": 30, "training_loss": 167.3502902984619, "training_acc": 80.0, "val_loss": 80.42227029800415, "val_acc": 52.0}
{"epoch": 31, "training_loss": 172.1971435546875, "training_acc": 76.0, "val_loss": 39.81239199638367, "val_acc": 72.0}
{"epoch": 32, "training_loss": 302.24147033691406, "training_acc": 60.0, "val_loss": 103.69381904602051, "val_acc": 72.0}
{"epoch": 33, "training_loss": 257.078164100647, "training_acc": 64.0, "val_loss": 52.859318256378174, "val_acc": 76.0}
{"epoch": 34, "training_loss": 127.23480606079102, "training_acc": 82.0, "val_loss": 56.36987090110779, "val_acc": 72.0}
{"epoch": 35, "training_loss": 255.69168186187744, "training_acc": 75.0, "val_loss": 50.46006441116333, "val_acc": 64.0}
{"epoch": 36, "training_loss": 92.34178379038349, "training_acc": 75.0, "val_loss": 111.81762218475342, "val_acc": 72.0}
{"epoch": 37, "training_loss": 239.52079963916913, "training_acc": 64.0, "val_loss": 165.415358543396, "val_acc": 72.0}
{"epoch": 38, "training_loss": 307.71174716949463, "training_acc": 74.0, "val_loss": 222.72686958312988, "val_acc": 28.0}
{"epoch": 39, "training_loss": 544.0254096984863, "training_acc": 61.0, "val_loss": 56.83407783508301, "val_acc": 80.0}
{"epoch": 40, "training_loss": 840.3615303039551, "training_acc": 44.0, "val_loss": 247.0886468887329, "val_acc": 72.0}
{"epoch": 41, "training_loss": 821.5416357517242, "training_acc": 72.0, "val_loss": 147.5286364555359, "val_acc": 56.0}
{"epoch": 42, "training_loss": 702.5656943321228, "training_acc": 57.0, "val_loss": 210.98554134368896, "val_acc": 76.0}
{"epoch": 43, "training_loss": 759.0137373208945, "training_acc": 75.0, "val_loss": 165.20018577575684, "val_acc": 76.0}
{"epoch": 44, "training_loss": 509.77856826782227, "training_acc": 68.0, "val_loss": 103.39031219482422, "val_acc": 64.0}
{"epoch": 45, "training_loss": 417.4441258907318, "training_acc": 75.0, "val_loss": 90.2557373046875, "val_acc": 76.0}
{"epoch": 46, "training_loss": 171.8152377953811, "training_acc": 77.0, "val_loss": 84.66723561286926, "val_acc": 76.0}
{"epoch": 47, "training_loss": 100.78105354309082, "training_acc": 78.0, "val_loss": 65.13421535491943, "val_acc": 76.0}
{"epoch": 48, "training_loss": 212.13599586486816, "training_acc": 74.0, "val_loss": 92.75398850440979, "val_acc": 40.0}
{"epoch": 49, "training_loss": 137.72441845806316, "training_acc": 76.0, "val_loss": 66.81233048439026, "val_acc": 64.0}
{"epoch": 50, "training_loss": 70.50674140382762, "training_acc": 79.0, "val_loss": 127.55407094955444, "val_acc": 76.0}
