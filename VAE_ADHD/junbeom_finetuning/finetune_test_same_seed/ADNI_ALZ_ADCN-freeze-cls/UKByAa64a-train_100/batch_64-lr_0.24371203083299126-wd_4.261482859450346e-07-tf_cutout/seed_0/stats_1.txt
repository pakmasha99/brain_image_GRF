"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 340.24055099487305, "training_acc": 70.0, "val_loss": 194.43563222885132, "val_acc": 72.0}
{"epoch": 1, "training_loss": 585.5221543312073, "training_acc": 72.0, "val_loss": 283.50372314453125, "val_acc": 28.0}
{"epoch": 2, "training_loss": 985.3427619934082, "training_acc": 28.0, "val_loss": 22.960297763347626, "val_acc": 56.0}
{"epoch": 3, "training_loss": 154.92380332946777, "training_acc": 74.0, "val_loss": 122.88562059402466, "val_acc": 72.0}
{"epoch": 4, "training_loss": 466.61442375183105, "training_acc": 72.0, "val_loss": 91.3857102394104, "val_acc": 72.0}
{"epoch": 5, "training_loss": 276.2850751876831, "training_acc": 74.0, "val_loss": 59.13147330284119, "val_acc": 56.0}
{"epoch": 6, "training_loss": 281.4677104949951, "training_acc": 55.0, "val_loss": 89.42176699638367, "val_acc": 60.0}
{"epoch": 7, "training_loss": 317.75331115722656, "training_acc": 53.0, "val_loss": 50.346940755844116, "val_acc": 76.0}
{"epoch": 8, "training_loss": 251.1184539794922, "training_acc": 73.0, "val_loss": 96.43266201019287, "val_acc": 72.0}
{"epoch": 9, "training_loss": 362.710485458374, "training_acc": 72.0, "val_loss": 53.224945068359375, "val_acc": 80.0}
{"epoch": 10, "training_loss": 163.36416673660278, "training_acc": 74.0, "val_loss": 68.78713369369507, "val_acc": 60.0}
{"epoch": 11, "training_loss": 310.0699806213379, "training_acc": 49.0, "val_loss": 41.335245966911316, "val_acc": 56.0}
{"epoch": 12, "training_loss": 128.46752643585205, "training_acc": 68.0, "val_loss": 47.858744859695435, "val_acc": 72.0}
{"epoch": 13, "training_loss": 204.05075025558472, "training_acc": 72.0, "val_loss": 41.501712799072266, "val_acc": 72.0}
{"epoch": 14, "training_loss": 155.02612042427063, "training_acc": 71.0, "val_loss": 35.81789433956146, "val_acc": 52.0}
{"epoch": 15, "training_loss": 125.1163592338562, "training_acc": 57.0, "val_loss": 14.320224523544312, "val_acc": 72.0}
{"epoch": 16, "training_loss": 73.21019005775452, "training_acc": 74.0, "val_loss": 30.50779700279236, "val_acc": 72.0}
{"epoch": 17, "training_loss": 89.6912214756012, "training_acc": 75.0, "val_loss": 27.896758913993835, "val_acc": 44.0}
{"epoch": 18, "training_loss": 95.17060852050781, "training_acc": 51.0, "val_loss": 24.625764787197113, "val_acc": 72.0}
{"epoch": 19, "training_loss": 88.86213397979736, "training_acc": 72.0, "val_loss": 22.471775114536285, "val_acc": 72.0}
{"epoch": 20, "training_loss": 68.4385392665863, "training_acc": 69.0, "val_loss": 23.27568382024765, "val_acc": 44.0}
{"epoch": 21, "training_loss": 63.07007849216461, "training_acc": 68.0, "val_loss": 32.93859362602234, "val_acc": 72.0}
{"epoch": 22, "training_loss": 97.22165417671204, "training_acc": 72.0, "val_loss": 15.44598937034607, "val_acc": 68.0}
{"epoch": 23, "training_loss": 67.48818492889404, "training_acc": 69.0, "val_loss": 17.00959801673889, "val_acc": 64.0}
{"epoch": 24, "training_loss": 51.61166203022003, "training_acc": 78.0, "val_loss": 35.522133111953735, "val_acc": 72.0}
{"epoch": 25, "training_loss": 93.14726543426514, "training_acc": 73.0, "val_loss": 16.84204488992691, "val_acc": 60.0}
{"epoch": 26, "training_loss": 57.028202295303345, "training_acc": 73.0, "val_loss": 16.734518110752106, "val_acc": 60.0}
{"epoch": 27, "training_loss": 43.89838147163391, "training_acc": 81.0, "val_loss": 25.508856773376465, "val_acc": 72.0}
{"epoch": 28, "training_loss": 57.655765533447266, "training_acc": 76.0, "val_loss": 24.096721410751343, "val_acc": 48.0}
{"epoch": 29, "training_loss": 63.024869322776794, "training_acc": 64.0, "val_loss": 20.882532000541687, "val_acc": 76.0}
{"epoch": 30, "training_loss": 50.72887063026428, "training_acc": 76.0, "val_loss": 16.793175041675568, "val_acc": 56.0}
{"epoch": 31, "training_loss": 55.2454035282135, "training_acc": 76.0, "val_loss": 19.40951943397522, "val_acc": 76.0}
{"epoch": 32, "training_loss": 53.024078130722046, "training_acc": 75.0, "val_loss": 16.669286787509918, "val_acc": 72.0}
{"epoch": 33, "training_loss": 53.18296194076538, "training_acc": 69.0, "val_loss": 15.912120044231415, "val_acc": 68.0}
{"epoch": 34, "training_loss": 40.79601836204529, "training_acc": 78.0, "val_loss": 19.453313946723938, "val_acc": 76.0}
