"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 16823.76987838745, "training_acc": 72.0, "val_loss": 8723.783111572266, "val_acc": 72.0}
{"epoch": 1, "training_loss": 25657.532836914062, "training_acc": 72.0, "val_loss": 14947.735595703125, "val_acc": 28.0}
{"epoch": 2, "training_loss": 59427.3544921875, "training_acc": 28.0, "val_loss": 1994.0185546875, "val_acc": 44.0}
{"epoch": 3, "training_loss": 14128.400695800781, "training_acc": 54.0, "val_loss": 8410.826110839844, "val_acc": 72.0}
{"epoch": 4, "training_loss": 35997.83203125, "training_acc": 72.0, "val_loss": 10567.948150634766, "val_acc": 72.0}
{"epoch": 5, "training_loss": 38746.05505371094, "training_acc": 72.0, "val_loss": 7285.015869140625, "val_acc": 72.0}
{"epoch": 6, "training_loss": 20573.01190185547, "training_acc": 72.0, "val_loss": 1906.2225341796875, "val_acc": 80.0}
{"epoch": 7, "training_loss": 15737.135986328125, "training_acc": 59.0, "val_loss": 5856.169128417969, "val_acc": 44.0}
{"epoch": 8, "training_loss": 27897.513244628906, "training_acc": 36.0, "val_loss": 2131.2124252319336, "val_acc": 80.0}
{"epoch": 9, "training_loss": 9468.975189208984, "training_acc": 70.0, "val_loss": 5772.203063964844, "val_acc": 72.0}
{"epoch": 10, "training_loss": 20319.766174316406, "training_acc": 72.0, "val_loss": 6320.770263671875, "val_acc": 72.0}
{"epoch": 11, "training_loss": 19100.285217285156, "training_acc": 72.0, "val_loss": 3284.903335571289, "val_acc": 76.0}
{"epoch": 12, "training_loss": 10541.2041015625, "training_acc": 72.0, "val_loss": 2683.667755126953, "val_acc": 56.0}
{"epoch": 13, "training_loss": 13356.747009277344, "training_acc": 55.0, "val_loss": 2005.2995681762695, "val_acc": 68.0}
{"epoch": 14, "training_loss": 7873.317184448242, "training_acc": 69.0, "val_loss": 2874.0234375, "val_acc": 72.0}
{"epoch": 15, "training_loss": 8652.551666259766, "training_acc": 74.0, "val_loss": 2600.2952575683594, "val_acc": 72.0}
{"epoch": 16, "training_loss": 5419.264999389648, "training_acc": 75.0, "val_loss": 1460.7494354248047, "val_acc": 60.0}
{"epoch": 17, "training_loss": 6157.396728515625, "training_acc": 60.0, "val_loss": 1090.279769897461, "val_acc": 68.0}
{"epoch": 18, "training_loss": 3579.665023803711, "training_acc": 74.0, "val_loss": 2127.1066665649414, "val_acc": 72.0}
{"epoch": 19, "training_loss": 5259.658248901367, "training_acc": 72.0, "val_loss": 1444.3564414978027, "val_acc": 52.0}
{"epoch": 20, "training_loss": 4085.501007080078, "training_acc": 53.0, "val_loss": 1027.9333114624023, "val_acc": 56.0}
{"epoch": 21, "training_loss": 2521.198066711426, "training_acc": 70.0, "val_loss": 608.2279682159424, "val_acc": 60.0}
{"epoch": 22, "training_loss": 1988.7191925048828, "training_acc": 67.0, "val_loss": 599.608039855957, "val_acc": 72.0}
{"epoch": 23, "training_loss": 2241.174041748047, "training_acc": 75.0, "val_loss": 839.0666007995605, "val_acc": 72.0}
{"epoch": 24, "training_loss": 1860.7827835083008, "training_acc": 81.0, "val_loss": 559.6176624298096, "val_acc": 68.0}
{"epoch": 25, "training_loss": 2024.7236251831055, "training_acc": 69.0, "val_loss": 1281.467056274414, "val_acc": 72.0}
{"epoch": 26, "training_loss": 2616.0232105255127, "training_acc": 77.0, "val_loss": 469.03648376464844, "val_acc": 72.0}
{"epoch": 27, "training_loss": 2269.1848678588867, "training_acc": 65.0, "val_loss": 890.8836364746094, "val_acc": 72.0}
{"epoch": 28, "training_loss": 2207.285270690918, "training_acc": 75.0, "val_loss": 287.7498388290405, "val_acc": 64.0}
{"epoch": 29, "training_loss": 1375.0316047668457, "training_acc": 69.0, "val_loss": 471.6707229614258, "val_acc": 72.0}
{"epoch": 30, "training_loss": 860.6356401443481, "training_acc": 79.0, "val_loss": 197.80242443084717, "val_acc": 68.0}
{"epoch": 31, "training_loss": 524.4576148986816, "training_acc": 80.0, "val_loss": 184.4132900238037, "val_acc": 64.0}
{"epoch": 32, "training_loss": 334.046875, "training_acc": 80.0, "val_loss": 569.1522598266602, "val_acc": 72.0}
{"epoch": 33, "training_loss": 760.5862483978271, "training_acc": 79.0, "val_loss": 1073.3903884887695, "val_acc": 36.0}
{"epoch": 34, "training_loss": 4325.825912475586, "training_acc": 49.0, "val_loss": 1245.4137802124023, "val_acc": 72.0}
{"epoch": 35, "training_loss": 2571.356897354126, "training_acc": 77.0, "val_loss": 403.35888862609863, "val_acc": 72.0}
{"epoch": 36, "training_loss": 1574.3489151000977, "training_acc": 68.0, "val_loss": 736.2088680267334, "val_acc": 76.0}
{"epoch": 37, "training_loss": 1890.5829620361328, "training_acc": 81.0, "val_loss": 298.4466791152954, "val_acc": 76.0}
{"epoch": 38, "training_loss": 1689.1797485351562, "training_acc": 71.0, "val_loss": 854.4339179992676, "val_acc": 72.0}
{"epoch": 39, "training_loss": 2288.2792739868164, "training_acc": 74.0, "val_loss": 341.9133424758911, "val_acc": 76.0}
{"epoch": 40, "training_loss": 2149.3002014160156, "training_acc": 67.0, "val_loss": 584.5636367797852, "val_acc": 72.0}
{"epoch": 41, "training_loss": 1971.7623138427734, "training_acc": 79.0, "val_loss": 784.374475479126, "val_acc": 72.0}
{"epoch": 42, "training_loss": 2365.166717529297, "training_acc": 65.0, "val_loss": 394.2413568496704, "val_acc": 60.0}
{"epoch": 43, "training_loss": 930.4315643310547, "training_acc": 82.0, "val_loss": 676.2596130371094, "val_acc": 68.0}
{"epoch": 44, "training_loss": 1199.9867515563965, "training_acc": 73.0, "val_loss": 470.3482151031494, "val_acc": 68.0}
{"epoch": 45, "training_loss": 908.706916809082, "training_acc": 79.0, "val_loss": 584.5921516418457, "val_acc": 68.0}
{"epoch": 46, "training_loss": 1540.5352630615234, "training_acc": 69.0, "val_loss": 864.3096923828125, "val_acc": 72.0}
{"epoch": 47, "training_loss": 1852.3991317749023, "training_acc": 76.0, "val_loss": 425.31890869140625, "val_acc": 64.0}
{"epoch": 48, "training_loss": 2110.473602294922, "training_acc": 68.0, "val_loss": 610.306978225708, "val_acc": 68.0}
{"epoch": 49, "training_loss": 1964.4784698486328, "training_acc": 76.0, "val_loss": 457.6357364654541, "val_acc": 68.0}
{"epoch": 50, "training_loss": 1376.533348083496, "training_acc": 79.0, "val_loss": 245.17276287078857, "val_acc": 68.0}
