"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 10702.008251190186, "training_acc": 72.0, "val_loss": 4714.128494262695, "val_acc": 72.0}
{"epoch": 1, "training_loss": 12840.182098388672, "training_acc": 72.0, "val_loss": 7399.955749511719, "val_acc": 28.0}
{"epoch": 2, "training_loss": 30967.164672851562, "training_acc": 28.0, "val_loss": 1349.0861892700195, "val_acc": 36.0}
{"epoch": 3, "training_loss": 8193.81655883789, "training_acc": 51.0, "val_loss": 4212.192916870117, "val_acc": 72.0}
{"epoch": 4, "training_loss": 18951.2333984375, "training_acc": 72.0, "val_loss": 5621.584701538086, "val_acc": 72.0}
{"epoch": 5, "training_loss": 22439.227996826172, "training_acc": 72.0, "val_loss": 4155.507659912109, "val_acc": 72.0}
{"epoch": 6, "training_loss": 14602.93490600586, "training_acc": 72.0, "val_loss": 902.2971153259277, "val_acc": 72.0}
{"epoch": 7, "training_loss": 7576.024383544922, "training_acc": 64.0, "val_loss": 1576.2107849121094, "val_acc": 52.0}
{"epoch": 8, "training_loss": 11840.855438232422, "training_acc": 47.0, "val_loss": 541.6640281677246, "val_acc": 80.0}
{"epoch": 9, "training_loss": 5562.714736938477, "training_acc": 71.0, "val_loss": 1460.5490684509277, "val_acc": 72.0}
{"epoch": 10, "training_loss": 6451.163146972656, "training_acc": 73.0, "val_loss": 1687.8368377685547, "val_acc": 72.0}
{"epoch": 11, "training_loss": 5367.247009277344, "training_acc": 71.0, "val_loss": 344.0782070159912, "val_acc": 80.0}
{"epoch": 12, "training_loss": 3619.7833404541016, "training_acc": 60.0, "val_loss": 483.7474822998047, "val_acc": 68.0}
{"epoch": 13, "training_loss": 2244.2462310791016, "training_acc": 68.0, "val_loss": 1171.8595504760742, "val_acc": 72.0}
{"epoch": 14, "training_loss": 4281.139892578125, "training_acc": 72.0, "val_loss": 818.1258201599121, "val_acc": 72.0}
{"epoch": 15, "training_loss": 2223.8680572509766, "training_acc": 66.0, "val_loss": 609.1435432434082, "val_acc": 56.0}
{"epoch": 16, "training_loss": 2544.59041595459, "training_acc": 59.0, "val_loss": 745.531702041626, "val_acc": 72.0}
{"epoch": 17, "training_loss": 1915.2469816207886, "training_acc": 69.0, "val_loss": 298.27232360839844, "val_acc": 52.0}
{"epoch": 18, "training_loss": 1132.6548042297363, "training_acc": 59.0, "val_loss": 208.4305763244629, "val_acc": 76.0}
{"epoch": 19, "training_loss": 1033.5326957702637, "training_acc": 71.0, "val_loss": 92.69247055053711, "val_acc": 84.0}
{"epoch": 20, "training_loss": 685.4349346160889, "training_acc": 80.0, "val_loss": 311.4051818847656, "val_acc": 72.0}
{"epoch": 21, "training_loss": 925.8726387023926, "training_acc": 81.0, "val_loss": 687.9029273986816, "val_acc": 44.0}
{"epoch": 22, "training_loss": 2115.2272548675537, "training_acc": 53.0, "val_loss": 653.2985210418701, "val_acc": 72.0}
{"epoch": 23, "training_loss": 2515.2339248657227, "training_acc": 72.0, "val_loss": 241.64159297943115, "val_acc": 72.0}
{"epoch": 24, "training_loss": 1942.870849609375, "training_acc": 60.0, "val_loss": 243.01469326019287, "val_acc": 52.0}
{"epoch": 25, "training_loss": 1520.842300415039, "training_acc": 67.0, "val_loss": 1113.506031036377, "val_acc": 72.0}
{"epoch": 26, "training_loss": 3878.064064025879, "training_acc": 72.0, "val_loss": 79.33957576751709, "val_acc": 84.0}
{"epoch": 27, "training_loss": 1730.4438934326172, "training_acc": 61.0, "val_loss": 176.20177268981934, "val_acc": 72.0}
{"epoch": 28, "training_loss": 1155.076919555664, "training_acc": 73.0, "val_loss": 717.7340030670166, "val_acc": 72.0}
{"epoch": 29, "training_loss": 2063.890480041504, "training_acc": 71.0, "val_loss": 389.2484426498413, "val_acc": 60.0}
{"epoch": 30, "training_loss": 1816.5776977539062, "training_acc": 60.0, "val_loss": 235.97891330718994, "val_acc": 76.0}
{"epoch": 31, "training_loss": 1698.2116088867188, "training_acc": 72.0, "val_loss": 544.9291706085205, "val_acc": 72.0}
{"epoch": 32, "training_loss": 1221.2133388519287, "training_acc": 78.0, "val_loss": 964.490795135498, "val_acc": 48.0}
{"epoch": 33, "training_loss": 2887.5785636901855, "training_acc": 55.0, "val_loss": 788.9745235443115, "val_acc": 72.0}
{"epoch": 34, "training_loss": 2824.6421508789062, "training_acc": 72.0, "val_loss": 728.7139892578125, "val_acc": 72.0}
{"epoch": 35, "training_loss": 1684.4868202209473, "training_acc": 74.0, "val_loss": 869.6540832519531, "val_acc": 44.0}
{"epoch": 36, "training_loss": 2054.0521850585938, "training_acc": 59.0, "val_loss": 631.9578647613525, "val_acc": 72.0}
{"epoch": 37, "training_loss": 1862.007095336914, "training_acc": 75.0, "val_loss": 171.7422604560852, "val_acc": 72.0}
{"epoch": 38, "training_loss": 971.742546081543, "training_acc": 75.0, "val_loss": 308.07766914367676, "val_acc": 64.0}
{"epoch": 39, "training_loss": 1165.2033081054688, "training_acc": 75.0, "val_loss": 583.458948135376, "val_acc": 72.0}
{"epoch": 40, "training_loss": 1450.488208770752, "training_acc": 76.0, "val_loss": 499.4643211364746, "val_acc": 56.0}
{"epoch": 41, "training_loss": 1441.7693519592285, "training_acc": 62.0, "val_loss": 368.00475120544434, "val_acc": 72.0}
{"epoch": 42, "training_loss": 1295.1938972473145, "training_acc": 77.0, "val_loss": 264.71269130706787, "val_acc": 76.0}
{"epoch": 43, "training_loss": 1073.3761520385742, "training_acc": 73.0, "val_loss": 152.72725820541382, "val_acc": 68.0}
{"epoch": 44, "training_loss": 710.6560478210449, "training_acc": 78.0, "val_loss": 272.1944808959961, "val_acc": 72.0}
{"epoch": 45, "training_loss": 486.9623107910156, "training_acc": 79.0, "val_loss": 189.36715126037598, "val_acc": 68.0}
