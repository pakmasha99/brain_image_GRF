"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 85.30697822570801, "training_acc": 59.0, "val_loss": 20.006857812404633, "val_acc": 72.0}
{"epoch": 1, "training_loss": 71.2620415687561, "training_acc": 72.0, "val_loss": 16.944164037704468, "val_acc": 28.0}
{"epoch": 2, "training_loss": 70.75672388076782, "training_acc": 51.0, "val_loss": 15.078942477703094, "val_acc": 44.0}
{"epoch": 3, "training_loss": 59.39286231994629, "training_acc": 66.0, "val_loss": 16.412445902824402, "val_acc": 72.0}
{"epoch": 4, "training_loss": 62.72526288032532, "training_acc": 72.0, "val_loss": 17.629356682300568, "val_acc": 72.0}
{"epoch": 5, "training_loss": 60.41271233558655, "training_acc": 72.0, "val_loss": 14.394727349281311, "val_acc": 68.0}
{"epoch": 6, "training_loss": 59.83066487312317, "training_acc": 70.0, "val_loss": 14.995074272155762, "val_acc": 60.0}
{"epoch": 7, "training_loss": 67.38065099716187, "training_acc": 62.0, "val_loss": 14.2890065908432, "val_acc": 80.0}
{"epoch": 8, "training_loss": 58.87908220291138, "training_acc": 72.0, "val_loss": 15.7173290848732, "val_acc": 76.0}
{"epoch": 9, "training_loss": 56.32153606414795, "training_acc": 76.0, "val_loss": 15.681824088096619, "val_acc": 76.0}
{"epoch": 10, "training_loss": 57.85833764076233, "training_acc": 76.0, "val_loss": 14.313448965549469, "val_acc": 68.0}
{"epoch": 11, "training_loss": 54.12679624557495, "training_acc": 71.0, "val_loss": 14.003752171993256, "val_acc": 80.0}
{"epoch": 12, "training_loss": 52.9353814125061, "training_acc": 74.0, "val_loss": 13.932998478412628, "val_acc": 80.0}
{"epoch": 13, "training_loss": 51.2357861995697, "training_acc": 72.0, "val_loss": 14.105355739593506, "val_acc": 68.0}
{"epoch": 14, "training_loss": 51.1033399105072, "training_acc": 74.0, "val_loss": 14.299096167087555, "val_acc": 68.0}
{"epoch": 15, "training_loss": 51.58770775794983, "training_acc": 72.0, "val_loss": 14.058905839920044, "val_acc": 68.0}
{"epoch": 16, "training_loss": 51.23188042640686, "training_acc": 75.0, "val_loss": 14.026756584644318, "val_acc": 76.0}
{"epoch": 17, "training_loss": 52.50248694419861, "training_acc": 75.0, "val_loss": 14.034533500671387, "val_acc": 76.0}
{"epoch": 18, "training_loss": 51.03973436355591, "training_acc": 81.0, "val_loss": 14.336441457271576, "val_acc": 68.0}
{"epoch": 19, "training_loss": 52.34040379524231, "training_acc": 74.0, "val_loss": 15.10612964630127, "val_acc": 72.0}
{"epoch": 20, "training_loss": 52.43621492385864, "training_acc": 72.0, "val_loss": 14.16802704334259, "val_acc": 72.0}
{"epoch": 21, "training_loss": 49.49203681945801, "training_acc": 78.0, "val_loss": 13.978452980518341, "val_acc": 76.0}
{"epoch": 22, "training_loss": 50.32919502258301, "training_acc": 77.0, "val_loss": 14.006447792053223, "val_acc": 72.0}
{"epoch": 23, "training_loss": 48.96183753013611, "training_acc": 78.0, "val_loss": 14.763282239437103, "val_acc": 68.0}
{"epoch": 24, "training_loss": 50.378378033638, "training_acc": 75.0, "val_loss": 14.730769395828247, "val_acc": 68.0}
{"epoch": 25, "training_loss": 50.16055512428284, "training_acc": 76.0, "val_loss": 14.19052928686142, "val_acc": 72.0}
{"epoch": 26, "training_loss": 49.5544798374176, "training_acc": 78.0, "val_loss": 14.012399315834045, "val_acc": 76.0}
{"epoch": 27, "training_loss": 50.05901384353638, "training_acc": 80.0, "val_loss": 14.219456911087036, "val_acc": 72.0}
{"epoch": 28, "training_loss": 47.743194222450256, "training_acc": 78.0, "val_loss": 14.840595424175262, "val_acc": 68.0}
{"epoch": 29, "training_loss": 49.67926645278931, "training_acc": 76.0, "val_loss": 14.663831889629364, "val_acc": 68.0}
{"epoch": 30, "training_loss": 49.589269161224365, "training_acc": 77.0, "val_loss": 13.90460729598999, "val_acc": 76.0}
{"epoch": 31, "training_loss": 50.16747689247131, "training_acc": 78.0, "val_loss": 13.900129497051239, "val_acc": 72.0}
{"epoch": 32, "training_loss": 49.4454482793808, "training_acc": 77.0, "val_loss": 14.67701941728592, "val_acc": 68.0}
{"epoch": 33, "training_loss": 49.33695411682129, "training_acc": 75.0, "val_loss": 14.25817608833313, "val_acc": 72.0}
{"epoch": 34, "training_loss": 47.31596052646637, "training_acc": 80.0, "val_loss": 13.94122838973999, "val_acc": 76.0}
{"epoch": 35, "training_loss": 49.86058735847473, "training_acc": 74.0, "val_loss": 14.049610495567322, "val_acc": 68.0}
{"epoch": 36, "training_loss": 47.367292284965515, "training_acc": 78.0, "val_loss": 15.03235548734665, "val_acc": 68.0}
{"epoch": 37, "training_loss": 47.787402987480164, "training_acc": 76.0, "val_loss": 14.553871750831604, "val_acc": 68.0}
{"epoch": 38, "training_loss": 48.03052854537964, "training_acc": 77.0, "val_loss": 14.04106318950653, "val_acc": 72.0}
{"epoch": 39, "training_loss": 48.0823210477829, "training_acc": 81.0, "val_loss": 14.064539968967438, "val_acc": 72.0}
{"epoch": 40, "training_loss": 46.723384857177734, "training_acc": 79.0, "val_loss": 14.636600017547607, "val_acc": 64.0}
{"epoch": 41, "training_loss": 48.0841144323349, "training_acc": 78.0, "val_loss": 14.880555868148804, "val_acc": 68.0}
{"epoch": 42, "training_loss": 47.44155180454254, "training_acc": 77.0, "val_loss": 14.202243089675903, "val_acc": 68.0}
{"epoch": 43, "training_loss": 48.03630065917969, "training_acc": 79.0, "val_loss": 14.172552525997162, "val_acc": 68.0}
{"epoch": 44, "training_loss": 46.806196808815, "training_acc": 80.0, "val_loss": 14.627979695796967, "val_acc": 68.0}
{"epoch": 45, "training_loss": 44.75662076473236, "training_acc": 77.0, "val_loss": 15.021644532680511, "val_acc": 64.0}
{"epoch": 46, "training_loss": 47.75714612007141, "training_acc": 75.0, "val_loss": 14.644147455692291, "val_acc": 68.0}
{"epoch": 47, "training_loss": 43.89386332035065, "training_acc": 80.0, "val_loss": 14.337678253650665, "val_acc": 76.0}
{"epoch": 48, "training_loss": 49.2831871509552, "training_acc": 76.0, "val_loss": 14.328017830848694, "val_acc": 68.0}
{"epoch": 49, "training_loss": 44.722445130348206, "training_acc": 82.0, "val_loss": 15.33745676279068, "val_acc": 68.0}
{"epoch": 50, "training_loss": 47.83576798439026, "training_acc": 78.0, "val_loss": 14.918474853038788, "val_acc": 68.0}
