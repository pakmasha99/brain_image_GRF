"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 5291.488792419434, "training_acc": 75.0, "val_loss": 3226.6674041748047, "val_acc": 72.0}
{"epoch": 1, "training_loss": 8655.94303894043, "training_acc": 72.0, "val_loss": 4554.831314086914, "val_acc": 28.0}
{"epoch": 2, "training_loss": 18061.955200195312, "training_acc": 28.0, "val_loss": 503.9858341217041, "val_acc": 52.0}
{"epoch": 3, "training_loss": 3886.621047973633, "training_acc": 60.0, "val_loss": 3057.7640533447266, "val_acc": 72.0}
{"epoch": 4, "training_loss": 12745.310546875, "training_acc": 72.0, "val_loss": 3826.2779235839844, "val_acc": 72.0}
{"epoch": 5, "training_loss": 14102.80419921875, "training_acc": 72.0, "val_loss": 2721.736717224121, "val_acc": 72.0}
{"epoch": 6, "training_loss": 7480.426818847656, "training_acc": 72.0, "val_loss": 690.195894241333, "val_acc": 80.0}
{"epoch": 7, "training_loss": 4956.5538330078125, "training_acc": 57.0, "val_loss": 1923.592185974121, "val_acc": 44.0}
{"epoch": 8, "training_loss": 9097.787017822266, "training_acc": 44.0, "val_loss": 769.1576480865479, "val_acc": 76.0}
{"epoch": 9, "training_loss": 3390.964065551758, "training_acc": 66.0, "val_loss": 1739.876937866211, "val_acc": 72.0}
{"epoch": 10, "training_loss": 5602.4835205078125, "training_acc": 72.0, "val_loss": 1666.7388916015625, "val_acc": 72.0}
{"epoch": 11, "training_loss": 4308.794204711914, "training_acc": 73.0, "val_loss": 726.4349937438965, "val_acc": 68.0}
{"epoch": 12, "training_loss": 2437.533058166504, "training_acc": 69.0, "val_loss": 755.3971290588379, "val_acc": 60.0}
{"epoch": 13, "training_loss": 3198.479736328125, "training_acc": 62.0, "val_loss": 630.1185131072998, "val_acc": 68.0}
{"epoch": 14, "training_loss": 1985.664566040039, "training_acc": 75.0, "val_loss": 896.9298362731934, "val_acc": 72.0}
{"epoch": 15, "training_loss": 2023.118709564209, "training_acc": 75.0, "val_loss": 388.3187770843506, "val_acc": 56.0}
{"epoch": 16, "training_loss": 1939.9884643554688, "training_acc": 56.0, "val_loss": 331.280517578125, "val_acc": 60.0}
{"epoch": 17, "training_loss": 863.5160102844238, "training_acc": 75.0, "val_loss": 555.8425903320312, "val_acc": 72.0}
{"epoch": 18, "training_loss": 1266.789442062378, "training_acc": 73.0, "val_loss": 830.9162139892578, "val_acc": 40.0}
{"epoch": 19, "training_loss": 2431.6206855773926, "training_acc": 42.0, "val_loss": 604.3609619140625, "val_acc": 72.0}
{"epoch": 20, "training_loss": 2495.6616973876953, "training_acc": 72.0, "val_loss": 806.8958282470703, "val_acc": 72.0}
{"epoch": 21, "training_loss": 2012.2100944519043, "training_acc": 73.0, "val_loss": 528.4670352935791, "val_acc": 48.0}
{"epoch": 22, "training_loss": 2592.658042907715, "training_acc": 48.0, "val_loss": 278.89864444732666, "val_acc": 72.0}
{"epoch": 23, "training_loss": 1004.293212890625, "training_acc": 79.0, "val_loss": 676.6480445861816, "val_acc": 72.0}
{"epoch": 24, "training_loss": 1747.1812191009521, "training_acc": 72.0, "val_loss": 379.31456565856934, "val_acc": 64.0}
{"epoch": 25, "training_loss": 1901.9550170898438, "training_acc": 66.0, "val_loss": 360.61511039733887, "val_acc": 80.0}
{"epoch": 26, "training_loss": 1524.5351791381836, "training_acc": 72.0, "val_loss": 763.6424541473389, "val_acc": 72.0}
{"epoch": 27, "training_loss": 1766.075210571289, "training_acc": 75.0, "val_loss": 333.721399307251, "val_acc": 72.0}
{"epoch": 28, "training_loss": 1804.546241760254, "training_acc": 63.0, "val_loss": 380.3973197937012, "val_acc": 60.0}
{"epoch": 29, "training_loss": 971.2871103286743, "training_acc": 72.0, "val_loss": 651.0731220245361, "val_acc": 72.0}
{"epoch": 30, "training_loss": 1787.3972244262695, "training_acc": 72.0, "val_loss": 421.05889320373535, "val_acc": 72.0}
{"epoch": 31, "training_loss": 900.6704292297363, "training_acc": 75.0, "val_loss": 408.89878273010254, "val_acc": 56.0}
{"epoch": 32, "training_loss": 1191.7113761901855, "training_acc": 68.0, "val_loss": 477.07786560058594, "val_acc": 72.0}
{"epoch": 33, "training_loss": 1261.4710083007812, "training_acc": 72.0, "val_loss": 234.8973512649536, "val_acc": 52.0}
{"epoch": 34, "training_loss": 1365.2426300048828, "training_acc": 58.0, "val_loss": 313.3164644241333, "val_acc": 64.0}
{"epoch": 35, "training_loss": 683.9879474639893, "training_acc": 77.0, "val_loss": 499.0061283111572, "val_acc": 72.0}
{"epoch": 36, "training_loss": 808.9742603302002, "training_acc": 77.0, "val_loss": 297.4018096923828, "val_acc": 64.0}
{"epoch": 37, "training_loss": 960.6220741271973, "training_acc": 61.0, "val_loss": 402.6066780090332, "val_acc": 68.0}
{"epoch": 38, "training_loss": 790.6297569274902, "training_acc": 79.0, "val_loss": 273.7187147140503, "val_acc": 68.0}
{"epoch": 39, "training_loss": 513.0727500915527, "training_acc": 79.0, "val_loss": 209.99410152435303, "val_acc": 64.0}
{"epoch": 40, "training_loss": 889.4958267211914, "training_acc": 71.0, "val_loss": 480.91845512390137, "val_acc": 72.0}
{"epoch": 41, "training_loss": 829.3032550811768, "training_acc": 80.0, "val_loss": 295.7712411880493, "val_acc": 60.0}
{"epoch": 42, "training_loss": 707.0922966003418, "training_acc": 60.0, "val_loss": 315.7890558242798, "val_acc": 72.0}
{"epoch": 43, "training_loss": 535.9384326934814, "training_acc": 79.0, "val_loss": 143.2214379310608, "val_acc": 64.0}
{"epoch": 44, "training_loss": 175.0703821182251, "training_acc": 85.0, "val_loss": 131.538987159729, "val_acc": 60.0}
{"epoch": 45, "training_loss": 136.6787765622139, "training_acc": 85.0, "val_loss": 143.81637573242188, "val_acc": 68.0}
{"epoch": 46, "training_loss": 126.76960372924805, "training_acc": 88.0, "val_loss": 114.00868892669678, "val_acc": 60.0}
{"epoch": 47, "training_loss": 191.98621940612793, "training_acc": 80.0, "val_loss": 470.80230712890625, "val_acc": 72.0}
{"epoch": 48, "training_loss": 1262.992919921875, "training_acc": 72.0, "val_loss": 98.85445833206177, "val_acc": 64.0}
{"epoch": 49, "training_loss": 1030.4332580566406, "training_acc": 65.0, "val_loss": 187.9150390625, "val_acc": 76.0}
{"epoch": 50, "training_loss": 659.536075592041, "training_acc": 82.0, "val_loss": 436.1997127532959, "val_acc": 72.0}
{"epoch": 51, "training_loss": 747.3773155212402, "training_acc": 75.0, "val_loss": 217.74001121520996, "val_acc": 72.0}
{"epoch": 52, "training_loss": 1107.3014183044434, "training_acc": 55.0, "val_loss": 417.4166679382324, "val_acc": 72.0}
{"epoch": 53, "training_loss": 627.8616380691528, "training_acc": 80.0, "val_loss": 243.3903455734253, "val_acc": 72.0}
{"epoch": 54, "training_loss": 681.2408475875854, "training_acc": 67.0, "val_loss": 310.59765815734863, "val_acc": 72.0}
{"epoch": 55, "training_loss": 378.4279441833496, "training_acc": 80.0, "val_loss": 233.233642578125, "val_acc": 64.0}
{"epoch": 56, "training_loss": 499.0022883415222, "training_acc": 63.0, "val_loss": 418.6302185058594, "val_acc": 72.0}
{"epoch": 57, "training_loss": 841.4419250488281, "training_acc": 76.0, "val_loss": 150.77985525131226, "val_acc": 60.0}
{"epoch": 58, "training_loss": 301.1561679840088, "training_acc": 76.0, "val_loss": 202.51574516296387, "val_acc": 64.0}
{"epoch": 59, "training_loss": 483.6127281188965, "training_acc": 82.0, "val_loss": 189.37004804611206, "val_acc": 68.0}
{"epoch": 60, "training_loss": 522.6107139587402, "training_acc": 72.0, "val_loss": 208.86476039886475, "val_acc": 72.0}
{"epoch": 61, "training_loss": 349.74268913269043, "training_acc": 80.0, "val_loss": 137.07259893417358, "val_acc": 68.0}
{"epoch": 62, "training_loss": 442.87297439575195, "training_acc": 73.0, "val_loss": 182.1805238723755, "val_acc": 68.0}
{"epoch": 63, "training_loss": 433.28612899780273, "training_acc": 85.0, "val_loss": 189.43045139312744, "val_acc": 68.0}
{"epoch": 64, "training_loss": 895.9784164428711, "training_acc": 65.0, "val_loss": 147.88808822631836, "val_acc": 68.0}
{"epoch": 65, "training_loss": 406.6395454406738, "training_acc": 86.0, "val_loss": 281.329083442688, "val_acc": 72.0}
{"epoch": 66, "training_loss": 314.32792472839355, "training_acc": 82.0, "val_loss": 262.4474763870239, "val_acc": 60.0}
{"epoch": 67, "training_loss": 935.708101272583, "training_acc": 62.0, "val_loss": 447.4853515625, "val_acc": 72.0}
