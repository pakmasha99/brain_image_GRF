"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 1647.1488189697266, "training_acc": 40.0, "val_loss": 751.7446041107178, "val_acc": 72.0}
{"epoch": 1, "training_loss": 2419.536293029785, "training_acc": 72.0, "val_loss": 222.73712158203125, "val_acc": 36.0}
{"epoch": 2, "training_loss": 1296.614688873291, "training_acc": 41.0, "val_loss": 133.7308645248413, "val_acc": 68.0}
{"epoch": 3, "training_loss": 513.5223617553711, "training_acc": 75.0, "val_loss": 269.881010055542, "val_acc": 72.0}
{"epoch": 4, "training_loss": 702.7620944976807, "training_acc": 71.0, "val_loss": 101.80842876434326, "val_acc": 64.0}
{"epoch": 5, "training_loss": 577.3465633392334, "training_acc": 58.0, "val_loss": 86.0871970653534, "val_acc": 64.0}
{"epoch": 6, "training_loss": 445.35247802734375, "training_acc": 71.0, "val_loss": 216.91641807556152, "val_acc": 72.0}
{"epoch": 7, "training_loss": 679.5392742156982, "training_acc": 72.0, "val_loss": 45.844703912734985, "val_acc": 64.0}
{"epoch": 8, "training_loss": 373.58677101135254, "training_acc": 57.0, "val_loss": 30.791670083999634, "val_acc": 64.0}
{"epoch": 9, "training_loss": 328.85941314697266, "training_acc": 71.0, "val_loss": 113.14059495925903, "val_acc": 72.0}
{"epoch": 10, "training_loss": 272.3688702583313, "training_acc": 75.0, "val_loss": 198.85371923446655, "val_acc": 28.0}
{"epoch": 11, "training_loss": 596.049928188324, "training_acc": 40.0, "val_loss": 117.3504114151001, "val_acc": 72.0}
{"epoch": 12, "training_loss": 593.8794059753418, "training_acc": 72.0, "val_loss": 154.14375066757202, "val_acc": 72.0}
{"epoch": 13, "training_loss": 457.70676159858704, "training_acc": 73.0, "val_loss": 103.00638675689697, "val_acc": 36.0}
{"epoch": 14, "training_loss": 391.3277015686035, "training_acc": 52.0, "val_loss": 115.84540605545044, "val_acc": 72.0}
{"epoch": 15, "training_loss": 426.2291030883789, "training_acc": 72.0, "val_loss": 134.1016173362732, "val_acc": 72.0}
{"epoch": 16, "training_loss": 340.56818294525146, "training_acc": 71.0, "val_loss": 100.57543516159058, "val_acc": 48.0}
{"epoch": 17, "training_loss": 455.7833080291748, "training_acc": 56.0, "val_loss": 82.30031132698059, "val_acc": 72.0}
{"epoch": 18, "training_loss": 306.68225860595703, "training_acc": 75.0, "val_loss": 139.3603801727295, "val_acc": 72.0}
{"epoch": 19, "training_loss": 317.18402004241943, "training_acc": 76.0, "val_loss": 58.22758078575134, "val_acc": 64.0}
{"epoch": 20, "training_loss": 300.52067852020264, "training_acc": 62.0, "val_loss": 53.29528450965881, "val_acc": 76.0}
{"epoch": 21, "training_loss": 160.88414764404297, "training_acc": 80.0, "val_loss": 61.357444524765015, "val_acc": 76.0}
{"epoch": 22, "training_loss": 153.69876718521118, "training_acc": 81.0, "val_loss": 36.311304569244385, "val_acc": 48.0}
{"epoch": 23, "training_loss": 163.32526874542236, "training_acc": 67.0, "val_loss": 44.42870616912842, "val_acc": 72.0}
{"epoch": 24, "training_loss": 98.52519536018372, "training_acc": 75.0, "val_loss": 54.072487354278564, "val_acc": 44.0}
{"epoch": 25, "training_loss": 215.68233013153076, "training_acc": 51.0, "val_loss": 49.54329431056976, "val_acc": 72.0}
{"epoch": 26, "training_loss": 153.4686541557312, "training_acc": 73.0, "val_loss": 30.664899945259094, "val_acc": 44.0}
{"epoch": 27, "training_loss": 109.27907299995422, "training_acc": 74.0, "val_loss": 69.37653422355652, "val_acc": 76.0}
{"epoch": 28, "training_loss": 168.8203592300415, "training_acc": 78.0, "val_loss": 27.800041437149048, "val_acc": 80.0}
{"epoch": 29, "training_loss": 138.86050510406494, "training_acc": 72.0, "val_loss": 39.50406610965729, "val_acc": 76.0}
{"epoch": 30, "training_loss": 99.7266309261322, "training_acc": 80.0, "val_loss": 28.640276193618774, "val_acc": 80.0}
{"epoch": 31, "training_loss": 72.09353542327881, "training_acc": 81.0, "val_loss": 14.774137735366821, "val_acc": 76.0}
{"epoch": 32, "training_loss": 91.78419780731201, "training_acc": 80.0, "val_loss": 17.37787574529648, "val_acc": 80.0}
{"epoch": 33, "training_loss": 199.88802909851074, "training_acc": 65.0, "val_loss": 47.891414165496826, "val_acc": 72.0}
{"epoch": 34, "training_loss": 175.54371309280396, "training_acc": 72.0, "val_loss": 18.83261203765869, "val_acc": 80.0}
{"epoch": 35, "training_loss": 101.5569715499878, "training_acc": 77.0, "val_loss": 29.275503754615784, "val_acc": 76.0}
{"epoch": 36, "training_loss": 119.68751239776611, "training_acc": 79.0, "val_loss": 23.315522074699402, "val_acc": 84.0}
{"epoch": 37, "training_loss": 115.69866752624512, "training_acc": 75.0, "val_loss": 29.361432790756226, "val_acc": 80.0}
{"epoch": 38, "training_loss": 129.9550313949585, "training_acc": 81.0, "val_loss": 63.05965185165405, "val_acc": 76.0}
{"epoch": 39, "training_loss": 141.77567791938782, "training_acc": 75.0, "val_loss": 43.787989020347595, "val_acc": 52.0}
{"epoch": 40, "training_loss": 174.19287490844727, "training_acc": 66.0, "val_loss": 59.7495973110199, "val_acc": 72.0}
{"epoch": 41, "training_loss": 138.83557498455048, "training_acc": 77.0, "val_loss": 45.842936635017395, "val_acc": 48.0}
{"epoch": 42, "training_loss": 149.3501238822937, "training_acc": 65.0, "val_loss": 44.21160817146301, "val_acc": 72.0}
{"epoch": 43, "training_loss": 88.84455180168152, "training_acc": 78.0, "val_loss": 36.61772906780243, "val_acc": 52.0}
{"epoch": 44, "training_loss": 142.93625164031982, "training_acc": 66.0, "val_loss": 36.35561764240265, "val_acc": 72.0}
{"epoch": 45, "training_loss": 131.10597324371338, "training_acc": 71.0, "val_loss": 16.273443400859833, "val_acc": 80.0}
{"epoch": 46, "training_loss": 43.94292092323303, "training_acc": 86.0, "val_loss": 15.992946922779083, "val_acc": 80.0}
{"epoch": 47, "training_loss": 39.743181109428406, "training_acc": 85.0, "val_loss": 17.301946878433228, "val_acc": 80.0}
{"epoch": 48, "training_loss": 63.85615348815918, "training_acc": 82.0, "val_loss": 71.42519354820251, "val_acc": 36.0}
{"epoch": 49, "training_loss": 159.50395768880844, "training_acc": 66.0, "val_loss": 62.72791028022766, "val_acc": 72.0}
{"epoch": 50, "training_loss": 139.0139389038086, "training_acc": 75.0, "val_loss": 70.78724503517151, "val_acc": 36.0}
