"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 3832.160820007324, "training_acc": 50.0, "val_loss": 3215.6936645507812, "val_acc": 72.0}
{"epoch": 1, "training_loss": 10082.982208251953, "training_acc": 72.0, "val_loss": 1287.7790451049805, "val_acc": 28.0}
{"epoch": 2, "training_loss": 5294.820579528809, "training_acc": 39.0, "val_loss": 724.6742248535156, "val_acc": 72.0}
{"epoch": 3, "training_loss": 3045.0493927001953, "training_acc": 72.0, "val_loss": 1134.2168807983398, "val_acc": 72.0}
{"epoch": 4, "training_loss": 3073.253433227539, "training_acc": 74.0, "val_loss": 490.1110649108887, "val_acc": 60.0}
{"epoch": 5, "training_loss": 2768.831985473633, "training_acc": 62.0, "val_loss": 579.9882888793945, "val_acc": 60.0}
{"epoch": 6, "training_loss": 2327.882423400879, "training_acc": 66.0, "val_loss": 933.8858604431152, "val_acc": 72.0}
{"epoch": 7, "training_loss": 3090.199417114258, "training_acc": 72.0, "val_loss": 838.8462066650391, "val_acc": 72.0}
{"epoch": 8, "training_loss": 2107.1951065063477, "training_acc": 71.0, "val_loss": 607.2542667388916, "val_acc": 52.0}
{"epoch": 9, "training_loss": 2859.886245727539, "training_acc": 48.0, "val_loss": 364.0373468399048, "val_acc": 68.0}
{"epoch": 10, "training_loss": 1427.0723037719727, "training_acc": 70.0, "val_loss": 493.7021255493164, "val_acc": 72.0}
{"epoch": 11, "training_loss": 1124.2931079864502, "training_acc": 74.0, "val_loss": 541.2936210632324, "val_acc": 40.0}
{"epoch": 12, "training_loss": 1627.8175773620605, "training_acc": 53.0, "val_loss": 595.2860355377197, "val_acc": 72.0}
{"epoch": 13, "training_loss": 2176.0837631225586, "training_acc": 72.0, "val_loss": 511.84115409851074, "val_acc": 72.0}
{"epoch": 14, "training_loss": 1228.913969039917, "training_acc": 71.0, "val_loss": 291.4774179458618, "val_acc": 48.0}
{"epoch": 15, "training_loss": 844.2242183685303, "training_acc": 60.0, "val_loss": 364.80727195739746, "val_acc": 72.0}
{"epoch": 16, "training_loss": 748.8424100875854, "training_acc": 73.0, "val_loss": 416.57018661499023, "val_acc": 44.0}
{"epoch": 17, "training_loss": 1291.3597249984741, "training_acc": 53.0, "val_loss": 214.2655849456787, "val_acc": 72.0}
{"epoch": 18, "training_loss": 377.6496162414551, "training_acc": 77.0, "val_loss": 165.69973230361938, "val_acc": 56.0}
{"epoch": 19, "training_loss": 385.91542959213257, "training_acc": 73.0, "val_loss": 216.6386365890503, "val_acc": 72.0}
{"epoch": 20, "training_loss": 418.24170112609863, "training_acc": 75.0, "val_loss": 91.50701761245728, "val_acc": 56.0}
{"epoch": 21, "training_loss": 581.3015518188477, "training_acc": 65.0, "val_loss": 149.31846857070923, "val_acc": 72.0}
{"epoch": 22, "training_loss": 815.297492980957, "training_acc": 60.0, "val_loss": 170.27825117111206, "val_acc": 72.0}
{"epoch": 23, "training_loss": 435.83406829833984, "training_acc": 75.0, "val_loss": 58.5796594619751, "val_acc": 76.0}
{"epoch": 24, "training_loss": 721.6056365966797, "training_acc": 66.0, "val_loss": 218.34344863891602, "val_acc": 72.0}
{"epoch": 25, "training_loss": 876.6509094238281, "training_acc": 73.0, "val_loss": 166.35515689849854, "val_acc": 76.0}
{"epoch": 26, "training_loss": 421.41785049438477, "training_acc": 78.0, "val_loss": 140.6650185585022, "val_acc": 68.0}
{"epoch": 27, "training_loss": 729.62060546875, "training_acc": 61.0, "val_loss": 364.95275497436523, "val_acc": 72.0}
{"epoch": 28, "training_loss": 862.8523268699646, "training_acc": 76.0, "val_loss": 152.61478424072266, "val_acc": 76.0}
{"epoch": 29, "training_loss": 542.4039821624756, "training_acc": 74.0, "val_loss": 163.03304433822632, "val_acc": 68.0}
{"epoch": 30, "training_loss": 376.4427328109741, "training_acc": 79.0, "val_loss": 88.18615674972534, "val_acc": 64.0}
{"epoch": 31, "training_loss": 312.40710639953613, "training_acc": 70.0, "val_loss": 342.63997077941895, "val_acc": 72.0}
{"epoch": 32, "training_loss": 1217.9760208129883, "training_acc": 72.0, "val_loss": 151.33819580078125, "val_acc": 56.0}
{"epoch": 33, "training_loss": 2191.6399993896484, "training_acc": 44.0, "val_loss": 256.8291187286377, "val_acc": 56.0}
{"epoch": 34, "training_loss": 990.5813102722168, "training_acc": 65.0, "val_loss": 1167.528247833252, "val_acc": 72.0}
{"epoch": 35, "training_loss": 4601.887222290039, "training_acc": 72.0, "val_loss": 1127.081298828125, "val_acc": 72.0}
{"epoch": 36, "training_loss": 3062.1972427368164, "training_acc": 72.0, "val_loss": 269.4934606552124, "val_acc": 64.0}
{"epoch": 37, "training_loss": 1869.830078125, "training_acc": 53.0, "val_loss": 309.05582904815674, "val_acc": 68.0}
{"epoch": 38, "training_loss": 1047.503656387329, "training_acc": 70.0, "val_loss": 680.424690246582, "val_acc": 72.0}
{"epoch": 39, "training_loss": 1863.984275817871, "training_acc": 75.0, "val_loss": 447.48873710632324, "val_acc": 72.0}
{"epoch": 40, "training_loss": 940.6850891113281, "training_acc": 76.0, "val_loss": 311.83276176452637, "val_acc": 68.0}
{"epoch": 41, "training_loss": 927.783182144165, "training_acc": 66.0, "val_loss": 325.63791275024414, "val_acc": 72.0}
{"epoch": 42, "training_loss": 597.9848041534424, "training_acc": 77.0, "val_loss": 217.81911849975586, "val_acc": 72.0}
