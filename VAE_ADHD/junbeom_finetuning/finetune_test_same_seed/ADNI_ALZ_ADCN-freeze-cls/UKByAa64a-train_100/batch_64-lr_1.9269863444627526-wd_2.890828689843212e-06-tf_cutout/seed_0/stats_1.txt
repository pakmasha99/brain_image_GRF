"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 6632.573486328125, "training_acc": 72.0, "val_loss": 4758.536529541016, "val_acc": 72.0}
{"epoch": 1, "training_loss": 14512.930404663086, "training_acc": 72.0, "val_loss": 6282.169723510742, "val_acc": 28.0}
{"epoch": 2, "training_loss": 22372.288024902344, "training_acc": 28.0, "val_loss": 705.7837963104248, "val_acc": 72.0}
{"epoch": 3, "training_loss": 5382.033874511719, "training_acc": 72.0, "val_loss": 2369.093704223633, "val_acc": 72.0}
{"epoch": 4, "training_loss": 8893.59683227539, "training_acc": 72.0, "val_loss": 999.3635177612305, "val_acc": 72.0}
{"epoch": 5, "training_loss": 5005.889694213867, "training_acc": 52.0, "val_loss": 754.7277450561523, "val_acc": 28.0}
{"epoch": 6, "training_loss": 2831.0141372680664, "training_acc": 50.0, "val_loss": 1905.838394165039, "val_acc": 72.0}
{"epoch": 7, "training_loss": 7979.355895996094, "training_acc": 72.0, "val_loss": 2023.988151550293, "val_acc": 72.0}
{"epoch": 8, "training_loss": 7125.8201904296875, "training_acc": 72.0, "val_loss": 418.86210441589355, "val_acc": 72.0}
{"epoch": 9, "training_loss": 5248.126983642578, "training_acc": 52.0, "val_loss": 2692.410087585449, "val_acc": 28.0}
{"epoch": 10, "training_loss": 7062.209482192993, "training_acc": 28.0, "val_loss": 1785.5772018432617, "val_acc": 72.0}
{"epoch": 11, "training_loss": 8952.885345458984, "training_acc": 72.0, "val_loss": 3837.253189086914, "val_acc": 72.0}
{"epoch": 12, "training_loss": 16136.622009277344, "training_acc": 72.0, "val_loss": 4054.4174194335938, "val_acc": 72.0}
{"epoch": 13, "training_loss": 15521.255493164062, "training_acc": 72.0, "val_loss": 2608.6374282836914, "val_acc": 72.0}
{"epoch": 14, "training_loss": 9037.893508911133, "training_acc": 72.0, "val_loss": 17.14087575674057, "val_acc": 64.0}
{"epoch": 15, "training_loss": 4732.946342468262, "training_acc": 55.0, "val_loss": 2642.2760009765625, "val_acc": 28.0}
{"epoch": 16, "training_loss": 7307.603102684021, "training_acc": 38.0, "val_loss": 581.0554504394531, "val_acc": 72.0}
{"epoch": 17, "training_loss": 2448.6168518066406, "training_acc": 72.0, "val_loss": 392.842698097229, "val_acc": 72.0}
{"epoch": 18, "training_loss": 1679.7252578735352, "training_acc": 58.0, "val_loss": 149.37430620193481, "val_acc": 72.0}
{"epoch": 19, "training_loss": 672.7822608947754, "training_acc": 72.0, "val_loss": 667.5347328186035, "val_acc": 28.0}
{"epoch": 20, "training_loss": 2122.931896209717, "training_acc": 42.0, "val_loss": 342.31176376342773, "val_acc": 72.0}
{"epoch": 21, "training_loss": 955.3275918960571, "training_acc": 74.0, "val_loss": 1210.1157188415527, "val_acc": 28.0}
{"epoch": 22, "training_loss": 3093.3322801589966, "training_acc": 28.0, "val_loss": 1258.3081245422363, "val_acc": 72.0}
{"epoch": 23, "training_loss": 6496.2886962890625, "training_acc": 72.0, "val_loss": 2485.1425170898438, "val_acc": 72.0}
{"epoch": 24, "training_loss": 9888.893524169922, "training_acc": 72.0, "val_loss": 2041.4791107177734, "val_acc": 72.0}
{"epoch": 25, "training_loss": 6922.857971191406, "training_acc": 72.0, "val_loss": 249.85458850860596, "val_acc": 72.0}
{"epoch": 26, "training_loss": 4889.462158203125, "training_acc": 54.0, "val_loss": 3427.4139404296875, "val_acc": 28.0}
{"epoch": 27, "training_loss": 11002.885192871094, "training_acc": 28.0, "val_loss": 889.0326499938965, "val_acc": 72.0}
{"epoch": 28, "training_loss": 4074.5762100219727, "training_acc": 72.0, "val_loss": 2553.765106201172, "val_acc": 72.0}
{"epoch": 29, "training_loss": 10607.328979492188, "training_acc": 72.0, "val_loss": 2745.4345703125, "val_acc": 72.0}
{"epoch": 30, "training_loss": 10388.013488769531, "training_acc": 72.0, "val_loss": 1517.8629875183105, "val_acc": 72.0}
{"epoch": 31, "training_loss": 4573.011207580566, "training_acc": 72.0, "val_loss": 2014.1996383666992, "val_acc": 28.0}
{"epoch": 32, "training_loss": 9193.428619384766, "training_acc": 28.0, "val_loss": 740.8881664276123, "val_acc": 28.0}
{"epoch": 33, "training_loss": 3458.250457763672, "training_acc": 46.0, "val_loss": 2372.09415435791, "val_acc": 72.0}
