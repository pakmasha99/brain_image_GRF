"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 3482.0402183532715, "training_acc": 50.0, "val_loss": 2175.1571655273438, "val_acc": 72.0}
{"epoch": 1, "training_loss": 6402.210678100586, "training_acc": 72.0, "val_loss": 1891.4386749267578, "val_acc": 28.0}
{"epoch": 2, "training_loss": 8655.52407836914, "training_acc": 29.0, "val_loss": 382.88414478302, "val_acc": 72.0}
{"epoch": 3, "training_loss": 1538.6327362060547, "training_acc": 76.0, "val_loss": 1475.6330490112305, "val_acc": 72.0}
{"epoch": 4, "training_loss": 4869.732307434082, "training_acc": 72.0, "val_loss": 1235.4838371276855, "val_acc": 72.0}
{"epoch": 5, "training_loss": 2881.175567626953, "training_acc": 72.0, "val_loss": 566.6237831115723, "val_acc": 68.0}
{"epoch": 6, "training_loss": 1838.155906677246, "training_acc": 66.0, "val_loss": 621.8234539031982, "val_acc": 60.0}
{"epoch": 7, "training_loss": 2378.005661010742, "training_acc": 61.0, "val_loss": 640.1596069335938, "val_acc": 76.0}
{"epoch": 8, "training_loss": 1742.7118072509766, "training_acc": 72.0, "val_loss": 833.3959579467773, "val_acc": 72.0}
{"epoch": 9, "training_loss": 1535.9247550964355, "training_acc": 74.0, "val_loss": 434.5433712005615, "val_acc": 60.0}
{"epoch": 10, "training_loss": 1765.280990600586, "training_acc": 56.0, "val_loss": 375.34544467926025, "val_acc": 68.0}
{"epoch": 11, "training_loss": 901.3513870239258, "training_acc": 71.0, "val_loss": 707.6767444610596, "val_acc": 72.0}
{"epoch": 12, "training_loss": 2024.425521850586, "training_acc": 72.0, "val_loss": 228.86865139007568, "val_acc": 64.0}
{"epoch": 13, "training_loss": 774.4284477233887, "training_acc": 67.0, "val_loss": 217.69888401031494, "val_acc": 64.0}
{"epoch": 14, "training_loss": 987.4388103485107, "training_acc": 58.0, "val_loss": 264.7534132003784, "val_acc": 72.0}
{"epoch": 15, "training_loss": 711.1428737640381, "training_acc": 62.0, "val_loss": 154.000723361969, "val_acc": 44.0}
{"epoch": 16, "training_loss": 681.2708225250244, "training_acc": 59.0, "val_loss": 210.06875038146973, "val_acc": 72.0}
{"epoch": 17, "training_loss": 639.6924028396606, "training_acc": 76.0, "val_loss": 581.8094253540039, "val_acc": 28.0}
{"epoch": 18, "training_loss": 1494.1975107192993, "training_acc": 49.0, "val_loss": 225.9746551513672, "val_acc": 72.0}
{"epoch": 19, "training_loss": 761.333740234375, "training_acc": 72.0, "val_loss": 147.7452516555786, "val_acc": 52.0}
{"epoch": 20, "training_loss": 563.8178768157959, "training_acc": 61.0, "val_loss": 252.3054838180542, "val_acc": 72.0}
{"epoch": 21, "training_loss": 980.7890777587891, "training_acc": 74.0, "val_loss": 226.2770652770996, "val_acc": 76.0}
{"epoch": 22, "training_loss": 705.6672554016113, "training_acc": 68.0, "val_loss": 161.07428073883057, "val_acc": 64.0}
{"epoch": 23, "training_loss": 656.6931819915771, "training_acc": 67.0, "val_loss": 171.1173176765442, "val_acc": 72.0}
{"epoch": 24, "training_loss": 541.4912033081055, "training_acc": 73.0, "val_loss": 61.844402551651, "val_acc": 80.0}
{"epoch": 25, "training_loss": 227.25508403778076, "training_acc": 75.0, "val_loss": 82.74198174476624, "val_acc": 60.0}
{"epoch": 26, "training_loss": 205.73413753509521, "training_acc": 76.0, "val_loss": 133.24073553085327, "val_acc": 68.0}
{"epoch": 27, "training_loss": 384.7776222229004, "training_acc": 64.0, "val_loss": 160.80608367919922, "val_acc": 68.0}
{"epoch": 28, "training_loss": 636.3109073638916, "training_acc": 72.0, "val_loss": 139.95596170425415, "val_acc": 48.0}
{"epoch": 29, "training_loss": 288.2440057992935, "training_acc": 64.0, "val_loss": 153.37704420089722, "val_acc": 76.0}
{"epoch": 30, "training_loss": 301.4156484603882, "training_acc": 77.0, "val_loss": 102.07576751708984, "val_acc": 60.0}
{"epoch": 31, "training_loss": 539.3894672393799, "training_acc": 66.0, "val_loss": 138.01941871643066, "val_acc": 76.0}
{"epoch": 32, "training_loss": 165.89180994033813, "training_acc": 80.0, "val_loss": 81.2758207321167, "val_acc": 68.0}
{"epoch": 33, "training_loss": 117.99742221832275, "training_acc": 82.0, "val_loss": 122.93689250946045, "val_acc": 76.0}
{"epoch": 34, "training_loss": 291.9123706817627, "training_acc": 70.0, "val_loss": 83.29220414161682, "val_acc": 72.0}
{"epoch": 35, "training_loss": 123.7521595954895, "training_acc": 81.0, "val_loss": 202.4717092514038, "val_acc": 32.0}
{"epoch": 36, "training_loss": 708.0492630004883, "training_acc": 56.0, "val_loss": 312.2706174850464, "val_acc": 72.0}
{"epoch": 37, "training_loss": 854.6574850082397, "training_acc": 74.0, "val_loss": 387.487530708313, "val_acc": 36.0}
{"epoch": 38, "training_loss": 1097.4075288772583, "training_acc": 50.0, "val_loss": 400.6282329559326, "val_acc": 72.0}
{"epoch": 39, "training_loss": 1276.847599029541, "training_acc": 72.0, "val_loss": 326.9420623779297, "val_acc": 72.0}
{"epoch": 40, "training_loss": 653.2110643386841, "training_acc": 77.0, "val_loss": 168.05237531661987, "val_acc": 64.0}
{"epoch": 41, "training_loss": 692.2439632415771, "training_acc": 62.0, "val_loss": 237.0448350906372, "val_acc": 72.0}
{"epoch": 42, "training_loss": 550.3444709777832, "training_acc": 78.0, "val_loss": 66.57459735870361, "val_acc": 84.0}
{"epoch": 43, "training_loss": 493.1206512451172, "training_acc": 66.0, "val_loss": 136.6773247718811, "val_acc": 72.0}
