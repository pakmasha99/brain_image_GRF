"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 5656.907684326172, "training_acc": 72.0, "val_loss": 211.88039779663086, "val_acc": 28.0}
{"epoch": 1, "training_loss": 6961.902893066406, "training_acc": 40.0, "val_loss": 2075.0640869140625, "val_acc": 72.0}
{"epoch": 2, "training_loss": 5679.672836303711, "training_acc": 72.0, "val_loss": 2675.21915435791, "val_acc": 28.0}
{"epoch": 3, "training_loss": 9014.51350402832, "training_acc": 40.0, "val_loss": 462.2922420501709, "val_acc": 72.0}
{"epoch": 4, "training_loss": 2960.6892700195312, "training_acc": 54.0, "val_loss": 1103.0749320983887, "val_acc": 72.0}
{"epoch": 5, "training_loss": 3854.197463989258, "training_acc": 72.0, "val_loss": 17.355646193027496, "val_acc": 72.0}
{"epoch": 6, "training_loss": 2977.305450439453, "training_acc": 44.0, "val_loss": 942.6679611206055, "val_acc": 72.0}
{"epoch": 7, "training_loss": 3354.828453063965, "training_acc": 72.0, "val_loss": 58.78356099128723, "val_acc": 72.0}
{"epoch": 8, "training_loss": 2557.6141052246094, "training_acc": 52.0, "val_loss": 629.931640625, "val_acc": 72.0}
{"epoch": 9, "training_loss": 2114.512195587158, "training_acc": 72.0, "val_loss": 469.99573707580566, "val_acc": 28.0}
{"epoch": 10, "training_loss": 1913.3239822387695, "training_acc": 50.0, "val_loss": 605.5422306060791, "val_acc": 72.0}
{"epoch": 11, "training_loss": 1716.3832216262817, "training_acc": 72.0, "val_loss": 1137.0980262756348, "val_acc": 28.0}
{"epoch": 12, "training_loss": 3772.927116394043, "training_acc": 44.0, "val_loss": 331.15484714508057, "val_acc": 72.0}
{"epoch": 13, "training_loss": 2393.9022674560547, "training_acc": 46.0, "val_loss": 881.9778442382812, "val_acc": 72.0}
{"epoch": 14, "training_loss": 3153.0296936035156, "training_acc": 72.0, "val_loss": 115.05817174911499, "val_acc": 72.0}
{"epoch": 15, "training_loss": 2117.3959197998047, "training_acc": 52.0, "val_loss": 611.277437210083, "val_acc": 72.0}
{"epoch": 16, "training_loss": 2179.696409225464, "training_acc": 72.0, "val_loss": 116.09525680541992, "val_acc": 28.0}
{"epoch": 17, "training_loss": 1451.5671691894531, "training_acc": 46.0, "val_loss": 652.2367000579834, "val_acc": 72.0}
{"epoch": 18, "training_loss": 1893.899326324463, "training_acc": 72.0, "val_loss": 910.6263160705566, "val_acc": 28.0}
{"epoch": 19, "training_loss": 2873.7233505249023, "training_acc": 48.0, "val_loss": 375.2103090286255, "val_acc": 72.0}
{"epoch": 20, "training_loss": 1025.9276218414307, "training_acc": 60.0, "val_loss": 828.0325889587402, "val_acc": 72.0}
{"epoch": 21, "training_loss": 2866.172637939453, "training_acc": 72.0, "val_loss": 15.232418477535248, "val_acc": 72.0}
{"epoch": 22, "training_loss": 843.3990173339844, "training_acc": 58.0, "val_loss": 748.9465713500977, "val_acc": 72.0}
{"epoch": 23, "training_loss": 2599.8181343078613, "training_acc": 72.0, "val_loss": 32.79650807380676, "val_acc": 28.0}
{"epoch": 24, "training_loss": 1087.2150497436523, "training_acc": 44.0, "val_loss": 375.56681632995605, "val_acc": 72.0}
{"epoch": 25, "training_loss": 1584.1312446594238, "training_acc": 54.0, "val_loss": 850.2605438232422, "val_acc": 72.0}
{"epoch": 26, "training_loss": 2999.0315170288086, "training_acc": 72.0, "val_loss": 77.7411937713623, "val_acc": 72.0}
{"epoch": 27, "training_loss": 1928.9788970947266, "training_acc": 54.0, "val_loss": 578.5727024078369, "val_acc": 72.0}
{"epoch": 28, "training_loss": 1951.0319213867188, "training_acc": 72.0, "val_loss": 528.4822940826416, "val_acc": 28.0}
{"epoch": 29, "training_loss": 2235.9440307617188, "training_acc": 46.0, "val_loss": 466.0917282104492, "val_acc": 72.0}
{"epoch": 30, "training_loss": 1596.2332229614258, "training_acc": 52.0, "val_loss": 951.6751289367676, "val_acc": 72.0}
{"epoch": 31, "training_loss": 3449.89347076416, "training_acc": 72.0, "val_loss": 204.42473888397217, "val_acc": 72.0}
{"epoch": 32, "training_loss": 1754.9783172607422, "training_acc": 54.0, "val_loss": 692.6385879516602, "val_acc": 72.0}
{"epoch": 33, "training_loss": 2390.0170936584473, "training_acc": 72.0, "val_loss": 119.14893388748169, "val_acc": 28.0}
{"epoch": 34, "training_loss": 1754.4245910644531, "training_acc": 42.0, "val_loss": 589.5758152008057, "val_acc": 72.0}
{"epoch": 35, "training_loss": 1560.7895164489746, "training_acc": 72.0, "val_loss": 1156.9202423095703, "val_acc": 28.0}
{"epoch": 36, "training_loss": 3678.3339920043945, "training_acc": 44.0, "val_loss": 246.96435928344727, "val_acc": 72.0}
{"epoch": 37, "training_loss": 1465.585319519043, "training_acc": 56.0, "val_loss": 737.9923343658447, "val_acc": 72.0}
{"epoch": 38, "training_loss": 2550.106330871582, "training_acc": 72.0, "val_loss": 194.94880437850952, "val_acc": 28.0}
{"epoch": 39, "training_loss": 1103.1436309814453, "training_acc": 52.0, "val_loss": 612.9190444946289, "val_acc": 72.0}
{"epoch": 40, "training_loss": 1734.8027973175049, "training_acc": 72.0, "val_loss": 974.3612289428711, "val_acc": 28.0}
