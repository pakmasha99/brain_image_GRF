"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 32 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 2671.7065982818604, "training_acc": 72.0, "val_loss": 560.1175308227539, "val_acc": 28.0}
{"epoch": 1, "training_loss": 1984.3004302978516, "training_acc": 38.0, "val_loss": 448.81982803344727, "val_acc": 72.0}
{"epoch": 2, "training_loss": 1517.2683262825012, "training_acc": 72.0, "val_loss": 144.6128487586975, "val_acc": 60.0}
{"epoch": 3, "training_loss": 1326.4760932922363, "training_acc": 48.0, "val_loss": 300.23837089538574, "val_acc": 72.0}
{"epoch": 4, "training_loss": 1481.0136413574219, "training_acc": 72.0, "val_loss": 534.5455169677734, "val_acc": 72.0}
{"epoch": 5, "training_loss": 1348.2828493118286, "training_acc": 73.0, "val_loss": 236.53991222381592, "val_acc": 48.0}
{"epoch": 6, "training_loss": 1529.4353866577148, "training_acc": 45.0, "val_loss": 265.0901794433594, "val_acc": 76.0}
{"epoch": 7, "training_loss": 1691.2424926757812, "training_acc": 72.0, "val_loss": 561.8785858154297, "val_acc": 72.0}
{"epoch": 8, "training_loss": 1341.5955047607422, "training_acc": 70.0, "val_loss": 186.45507097244263, "val_acc": 68.0}
{"epoch": 9, "training_loss": 677.874080657959, "training_acc": 70.0, "val_loss": 322.24676609039307, "val_acc": 72.0}
{"epoch": 10, "training_loss": 1174.6168518066406, "training_acc": 72.0, "val_loss": 196.41121625900269, "val_acc": 72.0}
{"epoch": 11, "training_loss": 321.9912748336792, "training_acc": 73.0, "val_loss": 176.39058828353882, "val_acc": 52.0}
{"epoch": 12, "training_loss": 452.1977958679199, "training_acc": 65.0, "val_loss": 217.65649318695068, "val_acc": 72.0}
{"epoch": 13, "training_loss": 450.0327968597412, "training_acc": 70.0, "val_loss": 164.32939767837524, "val_acc": 52.0}
{"epoch": 14, "training_loss": 367.2807083129883, "training_acc": 62.0, "val_loss": 73.67172241210938, "val_acc": 68.0}
{"epoch": 15, "training_loss": 211.8167932778597, "training_acc": 75.0, "val_loss": 106.41392469406128, "val_acc": 72.0}
{"epoch": 16, "training_loss": 204.74915313720703, "training_acc": 74.0, "val_loss": 123.51582050323486, "val_acc": 72.0}
{"epoch": 17, "training_loss": 478.05062964823446, "training_acc": 72.0, "val_loss": 54.03103232383728, "val_acc": 68.0}
{"epoch": 18, "training_loss": 210.10881233215332, "training_acc": 64.0, "val_loss": 102.1366834640503, "val_acc": 72.0}
{"epoch": 19, "training_loss": 134.27262842655182, "training_acc": 74.0, "val_loss": 58.23037624359131, "val_acc": 72.0}
{"epoch": 20, "training_loss": 143.33863830566406, "training_acc": 82.0, "val_loss": 53.49541902542114, "val_acc": 68.0}
{"epoch": 21, "training_loss": 101.19700662046671, "training_acc": 86.0, "val_loss": 72.72204160690308, "val_acc": 56.0}
{"epoch": 22, "training_loss": 138.24051651742775, "training_acc": 74.0, "val_loss": 57.42204785346985, "val_acc": 72.0}
{"epoch": 23, "training_loss": 65.49661654233932, "training_acc": 81.0, "val_loss": 36.160627007484436, "val_acc": 72.0}
{"epoch": 24, "training_loss": 52.63077135384083, "training_acc": 88.0, "val_loss": 26.955658197402954, "val_acc": 72.0}
{"epoch": 25, "training_loss": 36.39191269874573, "training_acc": 90.0, "val_loss": 99.77761507034302, "val_acc": 40.0}
{"epoch": 26, "training_loss": 312.44344091415405, "training_acc": 58.0, "val_loss": 206.38651847839355, "val_acc": 32.0}
{"epoch": 27, "training_loss": 630.1793441772461, "training_acc": 48.0, "val_loss": 275.2384662628174, "val_acc": 72.0}
{"epoch": 28, "training_loss": 650.9735746383667, "training_acc": 77.0, "val_loss": 76.7300546169281, "val_acc": 52.0}
{"epoch": 29, "training_loss": 581.872444152832, "training_acc": 63.0, "val_loss": 237.6401662826538, "val_acc": 72.0}
{"epoch": 30, "training_loss": 566.967776298523, "training_acc": 75.0, "val_loss": 118.03425550460815, "val_acc": 76.0}
{"epoch": 31, "training_loss": 395.6909484863281, "training_acc": 76.0, "val_loss": 170.97578048706055, "val_acc": 76.0}
{"epoch": 32, "training_loss": 368.1219673156738, "training_acc": 73.0, "val_loss": 73.06933999061584, "val_acc": 68.0}
{"epoch": 33, "training_loss": 481.72386741638184, "training_acc": 73.0, "val_loss": 86.74543499946594, "val_acc": 72.0}
{"epoch": 34, "training_loss": 475.5908579826355, "training_acc": 50.0, "val_loss": 173.09118509292603, "val_acc": 72.0}
{"epoch": 35, "training_loss": 399.4807004928589, "training_acc": 76.0, "val_loss": 119.94050741195679, "val_acc": 48.0}
{"epoch": 36, "training_loss": 400.34812927246094, "training_acc": 60.0, "val_loss": 72.40506410598755, "val_acc": 72.0}
{"epoch": 37, "training_loss": 198.07170486450195, "training_acc": 77.0, "val_loss": 102.99943685531616, "val_acc": 76.0}
{"epoch": 38, "training_loss": 125.35993127839174, "training_acc": 83.0, "val_loss": 53.26685309410095, "val_acc": 56.0}
{"epoch": 39, "training_loss": 113.81260108947754, "training_acc": 80.0, "val_loss": 109.07440185546875, "val_acc": 64.0}
{"epoch": 40, "training_loss": 110.98765826225281, "training_acc": 79.0, "val_loss": 152.084219455719, "val_acc": 52.0}
{"epoch": 41, "training_loss": 328.7000856399536, "training_acc": 68.0, "val_loss": 71.64098024368286, "val_acc": 72.0}
{"epoch": 42, "training_loss": 174.29376029968262, "training_acc": 70.0, "val_loss": 69.53039169311523, "val_acc": 80.0}
{"epoch": 43, "training_loss": 229.1780590415001, "training_acc": 71.0, "val_loss": 208.5207462310791, "val_acc": 72.0}
