"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 32 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 2283.781057357788, "training_acc": 67.0, "val_loss": 1062.8271102905273, "val_acc": 28.0}
{"epoch": 1, "training_loss": 2813.731674194336, "training_acc": 40.0, "val_loss": 568.6503887176514, "val_acc": 72.0}
{"epoch": 2, "training_loss": 2128.6877632141113, "training_acc": 72.0, "val_loss": 138.23028802871704, "val_acc": 72.0}
{"epoch": 3, "training_loss": 904.358455657959, "training_acc": 60.0, "val_loss": 131.31126165390015, "val_acc": 76.0}
{"epoch": 4, "training_loss": 935.9727172851562, "training_acc": 70.0, "val_loss": 115.45647382736206, "val_acc": 76.0}
{"epoch": 5, "training_loss": 589.5293817520142, "training_acc": 55.0, "val_loss": 81.10325932502747, "val_acc": 56.0}
{"epoch": 6, "training_loss": 266.6622292601387, "training_acc": 72.0, "val_loss": 105.8883786201477, "val_acc": 72.0}
{"epoch": 7, "training_loss": 275.96607398986816, "training_acc": 54.0, "val_loss": 198.25948476791382, "val_acc": 72.0}
{"epoch": 8, "training_loss": 439.7958450317383, "training_acc": 73.0, "val_loss": 138.33309412002563, "val_acc": 36.0}
{"epoch": 9, "training_loss": 554.9938430786133, "training_acc": 64.0, "val_loss": 198.96291494369507, "val_acc": 72.0}
{"epoch": 10, "training_loss": 540.6039876937866, "training_acc": 60.0, "val_loss": 114.17653560638428, "val_acc": 56.0}
{"epoch": 11, "training_loss": 212.1525036773237, "training_acc": 74.0, "val_loss": 100.52887201309204, "val_acc": 76.0}
{"epoch": 12, "training_loss": 171.68250513076782, "training_acc": 76.0, "val_loss": 44.06779408454895, "val_acc": 64.0}
{"epoch": 13, "training_loss": 189.43028926849365, "training_acc": 74.0, "val_loss": 45.845815539360046, "val_acc": 64.0}
{"epoch": 14, "training_loss": 103.59226036071777, "training_acc": 71.0, "val_loss": 133.76619815826416, "val_acc": 72.0}
{"epoch": 15, "training_loss": 348.957067489624, "training_acc": 71.0, "val_loss": 47.49849736690521, "val_acc": 76.0}
{"epoch": 16, "training_loss": 151.17851546406746, "training_acc": 76.0, "val_loss": 70.93822360038757, "val_acc": 56.0}
{"epoch": 17, "training_loss": 264.44978964328766, "training_acc": 67.0, "val_loss": 64.76547122001648, "val_acc": 76.0}
{"epoch": 18, "training_loss": 126.43731641769409, "training_acc": 75.0, "val_loss": 132.1044921875, "val_acc": 72.0}
{"epoch": 19, "training_loss": 338.18044662475586, "training_acc": 70.0, "val_loss": 60.68928241729736, "val_acc": 40.0}
{"epoch": 20, "training_loss": 564.4697723388672, "training_acc": 69.0, "val_loss": 75.3214418888092, "val_acc": 76.0}
{"epoch": 21, "training_loss": 722.8588333129883, "training_acc": 52.0, "val_loss": 231.96067810058594, "val_acc": 72.0}
{"epoch": 22, "training_loss": 981.6911239624023, "training_acc": 72.0, "val_loss": 182.9114317893982, "val_acc": 80.0}
{"epoch": 23, "training_loss": 588.2117233276367, "training_acc": 70.0, "val_loss": 211.71879768371582, "val_acc": 60.0}
{"epoch": 24, "training_loss": 607.1226949691772, "training_acc": 71.0, "val_loss": 162.11438179016113, "val_acc": 76.0}
{"epoch": 25, "training_loss": 401.8180923461914, "training_acc": 69.0, "val_loss": 65.81268906593323, "val_acc": 80.0}
{"epoch": 26, "training_loss": 264.1137523651123, "training_acc": 74.0, "val_loss": 272.6686954498291, "val_acc": 40.0}
{"epoch": 27, "training_loss": 645.1587791442871, "training_acc": 52.0, "val_loss": 227.33840942382812, "val_acc": 72.0}
{"epoch": 28, "training_loss": 588.1408538818359, "training_acc": 56.0, "val_loss": 56.77093863487244, "val_acc": 76.0}
{"epoch": 29, "training_loss": 237.11397775370278, "training_acc": 78.0, "val_loss": 48.35737645626068, "val_acc": 72.0}
{"epoch": 30, "training_loss": 221.59455382823944, "training_acc": 72.0, "val_loss": 82.90581107139587, "val_acc": 80.0}
{"epoch": 31, "training_loss": 158.89061986235902, "training_acc": 82.0, "val_loss": 50.826495885849, "val_acc": 68.0}
{"epoch": 32, "training_loss": 126.98289987444878, "training_acc": 77.0, "val_loss": 43.7554657459259, "val_acc": 72.0}
{"epoch": 33, "training_loss": 69.44072838686407, "training_acc": 82.0, "val_loss": 96.52634859085083, "val_acc": 72.0}
{"epoch": 34, "training_loss": 188.62052546441555, "training_acc": 66.0, "val_loss": 80.50609230995178, "val_acc": 76.0}
{"epoch": 35, "training_loss": 131.6027717590332, "training_acc": 80.0, "val_loss": 90.4371976852417, "val_acc": 76.0}
{"epoch": 36, "training_loss": 186.23834133148193, "training_acc": 76.0, "val_loss": 73.67294430732727, "val_acc": 76.0}
{"epoch": 37, "training_loss": 126.57555150985718, "training_acc": 82.0, "val_loss": 93.4130311012268, "val_acc": 52.0}
{"epoch": 38, "training_loss": 165.73763275146484, "training_acc": 76.0, "val_loss": 201.6850233078003, "val_acc": 72.0}
{"epoch": 39, "training_loss": 506.8580369949341, "training_acc": 52.0, "val_loss": 95.09427547454834, "val_acc": 76.0}
{"epoch": 40, "training_loss": 163.94420957565308, "training_acc": 78.0, "val_loss": 70.30770778656006, "val_acc": 80.0}
{"epoch": 41, "training_loss": 142.55316946655512, "training_acc": 81.0, "val_loss": 88.89931440353394, "val_acc": 56.0}
{"epoch": 42, "training_loss": 214.0416774749756, "training_acc": 76.0, "val_loss": 86.41616702079773, "val_acc": 76.0}
{"epoch": 43, "training_loss": 135.11655569076538, "training_acc": 79.0, "val_loss": 50.674909353256226, "val_acc": 76.0}
{"epoch": 44, "training_loss": 49.09644812345505, "training_acc": 84.0, "val_loss": 73.86782765388489, "val_acc": 76.0}
{"epoch": 45, "training_loss": 45.66284704208374, "training_acc": 89.0, "val_loss": 105.03482818603516, "val_acc": 72.0}
{"epoch": 46, "training_loss": 107.0921037197113, "training_acc": 76.0, "val_loss": 111.73263788223267, "val_acc": 76.0}
{"epoch": 47, "training_loss": 158.64897060394287, "training_acc": 80.0, "val_loss": 101.90342664718628, "val_acc": 76.0}
{"epoch": 48, "training_loss": 161.81847381591797, "training_acc": 76.0, "val_loss": 76.9657552242279, "val_acc": 68.0}
{"epoch": 49, "training_loss": 139.2743091583252, "training_acc": 85.0, "val_loss": 185.08684635162354, "val_acc": 28.0}
{"epoch": 50, "training_loss": 303.4177989959717, "training_acc": 65.0, "val_loss": 127.8821587562561, "val_acc": 72.0}
{"epoch": 51, "training_loss": 147.60986571013927, "training_acc": 74.0, "val_loss": 87.54090666770935, "val_acc": 64.0}
