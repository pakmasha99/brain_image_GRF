"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 29326.149925231934, "training_acc": 71.0, "val_loss": 12665.911102294922, "val_acc": 72.0}
{"epoch": 1, "training_loss": 35972.64422607422, "training_acc": 72.0, "val_loss": 18832.80029296875, "val_acc": 28.0}
{"epoch": 2, "training_loss": 69934.4775390625, "training_acc": 28.0, "val_loss": 2818.1251525878906, "val_acc": 48.0}
{"epoch": 3, "training_loss": 12800.093017578125, "training_acc": 59.0, "val_loss": 12278.804779052734, "val_acc": 72.0}
{"epoch": 4, "training_loss": 53066.1904296875, "training_acc": 72.0, "val_loss": 15782.951354980469, "val_acc": 72.0}
{"epoch": 5, "training_loss": 58879.162841796875, "training_acc": 72.0, "val_loss": 11330.316925048828, "val_acc": 72.0}
{"epoch": 6, "training_loss": 34432.84411621094, "training_acc": 72.0, "val_loss": 4282.191467285156, "val_acc": 72.0}
{"epoch": 7, "training_loss": 16925.811279296875, "training_acc": 67.0, "val_loss": 10869.427490234375, "val_acc": 48.0}
{"epoch": 8, "training_loss": 38284.870849609375, "training_acc": 43.0, "val_loss": 5803.403091430664, "val_acc": 64.0}
{"epoch": 9, "training_loss": 9935.13671875, "training_acc": 70.0, "val_loss": 6277.095413208008, "val_acc": 68.0}
{"epoch": 10, "training_loss": 24936.71826171875, "training_acc": 72.0, "val_loss": 7859.31396484375, "val_acc": 72.0}
{"epoch": 11, "training_loss": 26867.689208984375, "training_acc": 72.0, "val_loss": 5185.071182250977, "val_acc": 72.0}
{"epoch": 12, "training_loss": 13206.974212646484, "training_acc": 74.0, "val_loss": 6460.1715087890625, "val_acc": 64.0}
{"epoch": 13, "training_loss": 18278.60089111328, "training_acc": 61.0, "val_loss": 6009.77783203125, "val_acc": 64.0}
{"epoch": 14, "training_loss": 13468.30859375, "training_acc": 66.0, "val_loss": 3956.427764892578, "val_acc": 68.0}
{"epoch": 15, "training_loss": 12582.561889648438, "training_acc": 74.0, "val_loss": 3731.875991821289, "val_acc": 80.0}
{"epoch": 16, "training_loss": 11664.843170166016, "training_acc": 70.0, "val_loss": 2632.6433181762695, "val_acc": 68.0}
{"epoch": 17, "training_loss": 6161.998443603516, "training_acc": 66.0, "val_loss": 2044.0923690795898, "val_acc": 68.0}
{"epoch": 18, "training_loss": 4787.582565307617, "training_acc": 73.0, "val_loss": 1372.5010871887207, "val_acc": 80.0}
{"epoch": 19, "training_loss": 4287.666198730469, "training_acc": 75.0, "val_loss": 2341.510772705078, "val_acc": 36.0}
{"epoch": 20, "training_loss": 5650.755432128906, "training_acc": 51.0, "val_loss": 1885.4228973388672, "val_acc": 72.0}
{"epoch": 21, "training_loss": 9904.748779296875, "training_acc": 72.0, "val_loss": 2328.122901916504, "val_acc": 72.0}
{"epoch": 22, "training_loss": 6770.586471557617, "training_acc": 72.0, "val_loss": 6234.307479858398, "val_acc": 32.0}
{"epoch": 23, "training_loss": 19580.13262939453, "training_acc": 30.0, "val_loss": 663.1036281585693, "val_acc": 72.0}
{"epoch": 24, "training_loss": 5232.772521972656, "training_acc": 72.0, "val_loss": 2148.9904403686523, "val_acc": 72.0}
{"epoch": 25, "training_loss": 6909.83487701416, "training_acc": 73.0, "val_loss": 2565.0453567504883, "val_acc": 44.0}
{"epoch": 26, "training_loss": 7900.007369995117, "training_acc": 48.0, "val_loss": 890.0200843811035, "val_acc": 80.0}
{"epoch": 27, "training_loss": 5983.096466064453, "training_acc": 72.0, "val_loss": 1209.838581085205, "val_acc": 80.0}
{"epoch": 28, "training_loss": 4209.854965209961, "training_acc": 72.0, "val_loss": 1799.798583984375, "val_acc": 60.0}
{"epoch": 29, "training_loss": 4468.985725402832, "training_acc": 64.0, "val_loss": 1393.4967994689941, "val_acc": 80.0}
{"epoch": 30, "training_loss": 4940.998291015625, "training_acc": 77.0, "val_loss": 1029.2009353637695, "val_acc": 76.0}
{"epoch": 31, "training_loss": 3636.217529296875, "training_acc": 73.0, "val_loss": 1188.2790565490723, "val_acc": 60.0}
{"epoch": 32, "training_loss": 2652.677200317383, "training_acc": 71.0, "val_loss": 823.4026908874512, "val_acc": 80.0}
{"epoch": 33, "training_loss": 3272.8407306671143, "training_acc": 72.0, "val_loss": 1358.88032913208, "val_acc": 52.0}
{"epoch": 34, "training_loss": 3026.574104309082, "training_acc": 63.0, "val_loss": 590.400505065918, "val_acc": 72.0}
{"epoch": 35, "training_loss": 2983.120750427246, "training_acc": 75.0, "val_loss": 1763.5040283203125, "val_acc": 40.0}
{"epoch": 36, "training_loss": 3676.0710372924805, "training_acc": 56.0, "val_loss": 270.96261978149414, "val_acc": 80.0}
{"epoch": 37, "training_loss": 2199.7792053222656, "training_acc": 68.0, "val_loss": 376.34894847869873, "val_acc": 60.0}
{"epoch": 38, "training_loss": 1207.4522323608398, "training_acc": 82.0, "val_loss": 606.3507556915283, "val_acc": 68.0}
{"epoch": 39, "training_loss": 1357.1839179992676, "training_acc": 76.0, "val_loss": 481.4737319946289, "val_acc": 80.0}
{"epoch": 40, "training_loss": 994.7121162414551, "training_acc": 87.0, "val_loss": 688.9120101928711, "val_acc": 64.0}
{"epoch": 41, "training_loss": 1268.769401550293, "training_acc": 78.0, "val_loss": 250.37856101989746, "val_acc": 72.0}
{"epoch": 42, "training_loss": 980.3350715637207, "training_acc": 79.0, "val_loss": 218.06938648223877, "val_acc": 88.0}
{"epoch": 43, "training_loss": 900.0011596679688, "training_acc": 79.0, "val_loss": 213.47551345825195, "val_acc": 68.0}
{"epoch": 44, "training_loss": 541.8511772155762, "training_acc": 89.0, "val_loss": 185.79778671264648, "val_acc": 72.0}
{"epoch": 45, "training_loss": 625.0466499328613, "training_acc": 75.0, "val_loss": 707.9941749572754, "val_acc": 72.0}
{"epoch": 46, "training_loss": 2635.9008560180664, "training_acc": 74.0, "val_loss": 1417.446517944336, "val_acc": 44.0}
{"epoch": 47, "training_loss": 2634.22318649292, "training_acc": 65.0, "val_loss": 631.9390296936035, "val_acc": 72.0}
{"epoch": 48, "training_loss": 2188.5577239990234, "training_acc": 76.0, "val_loss": 1055.4811477661133, "val_acc": 52.0}
{"epoch": 49, "training_loss": 2043.6149444580078, "training_acc": 65.0, "val_loss": 431.3171863555908, "val_acc": 84.0}
{"epoch": 50, "training_loss": 1521.3455257415771, "training_acc": 81.0, "val_loss": 1007.5728416442871, "val_acc": 48.0}
{"epoch": 51, "training_loss": 1609.1895942687988, "training_acc": 75.0, "val_loss": 194.4352388381958, "val_acc": 80.0}
{"epoch": 52, "training_loss": 956.0163011550903, "training_acc": 86.0, "val_loss": 470.88961601257324, "val_acc": 60.0}
{"epoch": 53, "training_loss": 1114.3130798339844, "training_acc": 77.0, "val_loss": 129.70114946365356, "val_acc": 76.0}
{"epoch": 54, "training_loss": 514.196346282959, "training_acc": 84.0, "val_loss": 155.9639811515808, "val_acc": 76.0}
{"epoch": 55, "training_loss": 563.1638774871826, "training_acc": 87.0, "val_loss": 108.07476043701172, "val_acc": 88.0}
{"epoch": 56, "training_loss": 1558.4638061523438, "training_acc": 74.0, "val_loss": 469.991397857666, "val_acc": 76.0}
{"epoch": 57, "training_loss": 2247.473129272461, "training_acc": 76.0, "val_loss": 325.92451572418213, "val_acc": 72.0}
{"epoch": 58, "training_loss": 1182.268325805664, "training_acc": 77.0, "val_loss": 754.8148155212402, "val_acc": 76.0}
{"epoch": 59, "training_loss": 3084.7859115600586, "training_acc": 75.0, "val_loss": 440.875244140625, "val_acc": 80.0}
{"epoch": 60, "training_loss": 1514.1517639160156, "training_acc": 75.0, "val_loss": 468.78504753112793, "val_acc": 76.0}
{"epoch": 61, "training_loss": 963.9667091369629, "training_acc": 86.0, "val_loss": 327.3587465286255, "val_acc": 80.0}
{"epoch": 62, "training_loss": 635.4774417877197, "training_acc": 82.0, "val_loss": 189.80249166488647, "val_acc": 76.0}
{"epoch": 63, "training_loss": 682.0226440429688, "training_acc": 84.0, "val_loss": 186.58218383789062, "val_acc": 80.0}
{"epoch": 64, "training_loss": 973.4181823730469, "training_acc": 77.0, "val_loss": 1336.1517906188965, "val_acc": 72.0}
{"epoch": 65, "training_loss": 5957.050186157227, "training_acc": 72.0, "val_loss": 56.91244602203369, "val_acc": 88.0}
{"epoch": 66, "training_loss": 3399.740203857422, "training_acc": 72.0, "val_loss": 168.8936710357666, "val_acc": 76.0}
{"epoch": 67, "training_loss": 1684.8059539794922, "training_acc": 85.0, "val_loss": 2159.4005584716797, "val_acc": 72.0}
{"epoch": 68, "training_loss": 7021.415481567383, "training_acc": 74.0, "val_loss": 1219.3647384643555, "val_acc": 56.0}
{"epoch": 69, "training_loss": 3558.681838989258, "training_acc": 60.0, "val_loss": 1187.5361442565918, "val_acc": 76.0}
{"epoch": 70, "training_loss": 6597.156707763672, "training_acc": 74.0, "val_loss": 1357.956314086914, "val_acc": 76.0}
{"epoch": 71, "training_loss": 4158.297607421875, "training_acc": 71.0, "val_loss": 2163.848114013672, "val_acc": 52.0}
{"epoch": 72, "training_loss": 3315.706552505493, "training_acc": 68.0, "val_loss": 1267.498779296875, "val_acc": 80.0}
{"epoch": 73, "training_loss": 4260.679054260254, "training_acc": 76.0, "val_loss": 886.4381790161133, "val_acc": 76.0}
{"epoch": 74, "training_loss": 2112.398452758789, "training_acc": 72.0, "val_loss": 719.7701454162598, "val_acc": 72.0}
{"epoch": 75, "training_loss": 1669.8463745117188, "training_acc": 82.0, "val_loss": 560.7571125030518, "val_acc": 72.0}
{"epoch": 76, "training_loss": 1826.519645690918, "training_acc": 74.0, "val_loss": 430.1435947418213, "val_acc": 76.0}
{"epoch": 77, "training_loss": 912.231990814209, "training_acc": 83.0, "val_loss": 216.23647212982178, "val_acc": 80.0}
{"epoch": 78, "training_loss": 489.2583408355713, "training_acc": 88.0, "val_loss": 368.16112995147705, "val_acc": 60.0}
{"epoch": 79, "training_loss": 782.5020065307617, "training_acc": 85.0, "val_loss": 86.35451793670654, "val_acc": 88.0}
{"epoch": 80, "training_loss": 1570.3204040527344, "training_acc": 75.0, "val_loss": 502.30913162231445, "val_acc": 72.0}
{"epoch": 81, "training_loss": 2633.7862091064453, "training_acc": 73.0, "val_loss": 1045.0061798095703, "val_acc": 40.0}
{"epoch": 82, "training_loss": 1562.692211151123, "training_acc": 72.0, "val_loss": 473.63314628601074, "val_acc": 84.0}
{"epoch": 83, "training_loss": 1775.6062240600586, "training_acc": 80.0, "val_loss": 545.7080364227295, "val_acc": 68.0}
{"epoch": 84, "training_loss": 997.1926422119141, "training_acc": 80.0, "val_loss": 613.9976501464844, "val_acc": 84.0}
