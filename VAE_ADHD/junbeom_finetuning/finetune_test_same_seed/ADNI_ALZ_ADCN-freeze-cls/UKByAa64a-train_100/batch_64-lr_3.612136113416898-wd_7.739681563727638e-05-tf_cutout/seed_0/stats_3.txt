"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 4902.637222290039, "training_acc": 72.0, "val_loss": 2286.176300048828, "val_acc": 72.0}
{"epoch": 1, "training_loss": 5172.249817848206, "training_acc": 76.0, "val_loss": 2093.3271408081055, "val_acc": 28.0}
{"epoch": 2, "training_loss": 5422.855445861816, "training_acc": 44.0, "val_loss": 805.7797431945801, "val_acc": 68.0}
{"epoch": 3, "training_loss": 2680.123466491699, "training_acc": 73.0, "val_loss": 668.3570861816406, "val_acc": 64.0}
{"epoch": 4, "training_loss": 2245.8783264160156, "training_acc": 58.0, "val_loss": 603.5159587860107, "val_acc": 68.0}
{"epoch": 5, "training_loss": 1554.1839218139648, "training_acc": 72.0, "val_loss": 782.905387878418, "val_acc": 68.0}
{"epoch": 6, "training_loss": 2373.9591064453125, "training_acc": 72.0, "val_loss": 554.5844554901123, "val_acc": 64.0}
{"epoch": 7, "training_loss": 1662.8865585327148, "training_acc": 55.0, "val_loss": 353.684401512146, "val_acc": 72.0}
{"epoch": 8, "training_loss": 798.5623664855957, "training_acc": 76.0, "val_loss": 449.2880344390869, "val_acc": 72.0}
{"epoch": 9, "training_loss": 1340.5499420166016, "training_acc": 72.0, "val_loss": 722.5346565246582, "val_acc": 28.0}
{"epoch": 10, "training_loss": 1568.8830585479736, "training_acc": 52.0, "val_loss": 479.0289878845215, "val_acc": 72.0}
{"epoch": 11, "training_loss": 2179.633834838867, "training_acc": 72.0, "val_loss": 320.554256439209, "val_acc": 72.0}
{"epoch": 12, "training_loss": 2302.805679321289, "training_acc": 47.0, "val_loss": 230.27679920196533, "val_acc": 52.0}
{"epoch": 13, "training_loss": 1114.2902297973633, "training_acc": 61.0, "val_loss": 663.3301734924316, "val_acc": 72.0}
{"epoch": 14, "training_loss": 2463.4672527313232, "training_acc": 72.0, "val_loss": 153.7109613418579, "val_acc": 76.0}
{"epoch": 15, "training_loss": 912.5711669921875, "training_acc": 67.0, "val_loss": 302.8572082519531, "val_acc": 56.0}
{"epoch": 16, "training_loss": 940.9924049377441, "training_acc": 66.0, "val_loss": 488.53001594543457, "val_acc": 72.0}
{"epoch": 17, "training_loss": 1579.9482307434082, "training_acc": 72.0, "val_loss": 294.283652305603, "val_acc": 68.0}
{"epoch": 18, "training_loss": 869.7460899353027, "training_acc": 64.0, "val_loss": 252.00319290161133, "val_acc": 64.0}
{"epoch": 19, "training_loss": 671.5069274902344, "training_acc": 72.0, "val_loss": 220.943021774292, "val_acc": 80.0}
{"epoch": 20, "training_loss": 663.1328754425049, "training_acc": 70.0, "val_loss": 164.1579270362854, "val_acc": 72.0}
{"epoch": 21, "training_loss": 600.8292999267578, "training_acc": 67.0, "val_loss": 99.8966634273529, "val_acc": 76.0}
{"epoch": 22, "training_loss": 312.3512077331543, "training_acc": 73.0, "val_loss": 96.12218737602234, "val_acc": 80.0}
{"epoch": 23, "training_loss": 351.8616256713867, "training_acc": 78.0, "val_loss": 294.38464641571045, "val_acc": 40.0}
{"epoch": 24, "training_loss": 648.3118925094604, "training_acc": 57.0, "val_loss": 26.653572916984558, "val_acc": 84.0}
{"epoch": 25, "training_loss": 419.00006103515625, "training_acc": 65.0, "val_loss": 201.26409530639648, "val_acc": 72.0}
{"epoch": 26, "training_loss": 938.0095138549805, "training_acc": 72.0, "val_loss": 90.93931317329407, "val_acc": 80.0}
{"epoch": 27, "training_loss": 586.0873985290527, "training_acc": 71.0, "val_loss": 127.40817070007324, "val_acc": 80.0}
{"epoch": 28, "training_loss": 348.8884000778198, "training_acc": 75.0, "val_loss": 145.540189743042, "val_acc": 72.0}
{"epoch": 29, "training_loss": 439.1238269805908, "training_acc": 73.0, "val_loss": 130.23470640182495, "val_acc": 76.0}
{"epoch": 30, "training_loss": 241.43403100967407, "training_acc": 79.0, "val_loss": 75.13972520828247, "val_acc": 80.0}
{"epoch": 31, "training_loss": 334.1268672943115, "training_acc": 77.0, "val_loss": 94.25355195999146, "val_acc": 76.0}
{"epoch": 32, "training_loss": 476.29229736328125, "training_acc": 76.0, "val_loss": 241.94223880767822, "val_acc": 44.0}
{"epoch": 33, "training_loss": 433.77017307281494, "training_acc": 66.0, "val_loss": 59.700584411621094, "val_acc": 80.0}
{"epoch": 34, "training_loss": 475.3534851074219, "training_acc": 67.0, "val_loss": 84.99122262001038, "val_acc": 80.0}
{"epoch": 35, "training_loss": 352.8655433654785, "training_acc": 77.0, "val_loss": 204.30502891540527, "val_acc": 56.0}
{"epoch": 36, "training_loss": 494.89286041259766, "training_acc": 64.0, "val_loss": 297.36926555633545, "val_acc": 72.0}
{"epoch": 37, "training_loss": 1186.3308029174805, "training_acc": 73.0, "val_loss": 167.43245124816895, "val_acc": 80.0}
{"epoch": 38, "training_loss": 612.0119819641113, "training_acc": 75.0, "val_loss": 291.3724660873413, "val_acc": 52.0}
{"epoch": 39, "training_loss": 895.1921615600586, "training_acc": 66.0, "val_loss": 308.15439224243164, "val_acc": 72.0}
{"epoch": 40, "training_loss": 972.2199749946594, "training_acc": 73.0, "val_loss": 262.7748966217041, "val_acc": 52.0}
{"epoch": 41, "training_loss": 554.5040612220764, "training_acc": 71.0, "val_loss": 143.99198293685913, "val_acc": 80.0}
{"epoch": 42, "training_loss": 447.7691020965576, "training_acc": 79.0, "val_loss": 92.25854277610779, "val_acc": 68.0}
{"epoch": 43, "training_loss": 223.65841388702393, "training_acc": 76.0, "val_loss": 117.03866720199585, "val_acc": 80.0}
