"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 5268.913665771484, "training_acc": 36.0, "val_loss": 1995.6169128417969, "val_acc": 72.0}
{"epoch": 1, "training_loss": 6192.991928100586, "training_acc": 72.0, "val_loss": 1107.1005821228027, "val_acc": 28.0}
{"epoch": 2, "training_loss": 4980.900680541992, "training_acc": 28.0, "val_loss": 394.08276081085205, "val_acc": 72.0}
{"epoch": 3, "training_loss": 1552.9176788330078, "training_acc": 73.0, "val_loss": 889.4109725952148, "val_acc": 72.0}
{"epoch": 4, "training_loss": 2534.8863067626953, "training_acc": 72.0, "val_loss": 391.24279022216797, "val_acc": 68.0}
{"epoch": 5, "training_loss": 1701.641502380371, "training_acc": 63.0, "val_loss": 477.0144462585449, "val_acc": 60.0}
{"epoch": 6, "training_loss": 2203.3486938476562, "training_acc": 59.0, "val_loss": 656.6884517669678, "val_acc": 72.0}
{"epoch": 7, "training_loss": 1631.7714767456055, "training_acc": 75.0, "val_loss": 693.0093765258789, "val_acc": 72.0}
{"epoch": 8, "training_loss": 1395.6661682128906, "training_acc": 78.0, "val_loss": 376.87394618988037, "val_acc": 64.0}
{"epoch": 9, "training_loss": 1824.6608047485352, "training_acc": 56.0, "val_loss": 312.73040771484375, "val_acc": 64.0}
{"epoch": 10, "training_loss": 981.7417945861816, "training_acc": 67.0, "val_loss": 429.98270988464355, "val_acc": 72.0}
{"epoch": 11, "training_loss": 959.1725101470947, "training_acc": 74.0, "val_loss": 159.56944227218628, "val_acc": 64.0}
{"epoch": 12, "training_loss": 583.3391151428223, "training_acc": 64.0, "val_loss": 156.28081560134888, "val_acc": 72.0}
{"epoch": 13, "training_loss": 559.3129367828369, "training_acc": 72.0, "val_loss": 387.10615634918213, "val_acc": 28.0}
{"epoch": 14, "training_loss": 1074.4865951538086, "training_acc": 45.0, "val_loss": 54.427528381347656, "val_acc": 72.0}
{"epoch": 15, "training_loss": 484.5922203063965, "training_acc": 65.0, "val_loss": 75.46893954277039, "val_acc": 72.0}
{"epoch": 16, "training_loss": 428.7189302444458, "training_acc": 72.0, "val_loss": 256.8429708480835, "val_acc": 36.0}
{"epoch": 17, "training_loss": 794.7993488311768, "training_acc": 48.0, "val_loss": 70.83088755607605, "val_acc": 80.0}
{"epoch": 18, "training_loss": 307.14802169799805, "training_acc": 72.0, "val_loss": 74.17460680007935, "val_acc": 76.0}
{"epoch": 19, "training_loss": 329.1634273529053, "training_acc": 76.0, "val_loss": 85.88516116142273, "val_acc": 76.0}
{"epoch": 20, "training_loss": 408.149658203125, "training_acc": 74.0, "val_loss": 76.42022371292114, "val_acc": 76.0}
{"epoch": 21, "training_loss": 314.6790351867676, "training_acc": 78.0, "val_loss": 46.983692049980164, "val_acc": 84.0}
{"epoch": 22, "training_loss": 257.6817932128906, "training_acc": 79.0, "val_loss": 31.318050622940063, "val_acc": 80.0}
{"epoch": 23, "training_loss": 125.26793098449707, "training_acc": 83.0, "val_loss": 20.191018283367157, "val_acc": 72.0}
{"epoch": 24, "training_loss": 121.25836658477783, "training_acc": 77.0, "val_loss": 61.41897439956665, "val_acc": 72.0}
{"epoch": 25, "training_loss": 240.45915842056274, "training_acc": 71.0, "val_loss": 40.54386913776398, "val_acc": 80.0}
{"epoch": 26, "training_loss": 161.18359637260437, "training_acc": 80.0, "val_loss": 33.88579487800598, "val_acc": 84.0}
{"epoch": 27, "training_loss": 61.470141649246216, "training_acc": 81.0, "val_loss": 103.55784893035889, "val_acc": 44.0}
{"epoch": 28, "training_loss": 514.4360103607178, "training_acc": 56.0, "val_loss": 113.79773616790771, "val_acc": 76.0}
{"epoch": 29, "training_loss": 366.45423698425293, "training_acc": 70.0, "val_loss": 128.00782918930054, "val_acc": 76.0}
{"epoch": 30, "training_loss": 281.18418884277344, "training_acc": 78.0, "val_loss": 125.86250305175781, "val_acc": 76.0}
{"epoch": 31, "training_loss": 520.1338806152344, "training_acc": 67.0, "val_loss": 85.65797209739685, "val_acc": 76.0}
{"epoch": 32, "training_loss": 207.80584526062012, "training_acc": 80.0, "val_loss": 163.4594202041626, "val_acc": 72.0}
{"epoch": 33, "training_loss": 579.8330059051514, "training_acc": 59.0, "val_loss": 109.59925651550293, "val_acc": 76.0}
{"epoch": 34, "training_loss": 271.72559118270874, "training_acc": 79.0, "val_loss": 31.044313311576843, "val_acc": 84.0}
{"epoch": 35, "training_loss": 158.57332706451416, "training_acc": 77.0, "val_loss": 187.91991472244263, "val_acc": 72.0}
{"epoch": 36, "training_loss": 748.3102569580078, "training_acc": 72.0, "val_loss": 37.22705841064453, "val_acc": 84.0}
{"epoch": 37, "training_loss": 368.29454040527344, "training_acc": 68.0, "val_loss": 124.83851909637451, "val_acc": 76.0}
{"epoch": 38, "training_loss": 498.4064025878906, "training_acc": 76.0, "val_loss": 89.05501961708069, "val_acc": 80.0}
{"epoch": 39, "training_loss": 483.6838188171387, "training_acc": 69.0, "val_loss": 69.91494297981262, "val_acc": 84.0}
{"epoch": 40, "training_loss": 294.1979579925537, "training_acc": 80.0, "val_loss": 156.6813349723816, "val_acc": 72.0}
{"epoch": 41, "training_loss": 327.8068542480469, "training_acc": 76.0, "val_loss": 86.6351306438446, "val_acc": 52.0}
{"epoch": 42, "training_loss": 550.5237483978271, "training_acc": 65.0, "val_loss": 262.8003120422363, "val_acc": 72.0}
