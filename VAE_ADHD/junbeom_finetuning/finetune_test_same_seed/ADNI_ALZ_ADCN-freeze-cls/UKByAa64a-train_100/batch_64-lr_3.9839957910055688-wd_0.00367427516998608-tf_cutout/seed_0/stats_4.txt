"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 4686.485656738281, "training_acc": 61.0, "val_loss": 3153.688621520996, "val_acc": 72.0}
{"epoch": 1, "training_loss": 10076.469375610352, "training_acc": 72.0, "val_loss": 3284.9716186523438, "val_acc": 28.0}
{"epoch": 2, "training_loss": 13721.51187133789, "training_acc": 29.0, "val_loss": 555.7939529418945, "val_acc": 76.0}
{"epoch": 3, "training_loss": 2683.5303955078125, "training_acc": 75.0, "val_loss": 2234.939956665039, "val_acc": 72.0}
{"epoch": 4, "training_loss": 7645.550079345703, "training_acc": 72.0, "val_loss": 1812.5333786010742, "val_acc": 72.0}
{"epoch": 5, "training_loss": 4627.107032775879, "training_acc": 69.0, "val_loss": 731.1635494232178, "val_acc": 64.0}
{"epoch": 6, "training_loss": 3670.9842376708984, "training_acc": 57.0, "val_loss": 730.6142807006836, "val_acc": 60.0}
{"epoch": 7, "training_loss": 2731.05126953125, "training_acc": 59.0, "val_loss": 1358.2924842834473, "val_acc": 72.0}
{"epoch": 8, "training_loss": 4052.205337524414, "training_acc": 72.0, "val_loss": 1270.8959579467773, "val_acc": 72.0}
{"epoch": 9, "training_loss": 3321.35294342041, "training_acc": 69.0, "val_loss": 604.7572612762451, "val_acc": 60.0}
{"epoch": 10, "training_loss": 3281.581573486328, "training_acc": 51.0, "val_loss": 453.91979217529297, "val_acc": 72.0}
{"epoch": 11, "training_loss": 1297.6731643676758, "training_acc": 72.0, "val_loss": 743.2078838348389, "val_acc": 72.0}
{"epoch": 12, "training_loss": 1645.5759925842285, "training_acc": 73.0, "val_loss": 416.4000988006592, "val_acc": 48.0}
{"epoch": 13, "training_loss": 2492.0775756835938, "training_acc": 43.0, "val_loss": 310.7182502746582, "val_acc": 72.0}
{"epoch": 14, "training_loss": 1620.3130645751953, "training_acc": 76.0, "val_loss": 627.2454738616943, "val_acc": 72.0}
{"epoch": 15, "training_loss": 1732.4199619293213, "training_acc": 73.0, "val_loss": 652.4081707000732, "val_acc": 36.0}
{"epoch": 16, "training_loss": 2383.5554161071777, "training_acc": 38.0, "val_loss": 474.33295249938965, "val_acc": 72.0}
{"epoch": 17, "training_loss": 2351.6408081054688, "training_acc": 72.0, "val_loss": 741.9442176818848, "val_acc": 72.0}
{"epoch": 18, "training_loss": 2220.931137084961, "training_acc": 72.0, "val_loss": 392.3501253128052, "val_acc": 48.0}
{"epoch": 19, "training_loss": 1847.4426383972168, "training_acc": 52.0, "val_loss": 158.9529037475586, "val_acc": 76.0}
{"epoch": 20, "training_loss": 890.6313400268555, "training_acc": 73.0, "val_loss": 402.9656410217285, "val_acc": 72.0}
{"epoch": 21, "training_loss": 1072.3497695922852, "training_acc": 75.0, "val_loss": 279.5501947402954, "val_acc": 60.0}
{"epoch": 22, "training_loss": 1200.9227657318115, "training_acc": 62.0, "val_loss": 256.53605461120605, "val_acc": 72.0}
{"epoch": 23, "training_loss": 846.7534370422363, "training_acc": 77.0, "val_loss": 56.6883385181427, "val_acc": 76.0}
{"epoch": 24, "training_loss": 362.12892723083496, "training_acc": 72.0, "val_loss": 59.32968854904175, "val_acc": 80.0}
{"epoch": 25, "training_loss": 276.4027671813965, "training_acc": 76.0, "val_loss": 249.81799125671387, "val_acc": 40.0}
{"epoch": 26, "training_loss": 622.6891822814941, "training_acc": 59.0, "val_loss": 119.08012628555298, "val_acc": 76.0}
{"epoch": 27, "training_loss": 178.41732692718506, "training_acc": 85.0, "val_loss": 59.809303283691406, "val_acc": 68.0}
{"epoch": 28, "training_loss": 417.3840217590332, "training_acc": 69.0, "val_loss": 133.49772691726685, "val_acc": 76.0}
{"epoch": 29, "training_loss": 492.0354118347168, "training_acc": 67.0, "val_loss": 134.39488410949707, "val_acc": 76.0}
{"epoch": 30, "training_loss": 283.13869524002075, "training_acc": 78.0, "val_loss": 105.88053464889526, "val_acc": 76.0}
{"epoch": 31, "training_loss": 172.9992322921753, "training_acc": 78.0, "val_loss": 37.3290091753006, "val_acc": 76.0}
{"epoch": 32, "training_loss": 60.78560388088226, "training_acc": 83.0, "val_loss": 54.835182428359985, "val_acc": 76.0}
{"epoch": 33, "training_loss": 657.628475189209, "training_acc": 59.0, "val_loss": 413.60011100769043, "val_acc": 72.0}
{"epoch": 34, "training_loss": 2170.832778930664, "training_acc": 72.0, "val_loss": 491.78380966186523, "val_acc": 72.0}
{"epoch": 35, "training_loss": 1316.1692533493042, "training_acc": 71.0, "val_loss": 1277.8698921203613, "val_acc": 28.0}
{"epoch": 36, "training_loss": 4004.5244522094727, "training_acc": 29.0, "val_loss": 704.0349006652832, "val_acc": 72.0}
{"epoch": 37, "training_loss": 3278.1781158447266, "training_acc": 72.0, "val_loss": 1409.075927734375, "val_acc": 72.0}
{"epoch": 38, "training_loss": 4929.363067626953, "training_acc": 72.0, "val_loss": 758.5271835327148, "val_acc": 72.0}
{"epoch": 39, "training_loss": 1649.5578994750977, "training_acc": 73.0, "val_loss": 839.5932197570801, "val_acc": 48.0}
{"epoch": 40, "training_loss": 4599.681442260742, "training_acc": 44.0, "val_loss": 425.487756729126, "val_acc": 72.0}
{"epoch": 41, "training_loss": 2025.788459777832, "training_acc": 71.0, "val_loss": 1274.1178512573242, "val_acc": 72.0}
{"epoch": 42, "training_loss": 3567.5435180664062, "training_acc": 72.0, "val_loss": 810.3180885314941, "val_acc": 72.0}
{"epoch": 43, "training_loss": 2357.360466003418, "training_acc": 70.0, "val_loss": 438.6568069458008, "val_acc": 64.0}
{"epoch": 44, "training_loss": 2023.8053817749023, "training_acc": 67.0, "val_loss": 507.8145503997803, "val_acc": 76.0}
{"epoch": 45, "training_loss": 1363.5253982543945, "training_acc": 78.0, "val_loss": 529.628849029541, "val_acc": 72.0}
{"epoch": 46, "training_loss": 1184.5696487426758, "training_acc": 70.0, "val_loss": 256.63228034973145, "val_acc": 64.0}
{"epoch": 47, "training_loss": 1185.9718074798584, "training_acc": 61.0, "val_loss": 333.6153507232666, "val_acc": 72.0}
{"epoch": 48, "training_loss": 1261.8966026306152, "training_acc": 72.0, "val_loss": 78.49481701850891, "val_acc": 76.0}
{"epoch": 49, "training_loss": 1525.5988006591797, "training_acc": 61.0, "val_loss": 417.7119731903076, "val_acc": 28.0}
{"epoch": 50, "training_loss": 1928.1651916503906, "training_acc": 50.0, "val_loss": 1146.031665802002, "val_acc": 72.0}
