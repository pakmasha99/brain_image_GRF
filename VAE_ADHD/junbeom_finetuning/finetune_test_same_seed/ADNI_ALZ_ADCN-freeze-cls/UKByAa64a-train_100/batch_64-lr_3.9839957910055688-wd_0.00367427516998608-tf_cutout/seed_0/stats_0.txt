"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 6497.439403533936, "training_acc": 72.0, "val_loss": 2796.473503112793, "val_acc": 72.0}
{"epoch": 1, "training_loss": 7464.3286209106445, "training_acc": 72.0, "val_loss": 4747.629928588867, "val_acc": 28.0}
{"epoch": 2, "training_loss": 19721.45867919922, "training_acc": 28.0, "val_loss": 930.6723594665527, "val_acc": 36.0}
{"epoch": 3, "training_loss": 5197.750793457031, "training_acc": 48.0, "val_loss": 2565.9860610961914, "val_acc": 72.0}
{"epoch": 4, "training_loss": 11544.341125488281, "training_acc": 72.0, "val_loss": 3405.282211303711, "val_acc": 72.0}
{"epoch": 5, "training_loss": 13581.457061767578, "training_acc": 72.0, "val_loss": 2494.077491760254, "val_acc": 72.0}
{"epoch": 6, "training_loss": 8763.97412109375, "training_acc": 72.0, "val_loss": 530.8107376098633, "val_acc": 72.0}
{"epoch": 7, "training_loss": 4478.659271240234, "training_acc": 64.0, "val_loss": 1046.2964057922363, "val_acc": 44.0}
{"epoch": 8, "training_loss": 7114.754623413086, "training_acc": 44.0, "val_loss": 289.8346424102783, "val_acc": 80.0}
{"epoch": 9, "training_loss": 3029.3407135009766, "training_acc": 71.0, "val_loss": 894.6538925170898, "val_acc": 72.0}
{"epoch": 10, "training_loss": 3672.783660888672, "training_acc": 73.0, "val_loss": 740.1486873626709, "val_acc": 72.0}
{"epoch": 11, "training_loss": 2328.1029663085938, "training_acc": 71.0, "val_loss": 222.21143245697021, "val_acc": 76.0}
{"epoch": 12, "training_loss": 1979.3178634643555, "training_acc": 59.0, "val_loss": 140.94276428222656, "val_acc": 76.0}
{"epoch": 13, "training_loss": 863.6401863098145, "training_acc": 75.0, "val_loss": 685.1857662200928, "val_acc": 72.0}
{"epoch": 14, "training_loss": 2149.4695434570312, "training_acc": 72.0, "val_loss": 292.2760009765625, "val_acc": 52.0}
{"epoch": 15, "training_loss": 1513.4028778076172, "training_acc": 56.0, "val_loss": 293.0556535720825, "val_acc": 56.0}
{"epoch": 16, "training_loss": 1388.3728942871094, "training_acc": 66.0, "val_loss": 363.35198879241943, "val_acc": 72.0}
{"epoch": 17, "training_loss": 1317.7880821228027, "training_acc": 60.0, "val_loss": 83.96381139755249, "val_acc": 64.0}
{"epoch": 18, "training_loss": 597.8465461730957, "training_acc": 68.0, "val_loss": 104.72476482391357, "val_acc": 76.0}
{"epoch": 19, "training_loss": 826.8067359924316, "training_acc": 64.0, "val_loss": 65.69654941558838, "val_acc": 84.0}
{"epoch": 20, "training_loss": 461.37453269958496, "training_acc": 76.0, "val_loss": 73.10744524002075, "val_acc": 80.0}
{"epoch": 21, "training_loss": 350.29137802124023, "training_acc": 72.0, "val_loss": 148.64224195480347, "val_acc": 56.0}
{"epoch": 22, "training_loss": 629.3377513885498, "training_acc": 65.0, "val_loss": 124.31975603103638, "val_acc": 72.0}
{"epoch": 23, "training_loss": 825.8595161437988, "training_acc": 57.0, "val_loss": 99.5319128036499, "val_acc": 72.0}
{"epoch": 24, "training_loss": 431.7818260192871, "training_acc": 74.0, "val_loss": 120.57199478149414, "val_acc": 56.0}
{"epoch": 25, "training_loss": 309.3926429748535, "training_acc": 72.0, "val_loss": 243.5826063156128, "val_acc": 72.0}
{"epoch": 26, "training_loss": 735.6672992706299, "training_acc": 73.0, "val_loss": 201.77757740020752, "val_acc": 52.0}
{"epoch": 27, "training_loss": 578.6571049690247, "training_acc": 66.0, "val_loss": 150.61944723129272, "val_acc": 72.0}
{"epoch": 28, "training_loss": 380.4889750480652, "training_acc": 76.0, "val_loss": 86.57249212265015, "val_acc": 64.0}
{"epoch": 29, "training_loss": 250.15620136260986, "training_acc": 75.0, "val_loss": 49.62769150733948, "val_acc": 68.0}
{"epoch": 30, "training_loss": 180.85650825500488, "training_acc": 76.0, "val_loss": 341.1254405975342, "val_acc": 72.0}
{"epoch": 31, "training_loss": 1334.001350402832, "training_acc": 72.0, "val_loss": 32.95026421546936, "val_acc": 76.0}
{"epoch": 32, "training_loss": 1111.2074890136719, "training_acc": 65.0, "val_loss": 106.34808540344238, "val_acc": 72.0}
{"epoch": 33, "training_loss": 578.7525482177734, "training_acc": 78.0, "val_loss": 227.48100757598877, "val_acc": 72.0}
{"epoch": 34, "training_loss": 653.2941379547119, "training_acc": 71.0, "val_loss": 257.9601049423218, "val_acc": 52.0}
{"epoch": 35, "training_loss": 1342.8781509399414, "training_acc": 51.0, "val_loss": 487.9507541656494, "val_acc": 72.0}
{"epoch": 36, "training_loss": 1606.3794441223145, "training_acc": 72.0, "val_loss": 98.99119138717651, "val_acc": 72.0}
{"epoch": 37, "training_loss": 718.3313407897949, "training_acc": 61.0, "val_loss": 108.25910568237305, "val_acc": 76.0}
{"epoch": 38, "training_loss": 592.0781936645508, "training_acc": 78.0, "val_loss": 70.54111361503601, "val_acc": 76.0}
{"epoch": 39, "training_loss": 409.19599533081055, "training_acc": 68.0, "val_loss": 202.9360055923462, "val_acc": 72.0}
{"epoch": 40, "training_loss": 667.2650756835938, "training_acc": 72.0, "val_loss": 336.4687919616699, "val_acc": 40.0}
{"epoch": 41, "training_loss": 857.1370401382446, "training_acc": 52.0, "val_loss": 92.83559322357178, "val_acc": 72.0}
{"epoch": 42, "training_loss": 689.3123435974121, "training_acc": 60.0, "val_loss": 144.98134851455688, "val_acc": 72.0}
{"epoch": 43, "training_loss": 349.6373872756958, "training_acc": 74.0, "val_loss": 194.19796466827393, "val_acc": 48.0}
{"epoch": 44, "training_loss": 550.3099308013916, "training_acc": 60.0, "val_loss": 110.3628158569336, "val_acc": 72.0}
{"epoch": 45, "training_loss": 325.0859966278076, "training_acc": 70.0, "val_loss": 278.3306837081909, "val_acc": 72.0}
{"epoch": 46, "training_loss": 738.7884941101074, "training_acc": 72.0, "val_loss": 117.46416091918945, "val_acc": 60.0}
{"epoch": 47, "training_loss": 230.26616096496582, "training_acc": 74.0, "val_loss": 65.91938734054565, "val_acc": 68.0}
{"epoch": 48, "training_loss": 125.51753568649292, "training_acc": 84.0, "val_loss": 75.75919032096863, "val_acc": 60.0}
{"epoch": 49, "training_loss": 114.57940673828125, "training_acc": 83.0, "val_loss": 174.27620887756348, "val_acc": 72.0}
{"epoch": 50, "training_loss": 322.4351634979248, "training_acc": 69.0, "val_loss": 125.06572008132935, "val_acc": 72.0}
