"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 32 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 2128.1882820129395, "training_acc": 72.0, "val_loss": 605.6103229522705, "val_acc": 28.0}
{"epoch": 1, "training_loss": 2640.121440887451, "training_acc": 28.0, "val_loss": 237.86756992340088, "val_acc": 72.0}
{"epoch": 2, "training_loss": 1052.9516944885254, "training_acc": 72.0, "val_loss": 77.18391418457031, "val_acc": 72.0}
{"epoch": 3, "training_loss": 764.4733119010925, "training_acc": 42.0, "val_loss": 132.02548027038574, "val_acc": 72.0}
{"epoch": 4, "training_loss": 455.0422623157501, "training_acc": 70.0, "val_loss": 32.197555899620056, "val_acc": 28.0}
{"epoch": 5, "training_loss": 424.5368938446045, "training_acc": 52.0, "val_loss": 43.829962611198425, "val_acc": 72.0}
{"epoch": 6, "training_loss": 536.9542245864868, "training_acc": 40.0, "val_loss": 194.59404945373535, "val_acc": 72.0}
{"epoch": 7, "training_loss": 890.7087097167969, "training_acc": 72.0, "val_loss": 82.9651951789856, "val_acc": 72.0}
{"epoch": 8, "training_loss": 269.89501297473885, "training_acc": 44.0, "val_loss": 211.3178014755249, "val_acc": 72.0}
{"epoch": 9, "training_loss": 942.7477645874023, "training_acc": 72.0, "val_loss": 58.049190044403076, "val_acc": 72.0}
{"epoch": 10, "training_loss": 1039.908941268921, "training_acc": 40.0, "val_loss": 134.57720279693604, "val_acc": 72.0}
{"epoch": 11, "training_loss": 809.02734375, "training_acc": 72.0, "val_loss": 210.79607009887695, "val_acc": 72.0}
{"epoch": 12, "training_loss": 641.3848609924316, "training_acc": 54.0, "val_loss": 29.456573724746704, "val_acc": 72.0}
{"epoch": 13, "training_loss": 295.08339262008667, "training_acc": 72.0, "val_loss": 76.78719758987427, "val_acc": 28.0}
{"epoch": 14, "training_loss": 212.20425033569336, "training_acc": 55.0, "val_loss": 97.80135750770569, "val_acc": 72.0}
{"epoch": 15, "training_loss": 483.2351608276367, "training_acc": 52.0, "val_loss": 140.38366079330444, "val_acc": 72.0}
{"epoch": 16, "training_loss": 900.8928298950195, "training_acc": 72.0, "val_loss": 295.6085205078125, "val_acc": 72.0}
{"epoch": 17, "training_loss": 847.515251159668, "training_acc": 68.0, "val_loss": 95.448237657547, "val_acc": 28.0}
{"epoch": 18, "training_loss": 587.0142974853516, "training_acc": 54.0, "val_loss": 208.61387252807617, "val_acc": 72.0}
{"epoch": 19, "training_loss": 556.337085723877, "training_acc": 68.0, "val_loss": 175.00839233398438, "val_acc": 28.0}
{"epoch": 20, "training_loss": 696.4698734283447, "training_acc": 54.0, "val_loss": 215.78283309936523, "val_acc": 72.0}
{"epoch": 21, "training_loss": 598.7734527587891, "training_acc": 68.0, "val_loss": 122.8837251663208, "val_acc": 28.0}
{"epoch": 22, "training_loss": 680.3493347167969, "training_acc": 52.0, "val_loss": 269.67363357543945, "val_acc": 72.0}
{"epoch": 23, "training_loss": 915.6559351681863, "training_acc": 72.0, "val_loss": 53.308749198913574, "val_acc": 28.0}
{"epoch": 24, "training_loss": 147.36804234981378, "training_acc": 55.0, "val_loss": 132.39479064941406, "val_acc": 72.0}
{"epoch": 25, "training_loss": 384.8019485473633, "training_acc": 72.0, "val_loss": 115.26268720626831, "val_acc": 28.0}
{"epoch": 26, "training_loss": 336.8601884841919, "training_acc": 60.0, "val_loss": 63.90434503555298, "val_acc": 72.0}
{"epoch": 27, "training_loss": 144.3025740319863, "training_acc": 62.0, "val_loss": 108.5439920425415, "val_acc": 72.0}
{"epoch": 28, "training_loss": 388.67240607738495, "training_acc": 72.0, "val_loss": 64.65294361114502, "val_acc": 28.0}
{"epoch": 29, "training_loss": 340.3630633354169, "training_acc": 58.0, "val_loss": 83.12528133392334, "val_acc": 72.0}
{"epoch": 30, "training_loss": 435.5001792907715, "training_acc": 52.0, "val_loss": 126.76985263824463, "val_acc": 72.0}
{"epoch": 31, "training_loss": 893.2621307373047, "training_acc": 72.0, "val_loss": 185.53801774978638, "val_acc": 72.0}
{"epoch": 32, "training_loss": 799.5475082397461, "training_acc": 50.0, "val_loss": 20.411086082458496, "val_acc": 48.0}
{"epoch": 33, "training_loss": 740.3749732971191, "training_acc": 65.0, "val_loss": 366.66271686553955, "val_acc": 72.0}
{"epoch": 34, "training_loss": 1187.9744396209717, "training_acc": 72.0, "val_loss": 167.53350496292114, "val_acc": 28.0}
{"epoch": 35, "training_loss": 494.28997754816373, "training_acc": 46.0, "val_loss": 134.0356707572937, "val_acc": 72.0}
{"epoch": 36, "training_loss": 452.70145869255066, "training_acc": 72.0, "val_loss": 324.1210460662842, "val_acc": 28.0}
{"epoch": 37, "training_loss": 935.2097396850586, "training_acc": 34.0, "val_loss": 188.1032109260559, "val_acc": 72.0}
{"epoch": 38, "training_loss": 522.0769348144531, "training_acc": 70.0, "val_loss": 115.93480110168457, "val_acc": 28.0}
{"epoch": 39, "training_loss": 547.5160446166992, "training_acc": 56.0, "val_loss": 190.80623388290405, "val_acc": 72.0}
{"epoch": 40, "training_loss": 499.1424198150635, "training_acc": 72.0, "val_loss": 185.54080724716187, "val_acc": 28.0}
{"epoch": 41, "training_loss": 368.3960304260254, "training_acc": 63.0, "val_loss": 16.213785111904144, "val_acc": 68.0}
{"epoch": 42, "training_loss": 195.98178979428485, "training_acc": 63.0, "val_loss": 73.799067735672, "val_acc": 72.0}
{"epoch": 43, "training_loss": 208.5258687734604, "training_acc": 60.0, "val_loss": 19.767338037490845, "val_acc": 76.0}
{"epoch": 44, "training_loss": 72.7735915184021, "training_acc": 73.0, "val_loss": 13.933570683002472, "val_acc": 72.0}
{"epoch": 45, "training_loss": 61.44943046569824, "training_acc": 69.0, "val_loss": 29.563945531845093, "val_acc": 40.0}
{"epoch": 46, "training_loss": 87.99627494812012, "training_acc": 66.0, "val_loss": 67.8756594657898, "val_acc": 28.0}
{"epoch": 47, "training_loss": 342.819372177124, "training_acc": 58.0, "val_loss": 13.76989483833313, "val_acc": 76.0}
{"epoch": 48, "training_loss": 264.6780891418457, "training_acc": 54.0, "val_loss": 245.74363231658936, "val_acc": 72.0}
{"epoch": 49, "training_loss": 1060.5221557617188, "training_acc": 72.0, "val_loss": 191.08620882034302, "val_acc": 72.0}
{"epoch": 50, "training_loss": 343.65502166748047, "training_acc": 72.0, "val_loss": 446.0969924926758, "val_acc": 28.0}
{"epoch": 51, "training_loss": 970.2259368896484, "training_acc": 46.0, "val_loss": 353.8128614425659, "val_acc": 72.0}
{"epoch": 52, "training_loss": 1460.8933410644531, "training_acc": 72.0, "val_loss": 263.8207674026489, "val_acc": 72.0}
{"epoch": 53, "training_loss": 624.4577827453613, "training_acc": 73.0, "val_loss": 454.5886516571045, "val_acc": 28.0}
{"epoch": 54, "training_loss": 1225.8338623046875, "training_acc": 36.0, "val_loss": 316.9679880142212, "val_acc": 72.0}
{"epoch": 55, "training_loss": 1076.3945236206055, "training_acc": 72.0, "val_loss": 141.55398607254028, "val_acc": 28.0}
{"epoch": 56, "training_loss": 583.9324150085449, "training_acc": 30.0, "val_loss": 236.4971160888672, "val_acc": 72.0}
{"epoch": 57, "training_loss": 863.268560290122, "training_acc": 72.0, "val_loss": 20.21549940109253, "val_acc": 76.0}
{"epoch": 58, "training_loss": 472.0714359283447, "training_acc": 42.0, "val_loss": 177.26274728775024, "val_acc": 72.0}
{"epoch": 59, "training_loss": 670.1191806793213, "training_acc": 72.0, "val_loss": 134.97315645217896, "val_acc": 28.0}
{"epoch": 60, "training_loss": 491.65459632873535, "training_acc": 42.0, "val_loss": 191.05355739593506, "val_acc": 72.0}
{"epoch": 61, "training_loss": 565.9349858760834, "training_acc": 73.0, "val_loss": 152.8098464012146, "val_acc": 28.0}
{"epoch": 62, "training_loss": 307.09892880913685, "training_acc": 58.0, "val_loss": 109.38042402267456, "val_acc": 72.0}
{"epoch": 63, "training_loss": 314.6350679397583, "training_acc": 55.0, "val_loss": 78.46822142601013, "val_acc": 72.0}
{"epoch": 64, "training_loss": 278.9051653146744, "training_acc": 74.0, "val_loss": 137.70912885665894, "val_acc": 28.0}
{"epoch": 65, "training_loss": 266.54689025878906, "training_acc": 61.0, "val_loss": 29.277759790420532, "val_acc": 52.0}
{"epoch": 66, "training_loss": 147.8556251525879, "training_acc": 60.0, "val_loss": 34.474241733551025, "val_acc": 72.0}
