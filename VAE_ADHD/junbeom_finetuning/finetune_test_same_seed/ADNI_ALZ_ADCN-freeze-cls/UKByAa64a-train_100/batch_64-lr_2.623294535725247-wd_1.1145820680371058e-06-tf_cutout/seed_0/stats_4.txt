"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 3825.0646209716797, "training_acc": 69.0, "val_loss": 1953.243637084961, "val_acc": 72.0}
{"epoch": 1, "training_loss": 5688.555046081543, "training_acc": 72.0, "val_loss": 2368.441390991211, "val_acc": 28.0}
{"epoch": 2, "training_loss": 10239.853454589844, "training_acc": 28.0, "val_loss": 288.32428455352783, "val_acc": 60.0}
{"epoch": 3, "training_loss": 2222.619613647461, "training_acc": 64.0, "val_loss": 1697.6434707641602, "val_acc": 72.0}
{"epoch": 4, "training_loss": 6681.466064453125, "training_acc": 72.0, "val_loss": 1821.4841842651367, "val_acc": 72.0}
{"epoch": 5, "training_loss": 5947.981002807617, "training_acc": 72.0, "val_loss": 869.3256378173828, "val_acc": 72.0}
{"epoch": 6, "training_loss": 2312.5927810668945, "training_acc": 69.0, "val_loss": 732.3233604431152, "val_acc": 52.0}
{"epoch": 7, "training_loss": 4100.893783569336, "training_acc": 47.0, "val_loss": 467.25244522094727, "val_acc": 64.0}
{"epoch": 8, "training_loss": 1931.6706466674805, "training_acc": 64.0, "val_loss": 909.1529846191406, "val_acc": 72.0}
{"epoch": 9, "training_loss": 2721.564811706543, "training_acc": 72.0, "val_loss": 784.1584205627441, "val_acc": 72.0}
{"epoch": 10, "training_loss": 1830.0847473144531, "training_acc": 75.0, "val_loss": 351.5941619873047, "val_acc": 64.0}
{"epoch": 11, "training_loss": 1760.452507019043, "training_acc": 58.0, "val_loss": 281.9108724594116, "val_acc": 60.0}
{"epoch": 12, "training_loss": 1056.1615867614746, "training_acc": 68.0, "val_loss": 568.6861038208008, "val_acc": 72.0}
{"epoch": 13, "training_loss": 1735.6135330200195, "training_acc": 72.0, "val_loss": 251.857328414917, "val_acc": 72.0}
{"epoch": 14, "training_loss": 668.2558708190918, "training_acc": 71.0, "val_loss": 368.5624361038208, "val_acc": 36.0}
{"epoch": 15, "training_loss": 1265.3744049072266, "training_acc": 47.0, "val_loss": 250.55482387542725, "val_acc": 72.0}
{"epoch": 16, "training_loss": 804.9966802597046, "training_acc": 72.0, "val_loss": 269.7702884674072, "val_acc": 40.0}
{"epoch": 17, "training_loss": 727.694230556488, "training_acc": 55.0, "val_loss": 229.19766902923584, "val_acc": 72.0}
{"epoch": 18, "training_loss": 858.5087242126465, "training_acc": 72.0, "val_loss": 64.51993584632874, "val_acc": 72.0}
{"epoch": 19, "training_loss": 429.58449935913086, "training_acc": 69.0, "val_loss": 72.43044972419739, "val_acc": 60.0}
{"epoch": 20, "training_loss": 263.5757431983948, "training_acc": 74.0, "val_loss": 182.43703842163086, "val_acc": 72.0}
{"epoch": 21, "training_loss": 509.3072199821472, "training_acc": 75.0, "val_loss": 61.20937466621399, "val_acc": 64.0}
{"epoch": 22, "training_loss": 259.79317474365234, "training_acc": 74.0, "val_loss": 100.63536167144775, "val_acc": 76.0}
{"epoch": 23, "training_loss": 263.6626024246216, "training_acc": 78.0, "val_loss": 297.66650199890137, "val_acc": 32.0}
{"epoch": 24, "training_loss": 686.7397556304932, "training_acc": 55.0, "val_loss": 31.02891743183136, "val_acc": 84.0}
{"epoch": 25, "training_loss": 132.94906330108643, "training_acc": 74.0, "val_loss": 103.3193826675415, "val_acc": 72.0}
{"epoch": 26, "training_loss": 309.7168974876404, "training_acc": 77.0, "val_loss": 150.62700510025024, "val_acc": 48.0}
{"epoch": 27, "training_loss": 254.5356366634369, "training_acc": 70.0, "val_loss": 211.33496761322021, "val_acc": 72.0}
{"epoch": 28, "training_loss": 626.4376955032349, "training_acc": 72.0, "val_loss": 158.96601676940918, "val_acc": 44.0}
{"epoch": 29, "training_loss": 538.4509296417236, "training_acc": 63.0, "val_loss": 242.09628105163574, "val_acc": 72.0}
{"epoch": 30, "training_loss": 755.4004154205322, "training_acc": 73.0, "val_loss": 132.61091709136963, "val_acc": 76.0}
{"epoch": 31, "training_loss": 348.2644443511963, "training_acc": 69.0, "val_loss": 111.32954359054565, "val_acc": 76.0}
{"epoch": 32, "training_loss": 290.2282876968384, "training_acc": 80.0, "val_loss": 207.21065998077393, "val_acc": 76.0}
{"epoch": 33, "training_loss": 340.5972809791565, "training_acc": 82.0, "val_loss": 79.50878143310547, "val_acc": 68.0}
{"epoch": 34, "training_loss": 261.2494959831238, "training_acc": 76.0, "val_loss": 133.79037380218506, "val_acc": 76.0}
{"epoch": 35, "training_loss": 234.70556044578552, "training_acc": 79.0, "val_loss": 69.4478690624237, "val_acc": 48.0}
{"epoch": 36, "training_loss": 289.81062030792236, "training_acc": 67.0, "val_loss": 234.62748527526855, "val_acc": 72.0}
{"epoch": 37, "training_loss": 623.056957244873, "training_acc": 75.0, "val_loss": 357.2876214981079, "val_acc": 28.0}
{"epoch": 38, "training_loss": 925.6988134384155, "training_acc": 52.0, "val_loss": 197.42799997329712, "val_acc": 72.0}
{"epoch": 39, "training_loss": 439.54166412353516, "training_acc": 76.0, "val_loss": 246.9944953918457, "val_acc": 36.0}
{"epoch": 40, "training_loss": 729.3059787750244, "training_acc": 62.0, "val_loss": 227.93962955474854, "val_acc": 72.0}
{"epoch": 41, "training_loss": 583.4607286453247, "training_acc": 75.0, "val_loss": 81.12637400627136, "val_acc": 76.0}
{"epoch": 42, "training_loss": 167.80964374542236, "training_acc": 79.0, "val_loss": 52.316516637802124, "val_acc": 80.0}
{"epoch": 43, "training_loss": 247.5050506591797, "training_acc": 76.0, "val_loss": 90.5381977558136, "val_acc": 76.0}
