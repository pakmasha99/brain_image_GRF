"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 4293.410045623779, "training_acc": 72.0, "val_loss": 1880.142593383789, "val_acc": 72.0}
{"epoch": 1, "training_loss": 5121.848838806152, "training_acc": 72.0, "val_loss": 2948.792839050293, "val_acc": 28.0}
{"epoch": 2, "training_loss": 12340.089172363281, "training_acc": 28.0, "val_loss": 536.7355823516846, "val_acc": 36.0}
{"epoch": 3, "training_loss": 3265.797821044922, "training_acc": 51.0, "val_loss": 1681.148910522461, "val_acc": 72.0}
{"epoch": 4, "training_loss": 7563.764343261719, "training_acc": 72.0, "val_loss": 2244.3700790405273, "val_acc": 72.0}
{"epoch": 5, "training_loss": 8958.932678222656, "training_acc": 72.0, "val_loss": 1661.1618041992188, "val_acc": 72.0}
{"epoch": 6, "training_loss": 5840.281478881836, "training_acc": 72.0, "val_loss": 365.3247594833374, "val_acc": 72.0}
{"epoch": 7, "training_loss": 3038.219757080078, "training_acc": 65.0, "val_loss": 649.3045330047607, "val_acc": 52.0}
{"epoch": 8, "training_loss": 4805.0358810424805, "training_acc": 47.0, "val_loss": 219.38066482543945, "val_acc": 80.0}
{"epoch": 9, "training_loss": 2250.095355987549, "training_acc": 71.0, "val_loss": 577.120304107666, "val_acc": 72.0}
{"epoch": 10, "training_loss": 2566.233985900879, "training_acc": 73.0, "val_loss": 675.2407550811768, "val_acc": 72.0}
{"epoch": 11, "training_loss": 2158.1484146118164, "training_acc": 71.0, "val_loss": 138.4047031402588, "val_acc": 80.0}
{"epoch": 12, "training_loss": 1411.8880996704102, "training_acc": 61.0, "val_loss": 205.76870441436768, "val_acc": 64.0}
{"epoch": 13, "training_loss": 946.9370746612549, "training_acc": 67.0, "val_loss": 435.6971263885498, "val_acc": 72.0}
{"epoch": 14, "training_loss": 1566.9235305786133, "training_acc": 72.0, "val_loss": 281.14328384399414, "val_acc": 72.0}
{"epoch": 15, "training_loss": 914.9798011779785, "training_acc": 63.0, "val_loss": 231.88443183898926, "val_acc": 56.0}
{"epoch": 16, "training_loss": 1075.2421264648438, "training_acc": 60.0, "val_loss": 379.44462299346924, "val_acc": 72.0}
{"epoch": 17, "training_loss": 1047.7654972076416, "training_acc": 69.0, "val_loss": 269.7699308395386, "val_acc": 44.0}
{"epoch": 18, "training_loss": 786.4418730735779, "training_acc": 52.0, "val_loss": 206.8004846572876, "val_acc": 72.0}
{"epoch": 19, "training_loss": 568.961953163147, "training_acc": 72.0, "val_loss": 59.94923114776611, "val_acc": 72.0}
{"epoch": 20, "training_loss": 460.3326473236084, "training_acc": 67.0, "val_loss": 89.56187963485718, "val_acc": 76.0}
{"epoch": 21, "training_loss": 641.2967376708984, "training_acc": 77.0, "val_loss": 71.99448943138123, "val_acc": 80.0}
{"epoch": 22, "training_loss": 581.8808212280273, "training_acc": 67.0, "val_loss": 57.15894103050232, "val_acc": 72.0}
{"epoch": 23, "training_loss": 515.0317802429199, "training_acc": 73.0, "val_loss": 295.63539028167725, "val_acc": 72.0}
{"epoch": 24, "training_loss": 889.4787883758545, "training_acc": 72.0, "val_loss": 291.63169860839844, "val_acc": 44.0}
{"epoch": 25, "training_loss": 798.8120536804199, "training_acc": 48.0, "val_loss": 315.3282642364502, "val_acc": 72.0}
{"epoch": 26, "training_loss": 1288.6918907165527, "training_acc": 72.0, "val_loss": 336.633563041687, "val_acc": 72.0}
{"epoch": 27, "training_loss": 844.0314617156982, "training_acc": 77.0, "val_loss": 461.6954803466797, "val_acc": 36.0}
{"epoch": 28, "training_loss": 1493.947504043579, "training_acc": 40.0, "val_loss": 350.38585662841797, "val_acc": 72.0}
{"epoch": 29, "training_loss": 1561.9326095581055, "training_acc": 72.0, "val_loss": 576.0476112365723, "val_acc": 72.0}
{"epoch": 30, "training_loss": 1791.9136695861816, "training_acc": 72.0, "val_loss": 66.9215202331543, "val_acc": 76.0}
{"epoch": 31, "training_loss": 756.6970062255859, "training_acc": 70.0, "val_loss": 192.58065223693848, "val_acc": 56.0}
{"epoch": 32, "training_loss": 751.027494430542, "training_acc": 69.0, "val_loss": 368.7464237213135, "val_acc": 72.0}
{"epoch": 33, "training_loss": 1123.9706993103027, "training_acc": 74.0, "val_loss": 131.12057447433472, "val_acc": 76.0}
{"epoch": 34, "training_loss": 386.8191728591919, "training_acc": 73.0, "val_loss": 248.03612232208252, "val_acc": 48.0}
{"epoch": 35, "training_loss": 899.932131767273, "training_acc": 57.0, "val_loss": 315.6824588775635, "val_acc": 72.0}
{"epoch": 36, "training_loss": 801.6077919006348, "training_acc": 75.0, "val_loss": 89.91503715515137, "val_acc": 60.0}
{"epoch": 37, "training_loss": 343.51289558410645, "training_acc": 71.0, "val_loss": 114.55472707748413, "val_acc": 72.0}
{"epoch": 38, "training_loss": 277.2734775543213, "training_acc": 80.0, "val_loss": 56.02688789367676, "val_acc": 72.0}
{"epoch": 39, "training_loss": 130.6616930961609, "training_acc": 78.0, "val_loss": 61.277568340301514, "val_acc": 72.0}
{"epoch": 40, "training_loss": 201.75263595581055, "training_acc": 80.0, "val_loss": 87.0657205581665, "val_acc": 60.0}
{"epoch": 41, "training_loss": 165.51316118240356, "training_acc": 75.0, "val_loss": 166.97858572006226, "val_acc": 72.0}
{"epoch": 42, "training_loss": 484.3743619918823, "training_acc": 76.0, "val_loss": 88.68807554244995, "val_acc": 60.0}
{"epoch": 43, "training_loss": 174.31649780273438, "training_acc": 74.0, "val_loss": 70.49261331558228, "val_acc": 76.0}
{"epoch": 44, "training_loss": 185.68639087677002, "training_acc": 80.0, "val_loss": 141.3141131401062, "val_acc": 56.0}
{"epoch": 45, "training_loss": 358.4835453033447, "training_acc": 63.0, "val_loss": 250.6892204284668, "val_acc": 72.0}
{"epoch": 46, "training_loss": 900.349910736084, "training_acc": 72.0, "val_loss": 191.88793897628784, "val_acc": 72.0}
{"epoch": 47, "training_loss": 580.416955947876, "training_acc": 71.0, "val_loss": 198.3199119567871, "val_acc": 48.0}
{"epoch": 48, "training_loss": 458.7328462600708, "training_acc": 65.0, "val_loss": 279.16603088378906, "val_acc": 72.0}
{"epoch": 49, "training_loss": 692.1257762908936, "training_acc": 73.0, "val_loss": 140.8570647239685, "val_acc": 52.0}
{"epoch": 50, "training_loss": 399.49842739105225, "training_acc": 63.0, "val_loss": 112.50576972961426, "val_acc": 72.0}
{"epoch": 51, "training_loss": 301.91681575775146, "training_acc": 81.0, "val_loss": 72.95350432395935, "val_acc": 72.0}
{"epoch": 52, "training_loss": 281.0017852783203, "training_acc": 78.0, "val_loss": 59.99422073364258, "val_acc": 72.0}
{"epoch": 53, "training_loss": 134.74048805236816, "training_acc": 85.0, "val_loss": 112.00705766677856, "val_acc": 72.0}
{"epoch": 54, "training_loss": 243.29789972305298, "training_acc": 79.0, "val_loss": 68.40949058532715, "val_acc": 68.0}
{"epoch": 55, "training_loss": 204.02486896514893, "training_acc": 76.0, "val_loss": 115.20310640335083, "val_acc": 72.0}
{"epoch": 56, "training_loss": 291.05337047576904, "training_acc": 74.0, "val_loss": 67.12126135826111, "val_acc": 64.0}
{"epoch": 57, "training_loss": 217.04231548309326, "training_acc": 79.0, "val_loss": 204.41882610321045, "val_acc": 72.0}
