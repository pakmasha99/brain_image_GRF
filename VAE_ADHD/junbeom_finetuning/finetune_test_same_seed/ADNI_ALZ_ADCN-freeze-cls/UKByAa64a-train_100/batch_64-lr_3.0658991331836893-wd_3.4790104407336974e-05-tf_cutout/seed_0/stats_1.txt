"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 2144.498394012451, "training_acc": 65.0, "val_loss": 2674.276351928711, "val_acc": 72.0}
{"epoch": 1, "training_loss": 7232.498062133789, "training_acc": 72.0, "val_loss": 3378.8192749023438, "val_acc": 28.0}
{"epoch": 2, "training_loss": 12012.117279052734, "training_acc": 28.0, "val_loss": 311.9606018066406, "val_acc": 68.0}
{"epoch": 3, "training_loss": 2134.852737426758, "training_acc": 75.0, "val_loss": 1859.242820739746, "val_acc": 72.0}
{"epoch": 4, "training_loss": 7316.634094238281, "training_acc": 72.0, "val_loss": 1729.8040390014648, "val_acc": 72.0}
{"epoch": 5, "training_loss": 6213.828193664551, "training_acc": 72.0, "val_loss": 500.92029571533203, "val_acc": 72.0}
{"epoch": 6, "training_loss": 2592.595703125, "training_acc": 66.0, "val_loss": 980.1366806030273, "val_acc": 60.0}
{"epoch": 7, "training_loss": 3610.625015258789, "training_acc": 53.0, "val_loss": 430.40571212768555, "val_acc": 72.0}
{"epoch": 8, "training_loss": 1915.4522094726562, "training_acc": 76.0, "val_loss": 867.5180435180664, "val_acc": 72.0}
{"epoch": 9, "training_loss": 2988.8735122680664, "training_acc": 72.0, "val_loss": 311.2816333770752, "val_acc": 72.0}
{"epoch": 10, "training_loss": 1706.633399963379, "training_acc": 59.0, "val_loss": 338.5676383972168, "val_acc": 56.0}
{"epoch": 11, "training_loss": 1385.7854595184326, "training_acc": 67.0, "val_loss": 369.97320652008057, "val_acc": 72.0}
{"epoch": 12, "training_loss": 1116.823052406311, "training_acc": 72.0, "val_loss": 192.93837547302246, "val_acc": 56.0}
{"epoch": 13, "training_loss": 549.4607286453247, "training_acc": 61.0, "val_loss": 284.6851110458374, "val_acc": 72.0}
{"epoch": 14, "training_loss": 635.3547868728638, "training_acc": 70.0, "val_loss": 330.43131828308105, "val_acc": 28.0}
{"epoch": 15, "training_loss": 720.5373573303223, "training_acc": 61.0, "val_loss": 413.1394863128662, "val_acc": 72.0}
{"epoch": 16, "training_loss": 966.7438430786133, "training_acc": 70.0, "val_loss": 362.2483491897583, "val_acc": 36.0}
{"epoch": 17, "training_loss": 966.3273296356201, "training_acc": 48.0, "val_loss": 326.56469345092773, "val_acc": 72.0}
{"epoch": 18, "training_loss": 612.8029890060425, "training_acc": 68.0, "val_loss": 159.7172975540161, "val_acc": 56.0}
{"epoch": 19, "training_loss": 420.84410285949707, "training_acc": 70.0, "val_loss": 158.40847492218018, "val_acc": 68.0}
{"epoch": 20, "training_loss": 287.01005363464355, "training_acc": 72.0, "val_loss": 162.7850890159607, "val_acc": 60.0}
{"epoch": 21, "training_loss": 379.7742910385132, "training_acc": 69.0, "val_loss": 177.92447805404663, "val_acc": 76.0}
{"epoch": 22, "training_loss": 439.60492515563965, "training_acc": 73.0, "val_loss": 146.1805820465088, "val_acc": 68.0}
{"epoch": 23, "training_loss": 171.85845685005188, "training_acc": 78.0, "val_loss": 129.9225091934204, "val_acc": 48.0}
{"epoch": 24, "training_loss": 191.06590270996094, "training_acc": 74.0, "val_loss": 166.24916791915894, "val_acc": 48.0}
{"epoch": 25, "training_loss": 192.12394046783447, "training_acc": 72.0, "val_loss": 114.50893878936768, "val_acc": 68.0}
{"epoch": 26, "training_loss": 149.75073337554932, "training_acc": 81.0, "val_loss": 225.79612731933594, "val_acc": 72.0}
{"epoch": 27, "training_loss": 482.37585830688477, "training_acc": 75.0, "val_loss": 262.36186027526855, "val_acc": 44.0}
{"epoch": 28, "training_loss": 553.0089254379272, "training_acc": 62.0, "val_loss": 374.603533744812, "val_acc": 72.0}
{"epoch": 29, "training_loss": 1085.4169025421143, "training_acc": 72.0, "val_loss": 127.44163274765015, "val_acc": 72.0}
{"epoch": 30, "training_loss": 753.0143890380859, "training_acc": 66.0, "val_loss": 120.2677845954895, "val_acc": 64.0}
{"epoch": 31, "training_loss": 598.1084060668945, "training_acc": 75.0, "val_loss": 406.8401336669922, "val_acc": 72.0}
{"epoch": 32, "training_loss": 957.0630168914795, "training_acc": 75.0, "val_loss": 362.20648288726807, "val_acc": 44.0}
{"epoch": 33, "training_loss": 968.0646629333496, "training_acc": 51.0, "val_loss": 418.5028553009033, "val_acc": 72.0}
{"epoch": 34, "training_loss": 1561.9658966064453, "training_acc": 72.0, "val_loss": 304.53102588653564, "val_acc": 72.0}
{"epoch": 35, "training_loss": 980.19921875, "training_acc": 65.0, "val_loss": 230.0617218017578, "val_acc": 52.0}
{"epoch": 36, "training_loss": 754.3154773712158, "training_acc": 67.0, "val_loss": 366.9997453689575, "val_acc": 72.0}
{"epoch": 37, "training_loss": 1002.5113639831543, "training_acc": 73.0, "val_loss": 222.48690128326416, "val_acc": 44.0}
{"epoch": 38, "training_loss": 834.8395118713379, "training_acc": 56.0, "val_loss": 341.26853942871094, "val_acc": 72.0}
{"epoch": 39, "training_loss": 1363.7760925292969, "training_acc": 72.0, "val_loss": 425.24194717407227, "val_acc": 72.0}
{"epoch": 40, "training_loss": 1171.053629875183, "training_acc": 71.0, "val_loss": 446.8040943145752, "val_acc": 40.0}
{"epoch": 41, "training_loss": 1224.4530334472656, "training_acc": 55.0, "val_loss": 267.5286293029785, "val_acc": 76.0}
{"epoch": 42, "training_loss": 736.6554069519043, "training_acc": 77.0, "val_loss": 118.5897707939148, "val_acc": 68.0}
{"epoch": 43, "training_loss": 506.32608222961426, "training_acc": 69.0, "val_loss": 119.75066661834717, "val_acc": 72.0}
{"epoch": 44, "training_loss": 317.321626663208, "training_acc": 78.0, "val_loss": 234.84790325164795, "val_acc": 76.0}
