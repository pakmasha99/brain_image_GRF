"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 471.7826385498047, "training_acc": 72.0, "val_loss": 316.29159450531006, "val_acc": 72.0}
{"epoch": 1, "training_loss": 895.6045520305634, "training_acc": 71.0, "val_loss": 519.1322326660156, "val_acc": 28.0}
{"epoch": 2, "training_loss": 1597.0647163391113, "training_acc": 28.0, "val_loss": 109.8363995552063, "val_acc": 72.0}
{"epoch": 3, "training_loss": 644.2939376831055, "training_acc": 72.0, "val_loss": 329.5609712600708, "val_acc": 72.0}
{"epoch": 4, "training_loss": 1267.6009559631348, "training_acc": 72.0, "val_loss": 282.4559211730957, "val_acc": 72.0}
{"epoch": 5, "training_loss": 985.5286636352539, "training_acc": 72.0, "val_loss": 78.6586582660675, "val_acc": 76.0}
{"epoch": 6, "training_loss": 431.4834899902344, "training_acc": 59.0, "val_loss": 220.4782009124756, "val_acc": 48.0}
{"epoch": 7, "training_loss": 870.4654064178467, "training_acc": 42.0, "val_loss": 65.0285542011261, "val_acc": 68.0}
{"epoch": 8, "training_loss": 317.8628339767456, "training_acc": 71.0, "val_loss": 155.64628839492798, "val_acc": 72.0}
{"epoch": 9, "training_loss": 640.1115236282349, "training_acc": 72.0, "val_loss": 136.48736476898193, "val_acc": 72.0}
{"epoch": 10, "training_loss": 475.4700002670288, "training_acc": 74.0, "val_loss": 53.037601709365845, "val_acc": 64.0}
{"epoch": 11, "training_loss": 315.0093460083008, "training_acc": 66.0, "val_loss": 99.95693564414978, "val_acc": 60.0}
{"epoch": 12, "training_loss": 300.8988666534424, "training_acc": 58.0, "val_loss": 55.91309070587158, "val_acc": 80.0}
{"epoch": 13, "training_loss": 243.54426193237305, "training_acc": 76.0, "val_loss": 83.49770307540894, "val_acc": 72.0}
{"epoch": 14, "training_loss": 254.52606916427612, "training_acc": 73.0, "val_loss": 38.48971128463745, "val_acc": 60.0}
{"epoch": 15, "training_loss": 202.7303762435913, "training_acc": 52.0, "val_loss": 20.42985111474991, "val_acc": 68.0}
{"epoch": 16, "training_loss": 117.65997314453125, "training_acc": 75.0, "val_loss": 58.32099914550781, "val_acc": 72.0}
{"epoch": 17, "training_loss": 136.5761215686798, "training_acc": 74.0, "val_loss": 71.11332416534424, "val_acc": 36.0}
{"epoch": 18, "training_loss": 174.75396251678467, "training_acc": 50.0, "val_loss": 57.89722800254822, "val_acc": 72.0}
{"epoch": 19, "training_loss": 186.99182796478271, "training_acc": 72.0, "val_loss": 50.04122853279114, "val_acc": 72.0}
{"epoch": 20, "training_loss": 120.4100410938263, "training_acc": 68.0, "val_loss": 54.070258140563965, "val_acc": 36.0}
{"epoch": 21, "training_loss": 122.27812695503235, "training_acc": 63.0, "val_loss": 48.90240430831909, "val_acc": 72.0}
{"epoch": 22, "training_loss": 122.06152844429016, "training_acc": 72.0, "val_loss": 33.85418653488159, "val_acc": 52.0}
{"epoch": 23, "training_loss": 102.67738723754883, "training_acc": 64.0, "val_loss": 35.069113969802856, "val_acc": 76.0}
{"epoch": 24, "training_loss": 134.115647315979, "training_acc": 74.0, "val_loss": 28.185507655143738, "val_acc": 76.0}
{"epoch": 25, "training_loss": 132.68098163604736, "training_acc": 65.0, "val_loss": 26.540887355804443, "val_acc": 64.0}
{"epoch": 26, "training_loss": 64.00713515281677, "training_acc": 76.0, "val_loss": 47.55549430847168, "val_acc": 72.0}
{"epoch": 27, "training_loss": 111.59655046463013, "training_acc": 73.0, "val_loss": 31.234729290008545, "val_acc": 56.0}
{"epoch": 28, "training_loss": 85.23846244812012, "training_acc": 67.0, "val_loss": 29.102659225463867, "val_acc": 76.0}
{"epoch": 29, "training_loss": 73.76446390151978, "training_acc": 81.0, "val_loss": 21.36985957622528, "val_acc": 76.0}
{"epoch": 30, "training_loss": 48.183149576187134, "training_acc": 76.0, "val_loss": 21.226882934570312, "val_acc": 64.0}
{"epoch": 31, "training_loss": 44.780253171920776, "training_acc": 81.0, "val_loss": 23.155300319194794, "val_acc": 68.0}
{"epoch": 32, "training_loss": 43.621739864349365, "training_acc": 81.0, "val_loss": 25.807181000709534, "val_acc": 76.0}
{"epoch": 33, "training_loss": 45.22501873970032, "training_acc": 81.0, "val_loss": 23.63087832927704, "val_acc": 48.0}
{"epoch": 34, "training_loss": 40.7297477722168, "training_acc": 79.0, "val_loss": 20.72663903236389, "val_acc": 72.0}
