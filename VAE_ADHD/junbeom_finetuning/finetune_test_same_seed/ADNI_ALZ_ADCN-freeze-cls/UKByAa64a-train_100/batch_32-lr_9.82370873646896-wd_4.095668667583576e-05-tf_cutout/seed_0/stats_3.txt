"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 32 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 20639.13311767578, "training_acc": 62.0, "val_loss": 2953.8259506225586, "val_acc": 32.0}
{"epoch": 1, "training_loss": 18213.778911590576, "training_acc": 32.0, "val_loss": 3090.2753829956055, "val_acc": 68.0}
{"epoch": 2, "training_loss": 17984.610229492188, "training_acc": 72.0, "val_loss": 4477.941131591797, "val_acc": 68.0}
{"epoch": 3, "training_loss": 10113.5595703125, "training_acc": 68.0, "val_loss": 5482.344436645508, "val_acc": 64.0}
{"epoch": 4, "training_loss": 14198.82421875, "training_acc": 54.0, "val_loss": 4043.8011169433594, "val_acc": 72.0}
{"epoch": 5, "training_loss": 8223.227905273438, "training_acc": 74.0, "val_loss": 3453.4671783447266, "val_acc": 68.0}
{"epoch": 6, "training_loss": 9574.458530426025, "training_acc": 72.0, "val_loss": 2108.2216262817383, "val_acc": 68.0}
{"epoch": 7, "training_loss": 5480.895355224609, "training_acc": 61.0, "val_loss": 849.5092391967773, "val_acc": 72.0}
{"epoch": 8, "training_loss": 1551.3918914794922, "training_acc": 73.0, "val_loss": 2200.0654220581055, "val_acc": 32.0}
{"epoch": 9, "training_loss": 5845.62890625, "training_acc": 54.0, "val_loss": 2572.785758972168, "val_acc": 72.0}
{"epoch": 10, "training_loss": 9588.945678710938, "training_acc": 61.0, "val_loss": 2270.168685913086, "val_acc": 32.0}
{"epoch": 11, "training_loss": 5842.474060058594, "training_acc": 62.0, "val_loss": 831.7543983459473, "val_acc": 76.0}
{"epoch": 12, "training_loss": 10126.827819824219, "training_acc": 48.0, "val_loss": 1671.804428100586, "val_acc": 76.0}
{"epoch": 13, "training_loss": 14582.521484375, "training_acc": 73.0, "val_loss": 6288.95149230957, "val_acc": 72.0}
{"epoch": 14, "training_loss": 20975.93505859375, "training_acc": 72.0, "val_loss": 2090.226936340332, "val_acc": 68.0}
{"epoch": 15, "training_loss": 13128.268188476562, "training_acc": 56.0, "val_loss": 2692.3728942871094, "val_acc": 64.0}
{"epoch": 16, "training_loss": 10515.457641601562, "training_acc": 68.0, "val_loss": 3440.596389770508, "val_acc": 72.0}
{"epoch": 17, "training_loss": 9107.013000488281, "training_acc": 69.0, "val_loss": 2780.3442001342773, "val_acc": 64.0}
{"epoch": 18, "training_loss": 7484.683441162109, "training_acc": 69.0, "val_loss": 2774.822425842285, "val_acc": 72.0}
{"epoch": 19, "training_loss": 8441.891387939453, "training_acc": 69.0, "val_loss": 1781.3879013061523, "val_acc": 64.0}
{"epoch": 20, "training_loss": 2713.568950653076, "training_acc": 78.0, "val_loss": 870.5806732177734, "val_acc": 80.0}
{"epoch": 21, "training_loss": 2206.331344604492, "training_acc": 74.0, "val_loss": 209.81740951538086, "val_acc": 80.0}
{"epoch": 22, "training_loss": 1300.9821319580078, "training_acc": 73.0, "val_loss": 1405.4240226745605, "val_acc": 36.0}
{"epoch": 23, "training_loss": 2727.2078552246094, "training_acc": 67.0, "val_loss": 627.4396896362305, "val_acc": 72.0}
{"epoch": 24, "training_loss": 2091.609735161066, "training_acc": 68.0, "val_loss": 391.0933256149292, "val_acc": 68.0}
{"epoch": 25, "training_loss": 938.9567504054867, "training_acc": 82.0, "val_loss": 473.5762119293213, "val_acc": 72.0}
{"epoch": 26, "training_loss": 1216.7389755249023, "training_acc": 77.0, "val_loss": 964.052677154541, "val_acc": 72.0}
{"epoch": 27, "training_loss": 5168.073233604431, "training_acc": 72.0, "val_loss": 749.7393608093262, "val_acc": 52.0}
{"epoch": 28, "training_loss": 968.9236907958984, "training_acc": 72.0, "val_loss": 835.5498313903809, "val_acc": 72.0}
{"epoch": 29, "training_loss": 4031.7251586914062, "training_acc": 73.0, "val_loss": 4231.008529663086, "val_acc": 28.0}
{"epoch": 30, "training_loss": 8140.6964111328125, "training_acc": 47.0, "val_loss": 3267.7845001220703, "val_acc": 72.0}
{"epoch": 31, "training_loss": 12247.232231140137, "training_acc": 72.0, "val_loss": 729.3160915374756, "val_acc": 52.0}
{"epoch": 32, "training_loss": 5609.617950439453, "training_acc": 47.0, "val_loss": 2552.952003479004, "val_acc": 72.0}
{"epoch": 33, "training_loss": 11333.693359375, "training_acc": 72.0, "val_loss": 631.1994075775146, "val_acc": 76.0}
{"epoch": 34, "training_loss": 4424.089294433594, "training_acc": 54.0, "val_loss": 809.4210624694824, "val_acc": 80.0}
{"epoch": 35, "training_loss": 2968.349054336548, "training_acc": 76.0, "val_loss": 1004.7845840454102, "val_acc": 68.0}
{"epoch": 36, "training_loss": 1665.5722045898438, "training_acc": 78.0, "val_loss": 1156.7684173583984, "val_acc": 60.0}
{"epoch": 37, "training_loss": 2550.6699829101562, "training_acc": 67.0, "val_loss": 1955.8601379394531, "val_acc": 72.0}
{"epoch": 38, "training_loss": 6860.1784591674805, "training_acc": 73.0, "val_loss": 2816.741180419922, "val_acc": 28.0}
{"epoch": 39, "training_loss": 7604.2139892578125, "training_acc": 51.0, "val_loss": 3287.4195098876953, "val_acc": 72.0}
{"epoch": 40, "training_loss": 14787.8544921875, "training_acc": 72.0, "val_loss": 1747.8132247924805, "val_acc": 72.0}
