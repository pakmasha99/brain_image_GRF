"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 409.6883659362793, "training_acc": 42.0, "val_loss": 213.18001747131348, "val_acc": 72.0}
{"epoch": 1, "training_loss": 721.260103225708, "training_acc": 72.0, "val_loss": 31.830433011054993, "val_acc": 28.0}
{"epoch": 2, "training_loss": 116.89495015144348, "training_acc": 28.0, "val_loss": 55.08999228477478, "val_acc": 72.0}
{"epoch": 3, "training_loss": 262.0461711883545, "training_acc": 72.0, "val_loss": 54.18757200241089, "val_acc": 72.0}
{"epoch": 4, "training_loss": 178.73149943351746, "training_acc": 72.0, "val_loss": 50.8364200592041, "val_acc": 28.0}
{"epoch": 5, "training_loss": 157.81474995613098, "training_acc": 46.0, "val_loss": 41.41479730606079, "val_acc": 72.0}
{"epoch": 6, "training_loss": 153.97885036468506, "training_acc": 72.0, "val_loss": 14.996489882469177, "val_acc": 72.0}
{"epoch": 7, "training_loss": 79.09194993972778, "training_acc": 62.0, "val_loss": 16.849341988563538, "val_acc": 72.0}
{"epoch": 8, "training_loss": 74.1103527545929, "training_acc": 72.0, "val_loss": 16.331206262111664, "val_acc": 72.0}
{"epoch": 9, "training_loss": 84.3990330696106, "training_acc": 54.0, "val_loss": 16.63861870765686, "val_acc": 72.0}
{"epoch": 10, "training_loss": 71.24778032302856, "training_acc": 72.0, "val_loss": 15.461641550064087, "val_acc": 72.0}
{"epoch": 11, "training_loss": 68.06393122673035, "training_acc": 58.0, "val_loss": 14.83466923236847, "val_acc": 72.0}
{"epoch": 12, "training_loss": 72.0520248413086, "training_acc": 72.0, "val_loss": 14.51520323753357, "val_acc": 72.0}
{"epoch": 13, "training_loss": 65.61296892166138, "training_acc": 58.0, "val_loss": 16.288505494594574, "val_acc": 72.0}
{"epoch": 14, "training_loss": 69.61960196495056, "training_acc": 72.0, "val_loss": 14.616970717906952, "val_acc": 76.0}
{"epoch": 15, "training_loss": 73.02646565437317, "training_acc": 52.0, "val_loss": 22.610314190387726, "val_acc": 72.0}
{"epoch": 16, "training_loss": 93.94940495491028, "training_acc": 72.0, "val_loss": 16.315318644046783, "val_acc": 72.0}
{"epoch": 17, "training_loss": 81.37537813186646, "training_acc": 58.0, "val_loss": 14.675119519233704, "val_acc": 72.0}
{"epoch": 18, "training_loss": 64.01024580001831, "training_acc": 72.0, "val_loss": 17.28205978870392, "val_acc": 72.0}
{"epoch": 19, "training_loss": 71.18761420249939, "training_acc": 56.0, "val_loss": 14.323954284191132, "val_acc": 72.0}
{"epoch": 20, "training_loss": 56.40945076942444, "training_acc": 72.0, "val_loss": 15.915246307849884, "val_acc": 72.0}
{"epoch": 21, "training_loss": 60.0938503742218, "training_acc": 76.0, "val_loss": 14.750058948993683, "val_acc": 60.0}
{"epoch": 22, "training_loss": 63.11228561401367, "training_acc": 74.0, "val_loss": 14.600615203380585, "val_acc": 72.0}
{"epoch": 23, "training_loss": 68.34663414955139, "training_acc": 52.0, "val_loss": 19.221016764640808, "val_acc": 72.0}
{"epoch": 24, "training_loss": 74.40389752388, "training_acc": 72.0, "val_loss": 14.364489912986755, "val_acc": 72.0}
{"epoch": 25, "training_loss": 63.11547112464905, "training_acc": 56.0, "val_loss": 18.63217055797577, "val_acc": 72.0}
{"epoch": 26, "training_loss": 69.8775429725647, "training_acc": 72.0, "val_loss": 14.186660945415497, "val_acc": 76.0}
{"epoch": 27, "training_loss": 56.5171434879303, "training_acc": 75.0, "val_loss": 14.658688008785248, "val_acc": 72.0}
{"epoch": 28, "training_loss": 61.16799521446228, "training_acc": 72.0, "val_loss": 18.367378413677216, "val_acc": 28.0}
{"epoch": 29, "training_loss": 66.30338191986084, "training_acc": 48.0, "val_loss": 19.029925763607025, "val_acc": 72.0}
{"epoch": 30, "training_loss": 66.60351395606995, "training_acc": 72.0, "val_loss": 16.64937138557434, "val_acc": 28.0}
{"epoch": 31, "training_loss": 65.7403838634491, "training_acc": 60.0, "val_loss": 16.254176199436188, "val_acc": 72.0}
{"epoch": 32, "training_loss": 77.25736594200134, "training_acc": 50.0, "val_loss": 20.567084848880768, "val_acc": 72.0}
{"epoch": 33, "training_loss": 85.20402717590332, "training_acc": 72.0, "val_loss": 14.55485224723816, "val_acc": 56.0}
{"epoch": 34, "training_loss": 66.45582318305969, "training_acc": 58.0, "val_loss": 21.000832319259644, "val_acc": 72.0}
{"epoch": 35, "training_loss": 87.64952373504639, "training_acc": 72.0, "val_loss": 14.23809826374054, "val_acc": 64.0}
{"epoch": 36, "training_loss": 71.35374355316162, "training_acc": 63.0, "val_loss": 22.106292843818665, "val_acc": 72.0}
{"epoch": 37, "training_loss": 93.73707056045532, "training_acc": 72.0, "val_loss": 14.565077424049377, "val_acc": 72.0}
{"epoch": 38, "training_loss": 83.01501846313477, "training_acc": 58.0, "val_loss": 16.71832948923111, "val_acc": 72.0}
{"epoch": 39, "training_loss": 78.30570077896118, "training_acc": 72.0, "val_loss": 16.06214940547943, "val_acc": 72.0}
{"epoch": 40, "training_loss": 70.87100839614868, "training_acc": 60.0, "val_loss": 14.266404509544373, "val_acc": 72.0}
{"epoch": 41, "training_loss": 65.63280963897705, "training_acc": 72.0, "val_loss": 14.420194923877716, "val_acc": 72.0}
{"epoch": 42, "training_loss": 83.81082820892334, "training_acc": 52.0, "val_loss": 22.42574691772461, "val_acc": 72.0}
{"epoch": 43, "training_loss": 85.79402279853821, "training_acc": 72.0, "val_loss": 18.508540093898773, "val_acc": 72.0}
{"epoch": 44, "training_loss": 68.00077867507935, "training_acc": 60.0, "val_loss": 14.210566878318787, "val_acc": 60.0}
{"epoch": 45, "training_loss": 57.58516025543213, "training_acc": 77.0, "val_loss": 20.446807146072388, "val_acc": 72.0}
