"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 16207.915355682373, "training_acc": 72.0, "val_loss": 7149.388885498047, "val_acc": 72.0}
{"epoch": 1, "training_loss": 19473.813049316406, "training_acc": 72.0, "val_loss": 11221.990203857422, "val_acc": 28.0}
{"epoch": 2, "training_loss": 46962.59362792969, "training_acc": 28.0, "val_loss": 2046.1225509643555, "val_acc": 36.0}
{"epoch": 3, "training_loss": 12426.361022949219, "training_acc": 51.0, "val_loss": 6386.994934082031, "val_acc": 72.0}
{"epoch": 4, "training_loss": 28736.045288085938, "training_acc": 72.0, "val_loss": 8523.845672607422, "val_acc": 72.0}
{"epoch": 5, "training_loss": 34023.96270751953, "training_acc": 72.0, "val_loss": 6299.814224243164, "val_acc": 72.0}
{"epoch": 6, "training_loss": 22136.696655273438, "training_acc": 72.0, "val_loss": 1365.244197845459, "val_acc": 72.0}
{"epoch": 7, "training_loss": 11477.388488769531, "training_acc": 64.0, "val_loss": 2374.35245513916, "val_acc": 52.0}
{"epoch": 8, "training_loss": 17899.530334472656, "training_acc": 48.0, "val_loss": 820.0393676757812, "val_acc": 80.0}
{"epoch": 9, "training_loss": 8417.83578491211, "training_acc": 71.0, "val_loss": 2220.6193923950195, "val_acc": 72.0}
{"epoch": 10, "training_loss": 9794.562561035156, "training_acc": 73.0, "val_loss": 2564.7865295410156, "val_acc": 72.0}
{"epoch": 11, "training_loss": 8146.523010253906, "training_acc": 71.0, "val_loss": 520.5041408538818, "val_acc": 80.0}
{"epoch": 12, "training_loss": 5483.604095458984, "training_acc": 60.0, "val_loss": 733.9152812957764, "val_acc": 68.0}
{"epoch": 13, "training_loss": 3382.893928527832, "training_acc": 67.0, "val_loss": 1841.1386489868164, "val_acc": 72.0}
{"epoch": 14, "training_loss": 6801.5047607421875, "training_acc": 72.0, "val_loss": 1353.7550926208496, "val_acc": 72.0}
{"epoch": 15, "training_loss": 3298.261589050293, "training_acc": 68.0, "val_loss": 891.086483001709, "val_acc": 56.0}
{"epoch": 16, "training_loss": 3577.349609375, "training_acc": 59.0, "val_loss": 908.8775634765625, "val_acc": 72.0}
{"epoch": 17, "training_loss": 2612.094856262207, "training_acc": 68.0, "val_loss": 401.8068313598633, "val_acc": 52.0}
{"epoch": 18, "training_loss": 2128.288619995117, "training_acc": 58.0, "val_loss": 642.3467636108398, "val_acc": 72.0}
{"epoch": 19, "training_loss": 2230.2283821105957, "training_acc": 74.0, "val_loss": 489.58911895751953, "val_acc": 56.0}
{"epoch": 20, "training_loss": 1847.3228912353516, "training_acc": 71.0, "val_loss": 413.95654678344727, "val_acc": 72.0}
{"epoch": 21, "training_loss": 2191.324676513672, "training_acc": 79.0, "val_loss": 152.13772058486938, "val_acc": 84.0}
{"epoch": 22, "training_loss": 1783.005989074707, "training_acc": 66.0, "val_loss": 157.22088813781738, "val_acc": 84.0}
{"epoch": 23, "training_loss": 987.5172958374023, "training_acc": 77.0, "val_loss": 117.45240688323975, "val_acc": 68.0}
{"epoch": 24, "training_loss": 1365.5172271728516, "training_acc": 67.0, "val_loss": 522.690486907959, "val_acc": 72.0}
{"epoch": 25, "training_loss": 1981.715965270996, "training_acc": 72.0, "val_loss": 279.6173095703125, "val_acc": 60.0}
{"epoch": 26, "training_loss": 813.5323448181152, "training_acc": 65.0, "val_loss": 510.4891777038574, "val_acc": 72.0}
{"epoch": 27, "training_loss": 1593.0301551818848, "training_acc": 73.0, "val_loss": 615.2126789093018, "val_acc": 52.0}
{"epoch": 28, "training_loss": 1610.2550735473633, "training_acc": 59.0, "val_loss": 744.5353984832764, "val_acc": 72.0}
{"epoch": 29, "training_loss": 2537.5881118774414, "training_acc": 74.0, "val_loss": 137.56535053253174, "val_acc": 80.0}
{"epoch": 30, "training_loss": 2230.2853393554688, "training_acc": 61.0, "val_loss": 246.25797271728516, "val_acc": 76.0}
{"epoch": 31, "training_loss": 2093.9103088378906, "training_acc": 72.0, "val_loss": 543.5292720794678, "val_acc": 72.0}
{"epoch": 32, "training_loss": 1595.5222854614258, "training_acc": 73.0, "val_loss": 735.9885215759277, "val_acc": 52.0}
{"epoch": 33, "training_loss": 2240.6142234802246, "training_acc": 64.0, "val_loss": 1266.3853645324707, "val_acc": 72.0}
{"epoch": 34, "training_loss": 3532.442337036133, "training_acc": 73.0, "val_loss": 232.40180015563965, "val_acc": 72.0}
{"epoch": 35, "training_loss": 1977.780014038086, "training_acc": 66.0, "val_loss": 243.4964656829834, "val_acc": 68.0}
{"epoch": 36, "training_loss": 1212.2633819580078, "training_acc": 77.0, "val_loss": 914.9205207824707, "val_acc": 72.0}
{"epoch": 37, "training_loss": 1721.1549644470215, "training_acc": 77.0, "val_loss": 731.1627864837646, "val_acc": 48.0}
{"epoch": 38, "training_loss": 2101.4910469055176, "training_acc": 64.0, "val_loss": 538.7351989746094, "val_acc": 72.0}
{"epoch": 39, "training_loss": 1432.4426040649414, "training_acc": 74.0, "val_loss": 537.6341342926025, "val_acc": 52.0}
{"epoch": 40, "training_loss": 1354.0914916992188, "training_acc": 65.0, "val_loss": 181.5071940422058, "val_acc": 76.0}
{"epoch": 41, "training_loss": 439.8734188079834, "training_acc": 82.0, "val_loss": 259.36641693115234, "val_acc": 64.0}
{"epoch": 42, "training_loss": 703.2873725891113, "training_acc": 79.0, "val_loss": 391.7620658874512, "val_acc": 72.0}
