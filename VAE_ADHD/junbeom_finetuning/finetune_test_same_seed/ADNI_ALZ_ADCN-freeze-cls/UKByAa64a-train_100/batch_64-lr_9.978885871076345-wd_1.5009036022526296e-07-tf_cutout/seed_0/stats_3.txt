"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 15233.240711212158, "training_acc": 63.0, "val_loss": 7786.342620849609, "val_acc": 72.0}
{"epoch": 1, "training_loss": 22992.16665649414, "training_acc": 72.0, "val_loss": 9854.701232910156, "val_acc": 28.0}
{"epoch": 2, "training_loss": 34234.16046142578, "training_acc": 28.0, "val_loss": 1529.8937797546387, "val_acc": 68.0}
{"epoch": 3, "training_loss": 6908.268005371094, "training_acc": 75.0, "val_loss": 5616.835021972656, "val_acc": 72.0}
{"epoch": 4, "training_loss": 21149.291931152344, "training_acc": 72.0, "val_loss": 4939.398193359375, "val_acc": 72.0}
{"epoch": 5, "training_loss": 16493.158630371094, "training_acc": 72.0, "val_loss": 2261.626434326172, "val_acc": 68.0}
{"epoch": 6, "training_loss": 14444.348754882812, "training_acc": 60.0, "val_loss": 4360.343551635742, "val_acc": 44.0}
{"epoch": 7, "training_loss": 11408.426666259766, "training_acc": 44.0, "val_loss": 2328.1084060668945, "val_acc": 76.0}
{"epoch": 8, "training_loss": 8853.006225585938, "training_acc": 72.0, "val_loss": 3817.272186279297, "val_acc": 72.0}
{"epoch": 9, "training_loss": 13923.320892333984, "training_acc": 72.0, "val_loss": 2130.0668716430664, "val_acc": 76.0}
{"epoch": 10, "training_loss": 5218.566162109375, "training_acc": 78.0, "val_loss": 2953.362274169922, "val_acc": 48.0}
{"epoch": 11, "training_loss": 9443.271392822266, "training_acc": 47.0, "val_loss": 1760.4936599731445, "val_acc": 68.0}
{"epoch": 12, "training_loss": 4753.0408935546875, "training_acc": 70.0, "val_loss": 2084.4276428222656, "val_acc": 72.0}
{"epoch": 13, "training_loss": 7146.815689086914, "training_acc": 72.0, "val_loss": 1078.6799430847168, "val_acc": 68.0}
{"epoch": 14, "training_loss": 2958.9251251220703, "training_acc": 74.0, "val_loss": 2006.2673568725586, "val_acc": 48.0}
{"epoch": 15, "training_loss": 4830.810882568359, "training_acc": 55.0, "val_loss": 894.9007987976074, "val_acc": 80.0}
{"epoch": 16, "training_loss": 4297.1732177734375, "training_acc": 72.0, "val_loss": 606.2199592590332, "val_acc": 80.0}
{"epoch": 17, "training_loss": 3197.9154663085938, "training_acc": 62.0, "val_loss": 679.6804428100586, "val_acc": 48.0}
{"epoch": 18, "training_loss": 2767.917495727539, "training_acc": 60.0, "val_loss": 1578.4340858459473, "val_acc": 72.0}
{"epoch": 19, "training_loss": 6112.089340209961, "training_acc": 72.0, "val_loss": 208.9993715286255, "val_acc": 72.0}
{"epoch": 20, "training_loss": 3084.3388061523438, "training_acc": 66.0, "val_loss": 1236.1555099487305, "val_acc": 32.0}
{"epoch": 21, "training_loss": 4768.13069152832, "training_acc": 51.0, "val_loss": 2204.348564147949, "val_acc": 72.0}
{"epoch": 22, "training_loss": 8901.981231689453, "training_acc": 72.0, "val_loss": 1446.5815544128418, "val_acc": 72.0}
{"epoch": 23, "training_loss": 4405.156921386719, "training_acc": 75.0, "val_loss": 1999.1296768188477, "val_acc": 40.0}
{"epoch": 24, "training_loss": 5266.735237121582, "training_acc": 47.0, "val_loss": 843.5051918029785, "val_acc": 76.0}
{"epoch": 25, "training_loss": 4167.540435791016, "training_acc": 73.0, "val_loss": 1211.8057250976562, "val_acc": 80.0}
{"epoch": 26, "training_loss": 4218.124439239502, "training_acc": 73.0, "val_loss": 1438.6013984680176, "val_acc": 56.0}
{"epoch": 27, "training_loss": 4369.175598144531, "training_acc": 58.0, "val_loss": 805.783748626709, "val_acc": 72.0}
{"epoch": 28, "training_loss": 2781.563262939453, "training_acc": 75.0, "val_loss": 1159.056568145752, "val_acc": 80.0}
{"epoch": 29, "training_loss": 3537.385566711426, "training_acc": 74.0, "val_loss": 863.2564544677734, "val_acc": 52.0}
{"epoch": 30, "training_loss": 1737.4035396575928, "training_acc": 66.0, "val_loss": 462.3690128326416, "val_acc": 56.0}
{"epoch": 31, "training_loss": 992.9424333572388, "training_acc": 79.0, "val_loss": 346.8806028366089, "val_acc": 80.0}
{"epoch": 32, "training_loss": 979.1252708435059, "training_acc": 79.0, "val_loss": 1260.4057312011719, "val_acc": 32.0}
{"epoch": 33, "training_loss": 1928.4119338989258, "training_acc": 63.0, "val_loss": 906.0502052307129, "val_acc": 72.0}
{"epoch": 34, "training_loss": 4327.558364868164, "training_acc": 72.0, "val_loss": 463.1402015686035, "val_acc": 72.0}
{"epoch": 35, "training_loss": 2953.0938262939453, "training_acc": 62.0, "val_loss": 506.31208419799805, "val_acc": 52.0}
{"epoch": 36, "training_loss": 1677.530517578125, "training_acc": 66.0, "val_loss": 1043.4880256652832, "val_acc": 72.0}
{"epoch": 37, "training_loss": 3598.180694580078, "training_acc": 73.0, "val_loss": 708.7945461273193, "val_acc": 56.0}
{"epoch": 38, "training_loss": 2867.3521881103516, "training_acc": 61.0, "val_loss": 531.9375514984131, "val_acc": 72.0}
