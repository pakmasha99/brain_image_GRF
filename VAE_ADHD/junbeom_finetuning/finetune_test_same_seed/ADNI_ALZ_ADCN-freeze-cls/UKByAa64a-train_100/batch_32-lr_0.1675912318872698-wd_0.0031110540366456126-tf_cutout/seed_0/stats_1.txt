"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 32 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 1071.6357641220093, "training_acc": 72.0, "val_loss": 767.0000553131104, "val_acc": 28.0}
{"epoch": 1, "training_loss": 1840.0508270263672, "training_acc": 32.0, "val_loss": 496.98524475097656, "val_acc": 72.0}
{"epoch": 2, "training_loss": 2800.535713195801, "training_acc": 72.0, "val_loss": 824.7041702270508, "val_acc": 72.0}
{"epoch": 3, "training_loss": 2981.8830795288086, "training_acc": 72.0, "val_loss": 373.7865924835205, "val_acc": 72.0}
{"epoch": 4, "training_loss": 762.0395469665527, "training_acc": 58.0, "val_loss": 240.55302143096924, "val_acc": 28.0}
{"epoch": 5, "training_loss": 544.9879322052002, "training_acc": 58.0, "val_loss": 100.76397657394409, "val_acc": 72.0}
{"epoch": 6, "training_loss": 376.81872940063477, "training_acc": 58.0, "val_loss": 53.28906774520874, "val_acc": 72.0}
{"epoch": 7, "training_loss": 404.3902645111084, "training_acc": 72.0, "val_loss": 110.24657487869263, "val_acc": 72.0}
{"epoch": 8, "training_loss": 364.9580297470093, "training_acc": 52.0, "val_loss": 48.91424477100372, "val_acc": 72.0}
{"epoch": 9, "training_loss": 418.519736289978, "training_acc": 72.0, "val_loss": 49.61609244346619, "val_acc": 72.0}
{"epoch": 10, "training_loss": 274.8827962875366, "training_acc": 40.0, "val_loss": 109.2926025390625, "val_acc": 72.0}
{"epoch": 11, "training_loss": 444.7540760040283, "training_acc": 72.0, "val_loss": 45.94378173351288, "val_acc": 28.0}
{"epoch": 12, "training_loss": 162.93132209777832, "training_acc": 46.0, "val_loss": 44.5785790681839, "val_acc": 72.0}
{"epoch": 13, "training_loss": 153.0552749633789, "training_acc": 42.0, "val_loss": 62.65207529067993, "val_acc": 72.0}
{"epoch": 14, "training_loss": 341.80912685394287, "training_acc": 44.0, "val_loss": 109.13434028625488, "val_acc": 72.0}
{"epoch": 15, "training_loss": 701.9249649047852, "training_acc": 72.0, "val_loss": 222.31144905090332, "val_acc": 72.0}
{"epoch": 16, "training_loss": 673.3419494628906, "training_acc": 71.0, "val_loss": 112.58949041366577, "val_acc": 28.0}
{"epoch": 17, "training_loss": 267.58611583709717, "training_acc": 55.0, "val_loss": 90.91399908065796, "val_acc": 72.0}
{"epoch": 18, "training_loss": 288.21572041511536, "training_acc": 52.0, "val_loss": 27.639901638031006, "val_acc": 72.0}
{"epoch": 19, "training_loss": 112.5972843170166, "training_acc": 66.0, "val_loss": 28.287911415100098, "val_acc": 72.0}
{"epoch": 20, "training_loss": 122.96271324157715, "training_acc": 72.0, "val_loss": 24.451014399528503, "val_acc": 28.0}
{"epoch": 21, "training_loss": 151.12051599676488, "training_acc": 64.0, "val_loss": 54.56237196922302, "val_acc": 72.0}
{"epoch": 22, "training_loss": 209.1609468460083, "training_acc": 56.0, "val_loss": 69.46894526481628, "val_acc": 72.0}
{"epoch": 23, "training_loss": 313.9392879391671, "training_acc": 72.0, "val_loss": 33.82610380649567, "val_acc": 72.0}
{"epoch": 24, "training_loss": 181.43276405334473, "training_acc": 40.0, "val_loss": 104.89449501037598, "val_acc": 72.0}
{"epoch": 25, "training_loss": 329.1841468811035, "training_acc": 70.0, "val_loss": 15.174837410449982, "val_acc": 56.0}
{"epoch": 26, "training_loss": 103.91706454753876, "training_acc": 70.0, "val_loss": 38.442474603652954, "val_acc": 28.0}
{"epoch": 27, "training_loss": 96.85429620742798, "training_acc": 66.0, "val_loss": 33.51306915283203, "val_acc": 28.0}
{"epoch": 28, "training_loss": 140.38776803016663, "training_acc": 52.0, "val_loss": 51.691603660583496, "val_acc": 28.0}
{"epoch": 29, "training_loss": 161.62517499923706, "training_acc": 62.0, "val_loss": 16.6030615568161, "val_acc": 48.0}
{"epoch": 30, "training_loss": 82.44747446477413, "training_acc": 62.0, "val_loss": 19.386619329452515, "val_acc": 72.0}
{"epoch": 31, "training_loss": 89.9978960454464, "training_acc": 58.0, "val_loss": 27.986738085746765, "val_acc": 72.0}
{"epoch": 32, "training_loss": 83.51775932312012, "training_acc": 64.0, "val_loss": 13.011878728866577, "val_acc": 72.0}
{"epoch": 33, "training_loss": 124.651535987854, "training_acc": 62.0, "val_loss": 18.361642956733704, "val_acc": 72.0}
{"epoch": 34, "training_loss": 347.9726867675781, "training_acc": 34.0, "val_loss": 132.06695318222046, "val_acc": 72.0}
{"epoch": 35, "training_loss": 514.3234906196594, "training_acc": 72.0, "val_loss": 62.720948457717896, "val_acc": 28.0}
{"epoch": 36, "training_loss": 278.8265962600708, "training_acc": 38.0, "val_loss": 114.35866355895996, "val_acc": 72.0}
{"epoch": 37, "training_loss": 312.41919326782227, "training_acc": 70.0, "val_loss": 72.26892709732056, "val_acc": 28.0}
{"epoch": 38, "training_loss": 344.47681427001953, "training_acc": 58.0, "val_loss": 165.10251760482788, "val_acc": 72.0}
{"epoch": 39, "training_loss": 524.2696869373322, "training_acc": 73.0, "val_loss": 128.84774208068848, "val_acc": 28.0}
{"epoch": 40, "training_loss": 323.82725524902344, "training_acc": 46.0, "val_loss": 192.04868078231812, "val_acc": 72.0}
{"epoch": 41, "training_loss": 788.2943782806396, "training_acc": 72.0, "val_loss": 59.49535369873047, "val_acc": 72.0}
{"epoch": 42, "training_loss": 362.39942836761475, "training_acc": 44.0, "val_loss": 94.30405497550964, "val_acc": 72.0}
{"epoch": 43, "training_loss": 477.10759830474854, "training_acc": 72.0, "val_loss": 21.07953280210495, "val_acc": 72.0}
{"epoch": 44, "training_loss": 426.2909261137247, "training_acc": 44.0, "val_loss": 106.35374784469604, "val_acc": 72.0}
{"epoch": 45, "training_loss": 579.4321595430374, "training_acc": 72.0, "val_loss": 88.13183307647705, "val_acc": 72.0}
{"epoch": 46, "training_loss": 273.3742904663086, "training_acc": 62.0, "val_loss": 69.33861374855042, "val_acc": 72.0}
{"epoch": 47, "training_loss": 469.3387145996094, "training_acc": 72.0, "val_loss": 150.57398080825806, "val_acc": 72.0}
{"epoch": 48, "training_loss": 390.66933250427246, "training_acc": 72.0, "val_loss": 147.4090814590454, "val_acc": 28.0}
{"epoch": 49, "training_loss": 252.02153968811035, "training_acc": 64.0, "val_loss": 90.12672901153564, "val_acc": 72.0}
{"epoch": 50, "training_loss": 222.35747718811035, "training_acc": 68.0, "val_loss": 38.485485315322876, "val_acc": 28.0}
{"epoch": 51, "training_loss": 357.0468406677246, "training_acc": 59.0, "val_loss": 161.7383360862732, "val_acc": 72.0}
