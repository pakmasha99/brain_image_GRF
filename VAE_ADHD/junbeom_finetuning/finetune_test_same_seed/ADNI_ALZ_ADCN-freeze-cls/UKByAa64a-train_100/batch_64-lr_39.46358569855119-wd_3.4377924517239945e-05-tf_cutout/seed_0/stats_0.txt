"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 63966.90754318237, "training_acc": 72.0, "val_loss": 28217.459106445312, "val_acc": 72.0}
{"epoch": 1, "training_loss": 76715.44445800781, "training_acc": 72.0, "val_loss": 44635.65673828125, "val_acc": 28.0}
{"epoch": 2, "training_loss": 186665.94580078125, "training_acc": 28.0, "val_loss": 8211.293029785156, "val_acc": 36.0}
{"epoch": 3, "training_loss": 49326.477783203125, "training_acc": 51.0, "val_loss": 25201.315307617188, "val_acc": 72.0}
{"epoch": 4, "training_loss": 113357.36376953125, "training_acc": 72.0, "val_loss": 33572.81188964844, "val_acc": 72.0}
{"epoch": 5, "training_loss": 133955.46606445312, "training_acc": 72.0, "val_loss": 24689.923095703125, "val_acc": 72.0}
{"epoch": 6, "training_loss": 86588.6875, "training_acc": 72.0, "val_loss": 5118.178558349609, "val_acc": 72.0}
{"epoch": 7, "training_loss": 45517.42626953125, "training_acc": 64.0, "val_loss": 9882.268524169922, "val_acc": 52.0}
{"epoch": 8, "training_loss": 71692.56701660156, "training_acc": 45.0, "val_loss": 3150.289726257324, "val_acc": 80.0}
{"epoch": 9, "training_loss": 32543.888610839844, "training_acc": 73.0, "val_loss": 9111.934661865234, "val_acc": 72.0}
{"epoch": 10, "training_loss": 39416.691650390625, "training_acc": 73.0, "val_loss": 9891.353607177734, "val_acc": 72.0}
{"epoch": 11, "training_loss": 31184.275512695312, "training_acc": 71.0, "val_loss": 2201.837921142578, "val_acc": 80.0}
{"epoch": 12, "training_loss": 23777.099731445312, "training_acc": 58.0, "val_loss": 3155.1708221435547, "val_acc": 64.0}
{"epoch": 13, "training_loss": 14493.12124633789, "training_acc": 66.0, "val_loss": 7594.853973388672, "val_acc": 72.0}
{"epoch": 14, "training_loss": 28612.954223632812, "training_acc": 72.0, "val_loss": 6193.603515625, "val_acc": 72.0}
{"epoch": 15, "training_loss": 13115.914108276367, "training_acc": 69.0, "val_loss": 5696.335983276367, "val_acc": 40.0}
{"epoch": 16, "training_loss": 17065.716705322266, "training_acc": 49.0, "val_loss": 2824.0102767944336, "val_acc": 72.0}
{"epoch": 17, "training_loss": 6524.2910232543945, "training_acc": 70.0, "val_loss": 873.369026184082, "val_acc": 72.0}
{"epoch": 18, "training_loss": 1983.2156677246094, "training_acc": 76.0, "val_loss": 630.811071395874, "val_acc": 76.0}
{"epoch": 19, "training_loss": 2316.2574462890625, "training_acc": 81.0, "val_loss": 1222.2569465637207, "val_acc": 72.0}
{"epoch": 20, "training_loss": 3718.550365447998, "training_acc": 76.0, "val_loss": 811.7715835571289, "val_acc": 60.0}
{"epoch": 21, "training_loss": 5867.3487548828125, "training_acc": 64.0, "val_loss": 522.5405693054199, "val_acc": 60.0}
{"epoch": 22, "training_loss": 8340.997924804688, "training_acc": 65.0, "val_loss": 2048.740768432617, "val_acc": 72.0}
{"epoch": 23, "training_loss": 8796.629150390625, "training_acc": 72.0, "val_loss": 246.53701782226562, "val_acc": 88.0}
{"epoch": 24, "training_loss": 8422.080200195312, "training_acc": 65.0, "val_loss": 297.42674827575684, "val_acc": 88.0}
{"epoch": 25, "training_loss": 5264.759429931641, "training_acc": 75.0, "val_loss": 2131.5996170043945, "val_acc": 72.0}
{"epoch": 26, "training_loss": 7992.328414916992, "training_acc": 68.0, "val_loss": 783.3548545837402, "val_acc": 80.0}
{"epoch": 27, "training_loss": 7041.909225463867, "training_acc": 72.0, "val_loss": 1018.2414054870605, "val_acc": 76.0}
{"epoch": 28, "training_loss": 5504.538787841797, "training_acc": 75.0, "val_loss": 401.9620895385742, "val_acc": 84.0}
{"epoch": 29, "training_loss": 2305.4393348693848, "training_acc": 84.0, "val_loss": 533.3748817443848, "val_acc": 72.0}
{"epoch": 30, "training_loss": 1865.0057067871094, "training_acc": 78.0, "val_loss": 3377.5325775146484, "val_acc": 72.0}
{"epoch": 31, "training_loss": 11445.489135742188, "training_acc": 72.0, "val_loss": 1988.9450073242188, "val_acc": 56.0}
{"epoch": 32, "training_loss": 7181.847320556641, "training_acc": 59.0, "val_loss": 3533.92333984375, "val_acc": 72.0}
{"epoch": 33, "training_loss": 10811.653442382812, "training_acc": 72.0, "val_loss": 1311.1692428588867, "val_acc": 76.0}
{"epoch": 34, "training_loss": 4907.143798828125, "training_acc": 75.0, "val_loss": 1797.6055145263672, "val_acc": 60.0}
{"epoch": 35, "training_loss": 9493.099151611328, "training_acc": 63.0, "val_loss": 4811.443328857422, "val_acc": 72.0}
{"epoch": 36, "training_loss": 13868.89779663086, "training_acc": 75.0, "val_loss": 1114.5148277282715, "val_acc": 72.0}
{"epoch": 37, "training_loss": 8222.292419433594, "training_acc": 72.0, "val_loss": 660.7871055603027, "val_acc": 76.0}
{"epoch": 38, "training_loss": 6893.404602050781, "training_acc": 76.0, "val_loss": 3617.593002319336, "val_acc": 72.0}
{"epoch": 39, "training_loss": 6409.834205627441, "training_acc": 80.0, "val_loss": 5609.169769287109, "val_acc": 40.0}
{"epoch": 40, "training_loss": 15582.237258911133, "training_acc": 49.0, "val_loss": 2750.5165100097656, "val_acc": 72.0}
{"epoch": 41, "training_loss": 5963.256248474121, "training_acc": 74.0, "val_loss": 4219.563293457031, "val_acc": 44.0}
{"epoch": 42, "training_loss": 9069.721862792969, "training_acc": 59.0, "val_loss": 3031.888771057129, "val_acc": 72.0}
