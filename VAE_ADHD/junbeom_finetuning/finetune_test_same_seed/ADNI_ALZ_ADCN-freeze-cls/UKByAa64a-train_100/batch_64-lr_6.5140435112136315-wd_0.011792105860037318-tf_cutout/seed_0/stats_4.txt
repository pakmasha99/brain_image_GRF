"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 10583.20246887207, "training_acc": 60.0, "val_loss": 4606.407928466797, "val_acc": 72.0}
{"epoch": 1, "training_loss": 13777.42406463623, "training_acc": 72.0, "val_loss": 6657.872772216797, "val_acc": 28.0}
{"epoch": 2, "training_loss": 27204.594848632812, "training_acc": 28.0, "val_loss": 649.6881008148193, "val_acc": 68.0}
{"epoch": 3, "training_loss": 3125.509735107422, "training_acc": 73.0, "val_loss": 3805.532455444336, "val_acc": 72.0}
{"epoch": 4, "training_loss": 14079.843231201172, "training_acc": 72.0, "val_loss": 3452.0408630371094, "val_acc": 72.0}
{"epoch": 5, "training_loss": 10888.657745361328, "training_acc": 72.0, "val_loss": 947.4498748779297, "val_acc": 76.0}
{"epoch": 6, "training_loss": 4538.680755615234, "training_acc": 59.0, "val_loss": 1059.7919464111328, "val_acc": 48.0}
{"epoch": 7, "training_loss": 4783.192794799805, "training_acc": 53.0, "val_loss": 1431.424331665039, "val_acc": 72.0}
{"epoch": 8, "training_loss": 4409.3974609375, "training_acc": 72.0, "val_loss": 962.4392509460449, "val_acc": 72.0}
{"epoch": 9, "training_loss": 2903.567153930664, "training_acc": 66.0, "val_loss": 493.1264400482178, "val_acc": 60.0}
{"epoch": 10, "training_loss": 2313.915294647217, "training_acc": 64.0, "val_loss": 493.0511951446533, "val_acc": 72.0}
{"epoch": 11, "training_loss": 934.1667461395264, "training_acc": 80.0, "val_loss": 311.17632389068604, "val_acc": 36.0}
{"epoch": 12, "training_loss": 1459.8407287597656, "training_acc": 62.0, "val_loss": 571.2282657623291, "val_acc": 72.0}
{"epoch": 13, "training_loss": 1923.1432476043701, "training_acc": 72.0, "val_loss": 2027.8026580810547, "val_acc": 28.0}
{"epoch": 14, "training_loss": 5720.15958404541, "training_acc": 30.0, "val_loss": 1228.0221939086914, "val_acc": 72.0}
{"epoch": 15, "training_loss": 6292.6810302734375, "training_acc": 72.0, "val_loss": 2162.141227722168, "val_acc": 72.0}
{"epoch": 16, "training_loss": 8817.989631652832, "training_acc": 72.0, "val_loss": 995.0135231018066, "val_acc": 72.0}
{"epoch": 17, "training_loss": 2736.237377166748, "training_acc": 69.0, "val_loss": 1272.2603797912598, "val_acc": 32.0}
{"epoch": 18, "training_loss": 4259.988662719727, "training_acc": 44.0, "val_loss": 1043.780517578125, "val_acc": 72.0}
{"epoch": 19, "training_loss": 4586.033493041992, "training_acc": 72.0, "val_loss": 1157.7116012573242, "val_acc": 72.0}
{"epoch": 20, "training_loss": 3044.088222503662, "training_acc": 72.0, "val_loss": 902.9525756835938, "val_acc": 36.0}
{"epoch": 21, "training_loss": 3865.908332824707, "training_acc": 44.0, "val_loss": 524.9545097351074, "val_acc": 72.0}
{"epoch": 22, "training_loss": 2021.950668334961, "training_acc": 74.0, "val_loss": 367.60449409484863, "val_acc": 76.0}
{"epoch": 23, "training_loss": 1057.3987312316895, "training_acc": 71.0, "val_loss": 254.83791828155518, "val_acc": 64.0}
{"epoch": 24, "training_loss": 1476.980640411377, "training_acc": 65.0, "val_loss": 495.87411880493164, "val_acc": 72.0}
{"epoch": 25, "training_loss": 1552.9539813995361, "training_acc": 67.0, "val_loss": 41.118595004081726, "val_acc": 68.0}
{"epoch": 26, "training_loss": 215.2611608505249, "training_acc": 80.0, "val_loss": 717.4344539642334, "val_acc": 28.0}
{"epoch": 27, "training_loss": 2883.214874267578, "training_acc": 39.0, "val_loss": 788.6325359344482, "val_acc": 72.0}
{"epoch": 28, "training_loss": 2970.897590637207, "training_acc": 72.0, "val_loss": 856.1856269836426, "val_acc": 28.0}
{"epoch": 29, "training_loss": 2187.415330886841, "training_acc": 48.0, "val_loss": 208.935809135437, "val_acc": 72.0}
{"epoch": 30, "training_loss": 1625.5149993896484, "training_acc": 51.0, "val_loss": 525.272798538208, "val_acc": 72.0}
{"epoch": 31, "training_loss": 1973.2676391601562, "training_acc": 72.0, "val_loss": 470.4794406890869, "val_acc": 72.0}
{"epoch": 32, "training_loss": 1638.4247436523438, "training_acc": 69.0, "val_loss": 265.5040740966797, "val_acc": 60.0}
{"epoch": 33, "training_loss": 1095.8378219604492, "training_acc": 62.0, "val_loss": 432.94129371643066, "val_acc": 72.0}
{"epoch": 34, "training_loss": 1782.415756225586, "training_acc": 58.0, "val_loss": 303.2783031463623, "val_acc": 76.0}
{"epoch": 35, "training_loss": 783.4119491577148, "training_acc": 79.0, "val_loss": 133.64503383636475, "val_acc": 68.0}
{"epoch": 36, "training_loss": 597.4282341003418, "training_acc": 69.0, "val_loss": 430.40356636047363, "val_acc": 72.0}
{"epoch": 37, "training_loss": 1851.661849975586, "training_acc": 72.0, "val_loss": 154.42038774490356, "val_acc": 40.0}
{"epoch": 38, "training_loss": 425.08077907562256, "training_acc": 61.0, "val_loss": 240.26329517364502, "val_acc": 40.0}
{"epoch": 39, "training_loss": 1863.4179992675781, "training_acc": 46.0, "val_loss": 771.1233139038086, "val_acc": 72.0}
{"epoch": 40, "training_loss": 2254.935911178589, "training_acc": 73.0, "val_loss": 754.6822547912598, "val_acc": 40.0}
{"epoch": 41, "training_loss": 2731.6633701324463, "training_acc": 47.0, "val_loss": 688.3814334869385, "val_acc": 72.0}
{"epoch": 42, "training_loss": 1972.8840255737305, "training_acc": 72.0, "val_loss": 315.32037258148193, "val_acc": 68.0}
{"epoch": 43, "training_loss": 1254.990306854248, "training_acc": 67.0, "val_loss": 365.967059135437, "val_acc": 72.0}
{"epoch": 44, "training_loss": 1026.415870666504, "training_acc": 72.0, "val_loss": 363.08233737945557, "val_acc": 72.0}
