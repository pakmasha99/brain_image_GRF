"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 735.4893455505371, "training_acc": 72.0, "val_loss": 296.0274934768677, "val_acc": 72.0}
{"epoch": 1, "training_loss": 884.1403846740723, "training_acc": 72.0, "val_loss": 420.6240653991699, "val_acc": 28.0}
{"epoch": 2, "training_loss": 1582.1116790771484, "training_acc": 28.0, "val_loss": 50.867438316345215, "val_acc": 56.0}
{"epoch": 3, "training_loss": 344.40052223205566, "training_acc": 60.0, "val_loss": 270.0575590133667, "val_acc": 72.0}
{"epoch": 4, "training_loss": 1107.02632522583, "training_acc": 72.0, "val_loss": 323.0023384094238, "val_acc": 72.0}
{"epoch": 5, "training_loss": 1168.5923614501953, "training_acc": 72.0, "val_loss": 196.56951427459717, "val_acc": 72.0}
{"epoch": 6, "training_loss": 538.8314561843872, "training_acc": 72.0, "val_loss": 101.34848356246948, "val_acc": 56.0}
{"epoch": 7, "training_loss": 617.7388763427734, "training_acc": 50.0, "val_loss": 180.79180717468262, "val_acc": 48.0}
{"epoch": 8, "training_loss": 643.5992784500122, "training_acc": 45.0, "val_loss": 60.12812852859497, "val_acc": 76.0}
{"epoch": 9, "training_loss": 254.04381275177002, "training_acc": 81.0, "val_loss": 135.4075312614441, "val_acc": 72.0}
{"epoch": 10, "training_loss": 514.8923778533936, "training_acc": 72.0, "val_loss": 91.02025628089905, "val_acc": 72.0}
{"epoch": 11, "training_loss": 367.7617402076721, "training_acc": 69.0, "val_loss": 72.69363403320312, "val_acc": 56.0}
{"epoch": 12, "training_loss": 325.55224227905273, "training_acc": 62.0, "val_loss": 69.03160214424133, "val_acc": 60.0}
{"epoch": 13, "training_loss": 254.15873336791992, "training_acc": 63.0, "val_loss": 68.42678785324097, "val_acc": 72.0}
{"epoch": 14, "training_loss": 260.83322525024414, "training_acc": 72.0, "val_loss": 56.994396448135376, "val_acc": 72.0}
{"epoch": 15, "training_loss": 179.3472671508789, "training_acc": 74.0, "val_loss": 66.58622026443481, "val_acc": 56.0}
{"epoch": 16, "training_loss": 235.90296173095703, "training_acc": 52.0, "val_loss": 21.016284823417664, "val_acc": 72.0}
{"epoch": 17, "training_loss": 94.42588448524475, "training_acc": 78.0, "val_loss": 53.01998257637024, "val_acc": 72.0}
{"epoch": 18, "training_loss": 138.82118129730225, "training_acc": 72.0, "val_loss": 50.32168626785278, "val_acc": 40.0}
{"epoch": 19, "training_loss": 152.09814548492432, "training_acc": 52.0, "val_loss": 37.28543519973755, "val_acc": 72.0}
{"epoch": 20, "training_loss": 133.0799126625061, "training_acc": 72.0, "val_loss": 39.72262740135193, "val_acc": 72.0}
{"epoch": 21, "training_loss": 80.77060794830322, "training_acc": 75.0, "val_loss": 62.8673791885376, "val_acc": 40.0}
{"epoch": 22, "training_loss": 156.1295108795166, "training_acc": 56.0, "val_loss": 54.53776717185974, "val_acc": 72.0}
{"epoch": 23, "training_loss": 209.76389026641846, "training_acc": 72.0, "val_loss": 67.20001101493835, "val_acc": 72.0}
{"epoch": 24, "training_loss": 153.49059414863586, "training_acc": 73.0, "val_loss": 72.4193811416626, "val_acc": 40.0}
{"epoch": 25, "training_loss": 240.40428161621094, "training_acc": 44.0, "val_loss": 28.215602040290833, "val_acc": 72.0}
{"epoch": 26, "training_loss": 128.0791893005371, "training_acc": 77.0, "val_loss": 53.727203607559204, "val_acc": 72.0}
{"epoch": 27, "training_loss": 130.81571435928345, "training_acc": 75.0, "val_loss": 52.31753587722778, "val_acc": 44.0}
{"epoch": 28, "training_loss": 172.72552871704102, "training_acc": 57.0, "val_loss": 24.008651077747345, "val_acc": 72.0}
{"epoch": 29, "training_loss": 92.39190721511841, "training_acc": 78.0, "val_loss": 35.33502519130707, "val_acc": 72.0}
{"epoch": 30, "training_loss": 86.56553506851196, "training_acc": 71.0, "val_loss": 35.02141237258911, "val_acc": 56.0}
{"epoch": 31, "training_loss": 88.45997440814972, "training_acc": 72.0, "val_loss": 31.194958090782166, "val_acc": 72.0}
{"epoch": 32, "training_loss": 94.7541241645813, "training_acc": 72.0, "val_loss": 18.892624974250793, "val_acc": 68.0}
{"epoch": 33, "training_loss": 69.20498085021973, "training_acc": 73.0, "val_loss": 19.43524181842804, "val_acc": 68.0}
{"epoch": 34, "training_loss": 36.61373150348663, "training_acc": 83.0, "val_loss": 44.07362639904022, "val_acc": 72.0}
{"epoch": 35, "training_loss": 105.83488273620605, "training_acc": 72.0, "val_loss": 58.33476185798645, "val_acc": 40.0}
{"epoch": 36, "training_loss": 159.28976678848267, "training_acc": 46.0, "val_loss": 43.27931106090546, "val_acc": 72.0}
{"epoch": 37, "training_loss": 142.21889734268188, "training_acc": 72.0, "val_loss": 59.35872793197632, "val_acc": 72.0}
{"epoch": 38, "training_loss": 134.58167123794556, "training_acc": 76.0, "val_loss": 44.555145502090454, "val_acc": 44.0}
{"epoch": 39, "training_loss": 130.99548768997192, "training_acc": 55.0, "val_loss": 28.012308478355408, "val_acc": 76.0}
{"epoch": 40, "training_loss": 69.6134295463562, "training_acc": 79.0, "val_loss": 20.46101838350296, "val_acc": 68.0}
{"epoch": 41, "training_loss": 60.927544355392456, "training_acc": 75.0, "val_loss": 20.715229213237762, "val_acc": 68.0}
{"epoch": 42, "training_loss": 43.35293674468994, "training_acc": 81.0, "val_loss": 33.18266272544861, "val_acc": 72.0}
{"epoch": 43, "training_loss": 70.80094504356384, "training_acc": 76.0, "val_loss": 28.12201976776123, "val_acc": 40.0}
{"epoch": 44, "training_loss": 52.229645133018494, "training_acc": 75.0, "val_loss": 32.58286118507385, "val_acc": 72.0}
{"epoch": 45, "training_loss": 63.4146625995636, "training_acc": 73.0, "val_loss": 33.77231955528259, "val_acc": 44.0}
{"epoch": 46, "training_loss": 69.00235617160797, "training_acc": 67.0, "val_loss": 26.654323935508728, "val_acc": 76.0}
{"epoch": 47, "training_loss": 49.80954122543335, "training_acc": 78.0, "val_loss": 20.28985172510147, "val_acc": 64.0}
{"epoch": 48, "training_loss": 43.410736083984375, "training_acc": 83.0, "val_loss": 20.164543390274048, "val_acc": 64.0}
{"epoch": 49, "training_loss": 38.92898678779602, "training_acc": 85.0, "val_loss": 21.923454105854034, "val_acc": 72.0}
{"epoch": 50, "training_loss": 41.62806236743927, "training_acc": 79.0, "val_loss": 22.23687916994095, "val_acc": 60.0}
{"epoch": 51, "training_loss": 37.50320506095886, "training_acc": 81.0, "val_loss": 22.634658217430115, "val_acc": 76.0}
