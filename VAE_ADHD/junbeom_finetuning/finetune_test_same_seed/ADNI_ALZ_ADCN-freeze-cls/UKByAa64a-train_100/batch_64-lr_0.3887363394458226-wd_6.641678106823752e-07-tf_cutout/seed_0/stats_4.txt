"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 576.6187362670898, "training_acc": 68.0, "val_loss": 294.73462104797363, "val_acc": 72.0}
{"epoch": 1, "training_loss": 869.216625213623, "training_acc": 72.0, "val_loss": 323.3558177947998, "val_acc": 28.0}
{"epoch": 2, "training_loss": 1457.9220085144043, "training_acc": 28.0, "val_loss": 46.28383815288544, "val_acc": 64.0}
{"epoch": 3, "training_loss": 287.49768829345703, "training_acc": 70.0, "val_loss": 254.1046380996704, "val_acc": 72.0}
{"epoch": 4, "training_loss": 974.0070381164551, "training_acc": 72.0, "val_loss": 269.48771476745605, "val_acc": 72.0}
{"epoch": 5, "training_loss": 855.4445877075195, "training_acc": 72.0, "val_loss": 127.48665809631348, "val_acc": 72.0}
{"epoch": 6, "training_loss": 304.24166202545166, "training_acc": 73.0, "val_loss": 124.33075904846191, "val_acc": 56.0}
{"epoch": 7, "training_loss": 743.4866580963135, "training_acc": 45.0, "val_loss": 79.85761761665344, "val_acc": 60.0}
{"epoch": 8, "training_loss": 345.74464321136475, "training_acc": 65.0, "val_loss": 158.82564783096313, "val_acc": 72.0}
{"epoch": 9, "training_loss": 498.7612657546997, "training_acc": 72.0, "val_loss": 182.38062858581543, "val_acc": 72.0}
{"epoch": 10, "training_loss": 514.5618596076965, "training_acc": 72.0, "val_loss": 84.57085490226746, "val_acc": 72.0}
{"epoch": 11, "training_loss": 199.55343914031982, "training_acc": 78.0, "val_loss": 77.4634063243866, "val_acc": 56.0}
{"epoch": 12, "training_loss": 377.70042610168457, "training_acc": 52.0, "val_loss": 53.38641405105591, "val_acc": 68.0}
{"epoch": 13, "training_loss": 176.5912618637085, "training_acc": 75.0, "val_loss": 112.07479238510132, "val_acc": 72.0}
{"epoch": 14, "training_loss": 331.96875762939453, "training_acc": 72.0, "val_loss": 57.50749707221985, "val_acc": 72.0}
{"epoch": 15, "training_loss": 141.42655038833618, "training_acc": 66.0, "val_loss": 59.74692106246948, "val_acc": 36.0}
{"epoch": 16, "training_loss": 233.47058033943176, "training_acc": 57.0, "val_loss": 49.82079565525055, "val_acc": 72.0}
{"epoch": 17, "training_loss": 178.51586294174194, "training_acc": 72.0, "val_loss": 40.421995520591736, "val_acc": 72.0}
{"epoch": 18, "training_loss": 109.19171690940857, "training_acc": 72.0, "val_loss": 50.49976706504822, "val_acc": 28.0}
{"epoch": 19, "training_loss": 158.52083683013916, "training_acc": 50.0, "val_loss": 31.023120880126953, "val_acc": 72.0}
{"epoch": 20, "training_loss": 93.57461214065552, "training_acc": 73.0, "val_loss": 42.03575849533081, "val_acc": 28.0}
{"epoch": 21, "training_loss": 121.74953556060791, "training_acc": 48.0, "val_loss": 36.45312488079071, "val_acc": 72.0}
{"epoch": 22, "training_loss": 126.24169492721558, "training_acc": 73.0, "val_loss": 29.153645038604736, "val_acc": 76.0}
{"epoch": 23, "training_loss": 89.03697872161865, "training_acc": 72.0, "val_loss": 28.47122550010681, "val_acc": 60.0}
{"epoch": 24, "training_loss": 96.43728506565094, "training_acc": 64.0, "val_loss": 43.954482674598694, "val_acc": 72.0}
{"epoch": 25, "training_loss": 135.4292516708374, "training_acc": 74.0, "val_loss": 21.45821303129196, "val_acc": 76.0}
{"epoch": 26, "training_loss": 97.10129976272583, "training_acc": 68.0, "val_loss": 16.174203157424927, "val_acc": 80.0}
{"epoch": 27, "training_loss": 85.44122695922852, "training_acc": 74.0, "val_loss": 31.21623694896698, "val_acc": 76.0}
{"epoch": 28, "training_loss": 71.79205298423767, "training_acc": 78.0, "val_loss": 23.879197239875793, "val_acc": 48.0}
{"epoch": 29, "training_loss": 87.91352105140686, "training_acc": 58.0, "val_loss": 24.78921115398407, "val_acc": 76.0}
{"epoch": 30, "training_loss": 64.76578044891357, "training_acc": 76.0, "val_loss": 13.592852652072906, "val_acc": 60.0}
{"epoch": 31, "training_loss": 47.62750864028931, "training_acc": 76.0, "val_loss": 15.109369158744812, "val_acc": 76.0}
{"epoch": 32, "training_loss": 45.65636873245239, "training_acc": 81.0, "val_loss": 11.808276921510696, "val_acc": 68.0}
{"epoch": 33, "training_loss": 46.45500421524048, "training_acc": 80.0, "val_loss": 12.931850552558899, "val_acc": 76.0}
{"epoch": 34, "training_loss": 37.67548620700836, "training_acc": 81.0, "val_loss": 12.101191282272339, "val_acc": 72.0}
{"epoch": 35, "training_loss": 33.443710923194885, "training_acc": 88.0, "val_loss": 17.834754288196564, "val_acc": 76.0}
{"epoch": 36, "training_loss": 37.71104347705841, "training_acc": 83.0, "val_loss": 17.911402881145477, "val_acc": 48.0}
{"epoch": 37, "training_loss": 63.32397699356079, "training_acc": 65.0, "val_loss": 19.355742633342743, "val_acc": 76.0}
{"epoch": 38, "training_loss": 37.03803789615631, "training_acc": 81.0, "val_loss": 13.3795827627182, "val_acc": 72.0}
{"epoch": 39, "training_loss": 35.81272542476654, "training_acc": 83.0, "val_loss": 15.598978102207184, "val_acc": 80.0}
{"epoch": 40, "training_loss": 41.45787847042084, "training_acc": 83.0, "val_loss": 15.157276391983032, "val_acc": 76.0}
{"epoch": 41, "training_loss": 31.27202880382538, "training_acc": 83.0, "val_loss": 11.80887445807457, "val_acc": 76.0}
{"epoch": 42, "training_loss": 32.80362689495087, "training_acc": 86.0, "val_loss": 17.762276530265808, "val_acc": 76.0}
{"epoch": 43, "training_loss": 50.055246472358704, "training_acc": 74.0, "val_loss": 13.379815220832825, "val_acc": 88.0}
{"epoch": 44, "training_loss": 26.708339750766754, "training_acc": 86.0, "val_loss": 14.236503839492798, "val_acc": 88.0}
{"epoch": 45, "training_loss": 30.281073212623596, "training_acc": 85.0, "val_loss": 13.990519940853119, "val_acc": 88.0}
{"epoch": 46, "training_loss": 30.481746673583984, "training_acc": 91.0, "val_loss": 13.346630334854126, "val_acc": 80.0}
{"epoch": 47, "training_loss": 29.988985300064087, "training_acc": 86.0, "val_loss": 16.782814264297485, "val_acc": 80.0}
{"epoch": 48, "training_loss": 30.6065456867218, "training_acc": 85.0, "val_loss": 15.380620956420898, "val_acc": 64.0}
{"epoch": 49, "training_loss": 36.03731918334961, "training_acc": 81.0, "val_loss": 22.415712475776672, "val_acc": 76.0}
{"epoch": 50, "training_loss": 52.90041637420654, "training_acc": 72.0, "val_loss": 21.75048738718033, "val_acc": 72.0}
{"epoch": 51, "training_loss": 39.83758223056793, "training_acc": 80.0, "val_loss": 13.04849088191986, "val_acc": 76.0}
