"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 7122.78475189209, "training_acc": 48.0, "val_loss": 5137.883758544922, "val_acc": 72.0}
{"epoch": 1, "training_loss": 16131.583374023438, "training_acc": 72.0, "val_loss": 2365.9912109375, "val_acc": 28.0}
{"epoch": 2, "training_loss": 9630.289558410645, "training_acc": 37.0, "val_loss": 934.800910949707, "val_acc": 76.0}
{"epoch": 3, "training_loss": 4379.077285766602, "training_acc": 68.0, "val_loss": 1659.8220825195312, "val_acc": 72.0}
{"epoch": 4, "training_loss": 4807.777877807617, "training_acc": 71.0, "val_loss": 755.1394939422607, "val_acc": 60.0}
{"epoch": 5, "training_loss": 3950.500099182129, "training_acc": 58.0, "val_loss": 635.914945602417, "val_acc": 76.0}
{"epoch": 6, "training_loss": 2844.8136138916016, "training_acc": 73.0, "val_loss": 1167.0588493347168, "val_acc": 72.0}
{"epoch": 7, "training_loss": 2728.5024185180664, "training_acc": 74.0, "val_loss": 579.099702835083, "val_acc": 48.0}
{"epoch": 8, "training_loss": 2675.3302459716797, "training_acc": 56.0, "val_loss": 697.2307205200195, "val_acc": 72.0}
{"epoch": 9, "training_loss": 2213.676773071289, "training_acc": 72.0, "val_loss": 399.8624086380005, "val_acc": 68.0}
{"epoch": 10, "training_loss": 3014.9320678710938, "training_acc": 51.0, "val_loss": 380.6791067123413, "val_acc": 52.0}
{"epoch": 11, "training_loss": 2472.036346435547, "training_acc": 58.0, "val_loss": 1734.4999313354492, "val_acc": 72.0}
{"epoch": 12, "training_loss": 6290.321243286133, "training_acc": 72.0, "val_loss": 1213.6305809020996, "val_acc": 72.0}
{"epoch": 13, "training_loss": 3051.9153633117676, "training_acc": 75.0, "val_loss": 1167.5825119018555, "val_acc": 36.0}
{"epoch": 14, "training_loss": 4684.125137329102, "training_acc": 36.0, "val_loss": 795.4912662506104, "val_acc": 72.0}
{"epoch": 15, "training_loss": 3055.974624633789, "training_acc": 72.0, "val_loss": 1586.4704132080078, "val_acc": 72.0}
{"epoch": 16, "training_loss": 4835.1865234375, "training_acc": 72.0, "val_loss": 549.5076656341553, "val_acc": 68.0}
{"epoch": 17, "training_loss": 2200.9712524414062, "training_acc": 70.0, "val_loss": 555.2154541015625, "val_acc": 64.0}
{"epoch": 18, "training_loss": 2432.0407943725586, "training_acc": 68.0, "val_loss": 930.3650856018066, "val_acc": 72.0}
{"epoch": 19, "training_loss": 2443.456756591797, "training_acc": 76.0, "val_loss": 598.1059551239014, "val_acc": 72.0}
{"epoch": 20, "training_loss": 1349.4686470031738, "training_acc": 72.0, "val_loss": 488.4053707122803, "val_acc": 64.0}
{"epoch": 21, "training_loss": 1913.4694175720215, "training_acc": 65.0, "val_loss": 585.6079578399658, "val_acc": 72.0}
{"epoch": 22, "training_loss": 1233.8162307739258, "training_acc": 76.0, "val_loss": 247.34573364257812, "val_acc": 72.0}
{"epoch": 23, "training_loss": 872.2585182189941, "training_acc": 73.0, "val_loss": 226.24444961547852, "val_acc": 68.0}
{"epoch": 24, "training_loss": 713.5868797302246, "training_acc": 77.0, "val_loss": 171.55327796936035, "val_acc": 56.0}
{"epoch": 25, "training_loss": 602.9209604263306, "training_acc": 66.0, "val_loss": 472.16291427612305, "val_acc": 72.0}
{"epoch": 26, "training_loss": 1146.5503902435303, "training_acc": 73.0, "val_loss": 308.5904359817505, "val_acc": 52.0}
{"epoch": 27, "training_loss": 952.253270149231, "training_acc": 63.0, "val_loss": 367.23878383636475, "val_acc": 72.0}
{"epoch": 28, "training_loss": 588.1343903541565, "training_acc": 79.0, "val_loss": 103.12032699584961, "val_acc": 68.0}
{"epoch": 29, "training_loss": 368.03020191192627, "training_acc": 74.0, "val_loss": 160.42736768722534, "val_acc": 76.0}
{"epoch": 30, "training_loss": 454.75917053222656, "training_acc": 71.0, "val_loss": 306.89594745635986, "val_acc": 72.0}
{"epoch": 31, "training_loss": 551.40709400177, "training_acc": 78.0, "val_loss": 95.33776640892029, "val_acc": 68.0}
{"epoch": 32, "training_loss": 645.0989875793457, "training_acc": 58.0, "val_loss": 78.67835760116577, "val_acc": 68.0}
{"epoch": 33, "training_loss": 331.7670421600342, "training_acc": 76.0, "val_loss": 345.5815315246582, "val_acc": 72.0}
{"epoch": 34, "training_loss": 754.8474884033203, "training_acc": 77.0, "val_loss": 130.44817447662354, "val_acc": 64.0}
{"epoch": 35, "training_loss": 467.78938484191895, "training_acc": 69.0, "val_loss": 545.3286647796631, "val_acc": 72.0}
{"epoch": 36, "training_loss": 1630.6195068359375, "training_acc": 73.0, "val_loss": 241.4344072341919, "val_acc": 76.0}
{"epoch": 37, "training_loss": 569.8192481994629, "training_acc": 80.0, "val_loss": 202.8557300567627, "val_acc": 68.0}
{"epoch": 38, "training_loss": 480.5432548522949, "training_acc": 83.0, "val_loss": 241.83404445648193, "val_acc": 76.0}
{"epoch": 39, "training_loss": 464.00356674194336, "training_acc": 79.0, "val_loss": 181.156325340271, "val_acc": 68.0}
{"epoch": 40, "training_loss": 215.40333795547485, "training_acc": 84.0, "val_loss": 207.55822658538818, "val_acc": 52.0}
{"epoch": 41, "training_loss": 452.7810459136963, "training_acc": 71.0, "val_loss": 293.1455373764038, "val_acc": 60.0}
{"epoch": 42, "training_loss": 593.8142242431641, "training_acc": 66.0, "val_loss": 289.08772468566895, "val_acc": 72.0}
{"epoch": 43, "training_loss": 429.9745988845825, "training_acc": 78.0, "val_loss": 178.89559268951416, "val_acc": 68.0}
{"epoch": 44, "training_loss": 147.51358938217163, "training_acc": 89.0, "val_loss": 184.18740034103394, "val_acc": 68.0}
{"epoch": 45, "training_loss": 292.33057022094727, "training_acc": 78.0, "val_loss": 361.92381381988525, "val_acc": 72.0}
{"epoch": 46, "training_loss": 610.3748140335083, "training_acc": 79.0, "val_loss": 138.48605155944824, "val_acc": 64.0}
{"epoch": 47, "training_loss": 230.39531326293945, "training_acc": 80.0, "val_loss": 226.22406482696533, "val_acc": 68.0}
{"epoch": 48, "training_loss": 176.396381855011, "training_acc": 86.0, "val_loss": 321.23637199401855, "val_acc": 52.0}
{"epoch": 49, "training_loss": 1296.916633605957, "training_acc": 50.0, "val_loss": 628.0378341674805, "val_acc": 72.0}
{"epoch": 50, "training_loss": 1292.9334678649902, "training_acc": 74.0, "val_loss": 596.8332767486572, "val_acc": 36.0}
{"epoch": 51, "training_loss": 1594.0756511688232, "training_acc": 54.0, "val_loss": 648.0727195739746, "val_acc": 72.0}
