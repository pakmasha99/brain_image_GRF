"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 877.0298957824707, "training_acc": 68.0, "val_loss": 761.8624687194824, "val_acc": 72.0}
{"epoch": 1, "training_loss": 2383.974006652832, "training_acc": 72.0, "val_loss": 902.0029067993164, "val_acc": 28.0}
{"epoch": 2, "training_loss": 3545.1265716552734, "training_acc": 28.0, "val_loss": 97.06912040710449, "val_acc": 80.0}
{"epoch": 3, "training_loss": 650.4223213195801, "training_acc": 76.0, "val_loss": 577.9166221618652, "val_acc": 72.0}
{"epoch": 4, "training_loss": 2173.7972717285156, "training_acc": 72.0, "val_loss": 567.292594909668, "val_acc": 72.0}
{"epoch": 5, "training_loss": 1755.9251251220703, "training_acc": 72.0, "val_loss": 208.1761360168457, "val_acc": 72.0}
{"epoch": 6, "training_loss": 682.744665145874, "training_acc": 66.0, "val_loss": 343.07658672332764, "val_acc": 44.0}
{"epoch": 7, "training_loss": 1792.624439239502, "training_acc": 42.0, "val_loss": 177.40083932876587, "val_acc": 76.0}
{"epoch": 8, "training_loss": 753.7631816864014, "training_acc": 71.0, "val_loss": 351.27689838409424, "val_acc": 72.0}
{"epoch": 9, "training_loss": 1207.6503829956055, "training_acc": 72.0, "val_loss": 352.54454612731934, "val_acc": 72.0}
{"epoch": 10, "training_loss": 874.5553684234619, "training_acc": 76.0, "val_loss": 155.14276027679443, "val_acc": 80.0}
{"epoch": 11, "training_loss": 844.3517646789551, "training_acc": 64.0, "val_loss": 179.62888479232788, "val_acc": 56.0}
{"epoch": 12, "training_loss": 695.8898057937622, "training_acc": 58.0, "val_loss": 203.96056175231934, "val_acc": 72.0}
{"epoch": 13, "training_loss": 775.4544830322266, "training_acc": 74.0, "val_loss": 262.1408939361572, "val_acc": 72.0}
{"epoch": 14, "training_loss": 656.589358329773, "training_acc": 74.0, "val_loss": 104.61528301239014, "val_acc": 60.0}
{"epoch": 15, "training_loss": 560.5806274414062, "training_acc": 58.0, "val_loss": 82.82034397125244, "val_acc": 72.0}
{"epoch": 16, "training_loss": 192.1093521118164, "training_acc": 74.0, "val_loss": 208.85863304138184, "val_acc": 72.0}
{"epoch": 17, "training_loss": 656.8009090423584, "training_acc": 72.0, "val_loss": 104.621422290802, "val_acc": 56.0}
{"epoch": 18, "training_loss": 334.7693862915039, "training_acc": 66.0, "val_loss": 171.99538946151733, "val_acc": 52.0}
{"epoch": 19, "training_loss": 422.0444178581238, "training_acc": 53.0, "val_loss": 164.432692527771, "val_acc": 72.0}
{"epoch": 20, "training_loss": 478.07731437683105, "training_acc": 72.0, "val_loss": 85.40378212928772, "val_acc": 68.0}
{"epoch": 21, "training_loss": 244.97847747802734, "training_acc": 74.0, "val_loss": 96.23534083366394, "val_acc": 48.0}
{"epoch": 22, "training_loss": 304.01773023605347, "training_acc": 60.0, "val_loss": 127.72014141082764, "val_acc": 68.0}
{"epoch": 23, "training_loss": 256.3075489997864, "training_acc": 74.0, "val_loss": 77.03362703323364, "val_acc": 76.0}
{"epoch": 24, "training_loss": 198.12308883666992, "training_acc": 72.0, "val_loss": 71.16988897323608, "val_acc": 68.0}
{"epoch": 25, "training_loss": 161.41414833068848, "training_acc": 73.0, "val_loss": 64.08926248550415, "val_acc": 64.0}
{"epoch": 26, "training_loss": 194.84435653686523, "training_acc": 72.0, "val_loss": 43.99743676185608, "val_acc": 68.0}
{"epoch": 27, "training_loss": 98.35671091079712, "training_acc": 82.0, "val_loss": 69.33498978614807, "val_acc": 72.0}
{"epoch": 28, "training_loss": 137.67444825172424, "training_acc": 70.0, "val_loss": 23.584799468517303, "val_acc": 60.0}
{"epoch": 29, "training_loss": 134.87980365753174, "training_acc": 73.0, "val_loss": 41.83427393436432, "val_acc": 72.0}
{"epoch": 30, "training_loss": 140.12245655059814, "training_acc": 64.0, "val_loss": 32.488441467285156, "val_acc": 72.0}
{"epoch": 31, "training_loss": 75.08887922763824, "training_acc": 78.0, "val_loss": 18.42956244945526, "val_acc": 72.0}
{"epoch": 32, "training_loss": 62.2852942943573, "training_acc": 78.0, "val_loss": 37.53390312194824, "val_acc": 72.0}
{"epoch": 33, "training_loss": 67.36077511310577, "training_acc": 84.0, "val_loss": 40.2953565120697, "val_acc": 52.0}
{"epoch": 34, "training_loss": 210.65937757492065, "training_acc": 53.0, "val_loss": 60.01557111740112, "val_acc": 72.0}
{"epoch": 35, "training_loss": 124.52953124046326, "training_acc": 75.0, "val_loss": 25.017940998077393, "val_acc": 64.0}
{"epoch": 36, "training_loss": 125.27539110183716, "training_acc": 75.0, "val_loss": 95.65125107765198, "val_acc": 72.0}
{"epoch": 37, "training_loss": 186.28717756271362, "training_acc": 75.0, "val_loss": 96.39590382575989, "val_acc": 40.0}
{"epoch": 38, "training_loss": 299.3076648712158, "training_acc": 56.0, "val_loss": 88.00365924835205, "val_acc": 72.0}
{"epoch": 39, "training_loss": 185.60863161087036, "training_acc": 78.0, "val_loss": 33.526527881622314, "val_acc": 68.0}
{"epoch": 40, "training_loss": 94.3913722038269, "training_acc": 74.0, "val_loss": 33.13247561454773, "val_acc": 68.0}
{"epoch": 41, "training_loss": 105.01007175445557, "training_acc": 81.0, "val_loss": 39.84507918357849, "val_acc": 76.0}
{"epoch": 42, "training_loss": 92.02750253677368, "training_acc": 76.0, "val_loss": 24.13944900035858, "val_acc": 56.0}
{"epoch": 43, "training_loss": 106.31415367126465, "training_acc": 76.0, "val_loss": 36.80497705936432, "val_acc": 72.0}
{"epoch": 44, "training_loss": 165.12821769714355, "training_acc": 66.0, "val_loss": 58.66011381149292, "val_acc": 72.0}
{"epoch": 45, "training_loss": 160.1826295852661, "training_acc": 76.0, "val_loss": 28.443747758865356, "val_acc": 76.0}
{"epoch": 46, "training_loss": 121.27889060974121, "training_acc": 73.0, "val_loss": 42.28014647960663, "val_acc": 76.0}
{"epoch": 47, "training_loss": 116.48296642303467, "training_acc": 78.0, "val_loss": 42.85334646701813, "val_acc": 68.0}
{"epoch": 48, "training_loss": 81.97697353363037, "training_acc": 82.0, "val_loss": 44.768184423446655, "val_acc": 72.0}
{"epoch": 49, "training_loss": 164.60829496383667, "training_acc": 72.0, "val_loss": 98.150235414505, "val_acc": 72.0}
{"epoch": 50, "training_loss": 180.64906930923462, "training_acc": 77.0, "val_loss": 53.87389063835144, "val_acc": 64.0}
