"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 931.165168762207, "training_acc": 72.0, "val_loss": 522.15576171875, "val_acc": 72.0}
{"epoch": 1, "training_loss": 1571.3215675354004, "training_acc": 72.0, "val_loss": 945.0973510742188, "val_acc": 28.0}
{"epoch": 2, "training_loss": 3510.6612091064453, "training_acc": 28.0, "val_loss": 40.59471786022186, "val_acc": 28.0}
{"epoch": 3, "training_loss": 296.2661609649658, "training_acc": 56.0, "val_loss": 653.2292366027832, "val_acc": 72.0}
{"epoch": 4, "training_loss": 2873.7662353515625, "training_acc": 72.0, "val_loss": 926.9271850585938, "val_acc": 72.0}
{"epoch": 5, "training_loss": 3708.709716796875, "training_acc": 72.0, "val_loss": 877.0211219787598, "val_acc": 72.0}
{"epoch": 6, "training_loss": 3346.424835205078, "training_acc": 72.0, "val_loss": 600.0777244567871, "val_acc": 72.0}
{"epoch": 7, "training_loss": 2179.5512771606445, "training_acc": 72.0, "val_loss": 137.91418075561523, "val_acc": 72.0}
{"epoch": 8, "training_loss": 927.3093032836914, "training_acc": 54.0, "val_loss": 565.2865886688232, "val_acc": 28.0}
{"epoch": 9, "training_loss": 1993.7917594909668, "training_acc": 28.0, "val_loss": 39.79745805263519, "val_acc": 72.0}
{"epoch": 10, "training_loss": 234.4135284423828, "training_acc": 72.0, "val_loss": 217.67935752868652, "val_acc": 72.0}
{"epoch": 11, "training_loss": 890.2150840759277, "training_acc": 72.0, "val_loss": 195.0723648071289, "val_acc": 72.0}
{"epoch": 12, "training_loss": 677.5140228271484, "training_acc": 72.0, "val_loss": 37.58710324764252, "val_acc": 28.0}
{"epoch": 13, "training_loss": 210.39098262786865, "training_acc": 28.0, "val_loss": 98.31067323684692, "val_acc": 72.0}
{"epoch": 14, "training_loss": 476.9711093902588, "training_acc": 72.0, "val_loss": 182.051420211792, "val_acc": 72.0}
{"epoch": 15, "training_loss": 682.4004135131836, "training_acc": 72.0, "val_loss": 74.43428039550781, "val_acc": 72.0}
{"epoch": 16, "training_loss": 392.08025550842285, "training_acc": 52.0, "val_loss": 60.88979244232178, "val_acc": 28.0}
{"epoch": 17, "training_loss": 347.8675174713135, "training_acc": 40.0, "val_loss": 175.15896558761597, "val_acc": 72.0}
{"epoch": 18, "training_loss": 728.0872077941895, "training_acc": 72.0, "val_loss": 159.8449945449829, "val_acc": 72.0}
{"epoch": 19, "training_loss": 522.6462116241455, "training_acc": 72.0, "val_loss": 71.3999092578888, "val_acc": 28.0}
{"epoch": 20, "training_loss": 262.3063097000122, "training_acc": 28.0, "val_loss": 93.61881613731384, "val_acc": 72.0}
{"epoch": 21, "training_loss": 460.4564514160156, "training_acc": 72.0, "val_loss": 165.81127643585205, "val_acc": 72.0}
{"epoch": 22, "training_loss": 613.2865962982178, "training_acc": 72.0, "val_loss": 53.96003723144531, "val_acc": 72.0}
{"epoch": 23, "training_loss": 369.8719654083252, "training_acc": 54.0, "val_loss": 104.66327667236328, "val_acc": 28.0}
{"epoch": 24, "training_loss": 362.3931188583374, "training_acc": 46.0, "val_loss": 153.7987232208252, "val_acc": 72.0}
{"epoch": 25, "training_loss": 669.8129081726074, "training_acc": 72.0, "val_loss": 152.71117687225342, "val_acc": 72.0}
{"epoch": 26, "training_loss": 512.3471326828003, "training_acc": 72.0, "val_loss": 68.45152378082275, "val_acc": 28.0}
{"epoch": 27, "training_loss": 241.3052797317505, "training_acc": 28.0, "val_loss": 97.19144105911255, "val_acc": 72.0}
{"epoch": 28, "training_loss": 442.00962829589844, "training_acc": 72.0, "val_loss": 171.80474996566772, "val_acc": 72.0}
{"epoch": 29, "training_loss": 646.9966487884521, "training_acc": 72.0, "val_loss": 75.98937749862671, "val_acc": 72.0}
{"epoch": 30, "training_loss": 310.7910318374634, "training_acc": 54.0, "val_loss": 27.33781337738037, "val_acc": 28.0}
{"epoch": 31, "training_loss": 198.5261116027832, "training_acc": 42.0, "val_loss": 120.62797546386719, "val_acc": 72.0}
{"epoch": 32, "training_loss": 470.55743980407715, "training_acc": 72.0, "val_loss": 57.19730854034424, "val_acc": 72.0}
{"epoch": 33, "training_loss": 281.51334953308105, "training_acc": 56.0, "val_loss": 29.296356439590454, "val_acc": 28.0}
{"epoch": 34, "training_loss": 205.6734037399292, "training_acc": 44.0, "val_loss": 142.21700429916382, "val_acc": 72.0}
{"epoch": 35, "training_loss": 583.0015525817871, "training_acc": 72.0, "val_loss": 101.80590152740479, "val_acc": 72.0}
{"epoch": 36, "training_loss": 331.9490222930908, "training_acc": 69.0, "val_loss": 156.08612298965454, "val_acc": 28.0}
{"epoch": 37, "training_loss": 442.35887908935547, "training_acc": 30.0, "val_loss": 101.82245969772339, "val_acc": 72.0}
{"epoch": 38, "training_loss": 443.278790473938, "training_acc": 72.0, "val_loss": 163.12226057052612, "val_acc": 72.0}
{"epoch": 39, "training_loss": 606.0174617767334, "training_acc": 72.0, "val_loss": 59.526485204696655, "val_acc": 72.0}
{"epoch": 40, "training_loss": 364.2775630950928, "training_acc": 54.0, "val_loss": 108.50783586502075, "val_acc": 28.0}
{"epoch": 41, "training_loss": 382.5448579788208, "training_acc": 44.0, "val_loss": 144.88030672073364, "val_acc": 72.0}
{"epoch": 42, "training_loss": 589.6332473754883, "training_acc": 72.0, "val_loss": 132.2825312614441, "val_acc": 72.0}
{"epoch": 43, "training_loss": 424.1485118865967, "training_acc": 72.0, "val_loss": 88.04476857185364, "val_acc": 28.0}
{"epoch": 44, "training_loss": 314.2082862854004, "training_acc": 28.0, "val_loss": 99.19382333755493, "val_acc": 72.0}
{"epoch": 45, "training_loss": 513.1771812438965, "training_acc": 72.0, "val_loss": 193.4373140335083, "val_acc": 72.0}
{"epoch": 46, "training_loss": 731.3260154724121, "training_acc": 72.0, "val_loss": 97.10106253623962, "val_acc": 72.0}
{"epoch": 47, "training_loss": 290.1479911804199, "training_acc": 56.0, "val_loss": 85.25791764259338, "val_acc": 28.0}
{"epoch": 48, "training_loss": 321.0953092575073, "training_acc": 40.0, "val_loss": 81.63653612136841, "val_acc": 72.0}
{"epoch": 49, "training_loss": 298.3168077468872, "training_acc": 72.0, "val_loss": 12.783065438270569, "val_acc": 72.0}
{"epoch": 50, "training_loss": 149.14245510101318, "training_acc": 60.0, "val_loss": 27.83360779285431, "val_acc": 72.0}
{"epoch": 51, "training_loss": 158.48802089691162, "training_acc": 72.0, "val_loss": 23.35553616285324, "val_acc": 72.0}
{"epoch": 52, "training_loss": 176.85063076019287, "training_acc": 60.0, "val_loss": 12.937028706073761, "val_acc": 72.0}
{"epoch": 53, "training_loss": 94.76727151870728, "training_acc": 74.0, "val_loss": 30.52312731742859, "val_acc": 72.0}
{"epoch": 54, "training_loss": 142.11605978012085, "training_acc": 56.0, "val_loss": 27.81589925289154, "val_acc": 72.0}
{"epoch": 55, "training_loss": 113.5152735710144, "training_acc": 72.0, "val_loss": 15.855197608470917, "val_acc": 60.0}
{"epoch": 56, "training_loss": 63.350982904434204, "training_acc": 66.0, "val_loss": 35.875311493873596, "val_acc": 72.0}
{"epoch": 57, "training_loss": 131.18328523635864, "training_acc": 72.0, "val_loss": 37.279319763183594, "val_acc": 28.0}
{"epoch": 58, "training_loss": 124.21387577056885, "training_acc": 42.0, "val_loss": 22.80624955892563, "val_acc": 72.0}
{"epoch": 59, "training_loss": 90.4517502784729, "training_acc": 64.0, "val_loss": 22.327975928783417, "val_acc": 72.0}
{"epoch": 60, "training_loss": 76.68601107597351, "training_acc": 72.0, "val_loss": 37.608444690704346, "val_acc": 28.0}
{"epoch": 61, "training_loss": 100.09970736503601, "training_acc": 56.0, "val_loss": 52.36889719963074, "val_acc": 72.0}
{"epoch": 62, "training_loss": 193.2842502593994, "training_acc": 72.0, "val_loss": 67.98003911972046, "val_acc": 28.0}
{"epoch": 63, "training_loss": 181.88488566875458, "training_acc": 48.0, "val_loss": 44.6877658367157, "val_acc": 72.0}
{"epoch": 64, "training_loss": 156.29469299316406, "training_acc": 72.0, "val_loss": 29.028698801994324, "val_acc": 36.0}
{"epoch": 65, "training_loss": 85.84026789665222, "training_acc": 55.0, "val_loss": 20.10219097137451, "val_acc": 72.0}
{"epoch": 66, "training_loss": 58.41274642944336, "training_acc": 76.0, "val_loss": 20.132987201213837, "val_acc": 48.0}
{"epoch": 67, "training_loss": 122.58080101013184, "training_acc": 58.0, "val_loss": 37.106314301490784, "val_acc": 72.0}
{"epoch": 68, "training_loss": 153.33309888839722, "training_acc": 52.0, "val_loss": 30.141592025756836, "val_acc": 72.0}
