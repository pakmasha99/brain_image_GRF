"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 1129.427848815918, "training_acc": 42.0, "val_loss": 579.0812969207764, "val_acc": 72.0}
{"epoch": 1, "training_loss": 1780.8330116271973, "training_acc": 72.0, "val_loss": 440.5904293060303, "val_acc": 28.0}
{"epoch": 2, "training_loss": 1573.3950729370117, "training_acc": 28.0, "val_loss": 146.1513638496399, "val_acc": 72.0}
{"epoch": 3, "training_loss": 798.6963195800781, "training_acc": 72.0, "val_loss": 351.34618282318115, "val_acc": 72.0}
{"epoch": 4, "training_loss": 1366.2778511047363, "training_acc": 72.0, "val_loss": 230.99002838134766, "val_acc": 72.0}
{"epoch": 5, "training_loss": 776.9975218772888, "training_acc": 72.0, "val_loss": 327.6592969894409, "val_acc": 28.0}
{"epoch": 6, "training_loss": 1303.2733154296875, "training_acc": 28.0, "val_loss": 18.88926774263382, "val_acc": 72.0}
{"epoch": 7, "training_loss": 151.99669551849365, "training_acc": 72.0, "val_loss": 130.38935661315918, "val_acc": 72.0}
{"epoch": 8, "training_loss": 486.4389190673828, "training_acc": 72.0, "val_loss": 31.362107396125793, "val_acc": 72.0}
{"epoch": 9, "training_loss": 499.7547149658203, "training_acc": 50.0, "val_loss": 164.32064771652222, "val_acc": 28.0}
{"epoch": 10, "training_loss": 543.8161964416504, "training_acc": 44.0, "val_loss": 173.25586080551147, "val_acc": 72.0}
{"epoch": 11, "training_loss": 735.7936401367188, "training_acc": 72.0, "val_loss": 187.66882419586182, "val_acc": 72.0}
{"epoch": 12, "training_loss": 662.4502639770508, "training_acc": 72.0, "val_loss": 19.447776675224304, "val_acc": 72.0}
{"epoch": 13, "training_loss": 578.3619117736816, "training_acc": 48.0, "val_loss": 270.24850845336914, "val_acc": 28.0}
{"epoch": 14, "training_loss": 764.5156164169312, "training_acc": 40.0, "val_loss": 94.71938610076904, "val_acc": 72.0}
{"epoch": 15, "training_loss": 417.98067474365234, "training_acc": 72.0, "val_loss": 87.78864741325378, "val_acc": 72.0}
{"epoch": 16, "training_loss": 234.11509799957275, "training_acc": 72.0, "val_loss": 218.95437240600586, "val_acc": 28.0}
{"epoch": 17, "training_loss": 831.1437683105469, "training_acc": 28.0, "val_loss": 57.81295895576477, "val_acc": 72.0}
{"epoch": 18, "training_loss": 386.5222587585449, "training_acc": 72.0, "val_loss": 163.79293203353882, "val_acc": 72.0}
{"epoch": 19, "training_loss": 620.1889934539795, "training_acc": 72.0, "val_loss": 72.15355038642883, "val_acc": 72.0}
{"epoch": 20, "training_loss": 300.2757453918457, "training_acc": 54.0, "val_loss": 18.141449987888336, "val_acc": 28.0}
{"epoch": 21, "training_loss": 188.93281936645508, "training_acc": 47.0, "val_loss": 91.92548990249634, "val_acc": 72.0}
{"epoch": 22, "training_loss": 323.61149311065674, "training_acc": 72.0, "val_loss": 20.53549587726593, "val_acc": 28.0}
{"epoch": 23, "training_loss": 89.93715238571167, "training_acc": 28.0, "val_loss": 61.7709755897522, "val_acc": 72.0}
{"epoch": 24, "training_loss": 272.53256607055664, "training_acc": 72.0, "val_loss": 55.15490770339966, "val_acc": 72.0}
{"epoch": 25, "training_loss": 175.58295345306396, "training_acc": 58.0, "val_loss": 13.714796304702759, "val_acc": 72.0}
{"epoch": 26, "training_loss": 55.006333112716675, "training_acc": 72.0, "val_loss": 16.1330446600914, "val_acc": 40.0}
{"epoch": 27, "training_loss": 84.08882188796997, "training_acc": 62.0, "val_loss": 17.0244038105011, "val_acc": 72.0}
{"epoch": 28, "training_loss": 156.74433422088623, "training_acc": 54.0, "val_loss": 46.34292423725128, "val_acc": 72.0}
{"epoch": 29, "training_loss": 217.3619794845581, "training_acc": 72.0, "val_loss": 48.280444741249084, "val_acc": 72.0}
{"epoch": 30, "training_loss": 239.38075160980225, "training_acc": 50.0, "val_loss": 32.19093382358551, "val_acc": 72.0}
{"epoch": 31, "training_loss": 161.91581439971924, "training_acc": 72.0, "val_loss": 12.6609206199646, "val_acc": 76.0}
{"epoch": 32, "training_loss": 168.29581260681152, "training_acc": 57.0, "val_loss": 48.636290431022644, "val_acc": 72.0}
{"epoch": 33, "training_loss": 222.91797256469727, "training_acc": 72.0, "val_loss": 65.40653705596924, "val_acc": 72.0}
{"epoch": 34, "training_loss": 175.35795187950134, "training_acc": 72.0, "val_loss": 168.78952980041504, "val_acc": 28.0}
{"epoch": 35, "training_loss": 534.8962383270264, "training_acc": 28.0, "val_loss": 116.81751012802124, "val_acc": 72.0}
{"epoch": 36, "training_loss": 573.0209445953369, "training_acc": 72.0, "val_loss": 238.84320259094238, "val_acc": 72.0}
{"epoch": 37, "training_loss": 946.7386207580566, "training_acc": 72.0, "val_loss": 172.1350073814392, "val_acc": 72.0}
{"epoch": 38, "training_loss": 523.8556489944458, "training_acc": 72.0, "val_loss": 168.91545057296753, "val_acc": 28.0}
{"epoch": 39, "training_loss": 687.7628574371338, "training_acc": 28.0, "val_loss": 26.507285237312317, "val_acc": 72.0}
{"epoch": 40, "training_loss": 188.1606969833374, "training_acc": 72.0, "val_loss": 69.94187235832214, "val_acc": 72.0}
{"epoch": 41, "training_loss": 197.81030702590942, "training_acc": 72.0, "val_loss": 155.7898998260498, "val_acc": 28.0}
{"epoch": 42, "training_loss": 502.4470500946045, "training_acc": 28.0, "val_loss": 115.89016914367676, "val_acc": 72.0}
{"epoch": 43, "training_loss": 553.7823085784912, "training_acc": 72.0, "val_loss": 238.46697807312012, "val_acc": 72.0}
{"epoch": 44, "training_loss": 934.9811172485352, "training_acc": 72.0, "val_loss": 176.13165378570557, "val_acc": 72.0}
{"epoch": 45, "training_loss": 555.5021381378174, "training_acc": 72.0, "val_loss": 101.70650482177734, "val_acc": 28.0}
{"epoch": 46, "training_loss": 453.77735328674316, "training_acc": 28.0, "val_loss": 63.00453543663025, "val_acc": 72.0}
{"epoch": 47, "training_loss": 358.1280269622803, "training_acc": 72.0, "val_loss": 134.28510427474976, "val_acc": 72.0}
{"epoch": 48, "training_loss": 480.3669099807739, "training_acc": 72.0, "val_loss": 16.23338907957077, "val_acc": 72.0}
{"epoch": 49, "training_loss": 367.5961265563965, "training_acc": 54.0, "val_loss": 134.1002345085144, "val_acc": 28.0}
{"epoch": 50, "training_loss": 425.9289073944092, "training_acc": 48.0, "val_loss": 161.19500398635864, "val_acc": 72.0}
