"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 300 --layer_control tune_all --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 165.28737878799438, "training_acc": 67.33333333333333, "val_loss": 103.68291139602661, "val_acc": 28.0}
{"epoch": 1, "training_loss": 1025.6659970283508, "training_acc": 63.0, "val_loss": 45.57483211159706, "val_acc": 72.0}
{"epoch": 2, "training_loss": 185.44460821151733, "training_acc": 72.66666666666667, "val_loss": 54.72637140750885, "val_acc": 72.0}
{"epoch": 3, "training_loss": 243.72680735588074, "training_acc": 62.333333333333336, "val_loss": 51.294154703617096, "val_acc": 72.0}
{"epoch": 4, "training_loss": 186.32315254211426, "training_acc": 72.33333333333333, "val_loss": 46.304378271102905, "val_acc": 28.0}
{"epoch": 5, "training_loss": 176.1759340763092, "training_acc": 72.33333333333333, "val_loss": 47.31666415929794, "val_acc": 72.0}
{"epoch": 6, "training_loss": 182.35180115699768, "training_acc": 72.33333333333333, "val_loss": 45.17950302362442, "val_acc": 72.0}
{"epoch": 7, "training_loss": 178.67179250717163, "training_acc": 72.33333333333333, "val_loss": 44.31129777431488, "val_acc": 72.0}
{"epoch": 8, "training_loss": 176.36275100708008, "training_acc": 72.33333333333333, "val_loss": 44.56053936481476, "val_acc": 72.0}
{"epoch": 9, "training_loss": 174.03314542770386, "training_acc": 72.33333333333333, "val_loss": 44.36866095662117, "val_acc": 72.0}
{"epoch": 10, "training_loss": 172.2334599494934, "training_acc": 72.33333333333333, "val_loss": 44.29163694381714, "val_acc": 72.0}
{"epoch": 11, "training_loss": 169.2067732810974, "training_acc": 72.33333333333333, "val_loss": 43.728411704301834, "val_acc": 64.0}
{"epoch": 12, "training_loss": 164.19106888771057, "training_acc": 71.66666666666667, "val_loss": 43.91000089049339, "val_acc": 66.66666666666667}
{"epoch": 13, "training_loss": 162.30900168418884, "training_acc": 73.66666666666667, "val_loss": 46.42632633447647, "val_acc": 46.666666666666664}
{"epoch": 14, "training_loss": 170.71205306053162, "training_acc": 72.33333333333333, "val_loss": 43.68584954738617, "val_acc": 69.33333333333333}
{"epoch": 15, "training_loss": 170.35367941856384, "training_acc": 72.0, "val_loss": 43.30527061223984, "val_acc": 70.66666666666667}
{"epoch": 16, "training_loss": 179.49906945228577, "training_acc": 72.33333333333333, "val_loss": 43.9809912443161, "val_acc": 69.33333333333333}
{"epoch": 17, "training_loss": 175.23797225952148, "training_acc": 72.33333333333333, "val_loss": 45.19731479883194, "val_acc": 72.0}
{"epoch": 18, "training_loss": 173.08299779891968, "training_acc": 72.33333333333333, "val_loss": 43.84697788953781, "val_acc": 72.0}
{"epoch": 19, "training_loss": 168.80682349205017, "training_acc": 72.33333333333333, "val_loss": 43.47271943092346, "val_acc": 70.66666666666667}
{"epoch": 20, "training_loss": 161.17915046215057, "training_acc": 72.33333333333333, "val_loss": 44.555511355400085, "val_acc": 66.66666666666667}
{"epoch": 21, "training_loss": 183.44413304328918, "training_acc": 69.66666666666667, "val_loss": 45.753111362457275, "val_acc": 49.333333333333336}
{"epoch": 22, "training_loss": 179.39094519615173, "training_acc": 72.33333333333333, "val_loss": 45.55044758319855, "val_acc": 72.0}
{"epoch": 23, "training_loss": 172.2951111793518, "training_acc": 72.33333333333333, "val_loss": 43.38270151615143, "val_acc": 68.0}
{"epoch": 24, "training_loss": 170.17222547531128, "training_acc": 72.33333333333333, "val_loss": 43.22274512052536, "val_acc": 68.0}
{"epoch": 25, "training_loss": 153.1104212999344, "training_acc": 72.33333333333333, "val_loss": 48.03008195757866, "val_acc": 66.66666666666667}
{"epoch": 26, "training_loss": 154.48504447937012, "training_acc": 74.33333333333333, "val_loss": 45.292722284793854, "val_acc": 65.33333333333333}
{"epoch": 27, "training_loss": 159.47283554077148, "training_acc": 73.33333333333333, "val_loss": 45.15829873085022, "val_acc": 52.0}
{"epoch": 28, "training_loss": 159.69130396842957, "training_acc": 74.0, "val_loss": 45.64832752943039, "val_acc": 65.33333333333333}
{"epoch": 29, "training_loss": 156.91099166870117, "training_acc": 75.33333333333333, "val_loss": 46.98376202583313, "val_acc": 68.0}
{"epoch": 30, "training_loss": 167.54783058166504, "training_acc": 69.66666666666667, "val_loss": 43.41399383544922, "val_acc": 66.66666666666667}
{"epoch": 31, "training_loss": 157.7447714805603, "training_acc": 72.0, "val_loss": 43.25387477874756, "val_acc": 66.66666666666667}
{"epoch": 32, "training_loss": 155.05797457695007, "training_acc": 73.66666666666667, "val_loss": 44.03928396105766, "val_acc": 69.33333333333333}
{"epoch": 33, "training_loss": 156.6595208644867, "training_acc": 74.33333333333333, "val_loss": 45.4724583029747, "val_acc": 66.66666666666667}
{"epoch": 34, "training_loss": 149.81557250022888, "training_acc": 74.0, "val_loss": 44.54527509212494, "val_acc": 61.333333333333336}
{"epoch": 35, "training_loss": 153.06724560260773, "training_acc": 73.66666666666667, "val_loss": 44.6313282251358, "val_acc": 61.333333333333336}
{"epoch": 36, "training_loss": 147.47334337234497, "training_acc": 73.0, "val_loss": 53.68878549337387, "val_acc": 69.33333333333333}
{"epoch": 37, "training_loss": 161.24937677383423, "training_acc": 73.33333333333333, "val_loss": 46.46314615011215, "val_acc": 50.666666666666664}
{"epoch": 38, "training_loss": 157.7877209186554, "training_acc": 73.66666666666667, "val_loss": 44.901453107595444, "val_acc": 66.66666666666667}
{"epoch": 39, "training_loss": 159.4182333946228, "training_acc": 72.33333333333333, "val_loss": 44.09506797790527, "val_acc": 60.0}
{"epoch": 40, "training_loss": 158.3712043762207, "training_acc": 73.33333333333333, "val_loss": 43.99868428707123, "val_acc": 68.0}
{"epoch": 41, "training_loss": 154.14428985118866, "training_acc": 74.33333333333333, "val_loss": 43.28558200597763, "val_acc": 65.33333333333333}
{"epoch": 42, "training_loss": 155.99579334259033, "training_acc": 74.0, "val_loss": 43.50626742839813, "val_acc": 68.0}
{"epoch": 43, "training_loss": 156.45371007919312, "training_acc": 74.0, "val_loss": 45.764229983091354, "val_acc": 66.66666666666667}
