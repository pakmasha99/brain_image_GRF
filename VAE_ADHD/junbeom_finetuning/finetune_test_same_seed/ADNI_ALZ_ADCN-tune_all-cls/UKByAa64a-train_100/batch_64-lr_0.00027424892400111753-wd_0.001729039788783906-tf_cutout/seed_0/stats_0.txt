"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 69.6796886920929, "training_acc": 42.0, "val_loss": 16.24780297279358, "val_acc": 28.0}
{"epoch": 1, "training_loss": 64.59003925323486, "training_acc": 72.0, "val_loss": 15.303567051887512, "val_acc": 64.0}
{"epoch": 2, "training_loss": 60.06943678855896, "training_acc": 72.0, "val_loss": 14.805944263935089, "val_acc": 72.0}
{"epoch": 3, "training_loss": 59.20368671417236, "training_acc": 72.0, "val_loss": 14.784704148769379, "val_acc": 72.0}
{"epoch": 4, "training_loss": 59.29413986206055, "training_acc": 72.0, "val_loss": 14.78419303894043, "val_acc": 72.0}
{"epoch": 5, "training_loss": 59.739465951919556, "training_acc": 72.0, "val_loss": 14.831525087356567, "val_acc": 72.0}
{"epoch": 6, "training_loss": 59.388684034347534, "training_acc": 72.0, "val_loss": 14.87274169921875, "val_acc": 72.0}
{"epoch": 7, "training_loss": 59.74458909034729, "training_acc": 72.0, "val_loss": 14.86952006816864, "val_acc": 72.0}
{"epoch": 8, "training_loss": 59.53157377243042, "training_acc": 72.0, "val_loss": 14.844317734241486, "val_acc": 72.0}
{"epoch": 9, "training_loss": 59.69516658782959, "training_acc": 72.0, "val_loss": 14.826968312263489, "val_acc": 72.0}
{"epoch": 10, "training_loss": 59.318806648254395, "training_acc": 72.0, "val_loss": 14.833469688892365, "val_acc": 72.0}
{"epoch": 11, "training_loss": 59.32675886154175, "training_acc": 72.0, "val_loss": 14.812791347503662, "val_acc": 72.0}
{"epoch": 12, "training_loss": 59.30997681617737, "training_acc": 72.0, "val_loss": 14.804953336715698, "val_acc": 72.0}
{"epoch": 13, "training_loss": 59.53040099143982, "training_acc": 72.0, "val_loss": 14.811263978481293, "val_acc": 72.0}
{"epoch": 14, "training_loss": 59.48686337471008, "training_acc": 72.0, "val_loss": 14.85317349433899, "val_acc": 72.0}
{"epoch": 15, "training_loss": 59.39396810531616, "training_acc": 72.0, "val_loss": 14.813515543937683, "val_acc": 72.0}
{"epoch": 16, "training_loss": 59.16910719871521, "training_acc": 72.0, "val_loss": 14.80625867843628, "val_acc": 72.0}
{"epoch": 17, "training_loss": 59.56714224815369, "training_acc": 72.0, "val_loss": 14.815513789653778, "val_acc": 72.0}
{"epoch": 18, "training_loss": 59.5460467338562, "training_acc": 72.0, "val_loss": 14.796382188796997, "val_acc": 72.0}
{"epoch": 19, "training_loss": 59.33399200439453, "training_acc": 72.0, "val_loss": 14.780126512050629, "val_acc": 72.0}
{"epoch": 20, "training_loss": 59.10241389274597, "training_acc": 72.0, "val_loss": 14.779695868492126, "val_acc": 72.0}
{"epoch": 21, "training_loss": 59.28312849998474, "training_acc": 72.0, "val_loss": 14.716829359531403, "val_acc": 72.0}
{"epoch": 22, "training_loss": 58.9537353515625, "training_acc": 72.0, "val_loss": 14.653053879737854, "val_acc": 72.0}
{"epoch": 23, "training_loss": 58.687812089920044, "training_acc": 72.0, "val_loss": 14.785747230052948, "val_acc": 72.0}
{"epoch": 24, "training_loss": 59.01582312583923, "training_acc": 72.0, "val_loss": 14.644630253314972, "val_acc": 72.0}
{"epoch": 25, "training_loss": 58.53590512275696, "training_acc": 72.0, "val_loss": 14.445549249649048, "val_acc": 72.0}
{"epoch": 26, "training_loss": 57.81354331970215, "training_acc": 72.0, "val_loss": 14.423513412475586, "val_acc": 72.0}
{"epoch": 27, "training_loss": 57.94180965423584, "training_acc": 72.0, "val_loss": 14.375314116477966, "val_acc": 72.0}
{"epoch": 28, "training_loss": 57.72812509536743, "training_acc": 72.0, "val_loss": 14.906735718250275, "val_acc": 72.0}
{"epoch": 29, "training_loss": 58.50978970527649, "training_acc": 72.0, "val_loss": 14.172026515007019, "val_acc": 72.0}
{"epoch": 30, "training_loss": 58.91009044647217, "training_acc": 72.0, "val_loss": 14.6061509847641, "val_acc": 72.0}
{"epoch": 31, "training_loss": 59.94269418716431, "training_acc": 72.0, "val_loss": 14.501282572746277, "val_acc": 72.0}
{"epoch": 32, "training_loss": 58.30921697616577, "training_acc": 72.0, "val_loss": 14.47964608669281, "val_acc": 72.0}
{"epoch": 33, "training_loss": 58.07738995552063, "training_acc": 72.0, "val_loss": 14.297811686992645, "val_acc": 72.0}
{"epoch": 34, "training_loss": 57.87545084953308, "training_acc": 72.0, "val_loss": 14.195297658443451, "val_acc": 72.0}
{"epoch": 35, "training_loss": 55.926655292510986, "training_acc": 72.0, "val_loss": 14.02646005153656, "val_acc": 72.0}
{"epoch": 36, "training_loss": 54.339229106903076, "training_acc": 72.0, "val_loss": 15.572598576545715, "val_acc": 72.0}
{"epoch": 37, "training_loss": 54.99047780036926, "training_acc": 72.0, "val_loss": 14.946874976158142, "val_acc": 44.0}
{"epoch": 38, "training_loss": 61.36311435699463, "training_acc": 72.0, "val_loss": 14.737190306186676, "val_acc": 72.0}
{"epoch": 39, "training_loss": 60.32827687263489, "training_acc": 72.0, "val_loss": 14.773869514465332, "val_acc": 72.0}
{"epoch": 40, "training_loss": 59.31299543380737, "training_acc": 72.0, "val_loss": 14.720268547534943, "val_acc": 72.0}
{"epoch": 41, "training_loss": 58.772446632385254, "training_acc": 72.0, "val_loss": 14.681348204612732, "val_acc": 72.0}
{"epoch": 42, "training_loss": 58.700698137283325, "training_acc": 72.0, "val_loss": 14.664056897163391, "val_acc": 72.0}
{"epoch": 43, "training_loss": 58.63750982284546, "training_acc": 72.0, "val_loss": 14.690832793712616, "val_acc": 72.0}
{"epoch": 44, "training_loss": 58.441590309143066, "training_acc": 72.0, "val_loss": 14.486141502857208, "val_acc": 72.0}
{"epoch": 45, "training_loss": 57.846856355667114, "training_acc": 72.0, "val_loss": 14.297260344028473, "val_acc": 72.0}
{"epoch": 46, "training_loss": 57.59507393836975, "training_acc": 72.0, "val_loss": 14.920587837696075, "val_acc": 72.0}
{"epoch": 47, "training_loss": 59.33175277709961, "training_acc": 72.0, "val_loss": 13.979324698448181, "val_acc": 72.0}
{"epoch": 48, "training_loss": 58.049675941467285, "training_acc": 72.0, "val_loss": 15.16619473695755, "val_acc": 72.0}
{"epoch": 49, "training_loss": 60.10061740875244, "training_acc": 72.0, "val_loss": 14.840278029441833, "val_acc": 72.0}
{"epoch": 50, "training_loss": 59.22443890571594, "training_acc": 72.0, "val_loss": 14.844219386577606, "val_acc": 72.0}
{"epoch": 51, "training_loss": 59.37871265411377, "training_acc": 72.0, "val_loss": 14.87324982881546, "val_acc": 72.0}
{"epoch": 52, "training_loss": 59.46152591705322, "training_acc": 72.0, "val_loss": 14.828334748744965, "val_acc": 72.0}
{"epoch": 53, "training_loss": 59.621068239212036, "training_acc": 72.0, "val_loss": 14.822860062122345, "val_acc": 72.0}
{"epoch": 54, "training_loss": 59.27679753303528, "training_acc": 72.0, "val_loss": 14.823102951049805, "val_acc": 72.0}
{"epoch": 55, "training_loss": 59.36838889122009, "training_acc": 72.0, "val_loss": 14.824149012565613, "val_acc": 72.0}
{"epoch": 56, "training_loss": 59.33605217933655, "training_acc": 72.0, "val_loss": 14.828512072563171, "val_acc": 72.0}
{"epoch": 57, "training_loss": 59.36521816253662, "training_acc": 72.0, "val_loss": 14.83415812253952, "val_acc": 72.0}
{"epoch": 58, "training_loss": 59.723559617996216, "training_acc": 72.0, "val_loss": 14.817997813224792, "val_acc": 72.0}
{"epoch": 59, "training_loss": 59.38688659667969, "training_acc": 72.0, "val_loss": 14.812862873077393, "val_acc": 72.0}
{"epoch": 60, "training_loss": 59.37636637687683, "training_acc": 72.0, "val_loss": 14.799351990222931, "val_acc": 72.0}
{"epoch": 61, "training_loss": 59.14205265045166, "training_acc": 72.0, "val_loss": 14.804063737392426, "val_acc": 72.0}
{"epoch": 62, "training_loss": 59.26845860481262, "training_acc": 72.0, "val_loss": 14.795869588851929, "val_acc": 72.0}
{"epoch": 63, "training_loss": 59.26830720901489, "training_acc": 72.0, "val_loss": 14.757511019706726, "val_acc": 72.0}
{"epoch": 64, "training_loss": 59.168694734573364, "training_acc": 72.0, "val_loss": 14.854025840759277, "val_acc": 72.0}
{"epoch": 65, "training_loss": 59.57277178764343, "training_acc": 72.0, "val_loss": 14.846286177635193, "val_acc": 72.0}
{"epoch": 66, "training_loss": 59.552517890930176, "training_acc": 72.0, "val_loss": 14.828930795192719, "val_acc": 72.0}
