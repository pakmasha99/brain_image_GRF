"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 73.7524185180664, "training_acc": 28.0, "val_loss": 18.482062220573425, "val_acc": 28.0}
{"epoch": 1, "training_loss": 73.33538937568665, "training_acc": 28.0, "val_loss": 18.229466676712036, "val_acc": 28.0}
{"epoch": 2, "training_loss": 72.49718880653381, "training_acc": 28.0, "val_loss": 17.95358657836914, "val_acc": 28.0}
{"epoch": 3, "training_loss": 71.37100958824158, "training_acc": 28.0, "val_loss": 17.69605278968811, "val_acc": 28.0}
{"epoch": 4, "training_loss": 70.33156633377075, "training_acc": 28.0, "val_loss": 17.45154708623886, "val_acc": 28.0}
{"epoch": 5, "training_loss": 69.54141616821289, "training_acc": 43.0, "val_loss": 17.234444618225098, "val_acc": 28.0}
{"epoch": 6, "training_loss": 68.63164067268372, "training_acc": 71.0, "val_loss": 17.037394642829895, "val_acc": 28.0}
{"epoch": 7, "training_loss": 67.94874501228333, "training_acc": 72.0, "val_loss": 16.851337254047394, "val_acc": 28.0}
{"epoch": 8, "training_loss": 67.10737919807434, "training_acc": 72.0, "val_loss": 16.676929593086243, "val_acc": 28.0}
{"epoch": 9, "training_loss": 66.38525223731995, "training_acc": 72.0, "val_loss": 16.506953537464142, "val_acc": 28.0}
{"epoch": 10, "training_loss": 65.66714811325073, "training_acc": 72.0, "val_loss": 16.347792744636536, "val_acc": 28.0}
{"epoch": 11, "training_loss": 65.1135482788086, "training_acc": 72.0, "val_loss": 16.198931634426117, "val_acc": 28.0}
{"epoch": 12, "training_loss": 64.57431721687317, "training_acc": 72.0, "val_loss": 16.0594180226326, "val_acc": 28.0}
{"epoch": 13, "training_loss": 64.01518940925598, "training_acc": 72.0, "val_loss": 15.926462411880493, "val_acc": 28.0}
{"epoch": 14, "training_loss": 63.626439332962036, "training_acc": 72.0, "val_loss": 15.80226868391037, "val_acc": 28.0}
{"epoch": 15, "training_loss": 62.973408937454224, "training_acc": 72.0, "val_loss": 15.685375034809113, "val_acc": 28.0}
{"epoch": 16, "training_loss": 62.5474796295166, "training_acc": 72.0, "val_loss": 15.57835340499878, "val_acc": 28.0}
{"epoch": 17, "training_loss": 62.070852518081665, "training_acc": 72.0, "val_loss": 15.472592413425446, "val_acc": 28.0}
{"epoch": 18, "training_loss": 61.67264485359192, "training_acc": 72.0, "val_loss": 15.373590588569641, "val_acc": 36.0}
{"epoch": 19, "training_loss": 61.3735134601593, "training_acc": 72.0, "val_loss": 15.283530950546265, "val_acc": 64.0}
{"epoch": 20, "training_loss": 60.98249673843384, "training_acc": 72.0, "val_loss": 15.197274088859558, "val_acc": 64.0}
{"epoch": 21, "training_loss": 60.66801142692566, "training_acc": 72.0, "val_loss": 15.11877179145813, "val_acc": 68.0}
{"epoch": 22, "training_loss": 60.38241624832153, "training_acc": 72.0, "val_loss": 15.049614012241364, "val_acc": 76.0}
{"epoch": 23, "training_loss": 59.76043891906738, "training_acc": 72.0, "val_loss": 14.987412095069885, "val_acc": 76.0}
{"epoch": 24, "training_loss": 59.478440284729004, "training_acc": 72.0, "val_loss": 14.924663305282593, "val_acc": 72.0}
{"epoch": 25, "training_loss": 59.34623718261719, "training_acc": 72.0, "val_loss": 14.86576497554779, "val_acc": 72.0}
{"epoch": 26, "training_loss": 58.98538541793823, "training_acc": 72.0, "val_loss": 14.811436831951141, "val_acc": 72.0}
{"epoch": 27, "training_loss": 58.794458627700806, "training_acc": 72.0, "val_loss": 14.762800931930542, "val_acc": 72.0}
{"epoch": 28, "training_loss": 58.26815128326416, "training_acc": 72.0, "val_loss": 14.720726013183594, "val_acc": 72.0}
{"epoch": 29, "training_loss": 58.31941294670105, "training_acc": 72.0, "val_loss": 14.679715037345886, "val_acc": 72.0}
{"epoch": 30, "training_loss": 57.73920178413391, "training_acc": 72.0, "val_loss": 14.642533659934998, "val_acc": 72.0}
{"epoch": 31, "training_loss": 57.698421239852905, "training_acc": 72.0, "val_loss": 14.596733450889587, "val_acc": 72.0}
{"epoch": 32, "training_loss": 57.30056405067444, "training_acc": 72.0, "val_loss": 14.545786380767822, "val_acc": 72.0}
{"epoch": 33, "training_loss": 57.46604108810425, "training_acc": 72.0, "val_loss": 14.510002732276917, "val_acc": 72.0}
{"epoch": 34, "training_loss": 56.709670305252075, "training_acc": 72.0, "val_loss": 14.484554529190063, "val_acc": 72.0}
{"epoch": 35, "training_loss": 56.721169233322144, "training_acc": 72.0, "val_loss": 14.476975798606873, "val_acc": 72.0}
{"epoch": 36, "training_loss": 56.169235944747925, "training_acc": 72.0, "val_loss": 14.483165740966797, "val_acc": 72.0}
{"epoch": 37, "training_loss": 56.61655354499817, "training_acc": 72.0, "val_loss": 14.482559263706207, "val_acc": 72.0}
{"epoch": 38, "training_loss": 55.87133502960205, "training_acc": 72.0, "val_loss": 14.470627903938293, "val_acc": 72.0}
{"epoch": 39, "training_loss": 55.92691612243652, "training_acc": 72.0, "val_loss": 14.433562755584717, "val_acc": 72.0}
{"epoch": 40, "training_loss": 55.18563795089722, "training_acc": 72.0, "val_loss": 14.358164370059967, "val_acc": 72.0}
{"epoch": 41, "training_loss": 55.28557825088501, "training_acc": 72.0, "val_loss": 14.337849617004395, "val_acc": 72.0}
{"epoch": 42, "training_loss": 54.84665095806122, "training_acc": 72.0, "val_loss": 14.40078318119049, "val_acc": 72.0}
{"epoch": 43, "training_loss": 54.60056281089783, "training_acc": 72.0, "val_loss": 14.507973194122314, "val_acc": 76.0}
{"epoch": 44, "training_loss": 53.8744843006134, "training_acc": 72.0, "val_loss": 14.529937505722046, "val_acc": 76.0}
{"epoch": 45, "training_loss": 53.7708683013916, "training_acc": 72.0, "val_loss": 14.458198845386505, "val_acc": 68.0}
{"epoch": 46, "training_loss": 53.743709087371826, "training_acc": 72.0, "val_loss": 14.358827471733093, "val_acc": 68.0}
{"epoch": 47, "training_loss": 53.336785554885864, "training_acc": 72.0, "val_loss": 14.319765567779541, "val_acc": 68.0}
{"epoch": 48, "training_loss": 52.77256393432617, "training_acc": 72.0, "val_loss": 14.287582039833069, "val_acc": 64.0}
{"epoch": 49, "training_loss": 52.94060480594635, "training_acc": 72.0, "val_loss": 14.333844184875488, "val_acc": 64.0}
{"epoch": 50, "training_loss": 52.33930015563965, "training_acc": 72.0, "val_loss": 14.539779722690582, "val_acc": 64.0}
{"epoch": 51, "training_loss": 51.74465215206146, "training_acc": 72.0, "val_loss": 14.551739394664764, "val_acc": 64.0}
{"epoch": 52, "training_loss": 51.990471601486206, "training_acc": 72.0, "val_loss": 14.525368809700012, "val_acc": 64.0}
{"epoch": 53, "training_loss": 51.170984506607056, "training_acc": 72.0, "val_loss": 14.373569190502167, "val_acc": 64.0}
{"epoch": 54, "training_loss": 50.527029275894165, "training_acc": 72.0, "val_loss": 14.437919855117798, "val_acc": 64.0}
{"epoch": 55, "training_loss": 51.38016200065613, "training_acc": 72.0, "val_loss": 14.657869935035706, "val_acc": 64.0}
{"epoch": 56, "training_loss": 51.070979952812195, "training_acc": 72.0, "val_loss": 14.88240510225296, "val_acc": 64.0}
{"epoch": 57, "training_loss": 51.80681598186493, "training_acc": 72.0, "val_loss": 14.977143704891205, "val_acc": 64.0}
{"epoch": 58, "training_loss": 51.048989057540894, "training_acc": 72.0, "val_loss": 14.730504155158997, "val_acc": 64.0}
{"epoch": 59, "training_loss": 49.49669671058655, "training_acc": 72.0, "val_loss": 14.358346164226532, "val_acc": 64.0}
{"epoch": 60, "training_loss": 49.21256124973297, "training_acc": 72.0, "val_loss": 14.379602670669556, "val_acc": 64.0}
{"epoch": 61, "training_loss": 50.63722252845764, "training_acc": 72.0, "val_loss": 14.601528644561768, "val_acc": 64.0}
{"epoch": 62, "training_loss": 48.981210708618164, "training_acc": 72.0, "val_loss": 14.878681302070618, "val_acc": 64.0}
{"epoch": 63, "training_loss": 48.79521143436432, "training_acc": 72.0, "val_loss": 15.021690726280212, "val_acc": 64.0}
{"epoch": 64, "training_loss": 48.64671039581299, "training_acc": 72.0, "val_loss": 14.99636322259903, "val_acc": 64.0}
{"epoch": 65, "training_loss": 48.338852524757385, "training_acc": 72.0, "val_loss": 14.865583181381226, "val_acc": 64.0}
{"epoch": 66, "training_loss": 48.33712124824524, "training_acc": 72.0, "val_loss": 14.82866257429123, "val_acc": 64.0}
{"epoch": 67, "training_loss": 48.642030477523804, "training_acc": 74.0, "val_loss": 14.707086980342865, "val_acc": 64.0}
