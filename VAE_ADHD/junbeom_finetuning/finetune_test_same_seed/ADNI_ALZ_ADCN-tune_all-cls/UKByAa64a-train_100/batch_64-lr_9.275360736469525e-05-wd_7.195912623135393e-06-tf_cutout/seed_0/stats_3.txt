"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 73.30659556388855, "training_acc": 28.0, "val_loss": 17.900630831718445, "val_acc": 28.0}
{"epoch": 1, "training_loss": 71.17541098594666, "training_acc": 28.0, "val_loss": 16.794371604919434, "val_acc": 28.0}
{"epoch": 2, "training_loss": 66.18401885032654, "training_acc": 72.0, "val_loss": 15.485917031764984, "val_acc": 28.0}
{"epoch": 3, "training_loss": 61.51739859580994, "training_acc": 72.0, "val_loss": 14.96138721704483, "val_acc": 72.0}
{"epoch": 4, "training_loss": 59.91059422492981, "training_acc": 72.0, "val_loss": 14.78426307439804, "val_acc": 72.0}
{"epoch": 5, "training_loss": 59.02520680427551, "training_acc": 72.0, "val_loss": 14.702637493610382, "val_acc": 72.0}
{"epoch": 6, "training_loss": 58.59161567687988, "training_acc": 72.0, "val_loss": 14.629563689231873, "val_acc": 72.0}
{"epoch": 7, "training_loss": 58.19583249092102, "training_acc": 72.0, "val_loss": 14.537440240383148, "val_acc": 72.0}
{"epoch": 8, "training_loss": 58.02168011665344, "training_acc": 72.0, "val_loss": 14.36813771724701, "val_acc": 72.0}
{"epoch": 9, "training_loss": 57.35274624824524, "training_acc": 72.0, "val_loss": 14.314214885234833, "val_acc": 72.0}
{"epoch": 10, "training_loss": 57.632710695266724, "training_acc": 72.0, "val_loss": 14.023001492023468, "val_acc": 72.0}
{"epoch": 11, "training_loss": 57.32710838317871, "training_acc": 72.0, "val_loss": 13.915014266967773, "val_acc": 72.0}
{"epoch": 12, "training_loss": 57.50747895240784, "training_acc": 72.0, "val_loss": 14.080566167831421, "val_acc": 68.0}
{"epoch": 13, "training_loss": 56.13329267501831, "training_acc": 72.0, "val_loss": 14.200103282928467, "val_acc": 72.0}
{"epoch": 14, "training_loss": 56.33692264556885, "training_acc": 72.0, "val_loss": 13.771423697471619, "val_acc": 68.0}
{"epoch": 15, "training_loss": 55.258299350738525, "training_acc": 72.0, "val_loss": 13.565821945667267, "val_acc": 72.0}
{"epoch": 16, "training_loss": 53.30373024940491, "training_acc": 72.0, "val_loss": 13.900800049304962, "val_acc": 72.0}
{"epoch": 17, "training_loss": 57.17814111709595, "training_acc": 72.0, "val_loss": 13.600295782089233, "val_acc": 68.0}
{"epoch": 18, "training_loss": 54.40287446975708, "training_acc": 72.0, "val_loss": 13.252100348472595, "val_acc": 72.0}
{"epoch": 19, "training_loss": 53.296695947647095, "training_acc": 72.0, "val_loss": 13.160216808319092, "val_acc": 72.0}
{"epoch": 20, "training_loss": 51.56212258338928, "training_acc": 72.0, "val_loss": 13.378404080867767, "val_acc": 68.0}
{"epoch": 21, "training_loss": 50.24956738948822, "training_acc": 72.0, "val_loss": 12.994523346424103, "val_acc": 72.0}
{"epoch": 22, "training_loss": 51.28142213821411, "training_acc": 72.0, "val_loss": 14.078554511070251, "val_acc": 72.0}
{"epoch": 23, "training_loss": 56.39589834213257, "training_acc": 72.0, "val_loss": 12.88982778787613, "val_acc": 76.0}
{"epoch": 24, "training_loss": 49.4579873085022, "training_acc": 77.0, "val_loss": 13.00058513879776, "val_acc": 72.0}
{"epoch": 25, "training_loss": 49.6093213558197, "training_acc": 74.0, "val_loss": 13.37730884552002, "val_acc": 72.0}
{"epoch": 26, "training_loss": 49.991824984550476, "training_acc": 74.0, "val_loss": 12.945911288261414, "val_acc": 72.0}
{"epoch": 27, "training_loss": 51.15135049819946, "training_acc": 78.0, "val_loss": 13.153076171875, "val_acc": 68.0}
{"epoch": 28, "training_loss": 49.83524799346924, "training_acc": 73.0, "val_loss": 12.950420379638672, "val_acc": 72.0}
{"epoch": 29, "training_loss": 46.77487254142761, "training_acc": 78.0, "val_loss": 13.055486977100372, "val_acc": 72.0}
{"epoch": 30, "training_loss": 47.795814871788025, "training_acc": 83.0, "val_loss": 13.092312216758728, "val_acc": 72.0}
{"epoch": 31, "training_loss": 48.72075629234314, "training_acc": 80.0, "val_loss": 12.56319135427475, "val_acc": 72.0}
{"epoch": 32, "training_loss": 46.18102705478668, "training_acc": 85.0, "val_loss": 12.71771639585495, "val_acc": 72.0}
{"epoch": 33, "training_loss": 44.10721457004547, "training_acc": 83.0, "val_loss": 12.55982518196106, "val_acc": 72.0}
{"epoch": 34, "training_loss": 44.84533619880676, "training_acc": 85.0, "val_loss": 12.02591136097908, "val_acc": 72.0}
{"epoch": 35, "training_loss": 43.61827349662781, "training_acc": 88.0, "val_loss": 13.768719136714935, "val_acc": 76.0}
{"epoch": 36, "training_loss": 53.34088444709778, "training_acc": 73.0, "val_loss": 13.021153211593628, "val_acc": 76.0}
{"epoch": 37, "training_loss": 44.8117938041687, "training_acc": 81.0, "val_loss": 13.482317328453064, "val_acc": 68.0}
{"epoch": 38, "training_loss": 43.80793583393097, "training_acc": 90.0, "val_loss": 12.83818781375885, "val_acc": 72.0}
{"epoch": 39, "training_loss": 46.743133783340454, "training_acc": 73.0, "val_loss": 12.35639899969101, "val_acc": 80.0}
{"epoch": 40, "training_loss": 40.009430289268494, "training_acc": 90.0, "val_loss": 12.66014575958252, "val_acc": 76.0}
{"epoch": 41, "training_loss": 42.98533260822296, "training_acc": 84.0, "val_loss": 12.141593545675278, "val_acc": 72.0}
{"epoch": 42, "training_loss": 40.13771677017212, "training_acc": 91.0, "val_loss": 12.295929342508316, "val_acc": 76.0}
{"epoch": 43, "training_loss": 38.23588585853577, "training_acc": 90.0, "val_loss": 11.23182401061058, "val_acc": 80.0}
{"epoch": 44, "training_loss": 35.19717001914978, "training_acc": 90.0, "val_loss": 12.686221301555634, "val_acc": 68.0}
{"epoch": 45, "training_loss": 34.396114349365234, "training_acc": 96.0, "val_loss": 12.341586500406265, "val_acc": 80.0}
{"epoch": 46, "training_loss": 35.67488217353821, "training_acc": 87.0, "val_loss": 11.666745692491531, "val_acc": 80.0}
{"epoch": 47, "training_loss": 33.62710893154144, "training_acc": 93.0, "val_loss": 11.962077021598816, "val_acc": 80.0}
{"epoch": 48, "training_loss": 31.686073064804077, "training_acc": 94.0, "val_loss": 10.72067841887474, "val_acc": 80.0}
{"epoch": 49, "training_loss": 31.57034182548523, "training_acc": 94.0, "val_loss": 10.794573277235031, "val_acc": 80.0}
{"epoch": 50, "training_loss": 26.899391889572144, "training_acc": 97.0, "val_loss": 10.490426421165466, "val_acc": 80.0}
{"epoch": 51, "training_loss": 27.05471646785736, "training_acc": 97.0, "val_loss": 11.75684779882431, "val_acc": 76.0}
{"epoch": 52, "training_loss": 26.757822155952454, "training_acc": 97.0, "val_loss": 11.089638620615005, "val_acc": 80.0}
{"epoch": 53, "training_loss": 24.51578539609909, "training_acc": 97.0, "val_loss": 10.292837023735046, "val_acc": 80.0}
{"epoch": 54, "training_loss": 25.985775351524353, "training_acc": 97.0, "val_loss": 9.76940244436264, "val_acc": 84.0}
{"epoch": 55, "training_loss": 23.940104722976685, "training_acc": 99.0, "val_loss": 12.14178055524826, "val_acc": 80.0}
{"epoch": 56, "training_loss": 25.01119726896286, "training_acc": 95.0, "val_loss": 10.76645627617836, "val_acc": 76.0}
{"epoch": 57, "training_loss": 24.27884316444397, "training_acc": 99.0, "val_loss": 12.172506004571915, "val_acc": 76.0}
{"epoch": 58, "training_loss": 22.178636491298676, "training_acc": 98.0, "val_loss": 11.387989670038223, "val_acc": 80.0}
{"epoch": 59, "training_loss": 21.24563956260681, "training_acc": 99.0, "val_loss": 11.215049773454666, "val_acc": 80.0}
{"epoch": 60, "training_loss": 20.540390610694885, "training_acc": 99.0, "val_loss": 11.59316971898079, "val_acc": 76.0}
{"epoch": 61, "training_loss": 20.06917506456375, "training_acc": 99.0, "val_loss": 10.86985096335411, "val_acc": 76.0}
{"epoch": 62, "training_loss": 18.027290523052216, "training_acc": 100.0, "val_loss": 11.630041897296906, "val_acc": 80.0}
{"epoch": 63, "training_loss": 19.9545858502388, "training_acc": 98.0, "val_loss": 9.628266841173172, "val_acc": 80.0}
{"epoch": 64, "training_loss": 17.46598130464554, "training_acc": 100.0, "val_loss": 11.167997121810913, "val_acc": 76.0}
{"epoch": 65, "training_loss": 17.93885612487793, "training_acc": 99.0, "val_loss": 9.434955567121506, "val_acc": 84.0}
{"epoch": 66, "training_loss": 17.65586644411087, "training_acc": 99.0, "val_loss": 11.847633123397827, "val_acc": 80.0}
{"epoch": 67, "training_loss": 16.565877079963684, "training_acc": 100.0, "val_loss": 10.496872663497925, "val_acc": 80.0}
{"epoch": 68, "training_loss": 15.658508539199829, "training_acc": 100.0, "val_loss": 10.320059210062027, "val_acc": 84.0}
{"epoch": 69, "training_loss": 15.582073152065277, "training_acc": 100.0, "val_loss": 12.151498347520828, "val_acc": 76.0}
{"epoch": 70, "training_loss": 18.048277735710144, "training_acc": 98.0, "val_loss": 10.194209963083267, "val_acc": 80.0}
{"epoch": 71, "training_loss": 15.572695851325989, "training_acc": 100.0, "val_loss": 11.712846159934998, "val_acc": 76.0}
{"epoch": 72, "training_loss": 16.277854919433594, "training_acc": 99.0, "val_loss": 9.900759160518646, "val_acc": 80.0}
{"epoch": 73, "training_loss": 18.891500115394592, "training_acc": 98.0, "val_loss": 16.417011618614197, "val_acc": 72.0}
{"epoch": 74, "training_loss": 31.396282196044922, "training_acc": 89.0, "val_loss": 18.233267962932587, "val_acc": 48.0}
{"epoch": 75, "training_loss": 41.62700802087784, "training_acc": 80.0, "val_loss": 14.193244278430939, "val_acc": 72.0}
{"epoch": 76, "training_loss": 31.220597565174103, "training_acc": 89.0, "val_loss": 14.920249581336975, "val_acc": 56.0}
{"epoch": 77, "training_loss": 31.925501465797424, "training_acc": 89.0, "val_loss": 12.600061297416687, "val_acc": 80.0}
{"epoch": 78, "training_loss": 32.6986266374588, "training_acc": 88.0, "val_loss": 11.689050495624542, "val_acc": 76.0}
{"epoch": 79, "training_loss": 18.30873328447342, "training_acc": 99.0, "val_loss": 11.775588989257812, "val_acc": 80.0}
{"epoch": 80, "training_loss": 20.07901883125305, "training_acc": 98.0, "val_loss": 11.999750137329102, "val_acc": 76.0}
{"epoch": 81, "training_loss": 17.82093220949173, "training_acc": 99.0, "val_loss": 12.285060435533524, "val_acc": 76.0}
{"epoch": 82, "training_loss": 17.51020312309265, "training_acc": 99.0, "val_loss": 16.36846959590912, "val_acc": 48.0}
{"epoch": 83, "training_loss": 24.89778220653534, "training_acc": 93.0, "val_loss": 13.517461717128754, "val_acc": 80.0}
{"epoch": 84, "training_loss": 19.568538665771484, "training_acc": 97.0, "val_loss": 11.246928572654724, "val_acc": 80.0}
