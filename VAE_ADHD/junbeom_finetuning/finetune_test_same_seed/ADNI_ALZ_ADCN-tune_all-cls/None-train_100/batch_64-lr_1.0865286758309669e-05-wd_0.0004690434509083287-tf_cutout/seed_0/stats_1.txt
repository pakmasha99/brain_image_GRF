"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 69.03223657608032, "training_acc": 51.0, "val_loss": 16.179843246936798, "val_acc": 28.0}
{"epoch": 1, "training_loss": 64.06867837905884, "training_acc": 72.0, "val_loss": 15.256533026695251, "val_acc": 72.0}
{"epoch": 2, "training_loss": 60.4900643825531, "training_acc": 72.0, "val_loss": 14.9033784866333, "val_acc": 72.0}
{"epoch": 3, "training_loss": 59.56552577018738, "training_acc": 72.0, "val_loss": 15.120072662830353, "val_acc": 72.0}
{"epoch": 4, "training_loss": 60.865649700164795, "training_acc": 72.0, "val_loss": 15.315532684326172, "val_acc": 72.0}
{"epoch": 5, "training_loss": 61.079020977020264, "training_acc": 72.0, "val_loss": 15.170550346374512, "val_acc": 72.0}
{"epoch": 6, "training_loss": 60.611308574676514, "training_acc": 72.0, "val_loss": 14.96412307024002, "val_acc": 72.0}
{"epoch": 7, "training_loss": 59.730199098587036, "training_acc": 72.0, "val_loss": 14.871188998222351, "val_acc": 72.0}
{"epoch": 8, "training_loss": 59.692920446395874, "training_acc": 72.0, "val_loss": 14.871977269649506, "val_acc": 72.0}
{"epoch": 9, "training_loss": 59.64353537559509, "training_acc": 72.0, "val_loss": 14.898483455181122, "val_acc": 72.0}
{"epoch": 10, "training_loss": 59.59839344024658, "training_acc": 72.0, "val_loss": 14.888040721416473, "val_acc": 72.0}
{"epoch": 11, "training_loss": 59.702117919921875, "training_acc": 72.0, "val_loss": 14.870381355285645, "val_acc": 72.0}
{"epoch": 12, "training_loss": 59.56126379966736, "training_acc": 72.0, "val_loss": 14.824137091636658, "val_acc": 72.0}
{"epoch": 13, "training_loss": 59.15800213813782, "training_acc": 72.0, "val_loss": 14.780913293361664, "val_acc": 72.0}
{"epoch": 14, "training_loss": 59.253785133361816, "training_acc": 72.0, "val_loss": 14.774277806282043, "val_acc": 72.0}
{"epoch": 15, "training_loss": 59.17505383491516, "training_acc": 72.0, "val_loss": 14.77883756160736, "val_acc": 72.0}
{"epoch": 16, "training_loss": 59.08095955848694, "training_acc": 72.0, "val_loss": 14.788956940174103, "val_acc": 72.0}
{"epoch": 17, "training_loss": 59.20780968666077, "training_acc": 72.0, "val_loss": 14.766576886177063, "val_acc": 72.0}
{"epoch": 18, "training_loss": 59.14703822135925, "training_acc": 72.0, "val_loss": 14.734755456447601, "val_acc": 72.0}
{"epoch": 19, "training_loss": 58.95352220535278, "training_acc": 72.0, "val_loss": 14.701780676841736, "val_acc": 72.0}
{"epoch": 20, "training_loss": 58.90834140777588, "training_acc": 72.0, "val_loss": 14.690028131008148, "val_acc": 72.0}
{"epoch": 21, "training_loss": 58.80110144615173, "training_acc": 72.0, "val_loss": 14.690013229846954, "val_acc": 72.0}
{"epoch": 22, "training_loss": 58.605305910110474, "training_acc": 72.0, "val_loss": 14.669065177440643, "val_acc": 72.0}
{"epoch": 23, "training_loss": 58.72851085662842, "training_acc": 72.0, "val_loss": 14.633168280124664, "val_acc": 72.0}
{"epoch": 24, "training_loss": 58.31034708023071, "training_acc": 72.0, "val_loss": 14.620165526866913, "val_acc": 72.0}
{"epoch": 25, "training_loss": 58.2597222328186, "training_acc": 72.0, "val_loss": 14.620411396026611, "val_acc": 72.0}
{"epoch": 26, "training_loss": 58.4420371055603, "training_acc": 72.0, "val_loss": 14.604063332080841, "val_acc": 72.0}
{"epoch": 27, "training_loss": 57.99130630493164, "training_acc": 72.0, "val_loss": 14.561863243579865, "val_acc": 72.0}
{"epoch": 28, "training_loss": 58.1534743309021, "training_acc": 72.0, "val_loss": 14.539207518100739, "val_acc": 72.0}
{"epoch": 29, "training_loss": 57.88571310043335, "training_acc": 72.0, "val_loss": 14.518143236637115, "val_acc": 72.0}
{"epoch": 30, "training_loss": 57.87971878051758, "training_acc": 72.0, "val_loss": 14.49667364358902, "val_acc": 72.0}
{"epoch": 31, "training_loss": 57.7483766078949, "training_acc": 72.0, "val_loss": 14.478451013565063, "val_acc": 72.0}
{"epoch": 32, "training_loss": 58.198179960250854, "training_acc": 72.0, "val_loss": 14.473730325698853, "val_acc": 72.0}
{"epoch": 33, "training_loss": 57.59185743331909, "training_acc": 72.0, "val_loss": 14.430513978004456, "val_acc": 72.0}
{"epoch": 34, "training_loss": 57.611640214920044, "training_acc": 72.0, "val_loss": 14.455175399780273, "val_acc": 72.0}
{"epoch": 35, "training_loss": 57.09088087081909, "training_acc": 72.0, "val_loss": 14.42154347896576, "val_acc": 72.0}
{"epoch": 36, "training_loss": 57.04621601104736, "training_acc": 72.0, "val_loss": 14.38695639371872, "val_acc": 72.0}
{"epoch": 37, "training_loss": 57.387519121170044, "training_acc": 72.0, "val_loss": 14.457805454730988, "val_acc": 72.0}
{"epoch": 38, "training_loss": 57.67964553833008, "training_acc": 72.0, "val_loss": 14.354687929153442, "val_acc": 72.0}
{"epoch": 39, "training_loss": 57.031766176223755, "training_acc": 72.0, "val_loss": 14.3231600522995, "val_acc": 72.0}
{"epoch": 40, "training_loss": 56.240020513534546, "training_acc": 72.0, "val_loss": 14.358314871788025, "val_acc": 72.0}
{"epoch": 41, "training_loss": 56.5744743347168, "training_acc": 72.0, "val_loss": 14.374856650829315, "val_acc": 72.0}
{"epoch": 42, "training_loss": 56.671980142593384, "training_acc": 72.0, "val_loss": 14.288096129894257, "val_acc": 72.0}
{"epoch": 43, "training_loss": 56.473926067352295, "training_acc": 72.0, "val_loss": 14.257296919822693, "val_acc": 72.0}
{"epoch": 44, "training_loss": 56.32842564582825, "training_acc": 72.0, "val_loss": 14.241230487823486, "val_acc": 72.0}
{"epoch": 45, "training_loss": 55.85550653934479, "training_acc": 72.0, "val_loss": 14.22523707151413, "val_acc": 72.0}
{"epoch": 46, "training_loss": 55.05788230895996, "training_acc": 72.0, "val_loss": 14.32388573884964, "val_acc": 72.0}
{"epoch": 47, "training_loss": 57.02531576156616, "training_acc": 72.0, "val_loss": 14.28547203540802, "val_acc": 72.0}
{"epoch": 48, "training_loss": 55.88763642311096, "training_acc": 72.0, "val_loss": 14.256241917610168, "val_acc": 68.0}
{"epoch": 49, "training_loss": 55.804144620895386, "training_acc": 72.0, "val_loss": 14.285679161548615, "val_acc": 60.0}
{"epoch": 50, "training_loss": 55.97569227218628, "training_acc": 72.0, "val_loss": 14.153239130973816, "val_acc": 76.0}
{"epoch": 51, "training_loss": 55.40847826004028, "training_acc": 72.0, "val_loss": 14.358843863010406, "val_acc": 72.0}
{"epoch": 52, "training_loss": 55.939074754714966, "training_acc": 72.0, "val_loss": 14.440321922302246, "val_acc": 72.0}
{"epoch": 53, "training_loss": 55.68391132354736, "training_acc": 72.0, "val_loss": 14.114545285701752, "val_acc": 76.0}
{"epoch": 54, "training_loss": 55.30427861213684, "training_acc": 72.0, "val_loss": 14.334936439990997, "val_acc": 60.0}
{"epoch": 55, "training_loss": 55.762906312942505, "training_acc": 72.0, "val_loss": 14.108404517173767, "val_acc": 68.0}
{"epoch": 56, "training_loss": 54.11017966270447, "training_acc": 72.0, "val_loss": 14.395421743392944, "val_acc": 72.0}
{"epoch": 57, "training_loss": 57.35219192504883, "training_acc": 72.0, "val_loss": 14.750221371650696, "val_acc": 72.0}
{"epoch": 58, "training_loss": 57.50041127204895, "training_acc": 72.0, "val_loss": 14.115847647190094, "val_acc": 80.0}
{"epoch": 59, "training_loss": 54.830838441848755, "training_acc": 72.0, "val_loss": 14.182887971401215, "val_acc": 64.0}
{"epoch": 60, "training_loss": 54.799198389053345, "training_acc": 74.0, "val_loss": 14.156961441040039, "val_acc": 60.0}
{"epoch": 61, "training_loss": 54.198808908462524, "training_acc": 73.0, "val_loss": 14.07158076763153, "val_acc": 76.0}
{"epoch": 62, "training_loss": 54.61574959754944, "training_acc": 72.0, "val_loss": 14.2906054854393, "val_acc": 76.0}
{"epoch": 63, "training_loss": 55.08465242385864, "training_acc": 72.0, "val_loss": 14.105361700057983, "val_acc": 80.0}
{"epoch": 64, "training_loss": 53.66462755203247, "training_acc": 72.0, "val_loss": 14.075523614883423, "val_acc": 60.0}
{"epoch": 65, "training_loss": 54.23653721809387, "training_acc": 74.0, "val_loss": 14.100870490074158, "val_acc": 60.0}
{"epoch": 66, "training_loss": 54.80803656578064, "training_acc": 74.0, "val_loss": 14.028045535087585, "val_acc": 60.0}
{"epoch": 67, "training_loss": 54.40723776817322, "training_acc": 73.0, "val_loss": 14.028625190258026, "val_acc": 68.0}
{"epoch": 68, "training_loss": 53.48241055011749, "training_acc": 73.0, "val_loss": 14.042936265468597, "val_acc": 68.0}
{"epoch": 69, "training_loss": 52.991093158721924, "training_acc": 73.0, "val_loss": 14.056152105331421, "val_acc": 68.0}
{"epoch": 70, "training_loss": 53.91323518753052, "training_acc": 73.0, "val_loss": 14.04067575931549, "val_acc": 68.0}
{"epoch": 71, "training_loss": 54.06536078453064, "training_acc": 74.0, "val_loss": 14.097270369529724, "val_acc": 68.0}
{"epoch": 72, "training_loss": 53.30496788024902, "training_acc": 73.0, "val_loss": 14.043250679969788, "val_acc": 64.0}
{"epoch": 73, "training_loss": 53.045981645584106, "training_acc": 72.0, "val_loss": 14.072810113430023, "val_acc": 60.0}
{"epoch": 74, "training_loss": 52.848233222961426, "training_acc": 74.0, "val_loss": 14.09609168767929, "val_acc": 68.0}
{"epoch": 75, "training_loss": 51.844104528427124, "training_acc": 73.0, "val_loss": 14.263181388378143, "val_acc": 76.0}
{"epoch": 76, "training_loss": 53.28813171386719, "training_acc": 73.0, "val_loss": 14.103594422340393, "val_acc": 64.0}
{"epoch": 77, "training_loss": 50.91803574562073, "training_acc": 74.0, "val_loss": 14.188660681247711, "val_acc": 60.0}
{"epoch": 78, "training_loss": 53.30588459968567, "training_acc": 79.0, "val_loss": 14.113795757293701, "val_acc": 60.0}
{"epoch": 79, "training_loss": 51.640472769737244, "training_acc": 73.0, "val_loss": 14.348332583904266, "val_acc": 68.0}
{"epoch": 80, "training_loss": 52.21366047859192, "training_acc": 73.0, "val_loss": 14.17921930551529, "val_acc": 60.0}
{"epoch": 81, "training_loss": 52.85213541984558, "training_acc": 75.0, "val_loss": 14.431963860988617, "val_acc": 56.0}
{"epoch": 82, "training_loss": 53.08467197418213, "training_acc": 77.0, "val_loss": 14.165624976158142, "val_acc": 60.0}
{"epoch": 83, "training_loss": 51.30236101150513, "training_acc": 76.0, "val_loss": 14.630596339702606, "val_acc": 76.0}
{"epoch": 84, "training_loss": 52.55799198150635, "training_acc": 74.0, "val_loss": 14.273275434970856, "val_acc": 64.0}
{"epoch": 85, "training_loss": 51.842018127441406, "training_acc": 75.0, "val_loss": 14.371612668037415, "val_acc": 60.0}
