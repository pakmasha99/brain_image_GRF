"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 48593.943172454834, "training_acc": 42.0, "val_loss": 49001.02233886719, "val_acc": 28.0}
{"epoch": 1, "training_loss": 172148.619140625, "training_acc": 40.0, "val_loss": 14081.842041015625, "val_acc": 28.0}
{"epoch": 2, "training_loss": 49449.84436035156, "training_acc": 50.0, "val_loss": 1642.249870300293, "val_acc": 72.0}
{"epoch": 3, "training_loss": 8693.016998291016, "training_acc": 72.0, "val_loss": 3696.6976165771484, "val_acc": 28.0}
{"epoch": 4, "training_loss": 21457.48583984375, "training_acc": 42.0, "val_loss": 392.0092582702637, "val_acc": 28.0}
{"epoch": 5, "training_loss": 8123.8236083984375, "training_acc": 52.0, "val_loss": 2288.791847229004, "val_acc": 72.0}
{"epoch": 6, "training_loss": 8817.181518554688, "training_acc": 72.0, "val_loss": 528.5563945770264, "val_acc": 28.0}
{"epoch": 7, "training_loss": 6561.774475097656, "training_acc": 50.0, "val_loss": 1126.0519981384277, "val_acc": 72.0}
{"epoch": 8, "training_loss": 3660.135971069336, "training_acc": 72.0, "val_loss": 3723.3409881591797, "val_acc": 28.0}
{"epoch": 9, "training_loss": 13182.274932861328, "training_acc": 50.0, "val_loss": 337.06843852996826, "val_acc": 28.0}
{"epoch": 10, "training_loss": 10308.774169921875, "training_acc": 44.0, "val_loss": 1041.2620544433594, "val_acc": 72.0}
{"epoch": 11, "training_loss": 4259.782913208008, "training_acc": 72.0, "val_loss": 2286.306381225586, "val_acc": 28.0}
{"epoch": 12, "training_loss": 11799.6494140625, "training_acc": 46.0, "val_loss": 77.10741758346558, "val_acc": 72.0}
{"epoch": 13, "training_loss": 1821.9219360351562, "training_acc": 48.0, "val_loss": 5061.648941040039, "val_acc": 72.0}
{"epoch": 14, "training_loss": 12732.060729980469, "training_acc": 72.0, "val_loss": 117.30880737304688, "val_acc": 72.0}
{"epoch": 15, "training_loss": 4671.425323486328, "training_acc": 60.0, "val_loss": 3748.7537384033203, "val_acc": 72.0}
{"epoch": 16, "training_loss": 10010.31982421875, "training_acc": 60.0, "val_loss": 5365.066909790039, "val_acc": 72.0}
{"epoch": 17, "training_loss": 16402.280017852783, "training_acc": 72.0, "val_loss": 1032.1073532104492, "val_acc": 72.0}
{"epoch": 18, "training_loss": 5958.956237792969, "training_acc": 62.0, "val_loss": 3288.3140563964844, "val_acc": 72.0}
{"epoch": 19, "training_loss": 10524.005332946777, "training_acc": 50.0, "val_loss": 7326.872253417969, "val_acc": 72.0}
{"epoch": 20, "training_loss": 21640.03793334961, "training_acc": 72.0, "val_loss": 1960.725212097168, "val_acc": 72.0}
{"epoch": 21, "training_loss": 5005.304512023926, "training_acc": 62.0, "val_loss": 4911.827087402344, "val_acc": 72.0}
{"epoch": 22, "training_loss": 12735.709558486938, "training_acc": 72.0, "val_loss": 37.03723847866058, "val_acc": 28.0}
{"epoch": 23, "training_loss": 2565.7955017089844, "training_acc": 46.0, "val_loss": 15.27506709098816, "val_acc": 72.0}
{"epoch": 24, "training_loss": 1541.4668807983398, "training_acc": 72.0, "val_loss": 1096.2407112121582, "val_acc": 28.0}
{"epoch": 25, "training_loss": 6991.910186767578, "training_acc": 42.0, "val_loss": 26.7919659614563, "val_acc": 72.0}
{"epoch": 26, "training_loss": 250.62180519104004, "training_acc": 72.0, "val_loss": 2023.1239318847656, "val_acc": 28.0}
{"epoch": 27, "training_loss": 8297.997467041016, "training_acc": 44.0, "val_loss": 776.6273021697998, "val_acc": 28.0}
{"epoch": 28, "training_loss": 5828.775421142578, "training_acc": 46.0, "val_loss": 483.6996555328369, "val_acc": 72.0}
{"epoch": 29, "training_loss": 1937.796974182129, "training_acc": 72.0, "val_loss": 1204.6562194824219, "val_acc": 28.0}
{"epoch": 30, "training_loss": 5014.540802001953, "training_acc": 50.0, "val_loss": 135.27812957763672, "val_acc": 72.0}
{"epoch": 31, "training_loss": 623.354564666748, "training_acc": 72.0, "val_loss": 2811.179542541504, "val_acc": 28.0}
{"epoch": 32, "training_loss": 8998.746536254883, "training_acc": 48.0, "val_loss": 1074.9932289123535, "val_acc": 28.0}
{"epoch": 33, "training_loss": 5869.7431640625, "training_acc": 48.0, "val_loss": 402.36167907714844, "val_acc": 72.0}
{"epoch": 34, "training_loss": 1705.2126235961914, "training_acc": 72.0, "val_loss": 1745.3729629516602, "val_acc": 28.0}
{"epoch": 35, "training_loss": 7646.424865722656, "training_acc": 44.0, "val_loss": 635.6934547424316, "val_acc": 28.0}
{"epoch": 36, "training_loss": 5510.7957763671875, "training_acc": 46.0, "val_loss": 533.8017463684082, "val_acc": 72.0}
{"epoch": 37, "training_loss": 2522.037353515625, "training_acc": 72.0, "val_loss": 1809.872055053711, "val_acc": 28.0}
{"epoch": 38, "training_loss": 8448.612731933594, "training_acc": 42.0, "val_loss": 528.0817031860352, "val_acc": 28.0}
{"epoch": 39, "training_loss": 8219.374938964844, "training_acc": 36.0, "val_loss": 267.7614688873291, "val_acc": 72.0}
{"epoch": 40, "training_loss": 1746.9780807495117, "training_acc": 72.0, "val_loss": 1736.9508743286133, "val_acc": 28.0}
{"epoch": 41, "training_loss": 8182.278259277344, "training_acc": 42.0, "val_loss": 541.081953048706, "val_acc": 28.0}
{"epoch": 42, "training_loss": 5222.169525146484, "training_acc": 46.0, "val_loss": 581.2944412231445, "val_acc": 72.0}
