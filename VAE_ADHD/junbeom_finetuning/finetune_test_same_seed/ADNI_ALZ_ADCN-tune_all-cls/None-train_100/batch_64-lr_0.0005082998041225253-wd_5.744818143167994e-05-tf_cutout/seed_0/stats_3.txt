"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 75.94004583358765, "training_acc": 57.0, "val_loss": 16.023024916648865, "val_acc": 72.0}
{"epoch": 1, "training_loss": 71.48091840744019, "training_acc": 62.0, "val_loss": 15.76453149318695, "val_acc": 28.0}
{"epoch": 2, "training_loss": 66.30082416534424, "training_acc": 72.0, "val_loss": 15.119382739067078, "val_acc": 72.0}
{"epoch": 3, "training_loss": 60.46775555610657, "training_acc": 72.0, "val_loss": 15.15101045370102, "val_acc": 72.0}
{"epoch": 4, "training_loss": 61.03662037849426, "training_acc": 72.0, "val_loss": 14.957614243030548, "val_acc": 72.0}
{"epoch": 5, "training_loss": 60.129958629608154, "training_acc": 72.0, "val_loss": 14.926500618457794, "val_acc": 72.0}
{"epoch": 6, "training_loss": 60.3355438709259, "training_acc": 72.0, "val_loss": 14.907975494861603, "val_acc": 72.0}
{"epoch": 7, "training_loss": 61.321678161621094, "training_acc": 72.0, "val_loss": 15.012001991271973, "val_acc": 72.0}
{"epoch": 8, "training_loss": 60.051090717315674, "training_acc": 72.0, "val_loss": 14.867989718914032, "val_acc": 72.0}
{"epoch": 9, "training_loss": 59.50618028640747, "training_acc": 72.0, "val_loss": 14.883258938789368, "val_acc": 72.0}
{"epoch": 10, "training_loss": 59.39456915855408, "training_acc": 72.0, "val_loss": 14.836998283863068, "val_acc": 72.0}
{"epoch": 11, "training_loss": 59.34387564659119, "training_acc": 72.0, "val_loss": 14.83762115240097, "val_acc": 72.0}
{"epoch": 12, "training_loss": 59.179850578308105, "training_acc": 72.0, "val_loss": 14.890018105506897, "val_acc": 72.0}
{"epoch": 13, "training_loss": 60.435598850250244, "training_acc": 72.0, "val_loss": 14.855492115020752, "val_acc": 72.0}
{"epoch": 14, "training_loss": 58.99157762527466, "training_acc": 72.0, "val_loss": 14.901602268218994, "val_acc": 72.0}
{"epoch": 15, "training_loss": 59.44610953330994, "training_acc": 72.0, "val_loss": 15.04601538181305, "val_acc": 72.0}
{"epoch": 16, "training_loss": 59.825616121292114, "training_acc": 72.0, "val_loss": 14.806069433689117, "val_acc": 72.0}
{"epoch": 17, "training_loss": 61.10378611087799, "training_acc": 72.0, "val_loss": 14.886939525604248, "val_acc": 72.0}
{"epoch": 18, "training_loss": 59.452532052993774, "training_acc": 72.0, "val_loss": 14.809943735599518, "val_acc": 72.0}
{"epoch": 19, "training_loss": 59.445669174194336, "training_acc": 72.0, "val_loss": 14.82614129781723, "val_acc": 72.0}
{"epoch": 20, "training_loss": 59.20950508117676, "training_acc": 72.0, "val_loss": 14.809666574001312, "val_acc": 72.0}
{"epoch": 21, "training_loss": 59.18793869018555, "training_acc": 72.0, "val_loss": 14.769072830677032, "val_acc": 72.0}
{"epoch": 22, "training_loss": 59.03542757034302, "training_acc": 72.0, "val_loss": 14.746059477329254, "val_acc": 72.0}
{"epoch": 23, "training_loss": 59.020129442214966, "training_acc": 72.0, "val_loss": 14.733721315860748, "val_acc": 72.0}
{"epoch": 24, "training_loss": 58.84383821487427, "training_acc": 72.0, "val_loss": 14.699026942253113, "val_acc": 72.0}
{"epoch": 25, "training_loss": 58.71060514450073, "training_acc": 72.0, "val_loss": 14.669360220432281, "val_acc": 72.0}
{"epoch": 26, "training_loss": 58.57394051551819, "training_acc": 72.0, "val_loss": 14.617060124874115, "val_acc": 72.0}
{"epoch": 27, "training_loss": 59.740278363227844, "training_acc": 72.0, "val_loss": 14.75687026977539, "val_acc": 72.0}
{"epoch": 28, "training_loss": 58.7563841342926, "training_acc": 72.0, "val_loss": 14.82824981212616, "val_acc": 72.0}
{"epoch": 29, "training_loss": 59.228923320770264, "training_acc": 72.0, "val_loss": 14.637947082519531, "val_acc": 72.0}
{"epoch": 30, "training_loss": 58.109763383865356, "training_acc": 72.0, "val_loss": 14.512474834918976, "val_acc": 72.0}
{"epoch": 31, "training_loss": 58.29021954536438, "training_acc": 72.0, "val_loss": 14.437547326087952, "val_acc": 72.0}
{"epoch": 32, "training_loss": 58.154638051986694, "training_acc": 72.0, "val_loss": 14.29845243692398, "val_acc": 72.0}
{"epoch": 33, "training_loss": 57.60022449493408, "training_acc": 72.0, "val_loss": 14.175048470497131, "val_acc": 76.0}
{"epoch": 34, "training_loss": 57.32310175895691, "training_acc": 72.0, "val_loss": 14.064989984035492, "val_acc": 64.0}
{"epoch": 35, "training_loss": 58.619895458221436, "training_acc": 72.0, "val_loss": 15.267962217330933, "val_acc": 40.0}
{"epoch": 36, "training_loss": 61.23244905471802, "training_acc": 72.0, "val_loss": 14.910507202148438, "val_acc": 72.0}
{"epoch": 37, "training_loss": 59.537665605545044, "training_acc": 72.0, "val_loss": 14.858567714691162, "val_acc": 72.0}
{"epoch": 38, "training_loss": 59.35734724998474, "training_acc": 72.0, "val_loss": 14.736618101596832, "val_acc": 72.0}
{"epoch": 39, "training_loss": 58.65369391441345, "training_acc": 72.0, "val_loss": 14.686642587184906, "val_acc": 72.0}
{"epoch": 40, "training_loss": 59.308332681655884, "training_acc": 72.0, "val_loss": 14.674849808216095, "val_acc": 72.0}
{"epoch": 41, "training_loss": 58.59119725227356, "training_acc": 72.0, "val_loss": 14.46445882320404, "val_acc": 72.0}
{"epoch": 42, "training_loss": 59.90264630317688, "training_acc": 72.0, "val_loss": 14.557543396949768, "val_acc": 72.0}
{"epoch": 43, "training_loss": 57.722793102264404, "training_acc": 72.0, "val_loss": 14.483903348445892, "val_acc": 72.0}
{"epoch": 44, "training_loss": 58.09195613861084, "training_acc": 72.0, "val_loss": 14.567935466766357, "val_acc": 72.0}
{"epoch": 45, "training_loss": 58.411072731018066, "training_acc": 72.0, "val_loss": 14.261552691459656, "val_acc": 72.0}
{"epoch": 46, "training_loss": 57.428927183151245, "training_acc": 72.0, "val_loss": 14.06666487455368, "val_acc": 72.0}
{"epoch": 47, "training_loss": 56.8377685546875, "training_acc": 72.0, "val_loss": 13.81748765707016, "val_acc": 80.0}
{"epoch": 48, "training_loss": 56.99983191490173, "training_acc": 72.0, "val_loss": 13.724276423454285, "val_acc": 68.0}
{"epoch": 49, "training_loss": 59.59123945236206, "training_acc": 72.0, "val_loss": 14.817354083061218, "val_acc": 56.0}
{"epoch": 50, "training_loss": 59.93232274055481, "training_acc": 72.0, "val_loss": 15.508033335208893, "val_acc": 28.0}
{"epoch": 51, "training_loss": 61.442033529281616, "training_acc": 72.0, "val_loss": 14.930696785449982, "val_acc": 72.0}
{"epoch": 52, "training_loss": 60.21270418167114, "training_acc": 72.0, "val_loss": 14.974851906299591, "val_acc": 72.0}
{"epoch": 53, "training_loss": 59.80344200134277, "training_acc": 72.0, "val_loss": 14.852400124073029, "val_acc": 72.0}
{"epoch": 54, "training_loss": 59.263625383377075, "training_acc": 72.0, "val_loss": 14.768606424331665, "val_acc": 72.0}
{"epoch": 55, "training_loss": 59.779064893722534, "training_acc": 72.0, "val_loss": 14.716792106628418, "val_acc": 72.0}
{"epoch": 56, "training_loss": 58.66677904129028, "training_acc": 72.0, "val_loss": 14.818774163722992, "val_acc": 72.0}
{"epoch": 57, "training_loss": 59.64345097541809, "training_acc": 72.0, "val_loss": 14.667989313602448, "val_acc": 72.0}
{"epoch": 58, "training_loss": 58.263527512550354, "training_acc": 72.0, "val_loss": 15.128153562545776, "val_acc": 72.0}
{"epoch": 59, "training_loss": 60.15821027755737, "training_acc": 72.0, "val_loss": 14.78864997625351, "val_acc": 72.0}
{"epoch": 60, "training_loss": 59.02851891517639, "training_acc": 72.0, "val_loss": 14.723414182662964, "val_acc": 72.0}
{"epoch": 61, "training_loss": 58.835644006729126, "training_acc": 72.0, "val_loss": 14.693085849285126, "val_acc": 72.0}
{"epoch": 62, "training_loss": 58.62838172912598, "training_acc": 72.0, "val_loss": 14.591175317764282, "val_acc": 72.0}
{"epoch": 63, "training_loss": 58.3041627407074, "training_acc": 72.0, "val_loss": 14.554721117019653, "val_acc": 72.0}
{"epoch": 64, "training_loss": 57.68891763687134, "training_acc": 72.0, "val_loss": 14.473788440227509, "val_acc": 72.0}
{"epoch": 65, "training_loss": 57.481266260147095, "training_acc": 72.0, "val_loss": 14.13886845111847, "val_acc": 68.0}
{"epoch": 66, "training_loss": 56.310107469558716, "training_acc": 72.0, "val_loss": 15.100373327732086, "val_acc": 72.0}
{"epoch": 67, "training_loss": 56.66154432296753, "training_acc": 72.0, "val_loss": 15.297426283359528, "val_acc": 44.0}
