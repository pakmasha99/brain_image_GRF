"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 64.36099767684937, "training_acc": 72.0, "val_loss": 15.396139025688171, "val_acc": 28.0}
{"epoch": 1, "training_loss": 61.48313522338867, "training_acc": 72.0, "val_loss": 14.906412363052368, "val_acc": 72.0}
{"epoch": 2, "training_loss": 59.47084069252014, "training_acc": 72.0, "val_loss": 14.8421049118042, "val_acc": 72.0}
{"epoch": 3, "training_loss": 59.34232187271118, "training_acc": 72.0, "val_loss": 14.82914686203003, "val_acc": 72.0}
{"epoch": 4, "training_loss": 59.814353227615356, "training_acc": 72.0, "val_loss": 14.825697243213654, "val_acc": 72.0}
{"epoch": 5, "training_loss": 59.92193949222565, "training_acc": 72.0, "val_loss": 14.821787178516388, "val_acc": 72.0}
{"epoch": 6, "training_loss": 59.49129509925842, "training_acc": 72.0, "val_loss": 14.852765202522278, "val_acc": 72.0}
{"epoch": 7, "training_loss": 59.88128304481506, "training_acc": 72.0, "val_loss": 14.833325147628784, "val_acc": 72.0}
{"epoch": 8, "training_loss": 59.72536778450012, "training_acc": 72.0, "val_loss": 14.848753809928894, "val_acc": 72.0}
{"epoch": 9, "training_loss": 59.45884442329407, "training_acc": 72.0, "val_loss": 14.817063510417938, "val_acc": 72.0}
{"epoch": 10, "training_loss": 59.2871732711792, "training_acc": 72.0, "val_loss": 14.815530180931091, "val_acc": 72.0}
{"epoch": 11, "training_loss": 59.34496307373047, "training_acc": 72.0, "val_loss": 14.811913669109344, "val_acc": 72.0}
{"epoch": 12, "training_loss": 59.34696674346924, "training_acc": 72.0, "val_loss": 14.829005300998688, "val_acc": 72.0}
{"epoch": 13, "training_loss": 59.495487213134766, "training_acc": 72.0, "val_loss": 14.819730818271637, "val_acc": 72.0}
{"epoch": 14, "training_loss": 59.35021114349365, "training_acc": 72.0, "val_loss": 14.827953279018402, "val_acc": 72.0}
{"epoch": 15, "training_loss": 59.34530997276306, "training_acc": 72.0, "val_loss": 14.81483280658722, "val_acc": 72.0}
{"epoch": 16, "training_loss": 59.31161642074585, "training_acc": 72.0, "val_loss": 14.812727272510529, "val_acc": 72.0}
{"epoch": 17, "training_loss": 59.347983598709106, "training_acc": 72.0, "val_loss": 14.812266826629639, "val_acc": 72.0}
{"epoch": 18, "training_loss": 59.31173658370972, "training_acc": 72.0, "val_loss": 14.81473445892334, "val_acc": 72.0}
{"epoch": 19, "training_loss": 59.29317855834961, "training_acc": 72.0, "val_loss": 14.812470972537994, "val_acc": 72.0}
{"epoch": 20, "training_loss": 59.40771150588989, "training_acc": 72.0, "val_loss": 14.81228917837143, "val_acc": 72.0}
{"epoch": 21, "training_loss": 59.2484130859375, "training_acc": 72.0, "val_loss": 14.828670024871826, "val_acc": 72.0}
{"epoch": 22, "training_loss": 59.31236410140991, "training_acc": 72.0, "val_loss": 14.84735906124115, "val_acc": 72.0}
{"epoch": 23, "training_loss": 59.37937617301941, "training_acc": 72.0, "val_loss": 14.829692244529724, "val_acc": 72.0}
{"epoch": 24, "training_loss": 59.306517601013184, "training_acc": 72.0, "val_loss": 14.81231153011322, "val_acc": 72.0}
{"epoch": 25, "training_loss": 59.271486043930054, "training_acc": 72.0, "val_loss": 14.823035895824432, "val_acc": 72.0}
{"epoch": 26, "training_loss": 59.3381507396698, "training_acc": 72.0, "val_loss": 14.823402464389801, "val_acc": 72.0}
{"epoch": 27, "training_loss": 59.26833939552307, "training_acc": 72.0, "val_loss": 14.809969067573547, "val_acc": 72.0}
{"epoch": 28, "training_loss": 59.22249221801758, "training_acc": 72.0, "val_loss": 14.826267957687378, "val_acc": 72.0}
{"epoch": 29, "training_loss": 59.348487854003906, "training_acc": 72.0, "val_loss": 14.850486814975739, "val_acc": 72.0}
{"epoch": 30, "training_loss": 59.90888440608978, "training_acc": 72.0, "val_loss": 14.82553780078888, "val_acc": 72.0}
{"epoch": 31, "training_loss": 60.611088514328, "training_acc": 72.0, "val_loss": 14.836819469928741, "val_acc": 72.0}
{"epoch": 32, "training_loss": 59.3777801990509, "training_acc": 72.0, "val_loss": 14.823226630687714, "val_acc": 72.0}
{"epoch": 33, "training_loss": 59.37166786193848, "training_acc": 72.0, "val_loss": 14.815858006477356, "val_acc": 72.0}
{"epoch": 34, "training_loss": 59.2896192073822, "training_acc": 72.0, "val_loss": 14.815181493759155, "val_acc": 72.0}
{"epoch": 35, "training_loss": 59.46081876754761, "training_acc": 72.0, "val_loss": 14.816072583198547, "val_acc": 72.0}
{"epoch": 36, "training_loss": 59.23434638977051, "training_acc": 72.0, "val_loss": 14.818865060806274, "val_acc": 72.0}
{"epoch": 37, "training_loss": 59.434704065322876, "training_acc": 72.0, "val_loss": 14.839717745780945, "val_acc": 72.0}
{"epoch": 38, "training_loss": 59.429858922958374, "training_acc": 72.0, "val_loss": 14.819860458374023, "val_acc": 72.0}
{"epoch": 39, "training_loss": 59.21083664894104, "training_acc": 72.0, "val_loss": 14.818143844604492, "val_acc": 72.0}
{"epoch": 40, "training_loss": 59.483115434646606, "training_acc": 72.0, "val_loss": 14.857231080532074, "val_acc": 72.0}
{"epoch": 41, "training_loss": 59.45263934135437, "training_acc": 72.0, "val_loss": 14.840875566005707, "val_acc": 72.0}
{"epoch": 42, "training_loss": 59.363136529922485, "training_acc": 72.0, "val_loss": 14.815087616443634, "val_acc": 72.0}
{"epoch": 43, "training_loss": 59.276100635528564, "training_acc": 72.0, "val_loss": 14.817675948143005, "val_acc": 72.0}
{"epoch": 44, "training_loss": 59.379310607910156, "training_acc": 72.0, "val_loss": 14.824208617210388, "val_acc": 72.0}
{"epoch": 45, "training_loss": 59.30051803588867, "training_acc": 72.0, "val_loss": 14.813071489334106, "val_acc": 72.0}
{"epoch": 46, "training_loss": 59.242276430130005, "training_acc": 72.0, "val_loss": 14.813116192817688, "val_acc": 72.0}
