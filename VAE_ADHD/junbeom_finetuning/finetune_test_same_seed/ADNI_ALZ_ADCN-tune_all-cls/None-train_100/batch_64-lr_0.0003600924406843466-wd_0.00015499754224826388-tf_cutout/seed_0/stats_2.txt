"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 76.80382251739502, "training_acc": 42.0, "val_loss": 17.681704461574554, "val_acc": 72.0}
{"epoch": 1, "training_loss": 81.17636799812317, "training_acc": 48.0, "val_loss": 15.90844839811325, "val_acc": 28.0}
{"epoch": 2, "training_loss": 65.94612121582031, "training_acc": 72.0, "val_loss": 14.991258084774017, "val_acc": 72.0}
{"epoch": 3, "training_loss": 60.0933792591095, "training_acc": 72.0, "val_loss": 14.953513443470001, "val_acc": 72.0}
{"epoch": 4, "training_loss": 59.698516607284546, "training_acc": 72.0, "val_loss": 15.056464076042175, "val_acc": 72.0}
{"epoch": 5, "training_loss": 60.46859622001648, "training_acc": 72.0, "val_loss": 14.872202277183533, "val_acc": 72.0}
{"epoch": 6, "training_loss": 59.68234467506409, "training_acc": 72.0, "val_loss": 14.860747754573822, "val_acc": 72.0}
{"epoch": 7, "training_loss": 59.66054582595825, "training_acc": 72.0, "val_loss": 14.966030418872833, "val_acc": 72.0}
{"epoch": 8, "training_loss": 59.83320641517639, "training_acc": 72.0, "val_loss": 15.01648724079132, "val_acc": 72.0}
{"epoch": 9, "training_loss": 60.246801137924194, "training_acc": 72.0, "val_loss": 14.837323129177094, "val_acc": 72.0}
{"epoch": 10, "training_loss": 59.23750567436218, "training_acc": 72.0, "val_loss": 14.83125388622284, "val_acc": 72.0}
{"epoch": 11, "training_loss": 59.64453864097595, "training_acc": 72.0, "val_loss": 14.829713106155396, "val_acc": 72.0}
{"epoch": 12, "training_loss": 59.525716066360474, "training_acc": 72.0, "val_loss": 15.008078515529633, "val_acc": 72.0}
{"epoch": 13, "training_loss": 59.76262712478638, "training_acc": 72.0, "val_loss": 14.810070395469666, "val_acc": 72.0}
{"epoch": 14, "training_loss": 59.64810657501221, "training_acc": 72.0, "val_loss": 14.910127222537994, "val_acc": 72.0}
{"epoch": 15, "training_loss": 59.45864486694336, "training_acc": 72.0, "val_loss": 14.794550836086273, "val_acc": 72.0}
{"epoch": 16, "training_loss": 59.36706590652466, "training_acc": 72.0, "val_loss": 14.778396487236023, "val_acc": 72.0}
{"epoch": 17, "training_loss": 58.86260724067688, "training_acc": 72.0, "val_loss": 14.821402728557587, "val_acc": 72.0}
{"epoch": 18, "training_loss": 58.77557301521301, "training_acc": 72.0, "val_loss": 14.731715619564056, "val_acc": 72.0}
{"epoch": 19, "training_loss": 58.42607545852661, "training_acc": 72.0, "val_loss": 14.723505079746246, "val_acc": 72.0}
{"epoch": 20, "training_loss": 58.8306086063385, "training_acc": 72.0, "val_loss": 14.681077003479004, "val_acc": 72.0}
{"epoch": 21, "training_loss": 58.261457443237305, "training_acc": 72.0, "val_loss": 14.88734781742096, "val_acc": 72.0}
{"epoch": 22, "training_loss": 58.73443150520325, "training_acc": 72.0, "val_loss": 14.65390920639038, "val_acc": 72.0}
{"epoch": 23, "training_loss": 58.17375922203064, "training_acc": 72.0, "val_loss": 14.500218629837036, "val_acc": 72.0}
{"epoch": 24, "training_loss": 57.39084005355835, "training_acc": 72.0, "val_loss": 14.50929194688797, "val_acc": 72.0}
{"epoch": 25, "training_loss": 59.813071727752686, "training_acc": 72.0, "val_loss": 14.389398694038391, "val_acc": 72.0}
{"epoch": 26, "training_loss": 62.09661889076233, "training_acc": 72.0, "val_loss": 14.550302922725677, "val_acc": 72.0}
{"epoch": 27, "training_loss": 58.06143665313721, "training_acc": 72.0, "val_loss": 15.114785730838776, "val_acc": 72.0}
{"epoch": 28, "training_loss": 60.230438232421875, "training_acc": 72.0, "val_loss": 15.046975016593933, "val_acc": 72.0}
{"epoch": 29, "training_loss": 59.632511377334595, "training_acc": 72.0, "val_loss": 14.888323843479156, "val_acc": 72.0}
{"epoch": 30, "training_loss": 59.732911348342896, "training_acc": 72.0, "val_loss": 14.969049394130707, "val_acc": 72.0}
{"epoch": 31, "training_loss": 58.99679708480835, "training_acc": 72.0, "val_loss": 14.885477721691132, "val_acc": 72.0}
{"epoch": 32, "training_loss": 59.186362981796265, "training_acc": 72.0, "val_loss": 14.889417588710785, "val_acc": 72.0}
{"epoch": 33, "training_loss": 59.07519793510437, "training_acc": 72.0, "val_loss": 14.74417895078659, "val_acc": 72.0}
{"epoch": 34, "training_loss": 58.90385413169861, "training_acc": 72.0, "val_loss": 14.862236380577087, "val_acc": 72.0}
{"epoch": 35, "training_loss": 58.73831534385681, "training_acc": 72.0, "val_loss": 14.692041277885437, "val_acc": 72.0}
{"epoch": 36, "training_loss": 58.10399651527405, "training_acc": 72.0, "val_loss": 14.603100717067719, "val_acc": 72.0}
{"epoch": 37, "training_loss": 58.29260969161987, "training_acc": 72.0, "val_loss": 14.531247317790985, "val_acc": 72.0}
{"epoch": 38, "training_loss": 57.55835008621216, "training_acc": 72.0, "val_loss": 14.58180844783783, "val_acc": 72.0}
{"epoch": 39, "training_loss": 56.480767488479614, "training_acc": 72.0, "val_loss": 14.705023169517517, "val_acc": 72.0}
{"epoch": 40, "training_loss": 55.51316833496094, "training_acc": 72.0, "val_loss": 16.532886028289795, "val_acc": 32.0}
{"epoch": 41, "training_loss": 62.25791931152344, "training_acc": 65.0, "val_loss": 16.753925383090973, "val_acc": 72.0}
{"epoch": 42, "training_loss": 60.730286598205566, "training_acc": 72.0, "val_loss": 14.65829312801361, "val_acc": 72.0}
{"epoch": 43, "training_loss": 58.33532404899597, "training_acc": 72.0, "val_loss": 14.984437823295593, "val_acc": 64.0}
{"epoch": 44, "training_loss": 58.87975025177002, "training_acc": 72.0, "val_loss": 14.670233428478241, "val_acc": 72.0}
