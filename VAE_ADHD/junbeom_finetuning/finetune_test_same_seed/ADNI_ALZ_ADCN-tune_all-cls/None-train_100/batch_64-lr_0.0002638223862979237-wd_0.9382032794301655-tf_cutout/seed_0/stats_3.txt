"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 68.2719795703888, "training_acc": 52.0, "val_loss": 16.36420041322708, "val_acc": 28.0}
{"epoch": 1, "training_loss": 62.8756947517395, "training_acc": 72.0, "val_loss": 14.981748163700104, "val_acc": 72.0}
{"epoch": 2, "training_loss": 59.527464866638184, "training_acc": 72.0, "val_loss": 14.826910197734833, "val_acc": 72.0}
{"epoch": 3, "training_loss": 59.364625692367554, "training_acc": 72.0, "val_loss": 14.820119738578796, "val_acc": 72.0}
{"epoch": 4, "training_loss": 59.31351399421692, "training_acc": 72.0, "val_loss": 14.8109570145607, "val_acc": 72.0}
{"epoch": 5, "training_loss": 59.24035024642944, "training_acc": 72.0, "val_loss": 14.814761281013489, "val_acc": 72.0}
{"epoch": 6, "training_loss": 59.37662434577942, "training_acc": 72.0, "val_loss": 14.81328010559082, "val_acc": 72.0}
{"epoch": 7, "training_loss": 59.26825428009033, "training_acc": 72.0, "val_loss": 14.805847406387329, "val_acc": 72.0}
{"epoch": 8, "training_loss": 59.237560510635376, "training_acc": 72.0, "val_loss": 14.81255292892456, "val_acc": 72.0}
{"epoch": 9, "training_loss": 59.47967839241028, "training_acc": 72.0, "val_loss": 14.808452129364014, "val_acc": 72.0}
{"epoch": 10, "training_loss": 59.64584529399872, "training_acc": 72.0, "val_loss": 14.8076131939888, "val_acc": 72.0}
{"epoch": 11, "training_loss": 59.2025945186615, "training_acc": 72.0, "val_loss": 14.827574789524078, "val_acc": 72.0}
{"epoch": 12, "training_loss": 59.42283225059509, "training_acc": 72.0, "val_loss": 14.838738739490509, "val_acc": 72.0}
{"epoch": 13, "training_loss": 59.321885108947754, "training_acc": 72.0, "val_loss": 14.86334353685379, "val_acc": 72.0}
{"epoch": 14, "training_loss": 59.407631158828735, "training_acc": 72.0, "val_loss": 14.854222536087036, "val_acc": 72.0}
{"epoch": 15, "training_loss": 59.5867760181427, "training_acc": 72.0, "val_loss": 14.808420836925507, "val_acc": 72.0}
{"epoch": 16, "training_loss": 59.19805979728699, "training_acc": 72.0, "val_loss": 14.79683518409729, "val_acc": 72.0}
{"epoch": 17, "training_loss": 59.114190340042114, "training_acc": 72.0, "val_loss": 14.796413481235504, "val_acc": 72.0}
{"epoch": 18, "training_loss": 59.45726799964905, "training_acc": 72.0, "val_loss": 14.803525805473328, "val_acc": 72.0}
{"epoch": 19, "training_loss": 59.105446100234985, "training_acc": 72.0, "val_loss": 14.789332449436188, "val_acc": 72.0}
{"epoch": 20, "training_loss": 59.22216534614563, "training_acc": 72.0, "val_loss": 14.82829600572586, "val_acc": 72.0}
{"epoch": 21, "training_loss": 59.277626514434814, "training_acc": 72.0, "val_loss": 14.803840219974518, "val_acc": 72.0}
{"epoch": 22, "training_loss": 59.13771867752075, "training_acc": 72.0, "val_loss": 14.777632057666779, "val_acc": 72.0}
{"epoch": 23, "training_loss": 59.298221826553345, "training_acc": 72.0, "val_loss": 14.80129063129425, "val_acc": 72.0}
{"epoch": 24, "training_loss": 59.11889290809631, "training_acc": 72.0, "val_loss": 14.771133661270142, "val_acc": 72.0}
{"epoch": 25, "training_loss": 58.97487282752991, "training_acc": 72.0, "val_loss": 14.764882624149323, "val_acc": 72.0}
{"epoch": 26, "training_loss": 58.923283100128174, "training_acc": 72.0, "val_loss": 14.74938690662384, "val_acc": 72.0}
{"epoch": 27, "training_loss": 58.917839765548706, "training_acc": 72.0, "val_loss": 14.719052612781525, "val_acc": 72.0}
{"epoch": 28, "training_loss": 58.66726517677307, "training_acc": 72.0, "val_loss": 14.664649963378906, "val_acc": 72.0}
{"epoch": 29, "training_loss": 58.49737548828125, "training_acc": 72.0, "val_loss": 14.689862728118896, "val_acc": 72.0}
{"epoch": 30, "training_loss": 58.553523778915405, "training_acc": 72.0, "val_loss": 14.549584686756134, "val_acc": 72.0}
{"epoch": 31, "training_loss": 58.128310680389404, "training_acc": 72.0, "val_loss": 14.536036550998688, "val_acc": 72.0}
{"epoch": 32, "training_loss": 57.65290284156799, "training_acc": 72.0, "val_loss": 14.315256476402283, "val_acc": 72.0}
{"epoch": 33, "training_loss": 57.108906984329224, "training_acc": 72.0, "val_loss": 14.993982017040253, "val_acc": 72.0}
{"epoch": 34, "training_loss": 59.48229718208313, "training_acc": 72.0, "val_loss": 14.800268411636353, "val_acc": 72.0}
{"epoch": 35, "training_loss": 59.69140934944153, "training_acc": 72.0, "val_loss": 14.805848896503448, "val_acc": 72.0}
{"epoch": 36, "training_loss": 59.239131927490234, "training_acc": 72.0, "val_loss": 14.792676270008087, "val_acc": 72.0}
{"epoch": 37, "training_loss": 59.14167094230652, "training_acc": 72.0, "val_loss": 14.784479141235352, "val_acc": 72.0}
{"epoch": 38, "training_loss": 59.25292754173279, "training_acc": 72.0, "val_loss": 14.78811651468277, "val_acc": 72.0}
{"epoch": 39, "training_loss": 59.40274691581726, "training_acc": 72.0, "val_loss": 14.797775447368622, "val_acc": 72.0}
{"epoch": 40, "training_loss": 59.47636425495148, "training_acc": 72.0, "val_loss": 14.809684455394745, "val_acc": 72.0}
{"epoch": 41, "training_loss": 59.276793003082275, "training_acc": 72.0, "val_loss": 14.879298210144043, "val_acc": 72.0}
{"epoch": 42, "training_loss": 59.580018281936646, "training_acc": 72.0, "val_loss": 14.831092953681946, "val_acc": 72.0}
{"epoch": 43, "training_loss": 59.336241722106934, "training_acc": 72.0, "val_loss": 14.807489514350891, "val_acc": 72.0}
{"epoch": 44, "training_loss": 59.30924940109253, "training_acc": 72.0, "val_loss": 14.803363382816315, "val_acc": 72.0}
{"epoch": 45, "training_loss": 59.188029289245605, "training_acc": 72.0, "val_loss": 14.807233214378357, "val_acc": 72.0}
{"epoch": 46, "training_loss": 59.26761436462402, "training_acc": 72.0, "val_loss": 14.814414083957672, "val_acc": 72.0}
{"epoch": 47, "training_loss": 59.187464237213135, "training_acc": 72.0, "val_loss": 14.788010716438293, "val_acc": 72.0}
{"epoch": 48, "training_loss": 59.115772008895874, "training_acc": 72.0, "val_loss": 14.806771278381348, "val_acc": 72.0}
{"epoch": 49, "training_loss": 59.1827507019043, "training_acc": 72.0, "val_loss": 14.781911671161652, "val_acc": 72.0}
{"epoch": 50, "training_loss": 59.033344984054565, "training_acc": 72.0, "val_loss": 14.76294994354248, "val_acc": 72.0}
{"epoch": 51, "training_loss": 58.94082188606262, "training_acc": 72.0, "val_loss": 14.747200906276703, "val_acc": 72.0}
