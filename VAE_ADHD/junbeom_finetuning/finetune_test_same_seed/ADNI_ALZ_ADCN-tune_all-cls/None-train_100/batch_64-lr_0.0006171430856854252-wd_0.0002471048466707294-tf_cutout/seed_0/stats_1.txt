"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 66.34330987930298, "training_acc": 72.0, "val_loss": 14.990931749343872, "val_acc": 72.0}
{"epoch": 1, "training_loss": 300.3694019317627, "training_acc": 72.0, "val_loss": 15.090979635715485, "val_acc": 72.0}
{"epoch": 2, "training_loss": 64.94806432723999, "training_acc": 56.0, "val_loss": 16.254237294197083, "val_acc": 28.0}
{"epoch": 3, "training_loss": 63.13421106338501, "training_acc": 72.0, "val_loss": 16.10752046108246, "val_acc": 72.0}
{"epoch": 4, "training_loss": 60.74567151069641, "training_acc": 72.0, "val_loss": 20.256152749061584, "val_acc": 28.0}
{"epoch": 5, "training_loss": 71.04526114463806, "training_acc": 52.0, "val_loss": 14.87627774477005, "val_acc": 72.0}
{"epoch": 6, "training_loss": 60.44050669670105, "training_acc": 72.0, "val_loss": 14.959818124771118, "val_acc": 72.0}
{"epoch": 7, "training_loss": 62.20383429527283, "training_acc": 72.0, "val_loss": 14.863459765911102, "val_acc": 72.0}
{"epoch": 8, "training_loss": 61.69192624092102, "training_acc": 72.0, "val_loss": 14.855773746967316, "val_acc": 72.0}
{"epoch": 9, "training_loss": 60.20703959465027, "training_acc": 72.0, "val_loss": 15.023055672645569, "val_acc": 72.0}
{"epoch": 10, "training_loss": 60.31100010871887, "training_acc": 72.0, "val_loss": 14.915935695171356, "val_acc": 72.0}
{"epoch": 11, "training_loss": 59.74222135543823, "training_acc": 72.0, "val_loss": 14.835315942764282, "val_acc": 72.0}
{"epoch": 12, "training_loss": 60.11639904975891, "training_acc": 72.0, "val_loss": 14.934329688549042, "val_acc": 72.0}
{"epoch": 13, "training_loss": 60.55510377883911, "training_acc": 72.0, "val_loss": 14.816880226135254, "val_acc": 72.0}
{"epoch": 14, "training_loss": 59.548644065856934, "training_acc": 72.0, "val_loss": 14.82185572385788, "val_acc": 72.0}
{"epoch": 15, "training_loss": 59.627018213272095, "training_acc": 72.0, "val_loss": 14.818289875984192, "val_acc": 72.0}
{"epoch": 16, "training_loss": 59.4987952709198, "training_acc": 72.0, "val_loss": 14.78746235370636, "val_acc": 72.0}
{"epoch": 17, "training_loss": 59.67423391342163, "training_acc": 72.0, "val_loss": 14.791536331176758, "val_acc": 72.0}
{"epoch": 18, "training_loss": 59.520607471466064, "training_acc": 72.0, "val_loss": 14.768430590629578, "val_acc": 72.0}
{"epoch": 19, "training_loss": 59.34509205818176, "training_acc": 72.0, "val_loss": 14.762897789478302, "val_acc": 72.0}
{"epoch": 20, "training_loss": 59.39273500442505, "training_acc": 72.0, "val_loss": 14.765487611293793, "val_acc": 72.0}
{"epoch": 21, "training_loss": 59.30918836593628, "training_acc": 72.0, "val_loss": 14.79673981666565, "val_acc": 72.0}
{"epoch": 22, "training_loss": 59.396751165390015, "training_acc": 72.0, "val_loss": 14.772984385490417, "val_acc": 72.0}
{"epoch": 23, "training_loss": 59.267173528671265, "training_acc": 72.0, "val_loss": 14.712288975715637, "val_acc": 72.0}
{"epoch": 24, "training_loss": 59.10047101974487, "training_acc": 72.0, "val_loss": 14.711514115333557, "val_acc": 72.0}
{"epoch": 25, "training_loss": 59.07013130187988, "training_acc": 72.0, "val_loss": 14.667905867099762, "val_acc": 72.0}
{"epoch": 26, "training_loss": 58.90093111991882, "training_acc": 72.0, "val_loss": 14.703570306301117, "val_acc": 72.0}
{"epoch": 27, "training_loss": 58.945820808410645, "training_acc": 72.0, "val_loss": 14.604553580284119, "val_acc": 72.0}
{"epoch": 28, "training_loss": 59.13290095329285, "training_acc": 72.0, "val_loss": 14.726412296295166, "val_acc": 72.0}
{"epoch": 29, "training_loss": 59.05631709098816, "training_acc": 72.0, "val_loss": 14.579176902770996, "val_acc": 72.0}
{"epoch": 30, "training_loss": 60.13793182373047, "training_acc": 72.0, "val_loss": 14.852860569953918, "val_acc": 72.0}
{"epoch": 31, "training_loss": 63.95330047607422, "training_acc": 72.0, "val_loss": 14.830584824085236, "val_acc": 72.0}
{"epoch": 32, "training_loss": 59.93284296989441, "training_acc": 72.0, "val_loss": 15.08793830871582, "val_acc": 72.0}
{"epoch": 33, "training_loss": 60.91661262512207, "training_acc": 72.0, "val_loss": 15.24440199136734, "val_acc": 72.0}
{"epoch": 34, "training_loss": 60.92187857627869, "training_acc": 72.0, "val_loss": 15.016508102416992, "val_acc": 72.0}
{"epoch": 35, "training_loss": 60.48347449302673, "training_acc": 72.0, "val_loss": 14.816537499427795, "val_acc": 72.0}
{"epoch": 36, "training_loss": 59.59100270271301, "training_acc": 72.0, "val_loss": 14.780895411968231, "val_acc": 72.0}
{"epoch": 37, "training_loss": 59.372798681259155, "training_acc": 72.0, "val_loss": 14.77508544921875, "val_acc": 72.0}
{"epoch": 38, "training_loss": 59.36995244026184, "training_acc": 72.0, "val_loss": 14.780834317207336, "val_acc": 72.0}
{"epoch": 39, "training_loss": 59.30256915092468, "training_acc": 72.0, "val_loss": 14.809592068195343, "val_acc": 72.0}
{"epoch": 40, "training_loss": 59.584327936172485, "training_acc": 72.0, "val_loss": 14.74829912185669, "val_acc": 72.0}
{"epoch": 41, "training_loss": 58.965020418167114, "training_acc": 72.0, "val_loss": 14.794379472732544, "val_acc": 72.0}
{"epoch": 42, "training_loss": 59.351067304611206, "training_acc": 72.0, "val_loss": 14.82427567243576, "val_acc": 72.0}
{"epoch": 43, "training_loss": 59.29347062110901, "training_acc": 72.0, "val_loss": 14.729452133178711, "val_acc": 72.0}
{"epoch": 44, "training_loss": 58.88700270652771, "training_acc": 72.0, "val_loss": 14.74897563457489, "val_acc": 72.0}
{"epoch": 45, "training_loss": 59.119842767715454, "training_acc": 72.0, "val_loss": 14.783036708831787, "val_acc": 72.0}
{"epoch": 46, "training_loss": 59.160942792892456, "training_acc": 72.0, "val_loss": 14.678816497325897, "val_acc": 72.0}
{"epoch": 47, "training_loss": 58.694183111190796, "training_acc": 72.0, "val_loss": 14.666707813739777, "val_acc": 72.0}
{"epoch": 48, "training_loss": 58.42496299743652, "training_acc": 72.0, "val_loss": 14.592981338500977, "val_acc": 72.0}
{"epoch": 49, "training_loss": 60.571723222732544, "training_acc": 72.0, "val_loss": 14.571475982666016, "val_acc": 72.0}
{"epoch": 50, "training_loss": 57.97350716590881, "training_acc": 72.0, "val_loss": 15.011386573314667, "val_acc": 60.0}
{"epoch": 51, "training_loss": 59.940423011779785, "training_acc": 72.0, "val_loss": 14.637923240661621, "val_acc": 72.0}
{"epoch": 52, "training_loss": 58.44751453399658, "training_acc": 72.0, "val_loss": 14.629465341567993, "val_acc": 72.0}
{"epoch": 53, "training_loss": 58.282623052597046, "training_acc": 72.0, "val_loss": 14.575858414173126, "val_acc": 72.0}
{"epoch": 54, "training_loss": 57.74906826019287, "training_acc": 72.0, "val_loss": 14.483897387981415, "val_acc": 72.0}
{"epoch": 55, "training_loss": 58.350075006484985, "training_acc": 72.0, "val_loss": 14.504256844520569, "val_acc": 72.0}
{"epoch": 56, "training_loss": 58.99097466468811, "training_acc": 72.0, "val_loss": 14.514453709125519, "val_acc": 72.0}
{"epoch": 57, "training_loss": 59.51203227043152, "training_acc": 72.0, "val_loss": 14.970758557319641, "val_acc": 56.0}
{"epoch": 58, "training_loss": 58.579437255859375, "training_acc": 72.0, "val_loss": 14.445994794368744, "val_acc": 72.0}
{"epoch": 59, "training_loss": 57.96322560310364, "training_acc": 72.0, "val_loss": 14.95303362607956, "val_acc": 72.0}
{"epoch": 60, "training_loss": 59.04875588417053, "training_acc": 72.0, "val_loss": 14.591975510120392, "val_acc": 72.0}
{"epoch": 61, "training_loss": 57.59577655792236, "training_acc": 72.0, "val_loss": 14.520107209682465, "val_acc": 72.0}
{"epoch": 62, "training_loss": 58.12619709968567, "training_acc": 72.0, "val_loss": 14.524367451667786, "val_acc": 72.0}
{"epoch": 63, "training_loss": 57.519461154937744, "training_acc": 72.0, "val_loss": 14.450831711292267, "val_acc": 72.0}
{"epoch": 64, "training_loss": 56.794912576675415, "training_acc": 72.0, "val_loss": 14.400556683540344, "val_acc": 72.0}
{"epoch": 65, "training_loss": 56.0656464099884, "training_acc": 72.0, "val_loss": 14.312933385372162, "val_acc": 60.0}
{"epoch": 66, "training_loss": 55.01873707771301, "training_acc": 72.0, "val_loss": 15.888941287994385, "val_acc": 72.0}
{"epoch": 67, "training_loss": 55.408260107040405, "training_acc": 72.0, "val_loss": 15.40844738483429, "val_acc": 40.0}
{"epoch": 68, "training_loss": 58.85743021965027, "training_acc": 76.0, "val_loss": 18.23635697364807, "val_acc": 72.0}
{"epoch": 69, "training_loss": 54.712706089019775, "training_acc": 72.0, "val_loss": 16.456565260887146, "val_acc": 28.0}
{"epoch": 70, "training_loss": 65.52412056922913, "training_acc": 73.0, "val_loss": 15.486498177051544, "val_acc": 28.0}
{"epoch": 71, "training_loss": 62.274380922317505, "training_acc": 72.0, "val_loss": 14.805977046489716, "val_acc": 72.0}
{"epoch": 72, "training_loss": 60.25091075897217, "training_acc": 72.0, "val_loss": 14.824546873569489, "val_acc": 72.0}
{"epoch": 73, "training_loss": 59.62165188789368, "training_acc": 72.0, "val_loss": 14.806579053401947, "val_acc": 72.0}
{"epoch": 74, "training_loss": 59.529080629348755, "training_acc": 72.0, "val_loss": 14.819490909576416, "val_acc": 72.0}
{"epoch": 75, "training_loss": 59.614360094070435, "training_acc": 72.0, "val_loss": 14.75776880979538, "val_acc": 72.0}
{"epoch": 76, "training_loss": 59.56863594055176, "training_acc": 72.0, "val_loss": 14.91873562335968, "val_acc": 72.0}
{"epoch": 77, "training_loss": 60.599809765815735, "training_acc": 72.0, "val_loss": 14.758758246898651, "val_acc": 72.0}
{"epoch": 78, "training_loss": 59.303982973098755, "training_acc": 72.0, "val_loss": 14.732298254966736, "val_acc": 72.0}
{"epoch": 79, "training_loss": 59.12950873374939, "training_acc": 72.0, "val_loss": 14.736051857471466, "val_acc": 72.0}
{"epoch": 80, "training_loss": 59.11225962638855, "training_acc": 72.0, "val_loss": 14.74589854478836, "val_acc": 72.0}
{"epoch": 81, "training_loss": 59.066503286361694, "training_acc": 72.0, "val_loss": 14.739057421684265, "val_acc": 72.0}
{"epoch": 82, "training_loss": 58.996880292892456, "training_acc": 72.0, "val_loss": 14.656314253807068, "val_acc": 72.0}
{"epoch": 83, "training_loss": 58.70053148269653, "training_acc": 72.0, "val_loss": 14.644891023635864, "val_acc": 72.0}
{"epoch": 84, "training_loss": 58.646705627441406, "training_acc": 72.0, "val_loss": 14.557524025440216, "val_acc": 72.0}
