"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/BT_weights/BT_weights_no_norm5//ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 25725.05994796753, "training_acc": 42.0, "val_loss": 4120.829391479492, "val_acc": 48.0}
{"epoch": 1, "training_loss": 17761.355590820312, "training_acc": 53.0, "val_loss": 10719.220733642578, "val_acc": 52.0}
{"epoch": 2, "training_loss": 40869.14904785156, "training_acc": 53.0, "val_loss": 6310.101318359375, "val_acc": 52.0}
{"epoch": 3, "training_loss": 15890.759239196777, "training_acc": 59.0, "val_loss": 4299.853515625, "val_acc": 48.0}
{"epoch": 4, "training_loss": 20996.503784179688, "training_acc": 47.0, "val_loss": 4974.092483520508, "val_acc": 52.0}
{"epoch": 5, "training_loss": 16211.157623291016, "training_acc": 53.0, "val_loss": 2908.48388671875, "val_acc": 36.0}
{"epoch": 6, "training_loss": 8144.044189453125, "training_acc": 63.0, "val_loss": 6227.260208129883, "val_acc": 52.0}
{"epoch": 7, "training_loss": 14274.312408447266, "training_acc": 61.0, "val_loss": 3771.768569946289, "val_acc": 44.0}
{"epoch": 8, "training_loss": 7121.500747680664, "training_acc": 66.0, "val_loss": 2999.093246459961, "val_acc": 44.0}
{"epoch": 9, "training_loss": 11660.898864746094, "training_acc": 59.0, "val_loss": 3044.7025299072266, "val_acc": 48.0}
{"epoch": 10, "training_loss": 7558.400573730469, "training_acc": 63.0, "val_loss": 3638.959503173828, "val_acc": 48.0}
{"epoch": 11, "training_loss": 6333.804748535156, "training_acc": 68.0, "val_loss": 4926.393890380859, "val_acc": 48.0}
{"epoch": 12, "training_loss": 7345.389114379883, "training_acc": 64.0, "val_loss": 2478.009033203125, "val_acc": 36.0}
{"epoch": 13, "training_loss": 4481.879852294922, "training_acc": 62.0, "val_loss": 2412.104034423828, "val_acc": 28.0}
{"epoch": 14, "training_loss": 5534.811943054199, "training_acc": 55.0, "val_loss": 2840.835952758789, "val_acc": 44.0}
{"epoch": 15, "training_loss": 4357.018936157227, "training_acc": 65.0, "val_loss": 2247.450637817383, "val_acc": 40.0}
{"epoch": 16, "training_loss": 2685.5849227905273, "training_acc": 71.0, "val_loss": 2040.6803131103516, "val_acc": 28.0}
{"epoch": 17, "training_loss": 2755.4122924804688, "training_acc": 57.0, "val_loss": 2389.8794174194336, "val_acc": 40.0}
{"epoch": 18, "training_loss": 2613.685161590576, "training_acc": 63.0, "val_loss": 2079.336929321289, "val_acc": 36.0}
{"epoch": 19, "training_loss": 2515.3823471069336, "training_acc": 66.0, "val_loss": 2444.740867614746, "val_acc": 40.0}
{"epoch": 20, "training_loss": 3302.6239318847656, "training_acc": 69.0, "val_loss": 2236.9754791259766, "val_acc": 40.0}
{"epoch": 21, "training_loss": 2590.1153411865234, "training_acc": 71.0, "val_loss": 1827.2285461425781, "val_acc": 36.0}
{"epoch": 22, "training_loss": 2462.3072814941406, "training_acc": 59.0, "val_loss": 1911.0015869140625, "val_acc": 52.0}
{"epoch": 23, "training_loss": 2785.793960571289, "training_acc": 61.0, "val_loss": 1298.729419708252, "val_acc": 52.0}
{"epoch": 24, "training_loss": 1764.7969284057617, "training_acc": 61.0, "val_loss": 944.7552680969238, "val_acc": 44.0}
{"epoch": 25, "training_loss": 1667.3432312011719, "training_acc": 65.0, "val_loss": 1182.2786331176758, "val_acc": 44.0}
{"epoch": 26, "training_loss": 1631.415397644043, "training_acc": 64.0, "val_loss": 1005.7013511657715, "val_acc": 56.0}
{"epoch": 27, "training_loss": 1483.5867958068848, "training_acc": 66.0, "val_loss": 817.4884796142578, "val_acc": 60.0}
{"epoch": 28, "training_loss": 1058.7738933563232, "training_acc": 73.0, "val_loss": 857.4091911315918, "val_acc": 52.0}
{"epoch": 29, "training_loss": 1597.1918277740479, "training_acc": 67.0, "val_loss": 755.5492401123047, "val_acc": 36.0}
{"epoch": 30, "training_loss": 950.7291717529297, "training_acc": 69.0, "val_loss": 1308.9765548706055, "val_acc": 52.0}
{"epoch": 31, "training_loss": 2243.300926208496, "training_acc": 65.0, "val_loss": 1302.8447151184082, "val_acc": 44.0}
{"epoch": 32, "training_loss": 2823.2083740234375, "training_acc": 58.0, "val_loss": 1699.1535186767578, "val_acc": 44.0}
{"epoch": 33, "training_loss": 4161.913787841797, "training_acc": 62.0, "val_loss": 1088.2351875305176, "val_acc": 52.0}
{"epoch": 34, "training_loss": 1907.3967514038086, "training_acc": 65.0, "val_loss": 978.2258987426758, "val_acc": 52.0}
{"epoch": 35, "training_loss": 3137.7265815734863, "training_acc": 59.0, "val_loss": 1316.6308403015137, "val_acc": 52.0}
{"epoch": 36, "training_loss": 2039.618637084961, "training_acc": 67.0, "val_loss": 1273.2617378234863, "val_acc": 40.0}
{"epoch": 37, "training_loss": 1808.8726272583008, "training_acc": 59.0, "val_loss": 1251.3559341430664, "val_acc": 40.0}
{"epoch": 38, "training_loss": 1318.7691955566406, "training_acc": 70.0, "val_loss": 1228.666114807129, "val_acc": 28.0}
{"epoch": 39, "training_loss": 1967.3580207824707, "training_acc": 62.0, "val_loss": 924.6660232543945, "val_acc": 40.0}
{"epoch": 40, "training_loss": 1079.3044204711914, "training_acc": 73.0, "val_loss": 787.6719951629639, "val_acc": 52.0}
{"epoch": 41, "training_loss": 1567.6214065551758, "training_acc": 66.0, "val_loss": 1061.1480712890625, "val_acc": 48.0}
{"epoch": 42, "training_loss": 1505.4250679016113, "training_acc": 63.0, "val_loss": 1614.2236709594727, "val_acc": 52.0}
{"epoch": 43, "training_loss": 2619.023880004883, "training_acc": 69.0, "val_loss": 949.5624542236328, "val_acc": 52.0}
{"epoch": 44, "training_loss": 2108.1454315185547, "training_acc": 63.0, "val_loss": 875.4511833190918, "val_acc": 60.0}
{"epoch": 45, "training_loss": 1778.759620666504, "training_acc": 67.0, "val_loss": 1072.1529006958008, "val_acc": 56.0}
{"epoch": 46, "training_loss": 3038.1314849853516, "training_acc": 59.0, "val_loss": 784.9896907806396, "val_acc": 48.0}
{"epoch": 47, "training_loss": 2726.6055755615234, "training_acc": 62.0, "val_loss": 1615.568733215332, "val_acc": 48.0}
{"epoch": 48, "training_loss": 3568.5239639282227, "training_acc": 65.0, "val_loss": 1528.9965629577637, "val_acc": 56.0}
