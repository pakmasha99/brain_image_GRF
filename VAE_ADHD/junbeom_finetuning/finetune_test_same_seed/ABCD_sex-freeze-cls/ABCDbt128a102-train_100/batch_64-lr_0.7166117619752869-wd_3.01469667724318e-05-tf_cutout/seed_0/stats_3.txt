"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/BT_weights/BT_weights_no_norm5//ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 2440.1200942993164, "training_acc": 53.0, "val_loss": 579.3600559234619, "val_acc": 52.0}
{"epoch": 1, "training_loss": 2117.4607696533203, "training_acc": 51.0, "val_loss": 1080.9318542480469, "val_acc": 48.0}
{"epoch": 2, "training_loss": 2659.4515533447266, "training_acc": 50.0, "val_loss": 407.2138786315918, "val_acc": 52.0}
{"epoch": 3, "training_loss": 1574.0256881713867, "training_acc": 58.0, "val_loss": 366.84515476226807, "val_acc": 60.0}
{"epoch": 4, "training_loss": 1189.6421279907227, "training_acc": 54.0, "val_loss": 550.3696918487549, "val_acc": 52.0}
{"epoch": 5, "training_loss": 1389.3272743225098, "training_acc": 54.0, "val_loss": 393.78373622894287, "val_acc": 56.0}
{"epoch": 6, "training_loss": 632.1345138549805, "training_acc": 68.0, "val_loss": 436.30080223083496, "val_acc": 44.0}
{"epoch": 7, "training_loss": 969.1040649414062, "training_acc": 61.0, "val_loss": 382.3411703109741, "val_acc": 52.0}
{"epoch": 8, "training_loss": 609.5750770568848, "training_acc": 71.0, "val_loss": 565.8823490142822, "val_acc": 52.0}
{"epoch": 9, "training_loss": 811.1329612731934, "training_acc": 59.0, "val_loss": 373.90968799591064, "val_acc": 52.0}
{"epoch": 10, "training_loss": 664.4282760620117, "training_acc": 64.0, "val_loss": 325.8387327194214, "val_acc": 40.0}
{"epoch": 11, "training_loss": 483.26234912872314, "training_acc": 72.0, "val_loss": 317.5910949707031, "val_acc": 48.0}
{"epoch": 12, "training_loss": 583.3197937011719, "training_acc": 60.0, "val_loss": 205.09600639343262, "val_acc": 52.0}
{"epoch": 13, "training_loss": 350.05039978027344, "training_acc": 72.0, "val_loss": 273.26629161834717, "val_acc": 48.0}
{"epoch": 14, "training_loss": 467.2262372970581, "training_acc": 65.0, "val_loss": 202.31876373291016, "val_acc": 48.0}
{"epoch": 15, "training_loss": 167.51033306121826, "training_acc": 76.0, "val_loss": 290.62795639038086, "val_acc": 48.0}
{"epoch": 16, "training_loss": 226.8741021156311, "training_acc": 63.0, "val_loss": 257.5300931930542, "val_acc": 44.0}
{"epoch": 17, "training_loss": 312.81602573394775, "training_acc": 65.0, "val_loss": 300.42779445648193, "val_acc": 48.0}
{"epoch": 18, "training_loss": 335.34118270874023, "training_acc": 65.0, "val_loss": 232.57451057434082, "val_acc": 40.0}
{"epoch": 19, "training_loss": 260.40373516082764, "training_acc": 70.0, "val_loss": 233.08570384979248, "val_acc": 40.0}
{"epoch": 20, "training_loss": 216.50214672088623, "training_acc": 74.0, "val_loss": 291.54553413391113, "val_acc": 52.0}
{"epoch": 21, "training_loss": 219.05986166000366, "training_acc": 67.0, "val_loss": 255.3117036819458, "val_acc": 28.0}
{"epoch": 22, "training_loss": 201.55400848388672, "training_acc": 70.0, "val_loss": 381.4122200012207, "val_acc": 48.0}
{"epoch": 23, "training_loss": 478.68870401382446, "training_acc": 60.0, "val_loss": 321.49345874786377, "val_acc": 32.0}
{"epoch": 24, "training_loss": 258.780535697937, "training_acc": 71.0, "val_loss": 305.5426597595215, "val_acc": 32.0}
{"epoch": 25, "training_loss": 151.82678842544556, "training_acc": 79.0, "val_loss": 281.69641494750977, "val_acc": 40.0}
{"epoch": 26, "training_loss": 182.28272604942322, "training_acc": 76.0, "val_loss": 252.11448669433594, "val_acc": 44.0}
{"epoch": 27, "training_loss": 223.11414527893066, "training_acc": 70.0, "val_loss": 282.44261741638184, "val_acc": 36.0}
{"epoch": 28, "training_loss": 239.62055778503418, "training_acc": 63.0, "val_loss": 287.3307704925537, "val_acc": 36.0}
{"epoch": 29, "training_loss": 312.6648826599121, "training_acc": 71.0, "val_loss": 286.8016481399536, "val_acc": 32.0}
{"epoch": 30, "training_loss": 324.6945686340332, "training_acc": 72.0, "val_loss": 320.45555114746094, "val_acc": 48.0}
{"epoch": 31, "training_loss": 184.50935220718384, "training_acc": 74.0, "val_loss": 342.9863929748535, "val_acc": 40.0}
{"epoch": 32, "training_loss": 431.5979595184326, "training_acc": 67.0, "val_loss": 288.9887571334839, "val_acc": 40.0}
{"epoch": 33, "training_loss": 282.96094512939453, "training_acc": 71.0, "val_loss": 329.47895526885986, "val_acc": 48.0}
