"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/BT_weights/BT_weights_no_norm5//ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 1519.8484115600586, "training_acc": 53.0, "val_loss": 291.5677309036255, "val_acc": 52.0}
{"epoch": 1, "training_loss": 1272.0666275024414, "training_acc": 55.0, "val_loss": 667.4511432647705, "val_acc": 48.0}
{"epoch": 2, "training_loss": 2544.291244506836, "training_acc": 47.0, "val_loss": 223.18377494812012, "val_acc": 48.0}
{"epoch": 3, "training_loss": 780.8164043426514, "training_acc": 55.0, "val_loss": 434.844446182251, "val_acc": 52.0}
{"epoch": 4, "training_loss": 1769.0532608032227, "training_acc": 53.0, "val_loss": 367.30077266693115, "val_acc": 52.0}
{"epoch": 5, "training_loss": 1108.0864906311035, "training_acc": 53.0, "val_loss": 189.2838954925537, "val_acc": 48.0}
{"epoch": 6, "training_loss": 994.5809631347656, "training_acc": 47.0, "val_loss": 336.7239236831665, "val_acc": 48.0}
{"epoch": 7, "training_loss": 1169.0144691467285, "training_acc": 47.0, "val_loss": 47.827062010765076, "val_acc": 52.0}
{"epoch": 8, "training_loss": 319.4060745239258, "training_acc": 53.0, "val_loss": 139.99933004379272, "val_acc": 52.0}
{"epoch": 9, "training_loss": 403.159544467926, "training_acc": 53.0, "val_loss": 158.89325141906738, "val_acc": 48.0}
{"epoch": 10, "training_loss": 727.5607414245605, "training_acc": 47.0, "val_loss": 136.0798954963684, "val_acc": 48.0}
{"epoch": 11, "training_loss": 453.37399673461914, "training_acc": 45.0, "val_loss": 102.51106023788452, "val_acc": 52.0}
{"epoch": 12, "training_loss": 336.7599811553955, "training_acc": 53.0, "val_loss": 77.27273106575012, "val_acc": 48.0}
{"epoch": 13, "training_loss": 335.9899158477783, "training_acc": 47.0, "val_loss": 18.458735942840576, "val_acc": 52.0}
{"epoch": 14, "training_loss": 139.90829849243164, "training_acc": 53.0, "val_loss": 33.08789134025574, "val_acc": 52.0}
{"epoch": 15, "training_loss": 262.4301357269287, "training_acc": 43.0, "val_loss": 83.704674243927, "val_acc": 48.0}
{"epoch": 16, "training_loss": 304.62137603759766, "training_acc": 43.0, "val_loss": 51.325637102127075, "val_acc": 52.0}
{"epoch": 17, "training_loss": 149.009094953537, "training_acc": 55.0, "val_loss": 31.864851713180542, "val_acc": 48.0}
{"epoch": 18, "training_loss": 170.13444328308105, "training_acc": 45.0, "val_loss": 31.921091675758362, "val_acc": 52.0}
{"epoch": 19, "training_loss": 130.68189096450806, "training_acc": 59.0, "val_loss": 56.13369345664978, "val_acc": 48.0}
{"epoch": 20, "training_loss": 198.12699842453003, "training_acc": 51.0, "val_loss": 50.93052387237549, "val_acc": 52.0}
{"epoch": 21, "training_loss": 180.7134566307068, "training_acc": 49.0, "val_loss": 21.53051197528839, "val_acc": 48.0}
{"epoch": 22, "training_loss": 197.03857517242432, "training_acc": 37.0, "val_loss": 33.06664824485779, "val_acc": 52.0}
{"epoch": 23, "training_loss": 206.0020637512207, "training_acc": 51.0, "val_loss": 89.37299251556396, "val_acc": 48.0}
{"epoch": 24, "training_loss": 242.10783696174622, "training_acc": 53.0, "val_loss": 77.19662189483643, "val_acc": 52.0}
{"epoch": 25, "training_loss": 263.2575912475586, "training_acc": 53.0, "val_loss": 60.552871227264404, "val_acc": 48.0}
{"epoch": 26, "training_loss": 277.26564025878906, "training_acc": 47.0, "val_loss": 35.93160808086395, "val_acc": 52.0}
{"epoch": 27, "training_loss": 146.8797402381897, "training_acc": 53.0, "val_loss": 20.418578386306763, "val_acc": 52.0}
{"epoch": 28, "training_loss": 77.10645198822021, "training_acc": 47.0, "val_loss": 27.18752920627594, "val_acc": 52.0}
{"epoch": 29, "training_loss": 82.55299854278564, "training_acc": 55.0, "val_loss": 24.293547868728638, "val_acc": 48.0}
{"epoch": 30, "training_loss": 128.53393983840942, "training_acc": 45.0, "val_loss": 17.427261173725128, "val_acc": 52.0}
{"epoch": 31, "training_loss": 70.45868635177612, "training_acc": 64.0, "val_loss": 19.43677067756653, "val_acc": 52.0}
{"epoch": 32, "training_loss": 88.83600807189941, "training_acc": 53.0, "val_loss": 67.98049211502075, "val_acc": 48.0}
{"epoch": 33, "training_loss": 235.11766719818115, "training_acc": 47.0, "val_loss": 73.91626834869385, "val_acc": 52.0}
{"epoch": 34, "training_loss": 275.65613746643066, "training_acc": 53.0, "val_loss": 17.528162896633148, "val_acc": 44.0}
{"epoch": 35, "training_loss": 100.37434434890747, "training_acc": 56.0, "val_loss": 17.696762084960938, "val_acc": 44.0}
{"epoch": 36, "training_loss": 89.55508136749268, "training_acc": 62.0, "val_loss": 21.12765610218048, "val_acc": 52.0}
{"epoch": 37, "training_loss": 108.47564458847046, "training_acc": 57.0, "val_loss": 26.442506909370422, "val_acc": 48.0}
{"epoch": 38, "training_loss": 150.90139961242676, "training_acc": 53.0, "val_loss": 77.08765864372253, "val_acc": 52.0}
{"epoch": 39, "training_loss": 224.87378025054932, "training_acc": 49.0, "val_loss": 35.167503356933594, "val_acc": 48.0}
{"epoch": 40, "training_loss": 150.6989688873291, "training_acc": 49.0, "val_loss": 30.35753071308136, "val_acc": 52.0}
{"epoch": 41, "training_loss": 123.91494226455688, "training_acc": 59.0, "val_loss": 46.558353304862976, "val_acc": 48.0}
{"epoch": 42, "training_loss": 233.74258041381836, "training_acc": 45.0, "val_loss": 70.31410336494446, "val_acc": 52.0}
{"epoch": 43, "training_loss": 245.91216802597046, "training_acc": 45.0, "val_loss": 23.881816864013672, "val_acc": 48.0}
{"epoch": 44, "training_loss": 155.62691402435303, "training_acc": 47.0, "val_loss": 45.216262340545654, "val_acc": 52.0}
{"epoch": 45, "training_loss": 203.0877685546875, "training_acc": 51.0, "val_loss": 61.72599792480469, "val_acc": 48.0}
{"epoch": 46, "training_loss": 279.45893573760986, "training_acc": 41.0, "val_loss": 58.63500237464905, "val_acc": 52.0}
{"epoch": 47, "training_loss": 193.186758518219, "training_acc": 51.0, "val_loss": 32.1225106716156, "val_acc": 48.0}
{"epoch": 48, "training_loss": 135.67400979995728, "training_acc": 55.0, "val_loss": 58.84614586830139, "val_acc": 52.0}
{"epoch": 49, "training_loss": 212.52208614349365, "training_acc": 47.0, "val_loss": 23.200030624866486, "val_acc": 44.0}
