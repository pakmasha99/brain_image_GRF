"main_optuna_fix.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/BT_weights/BT_weights_no_norm5//ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed --binary_class True --run_where sdcc"
{"epoch": 0, "training_loss": 2447.2853469848633, "training_acc": 50.0, "val_loss": 27831.210327148438, "val_acc": 56.0}
{"epoch": 1, "training_loss": 121561.02099609375, "training_acc": 52.0, "val_loss": 14910.264587402344, "val_acc": 56.0}
{"epoch": 2, "training_loss": 46645.59893798828, "training_acc": 54.0, "val_loss": 14422.79052734375, "val_acc": 44.0}
{"epoch": 3, "training_loss": 46316.91711425781, "training_acc": 48.0, "val_loss": 3100.3787994384766, "val_acc": 56.0}
{"epoch": 4, "training_loss": 16098.737670898438, "training_acc": 52.0, "val_loss": 475.97341537475586, "val_acc": 52.0}
{"epoch": 5, "training_loss": 12832.892517089844, "training_acc": 59.0, "val_loss": 13633.059692382812, "val_acc": 44.0}
{"epoch": 6, "training_loss": 42752.95361328125, "training_acc": 48.0, "val_loss": 2437.966728210449, "val_acc": 56.0}
{"epoch": 7, "training_loss": 14210.777160644531, "training_acc": 52.0, "val_loss": 2867.837333679199, "val_acc": 56.0}
{"epoch": 8, "training_loss": 11313.281677246094, "training_acc": 58.0, "val_loss": 6179.914855957031, "val_acc": 44.0}
{"epoch": 9, "training_loss": 15480.72501373291, "training_acc": 48.0, "val_loss": 7447.8424072265625, "val_acc": 56.0}
{"epoch": 10, "training_loss": 36309.08703613281, "training_acc": 52.0, "val_loss": 8280.340576171875, "val_acc": 56.0}
{"epoch": 11, "training_loss": 25869.45425415039, "training_acc": 52.0, "val_loss": 9859.880828857422, "val_acc": 44.0}
{"epoch": 12, "training_loss": 41670.27722167969, "training_acc": 48.0, "val_loss": 13921.516418457031, "val_acc": 44.0}
{"epoch": 13, "training_loss": 41388.50646972656, "training_acc": 48.0, "val_loss": 3450.9204864501953, "val_acc": 56.0}
{"epoch": 14, "training_loss": 20303.365234375, "training_acc": 52.0, "val_loss": 7192.748260498047, "val_acc": 56.0}
{"epoch": 15, "training_loss": 24641.563751220703, "training_acc": 52.0, "val_loss": 6657.308197021484, "val_acc": 44.0}
{"epoch": 16, "training_loss": 27521.25, "training_acc": 48.0, "val_loss": 7074.852752685547, "val_acc": 44.0}
{"epoch": 17, "training_loss": 16740.43635559082, "training_acc": 50.0, "val_loss": 2275.5496978759766, "val_acc": 56.0}
{"epoch": 18, "training_loss": 6576.845539093018, "training_acc": 51.0, "val_loss": 421.60840034484863, "val_acc": 52.0}
{"epoch": 19, "training_loss": 4250.3792724609375, "training_acc": 55.0, "val_loss": 590.2309894561768, "val_acc": 40.0}
{"epoch": 20, "training_loss": 7143.643737792969, "training_acc": 55.0, "val_loss": 4875.519943237305, "val_acc": 56.0}
{"epoch": 21, "training_loss": 15521.339141845703, "training_acc": 52.0, "val_loss": 7676.960754394531, "val_acc": 44.0}
{"epoch": 22, "training_loss": 29621.195068359375, "training_acc": 48.0, "val_loss": 6687.467956542969, "val_acc": 44.0}
{"epoch": 23, "training_loss": 18240.02392578125, "training_acc": 46.0, "val_loss": 3000.5199432373047, "val_acc": 56.0}
{"epoch": 24, "training_loss": 8485.39826965332, "training_acc": 54.0, "val_loss": 6937.855529785156, "val_acc": 44.0}
{"epoch": 25, "training_loss": 26862.172485351562, "training_acc": 48.0, "val_loss": 4208.530044555664, "val_acc": 44.0}
{"epoch": 26, "training_loss": 20630.241455078125, "training_acc": 38.0, "val_loss": 6809.960174560547, "val_acc": 56.0}
{"epoch": 27, "training_loss": 25836.206420898438, "training_acc": 52.0, "val_loss": 1451.8352508544922, "val_acc": 44.0}
{"epoch": 28, "training_loss": 6827.244171142578, "training_acc": 48.0, "val_loss": 1056.2065124511719, "val_acc": 56.0}
{"epoch": 29, "training_loss": 3011.495803833008, "training_acc": 52.0, "val_loss": 2753.5797119140625, "val_acc": 44.0}
{"epoch": 30, "training_loss": 7377.602409362793, "training_acc": 44.0, "val_loss": 1413.1031036376953, "val_acc": 44.0}
{"epoch": 31, "training_loss": 3425.9295196533203, "training_acc": 58.0, "val_loss": 309.58878993988037, "val_acc": 48.0}
{"epoch": 32, "training_loss": 4781.491271972656, "training_acc": 60.0, "val_loss": 1560.8473777770996, "val_acc": 44.0}
{"epoch": 33, "training_loss": 8607.738586425781, "training_acc": 52.0, "val_loss": 5506.770324707031, "val_acc": 56.0}
{"epoch": 34, "training_loss": 18812.924072265625, "training_acc": 52.0, "val_loss": 4821.959686279297, "val_acc": 44.0}
{"epoch": 35, "training_loss": 19747.021606445312, "training_acc": 48.0, "val_loss": 3606.9717407226562, "val_acc": 44.0}
{"epoch": 36, "training_loss": 12638.005126953125, "training_acc": 52.0, "val_loss": 5985.688400268555, "val_acc": 56.0}
{"epoch": 37, "training_loss": 22626.12905883789, "training_acc": 52.0, "val_loss": 2120.8553314208984, "val_acc": 44.0}
{"epoch": 38, "training_loss": 8511.656555175781, "training_acc": 48.0, "val_loss": 1793.0908203125, "val_acc": 56.0}
{"epoch": 39, "training_loss": 7737.768737792969, "training_acc": 52.0, "val_loss": 2665.752410888672, "val_acc": 44.0}
{"epoch": 40, "training_loss": 7997.237060546875, "training_acc": 48.0, "val_loss": 3021.468925476074, "val_acc": 56.0}
{"epoch": 41, "training_loss": 12883.483337402344, "training_acc": 52.0, "val_loss": 559.1746807098389, "val_acc": 44.0}
{"epoch": 42, "training_loss": 2277.1743927001953, "training_acc": 63.0, "val_loss": 1608.9349746704102, "val_acc": 56.0}
{"epoch": 43, "training_loss": 4674.16015625, "training_acc": 54.0, "val_loss": 5665.137481689453, "val_acc": 44.0}
{"epoch": 44, "training_loss": 20887.13787841797, "training_acc": 48.0, "val_loss": 1428.2868385314941, "val_acc": 44.0}
{"epoch": 45, "training_loss": 14017.457885742188, "training_acc": 46.0, "val_loss": 9753.592681884766, "val_acc": 56.0}
{"epoch": 46, "training_loss": 39721.172607421875, "training_acc": 52.0, "val_loss": 3358.1531524658203, "val_acc": 56.0}
{"epoch": 47, "training_loss": 19323.783813476562, "training_acc": 46.0, "val_loss": 11130.972290039062, "val_acc": 44.0}
{"epoch": 48, "training_loss": 37678.37512207031, "training_acc": 48.0, "val_loss": 2597.023582458496, "val_acc": 44.0}
{"epoch": 49, "training_loss": 18625.568237304688, "training_acc": 44.0, "val_loss": 11192.878723144531, "val_acc": 56.0}
{"epoch": 50, "training_loss": 47719.242919921875, "training_acc": 52.0, "val_loss": 6639.447784423828, "val_acc": 56.0}
