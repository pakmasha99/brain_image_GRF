"main_optuna_fix_3.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 80 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_ABCD_fix_3 --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 55.79572105407715, "training_acc": 40.0, "val_loss": 13.825228214263916, "val_acc": 55.0}
{"epoch": 1, "training_loss": 55.36070728302002, "training_acc": 53.75, "val_loss": 13.814542293548584, "val_acc": 55.0}
{"epoch": 2, "training_loss": 55.363847732543945, "training_acc": 52.5, "val_loss": 13.788261413574219, "val_acc": 55.0}
{"epoch": 3, "training_loss": 55.29335975646973, "training_acc": 52.5, "val_loss": 13.769216537475586, "val_acc": 55.0}
{"epoch": 4, "training_loss": 55.37872505187988, "training_acc": 52.5, "val_loss": 13.762108087539673, "val_acc": 55.0}
{"epoch": 5, "training_loss": 55.280943870544434, "training_acc": 52.5, "val_loss": 13.756252527236938, "val_acc": 55.0}
{"epoch": 6, "training_loss": 55.28024482727051, "training_acc": 52.5, "val_loss": 13.749011754989624, "val_acc": 55.0}
{"epoch": 7, "training_loss": 55.27858829498291, "training_acc": 52.5, "val_loss": 13.74271035194397, "val_acc": 55.0}
{"epoch": 8, "training_loss": 55.28891372680664, "training_acc": 52.5, "val_loss": 13.729642629623413, "val_acc": 55.0}
{"epoch": 9, "training_loss": 55.18269348144531, "training_acc": 52.5, "val_loss": 13.725893497467041, "val_acc": 55.0}
{"epoch": 10, "training_loss": 55.24245071411133, "training_acc": 52.5, "val_loss": 13.737136125564575, "val_acc": 55.0}
{"epoch": 11, "training_loss": 55.09653854370117, "training_acc": 52.5, "val_loss": 13.734884262084961, "val_acc": 55.0}
{"epoch": 12, "training_loss": 55.11468029022217, "training_acc": 52.5, "val_loss": 13.725260496139526, "val_acc": 55.0}
{"epoch": 13, "training_loss": 55.01900291442871, "training_acc": 52.5, "val_loss": 13.719887733459473, "val_acc": 55.0}
{"epoch": 14, "training_loss": 55.10049629211426, "training_acc": 52.5, "val_loss": 13.713854551315308, "val_acc": 55.0}
{"epoch": 15, "training_loss": 54.923139572143555, "training_acc": 52.5, "val_loss": 13.708100318908691, "val_acc": 55.0}
{"epoch": 16, "training_loss": 54.94518566131592, "training_acc": 52.5, "val_loss": 13.71045470237732, "val_acc": 55.0}
{"epoch": 17, "training_loss": 54.9937801361084, "training_acc": 52.5, "val_loss": 13.726005554199219, "val_acc": 55.0}
{"epoch": 18, "training_loss": 54.90753173828125, "training_acc": 52.5, "val_loss": 13.725001811981201, "val_acc": 55.0}
{"epoch": 19, "training_loss": 54.95126247406006, "training_acc": 52.5, "val_loss": 13.697407245635986, "val_acc": 55.0}
{"epoch": 20, "training_loss": 55.04856586456299, "training_acc": 52.5, "val_loss": 13.676716089248657, "val_acc": 55.0}
{"epoch": 21, "training_loss": 54.90189170837402, "training_acc": 52.5, "val_loss": 13.679622411727905, "val_acc": 55.0}
{"epoch": 22, "training_loss": 54.8719482421875, "training_acc": 52.5, "val_loss": 13.679746389389038, "val_acc": 55.0}
{"epoch": 23, "training_loss": 54.923441886901855, "training_acc": 55.0, "val_loss": 13.662766218185425, "val_acc": 55.0}
{"epoch": 24, "training_loss": 54.74375534057617, "training_acc": 53.75, "val_loss": 13.647650480270386, "val_acc": 55.0}
{"epoch": 25, "training_loss": 54.14498329162598, "training_acc": 52.5, "val_loss": 13.688215017318726, "val_acc": 55.0}
{"epoch": 26, "training_loss": 54.81980800628662, "training_acc": 52.5, "val_loss": 13.737425804138184, "val_acc": 55.0}
{"epoch": 27, "training_loss": 54.756258964538574, "training_acc": 52.5, "val_loss": 13.750704526901245, "val_acc": 55.0}
{"epoch": 28, "training_loss": 55.22749614715576, "training_acc": 52.5, "val_loss": 13.679773807525635, "val_acc": 55.0}
{"epoch": 29, "training_loss": 54.61348247528076, "training_acc": 52.5, "val_loss": 13.622013330459595, "val_acc": 55.0}
{"epoch": 30, "training_loss": 54.39194202423096, "training_acc": 52.5, "val_loss": 13.631197214126587, "val_acc": 55.0}
{"epoch": 31, "training_loss": 54.43385410308838, "training_acc": 53.75, "val_loss": 13.664412498474121, "val_acc": 55.0}
{"epoch": 32, "training_loss": 54.53829860687256, "training_acc": 72.5, "val_loss": 13.687366247177124, "val_acc": 55.0}
{"epoch": 33, "training_loss": 54.73589897155762, "training_acc": 65.0, "val_loss": 13.678388595581055, "val_acc": 55.0}
{"epoch": 34, "training_loss": 54.44687271118164, "training_acc": 66.25, "val_loss": 13.661640882492065, "val_acc": 55.0}
{"epoch": 35, "training_loss": 53.851704597473145, "training_acc": 60.0, "val_loss": 13.682373762130737, "val_acc": 55.0}
{"epoch": 36, "training_loss": 54.36539840698242, "training_acc": 53.75, "val_loss": 13.656290769577026, "val_acc": 55.0}
{"epoch": 37, "training_loss": 53.96352291107178, "training_acc": 58.75, "val_loss": 13.601793050765991, "val_acc": 55.0}
{"epoch": 38, "training_loss": 53.708824157714844, "training_acc": 60.0, "val_loss": 13.546589612960815, "val_acc": 55.0}
{"epoch": 39, "training_loss": 53.11782264709473, "training_acc": 67.5, "val_loss": 13.521016836166382, "val_acc": 55.0}
{"epoch": 40, "training_loss": 53.00647449493408, "training_acc": 61.25, "val_loss": 13.507857322692871, "val_acc": 55.0}
{"epoch": 41, "training_loss": 52.98074817657471, "training_acc": 67.5, "val_loss": 13.477765321731567, "val_acc": 55.0}
{"epoch": 42, "training_loss": 53.623597145080566, "training_acc": 67.5, "val_loss": 13.56009840965271, "val_acc": 55.0}
{"epoch": 43, "training_loss": 53.24105453491211, "training_acc": 72.5, "val_loss": 13.394670486450195, "val_acc": 55.0}
{"epoch": 44, "training_loss": 52.93726921081543, "training_acc": 67.5, "val_loss": 13.430496454238892, "val_acc": 55.0}
{"epoch": 45, "training_loss": 52.52572059631348, "training_acc": 71.25, "val_loss": 13.364464044570923, "val_acc": 55.0}
{"epoch": 46, "training_loss": 52.75659942626953, "training_acc": 71.25, "val_loss": 13.403970003128052, "val_acc": 55.0}
{"epoch": 47, "training_loss": 52.91533660888672, "training_acc": 70.0, "val_loss": 13.379369974136353, "val_acc": 55.0}
{"epoch": 48, "training_loss": 52.17850971221924, "training_acc": 75.0, "val_loss": 13.709195852279663, "val_acc": 55.0}
{"epoch": 49, "training_loss": 54.661208152770996, "training_acc": 56.25, "val_loss": 13.491061925888062, "val_acc": 55.0}
{"epoch": 50, "training_loss": 52.027560234069824, "training_acc": 63.75, "val_loss": 13.706909418106079, "val_acc": 55.0}
