"main_optuna_fix_3.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 80 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_ABCD_fix_3 --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 55.52061462402344, "training_acc": 52.5, "val_loss": 13.775776624679565, "val_acc": 55.0}
{"epoch": 1, "training_loss": 55.56625938415527, "training_acc": 52.5, "val_loss": 13.793305158615112, "val_acc": 55.0}
{"epoch": 2, "training_loss": 55.36368179321289, "training_acc": 52.5, "val_loss": 13.758912086486816, "val_acc": 55.0}
{"epoch": 3, "training_loss": 55.37478160858154, "training_acc": 52.5, "val_loss": 13.755143880844116, "val_acc": 55.0}
{"epoch": 4, "training_loss": 55.41559028625488, "training_acc": 52.5, "val_loss": 13.754786252975464, "val_acc": 55.0}
{"epoch": 5, "training_loss": 55.404351234436035, "training_acc": 52.5, "val_loss": 13.780332803726196, "val_acc": 55.0}
{"epoch": 6, "training_loss": 55.531118392944336, "training_acc": 52.5, "val_loss": 13.775637149810791, "val_acc": 55.0}
{"epoch": 7, "training_loss": 55.31382465362549, "training_acc": 52.5, "val_loss": 13.751767873764038, "val_acc": 55.0}
{"epoch": 8, "training_loss": 55.474724769592285, "training_acc": 52.5, "val_loss": 13.753015995025635, "val_acc": 55.0}
{"epoch": 9, "training_loss": 55.25564765930176, "training_acc": 52.5, "val_loss": 13.798384666442871, "val_acc": 55.0}
{"epoch": 10, "training_loss": 55.35931587219238, "training_acc": 52.5, "val_loss": 13.848267793655396, "val_acc": 55.0}
{"epoch": 11, "training_loss": 55.426350593566895, "training_acc": 52.5, "val_loss": 13.799561262130737, "val_acc": 55.0}
{"epoch": 12, "training_loss": 55.414939880371094, "training_acc": 52.5, "val_loss": 13.773561716079712, "val_acc": 55.0}
{"epoch": 13, "training_loss": 55.35795974731445, "training_acc": 52.5, "val_loss": 13.766710758209229, "val_acc": 55.0}
{"epoch": 14, "training_loss": 55.36047840118408, "training_acc": 52.5, "val_loss": 13.776308298110962, "val_acc": 55.0}
{"epoch": 15, "training_loss": 55.428693771362305, "training_acc": 52.5, "val_loss": 13.76962661743164, "val_acc": 55.0}
{"epoch": 16, "training_loss": 55.34246063232422, "training_acc": 52.5, "val_loss": 13.755311965942383, "val_acc": 55.0}
{"epoch": 17, "training_loss": 55.56107425689697, "training_acc": 52.5, "val_loss": 13.761824369430542, "val_acc": 55.0}
{"epoch": 18, "training_loss": 55.49626541137695, "training_acc": 52.5, "val_loss": 13.756426572799683, "val_acc": 55.0}
{"epoch": 19, "training_loss": 55.31154823303223, "training_acc": 52.5, "val_loss": 13.778600692749023, "val_acc": 55.0}
{"epoch": 20, "training_loss": 55.41671943664551, "training_acc": 52.5, "val_loss": 13.819152116775513, "val_acc": 55.0}
{"epoch": 21, "training_loss": 55.43290710449219, "training_acc": 52.5, "val_loss": 13.809152841567993, "val_acc": 55.0}
{"epoch": 22, "training_loss": 55.352505683898926, "training_acc": 52.5, "val_loss": 13.78219485282898, "val_acc": 55.0}
{"epoch": 23, "training_loss": 55.271663665771484, "training_acc": 52.5, "val_loss": 13.763278722763062, "val_acc": 55.0}
{"epoch": 24, "training_loss": 55.367122650146484, "training_acc": 52.5, "val_loss": 13.751801252365112, "val_acc": 55.0}
{"epoch": 25, "training_loss": 55.39636516571045, "training_acc": 52.5, "val_loss": 13.764314651489258, "val_acc": 55.0}
{"epoch": 26, "training_loss": 55.555442810058594, "training_acc": 52.5, "val_loss": 13.775743246078491, "val_acc": 55.0}
{"epoch": 27, "training_loss": 55.679503440856934, "training_acc": 52.5, "val_loss": 13.779093027114868, "val_acc": 55.0}
{"epoch": 28, "training_loss": 55.70071029663086, "training_acc": 52.5, "val_loss": 13.756986856460571, "val_acc": 55.0}
{"epoch": 29, "training_loss": 55.52900695800781, "training_acc": 52.5, "val_loss": 13.77226710319519, "val_acc": 55.0}
{"epoch": 30, "training_loss": 55.35490417480469, "training_acc": 52.5, "val_loss": 13.79515528678894, "val_acc": 55.0}
{"epoch": 31, "training_loss": 55.37903118133545, "training_acc": 52.5, "val_loss": 13.821570873260498, "val_acc": 55.0}
{"epoch": 32, "training_loss": 55.35272789001465, "training_acc": 52.5, "val_loss": 13.889247179031372, "val_acc": 55.0}
{"epoch": 33, "training_loss": 55.51070213317871, "training_acc": 47.5, "val_loss": 13.936151266098022, "val_acc": 55.0}
{"epoch": 34, "training_loss": 55.586856842041016, "training_acc": 47.5, "val_loss": 13.900041580200195, "val_acc": 55.0}
{"epoch": 35, "training_loss": 55.552032470703125, "training_acc": 45.0, "val_loss": 13.834964036941528, "val_acc": 55.0}
{"epoch": 36, "training_loss": 55.385375022888184, "training_acc": 52.5, "val_loss": 13.808022737503052, "val_acc": 55.0}
{"epoch": 37, "training_loss": 55.37832832336426, "training_acc": 52.5, "val_loss": 13.803203105926514, "val_acc": 55.0}
{"epoch": 38, "training_loss": 55.34600257873535, "training_acc": 52.5, "val_loss": 13.798385858535767, "val_acc": 55.0}
{"epoch": 39, "training_loss": 55.28034019470215, "training_acc": 52.5, "val_loss": 13.771746158599854, "val_acc": 55.0}
{"epoch": 40, "training_loss": 55.42098426818848, "training_acc": 52.5, "val_loss": 13.769100904464722, "val_acc": 55.0}
{"epoch": 41, "training_loss": 55.28790473937988, "training_acc": 52.5, "val_loss": 13.817610740661621, "val_acc": 55.0}
{"epoch": 42, "training_loss": 55.50001525878906, "training_acc": 47.5, "val_loss": 13.87632966041565, "val_acc": 55.0}
{"epoch": 43, "training_loss": 55.452820777893066, "training_acc": 51.25, "val_loss": 13.858485221862793, "val_acc": 55.0}
