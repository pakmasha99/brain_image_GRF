"main_optuna_fix_3.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 80 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_ABCD_fix_3 --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 55.39155387878418, "training_acc": 56.25, "val_loss": 13.855069875717163, "val_acc": 55.0}
{"epoch": 1, "training_loss": 55.64172840118408, "training_acc": 47.5, "val_loss": 13.82360577583313, "val_acc": 55.0}
{"epoch": 2, "training_loss": 55.516096115112305, "training_acc": 43.75, "val_loss": 13.773659467697144, "val_acc": 55.0}
{"epoch": 3, "training_loss": 55.33895301818848, "training_acc": 52.5, "val_loss": 13.772534132003784, "val_acc": 55.0}
{"epoch": 4, "training_loss": 55.11996269226074, "training_acc": 52.5, "val_loss": 13.798011541366577, "val_acc": 55.0}
{"epoch": 5, "training_loss": 55.39919090270996, "training_acc": 52.5, "val_loss": 13.840235471725464, "val_acc": 55.0}
{"epoch": 6, "training_loss": 55.57037353515625, "training_acc": 52.5, "val_loss": 13.810006380081177, "val_acc": 55.0}
{"epoch": 7, "training_loss": 55.30441665649414, "training_acc": 52.5, "val_loss": 13.772801160812378, "val_acc": 55.0}
{"epoch": 8, "training_loss": 55.16923522949219, "training_acc": 52.5, "val_loss": 13.756040334701538, "val_acc": 55.0}
{"epoch": 9, "training_loss": 55.22009468078613, "training_acc": 52.5, "val_loss": 13.757164478302002, "val_acc": 55.0}
{"epoch": 10, "training_loss": 55.317596435546875, "training_acc": 52.5, "val_loss": 13.758200407028198, "val_acc": 55.0}
{"epoch": 11, "training_loss": 55.150150299072266, "training_acc": 52.5, "val_loss": 13.756272792816162, "val_acc": 55.0}
{"epoch": 12, "training_loss": 55.247779846191406, "training_acc": 52.5, "val_loss": 13.758491277694702, "val_acc": 55.0}
{"epoch": 13, "training_loss": 55.16072082519531, "training_acc": 52.5, "val_loss": 13.760837316513062, "val_acc": 55.0}
{"epoch": 14, "training_loss": 55.099252700805664, "training_acc": 52.5, "val_loss": 13.757694959640503, "val_acc": 55.0}
{"epoch": 15, "training_loss": 55.06593322753906, "training_acc": 52.5, "val_loss": 13.745759725570679, "val_acc": 55.0}
{"epoch": 16, "training_loss": 55.09817123413086, "training_acc": 53.75, "val_loss": 13.726882934570312, "val_acc": 55.0}
{"epoch": 17, "training_loss": 55.03210258483887, "training_acc": 52.5, "val_loss": 13.717491626739502, "val_acc": 55.0}
{"epoch": 18, "training_loss": 54.782392501831055, "training_acc": 52.5, "val_loss": 13.716344833374023, "val_acc": 55.0}
{"epoch": 19, "training_loss": 54.74690628051758, "training_acc": 52.5, "val_loss": 13.710991144180298, "val_acc": 55.0}
{"epoch": 20, "training_loss": 55.05735206604004, "training_acc": 52.5, "val_loss": 13.705320358276367, "val_acc": 55.0}
{"epoch": 21, "training_loss": 55.201229095458984, "training_acc": 52.5, "val_loss": 13.702462911605835, "val_acc": 55.0}
{"epoch": 22, "training_loss": 54.98745918273926, "training_acc": 53.75, "val_loss": 13.701480627059937, "val_acc": 55.0}
{"epoch": 23, "training_loss": 54.71963310241699, "training_acc": 53.75, "val_loss": 13.69937777519226, "val_acc": 55.0}
{"epoch": 24, "training_loss": 54.871599197387695, "training_acc": 55.0, "val_loss": 13.69844675064087, "val_acc": 55.0}
{"epoch": 25, "training_loss": 54.08570575714111, "training_acc": 51.25, "val_loss": 13.748306035995483, "val_acc": 55.0}
{"epoch": 26, "training_loss": 54.64799976348877, "training_acc": 52.5, "val_loss": 13.88536810874939, "val_acc": 55.0}
{"epoch": 27, "training_loss": 54.619873046875, "training_acc": 52.5, "val_loss": 13.972489833831787, "val_acc": 55.0}
{"epoch": 28, "training_loss": 55.880937576293945, "training_acc": 52.5, "val_loss": 13.81171703338623, "val_acc": 55.0}
{"epoch": 29, "training_loss": 54.68593406677246, "training_acc": 52.5, "val_loss": 13.663332462310791, "val_acc": 55.0}
{"epoch": 30, "training_loss": 53.79945182800293, "training_acc": 58.75, "val_loss": 13.691301345825195, "val_acc": 55.0}
{"epoch": 31, "training_loss": 54.57809543609619, "training_acc": 62.5, "val_loss": 13.728495836257935, "val_acc": 55.0}
{"epoch": 32, "training_loss": 54.44501495361328, "training_acc": 76.25, "val_loss": 13.72301459312439, "val_acc": 55.0}
{"epoch": 33, "training_loss": 54.84124755859375, "training_acc": 72.5, "val_loss": 13.683236837387085, "val_acc": 55.0}
{"epoch": 34, "training_loss": 54.29203414916992, "training_acc": 60.0, "val_loss": 13.662700653076172, "val_acc": 55.0}
{"epoch": 35, "training_loss": 53.79293632507324, "training_acc": 60.0, "val_loss": 13.690422773361206, "val_acc": 55.0}
{"epoch": 36, "training_loss": 53.99265670776367, "training_acc": 55.0, "val_loss": 13.717604875564575, "val_acc": 55.0}
{"epoch": 37, "training_loss": 54.4234561920166, "training_acc": 52.5, "val_loss": 13.737610578536987, "val_acc": 55.0}
{"epoch": 38, "training_loss": 54.01828384399414, "training_acc": 51.25, "val_loss": 13.712700605392456, "val_acc": 55.0}
{"epoch": 39, "training_loss": 53.28208351135254, "training_acc": 55.0, "val_loss": 13.670047521591187, "val_acc": 55.0}
{"epoch": 40, "training_loss": 52.54345512390137, "training_acc": 63.75, "val_loss": 13.6130952835083, "val_acc": 55.0}
{"epoch": 41, "training_loss": 52.95570182800293, "training_acc": 58.75, "val_loss": 13.56477975845337, "val_acc": 55.0}
{"epoch": 42, "training_loss": 53.59709072113037, "training_acc": 68.75, "val_loss": 13.654427528381348, "val_acc": 55.0}
{"epoch": 43, "training_loss": 53.1790771484375, "training_acc": 63.75, "val_loss": 13.527443408966064, "val_acc": 55.0}
{"epoch": 44, "training_loss": 52.9281530380249, "training_acc": 61.25, "val_loss": 13.636950254440308, "val_acc": 55.0}
{"epoch": 45, "training_loss": 51.53708076477051, "training_acc": 63.75, "val_loss": 13.53983998298645, "val_acc": 55.0}
{"epoch": 46, "training_loss": 52.56104373931885, "training_acc": 56.25, "val_loss": 13.696216344833374, "val_acc": 55.0}
{"epoch": 47, "training_loss": 54.2193660736084, "training_acc": 53.75, "val_loss": 13.559008836746216, "val_acc": 55.0}
{"epoch": 48, "training_loss": 52.874704360961914, "training_acc": 63.75, "val_loss": 13.858674764633179, "val_acc": 55.0}
{"epoch": 49, "training_loss": 54.470947265625, "training_acc": 46.25, "val_loss": 13.987764120101929, "val_acc": 55.0}
{"epoch": 50, "training_loss": 52.09284019470215, "training_acc": 53.75, "val_loss": 13.417906761169434, "val_acc": 55.0}
