"main_optuna_fix_3.py --pretrained_path None --mode finetuning --train_num 80 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_ABCD_fix_3 --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 57.56931686401367, "training_acc": 53.75, "val_loss": 761.7461395263672, "val_acc": 50.0}
{"epoch": 1, "training_loss": 2371.6708812713623, "training_acc": 46.25, "val_loss": 25.57379961013794, "val_acc": 50.0}
{"epoch": 2, "training_loss": 86.99057197570801, "training_acc": 53.75, "val_loss": 14.05442476272583, "val_acc": 50.0}
{"epoch": 3, "training_loss": 56.870781898498535, "training_acc": 46.25, "val_loss": 14.084134101867676, "val_acc": 50.0}
{"epoch": 4, "training_loss": 55.37592315673828, "training_acc": 53.75, "val_loss": 14.776341915130615, "val_acc": 50.0}
{"epoch": 5, "training_loss": 61.82828330993652, "training_acc": 43.75, "val_loss": 13.956141471862793, "val_acc": 50.0}
{"epoch": 6, "training_loss": 55.3355598449707, "training_acc": 53.75, "val_loss": 13.85938048362732, "val_acc": 50.0}
{"epoch": 7, "training_loss": 55.426963806152344, "training_acc": 53.75, "val_loss": 13.858469724655151, "val_acc": 50.0}
{"epoch": 8, "training_loss": 55.37248611450195, "training_acc": 53.75, "val_loss": 13.860307931900024, "val_acc": 50.0}
{"epoch": 9, "training_loss": 55.356913566589355, "training_acc": 53.75, "val_loss": 13.877207040786743, "val_acc": 50.0}
{"epoch": 10, "training_loss": 55.255300521850586, "training_acc": 53.75, "val_loss": 13.947474956512451, "val_acc": 50.0}
{"epoch": 11, "training_loss": 55.32825469970703, "training_acc": 53.75, "val_loss": 14.124544858932495, "val_acc": 50.0}
{"epoch": 12, "training_loss": 55.68708610534668, "training_acc": 53.75, "val_loss": 14.129284620285034, "val_acc": 50.0}
{"epoch": 13, "training_loss": 55.84516429901123, "training_acc": 53.75, "val_loss": 14.092787504196167, "val_acc": 50.0}
{"epoch": 14, "training_loss": 55.7078742980957, "training_acc": 53.75, "val_loss": 14.258602857589722, "val_acc": 50.0}
{"epoch": 15, "training_loss": 55.950815200805664, "training_acc": 53.75, "val_loss": 14.334232807159424, "val_acc": 50.0}
{"epoch": 16, "training_loss": 56.18406867980957, "training_acc": 53.75, "val_loss": 14.07163381576538, "val_acc": 50.0}
{"epoch": 17, "training_loss": 55.413042068481445, "training_acc": 53.75, "val_loss": 13.937751054763794, "val_acc": 50.0}
{"epoch": 18, "training_loss": 55.3392448425293, "training_acc": 53.75, "val_loss": 13.888169527053833, "val_acc": 50.0}
{"epoch": 19, "training_loss": 55.26912212371826, "training_acc": 53.75, "val_loss": 13.873060941696167, "val_acc": 50.0}
{"epoch": 20, "training_loss": 55.25763130187988, "training_acc": 53.75, "val_loss": 13.861774206161499, "val_acc": 50.0}
{"epoch": 21, "training_loss": 55.44816017150879, "training_acc": 48.75, "val_loss": 13.861788511276245, "val_acc": 50.0}
{"epoch": 22, "training_loss": 55.44434928894043, "training_acc": 50.0, "val_loss": 13.863778114318848, "val_acc": 50.0}
{"epoch": 23, "training_loss": 55.3734188079834, "training_acc": 53.75, "val_loss": 13.878250122070312, "val_acc": 50.0}
{"epoch": 24, "training_loss": 55.28446006774902, "training_acc": 53.75, "val_loss": 13.89926552772522, "val_acc": 50.0}
{"epoch": 25, "training_loss": 55.24772644042969, "training_acc": 53.75, "val_loss": 13.91053557395935, "val_acc": 50.0}
{"epoch": 26, "training_loss": 55.23393630981445, "training_acc": 53.75, "val_loss": 13.914527893066406, "val_acc": 50.0}
{"epoch": 27, "training_loss": 55.243804931640625, "training_acc": 53.75, "val_loss": 13.94351840019226, "val_acc": 50.0}
{"epoch": 28, "training_loss": 55.32692337036133, "training_acc": 53.75, "val_loss": 13.957643508911133, "val_acc": 50.0}
{"epoch": 29, "training_loss": 55.25962257385254, "training_acc": 53.75, "val_loss": 13.923014402389526, "val_acc": 50.0}
{"epoch": 30, "training_loss": 55.233076095581055, "training_acc": 53.75, "val_loss": 13.913835287094116, "val_acc": 50.0}
{"epoch": 31, "training_loss": 55.2320499420166, "training_acc": 53.75, "val_loss": 13.910350799560547, "val_acc": 50.0}
{"epoch": 32, "training_loss": 55.2257719039917, "training_acc": 53.75, "val_loss": 13.903816938400269, "val_acc": 50.0}
{"epoch": 33, "training_loss": 55.24625778198242, "training_acc": 53.75, "val_loss": 13.901795148849487, "val_acc": 50.0}
{"epoch": 34, "training_loss": 55.23194694519043, "training_acc": 53.75, "val_loss": 13.913687467575073, "val_acc": 50.0}
{"epoch": 35, "training_loss": 55.2569580078125, "training_acc": 53.75, "val_loss": 13.926126956939697, "val_acc": 50.0}
{"epoch": 36, "training_loss": 55.22844696044922, "training_acc": 53.75, "val_loss": 13.922040462493896, "val_acc": 50.0}
{"epoch": 37, "training_loss": 55.22955799102783, "training_acc": 53.75, "val_loss": 13.908756971359253, "val_acc": 50.0}
{"epoch": 38, "training_loss": 55.23820400238037, "training_acc": 53.75, "val_loss": 13.893157243728638, "val_acc": 50.0}
{"epoch": 39, "training_loss": 55.248695373535156, "training_acc": 53.75, "val_loss": 13.88730764389038, "val_acc": 50.0}
{"epoch": 40, "training_loss": 55.25407600402832, "training_acc": 53.75, "val_loss": 13.886914253234863, "val_acc": 50.0}
{"epoch": 41, "training_loss": 55.266706466674805, "training_acc": 53.75, "val_loss": 13.886133432388306, "val_acc": 50.0}
