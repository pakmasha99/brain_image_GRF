"main_optuna_fix_3.py --pretrained_path None --mode finetuning --train_num 80 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_ABCD_fix_3 --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 416.0519027709961, "training_acc": 52.5, "val_loss": 6883126668165120.0, "val_acc": 45.0}
{"epoch": 1, "training_loss": 1.925745655403676e+16, "training_acc": 47.5, "val_loss": 948971680.0, "val_acc": 55.0}
{"epoch": 2, "training_loss": 3424129728.0, "training_acc": 52.5, "val_loss": 34584305.0, "val_acc": 45.0}
{"epoch": 3, "training_loss": 187556656.0, "training_acc": 47.5, "val_loss": 34746120.0, "val_acc": 45.0}
{"epoch": 4, "training_loss": 4416506432.0, "training_acc": 52.5, "val_loss": 2078815.78125, "val_acc": 55.0}
{"epoch": 5, "training_loss": 68791260.0, "training_acc": 50.0, "val_loss": 1651724000.0, "val_acc": 55.0}
{"epoch": 6, "training_loss": 7878661120.0, "training_acc": 50.0, "val_loss": 264939460.0, "val_acc": 45.0}
{"epoch": 7, "training_loss": 885272496.0, "training_acc": 47.5, "val_loss": 393996800.0, "val_acc": 45.0}
{"epoch": 8, "training_loss": 1337376680.0, "training_acc": 47.5, "val_loss": 104614640.0, "val_acc": 55.0}
{"epoch": 9, "training_loss": 344558676.375, "training_acc": 52.5, "val_loss": 136512450.0, "val_acc": 45.0}
{"epoch": 10, "training_loss": 425192376.0, "training_acc": 47.5, "val_loss": 16913243.75, "val_acc": 55.0}
{"epoch": 11, "training_loss": 80569704.0, "training_acc": 52.5, "val_loss": 23363867.5, "val_acc": 45.0}
{"epoch": 12, "training_loss": 213932800.0, "training_acc": 45.0, "val_loss": 67792035840.0, "val_acc": 55.0}
{"epoch": 13, "training_loss": 227284712736.0, "training_acc": 50.0, "val_loss": 17714718720.0, "val_acc": 45.0}
{"epoch": 14, "training_loss": 54265016512.0, "training_acc": 47.5, "val_loss": 291277820.0, "val_acc": 45.0}
{"epoch": 15, "training_loss": 1121039232.0, "training_acc": 47.5, "val_loss": 102662080.0, "val_acc": 55.0}
{"epoch": 16, "training_loss": 2748951360.0, "training_acc": 42.5, "val_loss": 95060070.0, "val_acc": 55.0}
{"epoch": 17, "training_loss": 375863792.0, "training_acc": 52.5, "val_loss": 1894703840.0, "val_acc": 45.0}
{"epoch": 18, "training_loss": 6041293784.0, "training_acc": 45.0, "val_loss": 184183220.0, "val_acc": 55.0}
{"epoch": 19, "training_loss": 604812388.0, "training_acc": 55.0, "val_loss": 13902216.25, "val_acc": 45.0}
{"epoch": 20, "training_loss": 469818560.0, "training_acc": 50.0, "val_loss": 1496433920.0, "val_acc": 45.0}
{"epoch": 21, "training_loss": 4879267712.0, "training_acc": 47.5, "val_loss": 175631900.0, "val_acc": 45.0}
{"epoch": 22, "training_loss": 540115202.0, "training_acc": 50.0, "val_loss": 300112220.0, "val_acc": 45.0}
{"epoch": 23, "training_loss": 2003974720.0, "training_acc": 47.5, "val_loss": 1419468160.0, "val_acc": 55.0}
{"epoch": 24, "training_loss": 5234797824.0, "training_acc": 52.5, "val_loss": 749536640.0, "val_acc": 45.0}
{"epoch": 25, "training_loss": 2461102208.0, "training_acc": 55.0, "val_loss": 54448010.0, "val_acc": 55.0}
{"epoch": 26, "training_loss": 234657744.0, "training_acc": 47.5, "val_loss": 32288127.5, "val_acc": 45.0}
{"epoch": 27, "training_loss": 119164900.0, "training_acc": 50.0, "val_loss": 2706928640.0, "val_acc": 45.0}
{"epoch": 28, "training_loss": 8709378240.0, "training_acc": 47.5, "val_loss": 24464839680.0, "val_acc": 55.0}
{"epoch": 29, "training_loss": 83416605520.0, "training_acc": 52.5, "val_loss": 77312920.0, "val_acc": 45.0}
{"epoch": 30, "training_loss": 258842354.0, "training_acc": 47.5, "val_loss": 621852.7734375, "val_acc": 45.0}
{"epoch": 31, "training_loss": 80399309.75, "training_acc": 43.75, "val_loss": 801237360.0, "val_acc": 45.0}
{"epoch": 32, "training_loss": 3327557376.0, "training_acc": 47.5, "val_loss": 59260047360.0, "val_acc": 55.0}
{"epoch": 33, "training_loss": 200597298688.0, "training_acc": 50.0, "val_loss": 166566290.0, "val_acc": 45.0}
{"epoch": 34, "training_loss": 545699592.0, "training_acc": 50.0, "val_loss": 21880212.5, "val_acc": 55.0}
{"epoch": 35, "training_loss": 97230652.0, "training_acc": 55.0, "val_loss": 45907640.0, "val_acc": 45.0}
{"epoch": 36, "training_loss": 149613154.0, "training_acc": 47.5, "val_loss": 40908367.5, "val_acc": 55.0}
{"epoch": 37, "training_loss": 399299824.0, "training_acc": 52.5, "val_loss": 10631338.75, "val_acc": 45.0}
{"epoch": 38, "training_loss": 31717850.671875, "training_acc": 50.0, "val_loss": 1990140.46875, "val_acc": 45.0}
{"epoch": 39, "training_loss": 6139410.8125, "training_acc": 50.0, "val_loss": 664568.984375, "val_acc": 55.0}
{"epoch": 40, "training_loss": 5477654.0, "training_acc": 52.5, "val_loss": 650192.578125, "val_acc": 55.0}
{"epoch": 41, "training_loss": 4189591.375, "training_acc": 52.5, "val_loss": 1937431.25, "val_acc": 45.0}
{"epoch": 42, "training_loss": 5587431.5625, "training_acc": 48.75, "val_loss": 1557719.0625, "val_acc": 55.0}
{"epoch": 43, "training_loss": 5292831.265625, "training_acc": 52.5, "val_loss": 17747566.25, "val_acc": 45.0}
{"epoch": 44, "training_loss": 59910871.125, "training_acc": 47.5, "val_loss": 5815559.375, "val_acc": 55.0}
{"epoch": 45, "training_loss": 23061000.0, "training_acc": 52.5, "val_loss": 152387.998046875, "val_acc": 45.0}
{"epoch": 46, "training_loss": 2914036.28125, "training_acc": 47.5, "val_loss": 2770313.4375, "val_acc": 55.0}
{"epoch": 47, "training_loss": 9590193.875, "training_acc": 50.0, "val_loss": 335610.390625, "val_acc": 55.0}
{"epoch": 48, "training_loss": 2599269.875, "training_acc": 47.5, "val_loss": 640773.6328125, "val_acc": 45.0}
{"epoch": 49, "training_loss": 3163627.375, "training_acc": 45.0, "val_loss": 1184095.234375, "val_acc": 55.0}
{"epoch": 50, "training_loss": 4358889.0, "training_acc": 52.5, "val_loss": 42937.4755859375, "val_acc": 45.0}
