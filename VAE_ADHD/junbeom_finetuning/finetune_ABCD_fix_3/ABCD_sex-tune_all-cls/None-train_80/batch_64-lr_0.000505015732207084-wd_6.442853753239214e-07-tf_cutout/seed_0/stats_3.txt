"main_optuna_fix_3.py --pretrained_path None --mode finetuning --train_num 80 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_ABCD_fix_3 --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 56.43961715698242, "training_acc": 52.5, "val_loss": 39.886348247528076, "val_acc": 45.0}
{"epoch": 1, "training_loss": 120.7653112411499, "training_acc": 55.0, "val_loss": 17.276777029037476, "val_acc": 55.0}
{"epoch": 2, "training_loss": 69.52907180786133, "training_acc": 52.5, "val_loss": 13.877090215682983, "val_acc": 55.0}
{"epoch": 3, "training_loss": 55.47769355773926, "training_acc": 47.5, "val_loss": 13.895583152770996, "val_acc": 55.0}
{"epoch": 4, "training_loss": 55.43247413635254, "training_acc": 52.5, "val_loss": 13.949887752532959, "val_acc": 55.0}
{"epoch": 5, "training_loss": 56.66714000701904, "training_acc": 52.5, "val_loss": 13.765671253204346, "val_acc": 55.0}
{"epoch": 6, "training_loss": 55.56033515930176, "training_acc": 50.0, "val_loss": 13.881129026412964, "val_acc": 55.0}
{"epoch": 7, "training_loss": 55.48990058898926, "training_acc": 47.5, "val_loss": 13.80061388015747, "val_acc": 55.0}
{"epoch": 8, "training_loss": 55.61757469177246, "training_acc": 52.5, "val_loss": 13.85424017906189, "val_acc": 55.0}
{"epoch": 9, "training_loss": 55.454708099365234, "training_acc": 52.5, "val_loss": 14.03420090675354, "val_acc": 55.0}
{"epoch": 10, "training_loss": 55.896278381347656, "training_acc": 47.5, "val_loss": 13.902592658996582, "val_acc": 55.0}
{"epoch": 11, "training_loss": 55.39058303833008, "training_acc": 52.5, "val_loss": 13.77214789390564, "val_acc": 55.0}
{"epoch": 12, "training_loss": 55.57432174682617, "training_acc": 52.5, "val_loss": 13.769837617874146, "val_acc": 55.0}
{"epoch": 13, "training_loss": 55.54183578491211, "training_acc": 52.5, "val_loss": 13.774147033691406, "val_acc": 55.0}
{"epoch": 14, "training_loss": 55.36054039001465, "training_acc": 52.5, "val_loss": 13.816498517990112, "val_acc": 55.0}
{"epoch": 15, "training_loss": 55.58734607696533, "training_acc": 45.0, "val_loss": 13.834284543991089, "val_acc": 55.0}
{"epoch": 16, "training_loss": 55.32211971282959, "training_acc": 52.5, "val_loss": 13.767156600952148, "val_acc": 55.0}
{"epoch": 17, "training_loss": 55.620079040527344, "training_acc": 52.5, "val_loss": 13.771361112594604, "val_acc": 55.0}
{"epoch": 18, "training_loss": 55.521358489990234, "training_acc": 52.5, "val_loss": 13.768306970596313, "val_acc": 55.0}
{"epoch": 19, "training_loss": 55.32250785827637, "training_acc": 52.5, "val_loss": 13.80943775177002, "val_acc": 55.0}
{"epoch": 20, "training_loss": 55.400394439697266, "training_acc": 52.5, "val_loss": 13.856368064880371, "val_acc": 55.0}
{"epoch": 21, "training_loss": 55.43884468078613, "training_acc": 52.5, "val_loss": 13.859665393829346, "val_acc": 55.0}
{"epoch": 22, "training_loss": 55.44464683532715, "training_acc": 52.5, "val_loss": 13.838309049606323, "val_acc": 55.0}
{"epoch": 23, "training_loss": 55.3845272064209, "training_acc": 52.5, "val_loss": 13.802531957626343, "val_acc": 55.0}
{"epoch": 24, "training_loss": 55.318299293518066, "training_acc": 52.5, "val_loss": 13.772735595703125, "val_acc": 55.0}
{"epoch": 25, "training_loss": 55.287763595581055, "training_acc": 52.5, "val_loss": 13.7699294090271, "val_acc": 55.0}
{"epoch": 26, "training_loss": 55.522582054138184, "training_acc": 52.5, "val_loss": 13.825699090957642, "val_acc": 55.0}
{"epoch": 27, "training_loss": 55.9912223815918, "training_acc": 52.5, "val_loss": 13.8306725025177, "val_acc": 55.0}
{"epoch": 28, "training_loss": 55.96414566040039, "training_acc": 52.5, "val_loss": 13.769205808639526, "val_acc": 55.0}
{"epoch": 29, "training_loss": 55.56016159057617, "training_acc": 52.5, "val_loss": 13.774963617324829, "val_acc": 55.0}
{"epoch": 30, "training_loss": 55.349348068237305, "training_acc": 52.5, "val_loss": 13.79830002784729, "val_acc": 55.0}
{"epoch": 31, "training_loss": 55.324716567993164, "training_acc": 52.5, "val_loss": 13.833352327346802, "val_acc": 55.0}
{"epoch": 32, "training_loss": 55.4007511138916, "training_acc": 52.5, "val_loss": 13.880774974822998, "val_acc": 55.0}
{"epoch": 33, "training_loss": 55.50605392456055, "training_acc": 47.5, "val_loss": 13.89032244682312, "val_acc": 55.0}
{"epoch": 34, "training_loss": 55.499295234680176, "training_acc": 47.5, "val_loss": 13.856971263885498, "val_acc": 55.0}
{"epoch": 35, "training_loss": 55.46645927429199, "training_acc": 52.5, "val_loss": 13.828188180923462, "val_acc": 55.0}
{"epoch": 36, "training_loss": 55.38130855560303, "training_acc": 52.5, "val_loss": 13.808631896972656, "val_acc": 55.0}
{"epoch": 37, "training_loss": 55.37174320220947, "training_acc": 52.5, "val_loss": 13.786510229110718, "val_acc": 55.0}
{"epoch": 38, "training_loss": 55.343584060668945, "training_acc": 52.5, "val_loss": 13.774889707565308, "val_acc": 55.0}
{"epoch": 39, "training_loss": 55.35234069824219, "training_acc": 52.5, "val_loss": 13.768086433410645, "val_acc": 55.0}
{"epoch": 40, "training_loss": 55.39564514160156, "training_acc": 52.5, "val_loss": 13.766549825668335, "val_acc": 55.0}
{"epoch": 41, "training_loss": 55.39560127258301, "training_acc": 52.5, "val_loss": 13.773709535598755, "val_acc": 55.0}
