"main_optuna_fix_3.py --pretrained_path None --mode finetuning --train_num 80 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_ABCD_fix_3 --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 110.76668548583984, "training_acc": 58.75, "val_loss": 2052050288640.0, "val_acc": 50.0}
{"epoch": 1, "training_loss": 6427027518976.0, "training_acc": 53.75, "val_loss": 3366134080.0, "val_acc": 50.0}
{"epoch": 2, "training_loss": 14388557312.0, "training_acc": 48.75, "val_loss": 58576865.0, "val_acc": 50.0}
{"epoch": 3, "training_loss": 246888932.0, "training_acc": 46.25, "val_loss": 1318409200.0, "val_acc": 50.0}
{"epoch": 4, "training_loss": 4551952892.5, "training_acc": 46.25, "val_loss": 978054.140625, "val_acc": 50.0}
{"epoch": 5, "training_loss": 24505838.5, "training_acc": 51.25, "val_loss": 10747081.25, "val_acc": 50.0}
{"epoch": 6, "training_loss": 334019794.0, "training_acc": 51.25, "val_loss": 18033080.0, "val_acc": 50.0}
{"epoch": 7, "training_loss": 71239904.0, "training_acc": 53.75, "val_loss": 11693748.75, "val_acc": 50.0}
{"epoch": 8, "training_loss": 49112006.0, "training_acc": 46.25, "val_loss": 1652625.625, "val_acc": 50.0}
{"epoch": 9, "training_loss": 6430572.3125, "training_acc": 46.25, "val_loss": 377548.0078125, "val_acc": 50.0}
{"epoch": 10, "training_loss": 1645486.625, "training_acc": 48.75, "val_loss": 146318.974609375, "val_acc": 50.0}
{"epoch": 11, "training_loss": 505255.71875, "training_acc": 51.25, "val_loss": 352487.5390625, "val_acc": 50.0}
{"epoch": 12, "training_loss": 243212555.5625, "training_acc": 51.25, "val_loss": 124307.333984375, "val_acc": 50.0}
{"epoch": 13, "training_loss": 2347397.15625, "training_acc": 53.75, "val_loss": 3666606.5625, "val_acc": 50.0}
{"epoch": 14, "training_loss": 12044949.125, "training_acc": 53.75, "val_loss": 5578149.375, "val_acc": 50.0}
{"epoch": 15, "training_loss": 19063072.0, "training_acc": 51.25, "val_loss": 1096605.390625, "val_acc": 50.0}
{"epoch": 16, "training_loss": 3486836.1015625, "training_acc": 53.75, "val_loss": 108534.560546875, "val_acc": 50.0}
{"epoch": 17, "training_loss": 12858637.1875, "training_acc": 43.75, "val_loss": 1171682.890625, "val_acc": 50.0}
{"epoch": 18, "training_loss": 3884289.1875, "training_acc": 53.75, "val_loss": 20194853.75, "val_acc": 50.0}
{"epoch": 19, "training_loss": 75556290.25, "training_acc": 46.25, "val_loss": 1137778.984375, "val_acc": 50.0}
{"epoch": 20, "training_loss": 3950099.25, "training_acc": 53.75, "val_loss": 407732.6171875, "val_acc": 50.0}
{"epoch": 21, "training_loss": 1310060.37890625, "training_acc": 53.75, "val_loss": 189404.70703125, "val_acc": 50.0}
{"epoch": 22, "training_loss": 704560.59375, "training_acc": 46.25, "val_loss": 159625.87890625, "val_acc": 50.0}
{"epoch": 23, "training_loss": 541176.1875, "training_acc": 53.75, "val_loss": 70314.3505859375, "val_acc": 50.0}
{"epoch": 24, "training_loss": 214044.47985839844, "training_acc": 53.75, "val_loss": 94431.591796875, "val_acc": 50.0}
{"epoch": 25, "training_loss": 360688.2734375, "training_acc": 46.25, "val_loss": 50900.6787109375, "val_acc": 50.0}
{"epoch": 26, "training_loss": 176762.31640625, "training_acc": 53.75, "val_loss": 47138.9599609375, "val_acc": 50.0}
{"epoch": 27, "training_loss": 161583.60815429688, "training_acc": 46.25, "val_loss": 164903.37890625, "val_acc": 50.0}
{"epoch": 28, "training_loss": 541634.84375, "training_acc": 53.75, "val_loss": 64501.5673828125, "val_acc": 50.0}
{"epoch": 29, "training_loss": 191592.48376464844, "training_acc": 53.75, "val_loss": 12590.6640625, "val_acc": 50.0}
{"epoch": 30, "training_loss": 50410.80078125, "training_acc": 51.25, "val_loss": 10463.114013671875, "val_acc": 50.0}
