"main_optuna_fix_3.py --pretrained_path None --mode finetuning --train_num 80 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_ABCD_fix_3 --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 328.97143936157227, "training_acc": 52.5, "val_loss": 15173943040.0, "val_acc": 55.0}
{"epoch": 1, "training_loss": 54950790873.875, "training_acc": 45.0, "val_loss": 8109161.875, "val_acc": 45.0}
{"epoch": 2, "training_loss": 24609126.375, "training_acc": 47.5, "val_loss": 17909.979248046875, "val_acc": 45.0}
{"epoch": 3, "training_loss": 57618.695556640625, "training_acc": 47.5, "val_loss": 5492.54150390625, "val_acc": 55.0}
{"epoch": 4, "training_loss": 25019.3251953125, "training_acc": 47.5, "val_loss": 8292.001953125, "val_acc": 55.0}
{"epoch": 5, "training_loss": 31375.17578125, "training_acc": 52.5, "val_loss": 1552.3086547851562, "val_acc": 55.0}
{"epoch": 6, "training_loss": 6514.562744140625, "training_acc": 50.0, "val_loss": 15128.426513671875, "val_acc": 55.0}
{"epoch": 7, "training_loss": 50941.462890625, "training_acc": 52.5, "val_loss": 4685.4522705078125, "val_acc": 45.0}
{"epoch": 8, "training_loss": 17549.053466796875, "training_acc": 47.5, "val_loss": 2953.6456298828125, "val_acc": 45.0}
{"epoch": 9, "training_loss": 9521.475463867188, "training_acc": 47.5, "val_loss": 1231.8706512451172, "val_acc": 55.0}
{"epoch": 10, "training_loss": 31685.248046875, "training_acc": 47.5, "val_loss": 7789.962158203125, "val_acc": 45.0}
{"epoch": 11, "training_loss": 60331.66796875, "training_acc": 52.5, "val_loss": 24811.85302734375, "val_acc": 55.0}
{"epoch": 12, "training_loss": 81370.47900390625, "training_acc": 52.5, "val_loss": 10169.915161132812, "val_acc": 45.0}
{"epoch": 13, "training_loss": 159395.125, "training_acc": 50.0, "val_loss": 131116.533203125, "val_acc": 45.0}
{"epoch": 14, "training_loss": 468499.59765625, "training_acc": 47.5, "val_loss": 441.79840087890625, "val_acc": 55.0}
{"epoch": 15, "training_loss": 26149.8271484375, "training_acc": 45.0, "val_loss": 40554.34326171875, "val_acc": 45.0}
{"epoch": 16, "training_loss": 113075.5439453125, "training_acc": 57.5, "val_loss": 13295.84228515625, "val_acc": 45.0}
{"epoch": 17, "training_loss": 54846.759765625, "training_acc": 45.0, "val_loss": 7956.6302490234375, "val_acc": 55.0}
{"epoch": 18, "training_loss": 24996.46337890625, "training_acc": 55.0, "val_loss": 1289.5809936523438, "val_acc": 55.0}
{"epoch": 19, "training_loss": 4218.375244140625, "training_acc": 52.5, "val_loss": 5071.356506347656, "val_acc": 45.0}
{"epoch": 20, "training_loss": 17593.94189453125, "training_acc": 47.5, "val_loss": 1701.04736328125, "val_acc": 45.0}
{"epoch": 21, "training_loss": 5373.717346191406, "training_acc": 47.5, "val_loss": 831.7731475830078, "val_acc": 55.0}
{"epoch": 22, "training_loss": 3616.050048828125, "training_acc": 52.5, "val_loss": 729.0480041503906, "val_acc": 55.0}
{"epoch": 23, "training_loss": 2564.3390502929688, "training_acc": 52.5, "val_loss": 615.1657104492188, "val_acc": 45.0}
{"epoch": 24, "training_loss": 2082.8446655273438, "training_acc": 47.5, "val_loss": 56.94167137145996, "val_acc": 45.0}
{"epoch": 25, "training_loss": 719.7638244628906, "training_acc": 53.75, "val_loss": 991.8202209472656, "val_acc": 55.0}
{"epoch": 26, "training_loss": 3635.658935546875, "training_acc": 52.5, "val_loss": 699.7245025634766, "val_acc": 45.0}
{"epoch": 27, "training_loss": 2561.5364685058594, "training_acc": 47.5, "val_loss": 200.21419525146484, "val_acc": 45.0}
{"epoch": 28, "training_loss": 846.7320556640625, "training_acc": 42.5, "val_loss": 235.71407318115234, "val_acc": 55.0}
{"epoch": 29, "training_loss": 920.6035614013672, "training_acc": 52.5, "val_loss": 50.85549831390381, "val_acc": 55.0}
{"epoch": 30, "training_loss": 279.10533905029297, "training_acc": 52.5, "val_loss": 142.13873863220215, "val_acc": 45.0}
{"epoch": 31, "training_loss": 464.00134658813477, "training_acc": 48.75, "val_loss": 15.339874029159546, "val_acc": 55.0}
{"epoch": 32, "training_loss": 136.3638153076172, "training_acc": 52.5, "val_loss": 28.40510606765747, "val_acc": 45.0}
{"epoch": 33, "training_loss": 158.06396484375, "training_acc": 50.0, "val_loss": 107.72406578063965, "val_acc": 55.0}
{"epoch": 34, "training_loss": 404.0364456176758, "training_acc": 52.5, "val_loss": 31.32930040359497, "val_acc": 45.0}
{"epoch": 35, "training_loss": 135.93874740600586, "training_acc": 47.5, "val_loss": 43.54358673095703, "val_acc": 45.0}
{"epoch": 36, "training_loss": 137.0807590484619, "training_acc": 53.75, "val_loss": 36.175599098205566, "val_acc": 55.0}
{"epoch": 37, "training_loss": 155.3347625732422, "training_acc": 52.5, "val_loss": 27.71315574645996, "val_acc": 55.0}
{"epoch": 38, "training_loss": 102.82204246520996, "training_acc": 50.0, "val_loss": 26.324081420898438, "val_acc": 45.0}
{"epoch": 39, "training_loss": 97.89824485778809, "training_acc": 47.5, "val_loss": 14.105062484741211, "val_acc": 55.0}
{"epoch": 40, "training_loss": 64.06208038330078, "training_acc": 46.25, "val_loss": 19.774779081344604, "val_acc": 55.0}
{"epoch": 41, "training_loss": 76.20359230041504, "training_acc": 52.5, "val_loss": 16.013599634170532, "val_acc": 45.0}
{"epoch": 42, "training_loss": 66.0003776550293, "training_acc": 47.5, "val_loss": 18.514397144317627, "val_acc": 45.0}
{"epoch": 43, "training_loss": 68.13332176208496, "training_acc": 48.75, "val_loss": 13.980566263198853, "val_acc": 55.0}
{"epoch": 44, "training_loss": 59.72500419616699, "training_acc": 52.5, "val_loss": 15.342092514038086, "val_acc": 55.0}
{"epoch": 45, "training_loss": 61.82899761199951, "training_acc": 52.5, "val_loss": 14.017081260681152, "val_acc": 55.0}
{"epoch": 46, "training_loss": 57.635016441345215, "training_acc": 43.75, "val_loss": 16.63784384727478, "val_acc": 45.0}
{"epoch": 47, "training_loss": 63.76465606689453, "training_acc": 47.5, "val_loss": 13.879245519638062, "val_acc": 55.0}
{"epoch": 48, "training_loss": 54.996864318847656, "training_acc": 53.75, "val_loss": 14.997246265411377, "val_acc": 55.0}
{"epoch": 49, "training_loss": 62.41116237640381, "training_acc": 52.5, "val_loss": 14.605292081832886, "val_acc": 55.0}
{"epoch": 50, "training_loss": 58.798312187194824, "training_acc": 52.5, "val_loss": 14.416135549545288, "val_acc": 55.0}
{"epoch": 51, "training_loss": 58.6391716003418, "training_acc": 47.5, "val_loss": 16.22493028640747, "val_acc": 45.0}
{"epoch": 52, "training_loss": 62.07426452636719, "training_acc": 47.5, "val_loss": 13.723006248474121, "val_acc": 55.0}
