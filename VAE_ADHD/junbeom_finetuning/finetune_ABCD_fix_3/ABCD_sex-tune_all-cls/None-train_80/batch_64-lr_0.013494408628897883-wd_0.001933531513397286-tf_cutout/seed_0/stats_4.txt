"main_optuna_fix_3.py --pretrained_path None --mode finetuning --train_num 80 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_ABCD_fix_3 --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 90.63122177124023, "training_acc": 52.5, "val_loss": 1.0321703201459405e+18, "val_acc": 45.0}
{"epoch": 1, "training_loss": 2.840889556545582e+18, "training_acc": 47.5, "val_loss": 30441.9677734375, "val_acc": 45.0}
{"epoch": 2, "training_loss": 1656291.21875, "training_acc": 55.0, "val_loss": 417468.8671875, "val_acc": 55.0}
{"epoch": 3, "training_loss": 1341501.3671875, "training_acc": 52.5, "val_loss": 299708.80859375, "val_acc": 45.0}
{"epoch": 4, "training_loss": 900331.078125, "training_acc": 50.0, "val_loss": 34410.37109375, "val_acc": 55.0}
{"epoch": 5, "training_loss": 115946.45581054688, "training_acc": 52.5, "val_loss": 2296.0009765625, "val_acc": 55.0}
{"epoch": 6, "training_loss": 10333.928466796875, "training_acc": 52.5, "val_loss": 221670.60546875, "val_acc": 45.0}
{"epoch": 7, "training_loss": 2244450.6875, "training_acc": 47.5, "val_loss": 576629.8046875, "val_acc": 55.0}
{"epoch": 8, "training_loss": 1803133.09375, "training_acc": 57.5, "val_loss": 311408.18359375, "val_acc": 55.0}
{"epoch": 9, "training_loss": 1333556.234375, "training_acc": 52.5, "val_loss": 173310.21484375, "val_acc": 45.0}
{"epoch": 10, "training_loss": 553386.00390625, "training_acc": 50.0, "val_loss": 2819.2770385742188, "val_acc": 45.0}
{"epoch": 11, "training_loss": 87798.93115234375, "training_acc": 55.0, "val_loss": 22708.671875, "val_acc": 55.0}
{"epoch": 12, "training_loss": 81374.2177734375, "training_acc": 52.5, "val_loss": 10924.576416015625, "val_acc": 55.0}
{"epoch": 13, "training_loss": 49290.7099609375, "training_acc": 50.0, "val_loss": 700205.546875, "val_acc": 55.0}
{"epoch": 14, "training_loss": 6101282.0, "training_acc": 57.5, "val_loss": 34085.71533203125, "val_acc": 55.0}
{"epoch": 15, "training_loss": 153009.203125, "training_acc": 52.5, "val_loss": 37909.609375, "val_acc": 55.0}
{"epoch": 16, "training_loss": 149204.171875, "training_acc": 52.5, "val_loss": 39481.16943359375, "val_acc": 55.0}
{"epoch": 17, "training_loss": 152246.29296875, "training_acc": 52.5, "val_loss": 19427.562255859375, "val_acc": 55.0}
{"epoch": 18, "training_loss": 106439.734375, "training_acc": 55.0, "val_loss": 9695.662231445312, "val_acc": 55.0}
{"epoch": 19, "training_loss": 39582.4033203125, "training_acc": 50.0, "val_loss": 546334.375, "val_acc": 45.0}
{"epoch": 20, "training_loss": 1666724.5400390625, "training_acc": 47.5, "val_loss": 35561.8798828125, "val_acc": 55.0}
{"epoch": 21, "training_loss": 142866.03125, "training_acc": 52.5, "val_loss": 15227.1826171875, "val_acc": 55.0}
{"epoch": 22, "training_loss": 70990.4140625, "training_acc": 52.5, "val_loss": 14014.510498046875, "val_acc": 45.0}
{"epoch": 23, "training_loss": 43342.925048828125, "training_acc": 47.5, "val_loss": 8120.1629638671875, "val_acc": 55.0}
{"epoch": 24, "training_loss": 32582.2392578125, "training_acc": 52.5, "val_loss": 4699.712829589844, "val_acc": 55.0}
{"epoch": 25, "training_loss": 18381.685180664062, "training_acc": 52.5, "val_loss": 2000.587158203125, "val_acc": 45.0}
{"epoch": 26, "training_loss": 28542.4677734375, "training_acc": 50.0, "val_loss": 179.15119171142578, "val_acc": 45.0}
{"epoch": 27, "training_loss": 7548.386291503906, "training_acc": 48.75, "val_loss": 2521.220245361328, "val_acc": 45.0}
{"epoch": 28, "training_loss": 14093.0498046875, "training_acc": 47.5, "val_loss": 293.4944534301758, "val_acc": 55.0}
{"epoch": 29, "training_loss": 4802.01806640625, "training_acc": 43.75, "val_loss": 4951.71630859375, "val_acc": 55.0}
{"epoch": 30, "training_loss": 18116.369873046875, "training_acc": 52.5, "val_loss": 1269.7370910644531, "val_acc": 55.0}
{"epoch": 31, "training_loss": 5424.307861328125, "training_acc": 55.0, "val_loss": 1123.4234619140625, "val_acc": 55.0}
{"epoch": 32, "training_loss": 7018.560546875, "training_acc": 47.5, "val_loss": 861.0451507568359, "val_acc": 55.0}
{"epoch": 33, "training_loss": 3698.525146484375, "training_acc": 52.5, "val_loss": 2809.2147827148438, "val_acc": 55.0}
{"epoch": 34, "training_loss": 10356.822998046875, "training_acc": 52.5, "val_loss": 6365.7244873046875, "val_acc": 45.0}
{"epoch": 35, "training_loss": 23477.84521484375, "training_acc": 47.5, "val_loss": 698.2276916503906, "val_acc": 55.0}
{"epoch": 36, "training_loss": 2982.355712890625, "training_acc": 47.5, "val_loss": 3056.6415405273438, "val_acc": 55.0}
{"epoch": 37, "training_loss": 12493.130126953125, "training_acc": 52.5, "val_loss": 340.78399658203125, "val_acc": 55.0}
{"epoch": 38, "training_loss": 6400.920166015625, "training_acc": 50.0, "val_loss": 5448.8037109375, "val_acc": 45.0}
{"epoch": 39, "training_loss": 17927.67724609375, "training_acc": 47.5, "val_loss": 1054.0728759765625, "val_acc": 55.0}
{"epoch": 40, "training_loss": 5169.07421875, "training_acc": 52.5, "val_loss": 1594.6577453613281, "val_acc": 55.0}
{"epoch": 41, "training_loss": 5464.828125, "training_acc": 51.25, "val_loss": 2459.550018310547, "val_acc": 45.0}
{"epoch": 42, "training_loss": 9818.455322265625, "training_acc": 47.5, "val_loss": 2607.457275390625, "val_acc": 45.0}
{"epoch": 43, "training_loss": 8444.819885253906, "training_acc": 47.5, "val_loss": 879.3750762939453, "val_acc": 55.0}
{"epoch": 44, "training_loss": 4495.742919921875, "training_acc": 52.5, "val_loss": 1596.553955078125, "val_acc": 55.0}
{"epoch": 45, "training_loss": 6052.352294921875, "training_acc": 52.5, "val_loss": 62.17241287231445, "val_acc": 55.0}
{"epoch": 46, "training_loss": 1078.6117553710938, "training_acc": 52.5, "val_loss": 1653.6984252929688, "val_acc": 45.0}
