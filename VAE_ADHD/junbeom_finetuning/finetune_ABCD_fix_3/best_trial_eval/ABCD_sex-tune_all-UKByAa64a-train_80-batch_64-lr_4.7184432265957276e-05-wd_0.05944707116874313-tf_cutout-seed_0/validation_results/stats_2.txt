"main_optuna_fix_3.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 80 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_ABCD_fix_3 --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 55.88638496398926, "training_acc": 50.0, "val_loss": 13.76180648803711, "val_acc": 55.0}
{"epoch": 1, "training_loss": 56.426527976989746, "training_acc": 45.0, "val_loss": 13.711782693862915, "val_acc": 55.0}
{"epoch": 2, "training_loss": 55.31635570526123, "training_acc": 52.5, "val_loss": 13.77754807472229, "val_acc": 55.0}
{"epoch": 3, "training_loss": 55.94592094421387, "training_acc": 55.0, "val_loss": 13.682721853256226, "val_acc": 55.0}
{"epoch": 4, "training_loss": 55.35513114929199, "training_acc": 52.5, "val_loss": 13.760738372802734, "val_acc": 55.0}
{"epoch": 5, "training_loss": 55.3886833190918, "training_acc": 53.75, "val_loss": 13.770314455032349, "val_acc": 55.0}
{"epoch": 6, "training_loss": 55.78133773803711, "training_acc": 51.25, "val_loss": 13.903272151947021, "val_acc": 55.0}
{"epoch": 7, "training_loss": 56.57469177246094, "training_acc": 52.5, "val_loss": 14.324487447738647, "val_acc": 55.0}
{"epoch": 8, "training_loss": 57.6718635559082, "training_acc": 52.5, "val_loss": 13.730839490890503, "val_acc": 55.0}
{"epoch": 9, "training_loss": 55.180009841918945, "training_acc": 52.5, "val_loss": 13.863130807876587, "val_acc": 55.0}
{"epoch": 10, "training_loss": 55.634742736816406, "training_acc": 47.5, "val_loss": 14.000245332717896, "val_acc": 55.0}
{"epoch": 11, "training_loss": 55.735107421875, "training_acc": 47.5, "val_loss": 13.876338005065918, "val_acc": 55.0}
{"epoch": 12, "training_loss": 55.521424293518066, "training_acc": 47.5, "val_loss": 13.778715133666992, "val_acc": 55.0}
{"epoch": 13, "training_loss": 55.347970962524414, "training_acc": 52.5, "val_loss": 13.771919012069702, "val_acc": 55.0}
{"epoch": 14, "training_loss": 55.44061279296875, "training_acc": 52.5, "val_loss": 13.776386976242065, "val_acc": 55.0}
{"epoch": 15, "training_loss": 55.49740219116211, "training_acc": 52.5, "val_loss": 13.772960901260376, "val_acc": 55.0}
{"epoch": 16, "training_loss": 55.34185028076172, "training_acc": 52.5, "val_loss": 13.757553100585938, "val_acc": 55.0}
{"epoch": 17, "training_loss": 55.60952568054199, "training_acc": 52.5, "val_loss": 13.76712679862976, "val_acc": 55.0}
{"epoch": 18, "training_loss": 55.48098564147949, "training_acc": 52.5, "val_loss": 13.74988079071045, "val_acc": 55.0}
{"epoch": 19, "training_loss": 55.28098392486572, "training_acc": 52.5, "val_loss": 13.767820596694946, "val_acc": 55.0}
{"epoch": 20, "training_loss": 55.31380844116211, "training_acc": 52.5, "val_loss": 13.834228515625, "val_acc": 55.0}
{"epoch": 21, "training_loss": 55.35769081115723, "training_acc": 51.25, "val_loss": 13.85114073753357, "val_acc": 55.0}
{"epoch": 22, "training_loss": 55.33863639831543, "training_acc": 50.0, "val_loss": 13.80577802658081, "val_acc": 55.0}
{"epoch": 23, "training_loss": 55.1878776550293, "training_acc": 56.25, "val_loss": 13.753036260604858, "val_acc": 55.0}
{"epoch": 24, "training_loss": 55.17306327819824, "training_acc": 52.5, "val_loss": 13.735363483428955, "val_acc": 55.0}
{"epoch": 25, "training_loss": 55.28947830200195, "training_acc": 52.5, "val_loss": 13.763682842254639, "val_acc": 55.0}
{"epoch": 26, "training_loss": 55.571656227111816, "training_acc": 52.5, "val_loss": 13.803246021270752, "val_acc": 55.0}
{"epoch": 27, "training_loss": 55.813157081604004, "training_acc": 52.5, "val_loss": 13.825609683990479, "val_acc": 55.0}
{"epoch": 28, "training_loss": 55.96564865112305, "training_acc": 52.5, "val_loss": 13.765846490859985, "val_acc": 55.0}
{"epoch": 29, "training_loss": 55.599422454833984, "training_acc": 52.5, "val_loss": 13.74280571937561, "val_acc": 55.0}
{"epoch": 30, "training_loss": 55.176971435546875, "training_acc": 52.5, "val_loss": 13.792972564697266, "val_acc": 55.0}
{"epoch": 31, "training_loss": 55.27311134338379, "training_acc": 52.5, "val_loss": 13.865350484848022, "val_acc": 55.0}
{"epoch": 32, "training_loss": 55.23887825012207, "training_acc": 47.5, "val_loss": 13.958603143692017, "val_acc": 55.0}
{"epoch": 33, "training_loss": 55.697317123413086, "training_acc": 47.5, "val_loss": 13.990329504013062, "val_acc": 55.0}
{"epoch": 34, "training_loss": 55.76146984100342, "training_acc": 47.5, "val_loss": 13.916008472442627, "val_acc": 55.0}
{"epoch": 35, "training_loss": 55.42898178100586, "training_acc": 47.5, "val_loss": 13.831771612167358, "val_acc": 55.0}
{"epoch": 36, "training_loss": 55.18486213684082, "training_acc": 67.5, "val_loss": 13.786824941635132, "val_acc": 55.0}
{"epoch": 37, "training_loss": 55.27657604217529, "training_acc": 52.5, "val_loss": 13.76767635345459, "val_acc": 55.0}
{"epoch": 38, "training_loss": 55.13542461395264, "training_acc": 52.5, "val_loss": 13.755978345870972, "val_acc": 55.0}
{"epoch": 39, "training_loss": 55.08553504943848, "training_acc": 52.5, "val_loss": 13.742198944091797, "val_acc": 55.0}
{"epoch": 40, "training_loss": 55.21229648590088, "training_acc": 52.5, "val_loss": 13.736884593963623, "val_acc": 55.0}
{"epoch": 41, "training_loss": 55.02456188201904, "training_acc": 52.5, "val_loss": 13.75685453414917, "val_acc": 55.0}
{"epoch": 42, "training_loss": 55.22887992858887, "training_acc": 51.25, "val_loss": 13.817967176437378, "val_acc": 55.0}
{"epoch": 43, "training_loss": 55.18135356903076, "training_acc": 66.25, "val_loss": 13.827544450759888, "val_acc": 55.0}
{"epoch": 44, "training_loss": 55.19736862182617, "training_acc": 65.0, "val_loss": 13.813886642456055, "val_acc": 55.0}
