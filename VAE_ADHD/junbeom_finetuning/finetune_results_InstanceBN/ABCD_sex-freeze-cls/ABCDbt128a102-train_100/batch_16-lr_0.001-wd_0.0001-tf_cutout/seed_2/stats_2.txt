"main_modify.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 2 --task ABCD_sex --input_option BT_org --learning_rate 1e-3 --batch_size 16 --weight_decay 1e-4 --BN inst --save_path finetune_results_InstanceBN --run_where lab"
{"epoch": 0, "training_loss": 0.7405751895904541, "training_acc": 45.0, "val_loss": 0.6986666917800903, "val_acc": 52.0}
{"epoch": 1, "training_loss": 0.7242939186096191, "training_acc": 53.0, "val_loss": 0.7228358578681946, "val_acc": 52.0}
{"epoch": 2, "training_loss": 0.6957393550872802, "training_acc": 53.0, "val_loss": 0.6990311026573182, "val_acc": 48.0}
{"epoch": 3, "training_loss": 0.7066986179351806, "training_acc": 47.0, "val_loss": 0.6951183819770813, "val_acc": 48.0}
{"epoch": 4, "training_loss": 0.6997871017456054, "training_acc": 53.0, "val_loss": 0.7078945589065552, "val_acc": 52.0}
{"epoch": 5, "training_loss": 0.6966144299507141, "training_acc": 53.0, "val_loss": 0.6939902877807618, "val_acc": 40.0}
{"epoch": 6, "training_loss": 0.7232378721237183, "training_acc": 47.0, "val_loss": 0.6985669326782227, "val_acc": 48.0}
{"epoch": 7, "training_loss": 0.7155832743644714, "training_acc": 47.0, "val_loss": 0.7175591373443604, "val_acc": 52.0}
{"epoch": 8, "training_loss": 0.7066245603561402, "training_acc": 51.0, "val_loss": 0.6971856307983398, "val_acc": 48.0}
{"epoch": 9, "training_loss": 0.6977974200248718, "training_acc": 46.0, "val_loss": 0.6934161329269409, "val_acc": 56.0}
{"epoch": 10, "training_loss": 0.6940346002578736, "training_acc": 54.0, "val_loss": 0.6929871749877929, "val_acc": 52.0}
{"epoch": 11, "training_loss": 0.6956287336349487, "training_acc": 41.0, "val_loss": 0.6929419350624084, "val_acc": 52.0}
{"epoch": 12, "training_loss": 0.6921377420425415, "training_acc": 53.0, "val_loss": 0.6997461485862732, "val_acc": 52.0}
{"epoch": 13, "training_loss": 0.6862004089355469, "training_acc": 53.0, "val_loss": 0.6949298048019409, "val_acc": 48.0}
{"epoch": 14, "training_loss": 0.7018670272827149, "training_acc": 47.0, "val_loss": 0.7015859818458557, "val_acc": 48.0}
{"epoch": 15, "training_loss": 0.6915450787544251, "training_acc": 55.0, "val_loss": 0.6950232028961182, "val_acc": 52.0}
{"epoch": 16, "training_loss": 0.691748743057251, "training_acc": 53.0, "val_loss": 0.6930999732017518, "val_acc": 52.0}
{"epoch": 17, "training_loss": 0.6885718679428101, "training_acc": 53.0, "val_loss": 0.6943041586875915, "val_acc": 52.0}
{"epoch": 18, "training_loss": 0.692173376083374, "training_acc": 47.0, "val_loss": 0.6945719742774963, "val_acc": 48.0}
{"epoch": 19, "training_loss": 0.6987611198425293, "training_acc": 48.0, "val_loss": 0.694227147102356, "val_acc": 52.0}
{"epoch": 20, "training_loss": 0.694539008140564, "training_acc": 43.0, "val_loss": 0.6987345242500305, "val_acc": 48.0}
{"epoch": 21, "training_loss": 0.6947518754005432, "training_acc": 48.0, "val_loss": 0.6927606248855591, "val_acc": 52.0}
{"epoch": 22, "training_loss": 0.6935387420654296, "training_acc": 53.0, "val_loss": 0.7070538020133972, "val_acc": 52.0}
{"epoch": 23, "training_loss": 0.6957092213630677, "training_acc": 53.0, "val_loss": 0.6927460408210755, "val_acc": 52.0}
{"epoch": 24, "training_loss": 0.6862509989738464, "training_acc": 53.0, "val_loss": 0.6967496228218079, "val_acc": 52.0}
{"epoch": 25, "training_loss": 0.6979008626937866, "training_acc": 57.0, "val_loss": 0.6927196907997132, "val_acc": 52.0}
{"epoch": 26, "training_loss": 0.7017138934135437, "training_acc": 53.0, "val_loss": 0.6961696553230285, "val_acc": 52.0}
{"epoch": 27, "training_loss": 0.6836477923393249, "training_acc": 52.0, "val_loss": 0.6949861645698547, "val_acc": 48.0}
{"epoch": 28, "training_loss": 0.6932449913024903, "training_acc": 49.0, "val_loss": 0.6933698415756225, "val_acc": 52.0}
{"epoch": 29, "training_loss": 0.6964810276031494, "training_acc": 51.0, "val_loss": 0.6931302380561829, "val_acc": 56.0}
{"epoch": 30, "training_loss": 0.692049388885498, "training_acc": 50.0, "val_loss": 0.6941587543487548, "val_acc": 52.0}
{"epoch": 31, "training_loss": 0.684316029548645, "training_acc": 52.0, "val_loss": 0.7008316254615784, "val_acc": 52.0}
{"epoch": 32, "training_loss": 0.6895163607597351, "training_acc": 53.0, "val_loss": 0.6947004866600036, "val_acc": 52.0}
{"epoch": 33, "training_loss": 0.6886282539367676, "training_acc": 53.0, "val_loss": 0.693417820930481, "val_acc": 52.0}
{"epoch": 34, "training_loss": 0.6932793283462524, "training_acc": 49.0, "val_loss": 0.6939067435264588, "val_acc": 48.0}
{"epoch": 35, "training_loss": 0.6864504051208496, "training_acc": 52.0, "val_loss": 0.697709538936615, "val_acc": 52.0}
{"epoch": 36, "training_loss": 0.6891897201538086, "training_acc": 53.0, "val_loss": 0.6927692437171936, "val_acc": 52.0}
{"epoch": 37, "training_loss": 0.6918010234832763, "training_acc": 51.0, "val_loss": 0.6953407502174378, "val_acc": 48.0}
{"epoch": 38, "training_loss": 0.6933267569541931, "training_acc": 47.0, "val_loss": 0.694127504825592, "val_acc": 52.0}
{"epoch": 39, "training_loss": 0.6851282882690429, "training_acc": 53.0, "val_loss": 0.6949595379829406, "val_acc": 52.0}
{"epoch": 40, "training_loss": 0.6859609341621399, "training_acc": 53.0, "val_loss": 0.6965929889678955, "val_acc": 52.0}
{"epoch": 41, "training_loss": 0.6922880697250366, "training_acc": 53.0, "val_loss": 0.6932048344612122, "val_acc": 56.0}
{"epoch": 42, "training_loss": 0.6798599648475647, "training_acc": 62.0, "val_loss": 0.6968360614776611, "val_acc": 52.0}
{"epoch": 43, "training_loss": 0.6915039229393005, "training_acc": 53.0, "val_loss": 0.7010584139823913, "val_acc": 52.0}
{"epoch": 44, "training_loss": 0.6909398365020752, "training_acc": 59.0, "val_loss": 0.69406254529953, "val_acc": 44.0}
