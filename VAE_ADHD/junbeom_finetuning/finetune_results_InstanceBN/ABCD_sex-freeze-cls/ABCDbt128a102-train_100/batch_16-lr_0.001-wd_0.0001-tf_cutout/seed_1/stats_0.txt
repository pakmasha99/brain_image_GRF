"main_modify.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 1 --task ABCD_sex --input_option BT_org --learning_rate 1e-3 --batch_size 16 --weight_decay 1e-4 --BN inst --save_path finetune_results_InstanceBN --run_where lab"
{"epoch": 0, "training_loss": 0.7076517820358277, "training_acc": 47.0, "val_loss": 0.7106386613845825, "val_acc": 52.0}
{"epoch": 1, "training_loss": 0.7310205078125, "training_acc": 53.0, "val_loss": 0.7053260231018066, "val_acc": 52.0}
{"epoch": 2, "training_loss": 0.7009310054779053, "training_acc": 53.0, "val_loss": 0.6908815884590149, "val_acc": 52.0}
{"epoch": 3, "training_loss": 0.7033919954299926, "training_acc": 41.0, "val_loss": 0.691168053150177, "val_acc": 68.0}
{"epoch": 4, "training_loss": 0.6932764482498169, "training_acc": 48.0, "val_loss": 0.6921885776519775, "val_acc": 52.0}
{"epoch": 5, "training_loss": 0.7098130583763123, "training_acc": 53.0, "val_loss": 0.7096020555496216, "val_acc": 52.0}
{"epoch": 6, "training_loss": 0.7034673547744751, "training_acc": 53.0, "val_loss": 0.6905018210411071, "val_acc": 52.0}
{"epoch": 7, "training_loss": 0.7056497311592103, "training_acc": 48.0, "val_loss": 0.6981501770019531, "val_acc": 48.0}
{"epoch": 8, "training_loss": 0.6941471147537231, "training_acc": 51.0, "val_loss": 0.6934378623962403, "val_acc": 52.0}
{"epoch": 9, "training_loss": 0.7044798898696899, "training_acc": 53.0, "val_loss": 0.6960535573959351, "val_acc": 52.0}
{"epoch": 10, "training_loss": 0.703212366104126, "training_acc": 45.0, "val_loss": 0.6944947600364685, "val_acc": 48.0}
{"epoch": 11, "training_loss": 0.6934092879295349, "training_acc": 52.0, "val_loss": 0.6916463994979858, "val_acc": 52.0}
{"epoch": 12, "training_loss": 0.6977514362335205, "training_acc": 53.0, "val_loss": 0.7025878691673278, "val_acc": 52.0}
{"epoch": 13, "training_loss": 0.6920666241645813, "training_acc": 53.0, "val_loss": 0.6909863448143005, "val_acc": 64.0}
{"epoch": 14, "training_loss": 0.7126093888282776, "training_acc": 48.0, "val_loss": 0.7029964423179627, "val_acc": 48.0}
{"epoch": 15, "training_loss": 0.7237865352630615, "training_acc": 39.0, "val_loss": 0.6982479786872864, "val_acc": 52.0}
{"epoch": 16, "training_loss": 0.6920232343673706, "training_acc": 53.0, "val_loss": 0.690469970703125, "val_acc": 52.0}
{"epoch": 17, "training_loss": 0.6907201409339905, "training_acc": 53.0, "val_loss": 0.69048996925354, "val_acc": 52.0}
{"epoch": 18, "training_loss": 0.6926852416992187, "training_acc": 47.0, "val_loss": 0.6913243913650513, "val_acc": 56.0}
{"epoch": 19, "training_loss": 0.68732759475708, "training_acc": 56.0, "val_loss": 0.6955785131454468, "val_acc": 52.0}
{"epoch": 20, "training_loss": 0.7010939717292786, "training_acc": 53.0, "val_loss": 0.6924882030487061, "val_acc": 52.0}
{"epoch": 21, "training_loss": 0.6933304190635681, "training_acc": 53.0, "val_loss": 0.6920228171348571, "val_acc": 52.0}
{"epoch": 22, "training_loss": 0.696861252784729, "training_acc": 53.0, "val_loss": 0.6956304860115051, "val_acc": 52.0}
{"epoch": 23, "training_loss": 0.6906383752822876, "training_acc": 54.0, "val_loss": 0.6987353587150573, "val_acc": 48.0}
{"epoch": 24, "training_loss": 0.70696617603302, "training_acc": 47.0, "val_loss": 0.6906116700172424, "val_acc": 52.0}
{"epoch": 25, "training_loss": 0.6888835668563843, "training_acc": 53.0, "val_loss": 0.7221249341964722, "val_acc": 52.0}
{"epoch": 26, "training_loss": 0.7058370018005371, "training_acc": 53.0, "val_loss": 0.6908374905586243, "val_acc": 52.0}
{"epoch": 27, "training_loss": 0.694387276172638, "training_acc": 53.0, "val_loss": 0.6904388761520386, "val_acc": 52.0}
{"epoch": 28, "training_loss": 0.7144080018997192, "training_acc": 43.0, "val_loss": 0.7007573914527893, "val_acc": 48.0}
{"epoch": 29, "training_loss": 0.711604745388031, "training_acc": 45.0, "val_loss": 0.7060614442825317, "val_acc": 52.0}
{"epoch": 30, "training_loss": 0.6938695812225342, "training_acc": 53.0, "val_loss": 0.6906660962104797, "val_acc": 52.0}
{"epoch": 31, "training_loss": 0.6880732989311218, "training_acc": 61.0, "val_loss": 0.6941228485107422, "val_acc": 48.0}
{"epoch": 32, "training_loss": 0.694167742729187, "training_acc": 51.0, "val_loss": 0.6905935907363892, "val_acc": 52.0}
{"epoch": 33, "training_loss": 0.6972544622421265, "training_acc": 53.0, "val_loss": 0.7020985889434814, "val_acc": 52.0}
{"epoch": 34, "training_loss": 0.7019313287734985, "training_acc": 53.0, "val_loss": 0.6973884296417237, "val_acc": 52.0}
{"epoch": 35, "training_loss": 0.6943428421020508, "training_acc": 50.0, "val_loss": 0.6985363936424256, "val_acc": 48.0}
{"epoch": 36, "training_loss": 0.7016787219047547, "training_acc": 47.0, "val_loss": 0.692936327457428, "val_acc": 48.0}
{"epoch": 37, "training_loss": 0.6916308617591858, "training_acc": 45.0, "val_loss": 0.6908791208267212, "val_acc": 56.0}
{"epoch": 38, "training_loss": 0.6881278157234192, "training_acc": 56.0, "val_loss": 0.6937197542190552, "val_acc": 52.0}
{"epoch": 39, "training_loss": 0.6878743767738342, "training_acc": 53.0, "val_loss": 0.6911303997039795, "val_acc": 52.0}
{"epoch": 40, "training_loss": 0.6878491973876953, "training_acc": 56.0, "val_loss": 0.6916857266426086, "val_acc": 48.0}
{"epoch": 41, "training_loss": 0.6897279238700866, "training_acc": 53.0, "val_loss": 0.6907769203186035, "val_acc": 56.0}
{"epoch": 42, "training_loss": 0.6863565921783448, "training_acc": 54.0, "val_loss": 0.6937001800537109, "val_acc": 52.0}
{"epoch": 43, "training_loss": 0.6927021670341492, "training_acc": 52.0, "val_loss": 0.6905746507644653, "val_acc": 52.0}
{"epoch": 44, "training_loss": 0.6850981092453003, "training_acc": 53.0, "val_loss": 0.6922423171997071, "val_acc": 52.0}
{"epoch": 45, "training_loss": 0.6905414056777954, "training_acc": 53.0, "val_loss": 0.6957226586341858, "val_acc": 52.0}
{"epoch": 46, "training_loss": 0.7004312419891358, "training_acc": 50.0, "val_loss": 0.7022878646850585, "val_acc": 48.0}
