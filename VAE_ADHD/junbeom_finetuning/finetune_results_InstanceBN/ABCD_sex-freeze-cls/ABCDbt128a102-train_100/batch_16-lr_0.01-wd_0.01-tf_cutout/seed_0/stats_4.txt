"main_modify.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --learning_rate 1e-2 --batch_size 16 --weight_decay 1e-2 --BN inst --save_path finetune_results_InstanceBN --run_where lab"
{"epoch": 0, "training_loss": 0.9970499229431152, "training_acc": 59.0, "val_loss": 0.6950983595848084, "val_acc": 52.0}
{"epoch": 1, "training_loss": 0.7494384622573853, "training_acc": 55.0, "val_loss": 0.8671798753738403, "val_acc": 52.0}
{"epoch": 2, "training_loss": 0.7476418590545655, "training_acc": 53.0, "val_loss": 0.7207437205314636, "val_acc": 52.0}
{"epoch": 3, "training_loss": 1.01303297996521, "training_acc": 51.0, "val_loss": 1.1483200263977051, "val_acc": 48.0}
{"epoch": 4, "training_loss": 1.000866756439209, "training_acc": 41.0, "val_loss": 0.8151762509346008, "val_acc": 52.0}
{"epoch": 5, "training_loss": 0.8186421346664429, "training_acc": 43.0, "val_loss": 0.771496434211731, "val_acc": 52.0}
{"epoch": 6, "training_loss": 0.7937999439239501, "training_acc": 53.0, "val_loss": 0.8801626825332641, "val_acc": 48.0}
{"epoch": 7, "training_loss": 0.8165758132934571, "training_acc": 49.0, "val_loss": 0.8211586856842041, "val_acc": 52.0}
{"epoch": 8, "training_loss": 0.7348344135284424, "training_acc": 56.0, "val_loss": 0.7235942530632019, "val_acc": 48.0}
{"epoch": 9, "training_loss": 0.7606191229820252, "training_acc": 57.0, "val_loss": 0.7355594754219055, "val_acc": 48.0}
{"epoch": 10, "training_loss": 0.7902738809585571, "training_acc": 49.0, "val_loss": 0.8174507212638855, "val_acc": 52.0}
{"epoch": 11, "training_loss": 0.7689604377746582, "training_acc": 47.0, "val_loss": 0.6964479160308837, "val_acc": 48.0}
{"epoch": 12, "training_loss": 0.7525276184082031, "training_acc": 43.0, "val_loss": 0.7182340097427368, "val_acc": 48.0}
{"epoch": 13, "training_loss": 0.7044315671920777, "training_acc": 49.0, "val_loss": 0.6958197474479675, "val_acc": 48.0}
{"epoch": 14, "training_loss": 0.7854633045196533, "training_acc": 41.0, "val_loss": 0.7589928579330444, "val_acc": 52.0}
{"epoch": 15, "training_loss": 0.7811499881744385, "training_acc": 41.0, "val_loss": 0.7350631737709046, "val_acc": 52.0}
{"epoch": 16, "training_loss": 0.718701786994934, "training_acc": 51.0, "val_loss": 0.6960417246818542, "val_acc": 48.0}
{"epoch": 17, "training_loss": 0.6915168309211731, "training_acc": 51.0, "val_loss": 0.6943075299263001, "val_acc": 48.0}
{"epoch": 18, "training_loss": 0.6940635013580322, "training_acc": 48.0, "val_loss": 0.7059889125823975, "val_acc": 52.0}
{"epoch": 19, "training_loss": 0.6981139659881592, "training_acc": 51.0, "val_loss": 0.6940210342407227, "val_acc": 48.0}
{"epoch": 20, "training_loss": 0.6806193304061889, "training_acc": 55.0, "val_loss": 0.7539522695541382, "val_acc": 52.0}
{"epoch": 21, "training_loss": 0.7488353204727173, "training_acc": 53.0, "val_loss": 0.8690573787689209, "val_acc": 48.0}
{"epoch": 22, "training_loss": 0.7750888013839722, "training_acc": 51.0, "val_loss": 0.6937269711494446, "val_acc": 48.0}
{"epoch": 23, "training_loss": 0.7756600189208984, "training_acc": 49.0, "val_loss": 0.8456045246124267, "val_acc": 52.0}
{"epoch": 24, "training_loss": 0.7292709708213806, "training_acc": 57.0, "val_loss": 0.7331812691688537, "val_acc": 48.0}
{"epoch": 25, "training_loss": 0.7169807052612305, "training_acc": 53.0, "val_loss": 0.6867521405220032, "val_acc": 52.0}
{"epoch": 26, "training_loss": 0.6758256196975708, "training_acc": 55.0, "val_loss": 0.6866123747825622, "val_acc": 52.0}
{"epoch": 27, "training_loss": 0.6820071983337402, "training_acc": 55.0, "val_loss": 0.6876509356498718, "val_acc": 64.0}
{"epoch": 28, "training_loss": 0.6929320573806763, "training_acc": 57.0, "val_loss": 0.7550718426704407, "val_acc": 52.0}
{"epoch": 29, "training_loss": 0.7485142636299134, "training_acc": 45.0, "val_loss": 0.7006234788894653, "val_acc": 52.0}
{"epoch": 30, "training_loss": 0.6969573509693145, "training_acc": 50.0, "val_loss": 0.951174623966217, "val_acc": 52.0}
{"epoch": 31, "training_loss": 0.8208655214309692, "training_acc": 53.0, "val_loss": 0.8450347900390625, "val_acc": 48.0}
{"epoch": 32, "training_loss": 0.892230920791626, "training_acc": 45.0, "val_loss": 0.6924341058731079, "val_acc": 48.0}
{"epoch": 33, "training_loss": 0.7022421765327453, "training_acc": 45.0, "val_loss": 0.708228211402893, "val_acc": 48.0}
{"epoch": 34, "training_loss": 0.7038319754600525, "training_acc": 50.0, "val_loss": 0.6922058153152466, "val_acc": 52.0}
{"epoch": 35, "training_loss": 0.6811717104911804, "training_acc": 51.0, "val_loss": 0.7381886315345764, "val_acc": 52.0}
{"epoch": 36, "training_loss": 0.6848706674575805, "training_acc": 57.0, "val_loss": 0.8342589807510375, "val_acc": 48.0}
{"epoch": 37, "training_loss": 0.7987121105194092, "training_acc": 54.0, "val_loss": 0.8372381520271301, "val_acc": 52.0}
{"epoch": 38, "training_loss": 0.835713198184967, "training_acc": 46.0, "val_loss": 0.8455142903327942, "val_acc": 52.0}
{"epoch": 39, "training_loss": 0.8317869591712952, "training_acc": 49.0, "val_loss": 0.7801384472846985, "val_acc": 48.0}
{"epoch": 40, "training_loss": 0.7431162285804749, "training_acc": 56.0, "val_loss": 0.6892780661582947, "val_acc": 48.0}
{"epoch": 41, "training_loss": 0.671060152053833, "training_acc": 57.0, "val_loss": 0.7502703046798707, "val_acc": 52.0}
{"epoch": 42, "training_loss": 0.6911836814880371, "training_acc": 59.0, "val_loss": 0.6923528361320496, "val_acc": 52.0}
{"epoch": 43, "training_loss": 0.8280888319015502, "training_acc": 53.0, "val_loss": 1.1402207231521606, "val_acc": 48.0}
{"epoch": 44, "training_loss": 0.9079476547241211, "training_acc": 55.0, "val_loss": 0.799758038520813, "val_acc": 52.0}
{"epoch": 45, "training_loss": 0.7923790073394775, "training_acc": 51.0, "val_loss": 0.7330770921707154, "val_acc": 52.0}
