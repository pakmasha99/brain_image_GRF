"main_modify.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 1 --task ABCD_sex --input_option BT_org --learning_rate 1e-6 --batch_size 16 --weight_decay 1e-2 --BN inst --save_path finetune_results_InstanceBN --run_where lab"
{"epoch": 0, "training_loss": 0.6923522663116455, "training_acc": 53.0, "val_loss": 0.6905989789962769, "val_acc": 52.0}
{"epoch": 1, "training_loss": 0.69249516248703, "training_acc": 53.0, "val_loss": 0.690597620010376, "val_acc": 52.0}
{"epoch": 2, "training_loss": 0.6922402095794677, "training_acc": 53.0, "val_loss": 0.6905963945388794, "val_acc": 52.0}
{"epoch": 3, "training_loss": 0.6923623728752136, "training_acc": 53.0, "val_loss": 0.6905957341194153, "val_acc": 52.0}
{"epoch": 4, "training_loss": 0.692612977027893, "training_acc": 53.0, "val_loss": 0.6905954003334045, "val_acc": 52.0}
{"epoch": 5, "training_loss": 0.6927076768875122, "training_acc": 53.0, "val_loss": 0.6905944681167603, "val_acc": 52.0}
{"epoch": 6, "training_loss": 0.6920441937446594, "training_acc": 53.0, "val_loss": 0.6905937170982361, "val_acc": 52.0}
{"epoch": 7, "training_loss": 0.6923329162597657, "training_acc": 53.0, "val_loss": 0.6905934882164001, "val_acc": 52.0}
{"epoch": 8, "training_loss": 0.6927584385871888, "training_acc": 53.0, "val_loss": 0.6905932545661926, "val_acc": 52.0}
{"epoch": 9, "training_loss": 0.6924755620956421, "training_acc": 53.0, "val_loss": 0.6905929470062255, "val_acc": 52.0}
{"epoch": 10, "training_loss": 0.69217538356781, "training_acc": 53.0, "val_loss": 0.6905929017066955, "val_acc": 52.0}
{"epoch": 11, "training_loss": 0.6923684787750244, "training_acc": 53.0, "val_loss": 0.6905927729606628, "val_acc": 52.0}
{"epoch": 12, "training_loss": 0.6924961709976196, "training_acc": 53.0, "val_loss": 0.6905923867225647, "val_acc": 52.0}
{"epoch": 13, "training_loss": 0.6925893354415894, "training_acc": 53.0, "val_loss": 0.6905921411514282, "val_acc": 52.0}
{"epoch": 14, "training_loss": 0.6925709795951843, "training_acc": 53.0, "val_loss": 0.6905922436714172, "val_acc": 52.0}
{"epoch": 15, "training_loss": 0.6924448823928833, "training_acc": 53.0, "val_loss": 0.690592041015625, "val_acc": 52.0}
{"epoch": 16, "training_loss": 0.6920086336135864, "training_acc": 53.0, "val_loss": 0.6905920314788818, "val_acc": 52.0}
{"epoch": 17, "training_loss": 0.6921179056167602, "training_acc": 53.0, "val_loss": 0.6905918860435486, "val_acc": 52.0}
{"epoch": 18, "training_loss": 0.6926940631866455, "training_acc": 53.0, "val_loss": 0.6905918908119202, "val_acc": 52.0}
{"epoch": 19, "training_loss": 0.6923409700393677, "training_acc": 53.0, "val_loss": 0.6905917310714722, "val_acc": 52.0}
{"epoch": 20, "training_loss": 0.6922169542312622, "training_acc": 53.0, "val_loss": 0.6905916333198547, "val_acc": 52.0}
{"epoch": 21, "training_loss": 0.6919525218009949, "training_acc": 53.0, "val_loss": 0.6905914998054504, "val_acc": 52.0}
{"epoch": 22, "training_loss": 0.6923683953285217, "training_acc": 53.0, "val_loss": 0.6905914306640625, "val_acc": 52.0}
{"epoch": 23, "training_loss": 0.6922142553329468, "training_acc": 53.0, "val_loss": 0.6905914235115052, "val_acc": 52.0}
{"epoch": 24, "training_loss": 0.6925249934196472, "training_acc": 53.0, "val_loss": 0.6905914735794068, "val_acc": 52.0}
{"epoch": 25, "training_loss": 0.6923375463485718, "training_acc": 53.0, "val_loss": 0.6905915141105652, "val_acc": 52.0}
{"epoch": 26, "training_loss": 0.6922903823852539, "training_acc": 53.0, "val_loss": 0.6905916094779968, "val_acc": 52.0}
{"epoch": 27, "training_loss": 0.6925328969955444, "training_acc": 53.0, "val_loss": 0.6905918478965759, "val_acc": 52.0}
{"epoch": 28, "training_loss": 0.6927527403831482, "training_acc": 53.0, "val_loss": 0.6905917358398438, "val_acc": 52.0}
{"epoch": 29, "training_loss": 0.6922691822052002, "training_acc": 53.0, "val_loss": 0.6905918574333191, "val_acc": 52.0}
{"epoch": 30, "training_loss": 0.6921074676513672, "training_acc": 53.0, "val_loss": 0.6905919790267945, "val_acc": 52.0}
{"epoch": 31, "training_loss": 0.692040855884552, "training_acc": 53.0, "val_loss": 0.6905920529365539, "val_acc": 52.0}
{"epoch": 32, "training_loss": 0.6918547773361206, "training_acc": 53.0, "val_loss": 0.6905921292304993, "val_acc": 52.0}
{"epoch": 33, "training_loss": 0.6919174718856812, "training_acc": 53.0, "val_loss": 0.6905923461914063, "val_acc": 52.0}
{"epoch": 34, "training_loss": 0.6923549056053162, "training_acc": 53.0, "val_loss": 0.6905930590629578, "val_acc": 52.0}
{"epoch": 35, "training_loss": 0.6918871092796326, "training_acc": 53.0, "val_loss": 0.6905933356285096, "val_acc": 52.0}
{"epoch": 36, "training_loss": 0.6926759481430054, "training_acc": 53.0, "val_loss": 0.6905933046340942, "val_acc": 52.0}
{"epoch": 37, "training_loss": 0.6919858551025391, "training_acc": 53.0, "val_loss": 0.6905929231643677, "val_acc": 52.0}
{"epoch": 38, "training_loss": 0.6922921919822693, "training_acc": 53.0, "val_loss": 0.6905928039550782, "val_acc": 52.0}
{"epoch": 39, "training_loss": 0.6924382543563843, "training_acc": 53.0, "val_loss": 0.6905928659439087, "val_acc": 52.0}
{"epoch": 40, "training_loss": 0.6922296285629272, "training_acc": 53.0, "val_loss": 0.690592930316925, "val_acc": 52.0}
{"epoch": 41, "training_loss": 0.6926075077056885, "training_acc": 53.0, "val_loss": 0.690592839717865, "val_acc": 52.0}
{"epoch": 42, "training_loss": 0.692497751712799, "training_acc": 53.0, "val_loss": 0.6905929636955261, "val_acc": 52.0}
