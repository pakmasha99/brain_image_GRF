"main_modify.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 1 --task ABCD_sex --input_option BT_org --learning_rate 1e-6 --batch_size 16 --weight_decay 1e-2 --BN inst --save_path finetune_results_InstanceBN --run_where lab"
{"epoch": 0, "training_loss": 0.6929145574569702, "training_acc": 52.0, "val_loss": 0.6910426664352417, "val_acc": 56.0}
{"epoch": 1, "training_loss": 0.6932193470001221, "training_acc": 52.0, "val_loss": 0.691015613079071, "val_acc": 56.0}
{"epoch": 2, "training_loss": 0.6934168243408203, "training_acc": 52.0, "val_loss": 0.6910083365440368, "val_acc": 56.0}
{"epoch": 3, "training_loss": 0.6938080072402955, "training_acc": 52.0, "val_loss": 0.6909984064102173, "val_acc": 56.0}
{"epoch": 4, "training_loss": 0.6939293932914734, "training_acc": 52.0, "val_loss": 0.6909942030906677, "val_acc": 56.0}
{"epoch": 5, "training_loss": 0.6933618402481079, "training_acc": 52.0, "val_loss": 0.6909733653068543, "val_acc": 56.0}
{"epoch": 6, "training_loss": 0.6934565162658691, "training_acc": 52.0, "val_loss": 0.690979380607605, "val_acc": 56.0}
{"epoch": 7, "training_loss": 0.6934672927856446, "training_acc": 52.0, "val_loss": 0.6909827160835266, "val_acc": 56.0}
{"epoch": 8, "training_loss": 0.693430814743042, "training_acc": 52.0, "val_loss": 0.6910115885734558, "val_acc": 56.0}
{"epoch": 9, "training_loss": 0.6932647466659546, "training_acc": 52.0, "val_loss": 0.6910188603401184, "val_acc": 56.0}
{"epoch": 10, "training_loss": 0.693239164352417, "training_acc": 52.0, "val_loss": 0.6910207486152649, "val_acc": 56.0}
{"epoch": 11, "training_loss": 0.6933780097961426, "training_acc": 52.0, "val_loss": 0.6910087704658509, "val_acc": 56.0}
{"epoch": 12, "training_loss": 0.6933909440040589, "training_acc": 52.0, "val_loss": 0.6909869527816772, "val_acc": 56.0}
{"epoch": 13, "training_loss": 0.6929689693450928, "training_acc": 52.0, "val_loss": 0.6909685325622559, "val_acc": 56.0}
{"epoch": 14, "training_loss": 0.6935930466651916, "training_acc": 52.0, "val_loss": 0.6909467768669129, "val_acc": 56.0}
{"epoch": 15, "training_loss": 0.6932280731201171, "training_acc": 52.0, "val_loss": 0.6909442615509033, "val_acc": 56.0}
{"epoch": 16, "training_loss": 0.6936427021026611, "training_acc": 52.0, "val_loss": 0.6909474492073059, "val_acc": 56.0}
{"epoch": 17, "training_loss": 0.6938557338714599, "training_acc": 52.0, "val_loss": 0.6909329080581665, "val_acc": 56.0}
{"epoch": 18, "training_loss": 0.6935509252548218, "training_acc": 52.0, "val_loss": 0.6909253716468811, "val_acc": 56.0}
{"epoch": 19, "training_loss": 0.6932452726364136, "training_acc": 52.0, "val_loss": 0.6909292221069336, "val_acc": 56.0}
{"epoch": 20, "training_loss": 0.6930306744575501, "training_acc": 52.0, "val_loss": 0.6909174036979675, "val_acc": 56.0}
{"epoch": 21, "training_loss": 0.6936842131614686, "training_acc": 52.0, "val_loss": 0.6909164094924927, "val_acc": 56.0}
{"epoch": 22, "training_loss": 0.6931714963912964, "training_acc": 52.0, "val_loss": 0.6909185028076172, "val_acc": 56.0}
{"epoch": 23, "training_loss": 0.6931698131561279, "training_acc": 52.0, "val_loss": 0.6908990406990051, "val_acc": 56.0}
{"epoch": 24, "training_loss": 0.693419246673584, "training_acc": 52.0, "val_loss": 0.6908859014511108, "val_acc": 56.0}
{"epoch": 25, "training_loss": 0.6934800314903259, "training_acc": 52.0, "val_loss": 0.6908854246139526, "val_acc": 56.0}
{"epoch": 26, "training_loss": 0.6934227132797242, "training_acc": 52.0, "val_loss": 0.690895962715149, "val_acc": 56.0}
{"epoch": 27, "training_loss": 0.6931768345832825, "training_acc": 52.0, "val_loss": 0.6909205365180969, "val_acc": 56.0}
{"epoch": 28, "training_loss": 0.6934137582778931, "training_acc": 52.0, "val_loss": 0.6909203052520752, "val_acc": 56.0}
{"epoch": 29, "training_loss": 0.6934114170074462, "training_acc": 52.0, "val_loss": 0.6909077000617981, "val_acc": 56.0}
{"epoch": 30, "training_loss": 0.6930540084838868, "training_acc": 52.0, "val_loss": 0.6908918905258179, "val_acc": 56.0}
{"epoch": 31, "training_loss": 0.6932218003273011, "training_acc": 52.0, "val_loss": 0.6909140348434448, "val_acc": 56.0}
{"epoch": 32, "training_loss": 0.69336017370224, "training_acc": 52.0, "val_loss": 0.690904495716095, "val_acc": 56.0}
{"epoch": 33, "training_loss": 0.6930333709716797, "training_acc": 52.0, "val_loss": 0.6909030628204346, "val_acc": 56.0}
{"epoch": 34, "training_loss": 0.6937891149520874, "training_acc": 52.0, "val_loss": 0.6909259486198426, "val_acc": 56.0}
{"epoch": 35, "training_loss": 0.693192355632782, "training_acc": 52.0, "val_loss": 0.6909328079223633, "val_acc": 56.0}
{"epoch": 36, "training_loss": 0.693132004737854, "training_acc": 52.0, "val_loss": 0.6909474492073059, "val_acc": 56.0}
{"epoch": 37, "training_loss": 0.6932303810119629, "training_acc": 52.0, "val_loss": 0.690937659740448, "val_acc": 56.0}
{"epoch": 38, "training_loss": 0.6931491041183472, "training_acc": 52.0, "val_loss": 0.6909351086616516, "val_acc": 56.0}
{"epoch": 39, "training_loss": 0.6932620859146118, "training_acc": 52.0, "val_loss": 0.6909275364875793, "val_acc": 56.0}
{"epoch": 40, "training_loss": 0.6937038683891297, "training_acc": 52.0, "val_loss": 0.690913245677948, "val_acc": 56.0}
{"epoch": 41, "training_loss": 0.6937577533721924, "training_acc": 52.0, "val_loss": 0.6909002614021301, "val_acc": 56.0}
{"epoch": 42, "training_loss": 0.6934995889663697, "training_acc": 52.0, "val_loss": 0.6908985042572021, "val_acc": 56.0}
{"epoch": 43, "training_loss": 0.6929168748855591, "training_acc": 52.0, "val_loss": 0.6908738255500794, "val_acc": 56.0}
{"epoch": 44, "training_loss": 0.6932370758056641, "training_acc": 52.0, "val_loss": 0.690871467590332, "val_acc": 56.0}
{"epoch": 45, "training_loss": 0.693013162612915, "training_acc": 52.0, "val_loss": 0.6908799815177917, "val_acc": 56.0}
{"epoch": 46, "training_loss": 0.6935569858551025, "training_acc": 52.0, "val_loss": 0.6908959174156188, "val_acc": 56.0}
{"epoch": 47, "training_loss": 0.6934296417236329, "training_acc": 52.0, "val_loss": 0.6909151005744935, "val_acc": 56.0}
{"epoch": 48, "training_loss": 0.6932240080833435, "training_acc": 52.0, "val_loss": 0.6909452366828919, "val_acc": 56.0}
{"epoch": 49, "training_loss": 0.6934351587295532, "training_acc": 52.0, "val_loss": 0.6909564185142517, "val_acc": 56.0}
{"epoch": 50, "training_loss": 0.6936171913146972, "training_acc": 52.0, "val_loss": 0.6909627628326416, "val_acc": 56.0}
{"epoch": 51, "training_loss": 0.6934772944450378, "training_acc": 52.0, "val_loss": 0.690967857837677, "val_acc": 56.0}
{"epoch": 52, "training_loss": 0.6932581567764282, "training_acc": 52.0, "val_loss": 0.6909684801101684, "val_acc": 56.0}
{"epoch": 53, "training_loss": 0.6933422422409058, "training_acc": 52.0, "val_loss": 0.690961377620697, "val_acc": 56.0}
{"epoch": 54, "training_loss": 0.6932892084121705, "training_acc": 52.0, "val_loss": 0.6909376406669616, "val_acc": 56.0}
{"epoch": 55, "training_loss": 0.6934537410736084, "training_acc": 52.0, "val_loss": 0.6909122443199158, "val_acc": 56.0}
{"epoch": 56, "training_loss": 0.6931594586372376, "training_acc": 52.0, "val_loss": 0.6908973336219788, "val_acc": 56.0}
{"epoch": 57, "training_loss": 0.6933600425720214, "training_acc": 52.0, "val_loss": 0.6908964133262634, "val_acc": 56.0}
{"epoch": 58, "training_loss": 0.6930060648918152, "training_acc": 52.0, "val_loss": 0.6909007883071899, "val_acc": 56.0}
{"epoch": 59, "training_loss": 0.6937160730361939, "training_acc": 52.0, "val_loss": 0.6908935117721557, "val_acc": 56.0}
{"epoch": 60, "training_loss": 0.6931100702285766, "training_acc": 52.0, "val_loss": 0.690885157585144, "val_acc": 56.0}
{"epoch": 61, "training_loss": 0.6932758235931397, "training_acc": 52.0, "val_loss": 0.6908820128440857, "val_acc": 56.0}
{"epoch": 62, "training_loss": 0.6932818746566772, "training_acc": 52.0, "val_loss": 0.6908686995506287, "val_acc": 56.0}
{"epoch": 63, "training_loss": 0.6932371997833252, "training_acc": 52.0, "val_loss": 0.6908508586883545, "val_acc": 56.0}
{"epoch": 64, "training_loss": 0.693518123626709, "training_acc": 52.0, "val_loss": 0.6908439660072326, "val_acc": 56.0}
{"epoch": 65, "training_loss": 0.6931408929824829, "training_acc": 52.0, "val_loss": 0.6908656358718872, "val_acc": 56.0}
{"epoch": 66, "training_loss": 0.6932529020309448, "training_acc": 52.0, "val_loss": 0.6908722233772278, "val_acc": 56.0}
{"epoch": 67, "training_loss": 0.692872748374939, "training_acc": 52.0, "val_loss": 0.690888249874115, "val_acc": 56.0}
{"epoch": 68, "training_loss": 0.6936457061767578, "training_acc": 52.0, "val_loss": 0.6909003496170044, "val_acc": 56.0}
{"epoch": 69, "training_loss": 0.6933910584449768, "training_acc": 52.0, "val_loss": 0.6909298038482666, "val_acc": 56.0}
{"epoch": 70, "training_loss": 0.6933888816833496, "training_acc": 52.0, "val_loss": 0.6909146428108215, "val_acc": 56.0}
{"epoch": 71, "training_loss": 0.6933892011642456, "training_acc": 52.0, "val_loss": 0.6909308528900147, "val_acc": 56.0}
{"epoch": 72, "training_loss": 0.693355736732483, "training_acc": 52.0, "val_loss": 0.6909237003326416, "val_acc": 56.0}
{"epoch": 73, "training_loss": 0.6929682731628418, "training_acc": 52.0, "val_loss": 0.6909404993057251, "val_acc": 56.0}
{"epoch": 74, "training_loss": 0.6934683585166931, "training_acc": 52.0, "val_loss": 0.6909539031982422, "val_acc": 56.0}
{"epoch": 75, "training_loss": 0.6929211831092834, "training_acc": 52.0, "val_loss": 0.6909351801872253, "val_acc": 56.0}
{"epoch": 76, "training_loss": 0.6933525276184082, "training_acc": 52.0, "val_loss": 0.6909384846687316, "val_acc": 56.0}
{"epoch": 77, "training_loss": 0.693339262008667, "training_acc": 52.0, "val_loss": 0.6909157299995422, "val_acc": 56.0}
{"epoch": 78, "training_loss": 0.6927498030662537, "training_acc": 52.0, "val_loss": 0.6909016299247742, "val_acc": 56.0}
{"epoch": 79, "training_loss": 0.6932063555717468, "training_acc": 52.0, "val_loss": 0.6908992624282837, "val_acc": 56.0}
{"epoch": 80, "training_loss": 0.6935895538330078, "training_acc": 52.0, "val_loss": 0.6909091234207153, "val_acc": 56.0}
{"epoch": 81, "training_loss": 0.693356077671051, "training_acc": 52.0, "val_loss": 0.6909097218513489, "val_acc": 56.0}
{"epoch": 82, "training_loss": 0.6931883907318115, "training_acc": 52.0, "val_loss": 0.6908853673934936, "val_acc": 56.0}
{"epoch": 83, "training_loss": 0.6935515809059143, "training_acc": 52.0, "val_loss": 0.690857183933258, "val_acc": 56.0}
