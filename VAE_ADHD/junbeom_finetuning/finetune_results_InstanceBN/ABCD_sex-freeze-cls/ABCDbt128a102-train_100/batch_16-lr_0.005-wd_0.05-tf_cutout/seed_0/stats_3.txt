"main_modify.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --learning_rate 5e-3 --batch_size 16 --weight_decay 5e-2 --BN inst --save_path finetune_results_InstanceBN --run_where lab"
{"epoch": 0, "training_loss": 0.8902972984313965, "training_acc": 47.0, "val_loss": 0.6982192182540894, "val_acc": 48.0}
{"epoch": 1, "training_loss": 0.7362143516540527, "training_acc": 57.0, "val_loss": 0.6970288014411926, "val_acc": 48.0}
{"epoch": 2, "training_loss": 0.713998727798462, "training_acc": 51.0, "val_loss": 0.7457196140289306, "val_acc": 52.0}
{"epoch": 3, "training_loss": 0.7990586423873901, "training_acc": 53.0, "val_loss": 0.7374710464477539, "val_acc": 48.0}
{"epoch": 4, "training_loss": 0.7953917026519776, "training_acc": 39.0, "val_loss": 0.6928226470947265, "val_acc": 52.0}
{"epoch": 5, "training_loss": 0.7266324234008789, "training_acc": 43.0, "val_loss": 0.6969149398803711, "val_acc": 52.0}
{"epoch": 6, "training_loss": 0.6936255741119385, "training_acc": 53.0, "val_loss": 0.6928955578804016, "val_acc": 48.0}
{"epoch": 7, "training_loss": 0.7109941935539246, "training_acc": 46.0, "val_loss": 0.7028676533699035, "val_acc": 52.0}
{"epoch": 8, "training_loss": 0.7007314729690551, "training_acc": 49.0, "val_loss": 0.6984701490402222, "val_acc": 52.0}
{"epoch": 9, "training_loss": 0.7200512266159058, "training_acc": 51.0, "val_loss": 0.6931424975395203, "val_acc": 44.0}
{"epoch": 10, "training_loss": 0.6867982029914856, "training_acc": 53.0, "val_loss": 0.7148951649665832, "val_acc": 52.0}
{"epoch": 11, "training_loss": 0.7121240878105164, "training_acc": 58.0, "val_loss": 0.7730906724929809, "val_acc": 48.0}
{"epoch": 12, "training_loss": 0.7470385837554931, "training_acc": 47.0, "val_loss": 0.7233786821365357, "val_acc": 52.0}
{"epoch": 13, "training_loss": 0.7101467514038086, "training_acc": 55.0, "val_loss": 0.7749595189094544, "val_acc": 48.0}
{"epoch": 14, "training_loss": 0.8254943060874939, "training_acc": 47.0, "val_loss": 0.8476288008689881, "val_acc": 52.0}
{"epoch": 15, "training_loss": 0.7415843057632446, "training_acc": 55.0, "val_loss": 0.7776156163215637, "val_acc": 48.0}
{"epoch": 16, "training_loss": 0.7873394441604614, "training_acc": 49.0, "val_loss": 0.7474462628364563, "val_acc": 52.0}
{"epoch": 17, "training_loss": 0.7680968236923218, "training_acc": 43.0, "val_loss": 0.6962436366081238, "val_acc": 52.0}
{"epoch": 18, "training_loss": 0.7437856864929199, "training_acc": 53.0, "val_loss": 0.6969442749023438, "val_acc": 48.0}
{"epoch": 19, "training_loss": 0.8037707328796386, "training_acc": 45.0, "val_loss": 0.7414075446128845, "val_acc": 52.0}
{"epoch": 20, "training_loss": 0.7160606145858764, "training_acc": 51.0, "val_loss": 0.7083583021163941, "val_acc": 48.0}
{"epoch": 21, "training_loss": 0.7510504388809204, "training_acc": 47.0, "val_loss": 0.6926211047172547, "val_acc": 52.0}
{"epoch": 22, "training_loss": 0.7187208867073059, "training_acc": 57.0, "val_loss": 0.6925161528587341, "val_acc": 52.0}
{"epoch": 23, "training_loss": 0.725250198841095, "training_acc": 53.0, "val_loss": 0.6927152228355408, "val_acc": 48.0}
{"epoch": 24, "training_loss": 0.7209570026397705, "training_acc": 56.0, "val_loss": 0.7286264395713806, "val_acc": 48.0}
{"epoch": 25, "training_loss": 0.7965360879898071, "training_acc": 47.0, "val_loss": 0.7473186206817627, "val_acc": 52.0}
{"epoch": 26, "training_loss": 0.7618706703186036, "training_acc": 45.0, "val_loss": 0.7153303718566895, "val_acc": 48.0}
{"epoch": 27, "training_loss": 0.732592830657959, "training_acc": 45.0, "val_loss": 0.6961105394363404, "val_acc": 52.0}
{"epoch": 28, "training_loss": 0.7131596279144287, "training_acc": 46.0, "val_loss": 0.6985681271553039, "val_acc": 52.0}
{"epoch": 29, "training_loss": 0.7613012742996216, "training_acc": 53.0, "val_loss": 0.7773677563667297, "val_acc": 48.0}
{"epoch": 30, "training_loss": 0.8672554183006287, "training_acc": 49.0, "val_loss": 0.724633378982544, "val_acc": 52.0}
{"epoch": 31, "training_loss": 0.8202524423599243, "training_acc": 53.0, "val_loss": 0.7463482713699341, "val_acc": 48.0}
{"epoch": 32, "training_loss": 0.8225153803825378, "training_acc": 47.0, "val_loss": 0.7348118376731873, "val_acc": 52.0}
{"epoch": 33, "training_loss": 0.8195986557006836, "training_acc": 53.0, "val_loss": 0.7144868421554565, "val_acc": 48.0}
{"epoch": 34, "training_loss": 0.8232088518142701, "training_acc": 47.0, "val_loss": 0.7202255535125732, "val_acc": 52.0}
{"epoch": 35, "training_loss": 0.7400398921966552, "training_acc": 53.0, "val_loss": 0.6929224634170532, "val_acc": 44.0}
{"epoch": 36, "training_loss": 0.7026349043846131, "training_acc": 55.0, "val_loss": 0.6974355673789978, "val_acc": 48.0}
{"epoch": 37, "training_loss": 0.6902236318588257, "training_acc": 53.0, "val_loss": 0.7212338638305664, "val_acc": 52.0}
{"epoch": 38, "training_loss": 0.7014720749855041, "training_acc": 53.0, "val_loss": 0.692956383228302, "val_acc": 44.0}
{"epoch": 39, "training_loss": 0.7090805006027222, "training_acc": 52.0, "val_loss": 0.6996378374099731, "val_acc": 52.0}
{"epoch": 40, "training_loss": 0.6987357115745545, "training_acc": 55.0, "val_loss": 0.7493069958686829, "val_acc": 48.0}
{"epoch": 41, "training_loss": 0.8560304236412049, "training_acc": 45.0, "val_loss": 0.7717979502677917, "val_acc": 52.0}
