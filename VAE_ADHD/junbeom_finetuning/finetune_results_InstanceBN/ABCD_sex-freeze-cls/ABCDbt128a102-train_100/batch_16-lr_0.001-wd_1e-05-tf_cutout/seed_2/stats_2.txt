"main_modify.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 2 --task ABCD_sex --input_option BT_org --learning_rate 1e-3 --batch_size 16 --weight_decay 1e-5 --BN inst --save_path finetune_results_InstanceBN --run_where lab"
{"epoch": 0, "training_loss": 0.740575315952301, "training_acc": 45.0, "val_loss": 0.698666627407074, "val_acc": 52.0}
{"epoch": 1, "training_loss": 0.7242938709259034, "training_acc": 53.0, "val_loss": 0.7228360748291016, "val_acc": 52.0}
{"epoch": 2, "training_loss": 0.695739459991455, "training_acc": 53.0, "val_loss": 0.699031069278717, "val_acc": 48.0}
{"epoch": 3, "training_loss": 0.706698637008667, "training_acc": 47.0, "val_loss": 0.6951184892654418, "val_acc": 48.0}
{"epoch": 4, "training_loss": 0.6997871160507202, "training_acc": 53.0, "val_loss": 0.7078946352005004, "val_acc": 52.0}
{"epoch": 5, "training_loss": 0.6966144132614136, "training_acc": 53.0, "val_loss": 0.6939902520179748, "val_acc": 40.0}
{"epoch": 6, "training_loss": 0.7232378339767456, "training_acc": 47.0, "val_loss": 0.6985670137405395, "val_acc": 48.0}
{"epoch": 7, "training_loss": 0.7155831241607666, "training_acc": 47.0, "val_loss": 0.717559175491333, "val_acc": 52.0}
{"epoch": 8, "training_loss": 0.706624436378479, "training_acc": 51.0, "val_loss": 0.6971856021881103, "val_acc": 48.0}
{"epoch": 9, "training_loss": 0.6977972626686096, "training_acc": 46.0, "val_loss": 0.6934161758422852, "val_acc": 56.0}
{"epoch": 10, "training_loss": 0.6940344905853272, "training_acc": 54.0, "val_loss": 0.6929872488975525, "val_acc": 52.0}
{"epoch": 11, "training_loss": 0.6956285619735718, "training_acc": 41.0, "val_loss": 0.6929420042037964, "val_acc": 52.0}
{"epoch": 12, "training_loss": 0.692137622833252, "training_acc": 53.0, "val_loss": 0.6997463798522949, "val_acc": 52.0}
{"epoch": 13, "training_loss": 0.6862002515792847, "training_acc": 53.0, "val_loss": 0.6949299311637879, "val_acc": 48.0}
{"epoch": 14, "training_loss": 0.7018667578697204, "training_acc": 47.0, "val_loss": 0.7015861535072326, "val_acc": 48.0}
{"epoch": 15, "training_loss": 0.6915448379516601, "training_acc": 55.0, "val_loss": 0.6950233316421509, "val_acc": 52.0}
{"epoch": 16, "training_loss": 0.6917484760284424, "training_acc": 53.0, "val_loss": 0.6931000661849975, "val_acc": 52.0}
{"epoch": 17, "training_loss": 0.6885715341567993, "training_acc": 53.0, "val_loss": 0.6943042945861816, "val_acc": 52.0}
{"epoch": 18, "training_loss": 0.6921730542182922, "training_acc": 47.0, "val_loss": 0.6945720648765564, "val_acc": 48.0}
{"epoch": 19, "training_loss": 0.6987607574462891, "training_acc": 48.0, "val_loss": 0.694227261543274, "val_acc": 52.0}
{"epoch": 20, "training_loss": 0.694538459777832, "training_acc": 43.0, "val_loss": 0.6987346315383911, "val_acc": 48.0}
{"epoch": 21, "training_loss": 0.6947513151168824, "training_acc": 48.0, "val_loss": 0.6927607178688049, "val_acc": 52.0}
{"epoch": 22, "training_loss": 0.6935381221771241, "training_acc": 53.0, "val_loss": 0.7070540642738342, "val_acc": 52.0}
{"epoch": 23, "training_loss": 0.6957087087631225, "training_acc": 53.0, "val_loss": 0.6927461695671081, "val_acc": 52.0}
{"epoch": 24, "training_loss": 0.6862503361701965, "training_acc": 53.0, "val_loss": 0.6967497658729553, "val_acc": 52.0}
{"epoch": 25, "training_loss": 0.6979001665115356, "training_acc": 57.0, "val_loss": 0.6927197980880737, "val_acc": 52.0}
{"epoch": 26, "training_loss": 0.7017130374908447, "training_acc": 53.0, "val_loss": 0.696169843673706, "val_acc": 52.0}
{"epoch": 27, "training_loss": 0.6836469650268555, "training_acc": 52.0, "val_loss": 0.6949862718582154, "val_acc": 48.0}
{"epoch": 28, "training_loss": 0.6932441306114197, "training_acc": 49.0, "val_loss": 0.6933699512481689, "val_acc": 52.0}
{"epoch": 29, "training_loss": 0.6964800643920899, "training_acc": 51.0, "val_loss": 0.693130350112915, "val_acc": 56.0}
{"epoch": 30, "training_loss": 0.6920483493804932, "training_acc": 50.0, "val_loss": 0.6941588568687439, "val_acc": 52.0}
{"epoch": 31, "training_loss": 0.6843149089813232, "training_acc": 52.0, "val_loss": 0.7008319425582886, "val_acc": 52.0}
{"epoch": 32, "training_loss": 0.689515414237976, "training_acc": 53.0, "val_loss": 0.6947007036209106, "val_acc": 52.0}
{"epoch": 33, "training_loss": 0.6886271095275879, "training_acc": 53.0, "val_loss": 0.693418025970459, "val_acc": 52.0}
{"epoch": 34, "training_loss": 0.6932779955863952, "training_acc": 49.0, "val_loss": 0.6939069175720215, "val_acc": 48.0}
{"epoch": 35, "training_loss": 0.6864488101005555, "training_acc": 52.0, "val_loss": 0.6977097749710083, "val_acc": 52.0}
{"epoch": 36, "training_loss": 0.6891883611679077, "training_acc": 53.0, "val_loss": 0.6927694773674011, "val_acc": 52.0}
{"epoch": 37, "training_loss": 0.6917994832992553, "training_acc": 51.0, "val_loss": 0.695340964794159, "val_acc": 48.0}
{"epoch": 38, "training_loss": 0.6933252048492432, "training_acc": 47.0, "val_loss": 0.6941276478767395, "val_acc": 52.0}
{"epoch": 39, "training_loss": 0.6851266193389892, "training_acc": 53.0, "val_loss": 0.6949598693847656, "val_acc": 52.0}
{"epoch": 40, "training_loss": 0.68595942735672, "training_acc": 53.0, "val_loss": 0.6965933275222779, "val_acc": 52.0}
{"epoch": 41, "training_loss": 0.692286319732666, "training_acc": 53.0, "val_loss": 0.6932050800323486, "val_acc": 56.0}
{"epoch": 42, "training_loss": 0.6798578143119812, "training_acc": 62.0, "val_loss": 0.6968362736701965, "val_acc": 52.0}
{"epoch": 43, "training_loss": 0.691501784324646, "training_acc": 53.0, "val_loss": 0.7010586500167847, "val_acc": 52.0}
{"epoch": 44, "training_loss": 0.6909376573562622, "training_acc": 59.0, "val_loss": 0.6940627551078796, "val_acc": 44.0}
