"main_modify.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --learning_rate 5e-4 --batch_size 16 --weight_decay 5e-3 --BN inst --save_path finetune_results_InstanceBN --run_where lab"
{"epoch": 0, "training_loss": 0.6903768014907837, "training_acc": 56.0, "val_loss": 0.6986429953575134, "val_acc": 52.0}
{"epoch": 1, "training_loss": 0.7183081483840943, "training_acc": 53.0, "val_loss": 0.7202024483680725, "val_acc": 52.0}
{"epoch": 2, "training_loss": 0.7150447893142701, "training_acc": 53.0, "val_loss": 0.692383635044098, "val_acc": 52.0}
{"epoch": 3, "training_loss": 0.6941547274589539, "training_acc": 53.0, "val_loss": 0.6911163806915284, "val_acc": 52.0}
{"epoch": 4, "training_loss": 0.6951500916481018, "training_acc": 51.0, "val_loss": 0.6913396573066711, "val_acc": 52.0}
{"epoch": 5, "training_loss": 0.6941891098022461, "training_acc": 45.0, "val_loss": 0.6912115931510925, "val_acc": 52.0}
{"epoch": 6, "training_loss": 0.6949422788619996, "training_acc": 53.0, "val_loss": 0.6938026404380798, "val_acc": 52.0}
{"epoch": 7, "training_loss": 0.6945257234573364, "training_acc": 53.0, "val_loss": 0.6928251671791077, "val_acc": 52.0}
{"epoch": 8, "training_loss": 0.6957135939598084, "training_acc": 53.0, "val_loss": 0.6949147462844849, "val_acc": 48.0}
{"epoch": 9, "training_loss": 0.6983824300765992, "training_acc": 40.0, "val_loss": 0.691035373210907, "val_acc": 52.0}
{"epoch": 10, "training_loss": 0.6933726739883422, "training_acc": 53.0, "val_loss": 0.6913967418670655, "val_acc": 52.0}
{"epoch": 11, "training_loss": 0.6931189799308777, "training_acc": 53.0, "val_loss": 0.6920900082588196, "val_acc": 52.0}
{"epoch": 12, "training_loss": 0.6949862170219422, "training_acc": 53.0, "val_loss": 0.6976398038864136, "val_acc": 52.0}
{"epoch": 13, "training_loss": 0.6944233417510987, "training_acc": 53.0, "val_loss": 0.6921723222732544, "val_acc": 52.0}
{"epoch": 14, "training_loss": 0.6991528248786927, "training_acc": 42.0, "val_loss": 0.6927033567428589, "val_acc": 48.0}
{"epoch": 15, "training_loss": 0.6964333057403564, "training_acc": 44.0, "val_loss": 0.6909831357002258, "val_acc": 52.0}
{"epoch": 16, "training_loss": 0.6967097783088684, "training_acc": 53.0, "val_loss": 0.6927902722358703, "val_acc": 52.0}
{"epoch": 17, "training_loss": 0.6950570344924927, "training_acc": 53.0, "val_loss": 0.6929084706306458, "val_acc": 52.0}
{"epoch": 18, "training_loss": 0.6932519197463989, "training_acc": 53.0, "val_loss": 0.6909920239448547, "val_acc": 52.0}
{"epoch": 19, "training_loss": 0.6916749238967895, "training_acc": 53.0, "val_loss": 0.6914608836174011, "val_acc": 52.0}
{"epoch": 20, "training_loss": 0.6913489508628845, "training_acc": 53.0, "val_loss": 0.6911970257759095, "val_acc": 52.0}
{"epoch": 21, "training_loss": 0.6949659776687622, "training_acc": 52.0, "val_loss": 0.6909905552864075, "val_acc": 52.0}
{"epoch": 22, "training_loss": 0.6899420380592346, "training_acc": 53.0, "val_loss": 0.6918651962280273, "val_acc": 52.0}
{"epoch": 23, "training_loss": 0.6945688486099243, "training_acc": 53.0, "val_loss": 0.6932186150550842, "val_acc": 52.0}
{"epoch": 24, "training_loss": 0.6993479084968567, "training_acc": 53.0, "val_loss": 0.6916265177726746, "val_acc": 60.0}
{"epoch": 25, "training_loss": 0.6952456450462341, "training_acc": 49.0, "val_loss": 0.6907843399047852, "val_acc": 52.0}
{"epoch": 26, "training_loss": 0.6932071280479432, "training_acc": 53.0, "val_loss": 0.6925840306282044, "val_acc": 52.0}
{"epoch": 27, "training_loss": 0.6953430438041687, "training_acc": 52.0, "val_loss": 0.6928392004966736, "val_acc": 48.0}
{"epoch": 28, "training_loss": 0.6959427165985107, "training_acc": 47.0, "val_loss": 0.6915178990364075, "val_acc": 56.0}
{"epoch": 29, "training_loss": 0.6959102821350097, "training_acc": 53.0, "val_loss": 0.6962331008911132, "val_acc": 52.0}
{"epoch": 30, "training_loss": 0.7040928602218628, "training_acc": 53.0, "val_loss": 0.6994131278991699, "val_acc": 52.0}
{"epoch": 31, "training_loss": 0.6920561981201172, "training_acc": 53.0, "val_loss": 0.691444730758667, "val_acc": 60.0}
{"epoch": 32, "training_loss": 0.699816312789917, "training_acc": 43.0, "val_loss": 0.6919031715393067, "val_acc": 44.0}
{"epoch": 33, "training_loss": 0.6917924761772156, "training_acc": 56.0, "val_loss": 0.6907880067825317, "val_acc": 52.0}
{"epoch": 34, "training_loss": 0.6915553140640259, "training_acc": 53.0, "val_loss": 0.6912900638580323, "val_acc": 52.0}
{"epoch": 35, "training_loss": 0.6922989797592163, "training_acc": 53.0, "val_loss": 0.6938546824455262, "val_acc": 52.0}
{"epoch": 36, "training_loss": 0.6965904617309571, "training_acc": 53.0, "val_loss": 0.6907843375205993, "val_acc": 52.0}
{"epoch": 37, "training_loss": 0.6903789043426514, "training_acc": 53.0, "val_loss": 0.6908631467819214, "val_acc": 52.0}
{"epoch": 38, "training_loss": 0.6986625862121582, "training_acc": 45.0, "val_loss": 0.691147141456604, "val_acc": 56.0}
{"epoch": 39, "training_loss": 0.6949401116371154, "training_acc": 53.0, "val_loss": 0.6917409825325013, "val_acc": 52.0}
{"epoch": 40, "training_loss": 0.6889051485061646, "training_acc": 53.0, "val_loss": 0.6906262898445129, "val_acc": 52.0}
{"epoch": 41, "training_loss": 0.6943541741371155, "training_acc": 45.0, "val_loss": 0.6915857219696044, "val_acc": 56.0}
{"epoch": 42, "training_loss": 0.6937057113647461, "training_acc": 52.0, "val_loss": 0.6910502386093139, "val_acc": 52.0}
{"epoch": 43, "training_loss": 0.6915400266647339, "training_acc": 53.0, "val_loss": 0.6917306923866272, "val_acc": 52.0}
{"epoch": 44, "training_loss": 0.6909240078926087, "training_acc": 53.0, "val_loss": 0.6911719679832459, "val_acc": 52.0}
{"epoch": 45, "training_loss": 0.6903854012489319, "training_acc": 53.0, "val_loss": 0.6910025811195374, "val_acc": 52.0}
{"epoch": 46, "training_loss": 0.68957603931427, "training_acc": 53.0, "val_loss": 0.6916229796409606, "val_acc": 52.0}
{"epoch": 47, "training_loss": 0.6903129816055298, "training_acc": 53.0, "val_loss": 0.6947527360916138, "val_acc": 52.0}
{"epoch": 48, "training_loss": 0.6910411143302917, "training_acc": 53.0, "val_loss": 0.691312997341156, "val_acc": 52.0}
{"epoch": 49, "training_loss": 0.6925535488128662, "training_acc": 46.0, "val_loss": 0.6934185051918029, "val_acc": 48.0}
{"epoch": 50, "training_loss": 0.6929180645942687, "training_acc": 50.0, "val_loss": 0.6911134815216065, "val_acc": 56.0}
{"epoch": 51, "training_loss": 0.6890007257461548, "training_acc": 55.0, "val_loss": 0.6910857605934143, "val_acc": 52.0}
{"epoch": 52, "training_loss": 0.6893980860710144, "training_acc": 53.0, "val_loss": 0.6937842249870301, "val_acc": 52.0}
{"epoch": 53, "training_loss": 0.6901768875122071, "training_acc": 53.0, "val_loss": 0.6907065510749817, "val_acc": 52.0}
{"epoch": 54, "training_loss": 0.6920680356025696, "training_acc": 55.0, "val_loss": 0.6909449791908264, "val_acc": 52.0}
{"epoch": 55, "training_loss": 0.6898532557487488, "training_acc": 53.0, "val_loss": 0.6929477429389954, "val_acc": 52.0}
{"epoch": 56, "training_loss": 0.6945278215408325, "training_acc": 53.0, "val_loss": 0.6936203026771546, "val_acc": 52.0}
{"epoch": 57, "training_loss": 0.696547532081604, "training_acc": 54.0, "val_loss": 0.6936959195137024, "val_acc": 48.0}
{"epoch": 58, "training_loss": 0.6933643817901611, "training_acc": 48.0, "val_loss": 0.6908501815795899, "val_acc": 52.0}
{"epoch": 59, "training_loss": 0.6880542349815368, "training_acc": 53.0, "val_loss": 0.6926743030548096, "val_acc": 52.0}
