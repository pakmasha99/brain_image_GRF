"main_modify.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 1 --task ABCD_sex --input_option BT_org --learning_rate 1e-1 --batch_size 16 --weight_decay 1e-2 --BN inst --save_path finetune_results_InstanceBN --run_where lab"
{"epoch": 0, "training_loss": 7.127532348632813, "training_acc": 59.0, "val_loss": 3.311328163146973, "val_acc": 48.0}
{"epoch": 1, "training_loss": 2.2210029792785644, "training_acc": 45.0, "val_loss": 1.8131209564208985, "val_acc": 52.0}
{"epoch": 2, "training_loss": 1.2163417768478393, "training_acc": 47.0, "val_loss": 0.7748426914215087, "val_acc": 48.0}
{"epoch": 3, "training_loss": 1.1289468574523926, "training_acc": 45.0, "val_loss": 1.6821198511123656, "val_acc": 48.0}
{"epoch": 4, "training_loss": 1.9178860473632813, "training_acc": 51.0, "val_loss": 1.2864181089401245, "val_acc": 48.0}
{"epoch": 5, "training_loss": 1.0682698678970337, "training_acc": 52.0, "val_loss": 1.1919541120529176, "val_acc": 48.0}
{"epoch": 6, "training_loss": 1.1657620811462401, "training_acc": 51.0, "val_loss": 1.5507253551483153, "val_acc": 52.0}
{"epoch": 7, "training_loss": 1.6882461071014405, "training_acc": 53.0, "val_loss": 0.6898425006866455, "val_acc": 48.0}
{"epoch": 8, "training_loss": 0.9827158784866333, "training_acc": 54.0, "val_loss": 0.9119713068008423, "val_acc": 52.0}
{"epoch": 9, "training_loss": 1.3167944240570069, "training_acc": 50.0, "val_loss": 1.158993272781372, "val_acc": 48.0}
{"epoch": 10, "training_loss": 1.3112663888931275, "training_acc": 45.0, "val_loss": 1.604749071598053, "val_acc": 52.0}
{"epoch": 11, "training_loss": 1.7138735485076904, "training_acc": 47.0, "val_loss": 1.1193257880210876, "val_acc": 48.0}
{"epoch": 12, "training_loss": 1.295265793800354, "training_acc": 52.0, "val_loss": 2.134824914932251, "val_acc": 52.0}
{"epoch": 13, "training_loss": 1.5740596961975097, "training_acc": 51.0, "val_loss": 0.6775401353836059, "val_acc": 68.0}
{"epoch": 14, "training_loss": 0.9068374395370483, "training_acc": 53.0, "val_loss": 3.2030528450012206, "val_acc": 52.0}
{"epoch": 15, "training_loss": 2.925956615060568, "training_acc": 49.0, "val_loss": 5.159541625976562, "val_acc": 52.0}
{"epoch": 16, "training_loss": 3.4016914987564086, "training_acc": 45.0, "val_loss": 1.0027836227416993, "val_acc": 52.0}
{"epoch": 17, "training_loss": 0.8244673538208008, "training_acc": 54.0, "val_loss": 1.0024518537521363, "val_acc": 48.0}
{"epoch": 18, "training_loss": 0.8017273378372193, "training_acc": 63.0, "val_loss": 1.3439809417724609, "val_acc": 48.0}
{"epoch": 19, "training_loss": 1.4377744102478027, "training_acc": 51.0, "val_loss": 2.1912964820861816, "val_acc": 48.0}
{"epoch": 20, "training_loss": 1.940290412902832, "training_acc": 56.0, "val_loss": 2.286606159210205, "val_acc": 52.0}
{"epoch": 21, "training_loss": 3.456864986419678, "training_acc": 49.0, "val_loss": 1.9326459455490113, "val_acc": 52.0}
{"epoch": 22, "training_loss": 2.2736282920837403, "training_acc": 41.0, "val_loss": 1.6819239997863769, "val_acc": 48.0}
{"epoch": 23, "training_loss": 1.9220617198944092, "training_acc": 49.0, "val_loss": 1.442152738571167, "val_acc": 48.0}
{"epoch": 24, "training_loss": 0.8148657321929932, "training_acc": 64.0, "val_loss": 0.6631415581703186, "val_acc": 64.0}
{"epoch": 25, "training_loss": 1.3281124544143676, "training_acc": 54.0, "val_loss": 1.9864858102798462, "val_acc": 48.0}
{"epoch": 26, "training_loss": 3.557364139556885, "training_acc": 51.0, "val_loss": 5.4493137550354005, "val_acc": 52.0}
{"epoch": 27, "training_loss": 5.384791140556335, "training_acc": 41.0, "val_loss": 4.001628456115722, "val_acc": 52.0}
{"epoch": 28, "training_loss": 2.98551157951355, "training_acc": 59.0, "val_loss": 3.1813985443115236, "val_acc": 48.0}
{"epoch": 29, "training_loss": 1.7363467836380004, "training_acc": 60.0, "val_loss": 1.2887948179244995, "val_acc": 48.0}
{"epoch": 30, "training_loss": 1.4983391666412353, "training_acc": 51.0, "val_loss": 2.5521336460113524, "val_acc": 52.0}
{"epoch": 31, "training_loss": 2.4867738676071167, "training_acc": 47.0, "val_loss": 3.2772299098968505, "val_acc": 52.0}
{"epoch": 32, "training_loss": 2.522504849433899, "training_acc": 51.0, "val_loss": 2.134482364654541, "val_acc": 52.0}
{"epoch": 33, "training_loss": 1.6652106189727782, "training_acc": 57.0, "val_loss": 3.674976291656494, "val_acc": 52.0}
{"epoch": 34, "training_loss": 2.2164652156829834, "training_acc": 52.0, "val_loss": 2.531070575714111, "val_acc": 52.0}
{"epoch": 35, "training_loss": 2.5413310623168943, "training_acc": 47.0, "val_loss": 4.17069352388382, "val_acc": 52.0}
{"epoch": 36, "training_loss": 2.4341493606567384, "training_acc": 45.0, "val_loss": 0.8065613675117492, "val_acc": 52.0}
{"epoch": 37, "training_loss": 1.3383499383926392, "training_acc": 56.0, "val_loss": 2.907236166000366, "val_acc": 48.0}
{"epoch": 38, "training_loss": 2.1476801347732546, "training_acc": 42.0, "val_loss": 1.4816541910171508, "val_acc": 52.0}
{"epoch": 39, "training_loss": 1.5451244258880614, "training_acc": 51.0, "val_loss": 1.066806652545929, "val_acc": 52.0}
{"epoch": 40, "training_loss": 2.057880120277405, "training_acc": 53.0, "val_loss": 1.086706805229187, "val_acc": 52.0}
{"epoch": 41, "training_loss": 1.8436466693878173, "training_acc": 49.0, "val_loss": 2.472008409500122, "val_acc": 48.0}
{"epoch": 42, "training_loss": 1.7268126678466797, "training_acc": 59.0, "val_loss": 4.210736818313599, "val_acc": 48.0}
{"epoch": 43, "training_loss": 2.7270809981320054, "training_acc": 47.0, "val_loss": 6.019466571807861, "val_acc": 48.0}
