"main_modify.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 8 --task ADNI_sex --input_option BT_org --learning_rate 1e-2 --batch_size 16 --weight_decay 1e-3 --BN inst --save_path finetune_results_InstanceBN --run_where lab"
{"epoch": 0, "training_loss": 1.1260981559753418, "training_acc": 49.0, "val_loss": 0.7321313667297363, "val_acc": 48.0}
{"epoch": 1, "training_loss": 0.7211886358261108, "training_acc": 55.0, "val_loss": 0.7816168570518494, "val_acc": 52.0}
{"epoch": 2, "training_loss": 0.7520003747940064, "training_acc": 49.0, "val_loss": 0.7181257653236389, "val_acc": 52.0}
{"epoch": 3, "training_loss": 0.7369181728363037, "training_acc": 53.0, "val_loss": 0.7685367774963379, "val_acc": 48.0}
{"epoch": 4, "training_loss": 0.6953477048873902, "training_acc": 61.0, "val_loss": 0.8651172566413879, "val_acc": 52.0}
{"epoch": 5, "training_loss": 0.7724187850952149, "training_acc": 49.0, "val_loss": 0.7150045037269592, "val_acc": 52.0}
{"epoch": 6, "training_loss": 0.7503326225280762, "training_acc": 47.0, "val_loss": 0.7502119135856629, "val_acc": 48.0}
{"epoch": 7, "training_loss": 0.7405079650878906, "training_acc": 51.0, "val_loss": 0.7024714374542236, "val_acc": 52.0}
{"epoch": 8, "training_loss": 0.7017871904373169, "training_acc": 53.0, "val_loss": 0.7084500575065613, "val_acc": 48.0}
{"epoch": 9, "training_loss": 0.7067819571495056, "training_acc": 51.0, "val_loss": 0.8404448890686035, "val_acc": 48.0}
{"epoch": 10, "training_loss": 0.7896580600738525, "training_acc": 45.0, "val_loss": 0.7001482319831848, "val_acc": 48.0}
{"epoch": 11, "training_loss": 0.728207950592041, "training_acc": 45.0, "val_loss": 0.8761976337432862, "val_acc": 52.0}
{"epoch": 12, "training_loss": 0.7848509001731873, "training_acc": 57.0, "val_loss": 1.162450804710388, "val_acc": 48.0}
{"epoch": 13, "training_loss": 0.8854229927062989, "training_acc": 53.0, "val_loss": 0.7062741732597351, "val_acc": 48.0}
{"epoch": 14, "training_loss": 0.8642368340492248, "training_acc": 45.0, "val_loss": 0.7322601652145386, "val_acc": 52.0}
{"epoch": 15, "training_loss": 0.7399802684783936, "training_acc": 43.0, "val_loss": 0.7099066233634949, "val_acc": 48.0}
{"epoch": 16, "training_loss": 0.7009373021125793, "training_acc": 57.0, "val_loss": 0.7855966281890869, "val_acc": 52.0}
{"epoch": 17, "training_loss": 0.7899226808547973, "training_acc": 49.0, "val_loss": 0.7043038892745972, "val_acc": 52.0}
{"epoch": 18, "training_loss": 0.8012756776809692, "training_acc": 47.0, "val_loss": 0.9812389492988587, "val_acc": 52.0}
{"epoch": 19, "training_loss": 1.0265642786026001, "training_acc": 47.0, "val_loss": 0.7127695941925049, "val_acc": 48.0}
{"epoch": 20, "training_loss": 0.789456090927124, "training_acc": 51.0, "val_loss": 0.8628079771995545, "val_acc": 48.0}
{"epoch": 21, "training_loss": 0.7842502093315125, "training_acc": 47.0, "val_loss": 0.9503031349182129, "val_acc": 48.0}
{"epoch": 22, "training_loss": 0.9509310817718506, "training_acc": 43.0, "val_loss": 0.7276466035842896, "val_acc": 52.0}
{"epoch": 23, "training_loss": 0.7325077104568481, "training_acc": 49.0, "val_loss": 0.7085739588737487, "val_acc": 48.0}
{"epoch": 24, "training_loss": 0.707530164718628, "training_acc": 47.0, "val_loss": 0.6978466534614562, "val_acc": 48.0}
{"epoch": 25, "training_loss": 0.7226603507995606, "training_acc": 49.0, "val_loss": 0.7720827221870422, "val_acc": 52.0}
{"epoch": 26, "training_loss": 0.73475261926651, "training_acc": 51.0, "val_loss": 0.7197096776962281, "val_acc": 48.0}
{"epoch": 27, "training_loss": 0.6917317700386048, "training_acc": 53.0, "val_loss": 0.6958084964752197, "val_acc": 52.0}
{"epoch": 28, "training_loss": 0.7043089485168457, "training_acc": 51.0, "val_loss": 0.7206677269935607, "val_acc": 52.0}
{"epoch": 29, "training_loss": 0.7709045600891113, "training_acc": 51.0, "val_loss": 0.6973121738433838, "val_acc": 48.0}
{"epoch": 30, "training_loss": 0.7046087622642517, "training_acc": 49.0, "val_loss": 0.6943540120124817, "val_acc": 48.0}
{"epoch": 31, "training_loss": 0.7212482643127441, "training_acc": 51.0, "val_loss": 0.716885461807251, "val_acc": 52.0}
{"epoch": 32, "training_loss": 0.730548529624939, "training_acc": 49.0, "val_loss": 0.7107571959495544, "val_acc": 48.0}
{"epoch": 33, "training_loss": 0.7020229911804199, "training_acc": 49.0, "val_loss": 0.7020467090606689, "val_acc": 48.0}
{"epoch": 34, "training_loss": 0.7236978340148926, "training_acc": 51.0, "val_loss": 0.7278607106208801, "val_acc": 48.0}
{"epoch": 35, "training_loss": 0.7028246974945068, "training_acc": 53.0, "val_loss": 0.7577768969535827, "val_acc": 52.0}
{"epoch": 36, "training_loss": 0.7549443054199219, "training_acc": 51.0, "val_loss": 0.7309315824508666, "val_acc": 48.0}
{"epoch": 37, "training_loss": 0.7135884428024292, "training_acc": 45.0, "val_loss": 0.7065519785881043, "val_acc": 52.0}
{"epoch": 38, "training_loss": 0.7125359773635864, "training_acc": 49.0, "val_loss": 0.7331341552734375, "val_acc": 48.0}
{"epoch": 39, "training_loss": 0.735826416015625, "training_acc": 51.0, "val_loss": 0.6985569095611572, "val_acc": 52.0}
{"epoch": 40, "training_loss": 0.7180771160125733, "training_acc": 41.0, "val_loss": 0.6934642791748047, "val_acc": 52.0}
{"epoch": 41, "training_loss": 0.7270352149009705, "training_acc": 51.0, "val_loss": 0.7231681919097901, "val_acc": 48.0}
{"epoch": 42, "training_loss": 0.7156405591964722, "training_acc": 45.0, "val_loss": 0.6974583029747009, "val_acc": 48.0}
{"epoch": 43, "training_loss": 0.7070519709587098, "training_acc": 51.0, "val_loss": 0.6926816582679749, "val_acc": 52.0}
{"epoch": 44, "training_loss": 0.7174084734916687, "training_acc": 51.0, "val_loss": 0.8027765011787414, "val_acc": 48.0}
{"epoch": 45, "training_loss": 0.783485369682312, "training_acc": 53.0, "val_loss": 0.7965631747245788, "val_acc": 52.0}
{"epoch": 46, "training_loss": 0.7429576992988587, "training_acc": 53.0, "val_loss": 0.9662749910354614, "val_acc": 48.0}
{"epoch": 47, "training_loss": 0.8264236855506897, "training_acc": 47.0, "val_loss": 0.7133451080322266, "val_acc": 52.0}
{"epoch": 48, "training_loss": 0.7363115453720093, "training_acc": 47.0, "val_loss": 0.6926031088829041, "val_acc": 52.0}
{"epoch": 49, "training_loss": 0.7612522077560425, "training_acc": 45.0, "val_loss": 0.7125366258621216, "val_acc": 48.0}
{"epoch": 50, "training_loss": 0.7047528505325318, "training_acc": 49.0, "val_loss": 0.6985115551948547, "val_acc": 48.0}
{"epoch": 51, "training_loss": 0.7096773767471314, "training_acc": 45.0, "val_loss": 0.7457727742195129, "val_acc": 48.0}
{"epoch": 52, "training_loss": 0.7283456254005433, "training_acc": 43.0, "val_loss": 0.7125341796875, "val_acc": 48.0}
{"epoch": 53, "training_loss": 0.7169136810302734, "training_acc": 43.0, "val_loss": 0.7017570614814759, "val_acc": 48.0}
{"epoch": 54, "training_loss": 0.7143658828735352, "training_acc": 45.0, "val_loss": 0.7175384879112243, "val_acc": 48.0}
{"epoch": 55, "training_loss": 0.7114833712577819, "training_acc": 51.0, "val_loss": 0.7120786499977112, "val_acc": 52.0}
{"epoch": 56, "training_loss": 0.7175310182571412, "training_acc": 47.0, "val_loss": 0.6967159080505371, "val_acc": 48.0}
{"epoch": 57, "training_loss": 0.7203161191940307, "training_acc": 51.0, "val_loss": 0.7537935328483581, "val_acc": 48.0}
{"epoch": 58, "training_loss": 0.7491822600364685, "training_acc": 45.0, "val_loss": 0.7049589276313781, "val_acc": 52.0}
{"epoch": 59, "training_loss": 0.712533884048462, "training_acc": 47.0, "val_loss": 0.7016960978507996, "val_acc": 48.0}
{"epoch": 60, "training_loss": 0.7041310358047486, "training_acc": 53.0, "val_loss": 0.7304660129547119, "val_acc": 52.0}
{"epoch": 61, "training_loss": 0.7279559803009034, "training_acc": 49.0, "val_loss": 0.7338050842285156, "val_acc": 48.0}
{"epoch": 62, "training_loss": 0.7140867948532105, "training_acc": 53.0, "val_loss": 0.7010328364372254, "val_acc": 52.0}
{"epoch": 63, "training_loss": 0.7130466842651367, "training_acc": 49.0, "val_loss": 0.6997715044021606, "val_acc": 52.0}
{"epoch": 64, "training_loss": 0.7505873441696167, "training_acc": 53.0, "val_loss": 0.8220145058631897, "val_acc": 48.0}
{"epoch": 65, "training_loss": 0.8426923632621766, "training_acc": 45.0, "val_loss": 0.7455177426338195, "val_acc": 52.0}
{"epoch": 66, "training_loss": 0.755126507282257, "training_acc": 49.0, "val_loss": 0.7211029767990113, "val_acc": 48.0}
{"epoch": 67, "training_loss": 0.7141615581512452, "training_acc": 41.0, "val_loss": 0.712389953136444, "val_acc": 52.0}
