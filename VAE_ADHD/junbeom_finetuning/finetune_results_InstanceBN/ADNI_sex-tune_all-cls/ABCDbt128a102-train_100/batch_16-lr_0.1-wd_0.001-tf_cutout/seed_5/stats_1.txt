"main_modify.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 5 --task ADNI_sex --input_option BT_org --learning_rate 1e-1 --batch_size 16 --weight_decay 1e-3 --BN inst --save_path finetune_results_InstanceBN --run_where lab"
{"epoch": 0, "training_loss": 8.816214141845704, "training_acc": 50.0, "val_loss": 1.0685637021064758, "val_acc": 48.0}
{"epoch": 1, "training_loss": 1.575877857208252, "training_acc": 49.0, "val_loss": 0.8030556917190552, "val_acc": 52.0}
{"epoch": 2, "training_loss": 1.7499988985061645, "training_acc": 44.0, "val_loss": 1.998356156349182, "val_acc": 52.0}
{"epoch": 3, "training_loss": 1.5370413875579834, "training_acc": 50.0, "val_loss": 0.9066400384902954, "val_acc": 52.0}
{"epoch": 4, "training_loss": 0.8460992336273193, "training_acc": 58.0, "val_loss": 0.8124640369415284, "val_acc": 52.0}
{"epoch": 5, "training_loss": 0.9309906005859375, "training_acc": 54.0, "val_loss": 0.7231063199043274, "val_acc": 48.0}
{"epoch": 6, "training_loss": 1.4750449180603027, "training_acc": 50.0, "val_loss": 2.479908332824707, "val_acc": 52.0}
{"epoch": 7, "training_loss": 2.1555400609970095, "training_acc": 56.0, "val_loss": 2.198398160934448, "val_acc": 52.0}
{"epoch": 8, "training_loss": 1.3734501016139984, "training_acc": 56.0, "val_loss": 3.966488227844238, "val_acc": 48.0}
{"epoch": 9, "training_loss": 2.2486631965637205, "training_acc": 52.0, "val_loss": 0.8592508172988892, "val_acc": 48.0}
{"epoch": 10, "training_loss": 2.14448392868042, "training_acc": 46.0, "val_loss": 1.2491769647598268, "val_acc": 52.0}
{"epoch": 11, "training_loss": 2.291413679122925, "training_acc": 50.0, "val_loss": 3.7906348514556885, "val_acc": 52.0}
{"epoch": 12, "training_loss": 2.5932069492340086, "training_acc": 34.0, "val_loss": 0.9896278524398804, "val_acc": 52.0}
{"epoch": 13, "training_loss": 0.7528748941421509, "training_acc": 60.0, "val_loss": 1.2457803344726563, "val_acc": 48.0}
{"epoch": 14, "training_loss": 0.9507553768157959, "training_acc": 56.0, "val_loss": 2.134708733558655, "val_acc": 52.0}
{"epoch": 15, "training_loss": 2.0055882358551025, "training_acc": 44.0, "val_loss": 1.0429186940193176, "val_acc": 48.0}
{"epoch": 16, "training_loss": 1.6339547443389892, "training_acc": 48.0, "val_loss": 2.8758837509155275, "val_acc": 48.0}
{"epoch": 17, "training_loss": 1.7241644477844238, "training_acc": 44.0, "val_loss": 0.707991726398468, "val_acc": 52.0}
{"epoch": 18, "training_loss": 1.2001076698303224, "training_acc": 52.0, "val_loss": 1.0604475021362305, "val_acc": 48.0}
{"epoch": 19, "training_loss": 1.117247190475464, "training_acc": 52.0, "val_loss": 0.8653701448440552, "val_acc": 52.0}
{"epoch": 20, "training_loss": 1.0492217350006103, "training_acc": 48.0, "val_loss": 1.2886191272735597, "val_acc": 52.0}
{"epoch": 21, "training_loss": 1.102541551589966, "training_acc": 44.0, "val_loss": 0.9956983661651612, "val_acc": 52.0}
{"epoch": 22, "training_loss": 0.9413554894924164, "training_acc": 54.0, "val_loss": 1.542372431755066, "val_acc": 48.0}
{"epoch": 23, "training_loss": 1.0412902331352234, "training_acc": 50.0, "val_loss": 1.0416630601882935, "val_acc": 52.0}
{"epoch": 24, "training_loss": 0.8764515542984008, "training_acc": 46.0, "val_loss": 1.5926321363449096, "val_acc": 52.0}
{"epoch": 25, "training_loss": 1.2288600540161132, "training_acc": 48.0, "val_loss": 0.6950926446914673, "val_acc": 52.0}
{"epoch": 26, "training_loss": 1.5310537147521972, "training_acc": 42.0, "val_loss": 1.161571888923645, "val_acc": 52.0}
{"epoch": 27, "training_loss": 1.710748586654663, "training_acc": 40.0, "val_loss": 0.7335127091407776, "val_acc": 48.0}
{"epoch": 28, "training_loss": 2.3037796688079832, "training_acc": 42.0, "val_loss": 5.445528926849366, "val_acc": 52.0}
{"epoch": 29, "training_loss": 3.2323291301727295, "training_acc": 52.0, "val_loss": 1.3761653757095338, "val_acc": 52.0}
{"epoch": 30, "training_loss": 0.9844239354133606, "training_acc": 52.0, "val_loss": 0.8573640441894531, "val_acc": 48.0}
{"epoch": 31, "training_loss": 1.0633929300308227, "training_acc": 56.0, "val_loss": 2.0083470010757445, "val_acc": 52.0}
{"epoch": 32, "training_loss": 1.5970627880096435, "training_acc": 46.0, "val_loss": 0.7254432725906372, "val_acc": 52.0}
{"epoch": 33, "training_loss": 0.725324587225914, "training_acc": 58.0, "val_loss": 2.3565844917297363, "val_acc": 52.0}
{"epoch": 34, "training_loss": 1.4528875541687012, "training_acc": 50.0, "val_loss": 1.2685998630523683, "val_acc": 52.0}
{"epoch": 35, "training_loss": 1.33426899433136, "training_acc": 52.0, "val_loss": 2.2812557888031004, "val_acc": 48.0}
{"epoch": 36, "training_loss": 1.5453585052490235, "training_acc": 48.0, "val_loss": 0.7899202299118042, "val_acc": 52.0}
{"epoch": 37, "training_loss": 1.3871108055114747, "training_acc": 49.0, "val_loss": 1.2554555130004883, "val_acc": 48.0}
{"epoch": 38, "training_loss": 2.647502965927124, "training_acc": 52.0, "val_loss": 0.7019142079353332, "val_acc": 48.0}
{"epoch": 39, "training_loss": 1.9519839429855346, "training_acc": 49.0, "val_loss": 0.9388415479660034, "val_acc": 52.0}
{"epoch": 40, "training_loss": 1.5279204034805298, "training_acc": 50.0, "val_loss": 1.3586152124404907, "val_acc": 52.0}
{"epoch": 41, "training_loss": 1.0775816082954406, "training_acc": 52.0, "val_loss": 1.4476849460601806, "val_acc": 48.0}
{"epoch": 42, "training_loss": 1.3700058698654174, "training_acc": 44.0, "val_loss": 0.707153205871582, "val_acc": 48.0}
{"epoch": 43, "training_loss": 1.3017373371124268, "training_acc": 46.0, "val_loss": 1.5615617895126344, "val_acc": 48.0}
{"epoch": 44, "training_loss": 1.0219206285476685, "training_acc": 56.0, "val_loss": 1.083932785987854, "val_acc": 52.0}
