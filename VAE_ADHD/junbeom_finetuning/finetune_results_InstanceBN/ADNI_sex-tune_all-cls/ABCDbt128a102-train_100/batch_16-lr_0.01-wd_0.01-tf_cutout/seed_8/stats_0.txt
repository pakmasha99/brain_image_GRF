"main_modify.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 8 --task ADNI_sex --input_option BT_org --learning_rate 1e-2 --batch_size 16 --weight_decay 1e-2 --BN inst --save_path finetune_results_InstanceBN --run_where lab"
{"epoch": 0, "training_loss": 0.9820887851715088, "training_acc": 53.0, "val_loss": 0.6924817276000976, "val_acc": 52.0}
{"epoch": 1, "training_loss": 0.7861637115478516, "training_acc": 43.0, "val_loss": 0.6957774257659912, "val_acc": 52.0}
{"epoch": 2, "training_loss": 0.724366397857666, "training_acc": 49.0, "val_loss": 0.7636145925521851, "val_acc": 52.0}
{"epoch": 3, "training_loss": 0.8548107767105102, "training_acc": 47.0, "val_loss": 0.8671069645881653, "val_acc": 52.0}
{"epoch": 4, "training_loss": 0.7557433485984802, "training_acc": 55.0, "val_loss": 1.1524608755111694, "val_acc": 48.0}
{"epoch": 5, "training_loss": 0.8943746185302734, "training_acc": 53.0, "val_loss": 0.6923897099494934, "val_acc": 52.0}
{"epoch": 6, "training_loss": 0.8129879188537598, "training_acc": 41.0, "val_loss": 0.6926522159576416, "val_acc": 52.0}
{"epoch": 7, "training_loss": 0.7200760078430176, "training_acc": 39.0, "val_loss": 0.7964438128471375, "val_acc": 48.0}
{"epoch": 8, "training_loss": 0.7622342443466187, "training_acc": 49.0, "val_loss": 0.7171749806404114, "val_acc": 52.0}
{"epoch": 9, "training_loss": 0.7048669648170471, "training_acc": 53.0, "val_loss": 0.7372169184684754, "val_acc": 48.0}
{"epoch": 10, "training_loss": 0.7139869070053101, "training_acc": 51.0, "val_loss": 0.7105491423606872, "val_acc": 48.0}
{"epoch": 11, "training_loss": 0.7289306879043579, "training_acc": 47.0, "val_loss": 0.6955071759223937, "val_acc": 52.0}
{"epoch": 12, "training_loss": 0.7615782690048217, "training_acc": 43.0, "val_loss": 0.6979680633544922, "val_acc": 48.0}
{"epoch": 13, "training_loss": 0.7153824186325073, "training_acc": 45.0, "val_loss": 0.694312937259674, "val_acc": 52.0}
{"epoch": 14, "training_loss": 0.7461344695091248, "training_acc": 35.0, "val_loss": 0.69453204870224, "val_acc": 48.0}
{"epoch": 15, "training_loss": 0.6984078788757324, "training_acc": 51.0, "val_loss": 0.7048259496688842, "val_acc": 48.0}
{"epoch": 16, "training_loss": 0.7333346080780029, "training_acc": 45.0, "val_loss": 0.7041015195846557, "val_acc": 48.0}
{"epoch": 17, "training_loss": 0.7207685017585754, "training_acc": 45.0, "val_loss": 0.7957211661338807, "val_acc": 48.0}
{"epoch": 18, "training_loss": 0.7634433722496032, "training_acc": 43.0, "val_loss": 0.7008619284629822, "val_acc": 52.0}
{"epoch": 19, "training_loss": 0.7113959789276123, "training_acc": 49.0, "val_loss": 0.7047967839241028, "val_acc": 48.0}
{"epoch": 20, "training_loss": 0.7016031169891357, "training_acc": 53.0, "val_loss": 0.6926287817955017, "val_acc": 52.0}
{"epoch": 21, "training_loss": 0.7201993989944458, "training_acc": 43.0, "val_loss": 0.700804147720337, "val_acc": 52.0}
{"epoch": 22, "training_loss": 0.69064457654953, "training_acc": 57.0, "val_loss": 1.0540538263320922, "val_acc": 48.0}
{"epoch": 23, "training_loss": 0.8493844485282898, "training_acc": 43.0, "val_loss": 0.6923516130447388, "val_acc": 52.0}
{"epoch": 24, "training_loss": 0.7292046189308167, "training_acc": 45.0, "val_loss": 0.6943768286705017, "val_acc": 52.0}
{"epoch": 25, "training_loss": 0.7117688846588135, "training_acc": 45.0, "val_loss": 0.701127200126648, "val_acc": 52.0}
{"epoch": 26, "training_loss": 0.7094562363624572, "training_acc": 51.0, "val_loss": 0.7090850472450256, "val_acc": 48.0}
{"epoch": 27, "training_loss": 0.7046576833724976, "training_acc": 49.0, "val_loss": 0.6927169060707092, "val_acc": 52.0}
{"epoch": 28, "training_loss": 0.7043659305572509, "training_acc": 41.0, "val_loss": 0.7099843621253967, "val_acc": 48.0}
{"epoch": 29, "training_loss": 0.7045224571228027, "training_acc": 49.0, "val_loss": 0.6928622150421142, "val_acc": 52.0}
{"epoch": 30, "training_loss": 0.7228954267501831, "training_acc": 45.0, "val_loss": 0.6956646060943603, "val_acc": 48.0}
{"epoch": 31, "training_loss": 0.7176351356506347, "training_acc": 45.0, "val_loss": 0.6987137532234192, "val_acc": 48.0}
{"epoch": 32, "training_loss": 0.6981621813774109, "training_acc": 45.0, "val_loss": 0.7341244888305664, "val_acc": 48.0}
{"epoch": 33, "training_loss": 0.7736834049224853, "training_acc": 51.0, "val_loss": 0.7534384942054748, "val_acc": 52.0}
{"epoch": 34, "training_loss": 0.7472999668121338, "training_acc": 49.0, "val_loss": 0.7162343239784241, "val_acc": 48.0}
{"epoch": 35, "training_loss": 0.7100610876083374, "training_acc": 51.0, "val_loss": 0.6948913931846619, "val_acc": 52.0}
{"epoch": 36, "training_loss": 0.7166007113456726, "training_acc": 49.0, "val_loss": 0.6923488283157349, "val_acc": 52.0}
{"epoch": 37, "training_loss": 0.7339324522018432, "training_acc": 49.0, "val_loss": 0.7211047673225403, "val_acc": 48.0}
{"epoch": 38, "training_loss": 0.7303271961212158, "training_acc": 39.0, "val_loss": 0.6925520205497742, "val_acc": 52.0}
{"epoch": 39, "training_loss": 0.7107590389251709, "training_acc": 53.0, "val_loss": 0.8116303181648254, "val_acc": 48.0}
{"epoch": 40, "training_loss": 0.7508708477020264, "training_acc": 51.0, "val_loss": 0.7058383679389953, "val_acc": 52.0}
{"epoch": 41, "training_loss": 0.7175490760803223, "training_acc": 43.0, "val_loss": 0.7119337964057922, "val_acc": 48.0}
{"epoch": 42, "training_loss": 0.7051010274887085, "training_acc": 45.0, "val_loss": 0.7393371653556824, "val_acc": 48.0}
{"epoch": 43, "training_loss": 0.7333003568649292, "training_acc": 49.0, "val_loss": 0.762569420337677, "val_acc": 52.0}
{"epoch": 44, "training_loss": 0.7334718894958496, "training_acc": 53.0, "val_loss": 0.7398908448219299, "val_acc": 48.0}
{"epoch": 45, "training_loss": 0.7054902935028076, "training_acc": 49.0, "val_loss": 0.692412235736847, "val_acc": 52.0}
{"epoch": 46, "training_loss": 0.7174198961257935, "training_acc": 47.0, "val_loss": 0.7216845273971557, "val_acc": 48.0}
{"epoch": 47, "training_loss": 0.7079543352127076, "training_acc": 49.0, "val_loss": 0.6949524807929993, "val_acc": 48.0}
{"epoch": 48, "training_loss": 0.716348786354065, "training_acc": 49.0, "val_loss": 0.6927808094024658, "val_acc": 52.0}
{"epoch": 49, "training_loss": 0.7130583906173706, "training_acc": 47.0, "val_loss": 0.713160560131073, "val_acc": 52.0}
{"epoch": 50, "training_loss": 0.7281658601760864, "training_acc": 45.0, "val_loss": 0.6987936902046203, "val_acc": 48.0}
{"epoch": 51, "training_loss": 0.7120287466049194, "training_acc": 45.0, "val_loss": 0.7366266059875488, "val_acc": 48.0}
{"epoch": 52, "training_loss": 0.7371279001235962, "training_acc": 53.0, "val_loss": 0.7652781701087952, "val_acc": 52.0}
{"epoch": 53, "training_loss": 0.7712542629241943, "training_acc": 47.0, "val_loss": 0.7032777380943298, "val_acc": 48.0}
{"epoch": 54, "training_loss": 0.703361587524414, "training_acc": 55.0, "val_loss": 0.6938796615600586, "val_acc": 48.0}
{"epoch": 55, "training_loss": 0.7129802751541138, "training_acc": 51.0, "val_loss": 0.7031734848022461, "val_acc": 52.0}
