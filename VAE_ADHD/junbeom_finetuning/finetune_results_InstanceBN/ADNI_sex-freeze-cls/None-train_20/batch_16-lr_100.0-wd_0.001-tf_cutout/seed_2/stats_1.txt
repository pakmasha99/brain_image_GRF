"main_modify.py --pretrained_path None --mode finetuning --train_num 20 --layer_control freeze --stratify strat --random_seed 2 --task ADNI_sex --input_option yAware --learning_rate 1e2 --batch_size 16 --weight_decay 1e-3 --BN inst --save_path finetune_results_InstanceBN"
{"epoch": 0, "training_loss": 6427.412540483475, "training_acc": 40.0, "val_loss": 5205.13671875, "val_acc": 60.0}
{"epoch": 1, "training_loss": 7275.9626953125, "training_acc": 50.0, "val_loss": 19930.384765625, "val_acc": 40.0}
{"epoch": 2, "training_loss": 16517.22734375, "training_acc": 50.0, "val_loss": 12014.8359375, "val_acc": 40.0}
{"epoch": 3, "training_loss": 7441.991259765625, "training_acc": 50.0, "val_loss": 7720.38916015625, "val_acc": 60.0}
{"epoch": 4, "training_loss": 10241.550390625, "training_acc": 50.0, "val_loss": 14936.3662109375, "val_acc": 60.0}
{"epoch": 5, "training_loss": 18668.540625, "training_acc": 50.0, "val_loss": 12370.8203125, "val_acc": 60.0}
{"epoch": 6, "training_loss": 14534.48125, "training_acc": 50.0, "val_loss": 3299.943359375, "val_acc": 60.0}
{"epoch": 7, "training_loss": 4109.81611328125, "training_acc": 50.0, "val_loss": 10879.9052734375, "val_acc": 40.0}
{"epoch": 8, "training_loss": 9242.232421875, "training_acc": 50.0, "val_loss": 13091.6669921875, "val_acc": 40.0}
{"epoch": 9, "training_loss": 10438.8236328125, "training_acc": 50.0, "val_loss": 4977.56494140625, "val_acc": 40.0}
{"epoch": 10, "training_loss": 2488.1107421875, "training_acc": 70.0, "val_loss": 5183.09814453125, "val_acc": 60.0}
{"epoch": 11, "training_loss": 6930.6630859375, "training_acc": 50.0, "val_loss": 6772.47021484375, "val_acc": 60.0}
{"epoch": 12, "training_loss": 7776.23203125, "training_acc": 50.0, "val_loss": 876.1262817382812, "val_acc": 60.0}
{"epoch": 13, "training_loss": 1283.70654296875, "training_acc": 60.0, "val_loss": 11627.3505859375, "val_acc": 40.0}
{"epoch": 14, "training_loss": 9687.83984375, "training_acc": 50.0, "val_loss": 14006.7412109375, "val_acc": 40.0}
{"epoch": 15, "training_loss": 11474.75869140625, "training_acc": 50.0, "val_loss": 7921.42578125, "val_acc": 40.0}
{"epoch": 16, "training_loss": 6084.058361816406, "training_acc": 50.0, "val_loss": 3398.375, "val_acc": 60.0}
{"epoch": 17, "training_loss": 4529.795068359375, "training_acc": 50.0, "val_loss": 6687.85400390625, "val_acc": 60.0}
{"epoch": 18, "training_loss": 8148.7380859375, "training_acc": 50.0, "val_loss": 3315.407470703125, "val_acc": 60.0}
{"epoch": 19, "training_loss": 3462.610888671875, "training_acc": 50.0, "val_loss": 3873.774658203125, "val_acc": 40.0}
{"epoch": 20, "training_loss": 3272.6259765625, "training_acc": 50.0, "val_loss": 1949.2889404296875, "val_acc": 40.0}
{"epoch": 21, "training_loss": 1311.5210571289062, "training_acc": 60.0, "val_loss": 3023.417236328125, "val_acc": 60.0}
{"epoch": 22, "training_loss": 3731.185009765625, "training_acc": 50.0, "val_loss": 1022.7486572265625, "val_acc": 60.0}
{"epoch": 23, "training_loss": 2549.23291015625, "training_acc": 30.0, "val_loss": 3910.17041015625, "val_acc": 40.0}
{"epoch": 24, "training_loss": 3044.3508544921874, "training_acc": 50.0, "val_loss": 1446.1693115234375, "val_acc": 60.0}
{"epoch": 25, "training_loss": 1853.346728515625, "training_acc": 50.0, "val_loss": 1319.82861328125, "val_acc": 60.0}
{"epoch": 26, "training_loss": 1217.64140625, "training_acc": 60.0, "val_loss": 2048.7431640625, "val_acc": 40.0}
{"epoch": 27, "training_loss": 1508.0915283203126, "training_acc": 50.0, "val_loss": 1690.2523193359375, "val_acc": 60.0}
{"epoch": 28, "training_loss": 2223.9275390625, "training_acc": 50.0, "val_loss": 919.1975708007812, "val_acc": 60.0}
{"epoch": 29, "training_loss": 998.737109375, "training_acc": 60.0, "val_loss": 4437.96240234375, "val_acc": 40.0}
{"epoch": 30, "training_loss": 3623.1910888671873, "training_acc": 50.0, "val_loss": 1440.2947998046875, "val_acc": 40.0}
{"epoch": 31, "training_loss": 1433.732080078125, "training_acc": 50.0, "val_loss": 2839.78173828125, "val_acc": 60.0}
{"epoch": 32, "training_loss": 3423.562255859375, "training_acc": 50.0, "val_loss": 52.58536911010742, "val_acc": 60.0}
{"epoch": 33, "training_loss": 410.591064453125, "training_acc": 60.0, "val_loss": 7032.06103515625, "val_acc": 40.0}
{"epoch": 34, "training_loss": 5813.460888671875, "training_acc": 50.0, "val_loss": 4517.5078125, "val_acc": 40.0}
{"epoch": 35, "training_loss": 3044.8521270751953, "training_acc": 50.0, "val_loss": 4038.555908203125, "val_acc": 60.0}
{"epoch": 36, "training_loss": 5275.2009765625, "training_acc": 50.0, "val_loss": 6427.35009765625, "val_acc": 60.0}
{"epoch": 37, "training_loss": 7700.5490234375, "training_acc": 50.0, "val_loss": 2103.48486328125, "val_acc": 60.0}
{"epoch": 38, "training_loss": 2667.22041015625, "training_acc": 50.0, "val_loss": 6766.34130859375, "val_acc": 40.0}
{"epoch": 39, "training_loss": 5645.140869140625, "training_acc": 50.0, "val_loss": 5452.57470703125, "val_acc": 40.0}
{"epoch": 40, "training_loss": 3897.8598876953124, "training_acc": 50.0, "val_loss": 2892.16455078125, "val_acc": 60.0}
{"epoch": 41, "training_loss": 3821.680859375, "training_acc": 50.0, "val_loss": 5037.58349609375, "val_acc": 60.0}
{"epoch": 42, "training_loss": 5862.29013671875, "training_acc": 50.0, "val_loss": 310.40869140625, "val_acc": 60.0}
{"epoch": 43, "training_loss": 1430.98818359375, "training_acc": 50.0, "val_loss": 10556.8505859375, "val_acc": 40.0}
{"epoch": 44, "training_loss": 8823.6193359375, "training_acc": 50.0, "val_loss": 9514.8701171875, "val_acc": 40.0}
{"epoch": 45, "training_loss": 7261.408984375, "training_acc": 50.0, "val_loss": 460.3290710449219, "val_acc": 60.0}
{"epoch": 46, "training_loss": 1839.014306640625, "training_acc": 50.0, "val_loss": 1993.3883056640625, "val_acc": 60.0}
{"epoch": 47, "training_loss": 2128.6101318359374, "training_acc": 50.0, "val_loss": 1874.703125, "val_acc": 40.0}
{"epoch": 48, "training_loss": 1416.1565856933594, "training_acc": 50.0, "val_loss": 1759.0302734375, "val_acc": 60.0}
{"epoch": 49, "training_loss": 2278.8314453125, "training_acc": 50.0, "val_loss": 387.9055480957031, "val_acc": 40.0}
{"epoch": 50, "training_loss": 561.550244140625, "training_acc": 50.0, "val_loss": 900.6726684570312, "val_acc": 60.0}
{"epoch": 51, "training_loss": 1132.689111328125, "training_acc": 50.0, "val_loss": 966.5546875, "val_acc": 40.0}
