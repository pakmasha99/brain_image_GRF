"main_modify.py --pretrained_path None --mode finetuning --train_num 20 --layer_control freeze --stratify strat --random_seed 1 --task ADNI_sex --input_option yAware --learning_rate 1e2 --batch_size 16 --weight_decay 1e-3 --BN inst --save_path finetune_results_InstanceBN"
{"epoch": 0, "training_loss": 6434.603401088714, "training_acc": 55.0, "val_loss": 8520.2685546875, "val_acc": 40.0}
{"epoch": 1, "training_loss": 6785.61953125, "training_acc": 55.0, "val_loss": 11854.4345703125, "val_acc": 60.0}
{"epoch": 2, "training_loss": 16087.015234375, "training_acc": 45.0, "val_loss": 6588.01123046875, "val_acc": 60.0}
{"epoch": 3, "training_loss": 6776.323828125, "training_acc": 45.0, "val_loss": 11826.0478515625, "val_acc": 40.0}
{"epoch": 4, "training_loss": 8876.55234375, "training_acc": 55.0, "val_loss": 22337.724609375, "val_acc": 40.0}
{"epoch": 5, "training_loss": 16725.3951171875, "training_acc": 55.0, "val_loss": 18663.326171875, "val_acc": 40.0}
{"epoch": 6, "training_loss": 12555.923046875, "training_acc": 55.0, "val_loss": 3331.390380859375, "val_acc": 40.0}
{"epoch": 7, "training_loss": 5190.20234375, "training_acc": 35.0, "val_loss": 8272.33984375, "val_acc": 60.0}
{"epoch": 8, "training_loss": 11610.516015625, "training_acc": 45.0, "val_loss": 6854.0595703125, "val_acc": 60.0}
{"epoch": 9, "training_loss": 8479.25546875, "training_acc": 45.0, "val_loss": 2774.816650390625, "val_acc": 40.0}
{"epoch": 10, "training_loss": 2857.38232421875, "training_acc": 55.0, "val_loss": 9093.5068359375, "val_acc": 40.0}
{"epoch": 11, "training_loss": 6704.2154296875, "training_acc": 55.0, "val_loss": 5176.85107421875, "val_acc": 40.0}
{"epoch": 12, "training_loss": 3062.0567260742187, "training_acc": 55.0, "val_loss": 4258.40576171875, "val_acc": 60.0}
{"epoch": 13, "training_loss": 6098.988916015625, "training_acc": 45.0, "val_loss": 7075.640625, "val_acc": 60.0}
{"epoch": 14, "training_loss": 9361.9650390625, "training_acc": 45.0, "val_loss": 2912.026123046875, "val_acc": 60.0}
{"epoch": 15, "training_loss": 4315.9021484375, "training_acc": 35.0, "val_loss": 5794.80859375, "val_acc": 40.0}
{"epoch": 16, "training_loss": 4772.837109375, "training_acc": 55.0, "val_loss": 3857.400390625, "val_acc": 40.0}
{"epoch": 17, "training_loss": 2411.680310058594, "training_acc": 55.0, "val_loss": 2166.728515625, "val_acc": 60.0}
{"epoch": 18, "training_loss": 2926.0094482421873, "training_acc": 45.0, "val_loss": 634.189208984375, "val_acc": 60.0}
{"epoch": 19, "training_loss": 1580.951123046875, "training_acc": 35.0, "val_loss": 3654.943359375, "val_acc": 40.0}
{"epoch": 20, "training_loss": 2438.054638671875, "training_acc": 55.0, "val_loss": 1194.8857421875, "val_acc": 60.0}
{"epoch": 21, "training_loss": 1885.024609375, "training_acc": 45.0, "val_loss": 1599.545166015625, "val_acc": 60.0}
{"epoch": 22, "training_loss": 1924.2626953125, "training_acc": 45.0, "val_loss": 1519.5653076171875, "val_acc": 40.0}
{"epoch": 23, "training_loss": 947.5931640625, "training_acc": 55.0, "val_loss": 1840.2938232421875, "val_acc": 60.0}
{"epoch": 24, "training_loss": 2540.500244140625, "training_acc": 45.0, "val_loss": 1234.6661376953125, "val_acc": 60.0}
{"epoch": 25, "training_loss": 1636.668603515625, "training_acc": 45.0, "val_loss": 2357.864501953125, "val_acc": 40.0}
{"epoch": 26, "training_loss": 1553.9958984375, "training_acc": 55.0, "val_loss": 1645.5589599609375, "val_acc": 60.0}
{"epoch": 27, "training_loss": 2450.49208984375, "training_acc": 45.0, "val_loss": 1590.3359375, "val_acc": 60.0}
{"epoch": 28, "training_loss": 1663.9808837890625, "training_acc": 55.0, "val_loss": 2768.26611328125, "val_acc": 40.0}
{"epoch": 29, "training_loss": 2079.31796875, "training_acc": 55.0, "val_loss": 326.8116455078125, "val_acc": 60.0}
{"epoch": 30, "training_loss": 653.3964111328125, "training_acc": 45.0, "val_loss": 1166.4466552734375, "val_acc": 40.0}
{"epoch": 31, "training_loss": 1026.76875, "training_acc": 55.0, "val_loss": 603.1768798828125, "val_acc": 60.0}
{"epoch": 32, "training_loss": 830.6199829101563, "training_acc": 45.0, "val_loss": 1217.091064453125, "val_acc": 40.0}
{"epoch": 33, "training_loss": 893.2853332519531, "training_acc": 55.0, "val_loss": 222.15380859375, "val_acc": 60.0}
{"epoch": 34, "training_loss": 565.7137451171875, "training_acc": 35.0, "val_loss": 490.60577392578125, "val_acc": 60.0}
{"epoch": 35, "training_loss": 562.8184631347656, "training_acc": 45.0, "val_loss": 3285.320068359375, "val_acc": 40.0}
{"epoch": 36, "training_loss": 2548.107421875, "training_acc": 55.0, "val_loss": 3923.90625, "val_acc": 40.0}
{"epoch": 37, "training_loss": 2345.752783203125, "training_acc": 55.0, "val_loss": 2517.98046875, "val_acc": 60.0}
{"epoch": 38, "training_loss": 3925.14296875, "training_acc": 45.0, "val_loss": 4478.88232421875, "val_acc": 60.0}
{"epoch": 39, "training_loss": 5593.90400390625, "training_acc": 45.0, "val_loss": 871.1265869140625, "val_acc": 40.0}
{"epoch": 40, "training_loss": 1455.0798828125, "training_acc": 55.0, "val_loss": 3636.845703125, "val_acc": 40.0}
{"epoch": 41, "training_loss": 2509.841961669922, "training_acc": 55.0, "val_loss": 1665.6383056640625, "val_acc": 60.0}
{"epoch": 42, "training_loss": 2521.12197265625, "training_acc": 45.0, "val_loss": 637.9315795898438, "val_acc": 60.0}
{"epoch": 43, "training_loss": 922.893505859375, "training_acc": 55.0, "val_loss": 6178.23388671875, "val_acc": 40.0}
{"epoch": 44, "training_loss": 4799.965625, "training_acc": 55.0, "val_loss": 3956.29150390625, "val_acc": 40.0}
{"epoch": 45, "training_loss": 2011.574444580078, "training_acc": 65.0, "val_loss": 2155.08056640625, "val_acc": 60.0}
{"epoch": 46, "training_loss": 2965.890673828125, "training_acc": 45.0, "val_loss": 671.3903198242188, "val_acc": 60.0}
{"epoch": 47, "training_loss": 1333.15927734375, "training_acc": 45.0, "val_loss": 5215.5947265625, "val_acc": 40.0}
{"epoch": 48, "training_loss": 3832.253759765625, "training_acc": 55.0, "val_loss": 1908.4910888671875, "val_acc": 40.0}
{"epoch": 49, "training_loss": 1984.16025390625, "training_acc": 45.0, "val_loss": 2516.149169921875, "val_acc": 60.0}
{"epoch": 50, "training_loss": 3092.076611328125, "training_acc": 45.0, "val_loss": 2305.090087890625, "val_acc": 40.0}
{"epoch": 51, "training_loss": 2347.5322265625, "training_acc": 55.0, "val_loss": 3580.63720703125, "val_acc": 40.0}
{"epoch": 52, "training_loss": 2408.9299697875977, "training_acc": 55.0, "val_loss": 2592.14306640625, "val_acc": 60.0}
