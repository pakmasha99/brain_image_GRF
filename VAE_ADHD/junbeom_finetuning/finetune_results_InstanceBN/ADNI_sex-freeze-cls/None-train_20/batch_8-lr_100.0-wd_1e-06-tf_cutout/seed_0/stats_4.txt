"main_modify.py --pretrained_path None --mode finetuning --train_num 20 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_sex --input_option yAware --learning_rate 1e2 --batch_size 8 --weight_decay 1e-6 --BN inst --save_path finetune_results_InstanceBN"
{"epoch": 0, "training_loss": 10593.978110313416, "training_acc": 45.0, "val_loss": 4386.98779296875, "val_acc": 60.0}
{"epoch": 1, "training_loss": 12461.0095703125, "training_acc": 45.0, "val_loss": 1785.9520263671875, "val_acc": 60.0}
{"epoch": 2, "training_loss": 5484.68232421875, "training_acc": 65.0, "val_loss": 22309.25390625, "val_acc": 40.0}
{"epoch": 3, "training_loss": 17368.248046875, "training_acc": 55.0, "val_loss": 14027.0224609375, "val_acc": 40.0}
{"epoch": 4, "training_loss": 4336.79296875, "training_acc": 75.0, "val_loss": 9022.8125, "val_acc": 60.0}
{"epoch": 5, "training_loss": 15032.003125, "training_acc": 45.0, "val_loss": 10048.9052734375, "val_acc": 60.0}
{"epoch": 6, "training_loss": 8559.834619140625, "training_acc": 45.0, "val_loss": 9459.46875, "val_acc": 40.0}
{"epoch": 7, "training_loss": 11262.0982421875, "training_acc": 55.0, "val_loss": 21756.201171875, "val_acc": 40.0}
{"epoch": 8, "training_loss": 14953.2845703125, "training_acc": 55.0, "val_loss": 9850.4521484375, "val_acc": 40.0}
{"epoch": 9, "training_loss": 3546.30849609375, "training_acc": 65.0, "val_loss": 9329.666015625, "val_acc": 60.0}
{"epoch": 10, "training_loss": 14371.245703125, "training_acc": 45.0, "val_loss": 9295.8515625, "val_acc": 60.0}
{"epoch": 11, "training_loss": 8042.3062744140625, "training_acc": 45.0, "val_loss": 9358.2646484375, "val_acc": 40.0}
{"epoch": 12, "training_loss": 11683.3328125, "training_acc": 55.0, "val_loss": 21138.5625, "val_acc": 40.0}
{"epoch": 13, "training_loss": 14574.96064453125, "training_acc": 55.0, "val_loss": 7711.51708984375, "val_acc": 40.0}
{"epoch": 14, "training_loss": 3891.1718017578123, "training_acc": 35.0, "val_loss": 2674.287841796875, "val_acc": 60.0}
{"epoch": 15, "training_loss": 2192.4348022460936, "training_acc": 55.0, "val_loss": 4956.71826171875, "val_acc": 40.0}
{"epoch": 16, "training_loss": 3319.63056640625, "training_acc": 55.0, "val_loss": 952.2875366210938, "val_acc": 60.0}
{"epoch": 17, "training_loss": 2845.2179931640626, "training_acc": 45.0, "val_loss": 1139.2401123046875, "val_acc": 40.0}
{"epoch": 18, "training_loss": 1465.2607421875, "training_acc": 55.0, "val_loss": 419.5386657714844, "val_acc": 60.0}
{"epoch": 19, "training_loss": 305.35673370361326, "training_acc": 55.0, "val_loss": 2529.51806640625, "val_acc": 60.0}
{"epoch": 20, "training_loss": 3382.10009765625, "training_acc": 45.0, "val_loss": 2035.6317138671875, "val_acc": 40.0}
{"epoch": 21, "training_loss": 1690.484033203125, "training_acc": 55.0, "val_loss": 402.1241149902344, "val_acc": 60.0}
{"epoch": 22, "training_loss": 883.1209838867187, "training_acc": 45.0, "val_loss": 2268.038330078125, "val_acc": 40.0}
{"epoch": 23, "training_loss": 1553.67119140625, "training_acc": 25.0, "val_loss": 736.338623046875, "val_acc": 60.0}
{"epoch": 24, "training_loss": 687.213427734375, "training_acc": 65.0, "val_loss": 3133.146484375, "val_acc": 40.0}
{"epoch": 25, "training_loss": 1712.1060363769532, "training_acc": 55.0, "val_loss": 2832.015625, "val_acc": 60.0}
{"epoch": 26, "training_loss": 4239.242236328125, "training_acc": 45.0, "val_loss": 23.936262130737305, "val_acc": 60.0}
{"epoch": 27, "training_loss": 4185.032749938965, "training_acc": 35.0, "val_loss": 9177.41015625, "val_acc": 40.0}
{"epoch": 28, "training_loss": 5310.93544921875, "training_acc": 55.0, "val_loss": 1750.3909912109375, "val_acc": 60.0}
{"epoch": 29, "training_loss": 3550.8128173828127, "training_acc": 45.0, "val_loss": 1437.0001220703125, "val_acc": 60.0}
{"epoch": 30, "training_loss": 1674.4465087890626, "training_acc": 55.0, "val_loss": 3444.888671875, "val_acc": 40.0}
{"epoch": 31, "training_loss": 1500.2761352539062, "training_acc": 65.0, "val_loss": 2809.48291015625, "val_acc": 60.0}
{"epoch": 32, "training_loss": 2898.962451171875, "training_acc": 45.0, "val_loss": 2262.954345703125, "val_acc": 40.0}
{"epoch": 33, "training_loss": 1311.528173828125, "training_acc": 55.0, "val_loss": 1929.018798828125, "val_acc": 60.0}
{"epoch": 34, "training_loss": 1264.8884765625, "training_acc": 65.0, "val_loss": 5243.7265625, "val_acc": 40.0}
{"epoch": 35, "training_loss": 4662.23671875, "training_acc": 55.0, "val_loss": 1417.65966796875, "val_acc": 40.0}
{"epoch": 36, "training_loss": 3935.70126953125, "training_acc": 45.0, "val_loss": 5374.34375, "val_acc": 60.0}
{"epoch": 37, "training_loss": 5530.786926269531, "training_acc": 45.0, "val_loss": 3285.111328125, "val_acc": 40.0}
{"epoch": 38, "training_loss": 2389.9599609375, "training_acc": 55.0, "val_loss": 2183.400390625, "val_acc": 60.0}
{"epoch": 39, "training_loss": 3396.25107421875, "training_acc": 45.0, "val_loss": 734.363037109375, "val_acc": 40.0}
{"epoch": 40, "training_loss": 1055.479296875, "training_acc": 55.0, "val_loss": 704.3961181640625, "val_acc": 60.0}
{"epoch": 41, "training_loss": 905.34462890625, "training_acc": 35.0, "val_loss": 102.25089263916016, "val_acc": 40.0}
{"epoch": 42, "training_loss": 915.5583679199219, "training_acc": 40.0, "val_loss": 599.7255249023438, "val_acc": 40.0}
{"epoch": 43, "training_loss": 2053.8037963867187, "training_acc": 45.0, "val_loss": 414.1142578125, "val_acc": 60.0}
{"epoch": 44, "training_loss": 4400.743237304688, "training_acc": 35.0, "val_loss": 8213.431640625, "val_acc": 40.0}
{"epoch": 45, "training_loss": 4084.86123046875, "training_acc": 55.0, "val_loss": 3907.201171875, "val_acc": 60.0}
