"main_modify.py --pretrained_path None --mode finetuning --train_num 20 --layer_control freeze --stratify strat --random_seed 2 --task ADNI_sex --input_option yAware --learning_rate 1e2 --batch_size 16 --weight_decay 1e-6 --BN inst --save_path finetune_results_InstanceBN"
{"epoch": 0, "training_loss": 4296.023375892639, "training_acc": 50.0, "val_loss": 5812.12255859375, "val_acc": 60.0}
{"epoch": 1, "training_loss": 9621.7982421875, "training_acc": 40.0, "val_loss": 14000.3779296875, "val_acc": 40.0}
{"epoch": 2, "training_loss": 10884.7693359375, "training_acc": 50.0, "val_loss": 301.3001403808594, "val_acc": 40.0}
{"epoch": 3, "training_loss": 2162.3854125976563, "training_acc": 50.0, "val_loss": 11593.3359375, "val_acc": 60.0}
{"epoch": 4, "training_loss": 14782.68046875, "training_acc": 50.0, "val_loss": 9305.9716796875, "val_acc": 60.0}
{"epoch": 5, "training_loss": 9861.821484375, "training_acc": 50.0, "val_loss": 3871.696533203125, "val_acc": 40.0}
{"epoch": 6, "training_loss": 3707.731884765625, "training_acc": 50.0, "val_loss": 13251.2080078125, "val_acc": 40.0}
{"epoch": 7, "training_loss": 11026.36044921875, "training_acc": 50.0, "val_loss": 11265.96875, "val_acc": 40.0}
{"epoch": 8, "training_loss": 8988.818505859375, "training_acc": 50.0, "val_loss": 619.53173828125, "val_acc": 40.0}
{"epoch": 9, "training_loss": 3193.231689453125, "training_acc": 30.0, "val_loss": 6635.5048828125, "val_acc": 60.0}
{"epoch": 10, "training_loss": 8151.494921875, "training_acc": 50.0, "val_loss": 3370.199951171875, "val_acc": 60.0}
{"epoch": 11, "training_loss": 4196.435766601562, "training_acc": 40.0, "val_loss": 3792.57275390625, "val_acc": 40.0}
{"epoch": 12, "training_loss": 3077.151513671875, "training_acc": 50.0, "val_loss": 412.7803649902344, "val_acc": 40.0}
{"epoch": 13, "training_loss": 599.2326416015625, "training_acc": 60.0, "val_loss": 4917.49951171875, "val_acc": 60.0}
{"epoch": 14, "training_loss": 6453.698046875, "training_acc": 50.0, "val_loss": 2906.318115234375, "val_acc": 60.0}
{"epoch": 15, "training_loss": 3164.960888671875, "training_acc": 50.0, "val_loss": 4489.3056640625, "val_acc": 40.0}
{"epoch": 16, "training_loss": 3787.883203125, "training_acc": 50.0, "val_loss": 2682.135498046875, "val_acc": 40.0}
{"epoch": 17, "training_loss": 1338.86875, "training_acc": 70.0, "val_loss": 2894.686279296875, "val_acc": 60.0}
{"epoch": 18, "training_loss": 3722.26435546875, "training_acc": 50.0, "val_loss": 2070.1630859375, "val_acc": 60.0}
{"epoch": 19, "training_loss": 1868.8888916015626, "training_acc": 60.0, "val_loss": 2879.393310546875, "val_acc": 40.0}
{"epoch": 20, "training_loss": 2323.6805908203123, "training_acc": 50.0, "val_loss": 251.72207641601562, "val_acc": 60.0}
{"epoch": 21, "training_loss": 418.00450439453124, "training_acc": 50.0, "val_loss": 2442.461181640625, "val_acc": 40.0}
{"epoch": 22, "training_loss": 2050.8447021484376, "training_acc": 50.0, "val_loss": 1628.1214599609375, "val_acc": 40.0}
{"epoch": 23, "training_loss": 1066.5776733398438, "training_acc": 60.0, "val_loss": 1970.7978515625, "val_acc": 60.0}
{"epoch": 24, "training_loss": 2286.06630859375, "training_acc": 50.0, "val_loss": 1505.122314453125, "val_acc": 40.0}
{"epoch": 25, "training_loss": 1353.247265625, "training_acc": 50.0, "val_loss": 218.2985076904297, "val_acc": 40.0}
{"epoch": 26, "training_loss": 430.8710693359375, "training_acc": 60.0, "val_loss": 3938.680419921875, "val_acc": 60.0}
{"epoch": 27, "training_loss": 4848.6228515625, "training_acc": 50.0, "val_loss": 1734.0360107421875, "val_acc": 60.0}
{"epoch": 28, "training_loss": 2138.6996337890623, "training_acc": 50.0, "val_loss": 4535.09130859375, "val_acc": 40.0}
{"epoch": 29, "training_loss": 3701.6694091796876, "training_acc": 50.0, "val_loss": 1532.1370849609375, "val_acc": 40.0}
{"epoch": 30, "training_loss": 764.24248046875, "training_acc": 70.0, "val_loss": 3752.205078125, "val_acc": 60.0}
{"epoch": 31, "training_loss": 4781.64404296875, "training_acc": 50.0, "val_loss": 2759.685546875, "val_acc": 60.0}
{"epoch": 32, "training_loss": 2791.6289337158205, "training_acc": 50.0, "val_loss": 1604.4130859375, "val_acc": 40.0}
{"epoch": 33, "training_loss": 1205.6098106384277, "training_acc": 50.0, "val_loss": 1781.0745849609375, "val_acc": 60.0}
{"epoch": 34, "training_loss": 2184.35224609375, "training_acc": 50.0, "val_loss": 401.0707702636719, "val_acc": 60.0}
{"epoch": 35, "training_loss": 1381.20556640625, "training_acc": 40.0, "val_loss": 3826.323486328125, "val_acc": 40.0}
{"epoch": 36, "training_loss": 2959.380267333984, "training_acc": 50.0, "val_loss": 1773.6343994140625, "val_acc": 60.0}
{"epoch": 37, "training_loss": 2349.112255859375, "training_acc": 50.0, "val_loss": 1159.7972412109375, "val_acc": 60.0}
{"epoch": 38, "training_loss": 867.6361328125, "training_acc": 70.0, "val_loss": 4917.857421875, "val_acc": 40.0}
{"epoch": 39, "training_loss": 4098.53623046875, "training_acc": 50.0, "val_loss": 3594.060546875, "val_acc": 40.0}
{"epoch": 40, "training_loss": 2419.2273193359374, "training_acc": 50.0, "val_loss": 620.0354614257812, "val_acc": 60.0}
{"epoch": 41, "training_loss": 764.6447631835938, "training_acc": 50.0, "val_loss": 2.578890323638916, "val_acc": 80.0}
{"epoch": 42, "training_loss": 98.22107028961182, "training_acc": 80.0, "val_loss": 614.472900390625, "val_acc": 60.0}
{"epoch": 43, "training_loss": 1595.08544921875, "training_acc": 30.0, "val_loss": 474.19464111328125, "val_acc": 40.0}
{"epoch": 44, "training_loss": 1397.144580078125, "training_acc": 40.0, "val_loss": 3069.024169921875, "val_acc": 60.0}
{"epoch": 45, "training_loss": 3829.6171875, "training_acc": 50.0, "val_loss": 664.5007934570312, "val_acc": 40.0}
{"epoch": 46, "training_loss": 674.9631103515625, "training_acc": 50.0, "val_loss": 2411.55810546875, "val_acc": 60.0}
{"epoch": 47, "training_loss": 3509.3048828125, "training_acc": 50.0, "val_loss": 2476.410888671875, "val_acc": 60.0}
{"epoch": 48, "training_loss": 2173.951686096191, "training_acc": 60.0, "val_loss": 2453.668701171875, "val_acc": 40.0}
{"epoch": 49, "training_loss": 1965.7130859375, "training_acc": 50.0, "val_loss": 587.5546875, "val_acc": 60.0}
{"epoch": 50, "training_loss": 833.33359375, "training_acc": 50.0, "val_loss": 2030.4246826171875, "val_acc": 40.0}
{"epoch": 51, "training_loss": 1710.6548706054687, "training_acc": 50.0, "val_loss": 1198.4698486328125, "val_acc": 40.0}
{"epoch": 52, "training_loss": 1165.1279296875, "training_acc": 50.0, "val_loss": 1885.9501953125, "val_acc": 60.0}
{"epoch": 53, "training_loss": 2019.358984375, "training_acc": 50.0, "val_loss": 3370.931396484375, "val_acc": 40.0}
{"epoch": 54, "training_loss": 3212.89384765625, "training_acc": 50.0, "val_loss": 2767.85595703125, "val_acc": 40.0}
{"epoch": 55, "training_loss": 1720.8946411132813, "training_acc": 60.0, "val_loss": 2653.91259765625, "val_acc": 60.0}
{"epoch": 56, "training_loss": 3281.66708984375, "training_acc": 50.0, "val_loss": 731.0692749023438, "val_acc": 60.0}
{"epoch": 57, "training_loss": 928.3399658203125, "training_acc": 60.0, "val_loss": 6411.02099609375, "val_acc": 40.0}
{"epoch": 58, "training_loss": 5320.724560546875, "training_acc": 50.0, "val_loss": 4750.77685546875, "val_acc": 40.0}
{"epoch": 59, "training_loss": 3313.7438110351563, "training_acc": 50.0, "val_loss": 3349.107177734375, "val_acc": 60.0}
{"epoch": 60, "training_loss": 4387.025048828125, "training_acc": 50.0, "val_loss": 5715.00244140625, "val_acc": 60.0}
