"main_modify.py --pretrained_path None --mode finetuning --train_num 20 --layer_control freeze --stratify strat --random_seed 2 --task ADNI_sex --input_option yAware --learning_rate 1e2 --batch_size 16 --weight_decay 1e-6 --BN inst --save_path finetune_results_InstanceBN"
{"epoch": 0, "training_loss": 4277.891020870209, "training_acc": 65.0, "val_loss": 8839.2919921875, "val_acc": 40.0}
{"epoch": 1, "training_loss": 7112.06953125, "training_acc": 55.0, "val_loss": 10855.0732421875, "val_acc": 60.0}
{"epoch": 2, "training_loss": 13880.06640625, "training_acc": 45.0, "val_loss": 1043.1409912109375, "val_acc": 60.0}
{"epoch": 3, "training_loss": 2074.6974609375, "training_acc": 55.0, "val_loss": 22395.123046875, "val_acc": 40.0}
{"epoch": 4, "training_loss": 16795.7140625, "training_acc": 55.0, "val_loss": 29642.263671875, "val_acc": 40.0}
{"epoch": 5, "training_loss": 22174.865625, "training_acc": 55.0, "val_loss": 25387.69921875, "val_acc": 40.0}
{"epoch": 6, "training_loss": 17956.5470703125, "training_acc": 55.0, "val_loss": 9913.5927734375, "val_acc": 40.0}
{"epoch": 7, "training_loss": 6911.180200195313, "training_acc": 45.0, "val_loss": 4016.260986328125, "val_acc": 60.0}
{"epoch": 8, "training_loss": 5652.2673828125, "training_acc": 45.0, "val_loss": 3109.71484375, "val_acc": 60.0}
{"epoch": 9, "training_loss": 3571.1808471679688, "training_acc": 45.0, "val_loss": 2249.421142578125, "val_acc": 40.0}
{"epoch": 10, "training_loss": 1578.509454345703, "training_acc": 55.0, "val_loss": 1013.189453125, "val_acc": 60.0}
{"epoch": 11, "training_loss": 1236.8997314453125, "training_acc": 45.0, "val_loss": 2886.348388671875, "val_acc": 40.0}
{"epoch": 12, "training_loss": 2284.378173828125, "training_acc": 55.0, "val_loss": 1632.6845703125, "val_acc": 40.0}
{"epoch": 13, "training_loss": 1798.57578125, "training_acc": 45.0, "val_loss": 1929.7506103515625, "val_acc": 60.0}
{"epoch": 14, "training_loss": 2480.190496826172, "training_acc": 35.0, "val_loss": 419.07989501953125, "val_acc": 60.0}
{"epoch": 15, "training_loss": 358.43818359375, "training_acc": 65.0, "val_loss": 1971.494140625, "val_acc": 40.0}
{"epoch": 16, "training_loss": 1276.79560546875, "training_acc": 55.0, "val_loss": 1835.514892578125, "val_acc": 60.0}
{"epoch": 17, "training_loss": 2562.16083984375, "training_acc": 45.0, "val_loss": 572.3949584960938, "val_acc": 60.0}
{"epoch": 18, "training_loss": 1297.528564453125, "training_acc": 45.0, "val_loss": 5832.1162109375, "val_acc": 40.0}
{"epoch": 19, "training_loss": 4302.0818359375, "training_acc": 55.0, "val_loss": 1625.26220703125, "val_acc": 40.0}
{"epoch": 20, "training_loss": 2181.20537109375, "training_acc": 45.0, "val_loss": 4094.077880859375, "val_acc": 60.0}
{"epoch": 21, "training_loss": 5479.782080078125, "training_acc": 45.0, "val_loss": 1308.3841552734375, "val_acc": 60.0}
{"epoch": 22, "training_loss": 1531.297119140625, "training_acc": 55.0, "val_loss": 6037.1533203125, "val_acc": 40.0}
{"epoch": 23, "training_loss": 4541.237890625, "training_acc": 55.0, "val_loss": 5442.57373046875, "val_acc": 40.0}
{"epoch": 24, "training_loss": 3789.4372314453126, "training_acc": 55.0, "val_loss": 1484.2060546875, "val_acc": 60.0}
{"epoch": 25, "training_loss": 2081.0250244140625, "training_acc": 45.0, "val_loss": 1548.162109375, "val_acc": 60.0}
{"epoch": 26, "training_loss": 1342.1470703125, "training_acc": 65.0, "val_loss": 2992.216552734375, "val_acc": 40.0}
{"epoch": 27, "training_loss": 2202.4048583984377, "training_acc": 55.0, "val_loss": 1157.116455078125, "val_acc": 40.0}
{"epoch": 28, "training_loss": 1460.310693359375, "training_acc": 45.0, "val_loss": 1698.6500244140625, "val_acc": 60.0}
{"epoch": 29, "training_loss": 1699.4267883300781, "training_acc": 55.0, "val_loss": 1534.0618896484375, "val_acc": 40.0}
{"epoch": 30, "training_loss": 946.814599609375, "training_acc": 55.0, "val_loss": 2112.248779296875, "val_acc": 60.0}
{"epoch": 31, "training_loss": 2931.481787109375, "training_acc": 45.0, "val_loss": 817.6921997070312, "val_acc": 60.0}
{"epoch": 32, "training_loss": 1993.15673828125, "training_acc": 35.0, "val_loss": 4879.18896484375, "val_acc": 40.0}
{"epoch": 33, "training_loss": 3493.9165771484377, "training_acc": 55.0, "val_loss": 289.6697692871094, "val_acc": 40.0}
{"epoch": 34, "training_loss": 1899.7469848632813, "training_acc": 35.0, "val_loss": 3304.990234375, "val_acc": 60.0}
{"epoch": 35, "training_loss": 3853.58544921875, "training_acc": 45.0, "val_loss": 3793.388427734375, "val_acc": 40.0}
{"epoch": 36, "training_loss": 3729.287109375, "training_acc": 55.0, "val_loss": 6975.66748046875, "val_acc": 40.0}
{"epoch": 37, "training_loss": 4972.309716796875, "training_acc": 55.0, "val_loss": 318.6654968261719, "val_acc": 40.0}
{"epoch": 38, "training_loss": 1589.5741577148438, "training_acc": 45.0, "val_loss": 4809.93701171875, "val_acc": 60.0}
{"epoch": 39, "training_loss": 6444.217138671875, "training_acc": 45.0, "val_loss": 1936.589111328125, "val_acc": 60.0}
{"epoch": 40, "training_loss": 2071.2463500976564, "training_acc": 55.0, "val_loss": 5190.9140625, "val_acc": 40.0}
{"epoch": 41, "training_loss": 3988.8921875, "training_acc": 55.0, "val_loss": 4051.960205078125, "val_acc": 40.0}
{"epoch": 42, "training_loss": 2699.5998260498045, "training_acc": 45.0, "val_loss": 84.39653778076172, "val_acc": 60.0}
{"epoch": 43, "training_loss": 256.632177734375, "training_acc": 55.0, "val_loss": 3352.75634765625, "val_acc": 40.0}
{"epoch": 44, "training_loss": 2355.949560546875, "training_acc": 55.0, "val_loss": 632.5204467773438, "val_acc": 60.0}
{"epoch": 45, "training_loss": 764.1178405761718, "training_acc": 45.0, "val_loss": 2306.1044921875, "val_acc": 40.0}
{"epoch": 46, "training_loss": 1701.9892211914062, "training_acc": 55.0, "val_loss": 1179.9866943359375, "val_acc": 40.0}
{"epoch": 47, "training_loss": 1010.28271484375, "training_acc": 55.0, "val_loss": 1704.1234130859375, "val_acc": 60.0}
{"epoch": 48, "training_loss": 1890.7147308349608, "training_acc": 45.0, "val_loss": 4571.88427734375, "val_acc": 40.0}
{"epoch": 49, "training_loss": 3747.4046875, "training_acc": 55.0, "val_loss": 5915.04638671875, "val_acc": 40.0}
{"epoch": 50, "training_loss": 3791.8701171875, "training_acc": 55.0, "val_loss": 1725.6082763671875, "val_acc": 60.0}
{"epoch": 51, "training_loss": 2763.71572265625, "training_acc": 45.0, "val_loss": 3314.472412109375, "val_acc": 60.0}
{"epoch": 52, "training_loss": 3853.2796875, "training_acc": 45.0, "val_loss": 3818.729736328125, "val_acc": 40.0}
{"epoch": 53, "training_loss": 2833.69140625, "training_acc": 55.0, "val_loss": 8943.529296875, "val_acc": 40.0}
{"epoch": 54, "training_loss": 6681.79091796875, "training_acc": 55.0, "val_loss": 6481.3974609375, "val_acc": 40.0}
{"epoch": 55, "training_loss": 4091.95283203125, "training_acc": 55.0, "val_loss": 2967.628173828125, "val_acc": 60.0}
{"epoch": 56, "training_loss": 4249.8408203125, "training_acc": 45.0, "val_loss": 5556.8330078125, "val_acc": 60.0}
{"epoch": 57, "training_loss": 7231.66953125, "training_acc": 45.0, "val_loss": 1262.29150390625, "val_acc": 60.0}
{"epoch": 58, "training_loss": 1651.1041015625, "training_acc": 55.0, "val_loss": 10225.4541015625, "val_acc": 40.0}
{"epoch": 59, "training_loss": 8303.05234375, "training_acc": 55.0, "val_loss": 11611.083984375, "val_acc": 40.0}
{"epoch": 60, "training_loss": 8372.947802734376, "training_acc": 55.0, "val_loss": 3379.214111328125, "val_acc": 40.0}
{"epoch": 61, "training_loss": 2423.1708984375, "training_acc": 55.0, "val_loss": 4098.99365234375, "val_acc": 60.0}
