"main_modify.py --pretrained_path None --mode finetuning --train_num 20 --layer_control freeze --stratify strat --random_seed 3 --task ADNI_sex --input_option yAware --learning_rate 1e2 --batch_size 16 --weight_decay 1e-2 --BN inst --save_path finetune_results_InstanceBN"
{"epoch": 0, "training_loss": 4271.24783782959, "training_acc": 50.0, "val_loss": 4766.9755859375, "val_acc": 60.0}
{"epoch": 1, "training_loss": 8465.7126953125, "training_acc": 40.0, "val_loss": 18384.01953125, "val_acc": 40.0}
{"epoch": 2, "training_loss": 15057.69140625, "training_acc": 50.0, "val_loss": 7960.93310546875, "val_acc": 40.0}
{"epoch": 3, "training_loss": 5571.5796875, "training_acc": 50.0, "val_loss": 5483.47021484375, "val_acc": 60.0}
{"epoch": 4, "training_loss": 7518.267578125, "training_acc": 50.0, "val_loss": 6006.97265625, "val_acc": 60.0}
{"epoch": 5, "training_loss": 7101.1724609375, "training_acc": 50.0, "val_loss": 1559.6771240234375, "val_acc": 40.0}
{"epoch": 6, "training_loss": 2261.2033203125, "training_acc": 50.0, "val_loss": 6049.9521484375, "val_acc": 40.0}
{"epoch": 7, "training_loss": 5039.101953125, "training_acc": 50.0, "val_loss": 1432.918212890625, "val_acc": 40.0}
{"epoch": 8, "training_loss": 1684.653125, "training_acc": 40.0, "val_loss": 2536.50634765625, "val_acc": 60.0}
{"epoch": 9, "training_loss": 3066.2013671875, "training_acc": 50.0, "val_loss": 611.5607299804688, "val_acc": 60.0}
{"epoch": 10, "training_loss": 1322.96123046875, "training_acc": 40.0, "val_loss": 3517.263427734375, "val_acc": 40.0}
{"epoch": 11, "training_loss": 2842.45791015625, "training_acc": 50.0, "val_loss": 459.2725524902344, "val_acc": 40.0}
{"epoch": 12, "training_loss": 731.212451171875, "training_acc": 50.0, "val_loss": 2430.742919921875, "val_acc": 60.0}
{"epoch": 13, "training_loss": 2888.14482421875, "training_acc": 50.0, "val_loss": 178.91200256347656, "val_acc": 60.0}
{"epoch": 14, "training_loss": 708.4442260742187, "training_acc": 50.0, "val_loss": 4488.16552734375, "val_acc": 40.0}
{"epoch": 15, "training_loss": 3653.74921875, "training_acc": 50.0, "val_loss": 1185.9482421875, "val_acc": 40.0}
{"epoch": 16, "training_loss": 868.7246337890625, "training_acc": 60.0, "val_loss": 2728.899658203125, "val_acc": 60.0}
{"epoch": 17, "training_loss": 3360.26728515625, "training_acc": 50.0, "val_loss": 1033.773681640625, "val_acc": 60.0}
{"epoch": 18, "training_loss": 1369.6593017578125, "training_acc": 50.0, "val_loss": 3617.13525390625, "val_acc": 40.0}
{"epoch": 19, "training_loss": 2954.907470703125, "training_acc": 50.0, "val_loss": 1027.66650390625, "val_acc": 40.0}
{"epoch": 20, "training_loss": 1039.39697265625, "training_acc": 50.0, "val_loss": 2188.97900390625, "val_acc": 60.0}
{"epoch": 21, "training_loss": 2542.02294921875, "training_acc": 50.0, "val_loss": 586.1984252929688, "val_acc": 40.0}
{"epoch": 22, "training_loss": 888.1007080078125, "training_acc": 50.0, "val_loss": 1270.0643310546875, "val_acc": 40.0}
{"epoch": 23, "training_loss": 819.0770568847656, "training_acc": 60.0, "val_loss": 1401.504150390625, "val_acc": 60.0}
{"epoch": 24, "training_loss": 1678.350732421875, "training_acc": 50.0, "val_loss": 466.4300842285156, "val_acc": 40.0}
{"epoch": 25, "training_loss": 388.544921875, "training_acc": 50.0, "val_loss": 194.4530792236328, "val_acc": 60.0}
{"epoch": 26, "training_loss": 188.9104232788086, "training_acc": 60.0, "val_loss": 7.767205238342285, "val_acc": 60.0}
{"epoch": 27, "training_loss": 189.7943542480469, "training_acc": 50.0, "val_loss": 492.83642578125, "val_acc": 40.0}
{"epoch": 28, "training_loss": 246.3847900390625, "training_acc": 70.0, "val_loss": 1593.951416015625, "val_acc": 60.0}
{"epoch": 29, "training_loss": 1848.7830078125, "training_acc": 50.0, "val_loss": 975.9459228515625, "val_acc": 40.0}
{"epoch": 30, "training_loss": 923.828662109375, "training_acc": 50.0, "val_loss": 451.3265075683594, "val_acc": 40.0}
{"epoch": 31, "training_loss": 652.7936767578125, "training_acc": 50.0, "val_loss": 1674.255126953125, "val_acc": 60.0}
{"epoch": 32, "training_loss": 1714.931396484375, "training_acc": 50.0, "val_loss": 2687.673828125, "val_acc": 40.0}
{"epoch": 33, "training_loss": 2593.3267578125, "training_acc": 50.0, "val_loss": 2242.044921875, "val_acc": 40.0}
{"epoch": 34, "training_loss": 2077.5205322265624, "training_acc": 40.0, "val_loss": 1691.4263916015625, "val_acc": 60.0}
{"epoch": 35, "training_loss": 1829.45234375, "training_acc": 50.0, "val_loss": 2291.68408203125, "val_acc": 40.0}
{"epoch": 36, "training_loss": 2176.368408203125, "training_acc": 50.0, "val_loss": 2929.752685546875, "val_acc": 40.0}
{"epoch": 37, "training_loss": 1969.4815994262694, "training_acc": 50.0, "val_loss": 940.2601928710938, "val_acc": 60.0}
{"epoch": 38, "training_loss": 1109.7516540527345, "training_acc": 50.0, "val_loss": 941.0364379882812, "val_acc": 40.0}
{"epoch": 39, "training_loss": 742.834228515625, "training_acc": 50.0, "val_loss": 760.9635620117188, "val_acc": 60.0}
{"epoch": 40, "training_loss": 937.275341796875, "training_acc": 50.0, "val_loss": 731.39306640625, "val_acc": 40.0}
{"epoch": 41, "training_loss": 649.580419921875, "training_acc": 50.0, "val_loss": 916.0398559570312, "val_acc": 60.0}
{"epoch": 42, "training_loss": 1143.8244140625, "training_acc": 50.0, "val_loss": 520.5398559570312, "val_acc": 60.0}
{"epoch": 43, "training_loss": 956.896728515625, "training_acc": 40.0, "val_loss": 932.3410034179688, "val_acc": 40.0}
{"epoch": 44, "training_loss": 835.9900390625, "training_acc": 50.0, "val_loss": 981.5033569335938, "val_acc": 60.0}
{"epoch": 45, "training_loss": 1020.3453308105469, "training_acc": 50.0, "val_loss": 241.2237548828125, "val_acc": 40.0}
