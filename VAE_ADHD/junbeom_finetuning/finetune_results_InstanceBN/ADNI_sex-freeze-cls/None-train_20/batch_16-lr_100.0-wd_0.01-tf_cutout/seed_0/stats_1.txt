"main_modify.py --pretrained_path None --mode finetuning --train_num 20 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_sex --input_option yAware --learning_rate 1e2 --batch_size 16 --weight_decay 1e-2 --BN inst --save_path finetune_results_InstanceBN"
{"epoch": 0, "training_loss": 4284.884644889831, "training_acc": 50.0, "val_loss": 6746.1171875, "val_acc": 40.0}
{"epoch": 1, "training_loss": 4995.15625, "training_acc": 60.0, "val_loss": 13886.955078125, "val_acc": 60.0}
{"epoch": 2, "training_loss": 16931.7587890625, "training_acc": 50.0, "val_loss": 6674.91552734375, "val_acc": 60.0}
{"epoch": 3, "training_loss": 6784.479052734375, "training_acc": 50.0, "val_loss": 8532.5224609375, "val_acc": 40.0}
{"epoch": 4, "training_loss": 7399.34697265625, "training_acc": 50.0, "val_loss": 11734.2939453125, "val_acc": 40.0}
{"epoch": 5, "training_loss": 9156.13798828125, "training_acc": 50.0, "val_loss": 1967.2030029296875, "val_acc": 40.0}
{"epoch": 6, "training_loss": 1546.1468017578125, "training_acc": 60.0, "val_loss": 6204.4775390625, "val_acc": 60.0}
{"epoch": 7, "training_loss": 7838.509765625, "training_acc": 50.0, "val_loss": 5754.94775390625, "val_acc": 60.0}
{"epoch": 8, "training_loss": 6504.34443359375, "training_acc": 50.0, "val_loss": 1098.434326171875, "val_acc": 40.0}
{"epoch": 9, "training_loss": 1552.493115234375, "training_acc": 50.0, "val_loss": 6383.4736328125, "val_acc": 40.0}
{"epoch": 10, "training_loss": 5096.46953125, "training_acc": 50.0, "val_loss": 2017.4547119140625, "val_acc": 40.0}
{"epoch": 11, "training_loss": 1800.11728515625, "training_acc": 50.0, "val_loss": 3653.505859375, "val_acc": 60.0}
{"epoch": 12, "training_loss": 4637.30986328125, "training_acc": 50.0, "val_loss": 2748.214111328125, "val_acc": 60.0}
{"epoch": 13, "training_loss": 2866.3782470703127, "training_acc": 50.0, "val_loss": 3568.156982421875, "val_acc": 40.0}
{"epoch": 14, "training_loss": 3541.4111328125, "training_acc": 50.0, "val_loss": 5182.7333984375, "val_acc": 40.0}
{"epoch": 15, "training_loss": 3880.678662109375, "training_acc": 50.0, "val_loss": 916.6480712890625, "val_acc": 60.0}
{"epoch": 16, "training_loss": 1321.3066162109376, "training_acc": 50.0, "val_loss": 2775.412109375, "val_acc": 60.0}
{"epoch": 17, "training_loss": 3184.076220703125, "training_acc": 50.0, "val_loss": 224.23831176757812, "val_acc": 40.0}
{"epoch": 18, "training_loss": 326.89683837890624, "training_acc": 50.0, "val_loss": 2497.010498046875, "val_acc": 40.0}
{"epoch": 19, "training_loss": 1814.9751953125, "training_acc": 50.0, "val_loss": 1046.7506103515625, "val_acc": 60.0}
{"epoch": 20, "training_loss": 1726.585546875, "training_acc": 50.0, "val_loss": 1609.792236328125, "val_acc": 60.0}
{"epoch": 21, "training_loss": 1833.9833084106444, "training_acc": 40.0, "val_loss": 652.7535400390625, "val_acc": 40.0}
{"epoch": 22, "training_loss": 452.3846160888672, "training_acc": 50.0, "val_loss": 371.2632141113281, "val_acc": 40.0}
{"epoch": 23, "training_loss": 240.01515350341796, "training_acc": 60.0, "val_loss": 156.67430114746094, "val_acc": 60.0}
{"epoch": 24, "training_loss": 328.23515625, "training_acc": 50.0, "val_loss": 772.3388061523438, "val_acc": 40.0}
{"epoch": 25, "training_loss": 506.5734497070313, "training_acc": 60.0, "val_loss": 821.6619262695312, "val_acc": 60.0}
{"epoch": 26, "training_loss": 821.6921508789062, "training_acc": 50.0, "val_loss": 2237.195068359375, "val_acc": 40.0}
{"epoch": 27, "training_loss": 2071.614208984375, "training_acc": 50.0, "val_loss": 1294.2625732421875, "val_acc": 40.0}
{"epoch": 28, "training_loss": 1167.19912109375, "training_acc": 50.0, "val_loss": 2001.1414794921875, "val_acc": 60.0}
{"epoch": 29, "training_loss": 2325.418701171875, "training_acc": 50.0, "val_loss": 722.47216796875, "val_acc": 40.0}
{"epoch": 30, "training_loss": 699.0778930664062, "training_acc": 50.0, "val_loss": 1876.585205078125, "val_acc": 40.0}
{"epoch": 31, "training_loss": 1177.0825439453124, "training_acc": 50.0, "val_loss": 2028.5999755859375, "val_acc": 60.0}
{"epoch": 32, "training_loss": 2712.76435546875, "training_acc": 50.0, "val_loss": 1886.268798828125, "val_acc": 60.0}
{"epoch": 33, "training_loss": 1659.796184539795, "training_acc": 60.0, "val_loss": 2098.697265625, "val_acc": 40.0}
{"epoch": 34, "training_loss": 1751.334423828125, "training_acc": 50.0, "val_loss": 319.8544616699219, "val_acc": 40.0}
{"epoch": 35, "training_loss": 949.6690185546875, "training_acc": 40.0, "val_loss": 2029.8045654296875, "val_acc": 60.0}
{"epoch": 36, "training_loss": 2116.16748046875, "training_acc": 50.0, "val_loss": 2446.864013671875, "val_acc": 40.0}
{"epoch": 37, "training_loss": 2294.897509765625, "training_acc": 50.0, "val_loss": 3083.691650390625, "val_acc": 40.0}
{"epoch": 38, "training_loss": 2607.927600097656, "training_acc": 30.0, "val_loss": 223.68179321289062, "val_acc": 60.0}
{"epoch": 39, "training_loss": 367.4017578125, "training_acc": 50.0, "val_loss": 412.812255859375, "val_acc": 40.0}
{"epoch": 40, "training_loss": 345.3472412109375, "training_acc": 60.0, "val_loss": 1190.7314453125, "val_acc": 60.0}
{"epoch": 41, "training_loss": 1489.2294921875, "training_acc": 50.0, "val_loss": 1374.8248291015625, "val_acc": 40.0}
{"epoch": 42, "training_loss": 1107.3876220703125, "training_acc": 50.0, "val_loss": 837.6422119140625, "val_acc": 60.0}
{"epoch": 43, "training_loss": 1230.73388671875, "training_acc": 50.0, "val_loss": 72.83030700683594, "val_acc": 60.0}
{"epoch": 44, "training_loss": 569.9387573242187, "training_acc": 50.0, "val_loss": 3645.801513671875, "val_acc": 40.0}
{"epoch": 45, "training_loss": 2882.976940917969, "training_acc": 50.0, "val_loss": 698.659423828125, "val_acc": 60.0}
{"epoch": 46, "training_loss": 873.27578125, "training_acc": 50.0, "val_loss": 1131.202392578125, "val_acc": 60.0}
{"epoch": 47, "training_loss": 1140.1989303588866, "training_acc": 50.0, "val_loss": 221.0838165283203, "val_acc": 40.0}
{"epoch": 48, "training_loss": 327.966064453125, "training_acc": 50.0, "val_loss": 381.3529357910156, "val_acc": 60.0}
{"epoch": 49, "training_loss": 622.2777099609375, "training_acc": 50.0, "val_loss": 1469.835205078125, "val_acc": 40.0}
{"epoch": 50, "training_loss": 1224.336865234375, "training_acc": 40.0, "val_loss": 6.383903503417969, "val_acc": 40.0}
{"epoch": 51, "training_loss": 69.84419479370118, "training_acc": 60.0, "val_loss": 394.3910217285156, "val_acc": 60.0}
{"epoch": 52, "training_loss": 591.4559814453125, "training_acc": 50.0, "val_loss": 1032.51220703125, "val_acc": 40.0}
{"epoch": 53, "training_loss": 655.849072265625, "training_acc": 60.0, "val_loss": 837.1398315429688, "val_acc": 60.0}
{"epoch": 54, "training_loss": 783.435791015625, "training_acc": 50.0, "val_loss": 2980.10888671875, "val_acc": 40.0}
{"epoch": 55, "training_loss": 2484.2908203125, "training_acc": 50.0, "val_loss": 2569.57666015625, "val_acc": 40.0}
{"epoch": 56, "training_loss": 1743.006918334961, "training_acc": 50.0, "val_loss": 759.2055053710938, "val_acc": 60.0}
{"epoch": 57, "training_loss": 796.294970703125, "training_acc": 50.0, "val_loss": 2141.411376953125, "val_acc": 40.0}
{"epoch": 58, "training_loss": 1848.12724609375, "training_acc": 50.0, "val_loss": 161.2505340576172, "val_acc": 60.0}
{"epoch": 59, "training_loss": 456.7226318359375, "training_acc": 50.0, "val_loss": 385.0183410644531, "val_acc": 40.0}
{"epoch": 60, "training_loss": 296.941748046875, "training_acc": 50.0, "val_loss": 625.7318725585938, "val_acc": 60.0}
{"epoch": 61, "training_loss": 645.7966766357422, "training_acc": 50.0, "val_loss": 2148.837646484375, "val_acc": 40.0}
{"epoch": 62, "training_loss": 1824.603857421875, "training_acc": 50.0, "val_loss": 318.3424377441406, "val_acc": 60.0}
{"epoch": 63, "training_loss": 540.53095703125, "training_acc": 50.0, "val_loss": 150.17236328125, "val_acc": 60.0}
{"epoch": 64, "training_loss": 541.6868896484375, "training_acc": 50.0, "val_loss": 2497.405029296875, "val_acc": 40.0}
{"epoch": 65, "training_loss": 1734.9747314453125, "training_acc": 50.0, "val_loss": 2059.3447265625, "val_acc": 60.0}
{"epoch": 66, "training_loss": 2703.4146484375, "training_acc": 50.0, "val_loss": 1301.940673828125, "val_acc": 60.0}
{"epoch": 67, "training_loss": 1970.90146484375, "training_acc": 40.0, "val_loss": 2749.470703125, "val_acc": 40.0}
{"epoch": 68, "training_loss": 2018.4316650390624, "training_acc": 50.0, "val_loss": 1468.699951171875, "val_acc": 60.0}
{"epoch": 69, "training_loss": 1890.864306640625, "training_acc": 50.0, "val_loss": 1245.3941650390625, "val_acc": 60.0}
