"main_modify.py --pretrained_path None --mode finetuning --train_num 20 --layer_control freeze --stratify strat --random_seed 3 --task ADNI_sex --input_option yAware --learning_rate 1e2 --batch_size 8 --weight_decay 1e-8 --BN inst --save_path finetune_results_InstanceBN"
{"epoch": 0, "training_loss": 9696.45603480339, "training_acc": 50.0, "val_loss": 10050.8515625, "val_acc": 40.0}
{"epoch": 1, "training_loss": 16757.43046875, "training_acc": 50.0, "val_loss": 18960.259765625, "val_acc": 40.0}
{"epoch": 2, "training_loss": 10039.06904296875, "training_acc": 50.0, "val_loss": 7462.25, "val_acc": 60.0}
{"epoch": 3, "training_loss": 15137.191796875, "training_acc": 50.0, "val_loss": 14307.923828125, "val_acc": 60.0}
{"epoch": 4, "training_loss": 13978.9884765625, "training_acc": 50.0, "val_loss": 762.3111572265625, "val_acc": 60.0}
{"epoch": 5, "training_loss": 3837.4850341796873, "training_acc": 70.0, "val_loss": 21206.916015625, "val_acc": 40.0}
{"epoch": 6, "training_loss": 19466.31484375, "training_acc": 50.0, "val_loss": 18523.34765625, "val_acc": 40.0}
{"epoch": 7, "training_loss": 9614.80166015625, "training_acc": 50.0, "val_loss": 4944.68359375, "val_acc": 60.0}
{"epoch": 8, "training_loss": 9698.286328125, "training_acc": 50.0, "val_loss": 12960.7060546875, "val_acc": 60.0}
{"epoch": 9, "training_loss": 15050.5529296875, "training_acc": 50.0, "val_loss": 5679.6630859375, "val_acc": 60.0}
{"epoch": 10, "training_loss": 4968.86573677063, "training_acc": 50.0, "val_loss": 13501.2158203125, "val_acc": 40.0}
{"epoch": 11, "training_loss": 14138.99931640625, "training_acc": 50.0, "val_loss": 13294.2158203125, "val_acc": 40.0}
{"epoch": 12, "training_loss": 7734.991015625, "training_acc": 40.0, "val_loss": 3283.164794921875, "val_acc": 60.0}
{"epoch": 13, "training_loss": 4151.996508789062, "training_acc": 50.0, "val_loss": 228.77806091308594, "val_acc": 60.0}
{"epoch": 14, "training_loss": 1389.0209228515625, "training_acc": 70.0, "val_loss": 5680.1982421875, "val_acc": 40.0}
{"epoch": 15, "training_loss": 3795.452587890625, "training_acc": 40.0, "val_loss": 1823.1195068359375, "val_acc": 60.0}
{"epoch": 16, "training_loss": 2225.760595703125, "training_acc": 40.0, "val_loss": 4084.948974609375, "val_acc": 40.0}
{"epoch": 17, "training_loss": 2481.76005859375, "training_acc": 50.0, "val_loss": 2517.772216796875, "val_acc": 60.0}
{"epoch": 18, "training_loss": 2717.257275390625, "training_acc": 40.0, "val_loss": 1651.4769287109375, "val_acc": 40.0}
{"epoch": 19, "training_loss": 442.2118316650391, "training_acc": 70.0, "val_loss": 535.1791381835938, "val_acc": 40.0}
{"epoch": 20, "training_loss": 249.87479248046876, "training_acc": 60.0, "val_loss": 3762.66650390625, "val_acc": 40.0}
{"epoch": 21, "training_loss": 3850.21884765625, "training_acc": 50.0, "val_loss": 168.3539276123047, "val_acc": 40.0}
{"epoch": 22, "training_loss": 3367.66083984375, "training_acc": 50.0, "val_loss": 6775.16259765625, "val_acc": 60.0}
{"epoch": 23, "training_loss": 7764.66083984375, "training_acc": 50.0, "val_loss": 121.75728607177734, "val_acc": 60.0}
{"epoch": 24, "training_loss": 3694.405712890625, "training_acc": 50.0, "val_loss": 11687.2744140625, "val_acc": 40.0}
{"epoch": 25, "training_loss": 8557.1724609375, "training_acc": 50.0, "val_loss": 2137.4716796875, "val_acc": 40.0}
{"epoch": 26, "training_loss": 4306.853173828125, "training_acc": 50.0, "val_loss": 8012.3359375, "val_acc": 60.0}
{"epoch": 27, "training_loss": 8932.89111328125, "training_acc": 50.0, "val_loss": 1394.7474365234375, "val_acc": 60.0}
{"epoch": 28, "training_loss": 4352.803857421875, "training_acc": 50.0, "val_loss": 10233.8623046875, "val_acc": 40.0}
{"epoch": 29, "training_loss": 6738.901171875, "training_acc": 50.0, "val_loss": 1089.429931640625, "val_acc": 60.0}
{"epoch": 30, "training_loss": 5140.3390625, "training_acc": 50.0, "val_loss": 3892.56103515625, "val_acc": 60.0}
{"epoch": 31, "training_loss": 3249.008984375, "training_acc": 50.0, "val_loss": 8453.7666015625, "val_acc": 40.0}
{"epoch": 32, "training_loss": 6761.642041015625, "training_acc": 50.0, "val_loss": 2726.81591796875, "val_acc": 40.0}
{"epoch": 33, "training_loss": 471.7973388671875, "training_acc": 80.0, "val_loss": 2439.249267578125, "val_acc": 60.0}
{"epoch": 34, "training_loss": 2676.202685546875, "training_acc": 40.0, "val_loss": 1846.4635009765625, "val_acc": 40.0}
{"epoch": 35, "training_loss": 2075.335693359375, "training_acc": 30.0, "val_loss": 1609.4603271484375, "val_acc": 40.0}
{"epoch": 36, "training_loss": 1026.535181427002, "training_acc": 55.0, "val_loss": 166.1309051513672, "val_acc": 60.0}
{"epoch": 37, "training_loss": 587.6552185058594, "training_acc": 60.0, "val_loss": 1385.8231201171875, "val_acc": 60.0}
{"epoch": 38, "training_loss": 1883.5821533203125, "training_acc": 50.0, "val_loss": 2924.500732421875, "val_acc": 40.0}
{"epoch": 39, "training_loss": 2727.993896484375, "training_acc": 50.0, "val_loss": 569.9683837890625, "val_acc": 60.0}
{"epoch": 40, "training_loss": 1800.875927734375, "training_acc": 50.0, "val_loss": 1673.5550537109375, "val_acc": 40.0}
{"epoch": 41, "training_loss": 1734.6697265625, "training_acc": 50.0, "val_loss": 1904.595703125, "val_acc": 60.0}
{"epoch": 42, "training_loss": 3366.93935546875, "training_acc": 50.0, "val_loss": 269.02545166015625, "val_acc": 40.0}
{"epoch": 43, "training_loss": 2266.252359008789, "training_acc": 55.0, "val_loss": 92.19054412841797, "val_acc": 60.0}
{"epoch": 44, "training_loss": 775.9032104492187, "training_acc": 40.0, "val_loss": 327.6459045410156, "val_acc": 60.0}
{"epoch": 45, "training_loss": 748.0281494140625, "training_acc": 50.0, "val_loss": 3627.79931640625, "val_acc": 40.0}
{"epoch": 46, "training_loss": 2092.995654296875, "training_acc": 50.0, "val_loss": 2074.341552734375, "val_acc": 60.0}
{"epoch": 47, "training_loss": 1726.7220672607423, "training_acc": 60.0, "val_loss": 1804.098876953125, "val_acc": 40.0}
{"epoch": 48, "training_loss": 1040.1784255981445, "training_acc": 40.0, "val_loss": 69.33391571044922, "val_acc": 60.0}
{"epoch": 49, "training_loss": 578.7065383911133, "training_acc": 50.0, "val_loss": 741.3264770507812, "val_acc": 40.0}
{"epoch": 50, "training_loss": 932.4208740234375, "training_acc": 40.0, "val_loss": 2906.1689453125, "val_acc": 40.0}
{"epoch": 51, "training_loss": 2053.1199951171875, "training_acc": 50.0, "val_loss": 1880.917236328125, "val_acc": 60.0}
{"epoch": 52, "training_loss": 1990.4802734375, "training_acc": 50.0, "val_loss": 3345.46484375, "val_acc": 40.0}
{"epoch": 53, "training_loss": 2971.5833984375, "training_acc": 50.0, "val_loss": 285.13116455078125, "val_acc": 60.0}
{"epoch": 54, "training_loss": 655.8387084960938, "training_acc": 50.0, "val_loss": 4166.88427734375, "val_acc": 40.0}
{"epoch": 55, "training_loss": 4081.7037109375, "training_acc": 50.0, "val_loss": 1643.9105224609375, "val_acc": 40.0}
{"epoch": 56, "training_loss": 3702.16298828125, "training_acc": 40.0, "val_loss": 5213.95849609375, "val_acc": 60.0}
{"epoch": 57, "training_loss": 4798.6802734375, "training_acc": 50.0, "val_loss": 4190.9658203125, "val_acc": 40.0}
{"epoch": 58, "training_loss": 4968.99169921875, "training_acc": 50.0, "val_loss": 7746.23974609375, "val_acc": 40.0}
{"epoch": 59, "training_loss": 4656.89912109375, "training_acc": 40.0, "val_loss": 2465.415283203125, "val_acc": 60.0}
{"epoch": 60, "training_loss": 2870.4964599609375, "training_acc": 50.0, "val_loss": 4363.65625, "val_acc": 40.0}
{"epoch": 61, "training_loss": 4138.304663085937, "training_acc": 50.0, "val_loss": 1916.2841796875, "val_acc": 40.0}
{"epoch": 62, "training_loss": 2741.5655029296877, "training_acc": 40.0, "val_loss": 2857.98828125, "val_acc": 60.0}
{"epoch": 63, "training_loss": 2100.697998046875, "training_acc": 60.0, "val_loss": 4642.3564453125, "val_acc": 40.0}
{"epoch": 64, "training_loss": 3148.69921875, "training_acc": 50.0, "val_loss": 1792.9613037109375, "val_acc": 60.0}
{"epoch": 65, "training_loss": 3700.7644287109374, "training_acc": 50.0, "val_loss": 827.5238647460938, "val_acc": 60.0}
{"epoch": 66, "training_loss": 2258.748388671875, "training_acc": 50.0, "val_loss": 4203.62353515625, "val_acc": 40.0}
{"epoch": 67, "training_loss": 2876.188818359375, "training_acc": 40.0, "val_loss": 4192.99951171875, "val_acc": 60.0}
