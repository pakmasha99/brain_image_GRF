"main_modify.py --pretrained_path None --mode finetuning --train_num 20 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_sex --input_option yAware --learning_rate 1e2 --batch_size 8 --weight_decay 1e-8 --BN inst --save_path finetune_results_InstanceBN"
{"epoch": 0, "training_loss": 11591.7798589468, "training_acc": 40.0, "val_loss": 5155.72509765625, "val_acc": 60.0}
{"epoch": 1, "training_loss": 11628.02626953125, "training_acc": 50.0, "val_loss": 6608.08935546875, "val_acc": 60.0}
{"epoch": 2, "training_loss": 5047.065173339844, "training_acc": 60.0, "val_loss": 16925.376953125, "val_acc": 40.0}
{"epoch": 3, "training_loss": 14648.91171875, "training_acc": 50.0, "val_loss": 10831.6904296875, "val_acc": 40.0}
{"epoch": 4, "training_loss": 6853.746826171875, "training_acc": 40.0, "val_loss": 10229.02734375, "val_acc": 60.0}
{"epoch": 5, "training_loss": 13511.07646484375, "training_acc": 50.0, "val_loss": 7791.0361328125, "val_acc": 60.0}
{"epoch": 6, "training_loss": 6631.23076171875, "training_acc": 50.0, "val_loss": 6802.6337890625, "val_acc": 40.0}
{"epoch": 7, "training_loss": 5307.533154296875, "training_acc": 50.0, "val_loss": 696.4387817382812, "val_acc": 40.0}
{"epoch": 8, "training_loss": 4799.77060546875, "training_acc": 40.0, "val_loss": 7153.9580078125, "val_acc": 60.0}
{"epoch": 9, "training_loss": 7020.4373046875, "training_acc": 50.0, "val_loss": 1987.0931396484375, "val_acc": 40.0}
{"epoch": 10, "training_loss": 3005.93662109375, "training_acc": 50.0, "val_loss": 4687.7587890625, "val_acc": 40.0}
{"epoch": 11, "training_loss": 3793.2932739257812, "training_acc": 40.0, "val_loss": 4734.30517578125, "val_acc": 60.0}
{"epoch": 12, "training_loss": 4719.714794921875, "training_acc": 50.0, "val_loss": 3035.81982421875, "val_acc": 40.0}
{"epoch": 13, "training_loss": 4347.2083984375, "training_acc": 50.0, "val_loss": 2996.973876953125, "val_acc": 40.0}
{"epoch": 14, "training_loss": 899.7744140625, "training_acc": 70.0, "val_loss": 2582.69140625, "val_acc": 60.0}
{"epoch": 15, "training_loss": 2808.22666015625, "training_acc": 40.0, "val_loss": 2203.241943359375, "val_acc": 40.0}
{"epoch": 16, "training_loss": 1601.1603515625, "training_acc": 40.0, "val_loss": 2392.216552734375, "val_acc": 40.0}
{"epoch": 17, "training_loss": 2814.59609375, "training_acc": 50.0, "val_loss": 425.4853210449219, "val_acc": 60.0}
{"epoch": 18, "training_loss": 594.66298828125, "training_acc": 50.0, "val_loss": 203.0373992919922, "val_acc": 60.0}
{"epoch": 19, "training_loss": 960.2087280273438, "training_acc": 40.0, "val_loss": 1744.1068115234375, "val_acc": 60.0}
{"epoch": 20, "training_loss": 2595.848876953125, "training_acc": 50.0, "val_loss": 2016.36328125, "val_acc": 40.0}
{"epoch": 21, "training_loss": 2477.084228515625, "training_acc": 50.0, "val_loss": 225.47390747070312, "val_acc": 40.0}
{"epoch": 22, "training_loss": 1670.7613891601563, "training_acc": 60.0, "val_loss": 3067.335693359375, "val_acc": 60.0}
{"epoch": 23, "training_loss": 2809.5360107421875, "training_acc": 50.0, "val_loss": 4840.732421875, "val_acc": 40.0}
{"epoch": 24, "training_loss": 2930.319128417969, "training_acc": 50.0, "val_loss": 3143.180908203125, "val_acc": 60.0}
{"epoch": 25, "training_loss": 5643.88779296875, "training_acc": 50.0, "val_loss": 2853.271728515625, "val_acc": 60.0}
{"epoch": 26, "training_loss": 1749.93525390625, "training_acc": 60.0, "val_loss": 3164.3203125, "val_acc": 40.0}
{"epoch": 27, "training_loss": 1647.0273803710938, "training_acc": 60.0, "val_loss": 2805.463623046875, "val_acc": 60.0}
{"epoch": 28, "training_loss": 3214.423651123047, "training_acc": 50.0, "val_loss": 2977.787353515625, "val_acc": 40.0}
{"epoch": 29, "training_loss": 2490.3640625, "training_acc": 50.0, "val_loss": 857.5181884765625, "val_acc": 60.0}
{"epoch": 30, "training_loss": 1204.0697021484375, "training_acc": 50.0, "val_loss": 3551.26416015625, "val_acc": 40.0}
{"epoch": 31, "training_loss": 2888.4193359375, "training_acc": 50.0, "val_loss": 948.2250366210938, "val_acc": 60.0}
{"epoch": 32, "training_loss": 1669.7780517578126, "training_acc": 50.0, "val_loss": 637.2435913085938, "val_acc": 40.0}
{"epoch": 33, "training_loss": 874.7262329101562, "training_acc": 50.0, "val_loss": 283.1952819824219, "val_acc": 60.0}
{"epoch": 34, "training_loss": 1288.1120361328126, "training_acc": 50.0, "val_loss": 5.73732852935791, "val_acc": 80.0}
{"epoch": 35, "training_loss": 244.09100952148438, "training_acc": 55.0, "val_loss": 1744.916015625, "val_acc": 60.0}
{"epoch": 36, "training_loss": 1685.421044921875, "training_acc": 50.0, "val_loss": 4295.34130859375, "val_acc": 40.0}
{"epoch": 37, "training_loss": 2226.249169921875, "training_acc": 60.0, "val_loss": 1399.7861328125, "val_acc": 60.0}
{"epoch": 38, "training_loss": 1609.3197898864746, "training_acc": 60.0, "val_loss": 1637.9293212890625, "val_acc": 40.0}
{"epoch": 39, "training_loss": 1616.9485107421874, "training_acc": 40.0, "val_loss": 2294.369140625, "val_acc": 60.0}
{"epoch": 40, "training_loss": 3390.979345703125, "training_acc": 30.0, "val_loss": 5703.4072265625, "val_acc": 40.0}
{"epoch": 41, "training_loss": 3691.1965576171874, "training_acc": 40.0, "val_loss": 1329.8206787109375, "val_acc": 60.0}
{"epoch": 42, "training_loss": 1415.1424865722656, "training_acc": 30.0, "val_loss": 2287.635498046875, "val_acc": 60.0}
{"epoch": 43, "training_loss": 2903.42939453125, "training_acc": 50.0, "val_loss": 755.0048828125, "val_acc": 40.0}
{"epoch": 44, "training_loss": 742.2902099609375, "training_acc": 50.0, "val_loss": 2640.71484375, "val_acc": 60.0}
{"epoch": 45, "training_loss": 4023.7400390625, "training_acc": 50.0, "val_loss": 1107.2479248046875, "val_acc": 60.0}
{"epoch": 46, "training_loss": 3038.94384765625, "training_acc": 50.0, "val_loss": 5452.4892578125, "val_acc": 40.0}
{"epoch": 47, "training_loss": 3621.02421875, "training_acc": 40.0, "val_loss": 3528.681640625, "val_acc": 60.0}
{"epoch": 48, "training_loss": 3544.4735321044923, "training_acc": 50.0, "val_loss": 4136.3251953125, "val_acc": 40.0}
{"epoch": 49, "training_loss": 4572.16240234375, "training_acc": 50.0, "val_loss": 2206.158935546875, "val_acc": 40.0}
{"epoch": 50, "training_loss": 3751.6541015625, "training_acc": 40.0, "val_loss": 3701.861083984375, "val_acc": 60.0}
{"epoch": 51, "training_loss": 2504.3918579101564, "training_acc": 60.0, "val_loss": 4804.31298828125, "val_acc": 40.0}
{"epoch": 52, "training_loss": 3816.05908203125, "training_acc": 50.0, "val_loss": 673.168212890625, "val_acc": 60.0}
{"epoch": 53, "training_loss": 1057.060595703125, "training_acc": 50.0, "val_loss": 1392.472900390625, "val_acc": 40.0}
