"main_modify.py --pretrained_path None --mode finetuning --train_num 20 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_sex --input_option yAware --learning_rate 1e2 --batch_size 8 --weight_decay 1e-2 --BN inst --save_path finetune_results_InstanceBN"
{"epoch": 0, "training_loss": 11266.937526035308, "training_acc": 50.0, "val_loss": 11983.8818359375, "val_acc": 40.0}
{"epoch": 1, "training_loss": 11442.67734375, "training_acc": 50.0, "val_loss": 7772.16357421875, "val_acc": 40.0}
{"epoch": 2, "training_loss": 3157.39296875, "training_acc": 60.0, "val_loss": 7353.98291015625, "val_acc": 60.0}
{"epoch": 3, "training_loss": 7684.3658203125, "training_acc": 50.0, "val_loss": 749.4435424804688, "val_acc": 40.0}
{"epoch": 4, "training_loss": 2840.807666015625, "training_acc": 50.0, "val_loss": 6486.60888671875, "val_acc": 40.0}
{"epoch": 5, "training_loss": 2745.110699462891, "training_acc": 60.0, "val_loss": 2707.697998046875, "val_acc": 60.0}
{"epoch": 6, "training_loss": 3745.12822265625, "training_acc": 50.0, "val_loss": 684.97802734375, "val_acc": 60.0}
{"epoch": 7, "training_loss": 2633.6296875, "training_acc": 40.0, "val_loss": 4539.41845703125, "val_acc": 40.0}
{"epoch": 8, "training_loss": 2356.8469482421874, "training_acc": 50.0, "val_loss": 2455.716796875, "val_acc": 60.0}
{"epoch": 9, "training_loss": 2643.26708984375, "training_acc": 50.0, "val_loss": 1605.8255615234375, "val_acc": 40.0}
{"epoch": 10, "training_loss": 1933.94609375, "training_acc": 50.0, "val_loss": 995.7648315429688, "val_acc": 40.0}
{"epoch": 11, "training_loss": 1239.99814453125, "training_acc": 60.0, "val_loss": 1641.2117919921875, "val_acc": 60.0}
{"epoch": 12, "training_loss": 1232.02001953125, "training_acc": 50.0, "val_loss": 4243.84375, "val_acc": 40.0}
{"epoch": 13, "training_loss": 2655.125927734375, "training_acc": 50.0, "val_loss": 2187.26611328125, "val_acc": 60.0}
{"epoch": 14, "training_loss": 3739.63896484375, "training_acc": 50.0, "val_loss": 1321.2027587890625, "val_acc": 60.0}
{"epoch": 15, "training_loss": 2609.47685546875, "training_acc": 50.0, "val_loss": 4530.88134765625, "val_acc": 40.0}
{"epoch": 16, "training_loss": 2686.980908203125, "training_acc": 50.0, "val_loss": 3069.876708984375, "val_acc": 60.0}
{"epoch": 17, "training_loss": 3000.7074584960938, "training_acc": 50.0, "val_loss": 2870.48974609375, "val_acc": 40.0}
{"epoch": 18, "training_loss": 3330.0931640625, "training_acc": 50.0, "val_loss": 648.484375, "val_acc": 40.0}
{"epoch": 19, "training_loss": 1795.066943359375, "training_acc": 60.0, "val_loss": 3284.376708984375, "val_acc": 60.0}
{"epoch": 20, "training_loss": 2335.0792236328125, "training_acc": 60.0, "val_loss": 4330.20263671875, "val_acc": 40.0}
{"epoch": 21, "training_loss": 3061.605419921875, "training_acc": 50.0, "val_loss": 1418.3265380859375, "val_acc": 60.0}
{"epoch": 22, "training_loss": 3096.793994140625, "training_acc": 50.0, "val_loss": 821.0230712890625, "val_acc": 60.0}
{"epoch": 23, "training_loss": 1499.47587890625, "training_acc": 60.0, "val_loss": 3729.763427734375, "val_acc": 40.0}
{"epoch": 24, "training_loss": 1931.2776611328125, "training_acc": 50.0, "val_loss": 2743.069091796875, "val_acc": 60.0}
{"epoch": 25, "training_loss": 2747.2521003723145, "training_acc": 50.0, "val_loss": 3377.22900390625, "val_acc": 40.0}
{"epoch": 26, "training_loss": 2998.096875, "training_acc": 50.0, "val_loss": 528.5203857421875, "val_acc": 60.0}
{"epoch": 27, "training_loss": 1670.8854248046875, "training_acc": 50.0, "val_loss": 413.2982482910156, "val_acc": 40.0}
{"epoch": 28, "training_loss": 848.5356567382812, "training_acc": 50.0, "val_loss": 1536.494384765625, "val_acc": 60.0}
{"epoch": 29, "training_loss": 2247.570849609375, "training_acc": 50.0, "val_loss": 1015.9718017578125, "val_acc": 40.0}
{"epoch": 30, "training_loss": 1814.7024169921874, "training_acc": 50.0, "val_loss": 348.3206481933594, "val_acc": 60.0}
{"epoch": 31, "training_loss": 737.4379638671875, "training_acc": 30.0, "val_loss": 630.352294921875, "val_acc": 60.0}
{"epoch": 32, "training_loss": 971.114453125, "training_acc": 40.0, "val_loss": 678.916015625, "val_acc": 40.0}
{"epoch": 33, "training_loss": 349.2836669921875, "training_acc": 70.0, "val_loss": 41.22542953491211, "val_acc": 40.0}
{"epoch": 34, "training_loss": 433.22245025634766, "training_acc": 50.0, "val_loss": 2005.6898193359375, "val_acc": 40.0}
{"epoch": 35, "training_loss": 1825.157373046875, "training_acc": 50.0, "val_loss": 1586.9952392578125, "val_acc": 60.0}
{"epoch": 36, "training_loss": 3188.0377807617188, "training_acc": 50.0, "val_loss": 1284.1968994140625, "val_acc": 40.0}
{"epoch": 37, "training_loss": 1524.3045776367187, "training_acc": 50.0, "val_loss": 297.3066711425781, "val_acc": 60.0}
{"epoch": 38, "training_loss": 502.2683959960938, "training_acc": 50.0, "val_loss": 323.73614501953125, "val_acc": 40.0}
{"epoch": 39, "training_loss": 935.7191650390625, "training_acc": 50.0, "val_loss": 778.96728515625, "val_acc": 40.0}
{"epoch": 40, "training_loss": 909.4893676757813, "training_acc": 40.0, "val_loss": 464.4306640625, "val_acc": 40.0}
{"epoch": 41, "training_loss": 359.2533935546875, "training_acc": 50.0, "val_loss": 350.1109313964844, "val_acc": 60.0}
{"epoch": 42, "training_loss": 365.6629638671875, "training_acc": 50.0, "val_loss": 403.53448486328125, "val_acc": 40.0}
{"epoch": 43, "training_loss": 779.5292129516602, "training_acc": 30.0, "val_loss": 285.435302734375, "val_acc": 60.0}
{"epoch": 44, "training_loss": 381.223486328125, "training_acc": 60.0, "val_loss": 2953.031982421875, "val_acc": 40.0}
{"epoch": 45, "training_loss": 2202.950439453125, "training_acc": 40.0, "val_loss": 1844.1890869140625, "val_acc": 60.0}
{"epoch": 46, "training_loss": 2028.0595703125, "training_acc": 40.0, "val_loss": 3701.20947265625, "val_acc": 40.0}
{"epoch": 47, "training_loss": 2069.385205078125, "training_acc": 50.0, "val_loss": 2504.189453125, "val_acc": 60.0}
{"epoch": 48, "training_loss": 2027.5425598144532, "training_acc": 50.0, "val_loss": 5135.09521484375, "val_acc": 40.0}
{"epoch": 49, "training_loss": 4543.5994140625, "training_acc": 50.0, "val_loss": 300.6295166015625, "val_acc": 40.0}
{"epoch": 50, "training_loss": 4457.517529296875, "training_acc": 30.0, "val_loss": 3744.221923828125, "val_acc": 60.0}
{"epoch": 51, "training_loss": 1902.6238525390625, "training_acc": 60.0, "val_loss": 8103.5234375, "val_acc": 40.0}
{"epoch": 52, "training_loss": 5699.09248046875, "training_acc": 50.0, "val_loss": 1967.175048828125, "val_acc": 60.0}
