"main_modify.py --pretrained_path None --mode finetuning --train_num 20 --layer_control freeze --stratify strat --random_seed 5 --task ADNI_sex --input_option yAware --learning_rate 1e2 --batch_size 8 --weight_decay 1e-2 --BN inst --save_path finetune_results_InstanceBN"
{"epoch": 0, "training_loss": 12740.706213784219, "training_acc": 30.0, "val_loss": 11207.787109375, "val_acc": 40.0}
{"epoch": 1, "training_loss": 12315.2453125, "training_acc": 50.0, "val_loss": 10724.6884765625, "val_acc": 40.0}
{"epoch": 2, "training_loss": 5396.8709716796875, "training_acc": 50.0, "val_loss": 9777.0537109375, "val_acc": 60.0}
{"epoch": 3, "training_loss": 11734.667578125, "training_acc": 50.0, "val_loss": 3490.867919921875, "val_acc": 60.0}
{"epoch": 4, "training_loss": 4012.030712890625, "training_acc": 40.0, "val_loss": 9943.8984375, "val_acc": 40.0}
{"epoch": 5, "training_loss": 7790.59921875, "training_acc": 50.0, "val_loss": 1755.0474853515625, "val_acc": 40.0}
{"epoch": 6, "training_loss": 2617.653271484375, "training_acc": 50.0, "val_loss": 5062.97265625, "val_acc": 60.0}
{"epoch": 7, "training_loss": 5113.5861328125, "training_acc": 50.0, "val_loss": 1453.02001953125, "val_acc": 40.0}
{"epoch": 8, "training_loss": 3636.48251953125, "training_acc": 50.0, "val_loss": 4019.970458984375, "val_acc": 40.0}
{"epoch": 9, "training_loss": 1796.1505126953125, "training_acc": 60.0, "val_loss": 3551.643798828125, "val_acc": 60.0}
{"epoch": 10, "training_loss": 4233.3611328125, "training_acc": 50.0, "val_loss": 233.0840301513672, "val_acc": 60.0}
{"epoch": 11, "training_loss": 2143.6501220703126, "training_acc": 50.0, "val_loss": 5528.3427734375, "val_acc": 40.0}
{"epoch": 12, "training_loss": 3373.3699279785155, "training_acc": 40.0, "val_loss": 1429.3756103515625, "val_acc": 60.0}
{"epoch": 13, "training_loss": 1887.3509765625, "training_acc": 50.0, "val_loss": 1270.9737548828125, "val_acc": 40.0}
{"epoch": 14, "training_loss": 1160.7719299316407, "training_acc": 30.0, "val_loss": 513.2120971679688, "val_acc": 40.0}
{"epoch": 15, "training_loss": 521.215771484375, "training_acc": 30.0, "val_loss": 81.29127502441406, "val_acc": 60.0}
{"epoch": 16, "training_loss": 216.4098327636719, "training_acc": 50.0, "val_loss": 127.7835922241211, "val_acc": 60.0}
{"epoch": 17, "training_loss": 1031.2672424316406, "training_acc": 40.0, "val_loss": 163.78182983398438, "val_acc": 40.0}
{"epoch": 18, "training_loss": 1161.6488647460938, "training_acc": 50.0, "val_loss": 797.4003295898438, "val_acc": 60.0}
{"epoch": 19, "training_loss": 1616.565380859375, "training_acc": 40.0, "val_loss": 1477.064453125, "val_acc": 40.0}
{"epoch": 20, "training_loss": 1089.06376953125, "training_acc": 50.0, "val_loss": 576.2103881835938, "val_acc": 60.0}
{"epoch": 21, "training_loss": 299.6003173828125, "training_acc": 80.0, "val_loss": 372.01470947265625, "val_acc": 40.0}
{"epoch": 22, "training_loss": 1081.7646728515624, "training_acc": 50.0, "val_loss": 211.36497497558594, "val_acc": 60.0}
{"epoch": 23, "training_loss": 1909.2240234375, "training_acc": 40.0, "val_loss": 2395.416015625, "val_acc": 40.0}
{"epoch": 24, "training_loss": 1704.17353515625, "training_acc": 40.0, "val_loss": 564.0665283203125, "val_acc": 60.0}
{"epoch": 25, "training_loss": 794.3549194335938, "training_acc": 60.0, "val_loss": 1015.7326049804688, "val_acc": 40.0}
{"epoch": 26, "training_loss": 1078.5642333984374, "training_acc": 50.0, "val_loss": 68.62557220458984, "val_acc": 60.0}
{"epoch": 27, "training_loss": 1172.7263488769531, "training_acc": 60.0, "val_loss": 1797.2073974609375, "val_acc": 40.0}
{"epoch": 28, "training_loss": 2434.7158203125, "training_acc": 30.0, "val_loss": 970.4527587890625, "val_acc": 60.0}
{"epoch": 29, "training_loss": 383.3381591796875, "training_acc": 80.0, "val_loss": 1685.7681884765625, "val_acc": 40.0}
{"epoch": 30, "training_loss": 978.4813598632812, "training_acc": 50.0, "val_loss": 2400.167724609375, "val_acc": 60.0}
{"epoch": 31, "training_loss": 2082.9266357421875, "training_acc": 50.0, "val_loss": 2556.259521484375, "val_acc": 40.0}
{"epoch": 32, "training_loss": 1324.7446197509767, "training_acc": 60.0, "val_loss": 1097.6773681640625, "val_acc": 60.0}
{"epoch": 33, "training_loss": 1080.6808349609375, "training_acc": 50.0, "val_loss": 1632.2493896484375, "val_acc": 40.0}
{"epoch": 34, "training_loss": 526.9818481445312, "training_acc": 70.0, "val_loss": 2180.49560546875, "val_acc": 60.0}
{"epoch": 35, "training_loss": 2287.4225341796873, "training_acc": 40.0, "val_loss": 1293.968505859375, "val_acc": 40.0}
{"epoch": 36, "training_loss": 577.5223693847656, "training_acc": 40.0, "val_loss": 888.4519653320312, "val_acc": 60.0}
{"epoch": 37, "training_loss": 1018.4427017211914, "training_acc": 50.0, "val_loss": 3650.10986328125, "val_acc": 40.0}
{"epoch": 38, "training_loss": 3251.7083984375, "training_acc": 50.0, "val_loss": 550.8619995117188, "val_acc": 60.0}
{"epoch": 39, "training_loss": 1468.058544921875, "training_acc": 50.0, "val_loss": 407.94091796875, "val_acc": 40.0}
{"epoch": 40, "training_loss": 500.8026950836182, "training_acc": 50.0, "val_loss": 398.6297302246094, "val_acc": 40.0}
{"epoch": 41, "training_loss": 409.3680618286133, "training_acc": 50.0, "val_loss": 1874.6746826171875, "val_acc": 40.0}
{"epoch": 42, "training_loss": 1081.6987838745117, "training_acc": 50.0, "val_loss": 2418.796875, "val_acc": 60.0}
{"epoch": 43, "training_loss": 3429.8453125, "training_acc": 50.0, "val_loss": 1143.0076904296875, "val_acc": 40.0}
{"epoch": 44, "training_loss": 1557.413720703125, "training_acc": 50.0, "val_loss": 274.29119873046875, "val_acc": 60.0}
{"epoch": 45, "training_loss": 845.7282470703125, "training_acc": 50.0, "val_loss": 2065.254638671875, "val_acc": 40.0}
