"main_modify.py --pretrained_path None --mode finetuning --train_num 20 --layer_control freeze --stratify strat --random_seed 3 --task ADNI_sex --input_option yAware --learning_rate 1e2 --batch_size 8 --weight_decay 1e-2 --BN inst --save_path finetune_results_InstanceBN"
{"epoch": 0, "training_loss": 14195.39886674881, "training_acc": 50.0, "val_loss": 7290.43994140625, "val_acc": 60.0}
{"epoch": 1, "training_loss": 12807.91015625, "training_acc": 50.0, "val_loss": 6844.95068359375, "val_acc": 60.0}
{"epoch": 2, "training_loss": 4404.0533203125, "training_acc": 50.0, "val_loss": 15033.4501953125, "val_acc": 40.0}
{"epoch": 3, "training_loss": 12248.0775390625, "training_acc": 50.0, "val_loss": 5313.20654296875, "val_acc": 40.0}
{"epoch": 4, "training_loss": 2712.054736328125, "training_acc": 60.0, "val_loss": 7018.22216796875, "val_acc": 60.0}
{"epoch": 5, "training_loss": 8352.91279296875, "training_acc": 50.0, "val_loss": 1554.328857421875, "val_acc": 60.0}
{"epoch": 6, "training_loss": 1595.5224609375, "training_acc": 70.0, "val_loss": 7650.22021484375, "val_acc": 40.0}
{"epoch": 7, "training_loss": 5161.790625, "training_acc": 50.0, "val_loss": 565.3556518554688, "val_acc": 60.0}
{"epoch": 8, "training_loss": 2839.255712890625, "training_acc": 50.0, "val_loss": 2842.88134765625, "val_acc": 60.0}
{"epoch": 9, "training_loss": 3082.7570068359373, "training_acc": 40.0, "val_loss": 4119.77099609375, "val_acc": 40.0}
{"epoch": 10, "training_loss": 2676.536669921875, "training_acc": 50.0, "val_loss": 1167.9598388671875, "val_acc": 60.0}
{"epoch": 11, "training_loss": 1800.17099609375, "training_acc": 50.0, "val_loss": 1556.234619140625, "val_acc": 60.0}
{"epoch": 12, "training_loss": 942.2092895507812, "training_acc": 60.0, "val_loss": 3884.093017578125, "val_acc": 40.0}
{"epoch": 13, "training_loss": 2949.615283203125, "training_acc": 50.0, "val_loss": 1161.724609375, "val_acc": 60.0}
{"epoch": 14, "training_loss": 2042.1239135742187, "training_acc": 50.0, "val_loss": 358.5141296386719, "val_acc": 60.0}
{"epoch": 15, "training_loss": 2389.8135498046877, "training_acc": 30.0, "val_loss": 2338.1689453125, "val_acc": 40.0}
{"epoch": 16, "training_loss": 1327.1216796875, "training_acc": 50.0, "val_loss": 1190.503173828125, "val_acc": 60.0}
{"epoch": 17, "training_loss": 1761.495556640625, "training_acc": 20.0, "val_loss": 190.4921112060547, "val_acc": 60.0}
{"epoch": 18, "training_loss": 454.269482421875, "training_acc": 40.0, "val_loss": 257.0379333496094, "val_acc": 40.0}
{"epoch": 19, "training_loss": 976.9764587402344, "training_acc": 50.0, "val_loss": 364.1545104980469, "val_acc": 60.0}
{"epoch": 20, "training_loss": 985.57314453125, "training_acc": 60.0, "val_loss": 1452.401611328125, "val_acc": 40.0}
{"epoch": 21, "training_loss": 1496.329296875, "training_acc": 40.0, "val_loss": 663.3129272460938, "val_acc": 60.0}
{"epoch": 22, "training_loss": 1417.1644897460938, "training_acc": 40.0, "val_loss": 1039.2237548828125, "val_acc": 40.0}
{"epoch": 23, "training_loss": 1325.91484375, "training_acc": 40.0, "val_loss": 479.0685729980469, "val_acc": 60.0}
{"epoch": 24, "training_loss": 766.6575439453125, "training_acc": 60.0, "val_loss": 516.411376953125, "val_acc": 40.0}
{"epoch": 25, "training_loss": 1236.484228515625, "training_acc": 50.0, "val_loss": 705.3447265625, "val_acc": 60.0}
{"epoch": 26, "training_loss": 1364.507080078125, "training_acc": 50.0, "val_loss": 1418.1080322265625, "val_acc": 40.0}
{"epoch": 27, "training_loss": 1774.131884765625, "training_acc": 40.0, "val_loss": 644.9932861328125, "val_acc": 60.0}
{"epoch": 28, "training_loss": 1308.69775390625, "training_acc": 60.0, "val_loss": 1622.5174560546875, "val_acc": 40.0}
{"epoch": 29, "training_loss": 1757.0998901367188, "training_acc": 40.0, "val_loss": 1157.8941650390625, "val_acc": 60.0}
{"epoch": 30, "training_loss": 680.7756469726562, "training_acc": 60.0, "val_loss": 134.959716796875, "val_acc": 40.0}
{"epoch": 31, "training_loss": 488.40408477783205, "training_acc": 70.0, "val_loss": 353.5644226074219, "val_acc": 60.0}
{"epoch": 32, "training_loss": 588.4395751953125, "training_acc": 60.0, "val_loss": 70.8749008178711, "val_acc": 60.0}
{"epoch": 33, "training_loss": 444.90369873046876, "training_acc": 40.0, "val_loss": 7.870109558105469, "val_acc": 60.0}
{"epoch": 34, "training_loss": 1578.3910575866698, "training_acc": 40.0, "val_loss": 886.4403686523438, "val_acc": 40.0}
{"epoch": 35, "training_loss": 809.7701416015625, "training_acc": 60.0, "val_loss": 650.4674682617188, "val_acc": 60.0}
{"epoch": 36, "training_loss": 2163.5693359375, "training_acc": 20.0, "val_loss": 252.8952178955078, "val_acc": 60.0}
{"epoch": 37, "training_loss": 530.4925659179687, "training_acc": 40.0, "val_loss": 508.2697448730469, "val_acc": 60.0}
{"epoch": 38, "training_loss": 674.7589721679688, "training_acc": 50.0, "val_loss": 1695.8812255859375, "val_acc": 40.0}
{"epoch": 39, "training_loss": 1459.6025390625, "training_acc": 30.0, "val_loss": 1099.1915283203125, "val_acc": 40.0}
{"epoch": 40, "training_loss": 936.8797729492187, "training_acc": 50.0, "val_loss": 2055.0595703125, "val_acc": 60.0}
{"epoch": 41, "training_loss": 3218.58740234375, "training_acc": 50.0, "val_loss": 1361.8609619140625, "val_acc": 40.0}
{"epoch": 42, "training_loss": 1374.7705444335938, "training_acc": 50.0, "val_loss": 442.68511962890625, "val_acc": 40.0}
{"epoch": 43, "training_loss": 2107.0413452148437, "training_acc": 40.0, "val_loss": 885.4404296875, "val_acc": 60.0}
{"epoch": 44, "training_loss": 1556.3272705078125, "training_acc": 60.0, "val_loss": 2817.6962890625, "val_acc": 40.0}
{"epoch": 45, "training_loss": 1312.300558900833, "training_acc": 50.0, "val_loss": 2991.8115234375, "val_acc": 60.0}
{"epoch": 46, "training_loss": 1857.1009765625, "training_acc": 70.0, "val_loss": 3224.884033203125, "val_acc": 40.0}
{"epoch": 47, "training_loss": 2431.0734985351564, "training_acc": 50.0, "val_loss": 1625.4595947265625, "val_acc": 60.0}
{"epoch": 48, "training_loss": 2260.213525390625, "training_acc": 50.0, "val_loss": 2127.63427734375, "val_acc": 40.0}
{"epoch": 49, "training_loss": 2746.453857421875, "training_acc": 50.0, "val_loss": 254.1117706298828, "val_acc": 60.0}
{"epoch": 50, "training_loss": 916.333984375, "training_acc": 50.0, "val_loss": 1305.4588623046875, "val_acc": 40.0}
{"epoch": 51, "training_loss": 981.6052462100982, "training_acc": 50.0, "val_loss": 86.46891021728516, "val_acc": 60.0}
{"epoch": 52, "training_loss": 397.60267639160156, "training_acc": 60.0, "val_loss": 1130.51904296875, "val_acc": 60.0}
