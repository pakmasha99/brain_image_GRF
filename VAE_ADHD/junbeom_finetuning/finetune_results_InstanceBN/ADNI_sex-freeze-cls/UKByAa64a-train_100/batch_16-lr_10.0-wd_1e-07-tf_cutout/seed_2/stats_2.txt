"main_modify.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 2 --task ADNI_sex --input_option yAware --learning_rate 1e1 --batch_size 16 --weight_decay 1e-7 --BN inst --save_path finetune_results_InstanceBN"
{"epoch": 0, "training_loss": 906.1500951385498, "training_acc": 56.0, "val_loss": 1294.031572265625, "val_acc": 48.0}
{"epoch": 1, "training_loss": 657.2077880859375, "training_acc": 52.0, "val_loss": 242.17125305175782, "val_acc": 48.0}
{"epoch": 2, "training_loss": 121.88361907958985, "training_acc": 56.0, "val_loss": 67.0589208984375, "val_acc": 48.0}
{"epoch": 3, "training_loss": 147.03117614746094, "training_acc": 50.0, "val_loss": 258.4641778564453, "val_acc": 48.0}
{"epoch": 4, "training_loss": 293.54689819335937, "training_acc": 42.0, "val_loss": 375.3963989257812, "val_acc": 48.0}
{"epoch": 5, "training_loss": 215.40795318603514, "training_acc": 48.0, "val_loss": 345.76127685546874, "val_acc": 48.0}
{"epoch": 6, "training_loss": 368.74269451141356, "training_acc": 54.0, "val_loss": 401.15029296875, "val_acc": 48.0}
{"epoch": 7, "training_loss": 444.29566818237305, "training_acc": 40.0, "val_loss": 488.8617309570312, "val_acc": 48.0}
{"epoch": 8, "training_loss": 434.83620300292966, "training_acc": 47.0, "val_loss": 118.11802825927734, "val_acc": 48.0}
{"epoch": 9, "training_loss": 127.67089027404785, "training_acc": 57.0, "val_loss": 250.01168273925782, "val_acc": 48.0}
{"epoch": 10, "training_loss": 314.0210427856445, "training_acc": 44.0, "val_loss": 300.19908203125, "val_acc": 48.0}
{"epoch": 11, "training_loss": 298.81686737060545, "training_acc": 42.0, "val_loss": 463.9943798828125, "val_acc": 48.0}
{"epoch": 12, "training_loss": 266.9667266845703, "training_acc": 44.0, "val_loss": 54.117132873535155, "val_acc": 48.0}
{"epoch": 13, "training_loss": 192.95572265625, "training_acc": 48.0, "val_loss": 112.03547546386719, "val_acc": 52.0}
{"epoch": 14, "training_loss": 86.75039001464843, "training_acc": 54.0, "val_loss": 296.4251580810547, "val_acc": 48.0}
{"epoch": 15, "training_loss": 256.0655236816406, "training_acc": 46.0, "val_loss": 790.4082836914063, "val_acc": 48.0}
{"epoch": 16, "training_loss": 636.3380139160156, "training_acc": 46.0, "val_loss": 1091.1290478515625, "val_acc": 52.0}
{"epoch": 17, "training_loss": 706.6199243164062, "training_acc": 50.0, "val_loss": 751.5907739257813, "val_acc": 48.0}
{"epoch": 18, "training_loss": 424.8906005859375, "training_acc": 56.0, "val_loss": 119.60822723388672, "val_acc": 52.0}
{"epoch": 19, "training_loss": 452.97234375, "training_acc": 52.0, "val_loss": 552.9939575195312, "val_acc": 52.0}
{"epoch": 20, "training_loss": 391.8407897949219, "training_acc": 50.0, "val_loss": 304.852294921875, "val_acc": 52.0}
{"epoch": 21, "training_loss": 497.6001293945312, "training_acc": 50.0, "val_loss": 969.589853515625, "val_acc": 48.0}
{"epoch": 22, "training_loss": 720.6857641601563, "training_acc": 52.0, "val_loss": 538.6413598632812, "val_acc": 52.0}
{"epoch": 23, "training_loss": 295.9551889038086, "training_acc": 48.0, "val_loss": 514.2800427246094, "val_acc": 52.0}
{"epoch": 24, "training_loss": 364.29083770751953, "training_acc": 42.0, "val_loss": 221.54570739746094, "val_acc": 52.0}
{"epoch": 25, "training_loss": 172.9743505859375, "training_acc": 52.0, "val_loss": 83.94220275878907, "val_acc": 52.0}
{"epoch": 26, "training_loss": 198.2478469848633, "training_acc": 53.0, "val_loss": 47.429929656982424, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.07901504516602, "training_acc": 55.0, "val_loss": 62.25681182861328, "val_acc": 52.0}
{"epoch": 28, "training_loss": 41.28042144775391, "training_acc": 58.0, "val_loss": 35.2721549987793, "val_acc": 56.0}
{"epoch": 29, "training_loss": 36.65349807739258, "training_acc": 60.0, "val_loss": 28.469745101928712, "val_acc": 64.0}
{"epoch": 30, "training_loss": 54.639892578125, "training_acc": 57.0, "val_loss": 127.37998107910157, "val_acc": 48.0}
{"epoch": 31, "training_loss": 67.58065338134766, "training_acc": 57.0, "val_loss": 111.58240325927734, "val_acc": 52.0}
{"epoch": 32, "training_loss": 102.17210845947265, "training_acc": 50.0, "val_loss": 23.183955688476562, "val_acc": 72.0}
{"epoch": 33, "training_loss": 105.97339782714843, "training_acc": 51.0, "val_loss": 264.8572143554687, "val_acc": 48.0}
{"epoch": 34, "training_loss": 252.7080126953125, "training_acc": 54.0, "val_loss": 25.159869537353515, "val_acc": 72.0}
{"epoch": 35, "training_loss": 127.40369209289551, "training_acc": 56.0, "val_loss": 165.89407836914063, "val_acc": 48.0}
{"epoch": 36, "training_loss": 130.85466094970704, "training_acc": 56.0, "val_loss": 105.9691015625, "val_acc": 52.0}
{"epoch": 37, "training_loss": 42.11708251953125, "training_acc": 62.0, "val_loss": 34.61681022644043, "val_acc": 68.0}
{"epoch": 38, "training_loss": 86.2829736328125, "training_acc": 51.0, "val_loss": 111.0016812133789, "val_acc": 48.0}
{"epoch": 39, "training_loss": 67.91485481262207, "training_acc": 53.0, "val_loss": 85.43323303222657, "val_acc": 48.0}
{"epoch": 40, "training_loss": 43.269959716796876, "training_acc": 62.0, "val_loss": 236.25115539550782, "val_acc": 52.0}
{"epoch": 41, "training_loss": 138.0926806640625, "training_acc": 56.0, "val_loss": 265.1463232421875, "val_acc": 52.0}
{"epoch": 42, "training_loss": 264.60005157470704, "training_acc": 42.0, "val_loss": 138.3072659301758, "val_acc": 48.0}
{"epoch": 43, "training_loss": 74.4786297607422, "training_acc": 56.0, "val_loss": 275.5285913085938, "val_acc": 52.0}
{"epoch": 44, "training_loss": 123.77762939453125, "training_acc": 62.0, "val_loss": 44.772289733886716, "val_acc": 56.0}
{"epoch": 45, "training_loss": 146.36648468017577, "training_acc": 49.0, "val_loss": 628.216904296875, "val_acc": 48.0}
{"epoch": 46, "training_loss": 361.8648889160156, "training_acc": 42.0, "val_loss": 143.21155670166016, "val_acc": 48.0}
{"epoch": 47, "training_loss": 94.3727978515625, "training_acc": 51.0, "val_loss": 88.503427734375, "val_acc": 52.0}
{"epoch": 48, "training_loss": 183.64956787109375, "training_acc": 53.0, "val_loss": 33.37178611755371, "val_acc": 60.0}
{"epoch": 49, "training_loss": 102.48954620361329, "training_acc": 50.0, "val_loss": 38.22668701171875, "val_acc": 64.0}
{"epoch": 50, "training_loss": 25.407280578613282, "training_acc": 68.0, "val_loss": 41.85024425506592, "val_acc": 60.0}
{"epoch": 51, "training_loss": 86.28320384979249, "training_acc": 46.0, "val_loss": 33.29895118713379, "val_acc": 72.0}
