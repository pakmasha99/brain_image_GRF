"main_modify.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 20 --layer_control freeze --stratify strat --random_seed 3 --task ADNI_sex --input_option yAware --learning_rate 1e2 --batch_size 16 --weight_decay 1e-8 --BN inst --save_path finetune_results_InstanceBN"
{"epoch": 0, "training_loss": 8560.287910366058, "training_acc": 55.0, "val_loss": 8777.8408203125, "val_acc": 40.0}
{"epoch": 1, "training_loss": 6732.76806640625, "training_acc": 55.0, "val_loss": 11559.43359375, "val_acc": 60.0}
{"epoch": 2, "training_loss": 15860.3662109375, "training_acc": 45.0, "val_loss": 8352.947265625, "val_acc": 60.0}
{"epoch": 3, "training_loss": 10009.924609375, "training_acc": 45.0, "val_loss": 7288.82666015625, "val_acc": 40.0}
{"epoch": 4, "training_loss": 6499.832421875, "training_acc": 55.0, "val_loss": 15364.6357421875, "val_acc": 40.0}
{"epoch": 5, "training_loss": 11365.3619140625, "training_acc": 55.0, "val_loss": 10124.0556640625, "val_acc": 40.0}
{"epoch": 6, "training_loss": 6607.41328125, "training_acc": 55.0, "val_loss": 2670.724365234375, "val_acc": 60.0}
{"epoch": 7, "training_loss": 4648.3123046875, "training_acc": 45.0, "val_loss": 4870.4609375, "val_acc": 60.0}
{"epoch": 8, "training_loss": 5963.859912109375, "training_acc": 45.0, "val_loss": 2988.209228515625, "val_acc": 40.0}
{"epoch": 9, "training_loss": 2438.721240234375, "training_acc": 55.0, "val_loss": 6968.76171875, "val_acc": 40.0}
{"epoch": 10, "training_loss": 5036.505615234375, "training_acc": 55.0, "val_loss": 2293.61279296875, "val_acc": 40.0}
{"epoch": 11, "training_loss": 2773.333203125, "training_acc": 35.0, "val_loss": 2061.892578125, "val_acc": 60.0}
{"epoch": 12, "training_loss": 2584.1682861328127, "training_acc": 35.0, "val_loss": 244.7812957763672, "val_acc": 60.0}
{"epoch": 13, "training_loss": 100.76959228515625, "training_acc": 65.0, "val_loss": 2541.54296875, "val_acc": 40.0}
{"epoch": 14, "training_loss": 1655.990234375, "training_acc": 55.0, "val_loss": 1543.9666748046875, "val_acc": 60.0}
{"epoch": 15, "training_loss": 2354.221875, "training_acc": 45.0, "val_loss": 846.9857788085938, "val_acc": 60.0}
{"epoch": 16, "training_loss": 1010.532275390625, "training_acc": 55.0, "val_loss": 6202.74609375, "val_acc": 40.0}
{"epoch": 17, "training_loss": 4578.06142578125, "training_acc": 55.0, "val_loss": 5372.59814453125, "val_acc": 40.0}
{"epoch": 18, "training_loss": 3676.252685546875, "training_acc": 55.0, "val_loss": 1534.0838623046875, "val_acc": 60.0}
{"epoch": 19, "training_loss": 2106.72353515625, "training_acc": 45.0, "val_loss": 1292.2020263671875, "val_acc": 60.0}
{"epoch": 20, "training_loss": 1664.5593994140625, "training_acc": 45.0, "val_loss": 3343.722412109375, "val_acc": 40.0}
{"epoch": 21, "training_loss": 2213.779443359375, "training_acc": 55.0, "val_loss": 743.2386474609375, "val_acc": 60.0}
{"epoch": 22, "training_loss": 880.1692626953125, "training_acc": 45.0, "val_loss": 770.8760375976562, "val_acc": 40.0}
{"epoch": 23, "training_loss": 436.2518798828125, "training_acc": 55.0, "val_loss": 1371.2806396484375, "val_acc": 60.0}
{"epoch": 24, "training_loss": 1655.396923828125, "training_acc": 45.0, "val_loss": 1751.6422119140625, "val_acc": 40.0}
{"epoch": 25, "training_loss": 1567.13583984375, "training_acc": 55.0, "val_loss": 1009.7709350585938, "val_acc": 40.0}
{"epoch": 26, "training_loss": 985.69755859375, "training_acc": 55.0, "val_loss": 2999.994873046875, "val_acc": 60.0}
{"epoch": 27, "training_loss": 3802.9809326171876, "training_acc": 45.0, "val_loss": 195.18289184570312, "val_acc": 20.0}
{"epoch": 28, "training_loss": 440.6037170410156, "training_acc": 70.0, "val_loss": 190.51451110839844, "val_acc": 40.0}
{"epoch": 29, "training_loss": 108.48646240234375, "training_acc": 65.0, "val_loss": 735.5128784179688, "val_acc": 40.0}
{"epoch": 30, "training_loss": 365.44520263671876, "training_acc": 55.0, "val_loss": 1032.044921875, "val_acc": 40.0}
{"epoch": 31, "training_loss": 550.072900390625, "training_acc": 65.0, "val_loss": 405.6607971191406, "val_acc": 60.0}
{"epoch": 32, "training_loss": 446.6096923828125, "training_acc": 50.0, "val_loss": 274.1548767089844, "val_acc": 40.0}
{"epoch": 33, "training_loss": 217.26708984375, "training_acc": 70.0, "val_loss": 186.89553833007812, "val_acc": 60.0}
{"epoch": 34, "training_loss": 182.43934936523436, "training_acc": 65.0, "val_loss": 624.382568359375, "val_acc": 60.0}
{"epoch": 35, "training_loss": 486.09553773699736, "training_acc": 55.0, "val_loss": 1343.7080078125, "val_acc": 40.0}
{"epoch": 36, "training_loss": 684.3245727539063, "training_acc": 60.0, "val_loss": 1519.6214599609375, "val_acc": 60.0}
{"epoch": 37, "training_loss": 1732.312841796875, "training_acc": 45.0, "val_loss": 2254.246826171875, "val_acc": 40.0}
{"epoch": 38, "training_loss": 2245.79912109375, "training_acc": 55.0, "val_loss": 1345.5579833984375, "val_acc": 40.0}
{"epoch": 39, "training_loss": 1106.910791015625, "training_acc": 55.0, "val_loss": 3713.879638671875, "val_acc": 60.0}
{"epoch": 40, "training_loss": 4617.53798828125, "training_acc": 45.0, "val_loss": 739.61669921875, "val_acc": 60.0}
{"epoch": 41, "training_loss": 1911.52763671875, "training_acc": 35.0, "val_loss": 6842.3330078125, "val_acc": 40.0}
{"epoch": 42, "training_loss": 4864.383984375, "training_acc": 55.0, "val_loss": 2729.599365234375, "val_acc": 40.0}
{"epoch": 43, "training_loss": 1864.978515625, "training_acc": 55.0, "val_loss": 3688.910888671875, "val_acc": 60.0}
{"epoch": 44, "training_loss": 4770.9265625, "training_acc": 45.0, "val_loss": 1111.4522705078125, "val_acc": 60.0}
{"epoch": 45, "training_loss": 1748.7173828125, "training_acc": 45.0, "val_loss": 7489.875, "val_acc": 40.0}
{"epoch": 46, "training_loss": 5553.33603515625, "training_acc": 55.0, "val_loss": 6205.42431640625, "val_acc": 40.0}
{"epoch": 47, "training_loss": 3551.1349609375, "training_acc": 55.0, "val_loss": 2784.630615234375, "val_acc": 60.0}
{"epoch": 48, "training_loss": 3790.539599609375, "training_acc": 45.0, "val_loss": 5924.97021484375, "val_acc": 60.0}
{"epoch": 49, "training_loss": 7682.26787109375, "training_acc": 45.0, "val_loss": 3031.741455078125, "val_acc": 60.0}
{"epoch": 50, "training_loss": 3272.9511901855467, "training_acc": 50.0, "val_loss": 4067.710205078125, "val_acc": 40.0}
{"epoch": 51, "training_loss": 2824.13203125, "training_acc": 55.0, "val_loss": 4724.2958984375, "val_acc": 40.0}
{"epoch": 52, "training_loss": 2921.999951171875, "training_acc": 55.0, "val_loss": 863.7203979492188, "val_acc": 60.0}
