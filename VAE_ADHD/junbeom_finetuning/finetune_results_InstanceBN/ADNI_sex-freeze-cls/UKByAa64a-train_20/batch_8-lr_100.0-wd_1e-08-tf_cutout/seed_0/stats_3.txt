"main_modify.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 20 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_sex --input_option yAware --learning_rate 1e2 --batch_size 8 --weight_decay 1e-8 --BN inst --save_path finetune_results_InstanceBN"
{"epoch": 0, "training_loss": 9958.128577566147, "training_acc": 45.0, "val_loss": 2924.102783203125, "val_acc": 60.0}
{"epoch": 1, "training_loss": 6665.3560546875, "training_acc": 45.0, "val_loss": 1603.690673828125, "val_acc": 40.0}
{"epoch": 2, "training_loss": 1618.8826171875, "training_acc": 55.0, "val_loss": 2809.37890625, "val_acc": 60.0}
{"epoch": 3, "training_loss": 3111.34111328125, "training_acc": 45.0, "val_loss": 6092.1689453125, "val_acc": 40.0}
{"epoch": 4, "training_loss": 5846.262109375, "training_acc": 55.0, "val_loss": 7017.3916015625, "val_acc": 40.0}
{"epoch": 5, "training_loss": 3529.863916015625, "training_acc": 55.0, "val_loss": 3325.091552734375, "val_acc": 60.0}
{"epoch": 6, "training_loss": 3569.943212890625, "training_acc": 45.0, "val_loss": 3139.059326171875, "val_acc": 40.0}
{"epoch": 7, "training_loss": 2207.8661865234376, "training_acc": 45.0, "val_loss": 2063.242919921875, "val_acc": 60.0}
{"epoch": 8, "training_loss": 1509.46748046875, "training_acc": 55.0, "val_loss": 7253.15869140625, "val_acc": 40.0}
{"epoch": 9, "training_loss": 5600.8154296875, "training_acc": 55.0, "val_loss": 2396.777099609375, "val_acc": 40.0}
{"epoch": 10, "training_loss": 1909.8244873046874, "training_acc": 65.0, "val_loss": 4424.23681640625, "val_acc": 60.0}
{"epoch": 11, "training_loss": 4235.827691650391, "training_acc": 45.0, "val_loss": 2073.935302734375, "val_acc": 40.0}
{"epoch": 12, "training_loss": 1659.9564270019532, "training_acc": 45.0, "val_loss": 1544.9090576171875, "val_acc": 60.0}
{"epoch": 13, "training_loss": 1956.686279296875, "training_acc": 45.0, "val_loss": 595.5758056640625, "val_acc": 40.0}
{"epoch": 14, "training_loss": 2234.035205078125, "training_acc": 55.0, "val_loss": 1970.8695068359375, "val_acc": 60.0}
{"epoch": 15, "training_loss": 1346.7844482421874, "training_acc": 65.0, "val_loss": 2478.043212890625, "val_acc": 40.0}
{"epoch": 16, "training_loss": 1170.3587799072266, "training_acc": 65.0, "val_loss": 2479.065673828125, "val_acc": 60.0}
{"epoch": 17, "training_loss": 3771.3254638671874, "training_acc": 25.0, "val_loss": 6013.48583984375, "val_acc": 40.0}
{"epoch": 18, "training_loss": 3164.851611328125, "training_acc": 55.0, "val_loss": 2857.303955078125, "val_acc": 60.0}
{"epoch": 19, "training_loss": 5220.35703125, "training_acc": 45.0, "val_loss": 3172.169921875, "val_acc": 60.0}
{"epoch": 20, "training_loss": 3915.565625, "training_acc": 25.0, "val_loss": 2930.423583984375, "val_acc": 40.0}
{"epoch": 21, "training_loss": 1084.2045043945313, "training_acc": 65.0, "val_loss": 3556.448486328125, "val_acc": 60.0}
{"epoch": 22, "training_loss": 4228.527734375, "training_acc": 45.0, "val_loss": 3216.252685546875, "val_acc": 40.0}
{"epoch": 23, "training_loss": 5181.47236328125, "training_acc": 55.0, "val_loss": 8378.9921875, "val_acc": 40.0}
{"epoch": 24, "training_loss": 4303.378369140625, "training_acc": 55.0, "val_loss": 2958.577880859375, "val_acc": 60.0}
{"epoch": 25, "training_loss": 5876.66591796875, "training_acc": 45.0, "val_loss": 3616.632080078125, "val_acc": 60.0}
{"epoch": 26, "training_loss": 3018.598974609375, "training_acc": 45.0, "val_loss": 7932.8876953125, "val_acc": 40.0}
{"epoch": 27, "training_loss": 5674.578125, "training_acc": 55.0, "val_loss": 3838.116455078125, "val_acc": 40.0}
{"epoch": 28, "training_loss": 2641.80419921875, "training_acc": 45.0, "val_loss": 1650.1572265625, "val_acc": 60.0}
{"epoch": 29, "training_loss": 1955.16103515625, "training_acc": 45.0, "val_loss": 2019.111572265625, "val_acc": 40.0}
{"epoch": 30, "training_loss": 1158.3775756835937, "training_acc": 45.0, "val_loss": 861.3311767578125, "val_acc": 40.0}
{"epoch": 31, "training_loss": 771.8187255859375, "training_acc": 50.0, "val_loss": 527.5919189453125, "val_acc": 60.0}
{"epoch": 32, "training_loss": 1270.1640014648438, "training_acc": 65.0, "val_loss": 2224.090576171875, "val_acc": 40.0}
{"epoch": 33, "training_loss": 2351.233447265625, "training_acc": 35.0, "val_loss": 613.380126953125, "val_acc": 60.0}
{"epoch": 34, "training_loss": 1760.3907470703125, "training_acc": 55.0, "val_loss": 4275.87060546875, "val_acc": 40.0}
{"epoch": 35, "training_loss": 2056.947802734375, "training_acc": 55.0, "val_loss": 3164.110595703125, "val_acc": 60.0}
{"epoch": 36, "training_loss": 3577.386572265625, "training_acc": 45.0, "val_loss": 4713.55908203125, "val_acc": 40.0}
{"epoch": 37, "training_loss": 5466.1462890625, "training_acc": 55.0, "val_loss": 8993.1728515625, "val_acc": 40.0}
{"epoch": 38, "training_loss": 4427.067578125, "training_acc": 55.0, "val_loss": 2699.099365234375, "val_acc": 60.0}
{"epoch": 39, "training_loss": 6097.531640625, "training_acc": 45.0, "val_loss": 4381.0859375, "val_acc": 60.0}
{"epoch": 40, "training_loss": 4334.9830078125, "training_acc": 35.0, "val_loss": 9126.1279296875, "val_acc": 40.0}
{"epoch": 41, "training_loss": 6926.866796875, "training_acc": 55.0, "val_loss": 5452.875, "val_acc": 40.0}
{"epoch": 42, "training_loss": 3169.3885009765627, "training_acc": 45.0, "val_loss": 4591.314453125, "val_acc": 60.0}
{"epoch": 43, "training_loss": 4798.4857421875, "training_acc": 45.0, "val_loss": 2981.479736328125, "val_acc": 40.0}
{"epoch": 44, "training_loss": 3271.428515625, "training_acc": 55.0, "val_loss": 5737.3486328125, "val_acc": 40.0}
{"epoch": 45, "training_loss": 2597.1625885009767, "training_acc": 60.0, "val_loss": 1770.154296875, "val_acc": 60.0}
{"epoch": 46, "training_loss": 2050.5609802246095, "training_acc": 50.0, "val_loss": 1515.106689453125, "val_acc": 40.0}
{"epoch": 47, "training_loss": 1016.9023315429688, "training_acc": 45.0, "val_loss": 1175.49169921875, "val_acc": 60.0}
{"epoch": 48, "training_loss": 508.6268936157227, "training_acc": 70.0, "val_loss": 1203.1279296875, "val_acc": 40.0}
{"epoch": 49, "training_loss": 468.28001708984374, "training_acc": 55.0, "val_loss": 2578.697998046875, "val_acc": 40.0}
{"epoch": 50, "training_loss": 2184.65146484375, "training_acc": 55.0, "val_loss": 331.6820983886719, "val_acc": 60.0}
{"epoch": 51, "training_loss": 347.6583251953125, "training_acc": 60.0, "val_loss": 2188.6806640625, "val_acc": 40.0}
{"epoch": 52, "training_loss": 1254.05263671875, "training_acc": 55.0, "val_loss": 1956.5906982421875, "val_acc": 60.0}
{"epoch": 53, "training_loss": 3197.89892578125, "training_acc": 45.0, "val_loss": 577.7467041015625, "val_acc": 40.0}
{"epoch": 54, "training_loss": 528.3463134765625, "training_acc": 60.0, "val_loss": 846.5460815429688, "val_acc": 60.0}
{"epoch": 55, "training_loss": 723.1261352539062, "training_acc": 50.0, "val_loss": 1230.7152099609375, "val_acc": 40.0}
{"epoch": 56, "training_loss": 517.3784301757812, "training_acc": 55.0, "val_loss": 2637.790283203125, "val_acc": 40.0}
{"epoch": 57, "training_loss": 2043.82841796875, "training_acc": 55.0, "val_loss": 546.1160278320312, "val_acc": 60.0}
{"epoch": 58, "training_loss": 185.21395568847657, "training_acc": 75.0, "val_loss": 1651.9271240234375, "val_acc": 40.0}
{"epoch": 59, "training_loss": 433.944482421875, "training_acc": 80.0, "val_loss": 1262.885986328125, "val_acc": 60.0}
{"epoch": 60, "training_loss": 632.0663208007812, "training_acc": 65.0, "val_loss": 4892.6982421875, "val_acc": 40.0}
{"epoch": 61, "training_loss": 3147.0791137695314, "training_acc": 55.0, "val_loss": 641.3401489257812, "val_acc": 60.0}
{"epoch": 62, "training_loss": 693.69375, "training_acc": 45.0, "val_loss": 2998.597412109375, "val_acc": 40.0}
{"epoch": 63, "training_loss": 1872.3489379882812, "training_acc": 55.0, "val_loss": 1580.3126220703125, "val_acc": 60.0}
{"epoch": 64, "training_loss": 1562.7025634765625, "training_acc": 50.0, "val_loss": 1186.5389404296875, "val_acc": 40.0}
{"epoch": 65, "training_loss": 608.1360046386719, "training_acc": 60.0, "val_loss": 1086.492431640625, "val_acc": 60.0}
{"epoch": 66, "training_loss": 510.9890441894531, "training_acc": 70.0, "val_loss": 3947.01953125, "val_acc": 40.0}
{"epoch": 67, "training_loss": 1649.6215209960938, "training_acc": 60.0, "val_loss": 2665.774169921875, "val_acc": 60.0}
{"epoch": 68, "training_loss": 3485.9372436523436, "training_acc": 45.0, "val_loss": 836.6148681640625, "val_acc": 40.0}
{"epoch": 69, "training_loss": 1000.8236145019531, "training_acc": 55.0, "val_loss": 2034.23828125, "val_acc": 60.0}
