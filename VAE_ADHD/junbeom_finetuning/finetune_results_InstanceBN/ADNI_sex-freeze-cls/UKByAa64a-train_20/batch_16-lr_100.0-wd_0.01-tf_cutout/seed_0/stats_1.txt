"main_modify.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 20 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_sex --input_option yAware --learning_rate 1e2 --batch_size 16 --weight_decay 1e-2 --BN inst --save_path finetune_results_InstanceBN"
{"epoch": 0, "training_loss": 8523.80548439026, "training_acc": 50.0, "val_loss": 4611.6533203125, "val_acc": 60.0}
{"epoch": 1, "training_loss": 5029.2658203125, "training_acc": 60.0, "val_loss": 20844.982421875, "val_acc": 40.0}
{"epoch": 2, "training_loss": 17189.73671875, "training_acc": 50.0, "val_loss": 12282.181640625, "val_acc": 40.0}
{"epoch": 3, "training_loss": 9400.708044433593, "training_acc": 50.0, "val_loss": 5563.03369140625, "val_acc": 60.0}
{"epoch": 4, "training_loss": 7434.19462890625, "training_acc": 50.0, "val_loss": 10636.603515625, "val_acc": 60.0}
{"epoch": 5, "training_loss": 12886.8796875, "training_acc": 50.0, "val_loss": 5388.646484375, "val_acc": 60.0}
{"epoch": 6, "training_loss": 6129.156414794922, "training_acc": 50.0, "val_loss": 6311.0830078125, "val_acc": 40.0}
{"epoch": 7, "training_loss": 6008.8609375, "training_acc": 50.0, "val_loss": 11808.1201171875, "val_acc": 40.0}
{"epoch": 8, "training_loss": 9434.664453125, "training_acc": 50.0, "val_loss": 5557.7978515625, "val_acc": 40.0}
{"epoch": 9, "training_loss": 4281.2591796875, "training_acc": 40.0, "val_loss": 2927.989013671875, "val_acc": 60.0}
{"epoch": 10, "training_loss": 3810.351806640625, "training_acc": 50.0, "val_loss": 4347.8251953125, "val_acc": 60.0}
{"epoch": 11, "training_loss": 5133.619287109375, "training_acc": 50.0, "val_loss": 992.9402465820312, "val_acc": 60.0}
{"epoch": 12, "training_loss": 1079.0252197265625, "training_acc": 60.0, "val_loss": 5271.56103515625, "val_acc": 40.0}
{"epoch": 13, "training_loss": 4455.216162109375, "training_acc": 50.0, "val_loss": 4898.80029296875, "val_acc": 40.0}
{"epoch": 14, "training_loss": 3472.9275390625, "training_acc": 50.0, "val_loss": 1113.06884765625, "val_acc": 60.0}
{"epoch": 15, "training_loss": 1803.5322265625, "training_acc": 50.0, "val_loss": 3145.14892578125, "val_acc": 60.0}
{"epoch": 16, "training_loss": 3644.525, "training_acc": 50.0, "val_loss": 326.4051208496094, "val_acc": 60.0}
{"epoch": 17, "training_loss": 882.972314453125, "training_acc": 50.0, "val_loss": 5265.92138671875, "val_acc": 40.0}
{"epoch": 18, "training_loss": 4373.86865234375, "training_acc": 50.0, "val_loss": 3295.015625, "val_acc": 40.0}
{"epoch": 19, "training_loss": 2484.6359016418455, "training_acc": 50.0, "val_loss": 2233.772216796875, "val_acc": 60.0}
{"epoch": 20, "training_loss": 3042.34814453125, "training_acc": 50.0, "val_loss": 2854.46923828125, "val_acc": 60.0}
{"epoch": 21, "training_loss": 2878.658984375, "training_acc": 50.0, "val_loss": 2076.804443359375, "val_acc": 40.0}
{"epoch": 22, "training_loss": 2153.1044921875, "training_acc": 50.0, "val_loss": 4904.22119140625, "val_acc": 40.0}
{"epoch": 23, "training_loss": 3721.41796875, "training_acc": 50.0, "val_loss": 110.86449432373047, "val_acc": 60.0}
{"epoch": 24, "training_loss": 137.54925537109375, "training_acc": 50.0, "val_loss": 2268.739990234375, "val_acc": 60.0}
{"epoch": 25, "training_loss": 2778.6456787109373, "training_acc": 50.0, "val_loss": 735.1121826171875, "val_acc": 60.0}
{"epoch": 26, "training_loss": 771.1822143554688, "training_acc": 60.0, "val_loss": 3098.766845703125, "val_acc": 40.0}
{"epoch": 27, "training_loss": 2541.1673828125, "training_acc": 50.0, "val_loss": 890.0505981445312, "val_acc": 40.0}
{"epoch": 28, "training_loss": 1235.622119140625, "training_acc": 40.0, "val_loss": 1996.0386962890625, "val_acc": 60.0}
{"epoch": 29, "training_loss": 2496.8794921875, "training_acc": 50.0, "val_loss": 86.79833221435547, "val_acc": 60.0}
{"epoch": 30, "training_loss": 489.0923583984375, "training_acc": 50.0, "val_loss": 3068.154052734375, "val_acc": 40.0}
{"epoch": 31, "training_loss": 2371.22626953125, "training_acc": 50.0, "val_loss": 451.7404479980469, "val_acc": 60.0}
{"epoch": 32, "training_loss": 741.70244140625, "training_acc": 50.0, "val_loss": 790.5443725585938, "val_acc": 60.0}
{"epoch": 33, "training_loss": 740.1252197265625, "training_acc": 60.0, "val_loss": 1460.8660888671875, "val_acc": 40.0}
{"epoch": 34, "training_loss": 1088.837890625, "training_acc": 50.0, "val_loss": 997.0792846679688, "val_acc": 60.0}
{"epoch": 35, "training_loss": 1363.528857421875, "training_acc": 50.0, "val_loss": 776.5867309570312, "val_acc": 60.0}
{"epoch": 36, "training_loss": 1211.6446044921875, "training_acc": 40.0, "val_loss": 1542.30078125, "val_acc": 40.0}
{"epoch": 37, "training_loss": 1166.3478525161743, "training_acc": 50.0, "val_loss": 1292.569580078125, "val_acc": 60.0}
{"epoch": 38, "training_loss": 1765.9064453125, "training_acc": 50.0, "val_loss": 146.1149139404297, "val_acc": 40.0}
{"epoch": 39, "training_loss": 297.1222717285156, "training_acc": 50.0, "val_loss": 664.9223022460938, "val_acc": 40.0}
{"epoch": 40, "training_loss": 640.5444580078125, "training_acc": 50.0, "val_loss": 909.8251953125, "val_acc": 60.0}
{"epoch": 41, "training_loss": 1064.7260498046876, "training_acc": 40.0, "val_loss": 223.27244567871094, "val_acc": 60.0}
{"epoch": 42, "training_loss": 225.540869140625, "training_acc": 60.0, "val_loss": 299.8472595214844, "val_acc": 40.0}
{"epoch": 43, "training_loss": 534.1331298828125, "training_acc": 40.0, "val_loss": 311.22216796875, "val_acc": 60.0}
{"epoch": 44, "training_loss": 576.6021240234375, "training_acc": 50.0, "val_loss": 1808.618408203125, "val_acc": 40.0}
{"epoch": 45, "training_loss": 1178.3282470703125, "training_acc": 50.0, "val_loss": 1829.3170166015625, "val_acc": 60.0}
{"epoch": 46, "training_loss": 2451.291748046875, "training_acc": 50.0, "val_loss": 1639.162109375, "val_acc": 60.0}
{"epoch": 47, "training_loss": 1732.118341064453, "training_acc": 50.0, "val_loss": 1853.0931396484375, "val_acc": 40.0}
{"epoch": 48, "training_loss": 1456.4418212890625, "training_acc": 50.0, "val_loss": 533.8268432617188, "val_acc": 60.0}
{"epoch": 49, "training_loss": 735.7479736328125, "training_acc": 50.0, "val_loss": 5.179662227630615, "val_acc": 40.0}
{"epoch": 50, "training_loss": 6.578032302856445, "training_acc": 60.0, "val_loss": 591.5817260742188, "val_acc": 40.0}
{"epoch": 51, "training_loss": 444.7998565673828, "training_acc": 50.0, "val_loss": 284.0621032714844, "val_acc": 40.0}
{"epoch": 52, "training_loss": 200.19240112304686, "training_acc": 60.0, "val_loss": 184.94772338867188, "val_acc": 60.0}
{"epoch": 53, "training_loss": 393.306005859375, "training_acc": 50.0, "val_loss": 976.6900634765625, "val_acc": 40.0}
{"epoch": 54, "training_loss": 991.4783447265625, "training_acc": 40.0, "val_loss": 368.3853454589844, "val_acc": 60.0}
{"epoch": 55, "training_loss": 594.016650390625, "training_acc": 50.0, "val_loss": 1375.8421630859375, "val_acc": 40.0}
{"epoch": 56, "training_loss": 970.534228515625, "training_acc": 50.0, "val_loss": 257.97039794921875, "val_acc": 60.0}
{"epoch": 57, "training_loss": 424.5798828125, "training_acc": 50.0, "val_loss": 620.395263671875, "val_acc": 40.0}
{"epoch": 58, "training_loss": 821.36416015625, "training_acc": 40.0, "val_loss": 558.4406127929688, "val_acc": 60.0}
{"epoch": 59, "training_loss": 769.4732666015625, "training_acc": 50.0, "val_loss": 1470.7041015625, "val_acc": 40.0}
{"epoch": 60, "training_loss": 1002.4551544189453, "training_acc": 50.0, "val_loss": 83.00830841064453, "val_acc": 60.0}
{"epoch": 61, "training_loss": 164.1435791015625, "training_acc": 60.0, "val_loss": 1230.77001953125, "val_acc": 40.0}
{"epoch": 62, "training_loss": 738.6149505615234, "training_acc": 60.0, "val_loss": 501.5419921875, "val_acc": 60.0}
{"epoch": 63, "training_loss": 452.93224639892577, "training_acc": 60.0, "val_loss": 271.10394287109375, "val_acc": 40.0}
{"epoch": 64, "training_loss": 531.907275390625, "training_acc": 40.0, "val_loss": 177.7735595703125, "val_acc": 60.0}
{"epoch": 65, "training_loss": 539.4227416992187, "training_acc": 50.0, "val_loss": 2357.1171875, "val_acc": 40.0}
{"epoch": 66, "training_loss": 1659.7970397949218, "training_acc": 50.0, "val_loss": 1849.4984130859375, "val_acc": 60.0}
{"epoch": 67, "training_loss": 2439.394873046875, "training_acc": 50.0, "val_loss": 1227.6290283203125, "val_acc": 60.0}
{"epoch": 68, "training_loss": 2190.72265625, "training_acc": 30.0, "val_loss": 2000.5242919921875, "val_acc": 40.0}
