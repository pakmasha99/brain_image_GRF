"main_modify.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 20 --layer_control freeze --stratify strat --random_seed 5 --task ADNI_sex --input_option yAware --learning_rate 1e2 --batch_size 8 --weight_decay 1e-6 --BN inst --save_path finetune_results_InstanceBN"
{"epoch": 0, "training_loss": 13662.886137461663, "training_acc": 40.0, "val_loss": 6961.0283203125, "val_acc": 40.0}
{"epoch": 1, "training_loss": 6486.4416015625, "training_acc": 50.0, "val_loss": 1869.6082763671875, "val_acc": 60.0}
{"epoch": 2, "training_loss": 3060.746875, "training_acc": 40.0, "val_loss": 1932.6090087890625, "val_acc": 40.0}
{"epoch": 3, "training_loss": 1910.003662109375, "training_acc": 50.0, "val_loss": 281.0511474609375, "val_acc": 40.0}
{"epoch": 4, "training_loss": 1282.89560546875, "training_acc": 40.0, "val_loss": 2165.697021484375, "val_acc": 60.0}
{"epoch": 5, "training_loss": 1147.9974700927735, "training_acc": 50.0, "val_loss": 1516.5833740234375, "val_acc": 60.0}
{"epoch": 6, "training_loss": 1501.8926513671875, "training_acc": 50.0, "val_loss": 1776.1070556640625, "val_acc": 40.0}
{"epoch": 7, "training_loss": 903.8370361328125, "training_acc": 50.0, "val_loss": 2237.09375, "val_acc": 40.0}
{"epoch": 8, "training_loss": 1660.4899139404297, "training_acc": 50.0, "val_loss": 1087.3382568359375, "val_acc": 60.0}
{"epoch": 9, "training_loss": 1049.6829010009765, "training_acc": 50.0, "val_loss": 747.2763061523438, "val_acc": 60.0}
{"epoch": 10, "training_loss": 1068.621044921875, "training_acc": 30.0, "val_loss": 246.23452758789062, "val_acc": 60.0}
{"epoch": 11, "training_loss": 1245.7015869140625, "training_acc": 60.0, "val_loss": 2423.950927734375, "val_acc": 40.0}
{"epoch": 12, "training_loss": 1877.558251953125, "training_acc": 40.0, "val_loss": 534.5463256835938, "val_acc": 40.0}
{"epoch": 13, "training_loss": 959.13076171875, "training_acc": 40.0, "val_loss": 828.18310546875, "val_acc": 60.0}
{"epoch": 14, "training_loss": 914.4813232421875, "training_acc": 60.0, "val_loss": 61.2496337890625, "val_acc": 40.0}
{"epoch": 15, "training_loss": 3166.646484375, "training_acc": 35.0, "val_loss": 2096.375244140625, "val_acc": 60.0}
{"epoch": 16, "training_loss": 760.0282104492187, "training_acc": 60.0, "val_loss": 9725.5126953125, "val_acc": 40.0}
{"epoch": 17, "training_loss": 9150.519921875, "training_acc": 50.0, "val_loss": 7435.55419921875, "val_acc": 40.0}
{"epoch": 18, "training_loss": 3996.52783203125, "training_acc": 50.0, "val_loss": 5975.00830078125, "val_acc": 60.0}
{"epoch": 19, "training_loss": 7845.493017578125, "training_acc": 50.0, "val_loss": 3863.161376953125, "val_acc": 60.0}
{"epoch": 20, "training_loss": 3117.4324462890627, "training_acc": 50.0, "val_loss": 9353.0283203125, "val_acc": 40.0}
{"epoch": 21, "training_loss": 8609.678125, "training_acc": 50.0, "val_loss": 5524.28466796875, "val_acc": 40.0}
{"epoch": 22, "training_loss": 1876.9513916015626, "training_acc": 60.0, "val_loss": 3855.563232421875, "val_acc": 60.0}
{"epoch": 23, "training_loss": 4017.3595947265626, "training_acc": 50.0, "val_loss": 2870.1767578125, "val_acc": 40.0}
{"epoch": 24, "training_loss": 2465.4168701171875, "training_acc": 50.0, "val_loss": 253.6867218017578, "val_acc": 60.0}
{"epoch": 25, "training_loss": 482.7818115234375, "training_acc": 55.0, "val_loss": 535.0992431640625, "val_acc": 40.0}
{"epoch": 26, "training_loss": 1239.4906005859375, "training_acc": 50.0, "val_loss": 187.77740478515625, "val_acc": 60.0}
{"epoch": 27, "training_loss": 1924.2168212890624, "training_acc": 55.0, "val_loss": 2847.530517578125, "val_acc": 40.0}
{"epoch": 28, "training_loss": 1427.1218505859374, "training_acc": 50.0, "val_loss": 163.78306579589844, "val_acc": 60.0}
{"epoch": 29, "training_loss": 1709.0576782226562, "training_acc": 55.0, "val_loss": 1197.8809814453125, "val_acc": 40.0}
{"epoch": 30, "training_loss": 2986.3111328125, "training_acc": 40.0, "val_loss": 3447.624267578125, "val_acc": 60.0}
{"epoch": 31, "training_loss": 3250.82705078125, "training_acc": 40.0, "val_loss": 4563.60107421875, "val_acc": 40.0}
{"epoch": 32, "training_loss": 3561.1867797851564, "training_acc": 30.0, "val_loss": 26.563352584838867, "val_acc": 80.0}
{"epoch": 33, "training_loss": 315.4910827636719, "training_acc": 75.0, "val_loss": 809.603515625, "val_acc": 60.0}
{"epoch": 34, "training_loss": 653.451171875, "training_acc": 50.0, "val_loss": 1744.3670654296875, "val_acc": 60.0}
{"epoch": 35, "training_loss": 1875.7496337890625, "training_acc": 50.0, "val_loss": 2355.6044921875, "val_acc": 40.0}
{"epoch": 36, "training_loss": 2014.94814453125, "training_acc": 30.0, "val_loss": 633.5166015625, "val_acc": 40.0}
{"epoch": 37, "training_loss": 347.7520385742188, "training_acc": 70.0, "val_loss": 361.5473327636719, "val_acc": 60.0}
{"epoch": 38, "training_loss": 1160.6689208984376, "training_acc": 45.0, "val_loss": 1072.0921630859375, "val_acc": 60.0}
{"epoch": 39, "training_loss": 1119.5776123046876, "training_acc": 40.0, "val_loss": 296.49298095703125, "val_acc": 60.0}
{"epoch": 40, "training_loss": 678.9240524291993, "training_acc": 40.0, "val_loss": 1370.490234375, "val_acc": 60.0}
{"epoch": 41, "training_loss": 1037.5450500488282, "training_acc": 60.0, "val_loss": 1596.7965087890625, "val_acc": 40.0}
{"epoch": 42, "training_loss": 841.171337890625, "training_acc": 70.0, "val_loss": 573.7551879882812, "val_acc": 60.0}
{"epoch": 43, "training_loss": 2663.9864501953125, "training_acc": 30.0, "val_loss": 1447.691650390625, "val_acc": 40.0}
{"epoch": 44, "training_loss": 2132.96474609375, "training_acc": 50.0, "val_loss": 2942.98291015625, "val_acc": 60.0}
{"epoch": 45, "training_loss": 1962.106494140625, "training_acc": 60.0, "val_loss": 5330.29638671875, "val_acc": 40.0}
{"epoch": 46, "training_loss": 3884.74794921875, "training_acc": 50.0, "val_loss": 1192.5665283203125, "val_acc": 60.0}
{"epoch": 47, "training_loss": 1502.66220703125, "training_acc": 50.0, "val_loss": 2431.923828125, "val_acc": 40.0}
{"epoch": 48, "training_loss": 2310.2576171875, "training_acc": 50.0, "val_loss": 186.502197265625, "val_acc": 60.0}
{"epoch": 49, "training_loss": 1524.66064453125, "training_acc": 70.0, "val_loss": 707.3370971679688, "val_acc": 60.0}
{"epoch": 50, "training_loss": 2460.5645751953125, "training_acc": 50.0, "val_loss": 3573.888671875, "val_acc": 40.0}
{"epoch": 51, "training_loss": 1382.2077209472657, "training_acc": 60.0, "val_loss": 5503.67333984375, "val_acc": 60.0}
