"main_modify.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 20 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_sex --input_option yAware --learning_rate 1e2 --batch_size 8 --weight_decay 1e-2 --BN inst --save_path finetune_results_InstanceBN"
{"epoch": 0, "training_loss": 5966.932440328598, "training_acc": 55.0, "val_loss": 9117.1591796875, "val_acc": 60.0}
{"epoch": 1, "training_loss": 13853.80673828125, "training_acc": 45.0, "val_loss": 5059.30712890625, "val_acc": 60.0}
{"epoch": 2, "training_loss": 6882.450610351562, "training_acc": 45.0, "val_loss": 11351.1923828125, "val_acc": 40.0}
{"epoch": 3, "training_loss": 6778.8361328125, "training_acc": 55.0, "val_loss": 1333.672119140625, "val_acc": 60.0}
{"epoch": 4, "training_loss": 4538.03916015625, "training_acc": 45.0, "val_loss": 3650.426513671875, "val_acc": 60.0}
{"epoch": 5, "training_loss": 3568.5771484375, "training_acc": 35.0, "val_loss": 6113.12841796875, "val_acc": 40.0}
{"epoch": 6, "training_loss": 4572.7255859375, "training_acc": 55.0, "val_loss": 759.3955078125, "val_acc": 40.0}
{"epoch": 7, "training_loss": 2163.05400390625, "training_acc": 55.0, "val_loss": 3810.687255859375, "val_acc": 60.0}
{"epoch": 8, "training_loss": 3239.0257572174073, "training_acc": 55.0, "val_loss": 2713.1455078125, "val_acc": 40.0}
{"epoch": 9, "training_loss": 2102.403759765625, "training_acc": 55.0, "val_loss": 305.4225769042969, "val_acc": 40.0}
{"epoch": 10, "training_loss": 1439.41337890625, "training_acc": 55.0, "val_loss": 1846.982666015625, "val_acc": 60.0}
{"epoch": 11, "training_loss": 1207.5685302734375, "training_acc": 55.0, "val_loss": 4313.4560546875, "val_acc": 40.0}
{"epoch": 12, "training_loss": 2785.64970703125, "training_acc": 55.0, "val_loss": 814.6519165039062, "val_acc": 60.0}
{"epoch": 13, "training_loss": 1916.3303466796874, "training_acc": 45.0, "val_loss": 240.1386260986328, "val_acc": 40.0}
{"epoch": 14, "training_loss": 690.1210083007812, "training_acc": 55.0, "val_loss": 315.7397155761719, "val_acc": 60.0}
{"epoch": 15, "training_loss": 702.3106506347656, "training_acc": 45.0, "val_loss": 2595.959228515625, "val_acc": 40.0}
{"epoch": 16, "training_loss": 2269.94794921875, "training_acc": 55.0, "val_loss": 84.0168228149414, "val_acc": 60.0}
{"epoch": 17, "training_loss": 582.3633911132813, "training_acc": 35.0, "val_loss": 389.1163635253906, "val_acc": 40.0}
{"epoch": 18, "training_loss": 704.7279296875, "training_acc": 55.0, "val_loss": 448.6266784667969, "val_acc": 40.0}
{"epoch": 19, "training_loss": 340.35792846679686, "training_acc": 55.0, "val_loss": 46.0988883972168, "val_acc": 60.0}
{"epoch": 20, "training_loss": 829.8903533935547, "training_acc": 55.0, "val_loss": 531.7775268554688, "val_acc": 40.0}
{"epoch": 21, "training_loss": 1992.0744873046874, "training_acc": 35.0, "val_loss": 811.2453002929688, "val_acc": 60.0}
{"epoch": 22, "training_loss": 745.4639892578125, "training_acc": 65.0, "val_loss": 2028.234375, "val_acc": 40.0}
{"epoch": 23, "training_loss": 1150.3132263183593, "training_acc": 55.0, "val_loss": 1944.058837890625, "val_acc": 60.0}
{"epoch": 24, "training_loss": 2303.5895263671873, "training_acc": 35.0, "val_loss": 3464.839111328125, "val_acc": 40.0}
{"epoch": 25, "training_loss": 1738.9708129882813, "training_acc": 55.0, "val_loss": 1462.44384765625, "val_acc": 60.0}
{"epoch": 26, "training_loss": 1541.7358642578124, "training_acc": 45.0, "val_loss": 2078.4453125, "val_acc": 40.0}
{"epoch": 27, "training_loss": 885.5068725585937, "training_acc": 65.0, "val_loss": 1508.1922607421875, "val_acc": 60.0}
{"epoch": 28, "training_loss": 1269.781005859375, "training_acc": 55.0, "val_loss": 2822.62939453125, "val_acc": 40.0}
{"epoch": 29, "training_loss": 1275.5920974731446, "training_acc": 65.0, "val_loss": 913.3759765625, "val_acc": 60.0}
{"epoch": 30, "training_loss": 802.66201171875, "training_acc": 35.0, "val_loss": 788.1331787109375, "val_acc": 40.0}
{"epoch": 31, "training_loss": 591.6816040039063, "training_acc": 55.0, "val_loss": 821.6256103515625, "val_acc": 60.0}
{"epoch": 32, "training_loss": 425.6212890625, "training_acc": 65.0, "val_loss": 24.20157241821289, "val_acc": 40.0}
{"epoch": 33, "training_loss": 306.88101387023926, "training_acc": 55.0, "val_loss": 934.8438720703125, "val_acc": 60.0}
{"epoch": 34, "training_loss": 1106.86162109375, "training_acc": 45.0, "val_loss": 2189.65234375, "val_acc": 40.0}
{"epoch": 35, "training_loss": 1239.4442504882813, "training_acc": 55.0, "val_loss": 1833.6031494140625, "val_acc": 60.0}
{"epoch": 36, "training_loss": 1967.4787231445312, "training_acc": 45.0, "val_loss": 3326.977294921875, "val_acc": 40.0}
{"epoch": 37, "training_loss": 2090.808935546875, "training_acc": 45.0, "val_loss": 1656.101806640625, "val_acc": 60.0}
{"epoch": 38, "training_loss": 1291.910986328125, "training_acc": 55.0, "val_loss": 3981.154052734375, "val_acc": 40.0}
{"epoch": 39, "training_loss": 2101.284154129028, "training_acc": 55.0, "val_loss": 3064.283935546875, "val_acc": 60.0}
{"epoch": 40, "training_loss": 4731.410888671875, "training_acc": 45.0, "val_loss": 1047.0888671875, "val_acc": 40.0}
{"epoch": 41, "training_loss": 1538.3755859375, "training_acc": 55.0, "val_loss": 292.82574462890625, "val_acc": 40.0}
{"epoch": 42, "training_loss": 1063.6534515380858, "training_acc": 65.0, "val_loss": 823.8793334960938, "val_acc": 60.0}
{"epoch": 43, "training_loss": 1382.868896484375, "training_acc": 55.0, "val_loss": 1566.923828125, "val_acc": 40.0}
{"epoch": 44, "training_loss": 960.8024169921875, "training_acc": 65.0, "val_loss": 192.24632263183594, "val_acc": 60.0}
{"epoch": 45, "training_loss": 1874.880615234375, "training_acc": 45.0, "val_loss": 2276.222412109375, "val_acc": 40.0}
{"epoch": 46, "training_loss": 819.4732299804688, "training_acc": 65.0, "val_loss": 525.25048828125, "val_acc": 60.0}
{"epoch": 47, "training_loss": 708.1702392578125, "training_acc": 65.0, "val_loss": 1159.4296875, "val_acc": 40.0}
{"epoch": 48, "training_loss": 1019.4439697265625, "training_acc": 45.0, "val_loss": 1164.2276611328125, "val_acc": 40.0}
{"epoch": 49, "training_loss": 1080.3335815429687, "training_acc": 45.0, "val_loss": 5.025653839111328, "val_acc": 40.0}
{"epoch": 50, "training_loss": 224.07116961479187, "training_acc": 65.0, "val_loss": 1389.11474609375, "val_acc": 60.0}
{"epoch": 51, "training_loss": 1548.8049072265626, "training_acc": 35.0, "val_loss": 662.3350830078125, "val_acc": 60.0}
{"epoch": 52, "training_loss": 1103.3850463867188, "training_acc": 35.0, "val_loss": 1109.104248046875, "val_acc": 40.0}
{"epoch": 53, "training_loss": 1452.30380859375, "training_acc": 45.0, "val_loss": 614.80029296875, "val_acc": 40.0}
{"epoch": 54, "training_loss": 557.6163208007813, "training_acc": 55.0, "val_loss": 1592.0535888671875, "val_acc": 60.0}
{"epoch": 55, "training_loss": 1839.007147216797, "training_acc": 45.0, "val_loss": 4440.744140625, "val_acc": 40.0}
{"epoch": 56, "training_loss": 3599.219921875, "training_acc": 55.0, "val_loss": 278.6399841308594, "val_acc": 60.0}
{"epoch": 57, "training_loss": 1352.0193237304688, "training_acc": 45.0, "val_loss": 1397.6712646484375, "val_acc": 40.0}
{"epoch": 58, "training_loss": 1503.656787109375, "training_acc": 55.0, "val_loss": 626.5173950195312, "val_acc": 60.0}
{"epoch": 59, "training_loss": 888.6133056640625, "training_acc": 45.0, "val_loss": 195.24168395996094, "val_acc": 40.0}
{"epoch": 60, "training_loss": 599.1397155761719, "training_acc": 55.0, "val_loss": 1663.2662353515625, "val_acc": 40.0}
{"epoch": 61, "training_loss": 1052.3273803710938, "training_acc": 55.0, "val_loss": 480.5889587402344, "val_acc": 60.0}
{"epoch": 62, "training_loss": 260.9139129638672, "training_acc": 55.0, "val_loss": 2646.770751953125, "val_acc": 40.0}
{"epoch": 63, "training_loss": 1869.6647705078126, "training_acc": 55.0, "val_loss": 1944.1331787109375, "val_acc": 60.0}
{"epoch": 64, "training_loss": 3905.74306640625, "training_acc": 45.0, "val_loss": 1995.0819091796875, "val_acc": 40.0}
{"epoch": 65, "training_loss": 2134.8276611328124, "training_acc": 55.0, "val_loss": 989.4420166015625, "val_acc": 40.0}
{"epoch": 66, "training_loss": 2445.93564453125, "training_acc": 45.0, "val_loss": 1017.0333862304688, "val_acc": 60.0}
{"epoch": 67, "training_loss": 1908.16787109375, "training_acc": 55.0, "val_loss": 3741.360595703125, "val_acc": 40.0}
{"epoch": 68, "training_loss": 2500.814306640625, "training_acc": 45.0, "val_loss": 3457.270751953125, "val_acc": 60.0}
