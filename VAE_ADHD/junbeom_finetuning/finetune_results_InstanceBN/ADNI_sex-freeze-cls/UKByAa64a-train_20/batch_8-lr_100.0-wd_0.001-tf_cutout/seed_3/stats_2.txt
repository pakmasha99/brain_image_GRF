"main_modify.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 20 --layer_control freeze --stratify strat --random_seed 3 --task ADNI_sex --input_option yAware --learning_rate 1e2 --batch_size 8 --weight_decay 1e-3 --BN inst --save_path finetune_results_InstanceBN"
{"epoch": 0, "training_loss": 8581.425655269622, "training_acc": 50.0, "val_loss": 8100.0908203125, "val_acc": 60.0}
{"epoch": 1, "training_loss": 15147.6095703125, "training_acc": 50.0, "val_loss": 8059.53271484375, "val_acc": 60.0}
{"epoch": 2, "training_loss": 7168.028393554688, "training_acc": 50.0, "val_loss": 18059.787109375, "val_acc": 40.0}
{"epoch": 3, "training_loss": 14933.91640625, "training_acc": 50.0, "val_loss": 9599.1728515625, "val_acc": 40.0}
{"epoch": 4, "training_loss": 7291.611791992187, "training_acc": 40.0, "val_loss": 9097.6318359375, "val_acc": 60.0}
{"epoch": 5, "training_loss": 11071.728125, "training_acc": 50.0, "val_loss": 4446.84716796875, "val_acc": 60.0}
{"epoch": 6, "training_loss": 3157.799609375, "training_acc": 50.0, "val_loss": 13226.033203125, "val_acc": 40.0}
{"epoch": 7, "training_loss": 11307.11484375, "training_acc": 50.0, "val_loss": 9189.5263671875, "val_acc": 40.0}
{"epoch": 8, "training_loss": 4338.508349609375, "training_acc": 50.0, "val_loss": 4691.8525390625, "val_acc": 60.0}
{"epoch": 9, "training_loss": 6392.15263671875, "training_acc": 50.0, "val_loss": 2507.86962890625, "val_acc": 60.0}
{"epoch": 10, "training_loss": 2290.64208984375, "training_acc": 50.0, "val_loss": 4619.86669921875, "val_acc": 40.0}
{"epoch": 11, "training_loss": 2292.662481689453, "training_acc": 60.0, "val_loss": 1470.019775390625, "val_acc": 60.0}
{"epoch": 12, "training_loss": 1948.2517822265625, "training_acc": 40.0, "val_loss": 1023.9843139648438, "val_acc": 40.0}
{"epoch": 13, "training_loss": 996.2230102539063, "training_acc": 60.0, "val_loss": 1354.741943359375, "val_acc": 40.0}
{"epoch": 14, "training_loss": 1002.092578125, "training_acc": 50.0, "val_loss": 2139.944091796875, "val_acc": 60.0}
{"epoch": 15, "training_loss": 2825.10791015625, "training_acc": 50.0, "val_loss": 1329.1549072265625, "val_acc": 40.0}
{"epoch": 16, "training_loss": 1518.1740234375, "training_acc": 50.0, "val_loss": 781.7311401367188, "val_acc": 60.0}
{"epoch": 17, "training_loss": 1230.2487548828126, "training_acc": 50.0, "val_loss": 2830.9365234375, "val_acc": 40.0}
{"epoch": 18, "training_loss": 2587.25361328125, "training_acc": 50.0, "val_loss": 332.436279296875, "val_acc": 60.0}
{"epoch": 19, "training_loss": 877.7610595703125, "training_acc": 50.0, "val_loss": 2696.598876953125, "val_acc": 40.0}
{"epoch": 20, "training_loss": 2617.37001953125, "training_acc": 50.0, "val_loss": 507.9676208496094, "val_acc": 60.0}
{"epoch": 21, "training_loss": 1291.1022109985352, "training_acc": 50.0, "val_loss": 2852.8662109375, "val_acc": 40.0}
{"epoch": 22, "training_loss": 2235.703564453125, "training_acc": 50.0, "val_loss": 1806.755859375, "val_acc": 60.0}
{"epoch": 23, "training_loss": 2764.387109375, "training_acc": 50.0, "val_loss": 584.4060668945312, "val_acc": 40.0}
{"epoch": 24, "training_loss": 722.3357574462891, "training_acc": 40.0, "val_loss": 1851.3582763671875, "val_acc": 40.0}
{"epoch": 25, "training_loss": 1339.8904296875, "training_acc": 50.0, "val_loss": 1523.8255615234375, "val_acc": 60.0}
{"epoch": 26, "training_loss": 1754.9007186889648, "training_acc": 30.0, "val_loss": 1084.7703857421875, "val_acc": 60.0}
{"epoch": 27, "training_loss": 1513.1473388671875, "training_acc": 40.0, "val_loss": 2180.967041015625, "val_acc": 40.0}
{"epoch": 28, "training_loss": 1005.1406860351562, "training_acc": 50.0, "val_loss": 1269.8323974609375, "val_acc": 40.0}
{"epoch": 29, "training_loss": 825.6996459960938, "training_acc": 60.0, "val_loss": 1015.4181518554688, "val_acc": 60.0}
{"epoch": 30, "training_loss": 1977.1453125, "training_acc": 20.0, "val_loss": 1700.1302490234375, "val_acc": 60.0}
{"epoch": 31, "training_loss": 2448.869873046875, "training_acc": 50.0, "val_loss": 1163.0809326171875, "val_acc": 40.0}
{"epoch": 32, "training_loss": 1556.0571044921876, "training_acc": 50.0, "val_loss": 1673.4925537109375, "val_acc": 60.0}
{"epoch": 33, "training_loss": 2723.5728515625, "training_acc": 50.0, "val_loss": 63.35825729370117, "val_acc": 40.0}
{"epoch": 34, "training_loss": 858.166635131836, "training_acc": 40.0, "val_loss": 686.0650634765625, "val_acc": 60.0}
{"epoch": 35, "training_loss": 3018.653515625, "training_acc": 30.0, "val_loss": 1276.0867919921875, "val_acc": 40.0}
{"epoch": 36, "training_loss": 1824.624658203125, "training_acc": 60.0, "val_loss": 3618.41015625, "val_acc": 60.0}
{"epoch": 37, "training_loss": 2675.6209869384766, "training_acc": 60.0, "val_loss": 2412.492431640625, "val_acc": 40.0}
{"epoch": 38, "training_loss": 1698.0620944976806, "training_acc": 50.0, "val_loss": 2989.287841796875, "val_acc": 60.0}
{"epoch": 39, "training_loss": 4097.151318359375, "training_acc": 50.0, "val_loss": 1167.9417724609375, "val_acc": 60.0}
{"epoch": 40, "training_loss": 3619.45400390625, "training_acc": 40.0, "val_loss": 5404.43994140625, "val_acc": 40.0}
{"epoch": 41, "training_loss": 2754.9212158203127, "training_acc": 60.0, "val_loss": 4108.58935546875, "val_acc": 60.0}
{"epoch": 42, "training_loss": 4679.1068359375, "training_acc": 50.0, "val_loss": 476.7401428222656, "val_acc": 40.0}
{"epoch": 43, "training_loss": 924.86123046875, "training_acc": 30.0, "val_loss": 2449.554443359375, "val_acc": 40.0}
{"epoch": 44, "training_loss": 1823.8562133789062, "training_acc": 40.0, "val_loss": 466.8498229980469, "val_acc": 40.0}
{"epoch": 45, "training_loss": 867.5537963867188, "training_acc": 40.0, "val_loss": 2902.815185546875, "val_acc": 40.0}
{"epoch": 46, "training_loss": 2254.206494140625, "training_acc": 50.0, "val_loss": 1030.537841796875, "val_acc": 60.0}
{"epoch": 47, "training_loss": 2188.170703125, "training_acc": 50.0, "val_loss": 1242.2529296875, "val_acc": 40.0}
{"epoch": 48, "training_loss": 2593.2699584960938, "training_acc": 50.0, "val_loss": 639.9959716796875, "val_acc": 60.0}
{"epoch": 49, "training_loss": 1288.31240234375, "training_acc": 50.0, "val_loss": 832.0269775390625, "val_acc": 40.0}
{"epoch": 50, "training_loss": 792.7224243164062, "training_acc": 50.0, "val_loss": 3064.341796875, "val_acc": 60.0}
{"epoch": 51, "training_loss": 4674.59921875, "training_acc": 50.0, "val_loss": 2346.583984375, "val_acc": 60.0}
{"epoch": 52, "training_loss": 2742.00859375, "training_acc": 50.0, "val_loss": 3914.010986328125, "val_acc": 40.0}
