"main_modify.py --pretrained_path /sdcc/u/dyhan316/misc_VAE/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 20 --layer_control freeze --stratify strat --random_seed 1 --task ADNI_sex --input_option yAware --learning_rate 1e2 --batch_size 16 --weight_decay 1e-3 --BN inst --save_path finetune_results_InstanceBN"
{"epoch": 0, "training_loss": 2114.538334941864, "training_acc": 55.0, "val_loss": 7194.15185546875, "val_acc": 40.0}
{"epoch": 1, "training_loss": 4776.5595703125, "training_acc": 65.0, "val_loss": 14590.8798828125, "val_acc": 60.0}
{"epoch": 2, "training_loss": 19220.40078125, "training_acc": 45.0, "val_loss": 3811.823974609375, "val_acc": 60.0}
{"epoch": 3, "training_loss": 5707.363671875, "training_acc": 45.0, "val_loss": 15233.4208984375, "val_acc": 40.0}
{"epoch": 4, "training_loss": 11836.4046875, "training_acc": 55.0, "val_loss": 11989.7958984375, "val_acc": 40.0}
{"epoch": 5, "training_loss": 8326.209619140625, "training_acc": 55.0, "val_loss": 3324.926513671875, "val_acc": 60.0}
{"epoch": 6, "training_loss": 5252.13232421875, "training_acc": 45.0, "val_loss": 6091.83740234375, "val_acc": 60.0}
{"epoch": 7, "training_loss": 7429.587890625, "training_acc": 45.0, "val_loss": 1476.5009765625, "val_acc": 40.0}
{"epoch": 8, "training_loss": 1456.253759765625, "training_acc": 55.0, "val_loss": 8085.08740234375, "val_acc": 40.0}
{"epoch": 9, "training_loss": 6082.5345703125, "training_acc": 55.0, "val_loss": 3204.75634765625, "val_acc": 40.0}
{"epoch": 10, "training_loss": 1863.515185546875, "training_acc": 65.0, "val_loss": 5085.1171875, "val_acc": 60.0}
{"epoch": 11, "training_loss": 7165.03564453125, "training_acc": 45.0, "val_loss": 4536.6064453125, "val_acc": 60.0}
{"epoch": 12, "training_loss": 5205.281005859375, "training_acc": 45.0, "val_loss": 4630.4140625, "val_acc": 40.0}
{"epoch": 13, "training_loss": 4172.3236328125, "training_acc": 55.0, "val_loss": 10214.4375, "val_acc": 40.0}
{"epoch": 14, "training_loss": 7384.362890625, "training_acc": 55.0, "val_loss": 4649.44189453125, "val_acc": 40.0}
{"epoch": 15, "training_loss": 3557.73701171875, "training_acc": 45.0, "val_loss": 2851.22021484375, "val_acc": 60.0}
{"epoch": 16, "training_loss": 3865.549658203125, "training_acc": 45.0, "val_loss": 764.1724853515625, "val_acc": 60.0}
{"epoch": 17, "training_loss": 1057.2945068359375, "training_acc": 55.0, "val_loss": 6515.68310546875, "val_acc": 40.0}
{"epoch": 18, "training_loss": 4964.27548828125, "training_acc": 55.0, "val_loss": 5020.86572265625, "val_acc": 40.0}
{"epoch": 19, "training_loss": 2838.78271484375, "training_acc": 55.0, "val_loss": 3498.70556640625, "val_acc": 60.0}
{"epoch": 20, "training_loss": 5364.44736328125, "training_acc": 45.0, "val_loss": 5947.015625, "val_acc": 60.0}
{"epoch": 21, "training_loss": 7962.039111328125, "training_acc": 45.0, "val_loss": 1639.182861328125, "val_acc": 60.0}
{"epoch": 22, "training_loss": 2991.3654296875, "training_acc": 35.0, "val_loss": 5907.1025390625, "val_acc": 40.0}
{"epoch": 23, "training_loss": 4391.93330078125, "training_acc": 55.0, "val_loss": 3174.68359375, "val_acc": 40.0}
{"epoch": 24, "training_loss": 1654.8089477539063, "training_acc": 65.0, "val_loss": 2288.21533203125, "val_acc": 60.0}
{"epoch": 25, "training_loss": 3113.252197265625, "training_acc": 45.0, "val_loss": 749.7151489257812, "val_acc": 60.0}
{"epoch": 26, "training_loss": 1747.907373046875, "training_acc": 35.0, "val_loss": 3717.414794921875, "val_acc": 40.0}
{"epoch": 27, "training_loss": 2621.5743896484373, "training_acc": 55.0, "val_loss": 695.671875, "val_acc": 60.0}
{"epoch": 28, "training_loss": 1052.195654296875, "training_acc": 45.0, "val_loss": 1609.249267578125, "val_acc": 40.0}
{"epoch": 29, "training_loss": 1237.4369262695313, "training_acc": 55.0, "val_loss": 1295.416259765625, "val_acc": 40.0}
{"epoch": 30, "training_loss": 737.5449768066406, "training_acc": 65.0, "val_loss": 1425.1658935546875, "val_acc": 60.0}
{"epoch": 31, "training_loss": 1681.8052795410156, "training_acc": 45.0, "val_loss": 3283.040771484375, "val_acc": 40.0}
{"epoch": 32, "training_loss": 2542.3294189453127, "training_acc": 55.0, "val_loss": 3903.48046875, "val_acc": 40.0}
{"epoch": 33, "training_loss": 2693.271374511719, "training_acc": 55.0, "val_loss": 1684.16796875, "val_acc": 60.0}
{"epoch": 34, "training_loss": 2556.5265625, "training_acc": 45.0, "val_loss": 527.103515625, "val_acc": 60.0}
{"epoch": 35, "training_loss": 1774.05859375, "training_acc": 35.0, "val_loss": 5612.4072265625, "val_acc": 40.0}
{"epoch": 36, "training_loss": 4012.02099609375, "training_acc": 55.0, "val_loss": 1008.2559814453125, "val_acc": 40.0}
{"epoch": 37, "training_loss": 1760.11240234375, "training_acc": 45.0, "val_loss": 3785.192138671875, "val_acc": 60.0}
{"epoch": 38, "training_loss": 4959.56689453125, "training_acc": 45.0, "val_loss": 222.59971618652344, "val_acc": 60.0}
{"epoch": 39, "training_loss": 669.2738037109375, "training_acc": 55.0, "val_loss": 9107.9853515625, "val_acc": 40.0}
{"epoch": 40, "training_loss": 6884.0259765625, "training_acc": 55.0, "val_loss": 9055.7861328125, "val_acc": 40.0}
{"epoch": 41, "training_loss": 6069.84423828125, "training_acc": 55.0, "val_loss": 88.67192840576172, "val_acc": 40.0}
{"epoch": 42, "training_loss": 1967.1923706054688, "training_acc": 45.0, "val_loss": 7304.55615234375, "val_acc": 60.0}
{"epoch": 43, "training_loss": 10133.87890625, "training_acc": 45.0, "val_loss": 5151.7333984375, "val_acc": 60.0}
{"epoch": 44, "training_loss": 5673.32822265625, "training_acc": 45.0, "val_loss": 6445.52734375, "val_acc": 40.0}
{"epoch": 45, "training_loss": 5732.8009765625, "training_acc": 55.0, "val_loss": 14130.1474609375, "val_acc": 40.0}
{"epoch": 46, "training_loss": 10514.561328125, "training_acc": 55.0, "val_loss": 9826.2451171875, "val_acc": 40.0}
{"epoch": 47, "training_loss": 6830.512426757812, "training_acc": 55.0, "val_loss": 2206.379638671875, "val_acc": 60.0}
{"epoch": 48, "training_loss": 3642.319140625, "training_acc": 45.0, "val_loss": 5101.28515625, "val_acc": 60.0}
{"epoch": 49, "training_loss": 6835.84716796875, "training_acc": 45.0, "val_loss": 1217.043701171875, "val_acc": 60.0}
{"epoch": 50, "training_loss": 2553.90439453125, "training_acc": 35.0, "val_loss": 5977.93212890625, "val_acc": 40.0}
{"epoch": 51, "training_loss": 4397.3826171875, "training_acc": 55.0, "val_loss": 2636.365966796875, "val_acc": 40.0}
{"epoch": 52, "training_loss": 2803.59609375, "training_acc": 35.0, "val_loss": 1764.647705078125, "val_acc": 60.0}
{"epoch": 53, "training_loss": 1897.6158081054687, "training_acc": 45.0, "val_loss": 4713.10498046875, "val_acc": 40.0}
{"epoch": 54, "training_loss": 3953.09423828125, "training_acc": 55.0, "val_loss": 7200.97265625, "val_acc": 40.0}
{"epoch": 55, "training_loss": 5378.21796875, "training_acc": 55.0, "val_loss": 1606.259765625, "val_acc": 40.0}
{"epoch": 56, "training_loss": 1011.50185546875, "training_acc": 65.0, "val_loss": 3429.10791015625, "val_acc": 60.0}
{"epoch": 57, "training_loss": 4664.0794921875, "training_acc": 45.0, "val_loss": 1386.7772216796875, "val_acc": 60.0}
{"epoch": 58, "training_loss": 1611.5391845703125, "training_acc": 55.0, "val_loss": 5696.93505859375, "val_acc": 40.0}
{"epoch": 59, "training_loss": 4365.1513671875, "training_acc": 55.0, "val_loss": 4338.24951171875, "val_acc": 40.0}
{"epoch": 60, "training_loss": 2605.477331542969, "training_acc": 55.0, "val_loss": 3488.710693359375, "val_acc": 60.0}
