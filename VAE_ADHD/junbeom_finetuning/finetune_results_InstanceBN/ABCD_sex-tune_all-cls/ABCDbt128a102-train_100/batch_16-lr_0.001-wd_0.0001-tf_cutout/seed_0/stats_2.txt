"main_modify.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --learning_rate 1e-3 --batch_size 16 --weight_decay 1e-4 --BN inst --save_path finetune_results_InstanceBN --run_where lab"
{"epoch": 0, "training_loss": 0.7083706688880921, "training_acc": 49.0, "val_loss": 0.7186819958686829, "val_acc": 52.0}
{"epoch": 1, "training_loss": 0.712322986125946, "training_acc": 53.0, "val_loss": 0.7151080369949341, "val_acc": 52.0}
{"epoch": 2, "training_loss": 0.7122180533409118, "training_acc": 53.0, "val_loss": 0.6928345060348511, "val_acc": 52.0}
{"epoch": 3, "training_loss": 0.7080017924308777, "training_acc": 43.0, "val_loss": 0.6924537634849548, "val_acc": 52.0}
{"epoch": 4, "training_loss": 0.7058021640777588, "training_acc": 53.0, "val_loss": 0.692514910697937, "val_acc": 52.0}
{"epoch": 5, "training_loss": 0.7160637879371643, "training_acc": 43.0, "val_loss": 0.6945179677009583, "val_acc": 48.0}
{"epoch": 6, "training_loss": 0.6925335645675659, "training_acc": 53.0, "val_loss": 0.6985947346687317, "val_acc": 52.0}
{"epoch": 7, "training_loss": 0.6948055410385132, "training_acc": 53.0, "val_loss": 0.6932103681564331, "val_acc": 48.0}
{"epoch": 8, "training_loss": 0.6962944889068603, "training_acc": 47.0, "val_loss": 0.6961182141304016, "val_acc": 52.0}
{"epoch": 9, "training_loss": 0.6950673270225525, "training_acc": 53.0, "val_loss": 0.6947737383842468, "val_acc": 52.0}
{"epoch": 10, "training_loss": 0.6920897626876831, "training_acc": 53.0, "val_loss": 0.6935317945480347, "val_acc": 48.0}
{"epoch": 11, "training_loss": 0.6974830555915833, "training_acc": 47.0, "val_loss": 0.693059322834015, "val_acc": 52.0}
{"epoch": 12, "training_loss": 0.6998045587539673, "training_acc": 53.0, "val_loss": 0.6933144187927246, "val_acc": 52.0}
{"epoch": 13, "training_loss": 0.69566880941391, "training_acc": 49.0, "val_loss": 0.6929406452178956, "val_acc": 52.0}
{"epoch": 14, "training_loss": 0.6932685947418213, "training_acc": 53.0, "val_loss": 0.6935392308235169, "val_acc": 52.0}
{"epoch": 15, "training_loss": 0.7052313113212585, "training_acc": 53.0, "val_loss": 0.6979458951950073, "val_acc": 52.0}
{"epoch": 16, "training_loss": 0.721981041431427, "training_acc": 43.0, "val_loss": 0.6925403523445129, "val_acc": 52.0}
{"epoch": 17, "training_loss": 0.6824888229370117, "training_acc": 53.0, "val_loss": 0.7142985463142395, "val_acc": 52.0}
{"epoch": 18, "training_loss": 0.7025099468231201, "training_acc": 53.0, "val_loss": 0.6924146580696106, "val_acc": 52.0}
{"epoch": 19, "training_loss": 0.6931260371208191, "training_acc": 53.0, "val_loss": 0.6932075500488282, "val_acc": 48.0}
{"epoch": 20, "training_loss": 0.7021992421150207, "training_acc": 47.0, "val_loss": 0.6941664695739747, "val_acc": 48.0}
{"epoch": 21, "training_loss": 0.6950870847702026, "training_acc": 51.0, "val_loss": 0.7008124756813049, "val_acc": 52.0}
{"epoch": 22, "training_loss": 0.7009275078773498, "training_acc": 53.0, "val_loss": 0.6928008079528809, "val_acc": 52.0}
{"epoch": 23, "training_loss": 0.694218578338623, "training_acc": 53.0, "val_loss": 0.692435793876648, "val_acc": 52.0}
{"epoch": 24, "training_loss": 0.6935448789596558, "training_acc": 53.0, "val_loss": 0.692348096370697, "val_acc": 52.0}
{"epoch": 25, "training_loss": 0.7179312658309936, "training_acc": 49.0, "val_loss": 0.6973531103134155, "val_acc": 48.0}
{"epoch": 26, "training_loss": 0.7262744283676148, "training_acc": 49.0, "val_loss": 0.71750314950943, "val_acc": 52.0}
{"epoch": 27, "training_loss": 0.7025481033325195, "training_acc": 49.0, "val_loss": 0.695615246295929, "val_acc": 48.0}
{"epoch": 28, "training_loss": 0.6953608250617981, "training_acc": 47.0, "val_loss": 0.6932900929450989, "val_acc": 52.0}
{"epoch": 29, "training_loss": 0.6932222056388855, "training_acc": 53.0, "val_loss": 0.6938031458854675, "val_acc": 52.0}
{"epoch": 30, "training_loss": 0.692869176864624, "training_acc": 53.0, "val_loss": 0.6952280282974244, "val_acc": 52.0}
{"epoch": 31, "training_loss": 0.6952676200866699, "training_acc": 53.0, "val_loss": 0.6924072217941284, "val_acc": 52.0}
{"epoch": 32, "training_loss": 0.7047587728500366, "training_acc": 47.0, "val_loss": 0.7007196164131164, "val_acc": 48.0}
{"epoch": 33, "training_loss": 0.6967120742797852, "training_acc": 51.0, "val_loss": 0.6961954092979431, "val_acc": 52.0}
{"epoch": 34, "training_loss": 0.7088135385513306, "training_acc": 53.0, "val_loss": 0.7072051858901978, "val_acc": 52.0}
{"epoch": 35, "training_loss": 0.6914401984214783, "training_acc": 53.0, "val_loss": 0.6984083247184754, "val_acc": 48.0}
{"epoch": 36, "training_loss": 0.6997620964050293, "training_acc": 47.0, "val_loss": 0.6923484182357789, "val_acc": 52.0}
{"epoch": 37, "training_loss": 0.6939691781997681, "training_acc": 49.0, "val_loss": 0.6937301802635193, "val_acc": 52.0}
{"epoch": 38, "training_loss": 0.695629448890686, "training_acc": 53.0, "val_loss": 0.6946189475059509, "val_acc": 52.0}
{"epoch": 39, "training_loss": 0.6933045721054077, "training_acc": 53.0, "val_loss": 0.6924073886871338, "val_acc": 52.0}
{"epoch": 40, "training_loss": 0.6951661586761475, "training_acc": 51.0, "val_loss": 0.6941286897659302, "val_acc": 48.0}
{"epoch": 41, "training_loss": 0.6965166473388672, "training_acc": 51.0, "val_loss": 0.6986400365829468, "val_acc": 52.0}
{"epoch": 42, "training_loss": 0.7001812410354614, "training_acc": 53.0, "val_loss": 0.6923511052131652, "val_acc": 52.0}
{"epoch": 43, "training_loss": 0.6945284509658813, "training_acc": 49.0, "val_loss": 0.6942005848884583, "val_acc": 48.0}
