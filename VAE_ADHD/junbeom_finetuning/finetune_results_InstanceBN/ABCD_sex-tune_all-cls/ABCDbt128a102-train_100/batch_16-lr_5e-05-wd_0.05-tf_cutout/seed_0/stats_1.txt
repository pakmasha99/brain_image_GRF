"main_modify.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --learning_rate 5e-5 --batch_size 16 --weight_decay 5e-2 --BN inst --save_path finetune_results_InstanceBN --run_where lab"
{"epoch": 0, "training_loss": 0.7001215839385986, "training_acc": 47.0, "val_loss": 0.6972924447059632, "val_acc": 48.0}
{"epoch": 1, "training_loss": 0.6987770581245423, "training_acc": 48.0, "val_loss": 0.693407096862793, "val_acc": 48.0}
{"epoch": 2, "training_loss": 0.6937097549438477, "training_acc": 44.0, "val_loss": 0.6908363389968872, "val_acc": 72.0}
{"epoch": 3, "training_loss": 0.6922411823272705, "training_acc": 53.0, "val_loss": 0.6909896779060364, "val_acc": 52.0}
{"epoch": 4, "training_loss": 0.6906826400756836, "training_acc": 53.0, "val_loss": 0.6927605962753296, "val_acc": 52.0}
{"epoch": 5, "training_loss": 0.6918136477470398, "training_acc": 51.0, "val_loss": 0.6934817147254944, "val_acc": 52.0}
{"epoch": 6, "training_loss": 0.6923547601699829, "training_acc": 53.0, "val_loss": 0.6923603200912476, "val_acc": 52.0}
{"epoch": 7, "training_loss": 0.6924794220924377, "training_acc": 53.0, "val_loss": 0.6923690366744996, "val_acc": 52.0}
{"epoch": 8, "training_loss": 0.6915868234634399, "training_acc": 53.0, "val_loss": 0.6909690403938293, "val_acc": 52.0}
{"epoch": 9, "training_loss": 0.6935872960090638, "training_acc": 53.0, "val_loss": 0.6897464299201965, "val_acc": 52.0}
{"epoch": 10, "training_loss": 0.6924680590629577, "training_acc": 53.0, "val_loss": 0.6927141976356507, "val_acc": 52.0}
{"epoch": 11, "training_loss": 0.6910026621818542, "training_acc": 53.0, "val_loss": 0.6901311445236206, "val_acc": 52.0}
{"epoch": 12, "training_loss": 0.6924178433418274, "training_acc": 53.0, "val_loss": 0.6932436704635621, "val_acc": 52.0}
{"epoch": 13, "training_loss": 0.6921755743026733, "training_acc": 53.0, "val_loss": 0.692611882686615, "val_acc": 52.0}
{"epoch": 14, "training_loss": 0.6915353679656983, "training_acc": 53.0, "val_loss": 0.6912306952476501, "val_acc": 52.0}
{"epoch": 15, "training_loss": 0.6906595706939698, "training_acc": 53.0, "val_loss": 0.6905399179458618, "val_acc": 52.0}
{"epoch": 16, "training_loss": 0.6911855363845825, "training_acc": 53.0, "val_loss": 0.6913172936439514, "val_acc": 52.0}
{"epoch": 17, "training_loss": 0.6896762657165527, "training_acc": 53.0, "val_loss": 0.6853459978103638, "val_acc": 52.0}
{"epoch": 18, "training_loss": 0.6892300248146057, "training_acc": 53.0, "val_loss": 0.6901422095298767, "val_acc": 52.0}
{"epoch": 19, "training_loss": 0.6894024991989136, "training_acc": 53.0, "val_loss": 0.6911650252342224, "val_acc": 52.0}
{"epoch": 20, "training_loss": 0.6911622858047486, "training_acc": 53.0, "val_loss": 0.6897266244888306, "val_acc": 52.0}
{"epoch": 21, "training_loss": 0.6902704906463623, "training_acc": 53.0, "val_loss": 0.6851705551147461, "val_acc": 52.0}
{"epoch": 22, "training_loss": 0.6894818210601806, "training_acc": 53.0, "val_loss": 0.6862990117073059, "val_acc": 52.0}
{"epoch": 23, "training_loss": 0.6891250109672546, "training_acc": 53.0, "val_loss": 0.6844113111495972, "val_acc": 52.0}
{"epoch": 24, "training_loss": 0.6889193344116211, "training_acc": 53.0, "val_loss": 0.6922556757926941, "val_acc": 52.0}
{"epoch": 25, "training_loss": 0.6895279598236084, "training_acc": 53.0, "val_loss": 0.6894371771812439, "val_acc": 52.0}
{"epoch": 26, "training_loss": 0.6914870619773865, "training_acc": 53.0, "val_loss": 0.6933258271217346, "val_acc": 52.0}
{"epoch": 27, "training_loss": 0.6925132656097412, "training_acc": 53.0, "val_loss": 0.6917830824851989, "val_acc": 52.0}
{"epoch": 28, "training_loss": 0.6926095247268677, "training_acc": 53.0, "val_loss": 0.6899773049354553, "val_acc": 52.0}
{"epoch": 29, "training_loss": 0.6906865668296814, "training_acc": 53.0, "val_loss": 0.6861972546577454, "val_acc": 52.0}
{"epoch": 30, "training_loss": 0.6919189119338989, "training_acc": 53.0, "val_loss": 0.6944679927825927, "val_acc": 52.0}
{"epoch": 31, "training_loss": 0.6920236325263978, "training_acc": 53.0, "val_loss": 0.693383183479309, "val_acc": 52.0}
{"epoch": 32, "training_loss": 0.6918316078186035, "training_acc": 53.0, "val_loss": 0.6902266597747803, "val_acc": 52.0}
{"epoch": 33, "training_loss": 0.6907651519775391, "training_acc": 53.0, "val_loss": 0.689195454120636, "val_acc": 52.0}
{"epoch": 34, "training_loss": 0.691113440990448, "training_acc": 53.0, "val_loss": 0.688026351928711, "val_acc": 52.0}
{"epoch": 35, "training_loss": 0.6915374422073364, "training_acc": 53.0, "val_loss": 0.6921702289581299, "val_acc": 52.0}
{"epoch": 36, "training_loss": 0.6925211143493653, "training_acc": 53.0, "val_loss": 0.6902427768707275, "val_acc": 52.0}
{"epoch": 37, "training_loss": 0.6946936941146851, "training_acc": 53.0, "val_loss": 0.6885825562477111, "val_acc": 52.0}
{"epoch": 38, "training_loss": 0.6906076908111572, "training_acc": 53.0, "val_loss": 0.6889315938949585, "val_acc": 52.0}
{"epoch": 39, "training_loss": 0.6909498977661133, "training_acc": 53.0, "val_loss": 0.6884155297279357, "val_acc": 52.0}
{"epoch": 40, "training_loss": 0.6900221133232116, "training_acc": 53.0, "val_loss": 0.685650749206543, "val_acc": 52.0}
{"epoch": 41, "training_loss": 0.6916487264633179, "training_acc": 53.0, "val_loss": 0.6937740802764892, "val_acc": 52.0}
{"epoch": 42, "training_loss": 0.6912595176696777, "training_acc": 53.0, "val_loss": 0.6929517483711243, "val_acc": 52.0}
