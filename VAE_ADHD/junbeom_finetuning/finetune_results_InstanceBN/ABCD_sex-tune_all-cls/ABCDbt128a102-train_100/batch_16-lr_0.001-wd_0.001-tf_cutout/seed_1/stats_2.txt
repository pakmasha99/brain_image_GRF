"main_modify.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 1 --task ABCD_sex --input_option BT_org --learning_rate 1e-3 --batch_size 16 --weight_decay 1e-3 --BN inst --save_path finetune_results_InstanceBN --run_where lab"
{"epoch": 0, "training_loss": 0.7204975056648254, "training_acc": 45.0, "val_loss": 0.6928400492668152, "val_acc": 52.0}
{"epoch": 1, "training_loss": 0.693437066078186, "training_acc": 50.0, "val_loss": 0.6936066389083863, "val_acc": 48.0}
{"epoch": 2, "training_loss": 0.7007619094848633, "training_acc": 47.0, "val_loss": 0.6935179471969605, "val_acc": 52.0}
{"epoch": 3, "training_loss": 0.695146017074585, "training_acc": 53.0, "val_loss": 0.6928183102607727, "val_acc": 52.0}
{"epoch": 4, "training_loss": 0.7000814247131347, "training_acc": 49.0, "val_loss": 0.6934446096420288, "val_acc": 48.0}
{"epoch": 5, "training_loss": 0.6910417222976685, "training_acc": 53.0, "val_loss": 0.7006077527999878, "val_acc": 52.0}
{"epoch": 6, "training_loss": 0.7043846035003662, "training_acc": 53.0, "val_loss": 0.6979243516921997, "val_acc": 52.0}
{"epoch": 7, "training_loss": 0.7083534383773804, "training_acc": 43.0, "val_loss": 0.6933132243156434, "val_acc": 48.0}
{"epoch": 8, "training_loss": 0.7040194201469422, "training_acc": 43.0, "val_loss": 0.6926253437995911, "val_acc": 52.0}
{"epoch": 9, "training_loss": 0.6930072927474975, "training_acc": 53.0, "val_loss": 0.6924838089942932, "val_acc": 52.0}
{"epoch": 10, "training_loss": 0.6964979982376098, "training_acc": 45.0, "val_loss": 0.6923474383354187, "val_acc": 52.0}
{"epoch": 11, "training_loss": 0.6972498321533203, "training_acc": 53.0, "val_loss": 0.6936002397537231, "val_acc": 52.0}
{"epoch": 12, "training_loss": 0.6957085824012756, "training_acc": 53.0, "val_loss": 0.6949813175201416, "val_acc": 52.0}
{"epoch": 13, "training_loss": 0.7101344108581543, "training_acc": 53.0, "val_loss": 0.6928151321411132, "val_acc": 52.0}
{"epoch": 14, "training_loss": 0.718947639465332, "training_acc": 41.0, "val_loss": 0.6923891472816467, "val_acc": 52.0}
{"epoch": 15, "training_loss": 0.6958220529556275, "training_acc": 53.0, "val_loss": 0.7123212242126464, "val_acc": 52.0}
{"epoch": 16, "training_loss": 0.7171334552764893, "training_acc": 53.0, "val_loss": 0.7012450003623962, "val_acc": 52.0}
{"epoch": 17, "training_loss": 0.6983080673217773, "training_acc": 53.0, "val_loss": 0.6944620442390442, "val_acc": 52.0}
{"epoch": 18, "training_loss": 0.6923989057540894, "training_acc": 49.0, "val_loss": 0.6989669799804688, "val_acc": 48.0}
{"epoch": 19, "training_loss": 0.705713267326355, "training_acc": 45.0, "val_loss": 0.6924563670158386, "val_acc": 52.0}
{"epoch": 20, "training_loss": 0.7097276043891907, "training_acc": 53.0, "val_loss": 0.6946442198753356, "val_acc": 52.0}
{"epoch": 21, "training_loss": 0.6969922399520874, "training_acc": 49.0, "val_loss": 0.6927505493164062, "val_acc": 52.0}
{"epoch": 22, "training_loss": 0.7033258152008056, "training_acc": 53.0, "val_loss": 0.7065542554855346, "val_acc": 52.0}
{"epoch": 23, "training_loss": 0.7042170023918152, "training_acc": 53.0, "val_loss": 0.6925413537025452, "val_acc": 52.0}
{"epoch": 24, "training_loss": 0.7053054690361023, "training_acc": 47.0, "val_loss": 0.6964668226242066, "val_acc": 48.0}
{"epoch": 25, "training_loss": 0.6918365240097046, "training_acc": 55.0, "val_loss": 0.7053572154045105, "val_acc": 52.0}
{"epoch": 26, "training_loss": 0.7081508135795593, "training_acc": 53.0, "val_loss": 0.6923472332954407, "val_acc": 52.0}
{"epoch": 27, "training_loss": 0.7121010541915893, "training_acc": 43.0, "val_loss": 0.6925470471382141, "val_acc": 52.0}
{"epoch": 28, "training_loss": 0.6871014928817749, "training_acc": 53.0, "val_loss": 0.7025501346588134, "val_acc": 52.0}
{"epoch": 29, "training_loss": 0.7087103843688964, "training_acc": 53.0, "val_loss": 0.6923498630523681, "val_acc": 52.0}
{"epoch": 30, "training_loss": 0.7327058291435242, "training_acc": 43.0, "val_loss": 0.7008700489997863, "val_acc": 48.0}
{"epoch": 31, "training_loss": 0.7081833600997924, "training_acc": 49.0, "val_loss": 0.7018470048904419, "val_acc": 52.0}
{"epoch": 32, "training_loss": 0.7018057751655579, "training_acc": 53.0, "val_loss": 0.6929275631904602, "val_acc": 52.0}
{"epoch": 33, "training_loss": 0.6964799833297729, "training_acc": 53.0, "val_loss": 0.6931325340270996, "val_acc": 52.0}
{"epoch": 34, "training_loss": 0.7060892224311829, "training_acc": 41.0, "val_loss": 0.6925009846687317, "val_acc": 52.0}
{"epoch": 35, "training_loss": 0.7014852952957153, "training_acc": 53.0, "val_loss": 0.694752037525177, "val_acc": 52.0}
{"epoch": 36, "training_loss": 0.714255199432373, "training_acc": 41.0, "val_loss": 0.6946145033836365, "val_acc": 48.0}
{"epoch": 37, "training_loss": 0.7022105979919434, "training_acc": 49.0, "val_loss": 0.6960320258140564, "val_acc": 52.0}
{"epoch": 38, "training_loss": 0.6973626589775086, "training_acc": 43.0, "val_loss": 0.6970374727249146, "val_acc": 48.0}
{"epoch": 39, "training_loss": 0.7014792728424072, "training_acc": 47.0, "val_loss": 0.6978691959381104, "val_acc": 48.0}
{"epoch": 40, "training_loss": 0.6920773720741272, "training_acc": 53.0, "val_loss": 0.6960849714279175, "val_acc": 52.0}
{"epoch": 41, "training_loss": 0.6986949157714843, "training_acc": 53.0, "val_loss": 0.6943085670471192, "val_acc": 52.0}
{"epoch": 42, "training_loss": 0.7089320898056031, "training_acc": 53.0, "val_loss": 0.699695131778717, "val_acc": 52.0}
{"epoch": 43, "training_loss": 0.6997549915313721, "training_acc": 45.0, "val_loss": 0.6962086582183837, "val_acc": 48.0}
{"epoch": 44, "training_loss": 0.7029895472526551, "training_acc": 41.0, "val_loss": 0.69652348279953, "val_acc": 48.0}
{"epoch": 45, "training_loss": 0.7220626616477966, "training_acc": 47.0, "val_loss": 0.6946441674232483, "val_acc": 48.0}
