"main_modify.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --learning_rate 1e-5 --batch_size 16 --weight_decay 1e-2 --BN inst --save_path finetune_results_InstanceBN --run_where lab"
{"epoch": 0, "training_loss": 0.6926419568061829, "training_acc": 53.0, "val_loss": 0.6924451446533203, "val_acc": 52.0}
{"epoch": 1, "training_loss": 0.6921203374862671, "training_acc": 53.0, "val_loss": 0.692693886756897, "val_acc": 52.0}
{"epoch": 2, "training_loss": 0.6906190776824951, "training_acc": 53.0, "val_loss": 0.6928754258155823, "val_acc": 52.0}
{"epoch": 3, "training_loss": 0.6895618152618408, "training_acc": 53.0, "val_loss": 0.6931300759315491, "val_acc": 52.0}
{"epoch": 4, "training_loss": 0.689771203994751, "training_acc": 53.0, "val_loss": 0.6927194905281067, "val_acc": 52.0}
{"epoch": 5, "training_loss": 0.6886962795257568, "training_acc": 53.0, "val_loss": 0.6923804426193237, "val_acc": 52.0}
{"epoch": 6, "training_loss": 0.6881568646430969, "training_acc": 53.0, "val_loss": 0.6927007865905762, "val_acc": 52.0}
{"epoch": 7, "training_loss": 0.6864816999435425, "training_acc": 53.0, "val_loss": 0.6921867489814758, "val_acc": 52.0}
{"epoch": 8, "training_loss": 0.686803572177887, "training_acc": 53.0, "val_loss": 0.6926395821571351, "val_acc": 52.0}
{"epoch": 9, "training_loss": 0.6889135980606079, "training_acc": 53.0, "val_loss": 0.6925248980522156, "val_acc": 52.0}
{"epoch": 10, "training_loss": 0.6865648555755616, "training_acc": 53.0, "val_loss": 0.6926721358299255, "val_acc": 52.0}
{"epoch": 11, "training_loss": 0.6884637117385864, "training_acc": 53.0, "val_loss": 0.692217288017273, "val_acc": 52.0}
{"epoch": 12, "training_loss": 0.6903847551345825, "training_acc": 53.0, "val_loss": 0.6920565414428711, "val_acc": 52.0}
{"epoch": 13, "training_loss": 0.6886989545822143, "training_acc": 53.0, "val_loss": 0.6914935517311096, "val_acc": 52.0}
{"epoch": 14, "training_loss": 0.6886659860610962, "training_acc": 53.0, "val_loss": 0.6909800195693969, "val_acc": 52.0}
{"epoch": 15, "training_loss": 0.6876017165184021, "training_acc": 53.0, "val_loss": 0.6919772028923035, "val_acc": 52.0}
{"epoch": 16, "training_loss": 0.6862408781051635, "training_acc": 53.0, "val_loss": 0.6909725570678711, "val_acc": 52.0}
{"epoch": 17, "training_loss": 0.6840179133415222, "training_acc": 53.0, "val_loss": 0.6914934253692627, "val_acc": 52.0}
{"epoch": 18, "training_loss": 0.6833705091476441, "training_acc": 53.0, "val_loss": 0.6916879343986512, "val_acc": 52.0}
{"epoch": 19, "training_loss": 0.6821303915977478, "training_acc": 53.0, "val_loss": 0.6910590076446533, "val_acc": 52.0}
{"epoch": 20, "training_loss": 0.6802405881881713, "training_acc": 54.0, "val_loss": 0.6913722538948059, "val_acc": 52.0}
{"epoch": 21, "training_loss": 0.6810684061050415, "training_acc": 54.0, "val_loss": 0.6918841671943664, "val_acc": 52.0}
{"epoch": 22, "training_loss": 0.6815824127197265, "training_acc": 53.0, "val_loss": 0.6915707731246948, "val_acc": 52.0}
{"epoch": 23, "training_loss": 0.6803712034225464, "training_acc": 55.0, "val_loss": 0.690992751121521, "val_acc": 56.0}
{"epoch": 24, "training_loss": 0.6797588968276977, "training_acc": 55.0, "val_loss": 0.6931167435646057, "val_acc": 48.0}
{"epoch": 25, "training_loss": 0.6786506772041321, "training_acc": 55.0, "val_loss": 0.6910311532020569, "val_acc": 56.0}
{"epoch": 26, "training_loss": 0.6769280910491944, "training_acc": 62.0, "val_loss": 0.6917651581764221, "val_acc": 52.0}
{"epoch": 27, "training_loss": 0.6746382331848144, "training_acc": 59.0, "val_loss": 0.6910836005210876, "val_acc": 56.0}
{"epoch": 28, "training_loss": 0.6741011047363281, "training_acc": 60.0, "val_loss": 0.6918962788581848, "val_acc": 52.0}
{"epoch": 29, "training_loss": 0.6747023487091064, "training_acc": 60.0, "val_loss": 0.6936073327064514, "val_acc": 44.0}
{"epoch": 30, "training_loss": 0.6766145348548889, "training_acc": 60.0, "val_loss": 0.6924408960342407, "val_acc": 52.0}
{"epoch": 31, "training_loss": 0.6728627324104309, "training_acc": 60.0, "val_loss": 0.6902727270126343, "val_acc": 60.0}
{"epoch": 32, "training_loss": 0.672890419960022, "training_acc": 60.0, "val_loss": 0.6915904545783996, "val_acc": 52.0}
{"epoch": 33, "training_loss": 0.6721221303939819, "training_acc": 60.0, "val_loss": 0.6946739888191223, "val_acc": 48.0}
{"epoch": 34, "training_loss": 0.6761342000961303, "training_acc": 56.0, "val_loss": 0.6907267928123474, "val_acc": 60.0}
{"epoch": 35, "training_loss": 0.672609453201294, "training_acc": 59.0, "val_loss": 0.6915034532546998, "val_acc": 56.0}
{"epoch": 36, "training_loss": 0.6666559314727784, "training_acc": 62.0, "val_loss": 0.690679087638855, "val_acc": 52.0}
{"epoch": 37, "training_loss": 0.6673328948020935, "training_acc": 64.0, "val_loss": 0.6906360864639283, "val_acc": 56.0}
{"epoch": 38, "training_loss": 0.668591878414154, "training_acc": 63.0, "val_loss": 0.6912473726272583, "val_acc": 60.0}
{"epoch": 39, "training_loss": 0.6686813879013062, "training_acc": 61.0, "val_loss": 0.6927655792236328, "val_acc": 48.0}
{"epoch": 40, "training_loss": 0.6653187847137452, "training_acc": 62.0, "val_loss": 0.6925726985931396, "val_acc": 48.0}
{"epoch": 41, "training_loss": 0.6627926802635193, "training_acc": 65.0, "val_loss": 0.6925927329063416, "val_acc": 48.0}
{"epoch": 42, "training_loss": 0.662105565071106, "training_acc": 65.0, "val_loss": 0.6917610955238342, "val_acc": 60.0}
{"epoch": 43, "training_loss": 0.6633488154411316, "training_acc": 66.0, "val_loss": 0.6922332048416138, "val_acc": 60.0}
{"epoch": 44, "training_loss": 0.6679479432106018, "training_acc": 61.0, "val_loss": 0.6936168026924133, "val_acc": 48.0}
{"epoch": 45, "training_loss": 0.6634315061569214, "training_acc": 64.0, "val_loss": 0.6927399945259094, "val_acc": 60.0}
{"epoch": 46, "training_loss": 0.6669277763366699, "training_acc": 63.0, "val_loss": 0.6925152897834778, "val_acc": 52.0}
{"epoch": 47, "training_loss": 0.6623453092575073, "training_acc": 69.0, "val_loss": 0.6922457599639893, "val_acc": 48.0}
{"epoch": 48, "training_loss": 0.6591067790985108, "training_acc": 69.0, "val_loss": 0.6923522830009461, "val_acc": 48.0}
{"epoch": 49, "training_loss": 0.6573825979232788, "training_acc": 68.0, "val_loss": 0.6924433231353759, "val_acc": 56.0}
{"epoch": 50, "training_loss": 0.660456166267395, "training_acc": 68.0, "val_loss": 0.6938343524932862, "val_acc": 48.0}
