"main_modify.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 2 --task ABCD_sex --input_option BT_org --learning_rate 1e-5 --batch_size 16 --weight_decay 1e-2 --BN inst --save_path finetune_results_InstanceBN --run_where lab"
{"epoch": 0, "training_loss": 0.6916853785514832, "training_acc": 53.0, "val_loss": 0.6925124096870422, "val_acc": 52.0}
{"epoch": 1, "training_loss": 0.6918194341659546, "training_acc": 53.0, "val_loss": 0.6916524529457092, "val_acc": 52.0}
{"epoch": 2, "training_loss": 0.6907391619682312, "training_acc": 53.0, "val_loss": 0.6924360370635987, "val_acc": 52.0}
{"epoch": 3, "training_loss": 0.6902941679954528, "training_acc": 53.0, "val_loss": 0.6910600972175598, "val_acc": 52.0}
{"epoch": 4, "training_loss": 0.689631233215332, "training_acc": 53.0, "val_loss": 0.691628692150116, "val_acc": 52.0}
{"epoch": 5, "training_loss": 0.6888442611694336, "training_acc": 53.0, "val_loss": 0.6919661617279053, "val_acc": 52.0}
{"epoch": 6, "training_loss": 0.688799729347229, "training_acc": 53.0, "val_loss": 0.6907020258903503, "val_acc": 52.0}
{"epoch": 7, "training_loss": 0.687068829536438, "training_acc": 53.0, "val_loss": 0.6924832105636597, "val_acc": 52.0}
{"epoch": 8, "training_loss": 0.6882450032234192, "training_acc": 53.0, "val_loss": 0.6923689007759094, "val_acc": 52.0}
{"epoch": 9, "training_loss": 0.6869422340393067, "training_acc": 53.0, "val_loss": 0.6901336359977722, "val_acc": 52.0}
{"epoch": 10, "training_loss": 0.6878211021423339, "training_acc": 53.0, "val_loss": 0.6906299138069153, "val_acc": 52.0}
{"epoch": 11, "training_loss": 0.6856341648101807, "training_acc": 53.0, "val_loss": 0.6911753654479981, "val_acc": 52.0}
{"epoch": 12, "training_loss": 0.6862167644500733, "training_acc": 53.0, "val_loss": 0.6923042678833008, "val_acc": 52.0}
{"epoch": 13, "training_loss": 0.6842565965652466, "training_acc": 53.0, "val_loss": 0.6920472836494446, "val_acc": 52.0}
{"epoch": 14, "training_loss": 0.6830804872512818, "training_acc": 53.0, "val_loss": 0.6898563122749328, "val_acc": 52.0}
{"epoch": 15, "training_loss": 0.6813742852210999, "training_acc": 53.0, "val_loss": 0.6911910223960877, "val_acc": 52.0}
{"epoch": 16, "training_loss": 0.6807954740524292, "training_acc": 53.0, "val_loss": 0.6897520351409913, "val_acc": 52.0}
{"epoch": 17, "training_loss": 0.6803154492378235, "training_acc": 53.0, "val_loss": 0.6919270062446594, "val_acc": 52.0}
{"epoch": 18, "training_loss": 0.6782021331787109, "training_acc": 53.0, "val_loss": 0.6903512859344483, "val_acc": 52.0}
{"epoch": 19, "training_loss": 0.6782391500473023, "training_acc": 53.0, "val_loss": 0.6955055284500122, "val_acc": 52.0}
{"epoch": 20, "training_loss": 0.6766647052764893, "training_acc": 53.0, "val_loss": 0.687974693775177, "val_acc": 52.0}
{"epoch": 21, "training_loss": 0.6778909730911254, "training_acc": 53.0, "val_loss": 0.6944418454170227, "val_acc": 52.0}
{"epoch": 22, "training_loss": 0.6758424878120423, "training_acc": 53.0, "val_loss": 0.6892739868164063, "val_acc": 52.0}
{"epoch": 23, "training_loss": 0.6741577363014222, "training_acc": 53.0, "val_loss": 0.6922945547103881, "val_acc": 52.0}
{"epoch": 24, "training_loss": 0.6766805601119995, "training_acc": 53.0, "val_loss": 0.6880041241645813, "val_acc": 52.0}
{"epoch": 25, "training_loss": 0.6705225610733032, "training_acc": 53.0, "val_loss": 0.6918516325950622, "val_acc": 52.0}
{"epoch": 26, "training_loss": 0.6725565838813782, "training_acc": 53.0, "val_loss": 0.6891397190093994, "val_acc": 52.0}
{"epoch": 27, "training_loss": 0.6698221683502197, "training_acc": 54.0, "val_loss": 0.6897021269798279, "val_acc": 52.0}
{"epoch": 28, "training_loss": 0.6706025385856629, "training_acc": 54.0, "val_loss": 0.6894549465179444, "val_acc": 52.0}
{"epoch": 29, "training_loss": 0.6703165674209595, "training_acc": 56.0, "val_loss": 0.6887474417686462, "val_acc": 52.0}
{"epoch": 30, "training_loss": 0.666183078289032, "training_acc": 55.0, "val_loss": 0.6868888592720032, "val_acc": 52.0}
{"epoch": 31, "training_loss": 0.6655912590026856, "training_acc": 59.0, "val_loss": 0.689900438785553, "val_acc": 52.0}
{"epoch": 32, "training_loss": 0.6655209922790527, "training_acc": 59.0, "val_loss": 0.686058964729309, "val_acc": 52.0}
{"epoch": 33, "training_loss": 0.6661618900299072, "training_acc": 59.0, "val_loss": 0.6937484860420227, "val_acc": 52.0}
{"epoch": 34, "training_loss": 0.669851861000061, "training_acc": 54.0, "val_loss": 0.692638635635376, "val_acc": 52.0}
{"epoch": 35, "training_loss": 0.6617068195343018, "training_acc": 61.0, "val_loss": 0.6867592358589172, "val_acc": 52.0}
{"epoch": 36, "training_loss": 0.664000391960144, "training_acc": 60.0, "val_loss": 0.6937695240974426, "val_acc": 52.0}
{"epoch": 37, "training_loss": 0.6621842288970947, "training_acc": 59.0, "val_loss": 0.6917792797088623, "val_acc": 52.0}
{"epoch": 38, "training_loss": 0.6603155279159546, "training_acc": 64.0, "val_loss": 0.6837287831306458, "val_acc": 60.0}
{"epoch": 39, "training_loss": 0.6608266353607177, "training_acc": 60.0, "val_loss": 0.6896755695343018, "val_acc": 52.0}
{"epoch": 40, "training_loss": 0.6594811868667603, "training_acc": 68.0, "val_loss": 0.6859686970710754, "val_acc": 56.0}
{"epoch": 41, "training_loss": 0.6610256862640381, "training_acc": 63.0, "val_loss": 0.6850853967666626, "val_acc": 52.0}
{"epoch": 42, "training_loss": 0.6571996879577636, "training_acc": 69.0, "val_loss": 0.6871294379234314, "val_acc": 52.0}
{"epoch": 43, "training_loss": 0.656281430721283, "training_acc": 68.0, "val_loss": 0.684558072090149, "val_acc": 60.0}
{"epoch": 44, "training_loss": 0.6564128017425537, "training_acc": 69.0, "val_loss": 0.6877529525756836, "val_acc": 52.0}
{"epoch": 45, "training_loss": 0.6523283100128174, "training_acc": 73.0, "val_loss": 0.6827633714675904, "val_acc": 60.0}
{"epoch": 46, "training_loss": 0.6528224897384644, "training_acc": 76.0, "val_loss": 0.6799547028541565, "val_acc": 56.0}
{"epoch": 47, "training_loss": 0.6515582036972046, "training_acc": 73.0, "val_loss": 0.6903928685188293, "val_acc": 52.0}
{"epoch": 48, "training_loss": 0.6491344690322876, "training_acc": 76.0, "val_loss": 0.6842179989814758, "val_acc": 60.0}
{"epoch": 49, "training_loss": 0.6482744693756104, "training_acc": 74.0, "val_loss": 0.6824588680267334, "val_acc": 48.0}
{"epoch": 50, "training_loss": 0.6522064018249512, "training_acc": 73.0, "val_loss": 0.6913563418388367, "val_acc": 56.0}
{"epoch": 51, "training_loss": 0.6670799040794373, "training_acc": 62.0, "val_loss": 0.679770884513855, "val_acc": 48.0}
{"epoch": 52, "training_loss": 0.6580715608596802, "training_acc": 69.0, "val_loss": 0.6846033787727356, "val_acc": 60.0}
{"epoch": 53, "training_loss": 0.6580063390731812, "training_acc": 71.0, "val_loss": 0.6904940438270569, "val_acc": 48.0}
{"epoch": 54, "training_loss": 0.6602181673049927, "training_acc": 72.0, "val_loss": 0.6789334535598754, "val_acc": 60.0}
{"epoch": 55, "training_loss": 0.6469541120529175, "training_acc": 74.0, "val_loss": 0.6883283162117004, "val_acc": 60.0}
{"epoch": 56, "training_loss": 0.6438641762733459, "training_acc": 75.0, "val_loss": 0.6788480114936829, "val_acc": 60.0}
{"epoch": 57, "training_loss": 0.6491291427612305, "training_acc": 70.0, "val_loss": 0.6858250856399536, "val_acc": 52.0}
{"epoch": 58, "training_loss": 0.6481543874740601, "training_acc": 76.0, "val_loss": 0.6819657182693482, "val_acc": 56.0}
{"epoch": 59, "training_loss": 0.6601863718032837, "training_acc": 64.0, "val_loss": 0.691541678905487, "val_acc": 48.0}
{"epoch": 60, "training_loss": 0.6456773996353149, "training_acc": 72.0, "val_loss": 0.6775154423713684, "val_acc": 60.0}
{"epoch": 61, "training_loss": 0.6440749049186707, "training_acc": 73.0, "val_loss": 0.6905741047859192, "val_acc": 52.0}
{"epoch": 62, "training_loss": 0.6420827960968017, "training_acc": 76.0, "val_loss": 0.6696218132972718, "val_acc": 56.0}
{"epoch": 63, "training_loss": 0.6418203020095825, "training_acc": 75.0, "val_loss": 0.6839238858222961, "val_acc": 56.0}
{"epoch": 64, "training_loss": 0.6377674007415771, "training_acc": 74.0, "val_loss": 0.6788677906990052, "val_acc": 56.0}
{"epoch": 65, "training_loss": 0.6407602715492249, "training_acc": 76.0, "val_loss": 0.6930592393875122, "val_acc": 48.0}
{"epoch": 66, "training_loss": 0.6447643828392029, "training_acc": 72.0, "val_loss": 0.6802474331855773, "val_acc": 60.0}
{"epoch": 67, "training_loss": 0.6450717663764953, "training_acc": 73.0, "val_loss": 0.6849571967124939, "val_acc": 56.0}
{"epoch": 68, "training_loss": 0.6417401885986328, "training_acc": 76.0, "val_loss": 0.6838256168365479, "val_acc": 56.0}
{"epoch": 69, "training_loss": 0.6366599464416504, "training_acc": 75.0, "val_loss": 0.6829355359077454, "val_acc": 60.0}
{"epoch": 70, "training_loss": 0.6346121764183045, "training_acc": 80.0, "val_loss": 0.6795843362808227, "val_acc": 56.0}
{"epoch": 71, "training_loss": 0.6437118434906006, "training_acc": 71.0, "val_loss": 0.6860929989814758, "val_acc": 56.0}
{"epoch": 72, "training_loss": 0.6393555068969726, "training_acc": 75.0, "val_loss": 0.6863618135452271, "val_acc": 56.0}
{"epoch": 73, "training_loss": 0.6310694074630737, "training_acc": 76.0, "val_loss": 0.6775859570503235, "val_acc": 56.0}
{"epoch": 74, "training_loss": 0.6285884428024292, "training_acc": 76.0, "val_loss": 0.6886899566650391, "val_acc": 56.0}
{"epoch": 75, "training_loss": 0.6321488046646118, "training_acc": 77.0, "val_loss": 0.6810666871070862, "val_acc": 56.0}
{"epoch": 76, "training_loss": 0.6288858032226563, "training_acc": 77.0, "val_loss": 0.6920610380172729, "val_acc": 56.0}
{"epoch": 77, "training_loss": 0.6275561690330506, "training_acc": 77.0, "val_loss": 0.6775210571289062, "val_acc": 56.0}
{"epoch": 78, "training_loss": 0.6324062442779541, "training_acc": 76.0, "val_loss": 0.6969508934020996, "val_acc": 52.0}
{"epoch": 79, "training_loss": 0.6289204287528992, "training_acc": 76.0, "val_loss": 0.6780408501625061, "val_acc": 56.0}
{"epoch": 80, "training_loss": 0.6219342803955078, "training_acc": 76.0, "val_loss": 0.6984141492843627, "val_acc": 56.0}
{"epoch": 81, "training_loss": 0.6269834971427918, "training_acc": 77.0, "val_loss": 0.6749569439888, "val_acc": 56.0}
