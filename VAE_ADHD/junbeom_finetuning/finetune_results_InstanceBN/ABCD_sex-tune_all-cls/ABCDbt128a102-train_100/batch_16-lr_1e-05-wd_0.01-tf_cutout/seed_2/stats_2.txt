"main_modify.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 2 --task ABCD_sex --input_option BT_org --learning_rate 1e-5 --batch_size 16 --weight_decay 1e-2 --BN inst --save_path finetune_results_InstanceBN --run_where lab"
{"epoch": 0, "training_loss": 0.6931368684768677, "training_acc": 53.0, "val_loss": 0.6970681858062744, "val_acc": 52.0}
{"epoch": 1, "training_loss": 0.692454776763916, "training_acc": 53.0, "val_loss": 0.6965518689155579, "val_acc": 52.0}
{"epoch": 2, "training_loss": 0.6922639274597168, "training_acc": 53.0, "val_loss": 0.6960718607902527, "val_acc": 52.0}
{"epoch": 3, "training_loss": 0.6908191537857056, "training_acc": 53.0, "val_loss": 0.6963310265541076, "val_acc": 52.0}
{"epoch": 4, "training_loss": 0.6906366348266602, "training_acc": 53.0, "val_loss": 0.6962292170524598, "val_acc": 52.0}
{"epoch": 5, "training_loss": 0.6894753742218017, "training_acc": 53.0, "val_loss": 0.6964190316200256, "val_acc": 52.0}
{"epoch": 6, "training_loss": 0.6877863264083862, "training_acc": 53.0, "val_loss": 0.6953997874259948, "val_acc": 52.0}
{"epoch": 7, "training_loss": 0.6867036771774292, "training_acc": 53.0, "val_loss": 0.6962603449821472, "val_acc": 52.0}
{"epoch": 8, "training_loss": 0.6843604373931885, "training_acc": 53.0, "val_loss": 0.6958930683135987, "val_acc": 52.0}
{"epoch": 9, "training_loss": 0.6848469257354737, "training_acc": 53.0, "val_loss": 0.6946566700935364, "val_acc": 52.0}
{"epoch": 10, "training_loss": 0.680986135005951, "training_acc": 53.0, "val_loss": 0.6951774263381958, "val_acc": 52.0}
{"epoch": 11, "training_loss": 0.6816954708099365, "training_acc": 53.0, "val_loss": 0.6949562168121338, "val_acc": 52.0}
{"epoch": 12, "training_loss": 0.6803956556320191, "training_acc": 53.0, "val_loss": 0.694247977733612, "val_acc": 52.0}
{"epoch": 13, "training_loss": 0.6775310420989991, "training_acc": 53.0, "val_loss": 0.695090000629425, "val_acc": 52.0}
{"epoch": 14, "training_loss": 0.676477575302124, "training_acc": 53.0, "val_loss": 0.6947137475013733, "val_acc": 52.0}
{"epoch": 15, "training_loss": 0.674015085697174, "training_acc": 53.0, "val_loss": 0.6947592163085937, "val_acc": 52.0}
{"epoch": 16, "training_loss": 0.673938136100769, "training_acc": 53.0, "val_loss": 0.6938554310798645, "val_acc": 52.0}
{"epoch": 17, "training_loss": 0.6729433465003968, "training_acc": 53.0, "val_loss": 0.6944001317024231, "val_acc": 52.0}
{"epoch": 18, "training_loss": 0.6744712400436401, "training_acc": 53.0, "val_loss": 0.6929244375228882, "val_acc": 52.0}
{"epoch": 19, "training_loss": 0.6702363967895508, "training_acc": 54.0, "val_loss": 0.6926722955703736, "val_acc": 52.0}
{"epoch": 20, "training_loss": 0.6684444808959961, "training_acc": 57.0, "val_loss": 0.6935681104660034, "val_acc": 52.0}
{"epoch": 21, "training_loss": 0.6705174899101257, "training_acc": 54.0, "val_loss": 0.6924928307533265, "val_acc": 52.0}
{"epoch": 22, "training_loss": 0.6698334741592408, "training_acc": 59.0, "val_loss": 0.6914497494697571, "val_acc": 52.0}
{"epoch": 23, "training_loss": 0.675066339969635, "training_acc": 54.0, "val_loss": 0.6924510025978088, "val_acc": 52.0}
{"epoch": 24, "training_loss": 0.669041543006897, "training_acc": 55.0, "val_loss": 0.6928593778610229, "val_acc": 52.0}
{"epoch": 25, "training_loss": 0.6665936064720154, "training_acc": 56.0, "val_loss": 0.6917185997962951, "val_acc": 52.0}
{"epoch": 26, "training_loss": 0.6688237404823303, "training_acc": 59.0, "val_loss": 0.6921219658851624, "val_acc": 52.0}
{"epoch": 27, "training_loss": 0.665926947593689, "training_acc": 58.0, "val_loss": 0.6921352720260621, "val_acc": 52.0}
{"epoch": 28, "training_loss": 0.6622457599639893, "training_acc": 61.0, "val_loss": 0.6922522807121276, "val_acc": 52.0}
{"epoch": 29, "training_loss": 0.6632229089736938, "training_acc": 63.0, "val_loss": 0.6910583257675171, "val_acc": 52.0}
{"epoch": 30, "training_loss": 0.661728138923645, "training_acc": 61.0, "val_loss": 0.6948040771484375, "val_acc": 56.0}
{"epoch": 31, "training_loss": 0.6664850997924805, "training_acc": 64.0, "val_loss": 0.6920452165603638, "val_acc": 56.0}
{"epoch": 32, "training_loss": 0.6624800372123718, "training_acc": 64.0, "val_loss": 0.6912111639976501, "val_acc": 56.0}
{"epoch": 33, "training_loss": 0.6637013292312622, "training_acc": 64.0, "val_loss": 0.6935629224777222, "val_acc": 56.0}
{"epoch": 34, "training_loss": 0.6589842224121094, "training_acc": 62.0, "val_loss": 0.6908831906318664, "val_acc": 60.0}
{"epoch": 35, "training_loss": 0.6554173350334167, "training_acc": 68.0, "val_loss": 0.6910872769355774, "val_acc": 56.0}
{"epoch": 36, "training_loss": 0.6589273166656494, "training_acc": 65.0, "val_loss": 0.6919288229942322, "val_acc": 60.0}
{"epoch": 37, "training_loss": 0.654320855140686, "training_acc": 67.0, "val_loss": 0.6931648683547974, "val_acc": 56.0}
{"epoch": 38, "training_loss": 0.6526630210876465, "training_acc": 67.0, "val_loss": 0.6917680597305298, "val_acc": 60.0}
{"epoch": 39, "training_loss": 0.6528179955482483, "training_acc": 68.0, "val_loss": 0.6952264666557312, "val_acc": 52.0}
{"epoch": 40, "training_loss": 0.6582658624649048, "training_acc": 66.0, "val_loss": 0.6903976130485535, "val_acc": 56.0}
{"epoch": 41, "training_loss": 0.6575199270248413, "training_acc": 65.0, "val_loss": 0.6928420948982239, "val_acc": 52.0}
{"epoch": 42, "training_loss": 0.6514641761779785, "training_acc": 74.0, "val_loss": 0.6939115166664124, "val_acc": 48.0}
{"epoch": 43, "training_loss": 0.6552252054214478, "training_acc": 67.0, "val_loss": 0.6923300099372863, "val_acc": 52.0}
{"epoch": 44, "training_loss": 0.6541634273529052, "training_acc": 68.0, "val_loss": 0.6939853620529175, "val_acc": 52.0}
{"epoch": 45, "training_loss": 0.6549645256996155, "training_acc": 67.0, "val_loss": 0.6950507426261902, "val_acc": 48.0}
{"epoch": 46, "training_loss": 0.6485392665863037, "training_acc": 73.0, "val_loss": 0.6944878840446472, "val_acc": 52.0}
{"epoch": 47, "training_loss": 0.6552264904975891, "training_acc": 66.0, "val_loss": 0.6918152928352356, "val_acc": 56.0}
{"epoch": 48, "training_loss": 0.6489166355133057, "training_acc": 74.0, "val_loss": 0.6938074302673339, "val_acc": 52.0}
{"epoch": 49, "training_loss": 0.652024621963501, "training_acc": 68.0, "val_loss": 0.6943443274497986, "val_acc": 52.0}
{"epoch": 50, "training_loss": 0.6480217790603637, "training_acc": 70.0, "val_loss": 0.6934433460235596, "val_acc": 44.0}
{"epoch": 51, "training_loss": 0.6431601977348328, "training_acc": 72.0, "val_loss": 0.695418291091919, "val_acc": 52.0}
{"epoch": 52, "training_loss": 0.6468380689620972, "training_acc": 71.0, "val_loss": 0.69230309009552, "val_acc": 56.0}
{"epoch": 53, "training_loss": 0.6443467235565186, "training_acc": 74.0, "val_loss": 0.6949614024162293, "val_acc": 52.0}
{"epoch": 54, "training_loss": 0.6419020199775696, "training_acc": 72.0, "val_loss": 0.69264084815979, "val_acc": 56.0}
{"epoch": 55, "training_loss": 0.6403243827819824, "training_acc": 74.0, "val_loss": 0.6979247784614563, "val_acc": 48.0}
{"epoch": 56, "training_loss": 0.6416715955734253, "training_acc": 72.0, "val_loss": 0.6946844029426574, "val_acc": 56.0}
{"epoch": 57, "training_loss": 0.6473992228507995, "training_acc": 73.0, "val_loss": 0.693721764087677, "val_acc": 52.0}
{"epoch": 58, "training_loss": 0.6404613256454468, "training_acc": 69.0, "val_loss": 0.6959725284576416, "val_acc": 52.0}
{"epoch": 59, "training_loss": 0.6379029440879822, "training_acc": 72.0, "val_loss": 0.6912622046470642, "val_acc": 56.0}
