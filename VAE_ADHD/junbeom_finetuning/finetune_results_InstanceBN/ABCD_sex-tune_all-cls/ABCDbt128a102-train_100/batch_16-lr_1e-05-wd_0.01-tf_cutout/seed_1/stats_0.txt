"main_modify.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 1 --task ABCD_sex --input_option BT_org --learning_rate 1e-5 --batch_size 16 --weight_decay 1e-2 --BN inst --save_path finetune_results_InstanceBN --run_where lab"
{"epoch": 0, "training_loss": 0.6917661452293395, "training_acc": 53.0, "val_loss": 0.6918372654914856, "val_acc": 52.0}
{"epoch": 1, "training_loss": 0.6913073205947876, "training_acc": 53.0, "val_loss": 0.6908986306190491, "val_acc": 52.0}
{"epoch": 2, "training_loss": 0.6905283832550049, "training_acc": 53.0, "val_loss": 0.692371883392334, "val_acc": 52.0}
{"epoch": 3, "training_loss": 0.690302062034607, "training_acc": 53.0, "val_loss": 0.6913815450668335, "val_acc": 52.0}
{"epoch": 4, "training_loss": 0.6886892008781433, "training_acc": 53.0, "val_loss": 0.6903196454048157, "val_acc": 52.0}
{"epoch": 5, "training_loss": 0.6881239080429077, "training_acc": 53.0, "val_loss": 0.6909469223022461, "val_acc": 52.0}
{"epoch": 6, "training_loss": 0.6878706169128418, "training_acc": 53.0, "val_loss": 0.6912603855133057, "val_acc": 52.0}
{"epoch": 7, "training_loss": 0.6859462785720826, "training_acc": 53.0, "val_loss": 0.6917458438873291, "val_acc": 52.0}
{"epoch": 8, "training_loss": 0.6853936386108398, "training_acc": 53.0, "val_loss": 0.6901666307449341, "val_acc": 52.0}
{"epoch": 9, "training_loss": 0.6850992822647095, "training_acc": 53.0, "val_loss": 0.6929031682014465, "val_acc": 52.0}
{"epoch": 10, "training_loss": 0.6854393196105957, "training_acc": 53.0, "val_loss": 0.6898982000350952, "val_acc": 52.0}
{"epoch": 11, "training_loss": 0.6850042986869812, "training_acc": 53.0, "val_loss": 0.6913544559478759, "val_acc": 52.0}
{"epoch": 12, "training_loss": 0.6835495328903198, "training_acc": 53.0, "val_loss": 0.692366828918457, "val_acc": 52.0}
{"epoch": 13, "training_loss": 0.6841668224334717, "training_acc": 53.0, "val_loss": 0.6922456192970275, "val_acc": 52.0}
{"epoch": 14, "training_loss": 0.6823394989967346, "training_acc": 53.0, "val_loss": 0.6915851974487305, "val_acc": 52.0}
{"epoch": 15, "training_loss": 0.680414502620697, "training_acc": 53.0, "val_loss": 0.6894480633735657, "val_acc": 52.0}
{"epoch": 16, "training_loss": 0.6810791945457458, "training_acc": 53.0, "val_loss": 0.6918308568000794, "val_acc": 52.0}
{"epoch": 17, "training_loss": 0.6812537670135498, "training_acc": 53.0, "val_loss": 0.689850811958313, "val_acc": 52.0}
{"epoch": 18, "training_loss": 0.6792089796066284, "training_acc": 53.0, "val_loss": 0.6904669499397278, "val_acc": 52.0}
{"epoch": 19, "training_loss": 0.6803470540046692, "training_acc": 53.0, "val_loss": 0.6940544462203979, "val_acc": 48.0}
{"epoch": 20, "training_loss": 0.6812605142593384, "training_acc": 53.0, "val_loss": 0.6916647291183472, "val_acc": 52.0}
{"epoch": 21, "training_loss": 0.677766318321228, "training_acc": 53.0, "val_loss": 0.69122403383255, "val_acc": 52.0}
{"epoch": 22, "training_loss": 0.6762086939811707, "training_acc": 53.0, "val_loss": 0.691062912940979, "val_acc": 52.0}
{"epoch": 23, "training_loss": 0.67838627576828, "training_acc": 54.0, "val_loss": 0.687927963733673, "val_acc": 52.0}
{"epoch": 24, "training_loss": 0.6764699792861939, "training_acc": 55.0, "val_loss": 0.6958421611785889, "val_acc": 48.0}
{"epoch": 25, "training_loss": 0.6772255849838257, "training_acc": 53.0, "val_loss": 0.6928622961044312, "val_acc": 56.0}
{"epoch": 26, "training_loss": 0.6756464791297913, "training_acc": 54.0, "val_loss": 0.6934091544151306, "val_acc": 52.0}
{"epoch": 27, "training_loss": 0.6771793007850647, "training_acc": 54.0, "val_loss": 0.690512592792511, "val_acc": 56.0}
{"epoch": 28, "training_loss": 0.6810993146896362, "training_acc": 54.0, "val_loss": 0.6924564909934997, "val_acc": 48.0}
{"epoch": 29, "training_loss": 0.6849532842636108, "training_acc": 54.0, "val_loss": 0.6931110382080078, "val_acc": 48.0}
{"epoch": 30, "training_loss": 0.6772898674011231, "training_acc": 54.0, "val_loss": 0.6882901430130005, "val_acc": 56.0}
{"epoch": 31, "training_loss": 0.6742571640014648, "training_acc": 57.0, "val_loss": 0.6926723456382752, "val_acc": 52.0}
{"epoch": 32, "training_loss": 0.6737406921386718, "training_acc": 57.0, "val_loss": 0.6892414617538453, "val_acc": 56.0}
{"epoch": 33, "training_loss": 0.6710793495178222, "training_acc": 58.0, "val_loss": 0.693758544921875, "val_acc": 48.0}
{"epoch": 34, "training_loss": 0.6718886947631836, "training_acc": 57.0, "val_loss": 0.6932596254348755, "val_acc": 52.0}
{"epoch": 35, "training_loss": 0.6731587100028992, "training_acc": 59.0, "val_loss": 0.6916450953483582, "val_acc": 52.0}
{"epoch": 36, "training_loss": 0.6701791143417358, "training_acc": 60.0, "val_loss": 0.6973852729797363, "val_acc": 48.0}
{"epoch": 37, "training_loss": 0.6682280898094177, "training_acc": 64.0, "val_loss": 0.6943017530441284, "val_acc": 44.0}
{"epoch": 38, "training_loss": 0.6712171125411988, "training_acc": 66.0, "val_loss": 0.6934139895439148, "val_acc": 48.0}
{"epoch": 39, "training_loss": 0.6690802907943726, "training_acc": 63.0, "val_loss": 0.6934354209899902, "val_acc": 48.0}
{"epoch": 40, "training_loss": 0.6660432982444763, "training_acc": 69.0, "val_loss": 0.691213059425354, "val_acc": 52.0}
{"epoch": 41, "training_loss": 0.6711925053596497, "training_acc": 67.0, "val_loss": 0.696739912033081, "val_acc": 52.0}
{"epoch": 42, "training_loss": 0.6687879753112793, "training_acc": 63.0, "val_loss": 0.6921607947349548, "val_acc": 52.0}
