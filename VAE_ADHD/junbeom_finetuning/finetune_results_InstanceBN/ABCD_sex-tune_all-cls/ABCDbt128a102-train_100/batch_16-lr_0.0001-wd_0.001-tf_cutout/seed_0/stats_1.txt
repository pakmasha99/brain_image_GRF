"main_modify.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --learning_rate 1e-4 --batch_size 16 --weight_decay 1e-3 --BN inst --save_path finetune_results_InstanceBN --run_where lab"
{"epoch": 0, "training_loss": 0.7004207158088684, "training_acc": 47.0, "val_loss": 0.6952293515205383, "val_acc": 48.0}
{"epoch": 1, "training_loss": 0.6976832818984985, "training_acc": 42.0, "val_loss": 0.6932309651374817, "val_acc": 52.0}
{"epoch": 2, "training_loss": 0.693732476234436, "training_acc": 53.0, "val_loss": 0.6924967217445374, "val_acc": 52.0}
{"epoch": 3, "training_loss": 0.6936282920837402, "training_acc": 53.0, "val_loss": 0.6925619220733643, "val_acc": 52.0}
{"epoch": 4, "training_loss": 0.6916373157501221, "training_acc": 53.0, "val_loss": 0.6924833035469056, "val_acc": 52.0}
{"epoch": 5, "training_loss": 0.6934004688262939, "training_acc": 53.0, "val_loss": 0.6924647736549377, "val_acc": 52.0}
{"epoch": 6, "training_loss": 0.6923823833465577, "training_acc": 53.0, "val_loss": 0.6931550002098084, "val_acc": 52.0}
{"epoch": 7, "training_loss": 0.694472713470459, "training_acc": 53.0, "val_loss": 0.6924506950378418, "val_acc": 52.0}
{"epoch": 8, "training_loss": 0.6915564346313476, "training_acc": 53.0, "val_loss": 0.6927684831619263, "val_acc": 52.0}
{"epoch": 9, "training_loss": 0.6966658186912537, "training_acc": 53.0, "val_loss": 0.6923998403549194, "val_acc": 52.0}
{"epoch": 10, "training_loss": 0.6941326546669007, "training_acc": 53.0, "val_loss": 0.6928045201301575, "val_acc": 52.0}
{"epoch": 11, "training_loss": 0.6906009364128113, "training_acc": 53.0, "val_loss": 0.6924326848983765, "val_acc": 52.0}
{"epoch": 12, "training_loss": 0.6952115392684937, "training_acc": 40.0, "val_loss": 0.6925571537017823, "val_acc": 52.0}
{"epoch": 13, "training_loss": 0.6942381286621093, "training_acc": 53.0, "val_loss": 0.6925608730316162, "val_acc": 52.0}
{"epoch": 14, "training_loss": 0.6927883553504944, "training_acc": 53.0, "val_loss": 0.6927310109138489, "val_acc": 52.0}
{"epoch": 15, "training_loss": 0.6923438930511474, "training_acc": 53.0, "val_loss": 0.6926977562904358, "val_acc": 52.0}
{"epoch": 16, "training_loss": 0.6916240072250366, "training_acc": 53.0, "val_loss": 0.6931126022338867, "val_acc": 52.0}
{"epoch": 17, "training_loss": 0.6916850900650025, "training_acc": 53.0, "val_loss": 0.6933681058883667, "val_acc": 52.0}
{"epoch": 18, "training_loss": 0.6919585061073303, "training_acc": 53.0, "val_loss": 0.6948232221603393, "val_acc": 52.0}
{"epoch": 19, "training_loss": 0.6925643396377563, "training_acc": 53.0, "val_loss": 0.6929261422157288, "val_acc": 52.0}
{"epoch": 20, "training_loss": 0.6927406907081604, "training_acc": 53.0, "val_loss": 0.6926769852638245, "val_acc": 52.0}
{"epoch": 21, "training_loss": 0.692427122592926, "training_acc": 53.0, "val_loss": 0.6923907899856567, "val_acc": 52.0}
{"epoch": 22, "training_loss": 0.6919959354400634, "training_acc": 53.0, "val_loss": 0.6924224853515625, "val_acc": 52.0}
{"epoch": 23, "training_loss": 0.6916111063957214, "training_acc": 53.0, "val_loss": 0.6928934025764465, "val_acc": 52.0}
{"epoch": 24, "training_loss": 0.6918829798698425, "training_acc": 53.0, "val_loss": 0.6922296261787415, "val_acc": 52.0}
{"epoch": 25, "training_loss": 0.6927806568145752, "training_acc": 53.0, "val_loss": 0.6923336267471314, "val_acc": 52.0}
{"epoch": 26, "training_loss": 0.6922006559371948, "training_acc": 53.0, "val_loss": 0.6933787202835083, "val_acc": 52.0}
{"epoch": 27, "training_loss": 0.6951941633224488, "training_acc": 53.0, "val_loss": 0.6936654782295227, "val_acc": 52.0}
{"epoch": 28, "training_loss": 0.6949804711341858, "training_acc": 53.0, "val_loss": 0.692496371269226, "val_acc": 52.0}
{"epoch": 29, "training_loss": 0.6923342704772949, "training_acc": 53.0, "val_loss": 0.6923307704925538, "val_acc": 52.0}
{"epoch": 30, "training_loss": 0.6930953741073609, "training_acc": 53.0, "val_loss": 0.692818455696106, "val_acc": 52.0}
{"epoch": 31, "training_loss": 0.6937991595268249, "training_acc": 53.0, "val_loss": 0.6962751770019531, "val_acc": 52.0}
{"epoch": 32, "training_loss": 0.6934168577194214, "training_acc": 53.0, "val_loss": 0.6924677729606629, "val_acc": 52.0}
{"epoch": 33, "training_loss": 0.6921168994903565, "training_acc": 53.0, "val_loss": 0.692441794872284, "val_acc": 52.0}
{"epoch": 34, "training_loss": 0.6924501943588257, "training_acc": 53.0, "val_loss": 0.692703332901001, "val_acc": 52.0}
{"epoch": 35, "training_loss": 0.6923475408554077, "training_acc": 53.0, "val_loss": 0.6922973942756653, "val_acc": 52.0}
{"epoch": 36, "training_loss": 0.6939834833145142, "training_acc": 53.0, "val_loss": 0.6932497811317444, "val_acc": 52.0}
{"epoch": 37, "training_loss": 0.6979458093643188, "training_acc": 53.0, "val_loss": 0.6924693298339843, "val_acc": 52.0}
{"epoch": 38, "training_loss": 0.6924607467651367, "training_acc": 53.0, "val_loss": 0.6924295353889466, "val_acc": 52.0}
{"epoch": 39, "training_loss": 0.6927202558517456, "training_acc": 53.0, "val_loss": 0.6925880360603333, "val_acc": 52.0}
{"epoch": 40, "training_loss": 0.6915275049209595, "training_acc": 53.0, "val_loss": 0.6924139881134033, "val_acc": 52.0}
{"epoch": 41, "training_loss": 0.6934916424751282, "training_acc": 53.0, "val_loss": 0.6923439121246338, "val_acc": 52.0}
{"epoch": 42, "training_loss": 0.6915165138244629, "training_acc": 53.0, "val_loss": 0.6929918360710144, "val_acc": 52.0}
{"epoch": 43, "training_loss": 0.6916511011123657, "training_acc": 53.0, "val_loss": 0.6924827647209167, "val_acc": 52.0}
