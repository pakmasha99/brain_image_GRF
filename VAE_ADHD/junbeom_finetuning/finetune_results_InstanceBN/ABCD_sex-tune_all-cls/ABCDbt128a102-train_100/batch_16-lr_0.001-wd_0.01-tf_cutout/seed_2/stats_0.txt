"main_modify.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 2 --task ABCD_sex --input_option BT_org --learning_rate 1e-3 --batch_size 16 --weight_decay 1e-2 --BN inst --save_path finetune_results_InstanceBN --run_where lab"
{"epoch": 0, "training_loss": 0.7169643425941468, "training_acc": 50.0, "val_loss": 0.7059267926216125, "val_acc": 52.0}
{"epoch": 1, "training_loss": 0.7118061566352845, "training_acc": 47.0, "val_loss": 0.6992970705032349, "val_acc": 48.0}
{"epoch": 2, "training_loss": 0.7003387498855591, "training_acc": 50.0, "val_loss": 0.6926588392257691, "val_acc": 52.0}
{"epoch": 3, "training_loss": 0.6961835741996765, "training_acc": 49.0, "val_loss": 0.7139320063591004, "val_acc": 52.0}
{"epoch": 4, "training_loss": 0.7030794429779053, "training_acc": 53.0, "val_loss": 0.6926746058464051, "val_acc": 52.0}
{"epoch": 5, "training_loss": 0.6962800741195678, "training_acc": 53.0, "val_loss": 0.6940391254425049, "val_acc": 48.0}
{"epoch": 6, "training_loss": 0.7092833971977234, "training_acc": 47.0, "val_loss": 0.6941786527633667, "val_acc": 52.0}
{"epoch": 7, "training_loss": 0.7143529176712036, "training_acc": 53.0, "val_loss": 0.6939823293685913, "val_acc": 52.0}
{"epoch": 8, "training_loss": 0.7018741726875305, "training_acc": 43.0, "val_loss": 0.6934664845466614, "val_acc": 48.0}
{"epoch": 9, "training_loss": 0.6959668946266174, "training_acc": 47.0, "val_loss": 0.6937151789665222, "val_acc": 52.0}
{"epoch": 10, "training_loss": 0.6946638154983521, "training_acc": 53.0, "val_loss": 0.6924182605743409, "val_acc": 52.0}
{"epoch": 11, "training_loss": 0.6960916566848755, "training_acc": 47.0, "val_loss": 0.6982762813568115, "val_acc": 52.0}
{"epoch": 12, "training_loss": 0.704976167678833, "training_acc": 53.0, "val_loss": 0.6985463047027588, "val_acc": 52.0}
{"epoch": 13, "training_loss": 0.6987460565567016, "training_acc": 53.0, "val_loss": 0.6928643131256104, "val_acc": 52.0}
{"epoch": 14, "training_loss": 0.6929500341415405, "training_acc": 51.0, "val_loss": 0.6934526252746582, "val_acc": 52.0}
{"epoch": 15, "training_loss": 0.6941213369369507, "training_acc": 53.0, "val_loss": 0.6939701080322266, "val_acc": 52.0}
{"epoch": 16, "training_loss": 0.6963811492919922, "training_acc": 47.0, "val_loss": 0.692462465763092, "val_acc": 52.0}
{"epoch": 17, "training_loss": 0.6972550988197327, "training_acc": 47.0, "val_loss": 0.6952783179283142, "val_acc": 52.0}
{"epoch": 18, "training_loss": 0.7134780669212342, "training_acc": 53.0, "val_loss": 0.6939857482910157, "val_acc": 52.0}
{"epoch": 19, "training_loss": 0.700120918750763, "training_acc": 45.0, "val_loss": 0.6924110341072083, "val_acc": 52.0}
{"epoch": 20, "training_loss": 0.6977915239334106, "training_acc": 53.0, "val_loss": 0.6923676991462707, "val_acc": 52.0}
{"epoch": 21, "training_loss": 0.693785412311554, "training_acc": 53.0, "val_loss": 0.6957097268104553, "val_acc": 48.0}
{"epoch": 22, "training_loss": 0.7126852583885193, "training_acc": 47.0, "val_loss": 0.6924656224250794, "val_acc": 52.0}
{"epoch": 23, "training_loss": 0.7054899978637695, "training_acc": 53.0, "val_loss": 0.6944677710533143, "val_acc": 52.0}
{"epoch": 24, "training_loss": 0.6923308563232422, "training_acc": 53.0, "val_loss": 0.6953209805488586, "val_acc": 48.0}
{"epoch": 25, "training_loss": 0.7028846740722656, "training_acc": 37.0, "val_loss": 0.6948006200790405, "val_acc": 52.0}
{"epoch": 26, "training_loss": 0.7001995277404786, "training_acc": 53.0, "val_loss": 0.7104520368576049, "val_acc": 52.0}
{"epoch": 27, "training_loss": 0.7070478749275207, "training_acc": 53.0, "val_loss": 0.6953307151794433, "val_acc": 52.0}
{"epoch": 28, "training_loss": 0.7047324681282043, "training_acc": 53.0, "val_loss": 0.6927357220649719, "val_acc": 52.0}
{"epoch": 29, "training_loss": 0.6995468425750733, "training_acc": 51.0, "val_loss": 0.6953627514839172, "val_acc": 48.0}
{"epoch": 30, "training_loss": 0.7025014567375183, "training_acc": 49.0, "val_loss": 0.6939870619773865, "val_acc": 52.0}
{"epoch": 31, "training_loss": 0.6980140352249146, "training_acc": 45.0, "val_loss": 0.6926821637153625, "val_acc": 52.0}
{"epoch": 32, "training_loss": 0.6935061836242675, "training_acc": 53.0, "val_loss": 0.6958656787872315, "val_acc": 52.0}
{"epoch": 33, "training_loss": 0.700816764831543, "training_acc": 53.0, "val_loss": 0.7009908747673035, "val_acc": 52.0}
{"epoch": 34, "training_loss": 0.7009174847602844, "training_acc": 53.0, "val_loss": 0.7038017725944519, "val_acc": 52.0}
{"epoch": 35, "training_loss": 0.700395712852478, "training_acc": 53.0, "val_loss": 0.6923944973945617, "val_acc": 52.0}
{"epoch": 36, "training_loss": 0.7025185084342956, "training_acc": 41.0, "val_loss": 0.6932236480712891, "val_acc": 48.0}
{"epoch": 37, "training_loss": 0.7047438621520996, "training_acc": 45.0, "val_loss": 0.6942738509178161, "val_acc": 48.0}
{"epoch": 38, "training_loss": 0.6980347657203674, "training_acc": 47.0, "val_loss": 0.6940560102462768, "val_acc": 48.0}
{"epoch": 39, "training_loss": 0.6901593422889709, "training_acc": 55.0, "val_loss": 0.6960360145568848, "val_acc": 52.0}
