"main_modify.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 2 --task ABCD_sex --input_option BT_org --learning_rate 1e-3 --batch_size 16 --weight_decay 1e-2 --BN inst --save_path finetune_results_InstanceBN --run_where lab"
{"epoch": 0, "training_loss": 0.7480941104888916, "training_acc": 45.0, "val_loss": 0.6964222359657287, "val_acc": 48.0}
{"epoch": 1, "training_loss": 0.7071353220939636, "training_acc": 47.0, "val_loss": 0.6932609939575195, "val_acc": 52.0}
{"epoch": 2, "training_loss": 0.6958382511138916, "training_acc": 53.0, "val_loss": 0.7005571055412293, "val_acc": 52.0}
{"epoch": 3, "training_loss": 0.6952834463119507, "training_acc": 53.0, "val_loss": 0.6945329713821411, "val_acc": 48.0}
{"epoch": 4, "training_loss": 0.6999130439758301, "training_acc": 47.0, "val_loss": 0.6924960017204285, "val_acc": 52.0}
{"epoch": 5, "training_loss": 0.6993432211875915, "training_acc": 53.0, "val_loss": 0.6984307312965393, "val_acc": 52.0}
{"epoch": 6, "training_loss": 0.7146968126296998, "training_acc": 43.0, "val_loss": 0.6956819534301758, "val_acc": 52.0}
{"epoch": 7, "training_loss": 0.6993061113357544, "training_acc": 53.0, "val_loss": 0.6923588252067566, "val_acc": 52.0}
{"epoch": 8, "training_loss": 0.7074456906318665, "training_acc": 45.0, "val_loss": 0.6923721480369568, "val_acc": 52.0}
{"epoch": 9, "training_loss": 0.6922859954833984, "training_acc": 53.0, "val_loss": 0.7006219935417175, "val_acc": 52.0}
{"epoch": 10, "training_loss": 0.7107155966758728, "training_acc": 53.0, "val_loss": 0.693192789554596, "val_acc": 52.0}
{"epoch": 11, "training_loss": 0.6959039974212646, "training_acc": 53.0, "val_loss": 0.6963514304161071, "val_acc": 48.0}
{"epoch": 12, "training_loss": 0.6995505952835083, "training_acc": 47.0, "val_loss": 0.6924821305274963, "val_acc": 52.0}
{"epoch": 13, "training_loss": 0.7034002494812012, "training_acc": 41.0, "val_loss": 0.6948142623901368, "val_acc": 52.0}
{"epoch": 14, "training_loss": 0.6951956343650818, "training_acc": 53.0, "val_loss": 0.6934712958335877, "val_acc": 48.0}
{"epoch": 15, "training_loss": 0.6951265358924865, "training_acc": 45.0, "val_loss": 0.6935423970222473, "val_acc": 48.0}
{"epoch": 16, "training_loss": 0.6966511964797973, "training_acc": 47.0, "val_loss": 0.6935419607162475, "val_acc": 52.0}
{"epoch": 17, "training_loss": 0.6962975597381592, "training_acc": 53.0, "val_loss": 0.6926255917549133, "val_acc": 52.0}
{"epoch": 18, "training_loss": 0.6925156354904175, "training_acc": 53.0, "val_loss": 0.695199191570282, "val_acc": 48.0}
{"epoch": 19, "training_loss": 0.699055323600769, "training_acc": 47.0, "val_loss": 0.6926948928833008, "val_acc": 52.0}
{"epoch": 20, "training_loss": 0.7043954849243164, "training_acc": 49.0, "val_loss": 0.6923513102531433, "val_acc": 52.0}
{"epoch": 21, "training_loss": 0.690307023525238, "training_acc": 53.0, "val_loss": 0.7098760199546814, "val_acc": 52.0}
{"epoch": 22, "training_loss": 0.7094196653366089, "training_acc": 53.0, "val_loss": 0.6954827809333801, "val_acc": 52.0}
{"epoch": 23, "training_loss": 0.7028603172302246, "training_acc": 43.0, "val_loss": 0.6950024437904357, "val_acc": 52.0}
{"epoch": 24, "training_loss": 0.6933046054840087, "training_acc": 53.0, "val_loss": 0.7040553545951843, "val_acc": 52.0}
{"epoch": 25, "training_loss": 0.7013754844665527, "training_acc": 53.0, "val_loss": 0.6923717856407166, "val_acc": 52.0}
{"epoch": 26, "training_loss": 0.6998173594474792, "training_acc": 45.0, "val_loss": 0.6923472261428834, "val_acc": 52.0}
{"epoch": 27, "training_loss": 0.6949555683135986, "training_acc": 47.0, "val_loss": 0.6955080127716065, "val_acc": 52.0}
{"epoch": 28, "training_loss": 0.6943798089027404, "training_acc": 53.0, "val_loss": 0.6923500752449036, "val_acc": 52.0}
{"epoch": 29, "training_loss": 0.700910189151764, "training_acc": 53.0, "val_loss": 0.6925961303710938, "val_acc": 52.0}
{"epoch": 30, "training_loss": 0.6962279629707336, "training_acc": 51.0, "val_loss": 0.6923525524139404, "val_acc": 52.0}
{"epoch": 31, "training_loss": 0.7030286073684693, "training_acc": 53.0, "val_loss": 0.6935327887535095, "val_acc": 52.0}
{"epoch": 32, "training_loss": 0.6985322856903076, "training_acc": 49.0, "val_loss": 0.6930700588226318, "val_acc": 52.0}
{"epoch": 33, "training_loss": 0.6925533151626587, "training_acc": 53.0, "val_loss": 0.7046788096427917, "val_acc": 52.0}
{"epoch": 34, "training_loss": 0.70776358127594, "training_acc": 53.0, "val_loss": 0.6928800654411316, "val_acc": 52.0}
{"epoch": 35, "training_loss": 0.7061490678787231, "training_acc": 45.0, "val_loss": 0.6924099326133728, "val_acc": 52.0}
{"epoch": 36, "training_loss": 0.6943994665145874, "training_acc": 53.0, "val_loss": 0.7129769134521484, "val_acc": 52.0}
{"epoch": 37, "training_loss": 0.717169246673584, "training_acc": 53.0, "val_loss": 0.7063571858406067, "val_acc": 52.0}
{"epoch": 38, "training_loss": 0.7103917574882508, "training_acc": 43.0, "val_loss": 0.6924653625488282, "val_acc": 52.0}
{"epoch": 39, "training_loss": 0.6944714593887329, "training_acc": 53.0, "val_loss": 0.6955882215499878, "val_acc": 52.0}
{"epoch": 40, "training_loss": 0.6991626644134521, "training_acc": 53.0, "val_loss": 0.6923471069335938, "val_acc": 52.0}
{"epoch": 41, "training_loss": 0.6961889433860778, "training_acc": 53.0, "val_loss": 0.703665201663971, "val_acc": 48.0}
{"epoch": 42, "training_loss": 0.7039936304092407, "training_acc": 47.0, "val_loss": 0.6923739957809448, "val_acc": 52.0}
{"epoch": 43, "training_loss": 0.7073636603355408, "training_acc": 53.0, "val_loss": 0.6923542404174805, "val_acc": 52.0}
{"epoch": 44, "training_loss": 0.696759843826294, "training_acc": 49.0, "val_loss": 0.6932826566696167, "val_acc": 48.0}
{"epoch": 45, "training_loss": 0.6950404548645019, "training_acc": 51.0, "val_loss": 0.6962125229835511, "val_acc": 52.0}
{"epoch": 46, "training_loss": 0.7057150554656982, "training_acc": 53.0, "val_loss": 0.6925062274932862, "val_acc": 52.0}
{"epoch": 47, "training_loss": 0.7156355357170106, "training_acc": 43.0, "val_loss": 0.6925990319252014, "val_acc": 52.0}
{"epoch": 48, "training_loss": 0.7093137741088867, "training_acc": 53.0, "val_loss": 0.7011098718643188, "val_acc": 52.0}
{"epoch": 49, "training_loss": 0.7027496218681335, "training_acc": 53.0, "val_loss": 0.693267879486084, "val_acc": 52.0}
{"epoch": 50, "training_loss": 0.6968449974060058, "training_acc": 47.0, "val_loss": 0.698532555103302, "val_acc": 48.0}
{"epoch": 51, "training_loss": 0.7082815647125245, "training_acc": 41.0, "val_loss": 0.694715039730072, "val_acc": 48.0}
{"epoch": 52, "training_loss": 0.7155649375915527, "training_acc": 47.0, "val_loss": 0.6996576690673828, "val_acc": 48.0}
{"epoch": 53, "training_loss": 0.7056281685829162, "training_acc": 45.0, "val_loss": 0.6924181556701661, "val_acc": 52.0}
{"epoch": 54, "training_loss": 0.7033668422698974, "training_acc": 53.0, "val_loss": 0.6932370066642761, "val_acc": 48.0}
{"epoch": 55, "training_loss": 0.7087006878852844, "training_acc": 41.0, "val_loss": 0.6944130849838257, "val_acc": 48.0}
{"epoch": 56, "training_loss": 0.7011386299133301, "training_acc": 43.0, "val_loss": 0.694601993560791, "val_acc": 48.0}
{"epoch": 57, "training_loss": 0.696083800792694, "training_acc": 45.0, "val_loss": 0.692894070148468, "val_acc": 52.0}
{"epoch": 58, "training_loss": 0.7010372185707092, "training_acc": 43.0, "val_loss": 0.6925065422058105, "val_acc": 52.0}
{"epoch": 59, "training_loss": 0.7132325601577759, "training_acc": 41.0, "val_loss": 0.692738573551178, "val_acc": 52.0}
