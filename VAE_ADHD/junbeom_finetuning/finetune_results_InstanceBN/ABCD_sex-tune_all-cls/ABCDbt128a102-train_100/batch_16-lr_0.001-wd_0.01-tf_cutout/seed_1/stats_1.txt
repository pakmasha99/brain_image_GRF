"main_modify.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 1 --task ABCD_sex --input_option BT_org --learning_rate 1e-3 --batch_size 16 --weight_decay 1e-2 --BN inst --save_path finetune_results_InstanceBN --run_where lab"
{"epoch": 0, "training_loss": 0.711317105293274, "training_acc": 51.0, "val_loss": 0.7048659110069275, "val_acc": 52.0}
{"epoch": 1, "training_loss": 0.698089849948883, "training_acc": 53.0, "val_loss": 0.6926185321807862, "val_acc": 52.0}
{"epoch": 2, "training_loss": 0.6936854219436646, "training_acc": 51.0, "val_loss": 0.6923823261260986, "val_acc": 52.0}
{"epoch": 3, "training_loss": 0.699318642616272, "training_acc": 51.0, "val_loss": 0.6934116005897522, "val_acc": 52.0}
{"epoch": 4, "training_loss": 0.6965371751785279, "training_acc": 53.0, "val_loss": 0.694045717716217, "val_acc": 52.0}
{"epoch": 5, "training_loss": 0.6990335750579834, "training_acc": 53.0, "val_loss": 0.69254558801651, "val_acc": 52.0}
{"epoch": 6, "training_loss": 0.7066545391082764, "training_acc": 51.0, "val_loss": 0.6935785603523255, "val_acc": 48.0}
{"epoch": 7, "training_loss": 0.6922813558578491, "training_acc": 51.0, "val_loss": 0.7005018639564514, "val_acc": 52.0}
{"epoch": 8, "training_loss": 0.6973697137832642, "training_acc": 49.0, "val_loss": 0.6938910341262817, "val_acc": 52.0}
{"epoch": 9, "training_loss": 0.699626407623291, "training_acc": 53.0, "val_loss": 0.6923845601081848, "val_acc": 52.0}
{"epoch": 10, "training_loss": 0.6914559650421143, "training_acc": 53.0, "val_loss": 0.695303566455841, "val_acc": 48.0}
{"epoch": 11, "training_loss": 0.6982734489440918, "training_acc": 51.0, "val_loss": 0.6923469614982605, "val_acc": 52.0}
{"epoch": 12, "training_loss": 0.7061842155456542, "training_acc": 45.0, "val_loss": 0.6980129790306091, "val_acc": 52.0}
{"epoch": 13, "training_loss": 0.7325458288192749, "training_acc": 53.0, "val_loss": 0.6979886817932129, "val_acc": 52.0}
{"epoch": 14, "training_loss": 0.6860263395309448, "training_acc": 57.0, "val_loss": 0.7039378643035888, "val_acc": 48.0}
{"epoch": 15, "training_loss": 0.703601689338684, "training_acc": 45.0, "val_loss": 0.6927961039543152, "val_acc": 52.0}
{"epoch": 16, "training_loss": 0.6969769883155823, "training_acc": 45.0, "val_loss": 0.6929614281654358, "val_acc": 52.0}
{"epoch": 17, "training_loss": 0.6987691330909729, "training_acc": 47.0, "val_loss": 0.6958430194854737, "val_acc": 52.0}
{"epoch": 18, "training_loss": 0.7112703990936279, "training_acc": 53.0, "val_loss": 0.6961925601959229, "val_acc": 52.0}
{"epoch": 19, "training_loss": 0.6934552526473999, "training_acc": 53.0, "val_loss": 0.694632408618927, "val_acc": 48.0}
{"epoch": 20, "training_loss": 0.7122764015197753, "training_acc": 49.0, "val_loss": 0.6926390027999878, "val_acc": 52.0}
{"epoch": 21, "training_loss": 0.6999997806549072, "training_acc": 41.0, "val_loss": 0.6930916929244995, "val_acc": 52.0}
{"epoch": 22, "training_loss": 0.7051176595687866, "training_acc": 53.0, "val_loss": 0.6923470640182495, "val_acc": 52.0}
{"epoch": 23, "training_loss": 0.7018185043334961, "training_acc": 43.0, "val_loss": 0.6933190131187439, "val_acc": 48.0}
{"epoch": 24, "training_loss": 0.7045351457595825, "training_acc": 47.0, "val_loss": 0.6944284391403198, "val_acc": 52.0}
{"epoch": 25, "training_loss": 0.6976228809356689, "training_acc": 53.0, "val_loss": 0.6976602911949158, "val_acc": 52.0}
{"epoch": 26, "training_loss": 0.7030096197128296, "training_acc": 45.0, "val_loss": 0.692548439502716, "val_acc": 52.0}
{"epoch": 27, "training_loss": 0.7139764738082885, "training_acc": 53.0, "val_loss": 0.6923469352722168, "val_acc": 52.0}
{"epoch": 28, "training_loss": 0.6999464416503907, "training_acc": 45.0, "val_loss": 0.6924907159805298, "val_acc": 52.0}
{"epoch": 29, "training_loss": 0.7034974312782287, "training_acc": 53.0, "val_loss": 0.6951882719993592, "val_acc": 52.0}
{"epoch": 30, "training_loss": 0.6958721113204956, "training_acc": 53.0, "val_loss": 0.6941671323776245, "val_acc": 52.0}
{"epoch": 31, "training_loss": 0.6954903554916382, "training_acc": 53.0, "val_loss": 0.6940131592750549, "val_acc": 52.0}
{"epoch": 32, "training_loss": 0.7077722406387329, "training_acc": 53.0, "val_loss": 0.6924163103103638, "val_acc": 52.0}
{"epoch": 33, "training_loss": 0.7094677615165711, "training_acc": 43.0, "val_loss": 0.6924278378486634, "val_acc": 52.0}
{"epoch": 34, "training_loss": 0.6918564701080322, "training_acc": 53.0, "val_loss": 0.6959234356880188, "val_acc": 52.0}
{"epoch": 35, "training_loss": 0.7074723720550538, "training_acc": 47.0, "val_loss": 0.6923497295379639, "val_acc": 52.0}
{"epoch": 36, "training_loss": 0.6993111038208008, "training_acc": 53.0, "val_loss": 0.7001941680908204, "val_acc": 52.0}
{"epoch": 37, "training_loss": 0.7017146730422974, "training_acc": 53.0, "val_loss": 0.6925717258453369, "val_acc": 52.0}
{"epoch": 38, "training_loss": 0.7060782027244568, "training_acc": 47.0, "val_loss": 0.6923514246940613, "val_acc": 52.0}
{"epoch": 39, "training_loss": 0.6956504845619201, "training_acc": 45.0, "val_loss": 0.6979868865013122, "val_acc": 52.0}
{"epoch": 40, "training_loss": 0.694371087551117, "training_acc": 53.0, "val_loss": 0.6926285433769226, "val_acc": 52.0}
{"epoch": 41, "training_loss": 0.7086285638809204, "training_acc": 45.0, "val_loss": 0.6932539963722228, "val_acc": 52.0}
{"epoch": 42, "training_loss": 0.7088728761672973, "training_acc": 53.0, "val_loss": 0.6965984964370727, "val_acc": 52.0}
{"epoch": 43, "training_loss": 0.7006454205513001, "training_acc": 53.0, "val_loss": 0.6923469400405884, "val_acc": 52.0}
{"epoch": 44, "training_loss": 0.693815541267395, "training_acc": 51.0, "val_loss": 0.6970205044746399, "val_acc": 48.0}
{"epoch": 45, "training_loss": 0.7028781962394715, "training_acc": 37.0, "val_loss": 0.6923665356636047, "val_acc": 52.0}
{"epoch": 46, "training_loss": 0.6955454921722413, "training_acc": 53.0, "val_loss": 0.6938237881660462, "val_acc": 52.0}
