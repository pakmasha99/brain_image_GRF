"main_modify.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --learning_rate 1e-1 --batch_size 16 --weight_decay 1e-2 --BN inst --save_path finetune_results_InstanceBN --run_where lab"
{"epoch": 0, "training_loss": 10.310221290588379, "training_acc": 46.0, "val_loss": 2.88632550239563, "val_acc": 44.0}
{"epoch": 1, "training_loss": 7.29253436088562, "training_acc": 54.0, "val_loss": 5.40194242477417, "val_acc": 44.0}
{"epoch": 2, "training_loss": 6.428165664672852, "training_acc": 52.0, "val_loss": 5.996989841461182, "val_acc": 56.0}
{"epoch": 3, "training_loss": 3.6648068809509278, "training_acc": 42.0, "val_loss": 0.7116272497177124, "val_acc": 44.0}
{"epoch": 4, "training_loss": 2.44150053024292, "training_acc": 48.0, "val_loss": 0.6992202138900757, "val_acc": 44.0}
{"epoch": 5, "training_loss": 0.8629679298400879, "training_acc": 48.0, "val_loss": 0.9822414112091065, "val_acc": 56.0}
{"epoch": 6, "training_loss": 1.0292697048187256, "training_acc": 48.0, "val_loss": 1.8413274335861205, "val_acc": 44.0}
{"epoch": 7, "training_loss": 1.2119160270690919, "training_acc": 54.0, "val_loss": 2.053919610977173, "val_acc": 56.0}
{"epoch": 8, "training_loss": 1.4617708110809327, "training_acc": 52.0, "val_loss": 1.5286509609222412, "val_acc": 44.0}
{"epoch": 9, "training_loss": 0.9799140548706055, "training_acc": 50.0, "val_loss": 3.0846591567993165, "val_acc": 44.0}
{"epoch": 10, "training_loss": 1.8606610679626465, "training_acc": 52.0, "val_loss": 1.7375836563110352, "val_acc": 56.0}
{"epoch": 11, "training_loss": 1.5176904582977295, "training_acc": 42.0, "val_loss": 1.7794021272659302, "val_acc": 56.0}
{"epoch": 12, "training_loss": 2.110446882247925, "training_acc": 54.0, "val_loss": 1.559992685317993, "val_acc": 56.0}
{"epoch": 13, "training_loss": 1.7827987146377564, "training_acc": 46.0, "val_loss": 1.538254027366638, "val_acc": 44.0}
{"epoch": 14, "training_loss": 0.9408706259727478, "training_acc": 48.0, "val_loss": 1.015377221107483, "val_acc": 44.0}
{"epoch": 15, "training_loss": 0.793689136505127, "training_acc": 54.0, "val_loss": 1.702594337463379, "val_acc": 56.0}
{"epoch": 16, "training_loss": 1.0393397164344789, "training_acc": 56.0, "val_loss": 2.263607177734375, "val_acc": 44.0}
{"epoch": 17, "training_loss": 1.716923370361328, "training_acc": 46.0, "val_loss": 0.7081281590461731, "val_acc": 56.0}
{"epoch": 18, "training_loss": 0.9033482265472412, "training_acc": 48.0, "val_loss": 1.918430094718933, "val_acc": 44.0}
{"epoch": 19, "training_loss": 1.6192500114440918, "training_acc": 48.0, "val_loss": 2.0466601276397705, "val_acc": 56.0}
{"epoch": 20, "training_loss": 1.59060382604599, "training_acc": 52.0, "val_loss": 1.8617998123168946, "val_acc": 44.0}
{"epoch": 21, "training_loss": 1.4981593799591064, "training_acc": 46.0, "val_loss": 2.1667373752593995, "val_acc": 56.0}
{"epoch": 22, "training_loss": 1.3976363658905029, "training_acc": 46.0, "val_loss": 0.695644109249115, "val_acc": 44.0}
{"epoch": 23, "training_loss": 0.9438971042633056, "training_acc": 46.0, "val_loss": 2.2704207134246825, "val_acc": 56.0}
{"epoch": 24, "training_loss": 2.6905845856666564, "training_acc": 54.0, "val_loss": 1.5777780628204345, "val_acc": 56.0}
{"epoch": 25, "training_loss": 2.523809652328491, "training_acc": 52.0, "val_loss": 2.527347373962402, "val_acc": 56.0}
{"epoch": 26, "training_loss": 1.5448615264892578, "training_acc": 42.0, "val_loss": 1.6806033897399901, "val_acc": 56.0}
{"epoch": 27, "training_loss": 1.708077449798584, "training_acc": 58.0, "val_loss": 1.8221829223632813, "val_acc": 56.0}
{"epoch": 28, "training_loss": 1.5895372939109802, "training_acc": 58.0, "val_loss": 1.1458348083496093, "val_acc": 44.0}
{"epoch": 29, "training_loss": 1.3402954119443893, "training_acc": 48.0, "val_loss": 3.3528222465515136, "val_acc": 56.0}
{"epoch": 30, "training_loss": 2.1611181640625, "training_acc": 58.0, "val_loss": 2.4470538520812988, "val_acc": 56.0}
{"epoch": 31, "training_loss": 1.8043363041197882, "training_acc": 54.0, "val_loss": 3.7451949882507325, "val_acc": 56.0}
{"epoch": 32, "training_loss": 2.170467982292175, "training_acc": 52.0, "val_loss": 0.848697395324707, "val_acc": 56.0}
{"epoch": 33, "training_loss": 1.392542643547058, "training_acc": 54.0, "val_loss": 2.1821720886230467, "val_acc": 44.0}
{"epoch": 34, "training_loss": 2.7223250675201416, "training_acc": 42.0, "val_loss": 3.0513321113586427, "val_acc": 44.0}
{"epoch": 35, "training_loss": 2.046835641860962, "training_acc": 50.0, "val_loss": 0.7140020751953124, "val_acc": 56.0}
{"epoch": 36, "training_loss": 1.6116567516326905, "training_acc": 52.0, "val_loss": 3.677386484146118, "val_acc": 56.0}
{"epoch": 37, "training_loss": 2.787240619659424, "training_acc": 50.0, "val_loss": 4.586868076324463, "val_acc": 56.0}
{"epoch": 38, "training_loss": 4.149698944091797, "training_acc": 40.0, "val_loss": 1.8268612766265868, "val_acc": 56.0}
{"epoch": 39, "training_loss": 1.583336534500122, "training_acc": 48.0, "val_loss": 2.1925786972045898, "val_acc": 44.0}
{"epoch": 40, "training_loss": 1.1149194431304932, "training_acc": 60.0, "val_loss": 2.1057566571235657, "val_acc": 56.0}
{"epoch": 41, "training_loss": 1.933809232711792, "training_acc": 48.0, "val_loss": 1.3303706312179566, "val_acc": 44.0}
