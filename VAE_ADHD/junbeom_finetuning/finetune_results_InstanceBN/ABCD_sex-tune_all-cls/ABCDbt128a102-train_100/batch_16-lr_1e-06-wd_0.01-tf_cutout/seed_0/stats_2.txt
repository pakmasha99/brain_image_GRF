"main_modify.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --learning_rate 1e-6 --batch_size 16 --weight_decay 1e-2 --BN inst --save_path finetune_results_InstanceBN --run_where lab"
{"epoch": 0, "training_loss": 0.6902963209152222, "training_acc": 53.0, "val_loss": 0.6944794082641601, "val_acc": 52.0}
{"epoch": 1, "training_loss": 0.6916538333892822, "training_acc": 53.0, "val_loss": 0.6924237895011902, "val_acc": 52.0}
{"epoch": 2, "training_loss": 0.6915157318115235, "training_acc": 53.0, "val_loss": 0.691785318851471, "val_acc": 52.0}
{"epoch": 3, "training_loss": 0.6903267192840576, "training_acc": 53.0, "val_loss": 0.6916374588012695, "val_acc": 52.0}
{"epoch": 4, "training_loss": 0.6903949451446533, "training_acc": 53.0, "val_loss": 0.691854031085968, "val_acc": 52.0}
{"epoch": 5, "training_loss": 0.6907040619850159, "training_acc": 53.0, "val_loss": 0.6918625569343567, "val_acc": 52.0}
{"epoch": 6, "training_loss": 0.6901799154281616, "training_acc": 53.0, "val_loss": 0.6918451237678528, "val_acc": 52.0}
{"epoch": 7, "training_loss": 0.6899257516860962, "training_acc": 53.0, "val_loss": 0.6917160153388977, "val_acc": 52.0}
{"epoch": 8, "training_loss": 0.6892630767822265, "training_acc": 53.0, "val_loss": 0.6914207053184509, "val_acc": 52.0}
{"epoch": 9, "training_loss": 0.6897871136665344, "training_acc": 53.0, "val_loss": 0.6912302327156067, "val_acc": 52.0}
{"epoch": 10, "training_loss": 0.6894989776611328, "training_acc": 53.0, "val_loss": 0.6912469601631165, "val_acc": 52.0}
{"epoch": 11, "training_loss": 0.6888421773910522, "training_acc": 53.0, "val_loss": 0.6913107776641846, "val_acc": 52.0}
{"epoch": 12, "training_loss": 0.6890822696685791, "training_acc": 53.0, "val_loss": 0.6913406705856323, "val_acc": 52.0}
{"epoch": 13, "training_loss": 0.6890174269676208, "training_acc": 53.0, "val_loss": 0.6915044689178467, "val_acc": 52.0}
{"epoch": 14, "training_loss": 0.6889105677604676, "training_acc": 53.0, "val_loss": 0.6912950897216796, "val_acc": 52.0}
{"epoch": 15, "training_loss": 0.6887841582298279, "training_acc": 53.0, "val_loss": 0.6912241005897521, "val_acc": 52.0}
{"epoch": 16, "training_loss": 0.6886501479148864, "training_acc": 53.0, "val_loss": 0.6909782361984252, "val_acc": 52.0}
{"epoch": 17, "training_loss": 0.6882664966583252, "training_acc": 53.0, "val_loss": 0.6911196732521057, "val_acc": 52.0}
{"epoch": 18, "training_loss": 0.688467082977295, "training_acc": 53.0, "val_loss": 0.6916222310066223, "val_acc": 52.0}
{"epoch": 19, "training_loss": 0.6880351161956787, "training_acc": 53.0, "val_loss": 0.6917274665832519, "val_acc": 52.0}
{"epoch": 20, "training_loss": 0.6881180572509765, "training_acc": 53.0, "val_loss": 0.6917152452468872, "val_acc": 52.0}
{"epoch": 21, "training_loss": 0.6877719068527222, "training_acc": 53.0, "val_loss": 0.691180579662323, "val_acc": 52.0}
{"epoch": 22, "training_loss": 0.6874721956253051, "training_acc": 53.0, "val_loss": 0.6913344025611877, "val_acc": 52.0}
{"epoch": 23, "training_loss": 0.6875663065910339, "training_acc": 53.0, "val_loss": 0.6911603593826294, "val_acc": 52.0}
{"epoch": 24, "training_loss": 0.6873425769805909, "training_acc": 53.0, "val_loss": 0.691600410938263, "val_acc": 52.0}
{"epoch": 25, "training_loss": 0.6877107906341553, "training_acc": 53.0, "val_loss": 0.6917652010917663, "val_acc": 52.0}
{"epoch": 26, "training_loss": 0.6876375365257263, "training_acc": 53.0, "val_loss": 0.6915761685371399, "val_acc": 52.0}
{"epoch": 27, "training_loss": 0.6866664218902588, "training_acc": 53.0, "val_loss": 0.690850293636322, "val_acc": 52.0}
{"epoch": 28, "training_loss": 0.687018699645996, "training_acc": 53.0, "val_loss": 0.6915538001060486, "val_acc": 52.0}
{"epoch": 29, "training_loss": 0.6864600372314453, "training_acc": 53.0, "val_loss": 0.691471905708313, "val_acc": 52.0}
{"epoch": 30, "training_loss": 0.6864596843719483, "training_acc": 53.0, "val_loss": 0.6916661739349366, "val_acc": 52.0}
{"epoch": 31, "training_loss": 0.6858269834518432, "training_acc": 53.0, "val_loss": 0.6913865780830384, "val_acc": 52.0}
{"epoch": 32, "training_loss": 0.6865125322341918, "training_acc": 53.0, "val_loss": 0.691557776927948, "val_acc": 52.0}
{"epoch": 33, "training_loss": 0.6862294054031373, "training_acc": 53.0, "val_loss": 0.6916797852516174, "val_acc": 52.0}
{"epoch": 34, "training_loss": 0.6856424236297607, "training_acc": 53.0, "val_loss": 0.6915735363960266, "val_acc": 52.0}
{"epoch": 35, "training_loss": 0.6852183485031128, "training_acc": 53.0, "val_loss": 0.6917420887947082, "val_acc": 52.0}
{"epoch": 36, "training_loss": 0.6859568357467651, "training_acc": 53.0, "val_loss": 0.6919001984596252, "val_acc": 52.0}
{"epoch": 37, "training_loss": 0.685898084640503, "training_acc": 53.0, "val_loss": 0.6919605112075806, "val_acc": 52.0}
{"epoch": 38, "training_loss": 0.6857996892929077, "training_acc": 53.0, "val_loss": 0.691946096420288, "val_acc": 52.0}
{"epoch": 39, "training_loss": 0.6851837110519409, "training_acc": 53.0, "val_loss": 0.6917628741264343, "val_acc": 52.0}
{"epoch": 40, "training_loss": 0.6848624587059021, "training_acc": 53.0, "val_loss": 0.6913251662254334, "val_acc": 52.0}
{"epoch": 41, "training_loss": 0.684686074256897, "training_acc": 53.0, "val_loss": 0.6919205570220948, "val_acc": 52.0}
{"epoch": 42, "training_loss": 0.6850012636184692, "training_acc": 53.0, "val_loss": 0.6914666414260864, "val_acc": 52.0}
{"epoch": 43, "training_loss": 0.6846955609321594, "training_acc": 53.0, "val_loss": 0.6917179203033448, "val_acc": 52.0}
{"epoch": 44, "training_loss": 0.683993444442749, "training_acc": 53.0, "val_loss": 0.692059543132782, "val_acc": 52.0}
{"epoch": 45, "training_loss": 0.6841260552406311, "training_acc": 53.0, "val_loss": 0.6919150853157043, "val_acc": 52.0}
{"epoch": 46, "training_loss": 0.6837977933883667, "training_acc": 53.0, "val_loss": 0.6917840456962585, "val_acc": 52.0}
