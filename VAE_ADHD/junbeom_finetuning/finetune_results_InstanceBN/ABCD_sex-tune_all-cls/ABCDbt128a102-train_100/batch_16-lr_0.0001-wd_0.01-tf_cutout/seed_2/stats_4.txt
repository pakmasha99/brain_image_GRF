"main_modify.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 2 --task ABCD_sex --input_option BT_org --learning_rate 1e-4 --batch_size 16 --weight_decay 1e-2 --BN inst --save_path finetune_results_InstanceBN --run_where lab"
{"epoch": 0, "training_loss": 0.6930546164512634, "training_acc": 52.0, "val_loss": 0.688174409866333, "val_acc": 56.0}
{"epoch": 1, "training_loss": 0.6927683734893799, "training_acc": 52.0, "val_loss": 0.6887200093269348, "val_acc": 56.0}
{"epoch": 2, "training_loss": 0.6958354377746582, "training_acc": 52.0, "val_loss": 0.6894595265388489, "val_acc": 56.0}
{"epoch": 3, "training_loss": 0.6947026634216309, "training_acc": 52.0, "val_loss": 0.6921481895446777, "val_acc": 56.0}
{"epoch": 4, "training_loss": 0.6934815549850464, "training_acc": 54.0, "val_loss": 0.6933939695358277, "val_acc": 40.0}
{"epoch": 5, "training_loss": 0.6937464666366577, "training_acc": 51.0, "val_loss": 0.6924109673500061, "val_acc": 52.0}
{"epoch": 6, "training_loss": 0.6920855116844177, "training_acc": 51.0, "val_loss": 0.6953895974159241, "val_acc": 44.0}
{"epoch": 7, "training_loss": 0.6951101016998291, "training_acc": 49.0, "val_loss": 0.6906656384468078, "val_acc": 56.0}
{"epoch": 8, "training_loss": 0.6918708610534668, "training_acc": 52.0, "val_loss": 0.6931508350372314, "val_acc": 52.0}
{"epoch": 9, "training_loss": 0.6927293491363525, "training_acc": 54.0, "val_loss": 0.6930937957763672, "val_acc": 52.0}
{"epoch": 10, "training_loss": 0.6945858597755432, "training_acc": 51.0, "val_loss": 0.6886840724945068, "val_acc": 56.0}
{"epoch": 11, "training_loss": 0.6931026005744934, "training_acc": 52.0, "val_loss": 0.6911568832397461, "val_acc": 56.0}
{"epoch": 12, "training_loss": 0.6916117000579834, "training_acc": 51.0, "val_loss": 0.693510217666626, "val_acc": 52.0}
{"epoch": 13, "training_loss": 0.6916694831848145, "training_acc": 54.0, "val_loss": 0.6876532816886902, "val_acc": 56.0}
{"epoch": 14, "training_loss": 0.6932310485839843, "training_acc": 52.0, "val_loss": 0.6902390599250794, "val_acc": 56.0}
{"epoch": 15, "training_loss": 0.6915749454498291, "training_acc": 52.0, "val_loss": 0.692229130268097, "val_acc": 56.0}
{"epoch": 16, "training_loss": 0.691666705608368, "training_acc": 52.0, "val_loss": 0.6884911489486695, "val_acc": 56.0}
{"epoch": 17, "training_loss": 0.6905373191833496, "training_acc": 53.0, "val_loss": 0.6947066855430603, "val_acc": 44.0}
{"epoch": 18, "training_loss": 0.6977318024635315, "training_acc": 41.0, "val_loss": 0.6896486687660217, "val_acc": 56.0}
{"epoch": 19, "training_loss": 0.6930798721313477, "training_acc": 52.0, "val_loss": 0.6881852388381958, "val_acc": 56.0}
{"epoch": 20, "training_loss": 0.6964953851699829, "training_acc": 52.0, "val_loss": 0.6882016205787659, "val_acc": 56.0}
{"epoch": 21, "training_loss": 0.6941513657569885, "training_acc": 52.0, "val_loss": 0.6907953310012818, "val_acc": 56.0}
{"epoch": 22, "training_loss": 0.6993107795715332, "training_acc": 44.0, "val_loss": 0.6876699852943421, "val_acc": 56.0}
{"epoch": 23, "training_loss": 0.6932924509048461, "training_acc": 52.0, "val_loss": 0.6872180867195129, "val_acc": 56.0}
{"epoch": 24, "training_loss": 0.6940323686599732, "training_acc": 52.0, "val_loss": 0.6861506009101868, "val_acc": 56.0}
{"epoch": 25, "training_loss": 0.6968921232223511, "training_acc": 52.0, "val_loss": 0.6866200470924377, "val_acc": 56.0}
{"epoch": 26, "training_loss": 0.6940080690383911, "training_acc": 52.0, "val_loss": 0.6863281631469726, "val_acc": 56.0}
{"epoch": 27, "training_loss": 0.6949985790252685, "training_acc": 52.0, "val_loss": 0.6864386296272278, "val_acc": 56.0}
{"epoch": 28, "training_loss": 0.695285906791687, "training_acc": 52.0, "val_loss": 0.6870024466514587, "val_acc": 56.0}
{"epoch": 29, "training_loss": 0.6942429256439209, "training_acc": 52.0, "val_loss": 0.6875131249427795, "val_acc": 56.0}
{"epoch": 30, "training_loss": 0.6939696931838989, "training_acc": 52.0, "val_loss": 0.6878872299194336, "val_acc": 56.0}
{"epoch": 31, "training_loss": 0.6900050163269043, "training_acc": 52.0, "val_loss": 0.688996410369873, "val_acc": 56.0}
{"epoch": 32, "training_loss": 0.6940081071853638, "training_acc": 52.0, "val_loss": 0.6873028707504273, "val_acc": 56.0}
{"epoch": 33, "training_loss": 0.6943828654289246, "training_acc": 52.0, "val_loss": 0.6867627000808716, "val_acc": 56.0}
{"epoch": 34, "training_loss": 0.6920854020118713, "training_acc": 52.0, "val_loss": 0.6908289813995361, "val_acc": 56.0}
{"epoch": 35, "training_loss": 0.6944041585922242, "training_acc": 50.0, "val_loss": 0.6932426476478577, "val_acc": 52.0}
{"epoch": 36, "training_loss": 0.6934249067306518, "training_acc": 53.0, "val_loss": 0.6877599310874939, "val_acc": 56.0}
{"epoch": 37, "training_loss": 0.6907287335395813, "training_acc": 52.0, "val_loss": 0.6917065525054932, "val_acc": 56.0}
{"epoch": 38, "training_loss": 0.6917837333679199, "training_acc": 53.0, "val_loss": 0.6939329934120179, "val_acc": 52.0}
{"epoch": 39, "training_loss": 0.6927063274383545, "training_acc": 55.0, "val_loss": 0.6893775868415832, "val_acc": 56.0}
{"epoch": 40, "training_loss": 0.6897374200820923, "training_acc": 57.0, "val_loss": 0.6981670665740967, "val_acc": 44.0}
{"epoch": 41, "training_loss": 0.6907705402374268, "training_acc": 55.0, "val_loss": 0.6870087051391601, "val_acc": 56.0}
{"epoch": 42, "training_loss": 0.6923863339424133, "training_acc": 52.0, "val_loss": 0.6870311427116395, "val_acc": 56.0}
{"epoch": 43, "training_loss": 0.6888689875602723, "training_acc": 52.0, "val_loss": 0.6955134701728821, "val_acc": 52.0}
