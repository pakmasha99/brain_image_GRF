"main_optuna_fix_3.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKBsim64a.pth --mode finetuning --train_num 80 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_ABCD_fix_3_early_loss --binary_class True --run_where lab --early_criteria loss"
{"epoch": 0, "training_loss": 99.08263397216797, "training_acc": 56.25, "val_loss": 251614720000.0, "val_acc": 55.0, "val_auroc": 0.444, "time": 20.94}
{"epoch": 1, "training_loss": 912475967649.0, "training_acc": 46.25, "val_loss": 142152670.0, "val_acc": 45.0, "val_auroc": 0.444, "time": 45.75}
{"epoch": 2, "training_loss": 434741474.0, "training_acc": 51.25, "val_loss": 5353529.375, "val_acc": 45.0, "val_auroc": 0.455, "time": 70.6}
{"epoch": 3, "training_loss": 16399187.75, "training_acc": 48.75, "val_loss": 302092.94921875, "val_acc": 45.0, "val_auroc": 0.424, "time": 94.59}
{"epoch": 4, "training_loss": 967005.1796875, "training_acc": 53.75, "val_loss": 1210749.53125, "val_acc": 45.0, "val_auroc": 0.465, "time": 112.75}
{"epoch": 5, "training_loss": 4746520.25, "training_acc": 51.25, "val_loss": 566890.1171875, "val_acc": 55.0, "val_auroc": 0.444, "time": 133.8}
{"epoch": 6, "training_loss": 2134739.90625, "training_acc": 51.25, "val_loss": 93366.552734375, "val_acc": 45.0, "val_auroc": 0.404, "time": 155.48}
{"epoch": 7, "training_loss": 389846.46875, "training_acc": 48.75, "val_loss": 278861.69921875, "val_acc": 45.0, "val_auroc": 0.566, "time": 175.98}
{"epoch": 8, "training_loss": 934960.828125, "training_acc": 43.75, "val_loss": 113314.08203125, "val_acc": 45.0, "val_auroc": 0.485, "time": 194.9}
{"epoch": 9, "training_loss": 383418.5, "training_acc": 48.75, "val_loss": 12920.721435546875, "val_acc": 55.0, "val_auroc": 0.515, "time": 215.15}
{"epoch": 10, "training_loss": 171291.75, "training_acc": 46.25, "val_loss": 68709.501953125, "val_acc": 45.0, "val_auroc": 0.636, "time": 235.76}
{"epoch": 11, "training_loss": 185377.05783081055, "training_acc": 50.0, "val_loss": 110509.052734375, "val_acc": 55.0, "val_auroc": 0.556, "time": 259.99}
{"epoch": 12, "training_loss": 436122.078125, "training_acc": 51.25, "val_loss": 39180.4150390625, "val_acc": 55.0, "val_auroc": 0.576, "time": 277.6}
{"epoch": 13, "training_loss": 145652.755859375, "training_acc": 51.25, "val_loss": 4858.551940917969, "val_acc": 45.0, "val_auroc": 0.414, "time": 299.18}
{"epoch": 14, "training_loss": 17354.600830078125, "training_acc": 48.75, "val_loss": 3306.62841796875, "val_acc": 55.0, "val_auroc": 0.424, "time": 319.79}
{"epoch": 15, "training_loss": 17530.5048828125, "training_acc": 43.75, "val_loss": 5621.0467529296875, "val_acc": 45.0, "val_auroc": 0.384, "time": 343.92}
{"epoch": 16, "training_loss": 14591.46752166748, "training_acc": 51.25, "val_loss": 9223.931884765625, "val_acc": 55.0, "val_auroc": 0.485, "time": 362.47}
{"epoch": 17, "training_loss": 37682.560546875, "training_acc": 51.25, "val_loss": 354.7125244140625, "val_acc": 55.0, "val_auroc": 0.455, "time": 383.82}
{"epoch": 18, "training_loss": 7200.780517578125, "training_acc": 53.75, "val_loss": 8582.902221679688, "val_acc": 45.0, "val_auroc": 0.677, "time": 404.59}
{"epoch": 19, "training_loss": 28126.341796875, "training_acc": 48.75, "val_loss": 5007.547912597656, "val_acc": 55.0, "val_auroc": 0.545, "time": 425.73}
{"epoch": 20, "training_loss": 19003.303100585938, "training_acc": 51.25, "val_loss": 3487.7154541015625, "val_acc": 45.0, "val_auroc": 0.525, "time": 443.09}
{"epoch": 21, "training_loss": 10264.840637207031, "training_acc": 48.75, "val_loss": 5964.322509765625, "val_acc": 55.0, "val_auroc": 0.475, "time": 462.64}
{"epoch": 22, "training_loss": 23527.669921875, "training_acc": 51.25, "val_loss": 666.9618988037109, "val_acc": 45.0, "val_auroc": 0.515, "time": 483.11}
{"epoch": 23, "training_loss": 2669.034423828125, "training_acc": 48.75, "val_loss": 627.7190017700195, "val_acc": 55.0, "val_auroc": 0.505, "time": 504.34}
{"epoch": 24, "training_loss": 2407.408447265625, "training_acc": 51.25, "val_loss": 1014.1158294677734, "val_acc": 45.0, "val_auroc": 0.485, "time": 521.71}
{"epoch": 25, "training_loss": 3128.2440185546875, "training_acc": 48.75, "val_loss": 1930.6991577148438, "val_acc": 55.0, "val_auroc": 0.525, "time": 544.57}
{"epoch": 26, "training_loss": 7786.2081298828125, "training_acc": 51.25, "val_loss": 627.746467590332, "val_acc": 55.0, "val_auroc": 0.545, "time": 565.63}
{"epoch": 27, "training_loss": 2676.0394287109375, "training_acc": 48.75, "val_loss": 488.9408493041992, "val_acc": 45.0, "val_auroc": 0.556, "time": 586.4}
{"epoch": 28, "training_loss": 1859.3952331542969, "training_acc": 41.25, "val_loss": 283.0784606933594, "val_acc": 45.0, "val_auroc": 0.515, "time": 604.49}
{"epoch": 29, "training_loss": 898.8460693359375, "training_acc": 51.25, "val_loss": 43.39231014251709, "val_acc": 50.0, "val_auroc": 0.475, "time": 625.16}
{"epoch": 30, "training_loss": 151.0991973876953, "training_acc": 56.25, "val_loss": 276.5539360046387, "val_acc": 45.0, "val_auroc": 0.465, "time": 645.81}
{"epoch": 31, "training_loss": 1013.2363891601562, "training_acc": 43.75, "val_loss": 30.897815227508545, "val_acc": 55.0, "val_auroc": 0.495, "time": 668.31}
{"epoch": 32, "training_loss": 141.5050506591797, "training_acc": 52.5, "val_loss": 115.27621269226074, "val_acc": 55.0, "val_auroc": 0.505, "time": 687.75}
{"epoch": 33, "training_loss": 430.3214454650879, "training_acc": 48.75, "val_loss": 110.0284194946289, "val_acc": 55.0, "val_auroc": 0.505, "time": 707.9}
{"epoch": 34, "training_loss": 381.10493087768555, "training_acc": 51.25, "val_loss": 142.15048789978027, "val_acc": 45.0, "val_auroc": 0.505, "time": 728.58}
{"epoch": 35, "training_loss": 459.6157760620117, "training_acc": 47.5, "val_loss": 37.122440338134766, "val_acc": 40.0, "val_auroc": 0.495, "time": 748.42}
{"epoch": 36, "training_loss": 108.80498504638672, "training_acc": 52.5, "val_loss": 39.09869432449341, "val_acc": 40.0, "val_auroc": 0.444, "time": 765.78}
{"epoch": 37, "training_loss": 110.73995208740234, "training_acc": 48.75, "val_loss": 27.755067348480225, "val_acc": 40.0, "val_auroc": 0.455, "time": 786.12}
{"epoch": 38, "training_loss": 83.88495254516602, "training_acc": 61.25, "val_loss": 20.775928497314453, "val_acc": 50.0, "val_auroc": 0.434, "time": 806.84}
{"epoch": 39, "training_loss": 91.6249828338623, "training_acc": 51.25, "val_loss": 41.32845401763916, "val_acc": 45.0, "val_auroc": 0.444, "time": 825.23}
{"epoch": 40, "training_loss": 135.3518466949463, "training_acc": 41.25, "val_loss": 30.83193063735962, "val_acc": 40.0, "val_auroc": 0.444, "time": 842.89}
{"epoch": 41, "training_loss": 101.60610771179199, "training_acc": 53.75, "val_loss": 55.7456636428833, "val_acc": 45.0, "val_auroc": 0.475, "time": 863.85}
{"epoch": 42, "training_loss": 177.74663543701172, "training_acc": 51.25, "val_loss": 17.138484716415405, "val_acc": 50.0, "val_auroc": 0.465, "time": 884.3}
{"epoch": 43, "training_loss": 68.51027297973633, "training_acc": 52.5, "val_loss": 16.72307252883911, "val_acc": 55.0, "val_auroc": 0.475, "time": 904.06}
{"epoch": 44, "training_loss": 73.84300804138184, "training_acc": 56.25, "val_loss": 58.46980094909668, "val_acc": 45.0, "val_auroc": 0.535, "time": 922.17}
{"epoch": 45, "training_loss": 171.75007724761963, "training_acc": 48.75, "val_loss": 28.68319034576416, "val_acc": 55.0, "val_auroc": 0.545, "time": 942.58}
{"epoch": 46, "training_loss": 115.71883392333984, "training_acc": 52.5, "val_loss": 15.797462463378906, "val_acc": 50.0, "val_auroc": 0.515, "time": 963.12}
{"epoch": 47, "training_loss": 64.99944877624512, "training_acc": 61.25, "val_loss": 39.66777563095093, "val_acc": 45.0, "val_auroc": 0.455, "time": 982.37}
{"epoch": 48, "training_loss": 114.11800575256348, "training_acc": 52.5, "val_loss": 61.07372760772705, "val_acc": 55.0, "val_auroc": 0.485, "time": 999.91}
{"epoch": 49, "training_loss": 200.8325653076172, "training_acc": 51.25, "val_loss": 103.26664924621582, "val_acc": 45.0, "val_auroc": 0.465, "time": 1019.92}
{"epoch": 50, "training_loss": 326.77882385253906, "training_acc": 48.75, "val_loss": 40.794291496276855, "val_acc": 55.0, "val_auroc": 0.455, "time": 1043.45}
{"epoch": 51, "training_loss": 175.42481231689453, "training_acc": 52.5, "val_loss": 158.21619033813477, "val_acc": 55.0, "val_auroc": 0.434, "time": 1062.65}
{"epoch": 52, "training_loss": 662.95654296875, "training_acc": 51.25, "val_loss": 56.069321632385254, "val_acc": 55.0, "val_auroc": 0.525, "time": 1080.42}
{"epoch": 53, "training_loss": 291.05401611328125, "training_acc": 53.75, "val_loss": 252.27067947387695, "val_acc": 45.0, "val_auroc": 0.465, "time": 1099.75}
{"epoch": 54, "training_loss": 789.8283843994141, "training_acc": 48.75, "val_loss": 93.04375648498535, "val_acc": 55.0, "val_auroc": 0.535, "time": 1118.6}
{"epoch": 55, "training_loss": 378.12099838256836, "training_acc": 51.25, "val_loss": 20.832180976867676, "val_acc": 55.0, "val_auroc": 0.424, "time": 1136.49}
{"epoch": 56, "training_loss": 174.03021240234375, "training_acc": 52.5, "val_loss": 29.15226936340332, "val_acc": 35.0, "val_auroc": 0.485, "time": 1154.63}
{"epoch": 57, "training_loss": 184.43387603759766, "training_acc": 43.75, "val_loss": 106.14562034606934, "val_acc": 55.0, "val_auroc": 0.434, "time": 1173.4}
{"epoch": 58, "training_loss": 387.384072303772, "training_acc": 55.0, "val_loss": 131.73972129821777, "val_acc": 45.0, "val_auroc": 0.424, "time": 1191.59}
{"epoch": 59, "training_loss": 373.5597324371338, "training_acc": 50.0, "val_loss": 64.84475135803223, "val_acc": 55.0, "val_auroc": 0.434, "time": 1210.26}
{"epoch": 60, "training_loss": 271.5268440246582, "training_acc": 51.25, "val_loss": 68.60696315765381, "val_acc": 45.0, "val_auroc": 0.404, "time": 1227.46}
{"epoch": 61, "training_loss": 246.35906219482422, "training_acc": 48.75, "val_loss": 20.419952869415283, "val_acc": 50.0, "val_auroc": 0.394, "time": 1246.82}
{"epoch": 62, "training_loss": 102.28711700439453, "training_acc": 61.25, "val_loss": 18.134530782699585, "val_acc": 50.0, "val_auroc": 0.444, "time": 1264.86}
{"epoch": 63, "training_loss": 120.19332885742188, "training_acc": 56.25, "val_loss": 44.22473907470703, "val_acc": 45.0, "val_auroc": 0.424, "time": 1283.33}
{"epoch": 64, "training_loss": 207.9947509765625, "training_acc": 48.75, "val_loss": 97.33392715454102, "val_acc": 55.0, "val_auroc": 0.475, "time": 1301.65}
{"epoch": 65, "training_loss": 330.55653381347656, "training_acc": 51.25, "val_loss": 115.60511589050293, "val_acc": 45.0, "val_auroc": 0.444, "time": 1322.49}
