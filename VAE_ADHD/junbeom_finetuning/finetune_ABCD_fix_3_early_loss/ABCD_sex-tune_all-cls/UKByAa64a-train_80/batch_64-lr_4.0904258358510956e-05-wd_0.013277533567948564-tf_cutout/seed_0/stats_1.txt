"main_optuna.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 80 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_ABCD_fix_3_early_loss --binary_class True --run_where lab --early_criteria loss"
{"epoch": 0, "training_loss": 55.49070358276367, "training_acc": 52.5, "val_loss": 13.912546634674072, "val_acc": 50.0, "val_auroc": 0.5, "time": 19.3}
{"epoch": 1, "training_loss": 55.24281120300293, "training_acc": 52.5, "val_loss": 13.880316019058228, "val_acc": 50.0, "val_auroc": 0.54, "time": 37.66}
{"epoch": 2, "training_loss": 55.21824645996094, "training_acc": 52.5, "val_loss": 13.872946500778198, "val_acc": 50.0, "val_auroc": 0.61, "time": 62.43}
{"epoch": 3, "training_loss": 55.092453956604004, "training_acc": 52.5, "val_loss": 13.86344313621521, "val_acc": 50.0, "val_auroc": 0.64, "time": 86.74}
{"epoch": 4, "training_loss": 55.23604965209961, "training_acc": 52.5, "val_loss": 13.883498907089233, "val_acc": 50.0, "val_auroc": 0.52, "time": 105.33}
{"epoch": 5, "training_loss": 55.10155010223389, "training_acc": 52.5, "val_loss": 13.870781660079956, "val_acc": 50.0, "val_auroc": 0.54, "time": 124.26}
{"epoch": 6, "training_loss": 54.925865173339844, "training_acc": 52.5, "val_loss": 13.870949745178223, "val_acc": 50.0, "val_auroc": 0.48, "time": 142.3}
{"epoch": 7, "training_loss": 54.99225997924805, "training_acc": 52.5, "val_loss": 13.852251768112183, "val_acc": 50.0, "val_auroc": 0.56, "time": 160.65}
{"epoch": 8, "training_loss": 54.90375900268555, "training_acc": 52.5, "val_loss": 13.840490579605103, "val_acc": 50.0, "val_auroc": 0.58, "time": 179.89}
{"epoch": 9, "training_loss": 54.688599586486816, "training_acc": 52.5, "val_loss": 13.811982870101929, "val_acc": 50.0, "val_auroc": 0.61, "time": 197.34}
{"epoch": 10, "training_loss": 54.73173904418945, "training_acc": 55.0, "val_loss": 13.790339231491089, "val_acc": 50.0, "val_auroc": 0.71, "time": 216.65}
{"epoch": 11, "training_loss": 54.60392761230469, "training_acc": 52.5, "val_loss": 13.802305459976196, "val_acc": 50.0, "val_auroc": 0.69, "time": 235.08}
{"epoch": 12, "training_loss": 54.474491119384766, "training_acc": 52.5, "val_loss": 13.791006803512573, "val_acc": 50.0, "val_auroc": 0.62, "time": 253.35}
{"epoch": 13, "training_loss": 54.3702392578125, "training_acc": 56.25, "val_loss": 13.762985467910767, "val_acc": 50.0, "val_auroc": 0.67, "time": 270.53}
{"epoch": 14, "training_loss": 53.90619945526123, "training_acc": 53.75, "val_loss": 13.777759075164795, "val_acc": 50.0, "val_auroc": 0.75, "time": 287.62}
{"epoch": 15, "training_loss": 54.03679180145264, "training_acc": 53.75, "val_loss": 13.75483512878418, "val_acc": 50.0, "val_auroc": 0.67, "time": 304.69}
{"epoch": 16, "training_loss": 53.62939453125, "training_acc": 56.25, "val_loss": 13.72410774230957, "val_acc": 50.0, "val_auroc": 0.71, "time": 322.2}
{"epoch": 17, "training_loss": 53.12766742706299, "training_acc": 55.0, "val_loss": 13.659690618515015, "val_acc": 50.0, "val_auroc": 0.68, "time": 341.39}
{"epoch": 18, "training_loss": 53.104249000549316, "training_acc": 76.25, "val_loss": 13.608454465866089, "val_acc": 50.0, "val_auroc": 0.75, "time": 359.49}
{"epoch": 19, "training_loss": 52.51750564575195, "training_acc": 56.25, "val_loss": 13.479678630828857, "val_acc": 50.0, "val_auroc": 0.74, "time": 378.15}
{"epoch": 20, "training_loss": 51.85820770263672, "training_acc": 78.75, "val_loss": 13.336622714996338, "val_acc": 50.0, "val_auroc": 0.78, "time": 395.01}
{"epoch": 21, "training_loss": 50.876779556274414, "training_acc": 72.5, "val_loss": 13.265875577926636, "val_acc": 50.0, "val_auroc": 0.78, "time": 413.11}
{"epoch": 22, "training_loss": 50.578142166137695, "training_acc": 76.25, "val_loss": 13.055356740951538, "val_acc": 50.0, "val_auroc": 0.8, "time": 431.0}
{"epoch": 23, "training_loss": 48.911261558532715, "training_acc": 82.5, "val_loss": 13.69699239730835, "val_acc": 50.0, "val_auroc": 0.87, "time": 447.82}
{"epoch": 24, "training_loss": 52.47551441192627, "training_acc": 53.75, "val_loss": 13.214864730834961, "val_acc": 50.0, "val_auroc": 0.81, "time": 465.2}
{"epoch": 25, "training_loss": 49.333208084106445, "training_acc": 86.25, "val_loss": 12.990728616714478, "val_acc": 50.0, "val_auroc": 0.81, "time": 482.59}
{"epoch": 26, "training_loss": 48.253440856933594, "training_acc": 70.0, "val_loss": 13.415685892105103, "val_acc": 50.0, "val_auroc": 0.79, "time": 501.21}
{"epoch": 27, "training_loss": 47.98363494873047, "training_acc": 81.25, "val_loss": 13.479493856430054, "val_acc": 50.0, "val_auroc": 0.81, "time": 518.52}
{"epoch": 28, "training_loss": 49.49126625061035, "training_acc": 60.0, "val_loss": 14.469082355499268, "val_acc": 50.0, "val_auroc": 0.25, "time": 535.02}
{"epoch": 29, "training_loss": 58.25676155090332, "training_acc": 47.5, "val_loss": 13.937772512435913, "val_acc": 50.0, "val_auroc": 0.27, "time": 552.54}
{"epoch": 30, "training_loss": 56.06565284729004, "training_acc": 35.0, "val_loss": 13.839374780654907, "val_acc": 50.0, "val_auroc": 0.73, "time": 569.46}
{"epoch": 31, "training_loss": 55.30874443054199, "training_acc": 52.5, "val_loss": 13.85545015335083, "val_acc": 50.0, "val_auroc": 0.76, "time": 586.12}
{"epoch": 32, "training_loss": 55.229421615600586, "training_acc": 52.5, "val_loss": 13.85584831237793, "val_acc": 50.0, "val_auroc": 0.71, "time": 602.99}
{"epoch": 33, "training_loss": 55.24139595031738, "training_acc": 52.5, "val_loss": 13.829164505004883, "val_acc": 50.0, "val_auroc": 0.85, "time": 620.13}
{"epoch": 34, "training_loss": 55.18399620056152, "training_acc": 52.5, "val_loss": 13.837969303131104, "val_acc": 50.0, "val_auroc": 0.79, "time": 637.15}
{"epoch": 35, "training_loss": 55.34361457824707, "training_acc": 52.5, "val_loss": 13.84625792503357, "val_acc": 50.0, "val_auroc": 0.75, "time": 654.58}
{"epoch": 36, "training_loss": 55.08919429779053, "training_acc": 52.5, "val_loss": 13.820542097091675, "val_acc": 50.0, "val_auroc": 0.78, "time": 670.95}
{"epoch": 37, "training_loss": 55.12871742248535, "training_acc": 70.0, "val_loss": 13.828282356262207, "val_acc": 50.0, "val_auroc": 0.87, "time": 688.82}
{"epoch": 38, "training_loss": 55.56017875671387, "training_acc": 47.5, "val_loss": 13.8429594039917, "val_acc": 50.0, "val_auroc": 0.86, "time": 705.55}
{"epoch": 39, "training_loss": 55.60444164276123, "training_acc": 47.5, "val_loss": 13.823184967041016, "val_acc": 50.0, "val_auroc": 0.82, "time": 722.07}
{"epoch": 40, "training_loss": 55.471832275390625, "training_acc": 48.75, "val_loss": 13.81473183631897, "val_acc": 50.0, "val_auroc": 0.78, "time": 740.42}
{"epoch": 41, "training_loss": 55.310537338256836, "training_acc": 58.75, "val_loss": 13.81965160369873, "val_acc": 50.0, "val_auroc": 0.79, "time": 757.32}
{"epoch": 42, "training_loss": 55.21394157409668, "training_acc": 52.5, "val_loss": 13.818750381469727, "val_acc": 50.0, "val_auroc": 0.82, "time": 773.81}
{"epoch": 43, "training_loss": 55.10248947143555, "training_acc": 52.5, "val_loss": 13.826823234558105, "val_acc": 50.0, "val_auroc": 0.79, "time": 791.09}
{"epoch": 44, "training_loss": 55.087745666503906, "training_acc": 52.5, "val_loss": 13.83791208267212, "val_acc": 50.0, "val_auroc": 0.78, "time": 808.47}
