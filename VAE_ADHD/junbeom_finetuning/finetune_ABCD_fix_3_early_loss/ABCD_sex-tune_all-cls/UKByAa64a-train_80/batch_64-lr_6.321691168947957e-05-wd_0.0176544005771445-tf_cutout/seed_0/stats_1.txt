"main_optuna.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 80 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_ABCD_fix_3_early_loss --binary_class True --run_where lab --early_criteria loss"
{"epoch": 0, "training_loss": 55.53149223327637, "training_acc": 52.5, "val_loss": 13.89757513999939, "val_acc": 50.0, "val_auroc": 0.51, "time": 19.09}
{"epoch": 1, "training_loss": 55.239073753356934, "training_acc": 52.5, "val_loss": 13.881343603134155, "val_acc": 50.0, "val_auroc": 0.5, "time": 37.44}
{"epoch": 2, "training_loss": 55.382978439331055, "training_acc": 52.5, "val_loss": 13.919143676757812, "val_acc": 50.0, "val_auroc": 0.31, "time": 55.33}
{"epoch": 3, "training_loss": 55.41391181945801, "training_acc": 52.5, "val_loss": 13.881542682647705, "val_acc": 50.0, "val_auroc": 0.5, "time": 73.5}
{"epoch": 4, "training_loss": 55.31071853637695, "training_acc": 52.5, "val_loss": 13.876047134399414, "val_acc": 50.0, "val_auroc": 0.43, "time": 92.6}
{"epoch": 5, "training_loss": 55.172603607177734, "training_acc": 52.5, "val_loss": 13.874444961547852, "val_acc": 50.0, "val_auroc": 0.63, "time": 111.26}
{"epoch": 6, "training_loss": 55.216156005859375, "training_acc": 52.5, "val_loss": 13.836030960083008, "val_acc": 50.0, "val_auroc": 0.69, "time": 129.98}
{"epoch": 7, "training_loss": 55.103355407714844, "training_acc": 52.5, "val_loss": 13.82388949394226, "val_acc": 50.0, "val_auroc": 0.65, "time": 149.36}
{"epoch": 8, "training_loss": 55.05164909362793, "training_acc": 52.5, "val_loss": 13.817434310913086, "val_acc": 50.0, "val_auroc": 0.69, "time": 168.67}
{"epoch": 9, "training_loss": 55.00252819061279, "training_acc": 52.5, "val_loss": 13.776915073394775, "val_acc": 50.0, "val_auroc": 0.73, "time": 186.97}
{"epoch": 10, "training_loss": 55.14212703704834, "training_acc": 62.5, "val_loss": 13.755197525024414, "val_acc": 50.0, "val_auroc": 0.77, "time": 204.26}
{"epoch": 11, "training_loss": 54.81251335144043, "training_acc": 56.25, "val_loss": 13.855656385421753, "val_acc": 50.0, "val_auroc": 0.8, "time": 221.62}
{"epoch": 12, "training_loss": 55.03148365020752, "training_acc": 52.5, "val_loss": 13.796395063400269, "val_acc": 50.0, "val_auroc": 0.82, "time": 240.07}
{"epoch": 13, "training_loss": 54.84676551818848, "training_acc": 52.5, "val_loss": 13.760251998901367, "val_acc": 50.0, "val_auroc": 0.84, "time": 258.83}
{"epoch": 14, "training_loss": 54.64953804016113, "training_acc": 52.5, "val_loss": 13.934301137924194, "val_acc": 50.0, "val_auroc": 0.83, "time": 275.82}
{"epoch": 15, "training_loss": 55.19546127319336, "training_acc": 52.5, "val_loss": 14.019176959991455, "val_acc": 50.0, "val_auroc": 0.79, "time": 293.49}
{"epoch": 16, "training_loss": 55.240671157836914, "training_acc": 52.5, "val_loss": 13.786764144897461, "val_acc": 50.0, "val_auroc": 0.83, "time": 311.12}
{"epoch": 17, "training_loss": 54.34550857543945, "training_acc": 52.5, "val_loss": 13.748029470443726, "val_acc": 50.0, "val_auroc": 0.84, "time": 328.78}
{"epoch": 18, "training_loss": 54.66212463378906, "training_acc": 52.5, "val_loss": 13.674269914627075, "val_acc": 50.0, "val_auroc": 0.85, "time": 345.37}
{"epoch": 19, "training_loss": 54.73808288574219, "training_acc": 71.25, "val_loss": 13.57643723487854, "val_acc": 50.0, "val_auroc": 0.86, "time": 362.94}
{"epoch": 20, "training_loss": 53.8900089263916, "training_acc": 61.25, "val_loss": 13.49074125289917, "val_acc": 50.0, "val_auroc": 0.88, "time": 382.31}
{"epoch": 21, "training_loss": 53.35319995880127, "training_acc": 78.75, "val_loss": 13.792108297348022, "val_acc": 50.0, "val_auroc": 0.88, "time": 401.52}
{"epoch": 22, "training_loss": 54.45485019683838, "training_acc": 52.5, "val_loss": 13.444398641586304, "val_acc": 50.0, "val_auroc": 0.88, "time": 418.96}
{"epoch": 23, "training_loss": 53.67481708526611, "training_acc": 47.5, "val_loss": 13.39682936668396, "val_acc": 50.0, "val_auroc": 0.89, "time": 437.38}
{"epoch": 24, "training_loss": 52.625067710876465, "training_acc": 75.0, "val_loss": 13.68004560470581, "val_acc": 50.0, "val_auroc": 0.88, "time": 455.22}
{"epoch": 25, "training_loss": 53.57251167297363, "training_acc": 52.5, "val_loss": 13.281255960464478, "val_acc": 50.0, "val_auroc": 0.9, "time": 473.31}
{"epoch": 26, "training_loss": 52.57675266265869, "training_acc": 76.25, "val_loss": 13.085541725158691, "val_acc": 50.0, "val_auroc": 0.9, "time": 490.19}
{"epoch": 27, "training_loss": 50.88504409790039, "training_acc": 60.0, "val_loss": 12.83537745475769, "val_acc": 50.0, "val_auroc": 0.92, "time": 507.86}
{"epoch": 28, "training_loss": 49.98401069641113, "training_acc": 81.25, "val_loss": 14.147982597351074, "val_acc": 50.0, "val_auroc": 0.35, "time": 527.31}
{"epoch": 29, "training_loss": 56.24616241455078, "training_acc": 47.5, "val_loss": 13.921672105789185, "val_acc": 50.0, "val_auroc": 0.65, "time": 544.26}
{"epoch": 30, "training_loss": 55.77483367919922, "training_acc": 47.5, "val_loss": 13.838071823120117, "val_acc": 50.0, "val_auroc": 0.74, "time": 561.15}
{"epoch": 31, "training_loss": 55.15527534484863, "training_acc": 52.5, "val_loss": 13.93775224685669, "val_acc": 50.0, "val_auroc": 0.76, "time": 579.29}
{"epoch": 32, "training_loss": 55.41730308532715, "training_acc": 52.5, "val_loss": 13.939727544784546, "val_acc": 50.0, "val_auroc": 0.7, "time": 595.81}
{"epoch": 33, "training_loss": 55.310848236083984, "training_acc": 52.5, "val_loss": 13.869212865829468, "val_acc": 50.0, "val_auroc": 0.69, "time": 613.32}
{"epoch": 34, "training_loss": 55.137142181396484, "training_acc": 52.5, "val_loss": 13.851770162582397, "val_acc": 50.0, "val_auroc": 0.63, "time": 630.08}
{"epoch": 35, "training_loss": 55.217421531677246, "training_acc": 52.5, "val_loss": 13.841394186019897, "val_acc": 50.0, "val_auroc": 0.65, "time": 649.34}
{"epoch": 36, "training_loss": 55.141510009765625, "training_acc": 56.25, "val_loss": 13.849223852157593, "val_acc": 50.0, "val_auroc": 0.68, "time": 667.05}
{"epoch": 37, "training_loss": 55.587703704833984, "training_acc": 48.75, "val_loss": 13.860286474227905, "val_acc": 50.0, "val_auroc": 0.64, "time": 684.53}
{"epoch": 38, "training_loss": 55.86757755279541, "training_acc": 47.5, "val_loss": 13.848505020141602, "val_acc": 50.0, "val_auroc": 0.75, "time": 701.21}
{"epoch": 39, "training_loss": 55.71310520172119, "training_acc": 47.5, "val_loss": 13.856039047241211, "val_acc": 50.0, "val_auroc": 0.64, "time": 720.7}
{"epoch": 40, "training_loss": 55.45831489562988, "training_acc": 47.5, "val_loss": 13.853546380996704, "val_acc": 50.0, "val_auroc": 0.67, "time": 738.76}
{"epoch": 41, "training_loss": 55.441956520080566, "training_acc": 52.5, "val_loss": 13.85441541671753, "val_acc": 50.0, "val_auroc": 0.68, "time": 756.42}
{"epoch": 42, "training_loss": 55.304256439208984, "training_acc": 52.5, "val_loss": 13.84840726852417, "val_acc": 50.0, "val_auroc": 0.73, "time": 774.34}
{"epoch": 43, "training_loss": 55.32179260253906, "training_acc": 52.5, "val_loss": 13.864028453826904, "val_acc": 50.0, "val_auroc": 0.65, "time": 791.9}
{"epoch": 44, "training_loss": 55.29153060913086, "training_acc": 52.5, "val_loss": 13.885716199874878, "val_acc": 50.0, "val_auroc": 0.6, "time": 809.44}
{"epoch": 45, "training_loss": 55.300048828125, "training_acc": 52.5, "val_loss": 13.883593082427979, "val_acc": 50.0, "val_auroc": 0.62, "time": 827.35}
{"epoch": 46, "training_loss": 55.26308345794678, "training_acc": 52.5, "val_loss": 13.884350061416626, "val_acc": 50.0, "val_auroc": 0.64, "time": 843.77}
