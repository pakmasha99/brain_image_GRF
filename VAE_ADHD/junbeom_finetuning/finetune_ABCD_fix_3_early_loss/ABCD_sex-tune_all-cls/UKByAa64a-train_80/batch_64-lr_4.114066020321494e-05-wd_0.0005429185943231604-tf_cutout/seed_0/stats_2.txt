"main_optuna.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 80 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_ABCD_fix_3_early_loss --binary_class True --run_where lab --early_criteria loss"
{"epoch": 0, "training_loss": 55.43222236633301, "training_acc": 52.5, "val_loss": 13.84630560874939, "val_acc": 50.0, "val_auroc": 0.79, "time": 19.26}
{"epoch": 1, "training_loss": 55.37263011932373, "training_acc": 52.5, "val_loss": 13.955991268157959, "val_acc": 50.0, "val_auroc": 0.31, "time": 36.68}
{"epoch": 2, "training_loss": 55.25078010559082, "training_acc": 52.5, "val_loss": 13.952957391738892, "val_acc": 50.0, "val_auroc": 0.31, "time": 57.28}
{"epoch": 3, "training_loss": 55.259517669677734, "training_acc": 52.5, "val_loss": 13.970377445220947, "val_acc": 50.0, "val_auroc": 0.29, "time": 81.06}
{"epoch": 4, "training_loss": 55.32369422912598, "training_acc": 52.5, "val_loss": 13.965133428573608, "val_acc": 50.0, "val_auroc": 0.25, "time": 98.04}
{"epoch": 5, "training_loss": 55.309885025024414, "training_acc": 52.5, "val_loss": 13.965598344802856, "val_acc": 50.0, "val_auroc": 0.22, "time": 115.15}
{"epoch": 6, "training_loss": 55.34980392456055, "training_acc": 52.5, "val_loss": 13.97136926651001, "val_acc": 50.0, "val_auroc": 0.25, "time": 132.49}
{"epoch": 7, "training_loss": 55.145691871643066, "training_acc": 52.5, "val_loss": 14.042582511901855, "val_acc": 50.0, "val_auroc": 0.23, "time": 149.6}
{"epoch": 8, "training_loss": 55.238430976867676, "training_acc": 52.5, "val_loss": 14.071880578994751, "val_acc": 50.0, "val_auroc": 0.25, "time": 166.03}
{"epoch": 9, "training_loss": 55.12021827697754, "training_acc": 52.5, "val_loss": 13.871355056762695, "val_acc": 50.0, "val_auroc": 0.51, "time": 184.2}
{"epoch": 10, "training_loss": 55.127163887023926, "training_acc": 52.5, "val_loss": 13.827651739120483, "val_acc": 50.0, "val_auroc": 0.7, "time": 203.05}
{"epoch": 11, "training_loss": 55.23022270202637, "training_acc": 52.5, "val_loss": 13.887486457824707, "val_acc": 50.0, "val_auroc": 0.48, "time": 220.33}
{"epoch": 12, "training_loss": 55.04875564575195, "training_acc": 52.5, "val_loss": 13.948601484298706, "val_acc": 50.0, "val_auroc": 0.39, "time": 237.4}
{"epoch": 13, "training_loss": 55.008548736572266, "training_acc": 52.5, "val_loss": 13.958911895751953, "val_acc": 50.0, "val_auroc": 0.46, "time": 254.13}
{"epoch": 14, "training_loss": 54.73094463348389, "training_acc": 52.5, "val_loss": 13.973933458328247, "val_acc": 50.0, "val_auroc": 0.38, "time": 270.88}
{"epoch": 15, "training_loss": 54.77761745452881, "training_acc": 52.5, "val_loss": 13.992762565612793, "val_acc": 50.0, "val_auroc": 0.38, "time": 287.85}
{"epoch": 16, "training_loss": 54.465128898620605, "training_acc": 52.5, "val_loss": 13.984980583190918, "val_acc": 50.0, "val_auroc": 0.58, "time": 304.12}
{"epoch": 17, "training_loss": 54.99734973907471, "training_acc": 52.5, "val_loss": 14.005956649780273, "val_acc": 50.0, "val_auroc": 0.51, "time": 320.93}
{"epoch": 18, "training_loss": 54.67313003540039, "training_acc": 52.5, "val_loss": 13.961392641067505, "val_acc": 50.0, "val_auroc": 0.38, "time": 339.4}
{"epoch": 19, "training_loss": 54.62678146362305, "training_acc": 52.5, "val_loss": 13.86919379234314, "val_acc": 50.0, "val_auroc": 0.48, "time": 356.74}
{"epoch": 20, "training_loss": 54.66105270385742, "training_acc": 75.0, "val_loss": 13.813259601593018, "val_acc": 50.0, "val_auroc": 0.61, "time": 373.88}
{"epoch": 21, "training_loss": 54.49646472930908, "training_acc": 73.75, "val_loss": 13.878538608551025, "val_acc": 50.0, "val_auroc": 0.53, "time": 390.88}
{"epoch": 22, "training_loss": 54.51742362976074, "training_acc": 52.5, "val_loss": 14.027256965637207, "val_acc": 50.0, "val_auroc": 0.3, "time": 408.61}
{"epoch": 23, "training_loss": 54.236961364746094, "training_acc": 52.5, "val_loss": 14.00037407875061, "val_acc": 50.0, "val_auroc": 0.55, "time": 425.38}
{"epoch": 24, "training_loss": 54.25522804260254, "training_acc": 52.5, "val_loss": 14.004296064376831, "val_acc": 50.0, "val_auroc": 0.53, "time": 441.89}
{"epoch": 25, "training_loss": 54.275424003601074, "training_acc": 52.5, "val_loss": 14.024962186813354, "val_acc": 50.0, "val_auroc": 0.46, "time": 458.76}
{"epoch": 26, "training_loss": 54.02138900756836, "training_acc": 53.75, "val_loss": 14.013744592666626, "val_acc": 50.0, "val_auroc": 0.4, "time": 477.12}
{"epoch": 27, "training_loss": 53.44377899169922, "training_acc": 53.75, "val_loss": 14.099259376525879, "val_acc": 50.0, "val_auroc": 0.4, "time": 494.49}
{"epoch": 28, "training_loss": 53.536559104919434, "training_acc": 52.5, "val_loss": 13.977501392364502, "val_acc": 50.0, "val_auroc": 0.42, "time": 511.19}
{"epoch": 29, "training_loss": 53.052971839904785, "training_acc": 65.0, "val_loss": 13.970167636871338, "val_acc": 50.0, "val_auroc": 0.42, "time": 527.91}
{"epoch": 30, "training_loss": 53.14504528045654, "training_acc": 73.75, "val_loss": 14.2252516746521, "val_acc": 50.0, "val_auroc": 0.33, "time": 545.16}
{"epoch": 31, "training_loss": 52.29477119445801, "training_acc": 57.5, "val_loss": 14.181196689605713, "val_acc": 50.0, "val_auroc": 0.34, "time": 561.32}
{"epoch": 32, "training_loss": 51.0073938369751, "training_acc": 73.75, "val_loss": 13.98198127746582, "val_acc": 50.0, "val_auroc": 0.44, "time": 577.61}
{"epoch": 33, "training_loss": 51.300058364868164, "training_acc": 81.25, "val_loss": 14.09671425819397, "val_acc": 50.0, "val_auroc": 0.41, "time": 594.35}
{"epoch": 34, "training_loss": 50.54804992675781, "training_acc": 77.5, "val_loss": 14.080636501312256, "val_acc": 50.0, "val_auroc": 0.46, "time": 611.73}
{"epoch": 35, "training_loss": 49.1273136138916, "training_acc": 80.0, "val_loss": 13.843045234680176, "val_acc": 50.0, "val_auroc": 0.63, "time": 629.48}
{"epoch": 36, "training_loss": 53.20705509185791, "training_acc": 58.75, "val_loss": 13.826407194137573, "val_acc": 50.0, "val_auroc": 0.54, "time": 645.93}
{"epoch": 37, "training_loss": 49.980403900146484, "training_acc": 81.25, "val_loss": 14.091609716415405, "val_acc": 50.0, "val_auroc": 0.52, "time": 663.22}
{"epoch": 38, "training_loss": 49.584410667419434, "training_acc": 68.75, "val_loss": 13.91658902168274, "val_acc": 50.0, "val_auroc": 0.54, "time": 680.9}
{"epoch": 39, "training_loss": 48.08123779296875, "training_acc": 87.5, "val_loss": 13.788286447525024, "val_acc": 50.0, "val_auroc": 0.6, "time": 699.5}
{"epoch": 40, "training_loss": 46.230525970458984, "training_acc": 90.0, "val_loss": 14.087961912155151, "val_acc": 50.0, "val_auroc": 0.54, "time": 716.05}
{"epoch": 41, "training_loss": 45.932003021240234, "training_acc": 83.75, "val_loss": 13.994144201278687, "val_acc": 50.0, "val_auroc": 0.54, "time": 732.46}
{"epoch": 42, "training_loss": 43.404436111450195, "training_acc": 85.0, "val_loss": 13.953020572662354, "val_acc": 55.0, "val_auroc": 0.54, "time": 749.65}
{"epoch": 43, "training_loss": 41.77084159851074, "training_acc": 91.25, "val_loss": 13.925504684448242, "val_acc": 55.0, "val_auroc": 0.59, "time": 766.27}
{"epoch": 44, "training_loss": 42.93550491333008, "training_acc": 90.0, "val_loss": 13.987365961074829, "val_acc": 55.0, "val_auroc": 0.54, "time": 782.47}
{"epoch": 45, "training_loss": 39.91468811035156, "training_acc": 95.0, "val_loss": 13.868821859359741, "val_acc": 55.0, "val_auroc": 0.56, "time": 799.29}
{"epoch": 46, "training_loss": 37.38209915161133, "training_acc": 98.75, "val_loss": 13.733426332473755, "val_acc": 55.0, "val_auroc": 0.61, "time": 816.58}
{"epoch": 47, "training_loss": 35.40087604522705, "training_acc": 97.5, "val_loss": 13.902807235717773, "val_acc": 55.0, "val_auroc": 0.58, "time": 834.58}
{"epoch": 48, "training_loss": 36.917633056640625, "training_acc": 96.25, "val_loss": 14.30685043334961, "val_acc": 50.0, "val_auroc": 0.67, "time": 850.91}
{"epoch": 49, "training_loss": 37.238662242889404, "training_acc": 87.5, "val_loss": 13.846776485443115, "val_acc": 65.0, "val_auroc": 0.64, "time": 870.18}
{"epoch": 50, "training_loss": 33.13229942321777, "training_acc": 97.5, "val_loss": 14.278719425201416, "val_acc": 55.0, "val_auroc": 0.61, "time": 887.2}
{"epoch": 51, "training_loss": 32.616199016571045, "training_acc": 100.0, "val_loss": 13.905051946640015, "val_acc": 60.0, "val_auroc": 0.59, "time": 903.95}
{"epoch": 52, "training_loss": 30.04472255706787, "training_acc": 100.0, "val_loss": 14.748595952987671, "val_acc": 55.0, "val_auroc": 0.54, "time": 920.32}
{"epoch": 53, "training_loss": 32.90618085861206, "training_acc": 95.0, "val_loss": 14.535353183746338, "val_acc": 65.0, "val_auroc": 0.51, "time": 937.85}
{"epoch": 54, "training_loss": 33.06621789932251, "training_acc": 92.5, "val_loss": 15.410727262496948, "val_acc": 50.0, "val_auroc": 0.55, "time": 955.08}
{"epoch": 55, "training_loss": 33.92985010147095, "training_acc": 90.0, "val_loss": 14.614880084991455, "val_acc": 60.0, "val_auroc": 0.52, "time": 972.54}
{"epoch": 56, "training_loss": 30.237051486968994, "training_acc": 96.25, "val_loss": 14.658069610595703, "val_acc": 55.0, "val_auroc": 0.55, "time": 988.98}
{"epoch": 57, "training_loss": 27.406794548034668, "training_acc": 100.0, "val_loss": 14.218282699584961, "val_acc": 60.0, "val_auroc": 0.57, "time": 1008.11}
{"epoch": 58, "training_loss": 26.151073455810547, "training_acc": 98.75, "val_loss": 14.336199760437012, "val_acc": 55.0, "val_auroc": 0.55, "time": 1024.4}
{"epoch": 59, "training_loss": 26.332638263702393, "training_acc": 100.0, "val_loss": 14.797919988632202, "val_acc": 55.0, "val_auroc": 0.53, "time": 1041.07}
{"epoch": 60, "training_loss": 25.115649223327637, "training_acc": 100.0, "val_loss": 14.616929292678833, "val_acc": 50.0, "val_auroc": 0.54, "time": 1057.18}
{"epoch": 61, "training_loss": 24.910099983215332, "training_acc": 100.0, "val_loss": 14.963012933731079, "val_acc": 45.0, "val_auroc": 0.52, "time": 1075.04}
{"epoch": 62, "training_loss": 23.933849334716797, "training_acc": 100.0, "val_loss": 15.281418561935425, "val_acc": 55.0, "val_auroc": 0.51, "time": 1092.04}
{"epoch": 63, "training_loss": 22.889653205871582, "training_acc": 100.0, "val_loss": 14.933899641036987, "val_acc": 50.0, "val_auroc": 0.51, "time": 1109.54}
{"epoch": 64, "training_loss": 23.108582019805908, "training_acc": 100.0, "val_loss": 14.960988759994507, "val_acc": 50.0, "val_auroc": 0.5, "time": 1126.03}
{"epoch": 65, "training_loss": 21.641128540039062, "training_acc": 100.0, "val_loss": 15.518735647201538, "val_acc": 60.0, "val_auroc": 0.52, "time": 1143.69}
