"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 101.37751817703247, "training_acc": 53.0, "val_loss": 17.362768948078156, "val_acc": 52.0}
{"epoch": 1, "training_loss": 92.43747186660767, "training_acc": 56.0, "val_loss": 28.984227776527405, "val_acc": 48.0}
{"epoch": 2, "training_loss": 95.80831837654114, "training_acc": 50.0, "val_loss": 20.880205929279327, "val_acc": 52.0}
{"epoch": 3, "training_loss": 87.54588031768799, "training_acc": 53.0, "val_loss": 24.76235181093216, "val_acc": 52.0}
{"epoch": 4, "training_loss": 89.84821724891663, "training_acc": 53.0, "val_loss": 17.337828874588013, "val_acc": 52.0}
{"epoch": 5, "training_loss": 67.19142365455627, "training_acc": 64.0, "val_loss": 22.72631525993347, "val_acc": 48.0}
{"epoch": 6, "training_loss": 86.53655672073364, "training_acc": 47.0, "val_loss": 17.55729466676712, "val_acc": 56.0}
{"epoch": 7, "training_loss": 70.09623956680298, "training_acc": 50.0, "val_loss": 21.242186427116394, "val_acc": 52.0}
{"epoch": 8, "training_loss": 83.93978524208069, "training_acc": 53.0, "val_loss": 18.90440732240677, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.57674431800842, "training_acc": 54.0, "val_loss": 18.379580974578857, "val_acc": 60.0}
{"epoch": 10, "training_loss": 76.57591581344604, "training_acc": 48.0, "val_loss": 19.56542879343033, "val_acc": 48.0}
{"epoch": 11, "training_loss": 75.49102401733398, "training_acc": 47.0, "val_loss": 17.390792071819305, "val_acc": 52.0}
{"epoch": 12, "training_loss": 72.17744755744934, "training_acc": 52.0, "val_loss": 19.815772771835327, "val_acc": 52.0}
{"epoch": 13, "training_loss": 75.2438542842865, "training_acc": 53.0, "val_loss": 17.03874170780182, "val_acc": 52.0}
{"epoch": 14, "training_loss": 68.91525745391846, "training_acc": 50.0, "val_loss": 18.04259419441223, "val_acc": 60.0}
{"epoch": 15, "training_loss": 71.05675148963928, "training_acc": 48.0, "val_loss": 17.041243612766266, "val_acc": 52.0}
{"epoch": 16, "training_loss": 66.90700650215149, "training_acc": 60.0, "val_loss": 17.667274177074432, "val_acc": 52.0}
{"epoch": 17, "training_loss": 70.87688064575195, "training_acc": 53.0, "val_loss": 17.075811326503754, "val_acc": 52.0}
{"epoch": 18, "training_loss": 68.33548903465271, "training_acc": 56.0, "val_loss": 17.23737120628357, "val_acc": 56.0}
{"epoch": 19, "training_loss": 67.48059034347534, "training_acc": 56.0, "val_loss": 17.032603919506073, "val_acc": 52.0}
{"epoch": 20, "training_loss": 67.5265896320343, "training_acc": 56.0, "val_loss": 17.490306496620178, "val_acc": 52.0}
{"epoch": 21, "training_loss": 68.2674663066864, "training_acc": 52.0, "val_loss": 17.241233587265015, "val_acc": 52.0}
{"epoch": 22, "training_loss": 68.86333870887756, "training_acc": 51.0, "val_loss": 17.088384926319122, "val_acc": 52.0}
{"epoch": 23, "training_loss": 66.07236552238464, "training_acc": 58.0, "val_loss": 17.31419265270233, "val_acc": 52.0}
{"epoch": 24, "training_loss": 66.46667432785034, "training_acc": 53.0, "val_loss": 17.002788186073303, "val_acc": 52.0}
{"epoch": 25, "training_loss": 68.05872082710266, "training_acc": 59.0, "val_loss": 17.00778156518936, "val_acc": 52.0}
{"epoch": 26, "training_loss": 67.64952278137207, "training_acc": 60.0, "val_loss": 17.09866225719452, "val_acc": 52.0}
{"epoch": 27, "training_loss": 65.20120787620544, "training_acc": 63.0, "val_loss": 17.096617817878723, "val_acc": 52.0}
{"epoch": 28, "training_loss": 65.16848397254944, "training_acc": 63.0, "val_loss": 17.027942836284637, "val_acc": 52.0}
{"epoch": 29, "training_loss": 67.41635823249817, "training_acc": 58.0, "val_loss": 16.883647441864014, "val_acc": 52.0}
{"epoch": 30, "training_loss": 66.72589325904846, "training_acc": 55.0, "val_loss": 16.867583990097046, "val_acc": 52.0}
{"epoch": 31, "training_loss": 66.5540771484375, "training_acc": 57.0, "val_loss": 17.04384535551071, "val_acc": 52.0}
{"epoch": 32, "training_loss": 64.19292163848877, "training_acc": 66.0, "val_loss": 16.887249052524567, "val_acc": 52.0}
{"epoch": 33, "training_loss": 64.60896158218384, "training_acc": 72.0, "val_loss": 16.986213624477386, "val_acc": 52.0}
{"epoch": 34, "training_loss": 62.749011278152466, "training_acc": 72.0, "val_loss": 17.29167103767395, "val_acc": 52.0}
{"epoch": 35, "training_loss": 66.78727126121521, "training_acc": 56.0, "val_loss": 17.13741421699524, "val_acc": 52.0}
{"epoch": 36, "training_loss": 63.95021390914917, "training_acc": 68.0, "val_loss": 17.20656156539917, "val_acc": 52.0}
{"epoch": 37, "training_loss": 65.4240791797638, "training_acc": 71.0, "val_loss": 17.289099097251892, "val_acc": 52.0}
{"epoch": 38, "training_loss": 67.70271635055542, "training_acc": 55.0, "val_loss": 17.278772592544556, "val_acc": 52.0}
{"epoch": 39, "training_loss": 63.463987588882446, "training_acc": 63.0, "val_loss": 17.29225069284439, "val_acc": 52.0}
{"epoch": 40, "training_loss": 62.33178925514221, "training_acc": 66.0, "val_loss": 17.280417680740356, "val_acc": 52.0}
{"epoch": 41, "training_loss": 64.03655409812927, "training_acc": 65.0, "val_loss": 17.249074578285217, "val_acc": 52.0}
{"epoch": 42, "training_loss": 64.7420814037323, "training_acc": 60.0, "val_loss": 17.28508323431015, "val_acc": 52.0}
{"epoch": 43, "training_loss": 65.41957378387451, "training_acc": 61.0, "val_loss": 17.269092798233032, "val_acc": 52.0}
{"epoch": 44, "training_loss": 62.864521980285645, "training_acc": 67.0, "val_loss": 17.19682216644287, "val_acc": 60.0}
{"epoch": 45, "training_loss": 64.22106671333313, "training_acc": 65.0, "val_loss": 17.01483130455017, "val_acc": 52.0}
{"epoch": 46, "training_loss": 64.58937120437622, "training_acc": 65.0, "val_loss": 17.198356986045837, "val_acc": 52.0}
{"epoch": 47, "training_loss": 62.12197518348694, "training_acc": 70.0, "val_loss": 17.03581064939499, "val_acc": 52.0}
{"epoch": 48, "training_loss": 62.80219769477844, "training_acc": 70.0, "val_loss": 16.998010873794556, "val_acc": 52.0}
{"epoch": 49, "training_loss": 64.9673364162445, "training_acc": 71.0, "val_loss": 16.95997565984726, "val_acc": 52.0}
