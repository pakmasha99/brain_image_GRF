"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 6882637.495044708, "training_acc": 46.0, "val_loss": 1438636.62109375, "val_acc": 52.0}
{"epoch": 1, "training_loss": 6985820.65625, "training_acc": 53.0, "val_loss": 3725715.234375, "val_acc": 48.0}
{"epoch": 2, "training_loss": 14021765.1875, "training_acc": 47.0, "val_loss": 1032760.3515625, "val_acc": 48.0}
{"epoch": 3, "training_loss": 4885929.5, "training_acc": 51.0, "val_loss": 2720940.4296875, "val_acc": 52.0}
{"epoch": 4, "training_loss": 10337620.4375, "training_acc": 53.0, "val_loss": 2427975.78125, "val_acc": 52.0}
{"epoch": 5, "training_loss": 7206041.734375, "training_acc": 53.0, "val_loss": 421858.7890625, "val_acc": 48.0}
{"epoch": 6, "training_loss": 3888218.65625, "training_acc": 47.0, "val_loss": 1243591.89453125, "val_acc": 48.0}
{"epoch": 7, "training_loss": 4215279.03125, "training_acc": 47.0, "val_loss": 901104.58984375, "val_acc": 52.0}
{"epoch": 8, "training_loss": 3513637.3125, "training_acc": 53.0, "val_loss": 1562472.4609375, "val_acc": 52.0}
{"epoch": 9, "training_loss": 5073573.515625, "training_acc": 53.0, "val_loss": 196892.529296875, "val_acc": 52.0}
{"epoch": 10, "training_loss": 2029191.53125, "training_acc": 49.0, "val_loss": 1452776.85546875, "val_acc": 48.0}
{"epoch": 11, "training_loss": 5566316.984375, "training_acc": 47.0, "val_loss": 308171.923828125, "val_acc": 48.0}
{"epoch": 12, "training_loss": 2016793.59375, "training_acc": 49.0, "val_loss": 1528720.1171875, "val_acc": 52.0}
{"epoch": 13, "training_loss": 5877481.53125, "training_acc": 53.0, "val_loss": 1146025.48828125, "val_acc": 52.0}
{"epoch": 14, "training_loss": 3076524.544921875, "training_acc": 52.0, "val_loss": 1123846.19140625, "val_acc": 48.0}
{"epoch": 15, "training_loss": 5475356.71875, "training_acc": 47.0, "val_loss": 1671530.2734375, "val_acc": 48.0}
{"epoch": 16, "training_loss": 5639487.765625, "training_acc": 47.0, "val_loss": 152241.0888671875, "val_acc": 60.0}
{"epoch": 17, "training_loss": 1400388.3203125, "training_acc": 54.0, "val_loss": 950543.75, "val_acc": 52.0}
{"epoch": 18, "training_loss": 3051423.5859375, "training_acc": 53.0, "val_loss": 215394.1162109375, "val_acc": 48.0}
{"epoch": 19, "training_loss": 948248.9296875, "training_acc": 50.0, "val_loss": 102923.47412109375, "val_acc": 56.0}
{"epoch": 20, "training_loss": 692203.58203125, "training_acc": 60.0, "val_loss": 159425.68359375, "val_acc": 60.0}
{"epoch": 21, "training_loss": 595683.48828125, "training_acc": 55.0, "val_loss": 171413.14697265625, "val_acc": 44.0}
{"epoch": 22, "training_loss": 828444.4453125, "training_acc": 55.0, "val_loss": 452054.8828125, "val_acc": 52.0}
{"epoch": 23, "training_loss": 846602.6176757812, "training_acc": 62.0, "val_loss": 444396.09375, "val_acc": 48.0}
{"epoch": 24, "training_loss": 1644193.57421875, "training_acc": 48.0, "val_loss": 362537.109375, "val_acc": 52.0}
{"epoch": 25, "training_loss": 1173111.99609375, "training_acc": 55.0, "val_loss": 161759.33837890625, "val_acc": 60.0}
{"epoch": 26, "training_loss": 1202070.5859375, "training_acc": 52.0, "val_loss": 398439.892578125, "val_acc": 48.0}
{"epoch": 27, "training_loss": 1407617.619140625, "training_acc": 49.0, "val_loss": 374616.357421875, "val_acc": 52.0}
{"epoch": 28, "training_loss": 751050.8466796875, "training_acc": 58.0, "val_loss": 243147.0458984375, "val_acc": 48.0}
{"epoch": 29, "training_loss": 772502.275390625, "training_acc": 58.0, "val_loss": 610511.62109375, "val_acc": 52.0}
{"epoch": 30, "training_loss": 2091602.5546875, "training_acc": 53.0, "val_loss": 283697.2412109375, "val_acc": 56.0}
{"epoch": 31, "training_loss": 1360582.1015625, "training_acc": 52.0, "val_loss": 617777.05078125, "val_acc": 48.0}
{"epoch": 32, "training_loss": 1854367.416015625, "training_acc": 51.0, "val_loss": 694990.625, "val_acc": 52.0}
{"epoch": 33, "training_loss": 2774030.03125, "training_acc": 53.0, "val_loss": 634487.255859375, "val_acc": 52.0}
{"epoch": 34, "training_loss": 1174969.98828125, "training_acc": 66.0, "val_loss": 528988.8671875, "val_acc": 48.0}
{"epoch": 35, "training_loss": 1942975.0, "training_acc": 47.0, "val_loss": 333968.06640625, "val_acc": 52.0}
{"epoch": 36, "training_loss": 1155239.5703125, "training_acc": 55.0, "val_loss": 187053.6865234375, "val_acc": 60.0}
{"epoch": 37, "training_loss": 842481.72265625, "training_acc": 49.0, "val_loss": 312613.623046875, "val_acc": 48.0}
{"epoch": 38, "training_loss": 1137937.04296875, "training_acc": 48.0, "val_loss": 361761.9873046875, "val_acc": 52.0}
