"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 981220.5043716431, "training_acc": 53.0, "val_loss": 2761766.2109375, "val_acc": 48.0}
{"epoch": 1, "training_loss": 11149468.0, "training_acc": 47.0, "val_loss": 216597.1435546875, "val_acc": 44.0}
{"epoch": 2, "training_loss": 1886560.046875, "training_acc": 68.0, "val_loss": 2707756.640625, "val_acc": 52.0}
{"epoch": 3, "training_loss": 8441976.484375, "training_acc": 53.0, "val_loss": 754053.466796875, "val_acc": 52.0}
{"epoch": 4, "training_loss": 3608756.84375, "training_acc": 48.0, "val_loss": 2058706.640625, "val_acc": 48.0}
{"epoch": 5, "training_loss": 8350003.125, "training_acc": 47.0, "val_loss": 564878.3203125, "val_acc": 48.0}
{"epoch": 6, "training_loss": 2887743.96875, "training_acc": 51.0, "val_loss": 2011437.890625, "val_acc": 52.0}
{"epoch": 7, "training_loss": 6990635.8125, "training_acc": 53.0, "val_loss": 1568760.7421875, "val_acc": 52.0}
{"epoch": 8, "training_loss": 3880530.38671875, "training_acc": 54.0, "val_loss": 947995.3125, "val_acc": 48.0}
{"epoch": 9, "training_loss": 4908521.0625, "training_acc": 47.0, "val_loss": 1371162.5, "val_acc": 48.0}
{"epoch": 10, "training_loss": 4811708.1171875, "training_acc": 47.0, "val_loss": 692251.66015625, "val_acc": 52.0}
{"epoch": 11, "training_loss": 2483951.359375, "training_acc": 54.0, "val_loss": 1341118.75, "val_acc": 52.0}
{"epoch": 12, "training_loss": 3726037.2578125, "training_acc": 53.0, "val_loss": 179916.9677734375, "val_acc": 48.0}
{"epoch": 13, "training_loss": 1265338.1171875, "training_acc": 58.0, "val_loss": 431608.740234375, "val_acc": 52.0}
{"epoch": 14, "training_loss": 1947550.109375, "training_acc": 57.0, "val_loss": 713011.71875, "val_acc": 52.0}
{"epoch": 15, "training_loss": 1682283.14453125, "training_acc": 55.0, "val_loss": 240348.9501953125, "val_acc": 48.0}
{"epoch": 16, "training_loss": 884555.68359375, "training_acc": 57.0, "val_loss": 246033.8623046875, "val_acc": 48.0}
{"epoch": 17, "training_loss": 1080971.853515625, "training_acc": 54.0, "val_loss": 559908.544921875, "val_acc": 52.0}
{"epoch": 18, "training_loss": 978989.5546875, "training_acc": 56.0, "val_loss": 252628.3447265625, "val_acc": 52.0}
{"epoch": 19, "training_loss": 1276414.67578125, "training_acc": 51.0, "val_loss": 351828.173828125, "val_acc": 48.0}
{"epoch": 20, "training_loss": 1084221.875, "training_acc": 55.0, "val_loss": 262121.77734375, "val_acc": 48.0}
{"epoch": 21, "training_loss": 1146791.6640625, "training_acc": 58.0, "val_loss": 245195.4833984375, "val_acc": 52.0}
{"epoch": 22, "training_loss": 1144732.33203125, "training_acc": 55.0, "val_loss": 525250.341796875, "val_acc": 52.0}
{"epoch": 23, "training_loss": 1290303.494140625, "training_acc": 52.0, "val_loss": 447205.37109375, "val_acc": 48.0}
{"epoch": 24, "training_loss": 1786916.3828125, "training_acc": 48.0, "val_loss": 231395.556640625, "val_acc": 52.0}
{"epoch": 25, "training_loss": 851006.25390625, "training_acc": 59.0, "val_loss": 245048.2421875, "val_acc": 52.0}
{"epoch": 26, "training_loss": 893041.19140625, "training_acc": 62.0, "val_loss": 329555.078125, "val_acc": 48.0}
{"epoch": 27, "training_loss": 1302302.275390625, "training_acc": 50.0, "val_loss": 442169.62890625, "val_acc": 52.0}
{"epoch": 28, "training_loss": 867615.69921875, "training_acc": 58.0, "val_loss": 91235.16845703125, "val_acc": 56.0}
{"epoch": 29, "training_loss": 362628.5029296875, "training_acc": 66.0, "val_loss": 290954.248046875, "val_acc": 52.0}
{"epoch": 30, "training_loss": 648766.7890625, "training_acc": 55.0, "val_loss": 248504.2236328125, "val_acc": 52.0}
{"epoch": 31, "training_loss": 440299.623046875, "training_acc": 63.0, "val_loss": 92761.83471679688, "val_acc": 64.0}
{"epoch": 32, "training_loss": 438965.908203125, "training_acc": 61.0, "val_loss": 101838.06762695312, "val_acc": 52.0}
{"epoch": 33, "training_loss": 126816.4833984375, "training_acc": 75.0, "val_loss": 152392.41943359375, "val_acc": 52.0}
{"epoch": 34, "training_loss": 332891.1337890625, "training_acc": 66.0, "val_loss": 130246.630859375, "val_acc": 52.0}
{"epoch": 35, "training_loss": 229622.74365234375, "training_acc": 70.0, "val_loss": 38471.12121582031, "val_acc": 60.0}
{"epoch": 36, "training_loss": 182241.9208984375, "training_acc": 67.0, "val_loss": 358746.9970703125, "val_acc": 52.0}
{"epoch": 37, "training_loss": 759339.8359375, "training_acc": 57.0, "val_loss": 400279.345703125, "val_acc": 48.0}
{"epoch": 38, "training_loss": 1319899.5185546875, "training_acc": 49.0, "val_loss": 576125.537109375, "val_acc": 52.0}
{"epoch": 39, "training_loss": 2214380.890625, "training_acc": 53.0, "val_loss": 184847.021484375, "val_acc": 52.0}
{"epoch": 40, "training_loss": 1760743.765625, "training_acc": 50.0, "val_loss": 1225022.265625, "val_acc": 48.0}
{"epoch": 41, "training_loss": 4325416.734375, "training_acc": 47.0, "val_loss": 156312.29248046875, "val_acc": 52.0}
{"epoch": 42, "training_loss": 959962.7890625, "training_acc": 55.0, "val_loss": 271739.6728515625, "val_acc": 52.0}
{"epoch": 43, "training_loss": 1346084.1015625, "training_acc": 49.0, "val_loss": 637937.744140625, "val_acc": 48.0}
{"epoch": 44, "training_loss": 1725045.626953125, "training_acc": 50.0, "val_loss": 946583.3984375, "val_acc": 52.0}
{"epoch": 45, "training_loss": 3502377.53125, "training_acc": 53.0, "val_loss": 1179172.0703125, "val_acc": 52.0}
{"epoch": 46, "training_loss": 2852713.89453125, "training_acc": 53.0, "val_loss": 679726.123046875, "val_acc": 48.0}
{"epoch": 47, "training_loss": 3610803.703125, "training_acc": 47.0, "val_loss": 864299.609375, "val_acc": 48.0}
{"epoch": 48, "training_loss": 2764679.287109375, "training_acc": 55.0, "val_loss": 1053869.23828125, "val_acc": 52.0}
{"epoch": 49, "training_loss": 3117272.2578125, "training_acc": 53.0, "val_loss": 1174843.26171875, "val_acc": 52.0}
{"epoch": 50, "training_loss": 2357210.4140625, "training_acc": 55.0, "val_loss": 588704.4921875, "val_acc": 48.0}
{"epoch": 51, "training_loss": 3235053.4375, "training_acc": 47.0, "val_loss": 538820.60546875, "val_acc": 48.0}
{"epoch": 52, "training_loss": 1903392.83203125, "training_acc": 56.0, "val_loss": 1152263.18359375, "val_acc": 52.0}
{"epoch": 53, "training_loss": 3309276.390625, "training_acc": 52.0, "val_loss": 860814.35546875, "val_acc": 52.0}
{"epoch": 54, "training_loss": 1766146.25, "training_acc": 50.0, "val_loss": 395183.8623046875, "val_acc": 48.0}
