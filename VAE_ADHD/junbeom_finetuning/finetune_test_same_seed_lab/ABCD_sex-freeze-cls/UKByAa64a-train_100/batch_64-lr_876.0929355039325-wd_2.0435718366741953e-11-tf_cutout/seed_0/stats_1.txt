"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 1840060.9637947083, "training_acc": 46.0, "val_loss": 384610.5224609375, "val_acc": 52.0}
{"epoch": 1, "training_loss": 1867614.1640625, "training_acc": 53.0, "val_loss": 996045.5078125, "val_acc": 48.0}
{"epoch": 2, "training_loss": 3748627.296875, "training_acc": 47.0, "val_loss": 276102.1728515625, "val_acc": 48.0}
{"epoch": 3, "training_loss": 1306221.94140625, "training_acc": 51.0, "val_loss": 727425.48828125, "val_acc": 52.0}
{"epoch": 4, "training_loss": 2763695.2734375, "training_acc": 53.0, "val_loss": 649103.90625, "val_acc": 52.0}
{"epoch": 5, "training_loss": 1926490.42578125, "training_acc": 53.0, "val_loss": 112780.33447265625, "val_acc": 48.0}
{"epoch": 6, "training_loss": 1039487.0703125, "training_acc": 47.0, "val_loss": 332465.5517578125, "val_acc": 48.0}
{"epoch": 7, "training_loss": 1126925.599609375, "training_acc": 47.0, "val_loss": 240904.8583984375, "val_acc": 52.0}
{"epoch": 8, "training_loss": 939349.4453125, "training_acc": 53.0, "val_loss": 417717.529296875, "val_acc": 52.0}
{"epoch": 9, "training_loss": 1356389.359375, "training_acc": 53.0, "val_loss": 52638.909912109375, "val_acc": 52.0}
{"epoch": 10, "training_loss": 542491.53515625, "training_acc": 49.0, "val_loss": 388389.4287109375, "val_acc": 48.0}
{"epoch": 11, "training_loss": 1488115.6796875, "training_acc": 47.0, "val_loss": 82387.0361328125, "val_acc": 48.0}
{"epoch": 12, "training_loss": 539175.63671875, "training_acc": 49.0, "val_loss": 408694.04296875, "val_acc": 52.0}
{"epoch": 13, "training_loss": 1571308.4140625, "training_acc": 53.0, "val_loss": 306383.5205078125, "val_acc": 52.0}
{"epoch": 14, "training_loss": 822492.2416992188, "training_acc": 52.0, "val_loss": 300451.5869140625, "val_acc": 48.0}
{"epoch": 15, "training_loss": 1463796.65625, "training_acc": 47.0, "val_loss": 446871.6796875, "val_acc": 48.0}
{"epoch": 16, "training_loss": 1507677.640625, "training_acc": 47.0, "val_loss": 40701.23596191406, "val_acc": 60.0}
{"epoch": 17, "training_loss": 374394.587890625, "training_acc": 54.0, "val_loss": 254134.2041015625, "val_acc": 52.0}
{"epoch": 18, "training_loss": 815828.2587890625, "training_acc": 53.0, "val_loss": 57560.21728515625, "val_acc": 48.0}
{"epoch": 19, "training_loss": 253416.275390625, "training_acc": 50.0, "val_loss": 27527.664184570312, "val_acc": 56.0}
{"epoch": 20, "training_loss": 185106.875, "training_acc": 60.0, "val_loss": 42650.90637207031, "val_acc": 60.0}
{"epoch": 21, "training_loss": 159232.9619140625, "training_acc": 55.0, "val_loss": 45784.906005859375, "val_acc": 44.0}
{"epoch": 22, "training_loss": 221441.583984375, "training_acc": 55.0, "val_loss": 120898.5107421875, "val_acc": 52.0}
{"epoch": 23, "training_loss": 226448.73217773438, "training_acc": 62.0, "val_loss": 118753.60107421875, "val_acc": 48.0}
{"epoch": 24, "training_loss": 439363.0927734375, "training_acc": 48.0, "val_loss": 96972.47314453125, "val_acc": 52.0}
{"epoch": 25, "training_loss": 313806.548828125, "training_acc": 55.0, "val_loss": 43289.18762207031, "val_acc": 60.0}
{"epoch": 26, "training_loss": 321346.3828125, "training_acc": 52.0, "val_loss": 106460.21728515625, "val_acc": 48.0}
{"epoch": 27, "training_loss": 376238.45947265625, "training_acc": 49.0, "val_loss": 100207.177734375, "val_acc": 52.0}
{"epoch": 28, "training_loss": 200902.90600585938, "training_acc": 58.0, "val_loss": 64940.765380859375, "val_acc": 48.0}
{"epoch": 29, "training_loss": 206329.423828125, "training_acc": 58.0, "val_loss": 163274.40185546875, "val_acc": 52.0}
{"epoch": 30, "training_loss": 559396.91015625, "training_acc": 53.0, "val_loss": 75898.84033203125, "val_acc": 56.0}
{"epoch": 31, "training_loss": 363761.984375, "training_acc": 52.0, "val_loss": 165093.02978515625, "val_acc": 48.0}
{"epoch": 32, "training_loss": 495531.26025390625, "training_acc": 51.0, "val_loss": 185861.23046875, "val_acc": 52.0}
{"epoch": 33, "training_loss": 741842.796875, "training_acc": 53.0, "val_loss": 169686.62109375, "val_acc": 52.0}
{"epoch": 34, "training_loss": 314207.7844238281, "training_acc": 66.0, "val_loss": 141354.52880859375, "val_acc": 48.0}
{"epoch": 35, "training_loss": 519183.24609375, "training_acc": 47.0, "val_loss": 89345.29418945312, "val_acc": 52.0}
{"epoch": 36, "training_loss": 309057.638671875, "training_acc": 55.0, "val_loss": 50058.74938964844, "val_acc": 60.0}
{"epoch": 37, "training_loss": 225215.619140625, "training_acc": 49.0, "val_loss": 83507.2509765625, "val_acc": 48.0}
{"epoch": 38, "training_loss": 304129.943359375, "training_acc": 48.0, "val_loss": 96776.43432617188, "val_acc": 52.0}
