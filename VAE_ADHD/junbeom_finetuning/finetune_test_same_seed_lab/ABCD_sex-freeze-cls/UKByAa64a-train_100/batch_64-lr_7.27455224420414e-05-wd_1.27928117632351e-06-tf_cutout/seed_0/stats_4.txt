"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 71.59688138961792, "training_acc": 47.0, "val_loss": 17.69510954618454, "val_acc": 52.0}
{"epoch": 1, "training_loss": 70.79028630256653, "training_acc": 47.0, "val_loss": 17.57759302854538, "val_acc": 52.0}
{"epoch": 2, "training_loss": 70.42902565002441, "training_acc": 47.0, "val_loss": 17.482464015483856, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.9061815738678, "training_acc": 47.0, "val_loss": 17.41088479757309, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.71587944030762, "training_acc": 47.0, "val_loss": 17.360298335552216, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.53151941299438, "training_acc": 47.0, "val_loss": 17.32887625694275, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.24914121627808, "training_acc": 52.0, "val_loss": 17.311954498291016, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.25169825553894, "training_acc": 53.0, "val_loss": 17.305827140808105, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.09483790397644, "training_acc": 53.0, "val_loss": 17.308731377124786, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.05652260780334, "training_acc": 53.0, "val_loss": 17.315681278705597, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.10973691940308, "training_acc": 53.0, "val_loss": 17.325888574123383, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.03529691696167, "training_acc": 53.0, "val_loss": 17.33294278383255, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.20687413215637, "training_acc": 53.0, "val_loss": 17.338867485523224, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.12109732627869, "training_acc": 53.0, "val_loss": 17.34018325805664, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.2174768447876, "training_acc": 53.0, "val_loss": 17.337848246097565, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.15446782112122, "training_acc": 53.0, "val_loss": 17.337019741535187, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.20278239250183, "training_acc": 53.0, "val_loss": 17.336459457874298, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.14800596237183, "training_acc": 53.0, "val_loss": 17.331375181674957, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.16651678085327, "training_acc": 53.0, "val_loss": 17.32862889766693, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.18537998199463, "training_acc": 53.0, "val_loss": 17.325443029403687, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.10431361198425, "training_acc": 53.0, "val_loss": 17.323073744773865, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.14147090911865, "training_acc": 53.0, "val_loss": 17.322440445423126, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.08614730834961, "training_acc": 53.0, "val_loss": 17.321684956550598, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.13980984687805, "training_acc": 53.0, "val_loss": 17.321430146694183, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.1413996219635, "training_acc": 53.0, "val_loss": 17.320144176483154, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.11787915229797, "training_acc": 53.0, "val_loss": 17.318542301654816, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.08451890945435, "training_acc": 53.0, "val_loss": 17.31724441051483, "val_acc": 52.0}
