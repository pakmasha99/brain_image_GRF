"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.29445147514343, "training_acc": 53.0, "val_loss": 17.260345816612244, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.12500786781311, "training_acc": 53.0, "val_loss": 17.263470590114594, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.15889883041382, "training_acc": 53.0, "val_loss": 17.264464497566223, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.15382194519043, "training_acc": 53.0, "val_loss": 17.269279062747955, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.0936439037323, "training_acc": 53.0, "val_loss": 17.2716423869133, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.13825297355652, "training_acc": 53.0, "val_loss": 17.273154854774475, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.13243532180786, "training_acc": 53.0, "val_loss": 17.274905741214752, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.300048828125, "training_acc": 53.0, "val_loss": 17.274387180805206, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.08653712272644, "training_acc": 53.0, "val_loss": 17.27064549922943, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.1899197101593, "training_acc": 53.0, "val_loss": 17.266090214252472, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.25054669380188, "training_acc": 53.0, "val_loss": 17.261771857738495, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.1200258731842, "training_acc": 53.0, "val_loss": 17.2604963183403, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.15958333015442, "training_acc": 53.0, "val_loss": 17.258983850479126, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.15435004234314, "training_acc": 53.0, "val_loss": 17.257285118103027, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.08740472793579, "training_acc": 53.0, "val_loss": 17.2561913728714, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.22903800010681, "training_acc": 53.0, "val_loss": 17.25578010082245, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.30708432197571, "training_acc": 53.0, "val_loss": 17.2556534409523, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.11853814125061, "training_acc": 53.0, "val_loss": 17.256245017051697, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.17486310005188, "training_acc": 53.0, "val_loss": 17.257362604141235, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.20714116096497, "training_acc": 53.0, "val_loss": 17.257605493068695, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.16652607917786, "training_acc": 53.0, "val_loss": 17.258930206298828, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.28223752975464, "training_acc": 53.0, "val_loss": 17.26030260324478, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.17228817939758, "training_acc": 53.0, "val_loss": 17.261622846126556, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.16594052314758, "training_acc": 53.0, "val_loss": 17.26149022579193, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.21524739265442, "training_acc": 53.0, "val_loss": 17.261596024036407, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.1394739151001, "training_acc": 53.0, "val_loss": 17.261964082717896, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.2634162902832, "training_acc": 53.0, "val_loss": 17.261622846126556, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.0675573348999, "training_acc": 53.0, "val_loss": 17.261970043182373, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.12408566474915, "training_acc": 53.0, "val_loss": 17.262963950634003, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.12793850898743, "training_acc": 53.0, "val_loss": 17.263206839561462, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.14148378372192, "training_acc": 53.0, "val_loss": 17.26227104663849, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.21645712852478, "training_acc": 53.0, "val_loss": 17.261381447315216, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.04986119270325, "training_acc": 53.0, "val_loss": 17.260536551475525, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.22028923034668, "training_acc": 53.0, "val_loss": 17.26054698228836, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.20075583457947, "training_acc": 53.0, "val_loss": 17.260318994522095, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.20017695426941, "training_acc": 53.0, "val_loss": 17.26166158914566, "val_acc": 52.0}
