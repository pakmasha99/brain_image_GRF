"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 1844.1585388183594, "training_acc": 47.0, "val_loss": 270.96660137176514, "val_acc": 48.0}
{"epoch": 1, "training_loss": 1633.5760116577148, "training_acc": 49.0, "val_loss": 1031.0025215148926, "val_acc": 52.0}
{"epoch": 2, "training_loss": 3956.415573120117, "training_acc": 53.0, "val_loss": 681.0549259185791, "val_acc": 52.0}
{"epoch": 3, "training_loss": 1942.094596862793, "training_acc": 53.0, "val_loss": 406.3781261444092, "val_acc": 48.0}
{"epoch": 4, "training_loss": 1971.510871887207, "training_acc": 47.0, "val_loss": 754.960298538208, "val_acc": 48.0}
{"epoch": 5, "training_loss": 2560.7348251342773, "training_acc": 47.0, "val_loss": 257.3288917541504, "val_acc": 48.0}
{"epoch": 6, "training_loss": 945.477912902832, "training_acc": 49.0, "val_loss": 416.5680408477783, "val_acc": 52.0}
{"epoch": 7, "training_loss": 1816.9748611450195, "training_acc": 53.0, "val_loss": 387.9129886627197, "val_acc": 52.0}
{"epoch": 8, "training_loss": 1332.126953125, "training_acc": 53.0, "val_loss": 160.7258439064026, "val_acc": 44.0}
{"epoch": 9, "training_loss": 816.8079719543457, "training_acc": 53.0, "val_loss": 397.9299545288086, "val_acc": 48.0}
{"epoch": 10, "training_loss": 1367.5238456726074, "training_acc": 49.0, "val_loss": 77.96236872673035, "val_acc": 52.0}
{"epoch": 11, "training_loss": 544.4596824645996, "training_acc": 56.0, "val_loss": 340.63658714294434, "val_acc": 52.0}
{"epoch": 12, "training_loss": 1430.123649597168, "training_acc": 53.0, "val_loss": 200.64516067504883, "val_acc": 52.0}
{"epoch": 13, "training_loss": 553.6223287582397, "training_acc": 51.0, "val_loss": 223.915696144104, "val_acc": 48.0}
{"epoch": 14, "training_loss": 911.8304443359375, "training_acc": 47.0, "val_loss": 127.22592353820801, "val_acc": 48.0}
{"epoch": 15, "training_loss": 430.65316581726074, "training_acc": 51.0, "val_loss": 162.70949840545654, "val_acc": 52.0}
{"epoch": 16, "training_loss": 597.8217716217041, "training_acc": 53.0, "val_loss": 58.264726400375366, "val_acc": 48.0}
{"epoch": 17, "training_loss": 295.0965099334717, "training_acc": 48.0, "val_loss": 23.158767819404602, "val_acc": 64.0}
{"epoch": 18, "training_loss": 222.7527904510498, "training_acc": 54.0, "val_loss": 50.7862389087677, "val_acc": 52.0}
{"epoch": 19, "training_loss": 193.76353549957275, "training_acc": 57.0, "val_loss": 84.5407783985138, "val_acc": 48.0}
{"epoch": 20, "training_loss": 296.8607168197632, "training_acc": 47.0, "val_loss": 60.22161841392517, "val_acc": 52.0}
{"epoch": 21, "training_loss": 154.8581202030182, "training_acc": 65.0, "val_loss": 90.1683509349823, "val_acc": 48.0}
{"epoch": 22, "training_loss": 251.9650959968567, "training_acc": 53.0, "val_loss": 102.25170850753784, "val_acc": 52.0}
{"epoch": 23, "training_loss": 344.193829536438, "training_acc": 53.0, "val_loss": 30.32633662223816, "val_acc": 44.0}
{"epoch": 24, "training_loss": 114.68046426773071, "training_acc": 60.0, "val_loss": 17.764298617839813, "val_acc": 60.0}
{"epoch": 25, "training_loss": 99.3916449546814, "training_acc": 61.0, "val_loss": 21.01038545370102, "val_acc": 60.0}
{"epoch": 26, "training_loss": 72.28210663795471, "training_acc": 64.0, "val_loss": 26.555976271629333, "val_acc": 52.0}
{"epoch": 27, "training_loss": 91.89107823371887, "training_acc": 66.0, "val_loss": 49.5814174413681, "val_acc": 48.0}
{"epoch": 28, "training_loss": 116.13611114025116, "training_acc": 64.0, "val_loss": 48.43233525753021, "val_acc": 52.0}
{"epoch": 29, "training_loss": 171.21515703201294, "training_acc": 48.0, "val_loss": 19.52582746744156, "val_acc": 52.0}
{"epoch": 30, "training_loss": 67.84889507293701, "training_acc": 67.0, "val_loss": 52.704089879989624, "val_acc": 48.0}
{"epoch": 31, "training_loss": 176.6925983428955, "training_acc": 48.0, "val_loss": 17.231059074401855, "val_acc": 68.0}
{"epoch": 32, "training_loss": 59.185017824172974, "training_acc": 67.0, "val_loss": 34.82606112957001, "val_acc": 52.0}
{"epoch": 33, "training_loss": 79.45721936225891, "training_acc": 65.0, "val_loss": 41.418203711509705, "val_acc": 48.0}
{"epoch": 34, "training_loss": 256.23601150512695, "training_acc": 37.0, "val_loss": 25.06903111934662, "val_acc": 52.0}
{"epoch": 35, "training_loss": 185.9040765762329, "training_acc": 59.0, "val_loss": 91.16539359092712, "val_acc": 48.0}
{"epoch": 36, "training_loss": 226.0969705581665, "training_acc": 59.0, "val_loss": 101.86489820480347, "val_acc": 52.0}
{"epoch": 37, "training_loss": 232.67923045158386, "training_acc": 55.0, "val_loss": 145.18070220947266, "val_acc": 48.0}
{"epoch": 38, "training_loss": 585.3197956085205, "training_acc": 47.0, "val_loss": 18.078432977199554, "val_acc": 56.0}
{"epoch": 39, "training_loss": 264.6139621734619, "training_acc": 64.0, "val_loss": 108.67630243301392, "val_acc": 52.0}
{"epoch": 40, "training_loss": 335.99061584472656, "training_acc": 48.0, "val_loss": 66.02824926376343, "val_acc": 48.0}
{"epoch": 41, "training_loss": 217.0114288330078, "training_acc": 51.0, "val_loss": 63.292813301086426, "val_acc": 52.0}
{"epoch": 42, "training_loss": 210.51928997039795, "training_acc": 51.0, "val_loss": 39.29355442523956, "val_acc": 44.0}
{"epoch": 43, "training_loss": 251.12250518798828, "training_acc": 49.0, "val_loss": 54.19711470603943, "val_acc": 52.0}
{"epoch": 44, "training_loss": 216.19421863555908, "training_acc": 55.0, "val_loss": 88.23662996292114, "val_acc": 48.0}
{"epoch": 45, "training_loss": 243.23620700836182, "training_acc": 56.0, "val_loss": 70.26416063308716, "val_acc": 52.0}
{"epoch": 46, "training_loss": 164.9187614917755, "training_acc": 61.0, "val_loss": 43.998512625694275, "val_acc": 48.0}
{"epoch": 47, "training_loss": 196.80486679077148, "training_acc": 51.0, "val_loss": 28.963780403137207, "val_acc": 56.0}
{"epoch": 48, "training_loss": 100.5077977180481, "training_acc": 63.0, "val_loss": 24.05245155096054, "val_acc": 60.0}
{"epoch": 49, "training_loss": 123.67018175125122, "training_acc": 66.0, "val_loss": 34.74936485290527, "val_acc": 52.0}
{"epoch": 50, "training_loss": 225.46767044067383, "training_acc": 51.0, "val_loss": 37.578871846199036, "val_acc": 44.0}
