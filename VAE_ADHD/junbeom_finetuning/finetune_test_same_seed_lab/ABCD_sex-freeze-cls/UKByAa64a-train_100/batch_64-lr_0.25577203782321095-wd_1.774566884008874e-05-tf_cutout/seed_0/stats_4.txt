"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 598.6016464233398, "training_acc": 47.0, "val_loss": 63.38040828704834, "val_acc": 48.0}
{"epoch": 1, "training_loss": 515.5185508728027, "training_acc": 50.0, "val_loss": 356.8375825881958, "val_acc": 52.0}
{"epoch": 2, "training_loss": 1368.619270324707, "training_acc": 53.0, "val_loss": 238.76874446868896, "val_acc": 52.0}
{"epoch": 3, "training_loss": 721.8527698516846, "training_acc": 53.0, "val_loss": 109.7716212272644, "val_acc": 48.0}
{"epoch": 4, "training_loss": 612.4872398376465, "training_acc": 47.0, "val_loss": 220.51968574523926, "val_acc": 48.0}
{"epoch": 5, "training_loss": 747.0492877960205, "training_acc": 47.0, "val_loss": 45.43796479701996, "val_acc": 44.0}
{"epoch": 6, "training_loss": 295.2297840118408, "training_acc": 48.0, "val_loss": 172.34069108963013, "val_acc": 52.0}
{"epoch": 7, "training_loss": 698.1047325134277, "training_acc": 53.0, "val_loss": 159.82890129089355, "val_acc": 52.0}
{"epoch": 8, "training_loss": 537.5660667419434, "training_acc": 53.0, "val_loss": 26.162037253379822, "val_acc": 56.0}
{"epoch": 9, "training_loss": 176.19547271728516, "training_acc": 51.0, "val_loss": 125.68840980529785, "val_acc": 48.0}
{"epoch": 10, "training_loss": 462.48088359832764, "training_acc": 46.0, "val_loss": 35.2224200963974, "val_acc": 44.0}
{"epoch": 11, "training_loss": 161.32671070098877, "training_acc": 50.0, "val_loss": 85.73962450027466, "val_acc": 52.0}
{"epoch": 12, "training_loss": 336.12664699554443, "training_acc": 53.0, "val_loss": 28.825873136520386, "val_acc": 52.0}
{"epoch": 13, "training_loss": 117.8176965713501, "training_acc": 58.0, "val_loss": 73.60499501228333, "val_acc": 48.0}
{"epoch": 14, "training_loss": 243.84594631195068, "training_acc": 47.0, "val_loss": 18.693141639232635, "val_acc": 52.0}
{"epoch": 15, "training_loss": 133.68555545806885, "training_acc": 60.0, "val_loss": 52.90791392326355, "val_acc": 52.0}
{"epoch": 16, "training_loss": 165.3713710308075, "training_acc": 55.0, "val_loss": 45.43003737926483, "val_acc": 48.0}
{"epoch": 17, "training_loss": 177.08247756958008, "training_acc": 47.0, "val_loss": 22.384971380233765, "val_acc": 44.0}
{"epoch": 18, "training_loss": 99.23775148391724, "training_acc": 57.0, "val_loss": 46.31746709346771, "val_acc": 52.0}
{"epoch": 19, "training_loss": 135.70511865615845, "training_acc": 56.0, "val_loss": 33.229637145996094, "val_acc": 48.0}
{"epoch": 20, "training_loss": 126.41219425201416, "training_acc": 50.0, "val_loss": 25.139975547790527, "val_acc": 44.0}
{"epoch": 21, "training_loss": 93.64919853210449, "training_acc": 56.0, "val_loss": 40.10500907897949, "val_acc": 52.0}
{"epoch": 22, "training_loss": 128.55898904800415, "training_acc": 53.0, "val_loss": 32.797402143478394, "val_acc": 48.0}
{"epoch": 23, "training_loss": 130.55523777008057, "training_acc": 46.0, "val_loss": 18.51116567850113, "val_acc": 60.0}
{"epoch": 24, "training_loss": 104.72599983215332, "training_acc": 57.0, "val_loss": 28.634566068649292, "val_acc": 52.0}
{"epoch": 25, "training_loss": 99.62762975692749, "training_acc": 54.0, "val_loss": 34.98679995536804, "val_acc": 48.0}
{"epoch": 26, "training_loss": 102.46926808357239, "training_acc": 46.0, "val_loss": 30.932486057281494, "val_acc": 52.0}
{"epoch": 27, "training_loss": 117.88521337509155, "training_acc": 53.0, "val_loss": 18.08764934539795, "val_acc": 52.0}
{"epoch": 28, "training_loss": 65.29480051994324, "training_acc": 70.0, "val_loss": 29.799646139144897, "val_acc": 48.0}
{"epoch": 29, "training_loss": 87.05779337882996, "training_acc": 53.0, "val_loss": 26.46825611591339, "val_acc": 52.0}
{"epoch": 30, "training_loss": 89.82450604438782, "training_acc": 53.0, "val_loss": 23.88429194688797, "val_acc": 44.0}
{"epoch": 31, "training_loss": 96.39235162734985, "training_acc": 46.0, "val_loss": 18.207161128520966, "val_acc": 52.0}
{"epoch": 32, "training_loss": 81.00223398208618, "training_acc": 62.0, "val_loss": 18.62303465604782, "val_acc": 52.0}
{"epoch": 33, "training_loss": 74.69081449508667, "training_acc": 58.0, "val_loss": 23.293490707874298, "val_acc": 48.0}
{"epoch": 34, "training_loss": 79.60005688667297, "training_acc": 50.0, "val_loss": 26.132407784461975, "val_acc": 52.0}
{"epoch": 35, "training_loss": 79.3063440322876, "training_acc": 56.0, "val_loss": 23.50495159626007, "val_acc": 48.0}
{"epoch": 36, "training_loss": 81.69391083717346, "training_acc": 55.0, "val_loss": 27.524960041046143, "val_acc": 52.0}
{"epoch": 37, "training_loss": 92.45427441596985, "training_acc": 53.0, "val_loss": 20.175479352474213, "val_acc": 44.0}
{"epoch": 38, "training_loss": 77.8338475227356, "training_acc": 51.0, "val_loss": 17.240187525749207, "val_acc": 52.0}
{"epoch": 39, "training_loss": 64.17787957191467, "training_acc": 61.0, "val_loss": 17.891879379749298, "val_acc": 52.0}
{"epoch": 40, "training_loss": 61.37207102775574, "training_acc": 64.0, "val_loss": 20.175588130950928, "val_acc": 40.0}
{"epoch": 41, "training_loss": 63.65621256828308, "training_acc": 60.0, "val_loss": 25.654307007789612, "val_acc": 52.0}
{"epoch": 42, "training_loss": 76.39242148399353, "training_acc": 63.0, "val_loss": 21.208888292312622, "val_acc": 44.0}
{"epoch": 43, "training_loss": 74.56735849380493, "training_acc": 62.0, "val_loss": 21.118968725204468, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.98537397384644, "training_acc": 58.0, "val_loss": 19.08520609140396, "val_acc": 60.0}
{"epoch": 45, "training_loss": 69.27212858200073, "training_acc": 55.0, "val_loss": 21.983565390110016, "val_acc": 52.0}
{"epoch": 46, "training_loss": 71.36015772819519, "training_acc": 58.0, "val_loss": 16.92713499069214, "val_acc": 52.0}
{"epoch": 47, "training_loss": 61.17758131027222, "training_acc": 64.0, "val_loss": 17.085807025432587, "val_acc": 56.0}
{"epoch": 48, "training_loss": 67.88725733757019, "training_acc": 59.0, "val_loss": 17.741309106349945, "val_acc": 68.0}
{"epoch": 49, "training_loss": 61.932408571243286, "training_acc": 64.0, "val_loss": 17.60174185037613, "val_acc": 52.0}
{"epoch": 50, "training_loss": 60.0638792514801, "training_acc": 66.0, "val_loss": 17.53322035074234, "val_acc": 56.0}
{"epoch": 51, "training_loss": 59.02824354171753, "training_acc": 69.0, "val_loss": 17.665483057498932, "val_acc": 52.0}
{"epoch": 52, "training_loss": 57.00971722602844, "training_acc": 68.0, "val_loss": 18.539530038833618, "val_acc": 52.0}
{"epoch": 53, "training_loss": 59.85472869873047, "training_acc": 65.0, "val_loss": 21.699225902557373, "val_acc": 52.0}
{"epoch": 54, "training_loss": 68.87041640281677, "training_acc": 56.0, "val_loss": 17.276935279369354, "val_acc": 64.0}
{"epoch": 55, "training_loss": 60.642737865448, "training_acc": 70.0, "val_loss": 17.446479201316833, "val_acc": 52.0}
{"epoch": 56, "training_loss": 58.10089993476868, "training_acc": 75.0, "val_loss": 17.373670637607574, "val_acc": 64.0}
{"epoch": 57, "training_loss": 60.43523859977722, "training_acc": 66.0, "val_loss": 20.92110812664032, "val_acc": 52.0}
{"epoch": 58, "training_loss": 68.10244035720825, "training_acc": 59.0, "val_loss": 20.222564041614532, "val_acc": 48.0}
{"epoch": 59, "training_loss": 68.39108610153198, "training_acc": 58.0, "val_loss": 17.293450236320496, "val_acc": 56.0}
{"epoch": 60, "training_loss": 53.624167919158936, "training_acc": 72.0, "val_loss": 18.426623940467834, "val_acc": 48.0}
{"epoch": 61, "training_loss": 60.011945724487305, "training_acc": 69.0, "val_loss": 17.16785430908203, "val_acc": 56.0}
{"epoch": 62, "training_loss": 70.57783317565918, "training_acc": 59.0, "val_loss": 23.075823485851288, "val_acc": 48.0}
{"epoch": 63, "training_loss": 73.77912473678589, "training_acc": 54.0, "val_loss": 18.232177197933197, "val_acc": 52.0}
{"epoch": 64, "training_loss": 60.19709372520447, "training_acc": 68.0, "val_loss": 18.734164535999298, "val_acc": 52.0}
{"epoch": 65, "training_loss": 57.415117025375366, "training_acc": 68.0, "val_loss": 16.920126974582672, "val_acc": 56.0}
{"epoch": 66, "training_loss": 54.99888586997986, "training_acc": 76.0, "val_loss": 19.47469264268875, "val_acc": 40.0}
{"epoch": 67, "training_loss": 60.78635311126709, "training_acc": 63.0, "val_loss": 22.912345826625824, "val_acc": 52.0}
{"epoch": 68, "training_loss": 63.097491979599, "training_acc": 59.0, "val_loss": 19.71520185470581, "val_acc": 48.0}
{"epoch": 69, "training_loss": 60.537776708602905, "training_acc": 63.0, "val_loss": 21.455462276935577, "val_acc": 52.0}
{"epoch": 70, "training_loss": 54.39538609981537, "training_acc": 71.0, "val_loss": 20.9649920463562, "val_acc": 44.0}
{"epoch": 71, "training_loss": 60.148483753204346, "training_acc": 66.0, "val_loss": 25.36107897758484, "val_acc": 52.0}
{"epoch": 72, "training_loss": 68.575368642807, "training_acc": 58.0, "val_loss": 30.103564262390137, "val_acc": 48.0}
{"epoch": 73, "training_loss": 93.72862958908081, "training_acc": 53.0, "val_loss": 35.98019182682037, "val_acc": 52.0}
{"epoch": 74, "training_loss": 106.51888298988342, "training_acc": 53.0, "val_loss": 29.062160849571228, "val_acc": 48.0}
{"epoch": 75, "training_loss": 93.57350492477417, "training_acc": 48.0, "val_loss": 22.962775826454163, "val_acc": 52.0}
{"epoch": 76, "training_loss": 67.29848980903625, "training_acc": 58.0, "val_loss": 21.039053797721863, "val_acc": 44.0}
{"epoch": 77, "training_loss": 66.0912356376648, "training_acc": 54.0, "val_loss": 24.791650474071503, "val_acc": 52.0}
{"epoch": 78, "training_loss": 74.08956575393677, "training_acc": 56.0, "val_loss": 19.047027826309204, "val_acc": 56.0}
{"epoch": 79, "training_loss": 57.29084062576294, "training_acc": 67.0, "val_loss": 17.983879148960114, "val_acc": 56.0}
{"epoch": 80, "training_loss": 53.690064430236816, "training_acc": 75.0, "val_loss": 17.134369909763336, "val_acc": 56.0}
{"epoch": 81, "training_loss": 55.98921275138855, "training_acc": 72.0, "val_loss": 17.152172327041626, "val_acc": 56.0}
{"epoch": 82, "training_loss": 50.13382136821747, "training_acc": 79.0, "val_loss": 17.034196853637695, "val_acc": 68.0}
{"epoch": 83, "training_loss": 51.84931254386902, "training_acc": 79.0, "val_loss": 17.017993330955505, "val_acc": 56.0}
{"epoch": 84, "training_loss": 50.83663487434387, "training_acc": 79.0, "val_loss": 18.455804884433746, "val_acc": 52.0}
