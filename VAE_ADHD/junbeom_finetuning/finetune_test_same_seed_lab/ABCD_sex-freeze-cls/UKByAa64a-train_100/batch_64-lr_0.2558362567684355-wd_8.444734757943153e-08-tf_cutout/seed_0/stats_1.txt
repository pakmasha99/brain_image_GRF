"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 1509.2439270019531, "training_acc": 47.0, "val_loss": 171.4396595954895, "val_acc": 52.0}
{"epoch": 1, "training_loss": 862.4562683105469, "training_acc": 53.0, "val_loss": 64.07602429389954, "val_acc": 48.0}
{"epoch": 2, "training_loss": 185.2345666885376, "training_acc": 47.0, "val_loss": 302.49907970428467, "val_acc": 52.0}
{"epoch": 3, "training_loss": 1157.0034942626953, "training_acc": 53.0, "val_loss": 66.82004928588867, "val_acc": 52.0}
{"epoch": 4, "training_loss": 912.6858520507812, "training_acc": 41.0, "val_loss": 469.5540428161621, "val_acc": 48.0}
{"epoch": 5, "training_loss": 1689.6841926574707, "training_acc": 47.0, "val_loss": 23.086200654506683, "val_acc": 52.0}
{"epoch": 6, "training_loss": 305.3616714477539, "training_acc": 53.0, "val_loss": 213.02525997161865, "val_acc": 52.0}
{"epoch": 7, "training_loss": 605.8991250991821, "training_acc": 53.0, "val_loss": 234.26439762115479, "val_acc": 48.0}
{"epoch": 8, "training_loss": 1102.4290885925293, "training_acc": 47.0, "val_loss": 238.26866149902344, "val_acc": 48.0}
{"epoch": 9, "training_loss": 755.0979089736938, "training_acc": 41.0, "val_loss": 105.33993244171143, "val_acc": 52.0}
{"epoch": 10, "training_loss": 302.5876383781433, "training_acc": 53.0, "val_loss": 140.06158113479614, "val_acc": 48.0}
{"epoch": 11, "training_loss": 553.7308921813965, "training_acc": 47.0, "val_loss": 62.43988275527954, "val_acc": 52.0}
{"epoch": 12, "training_loss": 272.94171810150146, "training_acc": 53.0, "val_loss": 24.386318027973175, "val_acc": 48.0}
{"epoch": 13, "training_loss": 94.63581109046936, "training_acc": 47.0, "val_loss": 74.97583031654358, "val_acc": 52.0}
{"epoch": 14, "training_loss": 259.97018480300903, "training_acc": 37.0, "val_loss": 33.92673432826996, "val_acc": 52.0}
{"epoch": 15, "training_loss": 114.13502264022827, "training_acc": 57.0, "val_loss": 24.53484982252121, "val_acc": 52.0}
{"epoch": 16, "training_loss": 74.08765053749084, "training_acc": 62.0, "val_loss": 31.85552954673767, "val_acc": 48.0}
{"epoch": 17, "training_loss": 210.6902437210083, "training_acc": 47.0, "val_loss": 39.82032835483551, "val_acc": 52.0}
{"epoch": 18, "training_loss": 330.65196990966797, "training_acc": 47.0, "val_loss": 125.82652568817139, "val_acc": 48.0}
{"epoch": 19, "training_loss": 409.24945735931396, "training_acc": 47.0, "val_loss": 65.27878642082214, "val_acc": 52.0}
{"epoch": 20, "training_loss": 260.03265476226807, "training_acc": 49.0, "val_loss": 21.149402856826782, "val_acc": 48.0}
{"epoch": 21, "training_loss": 189.11914539337158, "training_acc": 51.0, "val_loss": 86.36183142662048, "val_acc": 52.0}
{"epoch": 22, "training_loss": 369.0228729248047, "training_acc": 45.0, "val_loss": 53.433823585510254, "val_acc": 48.0}
{"epoch": 23, "training_loss": 325.13784408569336, "training_acc": 47.0, "val_loss": 135.9278678894043, "val_acc": 52.0}
{"epoch": 24, "training_loss": 355.48577880859375, "training_acc": 53.0, "val_loss": 190.30107259750366, "val_acc": 48.0}
{"epoch": 25, "training_loss": 890.4044761657715, "training_acc": 47.0, "val_loss": 101.82149410247803, "val_acc": 48.0}
{"epoch": 26, "training_loss": 480.67759704589844, "training_acc": 51.0, "val_loss": 283.907151222229, "val_acc": 52.0}
{"epoch": 27, "training_loss": 1061.0004768371582, "training_acc": 53.0, "val_loss": 105.04801273345947, "val_acc": 52.0}
{"epoch": 28, "training_loss": 471.32636070251465, "training_acc": 53.0, "val_loss": 262.14985847473145, "val_acc": 48.0}
{"epoch": 29, "training_loss": 974.9132308959961, "training_acc": 47.0, "val_loss": 26.658141613006592, "val_acc": 52.0}
{"epoch": 30, "training_loss": 299.4684810638428, "training_acc": 53.0, "val_loss": 55.258142948150635, "val_acc": 52.0}
{"epoch": 31, "training_loss": 333.95730209350586, "training_acc": 53.0, "val_loss": 183.73565673828125, "val_acc": 48.0}
{"epoch": 32, "training_loss": 569.3434734344482, "training_acc": 47.0, "val_loss": 188.3048176765442, "val_acc": 52.0}
{"epoch": 33, "training_loss": 856.315502166748, "training_acc": 53.0, "val_loss": 233.52982997894287, "val_acc": 52.0}
{"epoch": 34, "training_loss": 662.0411596298218, "training_acc": 53.0, "val_loss": 234.10146236419678, "val_acc": 48.0}
{"epoch": 35, "training_loss": 1143.8868598937988, "training_acc": 47.0, "val_loss": 298.8840341567993, "val_acc": 48.0}
{"epoch": 36, "training_loss": 902.3782749176025, "training_acc": 47.0, "val_loss": 227.52723693847656, "val_acc": 52.0}
{"epoch": 37, "training_loss": 1088.4161987304688, "training_acc": 53.0, "val_loss": 407.6464653015137, "val_acc": 52.0}
{"epoch": 38, "training_loss": 1440.6145133972168, "training_acc": 53.0, "val_loss": 73.96594285964966, "val_acc": 52.0}
{"epoch": 39, "training_loss": 446.6226692199707, "training_acc": 59.0, "val_loss": 427.1631717681885, "val_acc": 48.0}
