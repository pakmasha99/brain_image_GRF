"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 83309.52390289307, "training_acc": 53.0, "val_loss": 234367.8466796875, "val_acc": 48.0}
{"epoch": 1, "training_loss": 946161.8671875, "training_acc": 47.0, "val_loss": 18380.804443359375, "val_acc": 44.0}
{"epoch": 2, "training_loss": 158755.15234375, "training_acc": 68.0, "val_loss": 226990.3076171875, "val_acc": 52.0}
{"epoch": 3, "training_loss": 704840.375, "training_acc": 53.0, "val_loss": 59425.3662109375, "val_acc": 52.0}
{"epoch": 4, "training_loss": 301172.73828125, "training_acc": 47.0, "val_loss": 177196.5576171875, "val_acc": 48.0}
{"epoch": 5, "training_loss": 715886.1875, "training_acc": 47.0, "val_loss": 47715.78063964844, "val_acc": 48.0}
{"epoch": 6, "training_loss": 246020.9501953125, "training_acc": 51.0, "val_loss": 172845.99609375, "val_acc": 52.0}
{"epoch": 7, "training_loss": 603736.849609375, "training_acc": 53.0, "val_loss": 136400.78125, "val_acc": 52.0}
{"epoch": 8, "training_loss": 344222.38134765625, "training_acc": 54.0, "val_loss": 81454.0771484375, "val_acc": 48.0}
{"epoch": 9, "training_loss": 425898.625, "training_acc": 47.0, "val_loss": 126269.5556640625, "val_acc": 48.0}
{"epoch": 10, "training_loss": 454826.2314453125, "training_acc": 47.0, "val_loss": 42821.905517578125, "val_acc": 52.0}
{"epoch": 11, "training_loss": 154943.4736328125, "training_acc": 54.0, "val_loss": 103914.90478515625, "val_acc": 52.0}
{"epoch": 12, "training_loss": 278182.15087890625, "training_acc": 53.0, "val_loss": 19036.380004882812, "val_acc": 48.0}
{"epoch": 13, "training_loss": 120276.54296875, "training_acc": 55.0, "val_loss": 25621.487426757812, "val_acc": 52.0}
{"epoch": 14, "training_loss": 141202.84326171875, "training_acc": 54.0, "val_loss": 75268.31665039062, "val_acc": 52.0}
{"epoch": 15, "training_loss": 188865.03076171875, "training_acc": 53.0, "val_loss": 21252.38037109375, "val_acc": 44.0}
{"epoch": 16, "training_loss": 79675.2890625, "training_acc": 57.0, "val_loss": 31352.34375, "val_acc": 52.0}
{"epoch": 17, "training_loss": 120156.07214355469, "training_acc": 55.0, "val_loss": 54976.08642578125, "val_acc": 52.0}
{"epoch": 18, "training_loss": 129532.705078125, "training_acc": 55.0, "val_loss": 16694.84405517578, "val_acc": 48.0}
{"epoch": 19, "training_loss": 82434.62548828125, "training_acc": 60.0, "val_loss": 19516.3818359375, "val_acc": 52.0}
{"epoch": 20, "training_loss": 124555.00732421875, "training_acc": 44.0, "val_loss": 53602.850341796875, "val_acc": 52.0}
{"epoch": 21, "training_loss": 109807.11865234375, "training_acc": 56.0, "val_loss": 12101.41372680664, "val_acc": 44.0}
{"epoch": 22, "training_loss": 64832.3837890625, "training_acc": 65.0, "val_loss": 23552.374267578125, "val_acc": 52.0}
{"epoch": 23, "training_loss": 55995.081298828125, "training_acc": 55.0, "val_loss": 16279.313659667969, "val_acc": 56.0}
{"epoch": 24, "training_loss": 68987.00561523438, "training_acc": 59.0, "val_loss": 38222.58605957031, "val_acc": 52.0}
{"epoch": 25, "training_loss": 81969.56372070312, "training_acc": 56.0, "val_loss": 17673.934936523438, "val_acc": 48.0}
{"epoch": 26, "training_loss": 74089.1015625, "training_acc": 51.0, "val_loss": 39988.32702636719, "val_acc": 52.0}
{"epoch": 27, "training_loss": 102916.43603515625, "training_acc": 55.0, "val_loss": 6203.640365600586, "val_acc": 60.0}
{"epoch": 28, "training_loss": 51515.896240234375, "training_acc": 51.0, "val_loss": 21822.59979248047, "val_acc": 52.0}
{"epoch": 29, "training_loss": 39471.018493652344, "training_acc": 54.0, "val_loss": 8328.042602539062, "val_acc": 60.0}
{"epoch": 30, "training_loss": 25743.859436035156, "training_acc": 61.0, "val_loss": 23024.267578125, "val_acc": 52.0}
{"epoch": 31, "training_loss": 28760.208251953125, "training_acc": 69.0, "val_loss": 7360.505676269531, "val_acc": 40.0}
{"epoch": 32, "training_loss": 21871.274047851562, "training_acc": 64.0, "val_loss": 22329.359436035156, "val_acc": 48.0}
{"epoch": 33, "training_loss": 66516.26705932617, "training_acc": 54.0, "val_loss": 36975.21057128906, "val_acc": 52.0}
{"epoch": 34, "training_loss": 86913.57794189453, "training_acc": 54.0, "val_loss": 35156.96105957031, "val_acc": 48.0}
{"epoch": 35, "training_loss": 131255.74609375, "training_acc": 47.0, "val_loss": 33107.51953125, "val_acc": 52.0}
{"epoch": 36, "training_loss": 116178.994140625, "training_acc": 53.0, "val_loss": 6556.292724609375, "val_acc": 60.0}
{"epoch": 37, "training_loss": 78352.6181640625, "training_acc": 60.0, "val_loss": 29886.990356445312, "val_acc": 48.0}
{"epoch": 38, "training_loss": 132025.48681640625, "training_acc": 45.0, "val_loss": 45284.82360839844, "val_acc": 52.0}
{"epoch": 39, "training_loss": 95426.74926757812, "training_acc": 59.0, "val_loss": 50907.525634765625, "val_acc": 48.0}
{"epoch": 40, "training_loss": 215492.890625, "training_acc": 47.0, "val_loss": 9397.14126586914, "val_acc": 48.0}
{"epoch": 41, "training_loss": 113355.107421875, "training_acc": 49.0, "val_loss": 99773.33984375, "val_acc": 52.0}
{"epoch": 42, "training_loss": 322723.3896484375, "training_acc": 53.0, "val_loss": 14347.268676757812, "val_acc": 52.0}
{"epoch": 43, "training_loss": 140414.40625, "training_acc": 60.0, "val_loss": 103595.9228515625, "val_acc": 48.0}
{"epoch": 44, "training_loss": 384237.4970703125, "training_acc": 47.0, "val_loss": 9286.28921508789, "val_acc": 48.0}
{"epoch": 45, "training_loss": 93934.3349609375, "training_acc": 65.0, "val_loss": 115843.7744140625, "val_acc": 52.0}
{"epoch": 46, "training_loss": 341490.8916015625, "training_acc": 53.0, "val_loss": 34668.44482421875, "val_acc": 52.0}
