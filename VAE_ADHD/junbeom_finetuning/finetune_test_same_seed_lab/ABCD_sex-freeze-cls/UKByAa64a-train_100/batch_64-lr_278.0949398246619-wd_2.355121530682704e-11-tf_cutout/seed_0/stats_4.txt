"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 747072.1913490295, "training_acc": 43.0, "val_loss": 133517.7978515625, "val_acc": 48.0}
{"epoch": 1, "training_loss": 717266.30859375, "training_acc": 43.0, "val_loss": 307958.837890625, "val_acc": 52.0}
{"epoch": 2, "training_loss": 1174383.15625, "training_acc": 53.0, "val_loss": 194131.11572265625, "val_acc": 52.0}
{"epoch": 3, "training_loss": 551237.4360351562, "training_acc": 53.0, "val_loss": 166749.6826171875, "val_acc": 48.0}
{"epoch": 4, "training_loss": 820492.9375, "training_acc": 47.0, "val_loss": 276895.4833984375, "val_acc": 48.0}
{"epoch": 5, "training_loss": 1001955.5078125, "training_acc": 47.0, "val_loss": 99775.54321289062, "val_acc": 48.0}
{"epoch": 6, "training_loss": 336411.2255859375, "training_acc": 49.0, "val_loss": 148695.4345703125, "val_acc": 52.0}
{"epoch": 7, "training_loss": 634470.21875, "training_acc": 53.0, "val_loss": 156211.80419921875, "val_acc": 52.0}
{"epoch": 8, "training_loss": 506370.4814453125, "training_acc": 53.0, "val_loss": 17674.378967285156, "val_acc": 44.0}
{"epoch": 9, "training_loss": 154875.1923828125, "training_acc": 51.0, "val_loss": 99065.91186523438, "val_acc": 48.0}
{"epoch": 10, "training_loss": 307213.423828125, "training_acc": 48.0, "val_loss": 28712.042236328125, "val_acc": 52.0}
{"epoch": 11, "training_loss": 178286.822265625, "training_acc": 53.0, "val_loss": 47690.4052734375, "val_acc": 52.0}
{"epoch": 12, "training_loss": 144501.38037109375, "training_acc": 55.0, "val_loss": 48622.33581542969, "val_acc": 48.0}
{"epoch": 13, "training_loss": 177937.05322265625, "training_acc": 48.0, "val_loss": 8583.563995361328, "val_acc": 44.0}
{"epoch": 14, "training_loss": 92652.98291015625, "training_acc": 56.0, "val_loss": 10774.476623535156, "val_acc": 52.0}
{"epoch": 15, "training_loss": 77661.18212890625, "training_acc": 55.0, "val_loss": 44266.51611328125, "val_acc": 48.0}
{"epoch": 16, "training_loss": 89374.61334228516, "training_acc": 53.0, "val_loss": 47576.068115234375, "val_acc": 52.0}
{"epoch": 17, "training_loss": 187152.25830078125, "training_acc": 53.0, "val_loss": 12146.876525878906, "val_acc": 52.0}
{"epoch": 18, "training_loss": 121993.1259765625, "training_acc": 52.0, "val_loss": 83452.54516601562, "val_acc": 48.0}
{"epoch": 19, "training_loss": 259944.4580078125, "training_acc": 47.0, "val_loss": 23602.50701904297, "val_acc": 52.0}
{"epoch": 20, "training_loss": 125897.10400390625, "training_acc": 53.0, "val_loss": 24563.345336914062, "val_acc": 52.0}
{"epoch": 21, "training_loss": 130050.46728515625, "training_acc": 45.0, "val_loss": 43771.13037109375, "val_acc": 48.0}
{"epoch": 22, "training_loss": 115939.74938964844, "training_acc": 52.0, "val_loss": 24982.745361328125, "val_acc": 52.0}
{"epoch": 23, "training_loss": 90659.56591796875, "training_acc": 56.0, "val_loss": 44731.71691894531, "val_acc": 48.0}
{"epoch": 24, "training_loss": 159299.7041015625, "training_acc": 47.0, "val_loss": 5014.310836791992, "val_acc": 64.0}
{"epoch": 25, "training_loss": 39330.2255859375, "training_acc": 60.0, "val_loss": 5197.356796264648, "val_acc": 60.0}
{"epoch": 26, "training_loss": 39101.30322265625, "training_acc": 54.0, "val_loss": 11770.384216308594, "val_acc": 48.0}
{"epoch": 27, "training_loss": 35931.652587890625, "training_acc": 61.0, "val_loss": 20915.41748046875, "val_acc": 48.0}
{"epoch": 28, "training_loss": 50301.070556640625, "training_acc": 55.0, "val_loss": 12598.876953125, "val_acc": 52.0}
{"epoch": 29, "training_loss": 29043.70962524414, "training_acc": 65.0, "val_loss": 17376.58233642578, "val_acc": 48.0}
{"epoch": 30, "training_loss": 58046.40234375, "training_acc": 51.0, "val_loss": 7772.880554199219, "val_acc": 48.0}
{"epoch": 31, "training_loss": 58757.42724609375, "training_acc": 54.0, "val_loss": 17546.156311035156, "val_acc": 48.0}
{"epoch": 32, "training_loss": 98300.77587890625, "training_acc": 50.0, "val_loss": 52990.97900390625, "val_acc": 52.0}
{"epoch": 33, "training_loss": 161057.37353515625, "training_acc": 54.0, "val_loss": 50116.00036621094, "val_acc": 48.0}
{"epoch": 34, "training_loss": 180879.32373046875, "training_acc": 47.0, "val_loss": 19422.53875732422, "val_acc": 44.0}
{"epoch": 35, "training_loss": 142248.0791015625, "training_acc": 48.0, "val_loss": 75351.06811523438, "val_acc": 52.0}
{"epoch": 36, "training_loss": 250999.15625, "training_acc": 53.0, "val_loss": 10689.24560546875, "val_acc": 44.0}
{"epoch": 37, "training_loss": 62639.28369140625, "training_acc": 50.0, "val_loss": 17477.40936279297, "val_acc": 48.0}
{"epoch": 38, "training_loss": 79388.04541015625, "training_acc": 54.0, "val_loss": 47275.62255859375, "val_acc": 52.0}
{"epoch": 39, "training_loss": 128052.66101074219, "training_acc": 53.0, "val_loss": 61011.663818359375, "val_acc": 48.0}
{"epoch": 40, "training_loss": 239660.7509765625, "training_acc": 47.0, "val_loss": 43725.27160644531, "val_acc": 48.0}
{"epoch": 41, "training_loss": 132650.42236328125, "training_acc": 49.0, "val_loss": 46970.02868652344, "val_acc": 52.0}
{"epoch": 42, "training_loss": 134266.2861328125, "training_acc": 53.0, "val_loss": 40791.56799316406, "val_acc": 48.0}
{"epoch": 43, "training_loss": 166823.900390625, "training_acc": 47.0, "val_loss": 7129.3914794921875, "val_acc": 48.0}
