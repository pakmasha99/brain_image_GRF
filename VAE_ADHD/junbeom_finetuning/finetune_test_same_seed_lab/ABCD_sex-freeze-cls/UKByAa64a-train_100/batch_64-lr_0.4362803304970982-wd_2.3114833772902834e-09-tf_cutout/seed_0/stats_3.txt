"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 866.3315010070801, "training_acc": 49.0, "val_loss": 195.033597946167, "val_acc": 52.0}
{"epoch": 1, "training_loss": 1068.8902435302734, "training_acc": 47.0, "val_loss": 467.0701026916504, "val_acc": 48.0}
{"epoch": 2, "training_loss": 1801.5807037353516, "training_acc": 47.0, "val_loss": 80.40049076080322, "val_acc": 48.0}
{"epoch": 3, "training_loss": 622.4023246765137, "training_acc": 45.0, "val_loss": 387.46697902679443, "val_acc": 52.0}
{"epoch": 4, "training_loss": 1380.0959739685059, "training_acc": 53.0, "val_loss": 307.87997245788574, "val_acc": 52.0}
{"epoch": 5, "training_loss": 820.7281169891357, "training_acc": 53.0, "val_loss": 100.32663345336914, "val_acc": 48.0}
{"epoch": 6, "training_loss": 646.4264030456543, "training_acc": 47.0, "val_loss": 204.33483123779297, "val_acc": 48.0}
{"epoch": 7, "training_loss": 782.1434984207153, "training_acc": 47.0, "val_loss": 90.1927649974823, "val_acc": 52.0}
{"epoch": 8, "training_loss": 340.3797836303711, "training_acc": 52.0, "val_loss": 158.40755701065063, "val_acc": 52.0}
{"epoch": 9, "training_loss": 405.1120433807373, "training_acc": 53.0, "val_loss": 60.48697233200073, "val_acc": 48.0}
{"epoch": 10, "training_loss": 373.1835746765137, "training_acc": 47.0, "val_loss": 73.92646670341492, "val_acc": 48.0}
{"epoch": 11, "training_loss": 292.79297065734863, "training_acc": 49.0, "val_loss": 106.49508237838745, "val_acc": 52.0}
{"epoch": 12, "training_loss": 285.498122215271, "training_acc": 53.0, "val_loss": 26.92003846168518, "val_acc": 44.0}
{"epoch": 13, "training_loss": 165.27690601348877, "training_acc": 48.0, "val_loss": 34.36724841594696, "val_acc": 48.0}
{"epoch": 14, "training_loss": 176.71872234344482, "training_acc": 50.0, "val_loss": 79.5581042766571, "val_acc": 52.0}
{"epoch": 15, "training_loss": 172.08398461341858, "training_acc": 57.0, "val_loss": 49.54148828983307, "val_acc": 48.0}
{"epoch": 16, "training_loss": 243.93485832214355, "training_acc": 47.0, "val_loss": 30.782979726791382, "val_acc": 52.0}
{"epoch": 17, "training_loss": 130.03277683258057, "training_acc": 53.0, "val_loss": 33.72651934623718, "val_acc": 52.0}
{"epoch": 18, "training_loss": 94.03924703598022, "training_acc": 63.0, "val_loss": 46.577903628349304, "val_acc": 48.0}
{"epoch": 19, "training_loss": 158.96144080162048, "training_acc": 57.0, "val_loss": 67.68406629562378, "val_acc": 52.0}
{"epoch": 20, "training_loss": 222.27252960205078, "training_acc": 53.0, "val_loss": 20.9261491894722, "val_acc": 56.0}
{"epoch": 21, "training_loss": 117.22329235076904, "training_acc": 49.0, "val_loss": 24.01769906282425, "val_acc": 52.0}
{"epoch": 22, "training_loss": 95.79936695098877, "training_acc": 57.0, "val_loss": 40.061718225479126, "val_acc": 52.0}
{"epoch": 23, "training_loss": 86.74009418487549, "training_acc": 60.0, "val_loss": 21.11264318227768, "val_acc": 48.0}
{"epoch": 24, "training_loss": 99.98646116256714, "training_acc": 55.0, "val_loss": 41.83885157108307, "val_acc": 52.0}
{"epoch": 25, "training_loss": 79.18623423576355, "training_acc": 62.0, "val_loss": 30.34430742263794, "val_acc": 48.0}
{"epoch": 26, "training_loss": 117.97356104850769, "training_acc": 51.0, "val_loss": 43.100592494010925, "val_acc": 52.0}
{"epoch": 27, "training_loss": 88.08741855621338, "training_acc": 63.0, "val_loss": 37.425497174263, "val_acc": 48.0}
{"epoch": 28, "training_loss": 129.2622766494751, "training_acc": 52.0, "val_loss": 51.242005825042725, "val_acc": 52.0}
{"epoch": 29, "training_loss": 154.13535070419312, "training_acc": 53.0, "val_loss": 40.590521693229675, "val_acc": 48.0}
{"epoch": 30, "training_loss": 172.8749074935913, "training_acc": 48.0, "val_loss": 40.011513233184814, "val_acc": 52.0}
{"epoch": 31, "training_loss": 147.46657180786133, "training_acc": 53.0, "val_loss": 16.426607966423035, "val_acc": 56.0}
{"epoch": 32, "training_loss": 89.99383211135864, "training_acc": 64.0, "val_loss": 16.893285512924194, "val_acc": 52.0}
{"epoch": 33, "training_loss": 89.12580490112305, "training_acc": 62.0, "val_loss": 34.00876820087433, "val_acc": 52.0}
{"epoch": 34, "training_loss": 122.83264970779419, "training_acc": 51.0, "val_loss": 22.06122577190399, "val_acc": 48.0}
{"epoch": 35, "training_loss": 167.76777362823486, "training_acc": 46.0, "val_loss": 62.348949909210205, "val_acc": 52.0}
{"epoch": 36, "training_loss": 145.4057593345642, "training_acc": 50.0, "val_loss": 27.288299798965454, "val_acc": 48.0}
{"epoch": 37, "training_loss": 142.6158585548401, "training_acc": 41.0, "val_loss": 36.92455589771271, "val_acc": 52.0}
{"epoch": 38, "training_loss": 84.79824495315552, "training_acc": 53.0, "val_loss": 17.530229687690735, "val_acc": 56.0}
{"epoch": 39, "training_loss": 92.10803937911987, "training_acc": 57.0, "val_loss": 24.479876458644867, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.37780857086182, "training_acc": 67.0, "val_loss": 19.006669521331787, "val_acc": 52.0}
{"epoch": 41, "training_loss": 74.8838722705841, "training_acc": 66.0, "val_loss": 30.997800827026367, "val_acc": 52.0}
{"epoch": 42, "training_loss": 75.38230919837952, "training_acc": 65.0, "val_loss": 18.013261258602142, "val_acc": 48.0}
{"epoch": 43, "training_loss": 56.714401721954346, "training_acc": 72.0, "val_loss": 25.87742507457733, "val_acc": 52.0}
{"epoch": 44, "training_loss": 59.25416898727417, "training_acc": 68.0, "val_loss": 16.547252237796783, "val_acc": 56.0}
{"epoch": 45, "training_loss": 61.21751093864441, "training_acc": 69.0, "val_loss": 18.19055825471878, "val_acc": 72.0}
{"epoch": 46, "training_loss": 75.1409261226654, "training_acc": 61.0, "val_loss": 31.274086236953735, "val_acc": 52.0}
{"epoch": 47, "training_loss": 82.22719740867615, "training_acc": 62.0, "val_loss": 25.288769602775574, "val_acc": 48.0}
{"epoch": 48, "training_loss": 75.66814386844635, "training_acc": 65.0, "val_loss": 44.91142928600311, "val_acc": 52.0}
{"epoch": 49, "training_loss": 107.49683260917664, "training_acc": 56.0, "val_loss": 17.68767386674881, "val_acc": 56.0}
{"epoch": 50, "training_loss": 60.59344434738159, "training_acc": 69.0, "val_loss": 22.255972027778625, "val_acc": 52.0}
