"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 235511.53691101074, "training_acc": 51.0, "val_loss": 44154.03137207031, "val_acc": 52.0}
{"epoch": 1, "training_loss": 259989.017578125, "training_acc": 55.0, "val_loss": 165271.74072265625, "val_acc": 48.0}
{"epoch": 2, "training_loss": 627181.064453125, "training_acc": 47.0, "val_loss": 50011.822509765625, "val_acc": 48.0}
{"epoch": 3, "training_loss": 186442.796875, "training_acc": 53.0, "val_loss": 104203.857421875, "val_acc": 52.0}
{"epoch": 4, "training_loss": 413268.599609375, "training_acc": 53.0, "val_loss": 92908.01391601562, "val_acc": 52.0}
{"epoch": 5, "training_loss": 281791.05908203125, "training_acc": 53.0, "val_loss": 27655.209350585938, "val_acc": 48.0}
{"epoch": 6, "training_loss": 182601.529296875, "training_acc": 47.0, "val_loss": 58376.617431640625, "val_acc": 48.0}
{"epoch": 7, "training_loss": 193419.05712890625, "training_acc": 47.0, "val_loss": 32314.059448242188, "val_acc": 52.0}
{"epoch": 8, "training_loss": 149557.173828125, "training_acc": 53.0, "val_loss": 53900.091552734375, "val_acc": 52.0}
{"epoch": 9, "training_loss": 169832.10791015625, "training_acc": 53.0, "val_loss": 12707.722473144531, "val_acc": 48.0}
{"epoch": 10, "training_loss": 71081.04150390625, "training_acc": 48.0, "val_loss": 14833.711242675781, "val_acc": 48.0}
{"epoch": 11, "training_loss": 69116.46606445312, "training_acc": 48.0, "val_loss": 29262.237548828125, "val_acc": 52.0}
{"epoch": 12, "training_loss": 90475.75463867188, "training_acc": 52.0, "val_loss": 11229.493713378906, "val_acc": 48.0}
{"epoch": 13, "training_loss": 66117.35620117188, "training_acc": 49.0, "val_loss": 5968.607711791992, "val_acc": 44.0}
{"epoch": 14, "training_loss": 28225.623046875, "training_acc": 61.0, "val_loss": 26291.506958007812, "val_acc": 52.0}
{"epoch": 15, "training_loss": 75169.67749023438, "training_acc": 53.0, "val_loss": 19384.89532470703, "val_acc": 48.0}
{"epoch": 16, "training_loss": 82972.13354492188, "training_acc": 48.0, "val_loss": 9786.097717285156, "val_acc": 48.0}
{"epoch": 17, "training_loss": 47378.53125, "training_acc": 57.0, "val_loss": 29047.897338867188, "val_acc": 52.0}
{"epoch": 18, "training_loss": 85775.12097167969, "training_acc": 56.0, "val_loss": 11801.935577392578, "val_acc": 48.0}
{"epoch": 19, "training_loss": 57908.856201171875, "training_acc": 49.0, "val_loss": 4112.36572265625, "val_acc": 44.0}
{"epoch": 20, "training_loss": 35325.21044921875, "training_acc": 57.0, "val_loss": 16975.218200683594, "val_acc": 52.0}
{"epoch": 21, "training_loss": 49248.22412109375, "training_acc": 51.0, "val_loss": 13665.863037109375, "val_acc": 48.0}
{"epoch": 22, "training_loss": 42553.99856567383, "training_acc": 53.0, "val_loss": 7299.871063232422, "val_acc": 48.0}
{"epoch": 23, "training_loss": 20875.049835205078, "training_acc": 58.0, "val_loss": 3111.845588684082, "val_acc": 68.0}
{"epoch": 24, "training_loss": 13151.83740234375, "training_acc": 63.0, "val_loss": 4663.926696777344, "val_acc": 56.0}
{"epoch": 25, "training_loss": 20373.532104492188, "training_acc": 57.0, "val_loss": 5715.273666381836, "val_acc": 44.0}
{"epoch": 26, "training_loss": 16306.342346191406, "training_acc": 65.0, "val_loss": 11863.286590576172, "val_acc": 52.0}
{"epoch": 27, "training_loss": 26942.37026977539, "training_acc": 59.0, "val_loss": 20249.12109375, "val_acc": 48.0}
{"epoch": 28, "training_loss": 72971.1416015625, "training_acc": 47.0, "val_loss": 9357.695007324219, "val_acc": 52.0}
{"epoch": 29, "training_loss": 30769.102783203125, "training_acc": 54.0, "val_loss": 7336.225891113281, "val_acc": 48.0}
{"epoch": 30, "training_loss": 24052.331909179688, "training_acc": 50.0, "val_loss": 10383.303833007812, "val_acc": 52.0}
{"epoch": 31, "training_loss": 25131.962829589844, "training_acc": 57.0, "val_loss": 3714.2772674560547, "val_acc": 48.0}
{"epoch": 32, "training_loss": 16575.20751953125, "training_acc": 58.0, "val_loss": 1783.9094161987305, "val_acc": 60.0}
{"epoch": 33, "training_loss": 12373.221801757812, "training_acc": 56.0, "val_loss": 10366.40396118164, "val_acc": 48.0}
{"epoch": 34, "training_loss": 26514.930465698242, "training_acc": 57.0, "val_loss": 14208.097839355469, "val_acc": 52.0}
{"epoch": 35, "training_loss": 34493.97106933594, "training_acc": 58.0, "val_loss": 10316.824340820312, "val_acc": 48.0}
{"epoch": 36, "training_loss": 31556.459533691406, "training_acc": 55.0, "val_loss": 12273.552703857422, "val_acc": 52.0}
{"epoch": 37, "training_loss": 32926.466369628906, "training_acc": 53.0, "val_loss": 7016.973114013672, "val_acc": 48.0}
{"epoch": 38, "training_loss": 22709.919677734375, "training_acc": 55.0, "val_loss": 4041.884231567383, "val_acc": 48.0}
{"epoch": 39, "training_loss": 22578.55078125, "training_acc": 61.0, "val_loss": 8104.325103759766, "val_acc": 48.0}
{"epoch": 40, "training_loss": 52208.171142578125, "training_acc": 45.0, "val_loss": 25341.004943847656, "val_acc": 52.0}
{"epoch": 41, "training_loss": 65823.27416992188, "training_acc": 53.0, "val_loss": 31374.859619140625, "val_acc": 48.0}
{"epoch": 42, "training_loss": 127684.751953125, "training_acc": 47.0, "val_loss": 27107.476806640625, "val_acc": 48.0}
{"epoch": 43, "training_loss": 95440.60668945312, "training_acc": 37.0, "val_loss": 18613.148498535156, "val_acc": 52.0}
{"epoch": 44, "training_loss": 51588.147552490234, "training_acc": 58.0, "val_loss": 25877.188110351562, "val_acc": 48.0}
{"epoch": 45, "training_loss": 99033.81982421875, "training_acc": 48.0, "val_loss": 9047.945404052734, "val_acc": 48.0}
{"epoch": 46, "training_loss": 66036.80859375, "training_acc": 47.0, "val_loss": 43296.380615234375, "val_acc": 52.0}
{"epoch": 47, "training_loss": 152619.5322265625, "training_acc": 53.0, "val_loss": 4180.338668823242, "val_acc": 56.0}
{"epoch": 48, "training_loss": 35469.49853515625, "training_acc": 69.0, "val_loss": 40362.701416015625, "val_acc": 48.0}
{"epoch": 49, "training_loss": 126570.85498046875, "training_acc": 49.0, "val_loss": 7309.799957275391, "val_acc": 52.0}
{"epoch": 50, "training_loss": 47908.778564453125, "training_acc": 59.0, "val_loss": 13717.120361328125, "val_acc": 52.0}
{"epoch": 51, "training_loss": 49063.154052734375, "training_acc": 50.0, "val_loss": 16207.339477539062, "val_acc": 48.0}
