"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 604.3305702209473, "training_acc": 47.0, "val_loss": 64.03253078460693, "val_acc": 48.0}
{"epoch": 1, "training_loss": 520.8141326904297, "training_acc": 50.0, "val_loss": 360.6226921081543, "val_acc": 52.0}
{"epoch": 2, "training_loss": 1383.2728424072266, "training_acc": 53.0, "val_loss": 241.54129028320312, "val_acc": 52.0}
{"epoch": 3, "training_loss": 730.6450805664062, "training_acc": 53.0, "val_loss": 110.37681102752686, "val_acc": 48.0}
{"epoch": 4, "training_loss": 616.5809669494629, "training_acc": 47.0, "val_loss": 222.19526767730713, "val_acc": 48.0}
{"epoch": 5, "training_loss": 752.2962589263916, "training_acc": 47.0, "val_loss": 45.32209038734436, "val_acc": 44.0}
{"epoch": 6, "training_loss": 297.4360828399658, "training_acc": 48.0, "val_loss": 174.3642807006836, "val_acc": 52.0}
{"epoch": 7, "training_loss": 706.0447883605957, "training_acc": 53.0, "val_loss": 161.45398616790771, "val_acc": 52.0}
{"epoch": 8, "training_loss": 542.8134460449219, "training_acc": 53.0, "val_loss": 26.475733518600464, "val_acc": 52.0}
{"epoch": 9, "training_loss": 178.03814601898193, "training_acc": 51.0, "val_loss": 126.6582727432251, "val_acc": 48.0}
{"epoch": 10, "training_loss": 465.7081298828125, "training_acc": 46.0, "val_loss": 34.890806674957275, "val_acc": 44.0}
{"epoch": 11, "training_loss": 162.10145473480225, "training_acc": 51.0, "val_loss": 86.91052198410034, "val_acc": 52.0}
{"epoch": 12, "training_loss": 340.37460803985596, "training_acc": 53.0, "val_loss": 28.90169322490692, "val_acc": 52.0}
{"epoch": 13, "training_loss": 118.92289113998413, "training_acc": 58.0, "val_loss": 74.70260262489319, "val_acc": 48.0}
{"epoch": 14, "training_loss": 247.45895671844482, "training_acc": 47.0, "val_loss": 18.739931285381317, "val_acc": 52.0}
{"epoch": 15, "training_loss": 134.78900337219238, "training_acc": 59.0, "val_loss": 53.49562168121338, "val_acc": 52.0}
{"epoch": 16, "training_loss": 166.9750485420227, "training_acc": 55.0, "val_loss": 46.03800177574158, "val_acc": 48.0}
{"epoch": 17, "training_loss": 179.45003509521484, "training_acc": 47.0, "val_loss": 22.54405915737152, "val_acc": 44.0}
{"epoch": 18, "training_loss": 100.15079736709595, "training_acc": 57.0, "val_loss": 47.01482355594635, "val_acc": 52.0}
{"epoch": 19, "training_loss": 137.74916315078735, "training_acc": 55.0, "val_loss": 33.55934023857117, "val_acc": 48.0}
{"epoch": 20, "training_loss": 127.81095504760742, "training_acc": 50.0, "val_loss": 25.425967574119568, "val_acc": 44.0}
{"epoch": 21, "training_loss": 94.62453508377075, "training_acc": 56.0, "val_loss": 40.70111811161041, "val_acc": 52.0}
{"epoch": 22, "training_loss": 130.38578939437866, "training_acc": 53.0, "val_loss": 33.14364552497864, "val_acc": 48.0}
{"epoch": 23, "training_loss": 132.15944528579712, "training_acc": 46.0, "val_loss": 18.58845353126526, "val_acc": 64.0}
{"epoch": 24, "training_loss": 105.69373559951782, "training_acc": 57.0, "val_loss": 29.12764847278595, "val_acc": 52.0}
{"epoch": 25, "training_loss": 100.79719090461731, "training_acc": 54.0, "val_loss": 35.37575900554657, "val_acc": 48.0}
{"epoch": 26, "training_loss": 103.6600112915039, "training_acc": 46.0, "val_loss": 31.142964959144592, "val_acc": 52.0}
{"epoch": 27, "training_loss": 118.96157312393188, "training_acc": 53.0, "val_loss": 18.18952113389969, "val_acc": 52.0}
{"epoch": 28, "training_loss": 65.60572361946106, "training_acc": 70.0, "val_loss": 30.247414112091064, "val_acc": 48.0}
{"epoch": 29, "training_loss": 87.98451566696167, "training_acc": 53.0, "val_loss": 26.786532998085022, "val_acc": 52.0}
{"epoch": 30, "training_loss": 90.84005308151245, "training_acc": 53.0, "val_loss": 24.103635549545288, "val_acc": 44.0}
{"epoch": 31, "training_loss": 97.40634489059448, "training_acc": 46.0, "val_loss": 18.211060762405396, "val_acc": 52.0}
{"epoch": 32, "training_loss": 81.49378490447998, "training_acc": 62.0, "val_loss": 18.710041046142578, "val_acc": 52.0}
{"epoch": 33, "training_loss": 75.19129633903503, "training_acc": 58.0, "val_loss": 23.533432185649872, "val_acc": 48.0}
{"epoch": 34, "training_loss": 80.27855014801025, "training_acc": 50.0, "val_loss": 26.360344886779785, "val_acc": 52.0}
{"epoch": 35, "training_loss": 79.80782198905945, "training_acc": 56.0, "val_loss": 23.689232766628265, "val_acc": 48.0}
{"epoch": 36, "training_loss": 82.10661816596985, "training_acc": 55.0, "val_loss": 27.779540419578552, "val_acc": 52.0}
{"epoch": 37, "training_loss": 93.14178156852722, "training_acc": 53.0, "val_loss": 20.30673325061798, "val_acc": 44.0}
{"epoch": 38, "training_loss": 78.30404329299927, "training_acc": 50.0, "val_loss": 17.27522313594818, "val_acc": 52.0}
{"epoch": 39, "training_loss": 64.30781602859497, "training_acc": 62.0, "val_loss": 17.833270132541656, "val_acc": 52.0}
{"epoch": 40, "training_loss": 61.36689019203186, "training_acc": 63.0, "val_loss": 20.069007575511932, "val_acc": 40.0}
{"epoch": 41, "training_loss": 63.393938064575195, "training_acc": 60.0, "val_loss": 25.583752989768982, "val_acc": 52.0}
{"epoch": 42, "training_loss": 76.10077929496765, "training_acc": 61.0, "val_loss": 21.227286756038666, "val_acc": 44.0}
{"epoch": 43, "training_loss": 74.46223425865173, "training_acc": 62.0, "val_loss": 21.21543437242508, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.97155618667603, "training_acc": 58.0, "val_loss": 19.21786069869995, "val_acc": 64.0}
{"epoch": 45, "training_loss": 68.91024541854858, "training_acc": 54.0, "val_loss": 22.421543300151825, "val_acc": 52.0}
{"epoch": 46, "training_loss": 72.43184232711792, "training_acc": 57.0, "val_loss": 16.913962364196777, "val_acc": 52.0}
{"epoch": 47, "training_loss": 60.93610143661499, "training_acc": 65.0, "val_loss": 17.092542350292206, "val_acc": 56.0}
{"epoch": 48, "training_loss": 67.20476698875427, "training_acc": 59.0, "val_loss": 17.92934685945511, "val_acc": 68.0}
{"epoch": 49, "training_loss": 62.89436745643616, "training_acc": 66.0, "val_loss": 17.605483531951904, "val_acc": 56.0}
{"epoch": 50, "training_loss": 59.91374635696411, "training_acc": 68.0, "val_loss": 17.531074583530426, "val_acc": 56.0}
{"epoch": 51, "training_loss": 58.941169023513794, "training_acc": 67.0, "val_loss": 17.82478839159012, "val_acc": 52.0}
{"epoch": 52, "training_loss": 56.86769413948059, "training_acc": 68.0, "val_loss": 18.871037662029266, "val_acc": 56.0}
{"epoch": 53, "training_loss": 60.25059175491333, "training_acc": 65.0, "val_loss": 22.212256491184235, "val_acc": 52.0}
{"epoch": 54, "training_loss": 70.18488550186157, "training_acc": 57.0, "val_loss": 17.40473061800003, "val_acc": 64.0}
{"epoch": 55, "training_loss": 60.71629071235657, "training_acc": 69.0, "val_loss": 17.583663761615753, "val_acc": 52.0}
{"epoch": 56, "training_loss": 58.139147996902466, "training_acc": 75.0, "val_loss": 17.498202621936798, "val_acc": 64.0}
{"epoch": 57, "training_loss": 60.13487505912781, "training_acc": 68.0, "val_loss": 21.306072175502777, "val_acc": 52.0}
{"epoch": 58, "training_loss": 68.55061984062195, "training_acc": 59.0, "val_loss": 20.603549480438232, "val_acc": 48.0}
{"epoch": 59, "training_loss": 69.68759298324585, "training_acc": 54.0, "val_loss": 17.330700159072876, "val_acc": 56.0}
{"epoch": 60, "training_loss": 53.62619137763977, "training_acc": 69.0, "val_loss": 18.33343207836151, "val_acc": 48.0}
{"epoch": 61, "training_loss": 59.68830370903015, "training_acc": 71.0, "val_loss": 17.179033160209656, "val_acc": 56.0}
{"epoch": 62, "training_loss": 69.92980647087097, "training_acc": 61.0, "val_loss": 23.486295342445374, "val_acc": 48.0}
{"epoch": 63, "training_loss": 74.97288513183594, "training_acc": 56.0, "val_loss": 18.388552963733673, "val_acc": 52.0}
{"epoch": 64, "training_loss": 60.926642417907715, "training_acc": 68.0, "val_loss": 18.660810589790344, "val_acc": 52.0}
{"epoch": 65, "training_loss": 57.11020493507385, "training_acc": 68.0, "val_loss": 16.920030117034912, "val_acc": 56.0}
