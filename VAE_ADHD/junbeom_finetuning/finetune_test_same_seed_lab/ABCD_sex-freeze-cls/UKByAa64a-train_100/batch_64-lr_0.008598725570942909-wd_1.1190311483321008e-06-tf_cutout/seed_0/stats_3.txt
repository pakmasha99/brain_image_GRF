"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 74.38312602043152, "training_acc": 52.0, "val_loss": 17.834873497486115, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.25815010070801, "training_acc": 55.0, "val_loss": 18.536841869354248, "val_acc": 52.0}
{"epoch": 2, "training_loss": 74.74015474319458, "training_acc": 47.0, "val_loss": 17.759700119495392, "val_acc": 52.0}
{"epoch": 3, "training_loss": 70.23862099647522, "training_acc": 49.0, "val_loss": 17.81488209962845, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.46201920509338, "training_acc": 54.0, "val_loss": 18.65542382001877, "val_acc": 52.0}
{"epoch": 5, "training_loss": 71.34276986122131, "training_acc": 53.0, "val_loss": 17.903858423233032, "val_acc": 52.0}
{"epoch": 6, "training_loss": 68.44900226593018, "training_acc": 53.0, "val_loss": 17.54612922668457, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.59217810630798, "training_acc": 46.0, "val_loss": 17.754003405570984, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.76038360595703, "training_acc": 47.0, "val_loss": 17.564140260219574, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.27630233764648, "training_acc": 53.0, "val_loss": 18.03390085697174, "val_acc": 52.0}
{"epoch": 10, "training_loss": 68.97745490074158, "training_acc": 53.0, "val_loss": 18.215057253837585, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.84698104858398, "training_acc": 53.0, "val_loss": 17.77970790863037, "val_acc": 52.0}
{"epoch": 12, "training_loss": 68.07112216949463, "training_acc": 53.0, "val_loss": 17.702695727348328, "val_acc": 52.0}
{"epoch": 13, "training_loss": 68.19516205787659, "training_acc": 61.0, "val_loss": 17.74180233478546, "val_acc": 52.0}
{"epoch": 14, "training_loss": 68.66823267936707, "training_acc": 55.0, "val_loss": 17.73022562265396, "val_acc": 52.0}
{"epoch": 15, "training_loss": 67.9516909122467, "training_acc": 60.0, "val_loss": 17.631347477436066, "val_acc": 52.0}
{"epoch": 16, "training_loss": 68.45143222808838, "training_acc": 61.0, "val_loss": 17.623205482959747, "val_acc": 52.0}
{"epoch": 17, "training_loss": 68.7604591846466, "training_acc": 58.0, "val_loss": 17.814502120018005, "val_acc": 52.0}
{"epoch": 18, "training_loss": 67.84623861312866, "training_acc": 55.0, "val_loss": 18.218649923801422, "val_acc": 52.0}
{"epoch": 19, "training_loss": 68.14242172241211, "training_acc": 54.0, "val_loss": 17.94392615556717, "val_acc": 52.0}
{"epoch": 20, "training_loss": 67.58525013923645, "training_acc": 53.0, "val_loss": 17.639857530593872, "val_acc": 52.0}
{"epoch": 21, "training_loss": 68.16195106506348, "training_acc": 60.0, "val_loss": 17.539767920970917, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.06108665466309, "training_acc": 55.0, "val_loss": 17.550872266292572, "val_acc": 52.0}
{"epoch": 23, "training_loss": 67.94319915771484, "training_acc": 55.0, "val_loss": 17.596925795078278, "val_acc": 52.0}
{"epoch": 24, "training_loss": 66.74980902671814, "training_acc": 63.0, "val_loss": 18.091270327568054, "val_acc": 52.0}
{"epoch": 25, "training_loss": 68.59524941444397, "training_acc": 52.0, "val_loss": 17.96397566795349, "val_acc": 52.0}
{"epoch": 26, "training_loss": 67.15450239181519, "training_acc": 56.0, "val_loss": 17.688293755054474, "val_acc": 52.0}
{"epoch": 27, "training_loss": 67.41203355789185, "training_acc": 59.0, "val_loss": 17.649810016155243, "val_acc": 52.0}
{"epoch": 28, "training_loss": 66.08235907554626, "training_acc": 64.0, "val_loss": 17.82568246126175, "val_acc": 52.0}
{"epoch": 29, "training_loss": 68.45745873451233, "training_acc": 59.0, "val_loss": 18.025346100330353, "val_acc": 52.0}
{"epoch": 30, "training_loss": 66.73706364631653, "training_acc": 57.0, "val_loss": 17.837026715278625, "val_acc": 52.0}
{"epoch": 31, "training_loss": 68.68297123908997, "training_acc": 50.0, "val_loss": 17.59006530046463, "val_acc": 52.0}
{"epoch": 32, "training_loss": 66.96330618858337, "training_acc": 62.0, "val_loss": 17.67546385526657, "val_acc": 52.0}
{"epoch": 33, "training_loss": 66.57419514656067, "training_acc": 59.0, "val_loss": 17.787589132785797, "val_acc": 52.0}
{"epoch": 34, "training_loss": 67.21716785430908, "training_acc": 55.0, "val_loss": 17.77331531047821, "val_acc": 52.0}
{"epoch": 35, "training_loss": 66.95999121665955, "training_acc": 58.0, "val_loss": 17.795516550540924, "val_acc": 52.0}
{"epoch": 36, "training_loss": 65.8215401172638, "training_acc": 66.0, "val_loss": 17.615731060504913, "val_acc": 52.0}
{"epoch": 37, "training_loss": 65.7501163482666, "training_acc": 65.0, "val_loss": 17.60113537311554, "val_acc": 52.0}
{"epoch": 38, "training_loss": 66.73791074752808, "training_acc": 55.0, "val_loss": 17.63337254524231, "val_acc": 52.0}
{"epoch": 39, "training_loss": 66.63654804229736, "training_acc": 65.0, "val_loss": 17.507845163345337, "val_acc": 52.0}
{"epoch": 40, "training_loss": 67.34666395187378, "training_acc": 66.0, "val_loss": 17.58895069360733, "val_acc": 52.0}
{"epoch": 41, "training_loss": 67.88368844985962, "training_acc": 53.0, "val_loss": 17.876335978507996, "val_acc": 52.0}
{"epoch": 42, "training_loss": 66.06476521492004, "training_acc": 57.0, "val_loss": 17.605029046535492, "val_acc": 52.0}
{"epoch": 43, "training_loss": 66.58347058296204, "training_acc": 60.0, "val_loss": 17.478980123996735, "val_acc": 52.0}
{"epoch": 44, "training_loss": 64.94515228271484, "training_acc": 75.0, "val_loss": 17.59524643421173, "val_acc": 52.0}
{"epoch": 45, "training_loss": 65.99950695037842, "training_acc": 65.0, "val_loss": 17.961910367012024, "val_acc": 52.0}
{"epoch": 46, "training_loss": 68.7786033153534, "training_acc": 51.0, "val_loss": 18.332596123218536, "val_acc": 52.0}
{"epoch": 47, "training_loss": 66.9380316734314, "training_acc": 61.0, "val_loss": 17.62147694826126, "val_acc": 52.0}
{"epoch": 48, "training_loss": 65.37242937088013, "training_acc": 63.0, "val_loss": 17.52726137638092, "val_acc": 52.0}
{"epoch": 49, "training_loss": 66.16196012496948, "training_acc": 61.0, "val_loss": 17.52604842185974, "val_acc": 52.0}
{"epoch": 50, "training_loss": 65.99545001983643, "training_acc": 65.0, "val_loss": 17.82374531030655, "val_acc": 52.0}
{"epoch": 51, "training_loss": 65.75876975059509, "training_acc": 64.0, "val_loss": 18.166136741638184, "val_acc": 52.0}
{"epoch": 52, "training_loss": 66.10278654098511, "training_acc": 56.0, "val_loss": 17.948018014431, "val_acc": 52.0}
{"epoch": 53, "training_loss": 65.99484992027283, "training_acc": 55.0, "val_loss": 17.685286700725555, "val_acc": 52.0}
{"epoch": 54, "training_loss": 64.559494972229, "training_acc": 69.0, "val_loss": 17.601434886455536, "val_acc": 52.0}
{"epoch": 55, "training_loss": 65.34826827049255, "training_acc": 63.0, "val_loss": 17.646391689777374, "val_acc": 52.0}
{"epoch": 56, "training_loss": 66.00132036209106, "training_acc": 64.0, "val_loss": 17.951244115829468, "val_acc": 52.0}
{"epoch": 57, "training_loss": 68.20971751213074, "training_acc": 57.0, "val_loss": 17.91190654039383, "val_acc": 52.0}
{"epoch": 58, "training_loss": 67.87951016426086, "training_acc": 62.0, "val_loss": 17.431429028511047, "val_acc": 52.0}
{"epoch": 59, "training_loss": 67.58188390731812, "training_acc": 61.0, "val_loss": 17.42417812347412, "val_acc": 52.0}
{"epoch": 60, "training_loss": 65.68209457397461, "training_acc": 63.0, "val_loss": 17.498083412647247, "val_acc": 52.0}
{"epoch": 61, "training_loss": 65.62700605392456, "training_acc": 70.0, "val_loss": 17.70061105489731, "val_acc": 52.0}
{"epoch": 62, "training_loss": 66.32085013389587, "training_acc": 60.0, "val_loss": 17.864185571670532, "val_acc": 52.0}
{"epoch": 63, "training_loss": 65.4866075515747, "training_acc": 62.0, "val_loss": 17.529453337192535, "val_acc": 52.0}
{"epoch": 64, "training_loss": 66.5501217842102, "training_acc": 61.0, "val_loss": 17.424795031547546, "val_acc": 52.0}
{"epoch": 65, "training_loss": 64.68707132339478, "training_acc": 72.0, "val_loss": 17.40562468767166, "val_acc": 52.0}
{"epoch": 66, "training_loss": 64.6117992401123, "training_acc": 68.0, "val_loss": 17.4626886844635, "val_acc": 52.0}
{"epoch": 67, "training_loss": 66.13912343978882, "training_acc": 62.0, "val_loss": 17.672812938690186, "val_acc": 52.0}
{"epoch": 68, "training_loss": 65.71336030960083, "training_acc": 68.0, "val_loss": 17.861488461494446, "val_acc": 52.0}
{"epoch": 69, "training_loss": 64.11714291572571, "training_acc": 61.0, "val_loss": 17.800673842430115, "val_acc": 52.0}
{"epoch": 70, "training_loss": 63.350624322891235, "training_acc": 67.0, "val_loss": 17.87846088409424, "val_acc": 52.0}
{"epoch": 71, "training_loss": 66.06751728057861, "training_acc": 61.0, "val_loss": 17.808127403259277, "val_acc": 52.0}
{"epoch": 72, "training_loss": 64.02284216880798, "training_acc": 71.0, "val_loss": 17.659683525562286, "val_acc": 52.0}
{"epoch": 73, "training_loss": 66.29037046432495, "training_acc": 66.0, "val_loss": 17.687685787677765, "val_acc": 52.0}
{"epoch": 74, "training_loss": 64.42211246490479, "training_acc": 71.0, "val_loss": 17.891810834407806, "val_acc": 52.0}
{"epoch": 75, "training_loss": 66.18941974639893, "training_acc": 61.0, "val_loss": 17.768535017967224, "val_acc": 52.0}
{"epoch": 76, "training_loss": 63.84752631187439, "training_acc": 69.0, "val_loss": 17.75049865245819, "val_acc": 52.0}
{"epoch": 77, "training_loss": 66.53173041343689, "training_acc": 62.0, "val_loss": 17.72853583097458, "val_acc": 52.0}
{"epoch": 78, "training_loss": 66.04880118370056, "training_acc": 67.0, "val_loss": 17.94448047876358, "val_acc": 52.0}
{"epoch": 79, "training_loss": 63.103442430496216, "training_acc": 68.0, "val_loss": 17.9300919175148, "val_acc": 52.0}
{"epoch": 80, "training_loss": 64.81113910675049, "training_acc": 64.0, "val_loss": 17.751462757587433, "val_acc": 52.0}
{"epoch": 81, "training_loss": 65.68615651130676, "training_acc": 64.0, "val_loss": 17.956842482089996, "val_acc": 52.0}
{"epoch": 82, "training_loss": 64.38509035110474, "training_acc": 65.0, "val_loss": 18.362419307231903, "val_acc": 52.0}
{"epoch": 83, "training_loss": 64.12477469444275, "training_acc": 61.0, "val_loss": 18.668849766254425, "val_acc": 52.0}
{"epoch": 84, "training_loss": 63.96548938751221, "training_acc": 70.0, "val_loss": 18.193355202674866, "val_acc": 52.0}
