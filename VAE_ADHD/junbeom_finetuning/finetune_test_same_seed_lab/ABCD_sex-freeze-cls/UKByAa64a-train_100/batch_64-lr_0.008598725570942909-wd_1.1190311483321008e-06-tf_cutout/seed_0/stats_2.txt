"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 72.41434073448181, "training_acc": 56.0, "val_loss": 17.510077357292175, "val_acc": 52.0}
{"epoch": 1, "training_loss": 71.05451512336731, "training_acc": 49.0, "val_loss": 18.295283615589142, "val_acc": 44.0}
{"epoch": 2, "training_loss": 72.91389894485474, "training_acc": 47.0, "val_loss": 17.28280782699585, "val_acc": 52.0}
{"epoch": 3, "training_loss": 68.55137324333191, "training_acc": 57.0, "val_loss": 18.270793557167053, "val_acc": 52.0}
{"epoch": 4, "training_loss": 73.68902039527893, "training_acc": 53.0, "val_loss": 18.261855840682983, "val_acc": 52.0}
{"epoch": 5, "training_loss": 72.63155770301819, "training_acc": 53.0, "val_loss": 17.16974675655365, "val_acc": 52.0}
{"epoch": 6, "training_loss": 68.8857433795929, "training_acc": 53.0, "val_loss": 17.508883774280548, "val_acc": 52.0}
{"epoch": 7, "training_loss": 70.64590907096863, "training_acc": 47.0, "val_loss": 17.46637672185898, "val_acc": 52.0}
{"epoch": 8, "training_loss": 70.50840759277344, "training_acc": 47.0, "val_loss": 17.165233194828033, "val_acc": 52.0}
{"epoch": 9, "training_loss": 68.41476368904114, "training_acc": 55.0, "val_loss": 17.287741601467133, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.17142224311829, "training_acc": 53.0, "val_loss": 17.391496896743774, "val_acc": 52.0}
{"epoch": 11, "training_loss": 67.9028217792511, "training_acc": 53.0, "val_loss": 17.269781231880188, "val_acc": 52.0}
{"epoch": 12, "training_loss": 68.76676154136658, "training_acc": 55.0, "val_loss": 17.27313995361328, "val_acc": 52.0}
{"epoch": 13, "training_loss": 66.19696354866028, "training_acc": 59.0, "val_loss": 17.359116673469543, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.99923324584961, "training_acc": 53.0, "val_loss": 17.310641705989838, "val_acc": 52.0}
{"epoch": 15, "training_loss": 68.13926386833191, "training_acc": 56.0, "val_loss": 17.31441020965576, "val_acc": 52.0}
{"epoch": 16, "training_loss": 67.45466232299805, "training_acc": 56.0, "val_loss": 17.32967495918274, "val_acc": 52.0}
{"epoch": 17, "training_loss": 68.39041781425476, "training_acc": 57.0, "val_loss": 17.347532510757446, "val_acc": 52.0}
{"epoch": 18, "training_loss": 67.96431827545166, "training_acc": 58.0, "val_loss": 17.365384101867676, "val_acc": 52.0}
{"epoch": 19, "training_loss": 67.01759815216064, "training_acc": 64.0, "val_loss": 17.350997030735016, "val_acc": 52.0}
{"epoch": 20, "training_loss": 68.55527973175049, "training_acc": 55.0, "val_loss": 17.348231375217438, "val_acc": 52.0}
{"epoch": 21, "training_loss": 68.75978636741638, "training_acc": 53.0, "val_loss": 17.38838404417038, "val_acc": 52.0}
{"epoch": 22, "training_loss": 67.67585945129395, "training_acc": 53.0, "val_loss": 17.484505474567413, "val_acc": 52.0}
{"epoch": 23, "training_loss": 66.70798993110657, "training_acc": 54.0, "val_loss": 17.30102449655533, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.31486439704895, "training_acc": 56.0, "val_loss": 17.28130280971527, "val_acc": 52.0}
{"epoch": 25, "training_loss": 67.5082368850708, "training_acc": 61.0, "val_loss": 17.22996234893799, "val_acc": 52.0}
{"epoch": 26, "training_loss": 66.94845199584961, "training_acc": 60.0, "val_loss": 17.16070920228958, "val_acc": 52.0}
{"epoch": 27, "training_loss": 67.25327968597412, "training_acc": 59.0, "val_loss": 17.329412698745728, "val_acc": 52.0}
{"epoch": 28, "training_loss": 67.54899573326111, "training_acc": 54.0, "val_loss": 17.389975488185883, "val_acc": 52.0}
{"epoch": 29, "training_loss": 67.88112831115723, "training_acc": 52.0, "val_loss": 17.1587735414505, "val_acc": 52.0}
{"epoch": 30, "training_loss": 67.10714077949524, "training_acc": 58.0, "val_loss": 17.067353427410126, "val_acc": 52.0}
{"epoch": 31, "training_loss": 66.67780780792236, "training_acc": 64.0, "val_loss": 17.085258662700653, "val_acc": 52.0}
{"epoch": 32, "training_loss": 66.11757016181946, "training_acc": 67.0, "val_loss": 17.02525168657303, "val_acc": 52.0}
{"epoch": 33, "training_loss": 65.86572575569153, "training_acc": 59.0, "val_loss": 17.20098853111267, "val_acc": 52.0}
{"epoch": 34, "training_loss": 66.87240242958069, "training_acc": 53.0, "val_loss": 17.172454297542572, "val_acc": 52.0}
{"epoch": 35, "training_loss": 67.356849193573, "training_acc": 57.0, "val_loss": 17.027057707309723, "val_acc": 52.0}
{"epoch": 36, "training_loss": 65.87759900093079, "training_acc": 69.0, "val_loss": 17.030900716781616, "val_acc": 52.0}
{"epoch": 37, "training_loss": 65.88027620315552, "training_acc": 67.0, "val_loss": 17.04704463481903, "val_acc": 52.0}
{"epoch": 38, "training_loss": 67.30365681648254, "training_acc": 67.0, "val_loss": 17.076916992664337, "val_acc": 52.0}
{"epoch": 39, "training_loss": 67.13979434967041, "training_acc": 54.0, "val_loss": 17.171847820281982, "val_acc": 52.0}
{"epoch": 40, "training_loss": 66.07864189147949, "training_acc": 59.0, "val_loss": 17.094631493091583, "val_acc": 52.0}
{"epoch": 41, "training_loss": 65.83531403541565, "training_acc": 66.0, "val_loss": 17.093829810619354, "val_acc": 52.0}
{"epoch": 42, "training_loss": 66.1080219745636, "training_acc": 65.0, "val_loss": 17.091377079486847, "val_acc": 52.0}
{"epoch": 43, "training_loss": 64.59442973136902, "training_acc": 70.0, "val_loss": 17.115598917007446, "val_acc": 52.0}
{"epoch": 44, "training_loss": 65.9704225063324, "training_acc": 60.0, "val_loss": 17.089076340198517, "val_acc": 52.0}
{"epoch": 45, "training_loss": 66.87711954116821, "training_acc": 64.0, "val_loss": 17.082299292087555, "val_acc": 52.0}
{"epoch": 46, "training_loss": 65.97302222251892, "training_acc": 65.0, "val_loss": 17.087921500205994, "val_acc": 52.0}
{"epoch": 47, "training_loss": 66.20620584487915, "training_acc": 66.0, "val_loss": 17.129477858543396, "val_acc": 52.0}
{"epoch": 48, "training_loss": 66.46213984489441, "training_acc": 66.0, "val_loss": 17.10679531097412, "val_acc": 52.0}
{"epoch": 49, "training_loss": 67.39251351356506, "training_acc": 65.0, "val_loss": 17.103733122348785, "val_acc": 52.0}
{"epoch": 50, "training_loss": 65.27116990089417, "training_acc": 69.0, "val_loss": 17.118245363235474, "val_acc": 52.0}
{"epoch": 51, "training_loss": 65.58861875534058, "training_acc": 66.0, "val_loss": 17.13211238384247, "val_acc": 52.0}
