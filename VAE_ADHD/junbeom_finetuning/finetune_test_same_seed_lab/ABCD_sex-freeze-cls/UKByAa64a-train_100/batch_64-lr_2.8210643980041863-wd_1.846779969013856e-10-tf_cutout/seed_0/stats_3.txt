"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 6160.50354385376, "training_acc": 49.0, "val_loss": 1392.9025650024414, "val_acc": 52.0}
{"epoch": 1, "training_loss": 6751.816955566406, "training_acc": 49.0, "val_loss": 2853.2440185546875, "val_acc": 48.0}
{"epoch": 2, "training_loss": 10478.929931640625, "training_acc": 47.0, "val_loss": 437.89753913879395, "val_acc": 48.0}
{"epoch": 3, "training_loss": 2831.5013580322266, "training_acc": 55.0, "val_loss": 2837.9310607910156, "val_acc": 52.0}
{"epoch": 4, "training_loss": 10664.035705566406, "training_acc": 53.0, "val_loss": 2704.8742294311523, "val_acc": 52.0}
{"epoch": 5, "training_loss": 8270.037170410156, "training_acc": 53.0, "val_loss": 386.7961883544922, "val_acc": 52.0}
{"epoch": 6, "training_loss": 3351.474365234375, "training_acc": 47.0, "val_loss": 2417.6225662231445, "val_acc": 48.0}
{"epoch": 7, "training_loss": 10245.96694946289, "training_acc": 47.0, "val_loss": 2077.1041870117188, "val_acc": 48.0}
{"epoch": 8, "training_loss": 7368.213455200195, "training_acc": 47.0, "val_loss": 336.38579845428467, "val_acc": 52.0}
{"epoch": 9, "training_loss": 1848.4044952392578, "training_acc": 52.0, "val_loss": 1352.0146369934082, "val_acc": 52.0}
{"epoch": 10, "training_loss": 4284.0404052734375, "training_acc": 53.0, "val_loss": 445.8582878112793, "val_acc": 52.0}
{"epoch": 11, "training_loss": 1801.883674621582, "training_acc": 51.0, "val_loss": 1136.4742279052734, "val_acc": 48.0}
{"epoch": 12, "training_loss": 4668.450271606445, "training_acc": 47.0, "val_loss": 407.04731941223145, "val_acc": 48.0}
{"epoch": 13, "training_loss": 1957.4050979614258, "training_acc": 47.0, "val_loss": 1001.0802268981934, "val_acc": 52.0}
{"epoch": 14, "training_loss": 3369.443161010742, "training_acc": 53.0, "val_loss": 472.50189781188965, "val_acc": 52.0}
{"epoch": 15, "training_loss": 1619.7964172363281, "training_acc": 51.0, "val_loss": 734.1987609863281, "val_acc": 48.0}
{"epoch": 16, "training_loss": 2903.4833068847656, "training_acc": 47.0, "val_loss": 140.77121019363403, "val_acc": 52.0}
{"epoch": 17, "training_loss": 612.6827621459961, "training_acc": 56.0, "val_loss": 463.3533000946045, "val_acc": 52.0}
{"epoch": 18, "training_loss": 1053.2776613235474, "training_acc": 56.0, "val_loss": 166.91572666168213, "val_acc": 48.0}
{"epoch": 19, "training_loss": 699.8650579452515, "training_acc": 51.0, "val_loss": 130.214262008667, "val_acc": 52.0}
{"epoch": 20, "training_loss": 477.9927349090576, "training_acc": 55.0, "val_loss": 74.71981644630432, "val_acc": 52.0}
{"epoch": 21, "training_loss": 221.37573337554932, "training_acc": 67.0, "val_loss": 42.730388045310974, "val_acc": 56.0}
{"epoch": 22, "training_loss": 254.82424449920654, "training_acc": 61.0, "val_loss": 152.62296199798584, "val_acc": 52.0}
{"epoch": 23, "training_loss": 295.06165170669556, "training_acc": 60.0, "val_loss": 106.84553384780884, "val_acc": 48.0}
{"epoch": 24, "training_loss": 718.9146003723145, "training_acc": 44.0, "val_loss": 76.17160081863403, "val_acc": 52.0}
{"epoch": 25, "training_loss": 723.2753982543945, "training_acc": 52.0, "val_loss": 204.26137447357178, "val_acc": 48.0}
{"epoch": 26, "training_loss": 1373.2505493164062, "training_acc": 39.0, "val_loss": 553.0055046081543, "val_acc": 52.0}
{"epoch": 27, "training_loss": 1221.2738304138184, "training_acc": 53.0, "val_loss": 634.891414642334, "val_acc": 48.0}
{"epoch": 28, "training_loss": 3017.357879638672, "training_acc": 47.0, "val_loss": 603.6691665649414, "val_acc": 48.0}
{"epoch": 29, "training_loss": 1709.9525117874146, "training_acc": 53.0, "val_loss": 559.9164962768555, "val_acc": 52.0}
{"epoch": 30, "training_loss": 1555.6447372436523, "training_acc": 53.0, "val_loss": 90.02034068107605, "val_acc": 48.0}
{"epoch": 31, "training_loss": 590.0624904632568, "training_acc": 50.0, "val_loss": 293.4514045715332, "val_acc": 52.0}
{"epoch": 32, "training_loss": 661.3503646850586, "training_acc": 54.0, "val_loss": 285.37232875823975, "val_acc": 48.0}
{"epoch": 33, "training_loss": 1128.3051147460938, "training_acc": 47.0, "val_loss": 269.0216541290283, "val_acc": 52.0}
{"epoch": 34, "training_loss": 789.7358894348145, "training_acc": 54.0, "val_loss": 60.01988649368286, "val_acc": 52.0}
{"epoch": 35, "training_loss": 381.87274742126465, "training_acc": 54.0, "val_loss": 364.83800411224365, "val_acc": 52.0}
{"epoch": 36, "training_loss": 955.428524017334, "training_acc": 54.0, "val_loss": 71.20696902275085, "val_acc": 60.0}
{"epoch": 37, "training_loss": 405.25970458984375, "training_acc": 57.0, "val_loss": 321.3508129119873, "val_acc": 52.0}
{"epoch": 38, "training_loss": 873.901554107666, "training_acc": 54.0, "val_loss": 139.79181051254272, "val_acc": 48.0}
{"epoch": 39, "training_loss": 569.0458736419678, "training_acc": 48.0, "val_loss": 395.51310539245605, "val_acc": 52.0}
{"epoch": 40, "training_loss": 1206.1757698059082, "training_acc": 54.0, "val_loss": 86.61919832229614, "val_acc": 52.0}
