"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 270.21949768066406, "training_acc": 51.0, "val_loss": 52.62543559074402, "val_acc": 52.0}
{"epoch": 1, "training_loss": 274.5856876373291, "training_acc": 51.0, "val_loss": 129.72956895828247, "val_acc": 48.0}
{"epoch": 2, "training_loss": 466.11887550354004, "training_acc": 47.0, "val_loss": 24.193643033504486, "val_acc": 48.0}
{"epoch": 3, "training_loss": 164.63823413848877, "training_acc": 47.0, "val_loss": 104.9208402633667, "val_acc": 52.0}
{"epoch": 4, "training_loss": 402.8248405456543, "training_acc": 53.0, "val_loss": 78.45776677131653, "val_acc": 52.0}
{"epoch": 5, "training_loss": 239.82497262954712, "training_acc": 53.0, "val_loss": 39.769530296325684, "val_acc": 48.0}
{"epoch": 6, "training_loss": 192.78067779541016, "training_acc": 47.0, "val_loss": 72.003835439682, "val_acc": 48.0}
{"epoch": 7, "training_loss": 243.89813089370728, "training_acc": 47.0, "val_loss": 18.094132840633392, "val_acc": 52.0}
{"epoch": 8, "training_loss": 102.87191152572632, "training_acc": 50.0, "val_loss": 49.5015025138855, "val_acc": 52.0}
{"epoch": 9, "training_loss": 178.32003927230835, "training_acc": 53.0, "val_loss": 19.58412677049637, "val_acc": 52.0}
{"epoch": 10, "training_loss": 86.83736872673035, "training_acc": 48.0, "val_loss": 42.977744340896606, "val_acc": 48.0}
{"epoch": 11, "training_loss": 148.05851221084595, "training_acc": 46.0, "val_loss": 19.145803153514862, "val_acc": 44.0}
{"epoch": 12, "training_loss": 86.6659483909607, "training_acc": 56.0, "val_loss": 32.686179876327515, "val_acc": 52.0}
{"epoch": 13, "training_loss": 119.17635440826416, "training_acc": 53.0, "val_loss": 18.06465834379196, "val_acc": 44.0}
{"epoch": 14, "training_loss": 82.96464395523071, "training_acc": 57.0, "val_loss": 26.680922508239746, "val_acc": 48.0}
{"epoch": 15, "training_loss": 96.9238121509552, "training_acc": 50.0, "val_loss": 21.99368178844452, "val_acc": 52.0}
{"epoch": 16, "training_loss": 86.11167931556702, "training_acc": 53.0, "val_loss": 20.204272866249084, "val_acc": 52.0}
{"epoch": 17, "training_loss": 77.51002359390259, "training_acc": 53.0, "val_loss": 19.757571816444397, "val_acc": 48.0}
{"epoch": 18, "training_loss": 80.23769760131836, "training_acc": 47.0, "val_loss": 17.3748642206192, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.59337019920349, "training_acc": 56.0, "val_loss": 19.18162852525711, "val_acc": 52.0}
{"epoch": 20, "training_loss": 73.3225634098053, "training_acc": 55.0, "val_loss": 17.608459293842316, "val_acc": 52.0}
{"epoch": 21, "training_loss": 72.47829747200012, "training_acc": 53.0, "val_loss": 16.885975003242493, "val_acc": 52.0}
{"epoch": 22, "training_loss": 64.25961351394653, "training_acc": 62.0, "val_loss": 17.241640388965607, "val_acc": 52.0}
{"epoch": 23, "training_loss": 65.68960475921631, "training_acc": 60.0, "val_loss": 16.900233924388885, "val_acc": 56.0}
{"epoch": 24, "training_loss": 62.85732579231262, "training_acc": 74.0, "val_loss": 17.671333253383636, "val_acc": 52.0}
{"epoch": 25, "training_loss": 65.09124684333801, "training_acc": 56.0, "val_loss": 17.158441245555878, "val_acc": 52.0}
{"epoch": 26, "training_loss": 61.34046220779419, "training_acc": 68.0, "val_loss": 17.850367724895477, "val_acc": 56.0}
{"epoch": 27, "training_loss": 72.24404644966125, "training_acc": 51.0, "val_loss": 18.6456099152565, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.35284876823425, "training_acc": 52.0, "val_loss": 19.00446116924286, "val_acc": 52.0}
{"epoch": 29, "training_loss": 61.99251317977905, "training_acc": 63.0, "val_loss": 19.378241896629333, "val_acc": 48.0}
{"epoch": 30, "training_loss": 75.25685143470764, "training_acc": 47.0, "val_loss": 17.24952459335327, "val_acc": 52.0}
{"epoch": 31, "training_loss": 66.79931974411011, "training_acc": 61.0, "val_loss": 18.587014079093933, "val_acc": 52.0}
{"epoch": 32, "training_loss": 75.6288914680481, "training_acc": 42.0, "val_loss": 16.86340719461441, "val_acc": 56.0}
{"epoch": 33, "training_loss": 65.00403785705566, "training_acc": 67.0, "val_loss": 19.332198798656464, "val_acc": 52.0}
{"epoch": 34, "training_loss": 63.783416748046875, "training_acc": 63.0, "val_loss": 17.041198909282684, "val_acc": 64.0}
{"epoch": 35, "training_loss": 66.60001015663147, "training_acc": 58.0, "val_loss": 18.65440458059311, "val_acc": 52.0}
{"epoch": 36, "training_loss": 67.52344870567322, "training_acc": 59.0, "val_loss": 18.077129125595093, "val_acc": 52.0}
{"epoch": 37, "training_loss": 60.22605276107788, "training_acc": 64.0, "val_loss": 17.668554186820984, "val_acc": 48.0}
{"epoch": 38, "training_loss": 66.86499810218811, "training_acc": 56.0, "val_loss": 17.3775777220726, "val_acc": 52.0}
{"epoch": 39, "training_loss": 62.47671151161194, "training_acc": 64.0, "val_loss": 17.921291291713715, "val_acc": 52.0}
{"epoch": 40, "training_loss": 64.03875255584717, "training_acc": 59.0, "val_loss": 17.086385190486908, "val_acc": 60.0}
{"epoch": 41, "training_loss": 63.47250485420227, "training_acc": 64.0, "val_loss": 18.365779519081116, "val_acc": 52.0}
{"epoch": 42, "training_loss": 62.10074710845947, "training_acc": 58.0, "val_loss": 17.192578315734863, "val_acc": 56.0}
{"epoch": 43, "training_loss": 60.879356145858765, "training_acc": 70.0, "val_loss": 17.472369968891144, "val_acc": 52.0}
{"epoch": 44, "training_loss": 61.09587025642395, "training_acc": 70.0, "val_loss": 18.863947689533234, "val_acc": 52.0}
{"epoch": 45, "training_loss": 63.7021963596344, "training_acc": 67.0, "val_loss": 18.874886631965637, "val_acc": 56.0}
{"epoch": 46, "training_loss": 62.61445212364197, "training_acc": 63.0, "val_loss": 17.837657034397125, "val_acc": 52.0}
{"epoch": 47, "training_loss": 62.84086298942566, "training_acc": 65.0, "val_loss": 17.866124212741852, "val_acc": 52.0}
{"epoch": 48, "training_loss": 60.03945994377136, "training_acc": 76.0, "val_loss": 17.936058342456818, "val_acc": 56.0}
{"epoch": 49, "training_loss": 60.080623149871826, "training_acc": 70.0, "val_loss": 18.420495092868805, "val_acc": 52.0}
{"epoch": 50, "training_loss": 64.57467937469482, "training_acc": 59.0, "val_loss": 17.3684224486351, "val_acc": 60.0}
{"epoch": 51, "training_loss": 64.95347285270691, "training_acc": 61.0, "val_loss": 17.253312468528748, "val_acc": 60.0}
