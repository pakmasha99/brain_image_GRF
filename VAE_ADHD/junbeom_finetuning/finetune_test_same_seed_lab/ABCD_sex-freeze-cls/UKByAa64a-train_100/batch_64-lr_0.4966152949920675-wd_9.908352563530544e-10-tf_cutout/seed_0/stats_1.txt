"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 1087.431827545166, "training_acc": 46.0, "val_loss": 217.8816795349121, "val_acc": 52.0}
{"epoch": 1, "training_loss": 1058.9395370483398, "training_acc": 53.0, "val_loss": 564.7225856781006, "val_acc": 48.0}
{"epoch": 2, "training_loss": 2124.9774169921875, "training_acc": 47.0, "val_loss": 156.6211462020874, "val_acc": 48.0}
{"epoch": 3, "training_loss": 740.3321933746338, "training_acc": 51.0, "val_loss": 412.21628189086914, "val_acc": 52.0}
{"epoch": 4, "training_loss": 1566.7286224365234, "training_acc": 53.0, "val_loss": 367.83130168914795, "val_acc": 52.0}
{"epoch": 5, "training_loss": 1092.144214630127, "training_acc": 53.0, "val_loss": 64.2179012298584, "val_acc": 48.0}
{"epoch": 6, "training_loss": 591.0792083740234, "training_acc": 47.0, "val_loss": 190.25423526763916, "val_acc": 48.0}
{"epoch": 7, "training_loss": 646.1160907745361, "training_acc": 47.0, "val_loss": 133.48753452301025, "val_acc": 52.0}
{"epoch": 8, "training_loss": 520.6479530334473, "training_acc": 53.0, "val_loss": 231.05480670928955, "val_acc": 52.0}
{"epoch": 9, "training_loss": 745.8328819274902, "training_acc": 53.0, "val_loss": 26.20549201965332, "val_acc": 56.0}
{"epoch": 10, "training_loss": 285.36519622802734, "training_acc": 50.0, "val_loss": 176.50487422943115, "val_acc": 48.0}
{"epoch": 11, "training_loss": 644.9448871612549, "training_acc": 47.0, "val_loss": 35.85833013057709, "val_acc": 52.0}
{"epoch": 12, "training_loss": 193.02631855010986, "training_acc": 54.0, "val_loss": 75.58795809745789, "val_acc": 52.0}
{"epoch": 13, "training_loss": 166.05826616287231, "training_acc": 64.0, "val_loss": 72.23718166351318, "val_acc": 48.0}
{"epoch": 14, "training_loss": 255.72998523712158, "training_acc": 47.0, "val_loss": 68.49690079689026, "val_acc": 52.0}
{"epoch": 15, "training_loss": 252.71402263641357, "training_acc": 54.0, "val_loss": 48.53144586086273, "val_acc": 52.0}
{"epoch": 16, "training_loss": 181.83220767974854, "training_acc": 50.0, "val_loss": 67.87646412849426, "val_acc": 48.0}
{"epoch": 17, "training_loss": 225.8002998828888, "training_acc": 51.0, "val_loss": 76.50219798088074, "val_acc": 52.0}
{"epoch": 18, "training_loss": 218.32112169265747, "training_acc": 54.0, "val_loss": 16.360563039779663, "val_acc": 68.0}
{"epoch": 19, "training_loss": 91.8447060585022, "training_acc": 64.0, "val_loss": 23.884646594524384, "val_acc": 56.0}
{"epoch": 20, "training_loss": 100.26580667495728, "training_acc": 59.0, "val_loss": 16.739492118358612, "val_acc": 64.0}
{"epoch": 21, "training_loss": 79.45728659629822, "training_acc": 59.0, "val_loss": 20.948056876659393, "val_acc": 56.0}
{"epoch": 22, "training_loss": 72.23089361190796, "training_acc": 61.0, "val_loss": 17.657043039798737, "val_acc": 64.0}
{"epoch": 23, "training_loss": 69.88875985145569, "training_acc": 61.0, "val_loss": 19.465264678001404, "val_acc": 64.0}
{"epoch": 24, "training_loss": 71.91142344474792, "training_acc": 62.0, "val_loss": 19.64689940214157, "val_acc": 60.0}
{"epoch": 25, "training_loss": 76.10058283805847, "training_acc": 63.0, "val_loss": 24.019722640514374, "val_acc": 56.0}
{"epoch": 26, "training_loss": 91.40541052818298, "training_acc": 52.0, "val_loss": 30.688264966011047, "val_acc": 56.0}
{"epoch": 27, "training_loss": 77.58530068397522, "training_acc": 57.0, "val_loss": 25.836333632469177, "val_acc": 48.0}
{"epoch": 28, "training_loss": 106.4927909374237, "training_acc": 50.0, "val_loss": 19.35090571641922, "val_acc": 56.0}
{"epoch": 29, "training_loss": 85.57464504241943, "training_acc": 55.0, "val_loss": 48.42454493045807, "val_acc": 52.0}
{"epoch": 30, "training_loss": 136.24225544929504, "training_acc": 53.0, "val_loss": 43.68181228637695, "val_acc": 48.0}
{"epoch": 31, "training_loss": 147.84650897979736, "training_acc": 48.0, "val_loss": 76.8581748008728, "val_acc": 52.0}
{"epoch": 32, "training_loss": 276.83698749542236, "training_acc": 53.0, "val_loss": 15.848203003406525, "val_acc": 64.0}
{"epoch": 33, "training_loss": 104.51744079589844, "training_acc": 56.0, "val_loss": 15.978029370307922, "val_acc": 60.0}
{"epoch": 34, "training_loss": 152.68116092681885, "training_acc": 51.0, "val_loss": 17.269951105117798, "val_acc": 60.0}
{"epoch": 35, "training_loss": 123.55588340759277, "training_acc": 60.0, "val_loss": 21.08246386051178, "val_acc": 48.0}
{"epoch": 36, "training_loss": 168.20300483703613, "training_acc": 50.0, "val_loss": 84.34677720069885, "val_acc": 52.0}
{"epoch": 37, "training_loss": 199.55601143836975, "training_acc": 61.0, "val_loss": 61.47139072418213, "val_acc": 48.0}
{"epoch": 38, "training_loss": 209.40426683425903, "training_acc": 47.0, "val_loss": 82.6099693775177, "val_acc": 52.0}
{"epoch": 39, "training_loss": 298.07072162628174, "training_acc": 53.0, "val_loss": 20.186932384967804, "val_acc": 56.0}
{"epoch": 40, "training_loss": 176.3013916015625, "training_acc": 56.0, "val_loss": 68.50074529647827, "val_acc": 48.0}
{"epoch": 41, "training_loss": 214.0700249671936, "training_acc": 51.0, "val_loss": 67.16665029525757, "val_acc": 52.0}
{"epoch": 42, "training_loss": 184.18571758270264, "training_acc": 54.0, "val_loss": 42.488089203834534, "val_acc": 48.0}
{"epoch": 43, "training_loss": 147.68235445022583, "training_acc": 46.0, "val_loss": 20.299842953681946, "val_acc": 56.0}
{"epoch": 44, "training_loss": 66.05516338348389, "training_acc": 65.0, "val_loss": 16.300979256629944, "val_acc": 60.0}
{"epoch": 45, "training_loss": 55.90186262130737, "training_acc": 71.0, "val_loss": 17.405860126018524, "val_acc": 64.0}
{"epoch": 46, "training_loss": 50.6981064081192, "training_acc": 77.0, "val_loss": 15.948852896690369, "val_acc": 60.0}
{"epoch": 47, "training_loss": 65.8584840297699, "training_acc": 67.0, "val_loss": 16.046646237373352, "val_acc": 68.0}
{"epoch": 48, "training_loss": 70.69119596481323, "training_acc": 58.0, "val_loss": 36.47705316543579, "val_acc": 52.0}
{"epoch": 49, "training_loss": 92.54171967506409, "training_acc": 60.0, "val_loss": 32.27562606334686, "val_acc": 48.0}
{"epoch": 50, "training_loss": 88.87483286857605, "training_acc": 58.0, "val_loss": 47.438475489616394, "val_acc": 52.0}
{"epoch": 51, "training_loss": 121.11483383178711, "training_acc": 62.0, "val_loss": 53.63758206367493, "val_acc": 48.0}
