"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 1577926.9771575928, "training_acc": 46.0, "val_loss": 373381.4208984375, "val_acc": 52.0}
{"epoch": 1, "training_loss": 1710739.1640625, "training_acc": 47.0, "val_loss": 614014.990234375, "val_acc": 48.0}
{"epoch": 2, "training_loss": 2217693.2578125, "training_acc": 47.0, "val_loss": 40958.31298828125, "val_acc": 48.0}
{"epoch": 3, "training_loss": 612402.08203125, "training_acc": 55.0, "val_loss": 701391.9921875, "val_acc": 52.0}
{"epoch": 4, "training_loss": 2759641.0390625, "training_acc": 53.0, "val_loss": 661887.6953125, "val_acc": 52.0}
{"epoch": 5, "training_loss": 2201456.0, "training_acc": 53.0, "val_loss": 87913.671875, "val_acc": 52.0}
{"epoch": 6, "training_loss": 738322.6484375, "training_acc": 50.0, "val_loss": 582128.02734375, "val_acc": 48.0}
{"epoch": 7, "training_loss": 2492490.2890625, "training_acc": 47.0, "val_loss": 523630.17578125, "val_acc": 48.0}
{"epoch": 8, "training_loss": 1873998.140625, "training_acc": 47.0, "val_loss": 32353.298950195312, "val_acc": 56.0}
{"epoch": 9, "training_loss": 411090.56640625, "training_acc": 55.0, "val_loss": 294663.7939453125, "val_acc": 52.0}
{"epoch": 10, "training_loss": 1019847.80859375, "training_acc": 53.0, "val_loss": 82679.3701171875, "val_acc": 52.0}
{"epoch": 11, "training_loss": 406693.962890625, "training_acc": 53.0, "val_loss": 270602.880859375, "val_acc": 48.0}
{"epoch": 12, "training_loss": 1027447.3671875, "training_acc": 47.0, "val_loss": 77660.61401367188, "val_acc": 48.0}
{"epoch": 13, "training_loss": 424733.994140625, "training_acc": 49.0, "val_loss": 287972.55859375, "val_acc": 52.0}
{"epoch": 14, "training_loss": 1111865.88671875, "training_acc": 53.0, "val_loss": 194132.99560546875, "val_acc": 52.0}
{"epoch": 15, "training_loss": 514671.39709472656, "training_acc": 57.0, "val_loss": 185981.60400390625, "val_acc": 48.0}
{"epoch": 16, "training_loss": 811439.17578125, "training_acc": 47.0, "val_loss": 147608.50830078125, "val_acc": 48.0}
{"epoch": 17, "training_loss": 414343.7294921875, "training_acc": 51.0, "val_loss": 117541.00341796875, "val_acc": 52.0}
{"epoch": 18, "training_loss": 393993.6689453125, "training_acc": 54.0, "val_loss": 29576.776123046875, "val_acc": 52.0}
{"epoch": 19, "training_loss": 140312.34423828125, "training_acc": 53.0, "val_loss": 18224.644470214844, "val_acc": 56.0}
{"epoch": 20, "training_loss": 175030.705078125, "training_acc": 54.0, "val_loss": 62211.724853515625, "val_acc": 52.0}
{"epoch": 21, "training_loss": 198555.28076171875, "training_acc": 54.0, "val_loss": 65311.004638671875, "val_acc": 48.0}
{"epoch": 22, "training_loss": 248136.09375, "training_acc": 42.0, "val_loss": 43936.370849609375, "val_acc": 52.0}
{"epoch": 23, "training_loss": 147015.744140625, "training_acc": 57.0, "val_loss": 19156.265258789062, "val_acc": 60.0}
{"epoch": 24, "training_loss": 89993.54345703125, "training_acc": 67.0, "val_loss": 18692.31719970703, "val_acc": 48.0}
{"epoch": 25, "training_loss": 149670.771484375, "training_acc": 52.0, "val_loss": 25997.705078125, "val_acc": 44.0}
{"epoch": 26, "training_loss": 204824.47265625, "training_acc": 48.0, "val_loss": 100113.68408203125, "val_acc": 52.0}
{"epoch": 27, "training_loss": 212287.009765625, "training_acc": 63.0, "val_loss": 136714.58740234375, "val_acc": 48.0}
{"epoch": 28, "training_loss": 591488.9375, "training_acc": 47.0, "val_loss": 44702.27966308594, "val_acc": 48.0}
{"epoch": 29, "training_loss": 293480.83984375, "training_acc": 55.0, "val_loss": 264957.6171875, "val_acc": 52.0}
{"epoch": 30, "training_loss": 979672.71484375, "training_acc": 53.0, "val_loss": 147970.166015625, "val_acc": 52.0}
{"epoch": 31, "training_loss": 378294.81591796875, "training_acc": 55.0, "val_loss": 142850.69580078125, "val_acc": 48.0}
{"epoch": 32, "training_loss": 522458.158203125, "training_acc": 47.0, "val_loss": 56790.472412109375, "val_acc": 52.0}
{"epoch": 33, "training_loss": 173451.12353515625, "training_acc": 52.0, "val_loss": 39646.405029296875, "val_acc": 52.0}
{"epoch": 34, "training_loss": 198577.619140625, "training_acc": 54.0, "val_loss": 48741.86706542969, "val_acc": 44.0}
{"epoch": 35, "training_loss": 194402.91162109375, "training_acc": 57.0, "val_loss": 108926.79443359375, "val_acc": 52.0}
{"epoch": 36, "training_loss": 292605.525390625, "training_acc": 53.0, "val_loss": 78185.9375, "val_acc": 48.0}
{"epoch": 37, "training_loss": 308363.8251953125, "training_acc": 47.0, "val_loss": 76869.75708007812, "val_acc": 52.0}
{"epoch": 38, "training_loss": 287257.39453125, "training_acc": 53.0, "val_loss": 31217.324829101562, "val_acc": 52.0}
