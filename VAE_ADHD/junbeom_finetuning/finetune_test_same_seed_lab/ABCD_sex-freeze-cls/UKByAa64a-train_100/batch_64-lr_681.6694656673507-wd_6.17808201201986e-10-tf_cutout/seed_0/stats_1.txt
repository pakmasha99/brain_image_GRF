"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 1431722.3856697083, "training_acc": 46.0, "val_loss": 299256.9091796875, "val_acc": 52.0}
{"epoch": 1, "training_loss": 1453151.25, "training_acc": 53.0, "val_loss": 775002.490234375, "val_acc": 48.0}
{"epoch": 2, "training_loss": 2916729.0390625, "training_acc": 47.0, "val_loss": 214829.150390625, "val_acc": 48.0}
{"epoch": 3, "training_loss": 1016343.58984375, "training_acc": 51.0, "val_loss": 565994.62890625, "val_acc": 52.0}
{"epoch": 4, "training_loss": 2150373.6796875, "training_acc": 53.0, "val_loss": 505053.662109375, "val_acc": 52.0}
{"epoch": 5, "training_loss": 1498960.23046875, "training_acc": 53.0, "val_loss": 87752.7587890625, "val_acc": 48.0}
{"epoch": 6, "training_loss": 808805.3125, "training_acc": 47.0, "val_loss": 258685.0341796875, "val_acc": 48.0}
{"epoch": 7, "training_loss": 876838.40625, "training_acc": 47.0, "val_loss": 187442.6025390625, "val_acc": 52.0}
{"epoch": 8, "training_loss": 730886.99609375, "training_acc": 53.0, "val_loss": 325016.5771484375, "val_acc": 52.0}
{"epoch": 9, "training_loss": 1055376.203125, "training_acc": 53.0, "val_loss": 40956.365966796875, "val_acc": 52.0}
{"epoch": 10, "training_loss": 422100.91015625, "training_acc": 49.0, "val_loss": 302198.53515625, "val_acc": 48.0}
{"epoch": 11, "training_loss": 1157873.873046875, "training_acc": 47.0, "val_loss": 64104.28466796875, "val_acc": 48.0}
{"epoch": 12, "training_loss": 419522.236328125, "training_acc": 49.0, "val_loss": 317995.6298828125, "val_acc": 52.0}
{"epoch": 13, "training_loss": 1222600.62109375, "training_acc": 53.0, "val_loss": 238389.6484375, "val_acc": 52.0}
{"epoch": 14, "training_loss": 639961.4399414062, "training_acc": 52.0, "val_loss": 233776.1962890625, "val_acc": 48.0}
{"epoch": 15, "training_loss": 1138952.75390625, "training_acc": 47.0, "val_loss": 347702.3681640625, "val_acc": 48.0}
{"epoch": 16, "training_loss": 1173094.46484375, "training_acc": 47.0, "val_loss": 31668.234252929688, "val_acc": 60.0}
{"epoch": 17, "training_loss": 291329.84375, "training_acc": 54.0, "val_loss": 197766.93115234375, "val_acc": 52.0}
{"epoch": 18, "training_loss": 634905.2602539062, "training_acc": 53.0, "val_loss": 44726.416015625, "val_acc": 48.0}
{"epoch": 19, "training_loss": 196944.6806640625, "training_acc": 50.0, "val_loss": 21448.243713378906, "val_acc": 56.0}
{"epoch": 20, "training_loss": 144158.6533203125, "training_acc": 60.0, "val_loss": 33261.15417480469, "val_acc": 60.0}
{"epoch": 21, "training_loss": 123846.01025390625, "training_acc": 55.0, "val_loss": 35517.97180175781, "val_acc": 44.0}
{"epoch": 22, "training_loss": 172200.8095703125, "training_acc": 55.0, "val_loss": 94183.740234375, "val_acc": 52.0}
{"epoch": 23, "training_loss": 176494.84997558594, "training_acc": 62.0, "val_loss": 92263.5009765625, "val_acc": 48.0}
{"epoch": 24, "training_loss": 341335.5517578125, "training_acc": 48.0, "val_loss": 75583.23974609375, "val_acc": 52.0}
{"epoch": 25, "training_loss": 244645.6591796875, "training_acc": 55.0, "val_loss": 33795.654296875, "val_acc": 60.0}
{"epoch": 26, "training_loss": 249981.54296875, "training_acc": 52.0, "val_loss": 82678.86352539062, "val_acc": 48.0}
{"epoch": 27, "training_loss": 292537.0458984375, "training_acc": 49.0, "val_loss": 78114.24560546875, "val_acc": 52.0}
{"epoch": 28, "training_loss": 156618.4501953125, "training_acc": 58.0, "val_loss": 50365.88134765625, "val_acc": 48.0}
{"epoch": 29, "training_loss": 160032.7880859375, "training_acc": 58.0, "val_loss": 127190.90576171875, "val_acc": 52.0}
{"epoch": 30, "training_loss": 435831.03515625, "training_acc": 53.0, "val_loss": 59194.61669921875, "val_acc": 56.0}
{"epoch": 31, "training_loss": 287773.748046875, "training_acc": 51.0, "val_loss": 133996.826171875, "val_acc": 48.0}
{"epoch": 32, "training_loss": 408061.60205078125, "training_acc": 49.0, "val_loss": 146484.21630859375, "val_acc": 52.0}
{"epoch": 33, "training_loss": 599822.20703125, "training_acc": 53.0, "val_loss": 150270.556640625, "val_acc": 52.0}
{"epoch": 34, "training_loss": 290382.3664550781, "training_acc": 58.0, "val_loss": 180496.1669921875, "val_acc": 48.0}
{"epoch": 35, "training_loss": 798735.9375, "training_acc": 47.0, "val_loss": 158153.1005859375, "val_acc": 48.0}
{"epoch": 36, "training_loss": 453089.888671875, "training_acc": 54.0, "val_loss": 120746.875, "val_acc": 52.0}
{"epoch": 37, "training_loss": 359708.1630859375, "training_acc": 53.0, "val_loss": 13400.794982910156, "val_acc": 44.0}
{"epoch": 38, "training_loss": 163865.5361328125, "training_acc": 62.0, "val_loss": 12693.783569335938, "val_acc": 44.0}
{"epoch": 39, "training_loss": 138674.7724609375, "training_acc": 57.0, "val_loss": 66762.2802734375, "val_acc": 52.0}
{"epoch": 40, "training_loss": 192072.44091796875, "training_acc": 49.0, "val_loss": 36760.943603515625, "val_acc": 52.0}
{"epoch": 41, "training_loss": 182097.009765625, "training_acc": 51.0, "val_loss": 86811.72485351562, "val_acc": 52.0}
{"epoch": 42, "training_loss": 232052.97778320312, "training_acc": 54.0, "val_loss": 24156.015014648438, "val_acc": 44.0}
{"epoch": 43, "training_loss": 142894.03173828125, "training_acc": 54.0, "val_loss": 20550.831604003906, "val_acc": 68.0}
{"epoch": 44, "training_loss": 88445.822265625, "training_acc": 66.0, "val_loss": 17564.92919921875, "val_acc": 44.0}
{"epoch": 45, "training_loss": 116214.06201171875, "training_acc": 60.0, "val_loss": 74142.03491210938, "val_acc": 52.0}
{"epoch": 46, "training_loss": 130773.48046875, "training_acc": 62.0, "val_loss": 48899.71008300781, "val_acc": 48.0}
{"epoch": 47, "training_loss": 168641.99243164062, "training_acc": 59.0, "val_loss": 48405.51452636719, "val_acc": 52.0}
{"epoch": 48, "training_loss": 135300.06298828125, "training_acc": 50.0, "val_loss": 17541.195678710938, "val_acc": 68.0}
{"epoch": 49, "training_loss": 32497.10186767578, "training_acc": 73.0, "val_loss": 9812.261199951172, "val_acc": 60.0}
{"epoch": 50, "training_loss": 65242.8349609375, "training_acc": 66.0, "val_loss": 60646.8994140625, "val_acc": 52.0}
{"epoch": 51, "training_loss": 150547.54418945312, "training_acc": 54.0, "val_loss": 80395.95336914062, "val_acc": 48.0}
{"epoch": 52, "training_loss": 308952.4716796875, "training_acc": 48.0, "val_loss": 59025.152587890625, "val_acc": 52.0}
{"epoch": 53, "training_loss": 181824.17431640625, "training_acc": 53.0, "val_loss": 28650.81787109375, "val_acc": 44.0}
{"epoch": 54, "training_loss": 91545.865234375, "training_acc": 61.0, "val_loss": 48288.82751464844, "val_acc": 56.0}
{"epoch": 55, "training_loss": 90389.53009033203, "training_acc": 62.0, "val_loss": 20324.435424804688, "val_acc": 44.0}
{"epoch": 56, "training_loss": 83296.76806640625, "training_acc": 62.0, "val_loss": 39616.69921875, "val_acc": 56.0}
{"epoch": 57, "training_loss": 168180.73046875, "training_acc": 53.0, "val_loss": 50995.172119140625, "val_acc": 48.0}
{"epoch": 58, "training_loss": 189966.0859375, "training_acc": 55.0, "val_loss": 99227.24609375, "val_acc": 52.0}
{"epoch": 59, "training_loss": 227655.9912109375, "training_acc": 56.0, "val_loss": 126761.65771484375, "val_acc": 48.0}
{"epoch": 60, "training_loss": 466958.1455078125, "training_acc": 47.0, "val_loss": 17652.842712402344, "val_acc": 44.0}
{"epoch": 61, "training_loss": 208342.46875, "training_acc": 55.0, "val_loss": 170915.56396484375, "val_acc": 52.0}
{"epoch": 62, "training_loss": 532242.8955078125, "training_acc": 53.0, "val_loss": 73334.48486328125, "val_acc": 48.0}
{"epoch": 63, "training_loss": 372677.978515625, "training_acc": 47.0, "val_loss": 23578.51104736328, "val_acc": 44.0}
{"epoch": 64, "training_loss": 248947.599609375, "training_acc": 54.0, "val_loss": 215800.2197265625, "val_acc": 52.0}
{"epoch": 65, "training_loss": 693831.556640625, "training_acc": 53.0, "val_loss": 14482.438659667969, "val_acc": 60.0}
{"epoch": 66, "training_loss": 222695.41796875, "training_acc": 58.0, "val_loss": 94386.02294921875, "val_acc": 48.0}
{"epoch": 67, "training_loss": 327729.1083984375, "training_acc": 42.0, "val_loss": 81328.74145507812, "val_acc": 52.0}
{"epoch": 68, "training_loss": 156908.80334472656, "training_acc": 62.0, "val_loss": 37242.9443359375, "val_acc": 44.0}
