"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 1717260.8379821777, "training_acc": 48.0, "val_loss": 373613.0615234375, "val_acc": 52.0}
{"epoch": 1, "training_loss": 1277548.4921875, "training_acc": 59.0, "val_loss": 678508.69140625, "val_acc": 48.0}
{"epoch": 2, "training_loss": 2573809.671875, "training_acc": 47.0, "val_loss": 184252.96630859375, "val_acc": 48.0}
{"epoch": 3, "training_loss": 863755.8046875, "training_acc": 51.0, "val_loss": 546376.806640625, "val_acc": 52.0}
{"epoch": 4, "training_loss": 2222870.640625, "training_acc": 53.0, "val_loss": 457878.80859375, "val_acc": 52.0}
{"epoch": 5, "training_loss": 1328888.228515625, "training_acc": 54.0, "val_loss": 198313.95263671875, "val_acc": 48.0}
{"epoch": 6, "training_loss": 1158545.3828125, "training_acc": 47.0, "val_loss": 385998.8037109375, "val_acc": 48.0}
{"epoch": 7, "training_loss": 1389270.27734375, "training_acc": 47.0, "val_loss": 39136.19689941406, "val_acc": 52.0}
{"epoch": 8, "training_loss": 302282.94921875, "training_acc": 54.0, "val_loss": 216276.611328125, "val_acc": 52.0}
{"epoch": 9, "training_loss": 690146.2353515625, "training_acc": 53.0, "val_loss": 39447.55859375, "val_acc": 48.0}
{"epoch": 10, "training_loss": 212293.3427734375, "training_acc": 48.0, "val_loss": 30959.2529296875, "val_acc": 52.0}
{"epoch": 11, "training_loss": 77913.0595703125, "training_acc": 55.0, "val_loss": 83882.68432617188, "val_acc": 48.0}
{"epoch": 12, "training_loss": 265007.50048828125, "training_acc": 49.0, "val_loss": 82810.7666015625, "val_acc": 52.0}
{"epoch": 13, "training_loss": 329240.46875, "training_acc": 53.0, "val_loss": 22205.116271972656, "val_acc": 52.0}
{"epoch": 14, "training_loss": 113875.46435546875, "training_acc": 54.0, "val_loss": 23344.265747070312, "val_acc": 52.0}
{"epoch": 15, "training_loss": 185164.4580078125, "training_acc": 51.0, "val_loss": 70156.89086914062, "val_acc": 52.0}
{"epoch": 16, "training_loss": 299406.341796875, "training_acc": 41.0, "val_loss": 39310.20202636719, "val_acc": 48.0}
{"epoch": 17, "training_loss": 234648.228515625, "training_acc": 43.0, "val_loss": 112043.49365234375, "val_acc": 52.0}
{"epoch": 18, "training_loss": 287468.20361328125, "training_acc": 54.0, "val_loss": 120714.453125, "val_acc": 48.0}
{"epoch": 19, "training_loss": 479643.673828125, "training_acc": 47.0, "val_loss": 16200.42724609375, "val_acc": 60.0}
{"epoch": 20, "training_loss": 175930.8095703125, "training_acc": 63.0, "val_loss": 90215.15502929688, "val_acc": 52.0}
{"epoch": 21, "training_loss": 228506.20654296875, "training_acc": 56.0, "val_loss": 54653.106689453125, "val_acc": 48.0}
{"epoch": 22, "training_loss": 158449.97607421875, "training_acc": 54.0, "val_loss": 43761.60888671875, "val_acc": 52.0}
{"epoch": 23, "training_loss": 120412.55786132812, "training_acc": 58.0, "val_loss": 58734.698486328125, "val_acc": 48.0}
{"epoch": 24, "training_loss": 162289.8828125, "training_acc": 49.0, "val_loss": 8883.573913574219, "val_acc": 52.0}
{"epoch": 25, "training_loss": 58817.362548828125, "training_acc": 59.0, "val_loss": 8087.171936035156, "val_acc": 60.0}
{"epoch": 26, "training_loss": 85758.54150390625, "training_acc": 62.0, "val_loss": 22928.916931152344, "val_acc": 44.0}
{"epoch": 27, "training_loss": 97965.2353515625, "training_acc": 55.0, "val_loss": 13942.123413085938, "val_acc": 40.0}
{"epoch": 28, "training_loss": 64062.046630859375, "training_acc": 62.0, "val_loss": 53856.2255859375, "val_acc": 48.0}
{"epoch": 29, "training_loss": 139617.60180664062, "training_acc": 52.0, "val_loss": 17302.84881591797, "val_acc": 56.0}
{"epoch": 30, "training_loss": 138784.1748046875, "training_acc": 55.0, "val_loss": 12151.937103271484, "val_acc": 56.0}
{"epoch": 31, "training_loss": 85317.505859375, "training_acc": 57.0, "val_loss": 12362.421417236328, "val_acc": 60.0}
{"epoch": 32, "training_loss": 39240.36267089844, "training_acc": 64.0, "val_loss": 14388.198852539062, "val_acc": 52.0}
{"epoch": 33, "training_loss": 97376.43994140625, "training_acc": 58.0, "val_loss": 21475.460815429688, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69826.13061523438, "training_acc": 63.0, "val_loss": 21587.359619140625, "val_acc": 48.0}
{"epoch": 35, "training_loss": 85495.93017578125, "training_acc": 62.0, "val_loss": 12978.74755859375, "val_acc": 56.0}
{"epoch": 36, "training_loss": 166835.4091796875, "training_acc": 57.0, "val_loss": 69847.18627929688, "val_acc": 48.0}
{"epoch": 37, "training_loss": 251050.78515625, "training_acc": 50.0, "val_loss": 97258.06884765625, "val_acc": 52.0}
{"epoch": 38, "training_loss": 236065.4842529297, "training_acc": 61.0, "val_loss": 131337.24365234375, "val_acc": 48.0}
{"epoch": 39, "training_loss": 486473.919921875, "training_acc": 47.0, "val_loss": 16761.155700683594, "val_acc": 56.0}
{"epoch": 40, "training_loss": 103586.89892578125, "training_acc": 58.0, "val_loss": 18639.8193359375, "val_acc": 44.0}
{"epoch": 41, "training_loss": 50353.016357421875, "training_acc": 53.0, "val_loss": 51661.785888671875, "val_acc": 48.0}
{"epoch": 42, "training_loss": 128603.58654785156, "training_acc": 54.0, "val_loss": 17409.27276611328, "val_acc": 60.0}
{"epoch": 43, "training_loss": 60652.57568359375, "training_acc": 70.0, "val_loss": 24230.57403564453, "val_acc": 56.0}
{"epoch": 44, "training_loss": 87171.92431640625, "training_acc": 64.0, "val_loss": 75675.96435546875, "val_acc": 48.0}
