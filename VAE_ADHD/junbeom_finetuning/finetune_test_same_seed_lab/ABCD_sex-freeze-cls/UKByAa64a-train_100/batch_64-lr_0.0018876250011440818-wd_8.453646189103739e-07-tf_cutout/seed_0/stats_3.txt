"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.48242402076721, "training_acc": 53.0, "val_loss": 17.358382046222687, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.40633726119995, "training_acc": 53.0, "val_loss": 17.291873693466187, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.40277004241943, "training_acc": 52.0, "val_loss": 17.282070219516754, "val_acc": 52.0}
{"epoch": 3, "training_loss": 68.95398545265198, "training_acc": 53.0, "val_loss": 17.268195748329163, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.24252772331238, "training_acc": 53.0, "val_loss": 17.284801602363586, "val_acc": 52.0}
{"epoch": 5, "training_loss": 68.93207955360413, "training_acc": 53.0, "val_loss": 17.265954613685608, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.29685163497925, "training_acc": 53.0, "val_loss": 17.25805252790451, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.02555823326111, "training_acc": 53.0, "val_loss": 17.23473370075226, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.0895266532898, "training_acc": 53.0, "val_loss": 17.240457236766815, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.00056886672974, "training_acc": 51.0, "val_loss": 17.254699766635895, "val_acc": 52.0}
{"epoch": 10, "training_loss": 68.93669748306274, "training_acc": 59.0, "val_loss": 17.266669869422913, "val_acc": 52.0}
{"epoch": 11, "training_loss": 68.95843434333801, "training_acc": 55.0, "val_loss": 17.29111820459366, "val_acc": 52.0}
{"epoch": 12, "training_loss": 68.62873888015747, "training_acc": 53.0, "val_loss": 17.370489239692688, "val_acc": 52.0}
{"epoch": 13, "training_loss": 68.78639101982117, "training_acc": 53.0, "val_loss": 17.461860179901123, "val_acc": 52.0}
{"epoch": 14, "training_loss": 68.88633680343628, "training_acc": 53.0, "val_loss": 17.466503381729126, "val_acc": 52.0}
{"epoch": 15, "training_loss": 68.68156123161316, "training_acc": 53.0, "val_loss": 17.44043081998825, "val_acc": 52.0}
{"epoch": 16, "training_loss": 68.72258734703064, "training_acc": 54.0, "val_loss": 17.416109144687653, "val_acc": 52.0}
{"epoch": 17, "training_loss": 68.5177595615387, "training_acc": 56.0, "val_loss": 17.44534820318222, "val_acc": 52.0}
{"epoch": 18, "training_loss": 68.54129409790039, "training_acc": 55.0, "val_loss": 17.51668006181717, "val_acc": 52.0}
{"epoch": 19, "training_loss": 68.53853058815002, "training_acc": 54.0, "val_loss": 17.616689205169678, "val_acc": 52.0}
{"epoch": 20, "training_loss": 68.28524088859558, "training_acc": 53.0, "val_loss": 17.717190086841583, "val_acc": 52.0}
{"epoch": 21, "training_loss": 68.7059965133667, "training_acc": 53.0, "val_loss": 17.76038557291031, "val_acc": 52.0}
{"epoch": 22, "training_loss": 67.4393162727356, "training_acc": 53.0, "val_loss": 17.74841696023941, "val_acc": 52.0}
{"epoch": 23, "training_loss": 68.60313773155212, "training_acc": 51.0, "val_loss": 17.722560465335846, "val_acc": 52.0}
{"epoch": 24, "training_loss": 68.5036895275116, "training_acc": 50.0, "val_loss": 17.696021497249603, "val_acc": 52.0}
{"epoch": 25, "training_loss": 67.31969404220581, "training_acc": 58.0, "val_loss": 17.644980549812317, "val_acc": 52.0}
{"epoch": 26, "training_loss": 67.49594807624817, "training_acc": 59.0, "val_loss": 17.638003826141357, "val_acc": 52.0}
