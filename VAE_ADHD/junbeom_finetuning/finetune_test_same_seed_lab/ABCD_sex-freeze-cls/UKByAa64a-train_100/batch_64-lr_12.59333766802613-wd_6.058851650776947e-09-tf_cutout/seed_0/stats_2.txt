"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 30544.93145751953, "training_acc": 43.0, "val_loss": 7403.72314453125, "val_acc": 52.0}
{"epoch": 1, "training_loss": 26141.880249023438, "training_acc": 55.0, "val_loss": 11101.978302001953, "val_acc": 48.0}
{"epoch": 2, "training_loss": 40184.505859375, "training_acc": 47.0, "val_loss": 1145.0057983398438, "val_acc": 48.0}
{"epoch": 3, "training_loss": 15430.143188476562, "training_acc": 47.0, "val_loss": 12623.697662353516, "val_acc": 52.0}
{"epoch": 4, "training_loss": 49280.50769042969, "training_acc": 53.0, "val_loss": 11048.745727539062, "val_acc": 52.0}
{"epoch": 5, "training_loss": 37144.789978027344, "training_acc": 53.0, "val_loss": 498.4736919403076, "val_acc": 52.0}
{"epoch": 6, "training_loss": 13100.301513671875, "training_acc": 49.0, "val_loss": 12253.659057617188, "val_acc": 48.0}
{"epoch": 7, "training_loss": 50610.328857421875, "training_acc": 47.0, "val_loss": 11473.260498046875, "val_acc": 48.0}
{"epoch": 8, "training_loss": 39447.174072265625, "training_acc": 47.0, "val_loss": 812.6738548278809, "val_acc": 48.0}
{"epoch": 9, "training_loss": 10130.501098632812, "training_acc": 50.0, "val_loss": 10750.908660888672, "val_acc": 52.0}
{"epoch": 10, "training_loss": 43644.91955566406, "training_acc": 53.0, "val_loss": 12387.239837646484, "val_acc": 52.0}
{"epoch": 11, "training_loss": 44701.803466796875, "training_acc": 53.0, "val_loss": 6484.318542480469, "val_acc": 52.0}
{"epoch": 12, "training_loss": 18995.17967224121, "training_acc": 54.0, "val_loss": 6013.087844848633, "val_acc": 48.0}
{"epoch": 13, "training_loss": 28557.719116210938, "training_acc": 47.0, "val_loss": 11054.105377197266, "val_acc": 48.0}
{"epoch": 14, "training_loss": 40595.66796875, "training_acc": 47.0, "val_loss": 6535.626220703125, "val_acc": 48.0}
{"epoch": 15, "training_loss": 17141.483001708984, "training_acc": 47.0, "val_loss": 5061.281967163086, "val_acc": 52.0}
{"epoch": 16, "training_loss": 24778.342895507812, "training_acc": 53.0, "val_loss": 10439.022827148438, "val_acc": 52.0}
{"epoch": 17, "training_loss": 40354.09777832031, "training_acc": 53.0, "val_loss": 7803.595733642578, "val_acc": 52.0}
{"epoch": 18, "training_loss": 24357.26580810547, "training_acc": 53.0, "val_loss": 1589.9677276611328, "val_acc": 48.0}
{"epoch": 19, "training_loss": 10892.255737304688, "training_acc": 48.0, "val_loss": 5472.882843017578, "val_acc": 48.0}
{"epoch": 20, "training_loss": 18077.273040771484, "training_acc": 47.0, "val_loss": 334.8231077194214, "val_acc": 48.0}
{"epoch": 21, "training_loss": 5946.026428222656, "training_acc": 54.0, "val_loss": 4984.082412719727, "val_acc": 52.0}
{"epoch": 22, "training_loss": 17824.960815429688, "training_acc": 53.0, "val_loss": 2143.159866333008, "val_acc": 52.0}
{"epoch": 23, "training_loss": 7076.305114746094, "training_acc": 53.0, "val_loss": 3386.0931396484375, "val_acc": 48.0}
{"epoch": 24, "training_loss": 12740.808471679688, "training_acc": 47.0, "val_loss": 310.98759174346924, "val_acc": 52.0}
{"epoch": 25, "training_loss": 5097.674224853516, "training_acc": 59.0, "val_loss": 4030.865478515625, "val_acc": 52.0}
{"epoch": 26, "training_loss": 12708.733459472656, "training_acc": 53.0, "val_loss": 367.71678924560547, "val_acc": 56.0}
{"epoch": 27, "training_loss": 5291.087615966797, "training_acc": 55.0, "val_loss": 3182.921028137207, "val_acc": 48.0}
{"epoch": 28, "training_loss": 10313.156967163086, "training_acc": 47.0, "val_loss": 2053.282928466797, "val_acc": 52.0}
{"epoch": 29, "training_loss": 8356.962341308594, "training_acc": 53.0, "val_loss": 2689.94140625, "val_acc": 52.0}
{"epoch": 30, "training_loss": 5998.278671264648, "training_acc": 60.0, "val_loss": 2746.8889236450195, "val_acc": 48.0}
{"epoch": 31, "training_loss": 11224.109588623047, "training_acc": 48.0, "val_loss": 1696.8132019042969, "val_acc": 48.0}
{"epoch": 32, "training_loss": 5943.074020385742, "training_acc": 48.0, "val_loss": 2490.8729553222656, "val_acc": 52.0}
{"epoch": 33, "training_loss": 7461.784042358398, "training_acc": 54.0, "val_loss": 801.3202667236328, "val_acc": 44.0}
{"epoch": 34, "training_loss": 4777.92626953125, "training_acc": 48.0, "val_loss": 335.6600284576416, "val_acc": 52.0}
{"epoch": 35, "training_loss": 2907.158676147461, "training_acc": 62.0, "val_loss": 1604.5516967773438, "val_acc": 52.0}
{"epoch": 36, "training_loss": 3006.313564300537, "training_acc": 64.0, "val_loss": 1422.877311706543, "val_acc": 48.0}
{"epoch": 37, "training_loss": 5307.312255859375, "training_acc": 49.0, "val_loss": 1780.9499740600586, "val_acc": 52.0}
{"epoch": 38, "training_loss": 6522.624481201172, "training_acc": 55.0, "val_loss": 1653.3672332763672, "val_acc": 52.0}
{"epoch": 39, "training_loss": 3740.8524475097656, "training_acc": 59.0, "val_loss": 1112.1771812438965, "val_acc": 48.0}
{"epoch": 40, "training_loss": 3484.382350921631, "training_acc": 55.0, "val_loss": 1556.633472442627, "val_acc": 52.0}
{"epoch": 41, "training_loss": 3482.946044921875, "training_acc": 55.0, "val_loss": 1488.6591911315918, "val_acc": 48.0}
{"epoch": 42, "training_loss": 7950.039215087891, "training_acc": 47.0, "val_loss": 500.9683132171631, "val_acc": 64.0}
{"epoch": 43, "training_loss": 3274.8998107910156, "training_acc": 65.0, "val_loss": 1994.874382019043, "val_acc": 52.0}
