"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 26493.604419708252, "training_acc": 46.0, "val_loss": 5528.422546386719, "val_acc": 52.0}
{"epoch": 1, "training_loss": 26846.165649414062, "training_acc": 53.0, "val_loss": 14317.698669433594, "val_acc": 48.0}
{"epoch": 2, "training_loss": 53884.446044921875, "training_acc": 47.0, "val_loss": 3968.9212799072266, "val_acc": 48.0}
{"epoch": 3, "training_loss": 18776.094787597656, "training_acc": 51.0, "val_loss": 10456.195068359375, "val_acc": 52.0}
{"epoch": 4, "training_loss": 39726.625244140625, "training_acc": 53.0, "val_loss": 9330.364227294922, "val_acc": 52.0}
{"epoch": 5, "training_loss": 27692.205627441406, "training_acc": 53.0, "val_loss": 1621.2663650512695, "val_acc": 48.0}
{"epoch": 6, "training_loss": 14942.156372070312, "training_acc": 47.0, "val_loss": 4779.115676879883, "val_acc": 48.0}
{"epoch": 7, "training_loss": 16198.808135986328, "training_acc": 47.0, "val_loss": 3462.735366821289, "val_acc": 52.0}
{"epoch": 8, "training_loss": 13502.601684570312, "training_acc": 53.0, "val_loss": 6004.316329956055, "val_acc": 52.0}
{"epoch": 9, "training_loss": 19497.354522705078, "training_acc": 53.0, "val_loss": 756.5510272979736, "val_acc": 52.0}
{"epoch": 10, "training_loss": 7822.221008300781, "training_acc": 49.0, "val_loss": 5611.216735839844, "val_acc": 48.0}
{"epoch": 11, "training_loss": 21520.358337402344, "training_acc": 47.0, "val_loss": 1236.5723609924316, "val_acc": 48.0}
{"epoch": 12, "training_loss": 7802.8502197265625, "training_acc": 49.0, "val_loss": 5809.372329711914, "val_acc": 52.0}
{"epoch": 13, "training_loss": 22314.647094726562, "training_acc": 53.0, "val_loss": 4325.563430786133, "val_acc": 52.0}
{"epoch": 14, "training_loss": 11524.196060180664, "training_acc": 52.0, "val_loss": 4404.961395263672, "val_acc": 48.0}
{"epoch": 15, "training_loss": 21383.651489257812, "training_acc": 47.0, "val_loss": 6499.6490478515625, "val_acc": 48.0}
{"epoch": 16, "training_loss": 21981.974548339844, "training_acc": 47.0, "val_loss": 533.6375713348389, "val_acc": 60.0}
{"epoch": 17, "training_loss": 5491.3516845703125, "training_acc": 55.0, "val_loss": 3998.2498168945312, "val_acc": 52.0}
{"epoch": 18, "training_loss": 13141.476577758789, "training_acc": 53.0, "val_loss": 269.1124200820923, "val_acc": 44.0}
{"epoch": 19, "training_loss": 3272.542694091797, "training_acc": 61.0, "val_loss": 1672.6852416992188, "val_acc": 48.0}
{"epoch": 20, "training_loss": 6076.361312866211, "training_acc": 46.0, "val_loss": 1737.137794494629, "val_acc": 52.0}
{"epoch": 21, "training_loss": 3783.5174713134766, "training_acc": 60.0, "val_loss": 1656.4104080200195, "val_acc": 48.0}
{"epoch": 22, "training_loss": 6915.0596923828125, "training_acc": 47.0, "val_loss": 778.1147480010986, "val_acc": 56.0}
{"epoch": 23, "training_loss": 3447.103530883789, "training_acc": 60.0, "val_loss": 1173.7689971923828, "val_acc": 56.0}
{"epoch": 24, "training_loss": 4045.9410247802734, "training_acc": 51.0, "val_loss": 1348.577880859375, "val_acc": 48.0}
{"epoch": 25, "training_loss": 3751.977569580078, "training_acc": 48.0, "val_loss": 1431.9464683532715, "val_acc": 52.0}
{"epoch": 26, "training_loss": 3594.5176887512207, "training_acc": 49.0, "val_loss": 356.04708194732666, "val_acc": 44.0}
{"epoch": 27, "training_loss": 2242.515167236328, "training_acc": 53.0, "val_loss": 611.6963386535645, "val_acc": 60.0}
{"epoch": 28, "training_loss": 1977.358642578125, "training_acc": 60.0, "val_loss": 341.620135307312, "val_acc": 44.0}
{"epoch": 29, "training_loss": 1758.266258239746, "training_acc": 63.0, "val_loss": 1302.2847175598145, "val_acc": 56.0}
{"epoch": 30, "training_loss": 3617.0475845336914, "training_acc": 50.0, "val_loss": 314.94905948638916, "val_acc": 48.0}
{"epoch": 31, "training_loss": 1823.5706176757812, "training_acc": 66.0, "val_loss": 1369.792366027832, "val_acc": 52.0}
{"epoch": 32, "training_loss": 2868.484115600586, "training_acc": 53.0, "val_loss": 283.02416801452637, "val_acc": 36.0}
{"epoch": 33, "training_loss": 2288.112075805664, "training_acc": 53.0, "val_loss": 370.23472785949707, "val_acc": 64.0}
{"epoch": 34, "training_loss": 1080.3328247070312, "training_acc": 68.0, "val_loss": 345.1469659805298, "val_acc": 64.0}
{"epoch": 35, "training_loss": 1102.4174995422363, "training_acc": 65.0, "val_loss": 661.8294715881348, "val_acc": 56.0}
{"epoch": 36, "training_loss": 1250.0133152008057, "training_acc": 61.0, "val_loss": 239.05658721923828, "val_acc": 68.0}
{"epoch": 37, "training_loss": 914.0802955627441, "training_acc": 61.0, "val_loss": 562.861967086792, "val_acc": 48.0}
{"epoch": 38, "training_loss": 2095.847068786621, "training_acc": 50.0, "val_loss": 141.69594049453735, "val_acc": 56.0}
{"epoch": 39, "training_loss": 790.5246238708496, "training_acc": 56.0, "val_loss": 103.28118801116943, "val_acc": 64.0}
{"epoch": 40, "training_loss": 1162.560890197754, "training_acc": 64.0, "val_loss": 83.03109407424927, "val_acc": 64.0}
{"epoch": 41, "training_loss": 223.4361915588379, "training_acc": 78.0, "val_loss": 325.1681327819824, "val_acc": 48.0}
{"epoch": 42, "training_loss": 1915.9819641113281, "training_acc": 55.0, "val_loss": 805.128288269043, "val_acc": 52.0}
{"epoch": 43, "training_loss": 2736.366500854492, "training_acc": 59.0, "val_loss": 1448.739242553711, "val_acc": 48.0}
{"epoch": 44, "training_loss": 5100.522079467773, "training_acc": 45.0, "val_loss": 856.4044952392578, "val_acc": 52.0}
{"epoch": 45, "training_loss": 4445.626617431641, "training_acc": 45.0, "val_loss": 983.0048561096191, "val_acc": 48.0}
{"epoch": 46, "training_loss": 5761.066314697266, "training_acc": 41.0, "val_loss": 2190.3974533081055, "val_acc": 52.0}
{"epoch": 47, "training_loss": 5456.727865219116, "training_acc": 58.0, "val_loss": 2649.990463256836, "val_acc": 48.0}
{"epoch": 48, "training_loss": 10708.450805664062, "training_acc": 47.0, "val_loss": 790.5173778533936, "val_acc": 48.0}
{"epoch": 49, "training_loss": 4866.819000244141, "training_acc": 57.0, "val_loss": 4752.272796630859, "val_acc": 52.0}
{"epoch": 50, "training_loss": 17298.842712402344, "training_acc": 53.0, "val_loss": 1857.0430755615234, "val_acc": 52.0}
{"epoch": 51, "training_loss": 7122.975616455078, "training_acc": 51.0, "val_loss": 3807.6675415039062, "val_acc": 48.0}
{"epoch": 52, "training_loss": 12263.46337890625, "training_acc": 48.0, "val_loss": 502.92468070983887, "val_acc": 56.0}
{"epoch": 53, "training_loss": 3527.800567626953, "training_acc": 60.0, "val_loss": 764.8055076599121, "val_acc": 56.0}
{"epoch": 54, "training_loss": 4183.684539794922, "training_acc": 53.0, "val_loss": 2363.3766174316406, "val_acc": 48.0}
{"epoch": 55, "training_loss": 4928.720775604248, "training_acc": 54.0, "val_loss": 2476.588821411133, "val_acc": 52.0}
{"epoch": 56, "training_loss": 9899.014678955078, "training_acc": 53.0, "val_loss": 1923.5176086425781, "val_acc": 52.0}
{"epoch": 57, "training_loss": 5565.682647705078, "training_acc": 51.0, "val_loss": 1457.8292846679688, "val_acc": 48.0}
{"epoch": 58, "training_loss": 3475.706974029541, "training_acc": 53.0, "val_loss": 827.8354644775391, "val_acc": 56.0}
{"epoch": 59, "training_loss": 2010.4609603881836, "training_acc": 58.0, "val_loss": 264.0911102294922, "val_acc": 64.0}
