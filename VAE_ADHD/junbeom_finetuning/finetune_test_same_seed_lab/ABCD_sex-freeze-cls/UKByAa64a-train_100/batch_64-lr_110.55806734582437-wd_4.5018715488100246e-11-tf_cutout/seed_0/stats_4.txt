"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 349516.52013778687, "training_acc": 44.0, "val_loss": 218901.85546875, "val_acc": 52.0}
{"epoch": 1, "training_loss": 895060.98828125, "training_acc": 53.0, "val_loss": 88373.74877929688, "val_acc": 52.0}
{"epoch": 2, "training_loss": 631296.93359375, "training_acc": 41.0, "val_loss": 245224.609375, "val_acc": 48.0}
{"epoch": 3, "training_loss": 877579.453125, "training_acc": 47.0, "val_loss": 24615.91033935547, "val_acc": 52.0}
{"epoch": 4, "training_loss": 169989.55078125, "training_acc": 53.0, "val_loss": 36661.15417480469, "val_acc": 52.0}
{"epoch": 5, "training_loss": 261105.904296875, "training_acc": 43.0, "val_loss": 92860.38818359375, "val_acc": 48.0}
{"epoch": 6, "training_loss": 257694.36462402344, "training_acc": 47.0, "val_loss": 128644.384765625, "val_acc": 52.0}
{"epoch": 7, "training_loss": 587963.978515625, "training_acc": 53.0, "val_loss": 174230.87158203125, "val_acc": 52.0}
{"epoch": 8, "training_loss": 575489.966796875, "training_acc": 53.0, "val_loss": 27117.263793945312, "val_acc": 48.0}
{"epoch": 9, "training_loss": 189042.0908203125, "training_acc": 47.0, "val_loss": 51223.6328125, "val_acc": 48.0}
{"epoch": 10, "training_loss": 161291.322265625, "training_acc": 55.0, "val_loss": 63062.14599609375, "val_acc": 52.0}
{"epoch": 11, "training_loss": 202738.060546875, "training_acc": 53.0, "val_loss": 61705.560302734375, "val_acc": 48.0}
{"epoch": 12, "training_loss": 268290.0830078125, "training_acc": 47.0, "val_loss": 24317.727661132812, "val_acc": 48.0}
{"epoch": 13, "training_loss": 178534.583984375, "training_acc": 49.0, "val_loss": 121618.896484375, "val_acc": 52.0}
{"epoch": 14, "training_loss": 449783.5634765625, "training_acc": 53.0, "val_loss": 27802.79541015625, "val_acc": 52.0}
{"epoch": 15, "training_loss": 226611.640625, "training_acc": 49.0, "val_loss": 150670.17822265625, "val_acc": 48.0}
{"epoch": 16, "training_loss": 580119.5390625, "training_acc": 47.0, "val_loss": 41758.87756347656, "val_acc": 48.0}
{"epoch": 17, "training_loss": 230197.80859375, "training_acc": 51.0, "val_loss": 160453.77197265625, "val_acc": 52.0}
{"epoch": 18, "training_loss": 649738.005859375, "training_acc": 53.0, "val_loss": 120764.8681640625, "val_acc": 52.0}
{"epoch": 19, "training_loss": 341790.9270019531, "training_acc": 53.0, "val_loss": 131058.06884765625, "val_acc": 48.0}
{"epoch": 20, "training_loss": 645427.40625, "training_acc": 47.0, "val_loss": 203055.11474609375, "val_acc": 48.0}
{"epoch": 21, "training_loss": 725864.236328125, "training_acc": 47.0, "val_loss": 10936.402130126953, "val_acc": 48.0}
{"epoch": 22, "training_loss": 180000.49609375, "training_acc": 57.0, "val_loss": 247020.0927734375, "val_acc": 52.0}
{"epoch": 23, "training_loss": 1039848.1015625, "training_acc": 53.0, "val_loss": 265227.9541015625, "val_acc": 52.0}
{"epoch": 24, "training_loss": 927753.6796875, "training_acc": 53.0, "val_loss": 68298.32153320312, "val_acc": 52.0}
{"epoch": 25, "training_loss": 291901.138671875, "training_acc": 55.0, "val_loss": 200990.087890625, "val_acc": 48.0}
{"epoch": 26, "training_loss": 835032.841796875, "training_acc": 47.0, "val_loss": 181376.28173828125, "val_acc": 48.0}
{"epoch": 27, "training_loss": 587618.375, "training_acc": 47.0, "val_loss": 62972.894287109375, "val_acc": 52.0}
{"epoch": 28, "training_loss": 346415.13671875, "training_acc": 53.0, "val_loss": 152146.10595703125, "val_acc": 52.0}
{"epoch": 29, "training_loss": 542510.822265625, "training_acc": 53.0, "val_loss": 25577.4658203125, "val_acc": 52.0}
{"epoch": 30, "training_loss": 245844.060546875, "training_acc": 51.0, "val_loss": 197502.34375, "val_acc": 48.0}
{"epoch": 31, "training_loss": 807436.24609375, "training_acc": 47.0, "val_loss": 134276.50146484375, "val_acc": 48.0}
{"epoch": 32, "training_loss": 367882.3122558594, "training_acc": 47.0, "val_loss": 155109.814453125, "val_acc": 52.0}
{"epoch": 33, "training_loss": 693073.48046875, "training_acc": 53.0, "val_loss": 277758.1787109375, "val_acc": 52.0}
{"epoch": 34, "training_loss": 1060852.86328125, "training_acc": 53.0, "val_loss": 188689.84375, "val_acc": 52.0}
{"epoch": 35, "training_loss": 569202.935546875, "training_acc": 53.0, "val_loss": 82913.40942382812, "val_acc": 48.0}
{"epoch": 36, "training_loss": 443816.236328125, "training_acc": 47.0, "val_loss": 185131.3232421875, "val_acc": 48.0}
{"epoch": 37, "training_loss": 681553.880859375, "training_acc": 47.0, "val_loss": 27059.912109375, "val_acc": 48.0}
{"epoch": 38, "training_loss": 303132.0546875, "training_acc": 41.0, "val_loss": 201288.623046875, "val_acc": 52.0}
{"epoch": 39, "training_loss": 835096.265625, "training_acc": 53.0, "val_loss": 173983.49609375, "val_acc": 52.0}
{"epoch": 40, "training_loss": 537904.59765625, "training_acc": 53.0, "val_loss": 70592.25463867188, "val_acc": 48.0}
