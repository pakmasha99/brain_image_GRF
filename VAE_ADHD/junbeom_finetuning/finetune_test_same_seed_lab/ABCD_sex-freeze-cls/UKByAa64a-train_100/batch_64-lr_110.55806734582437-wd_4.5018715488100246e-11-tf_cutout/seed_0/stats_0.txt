"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 422876.8478088379, "training_acc": 52.0, "val_loss": 140523.8525390625, "val_acc": 44.0}
{"epoch": 1, "training_loss": 431604.0888671875, "training_acc": 48.0, "val_loss": 147812.60986328125, "val_acc": 56.0}
{"epoch": 2, "training_loss": 684958.51171875, "training_acc": 52.0, "val_loss": 62154.23583984375, "val_acc": 56.0}
{"epoch": 3, "training_loss": 353512.4453125, "training_acc": 54.0, "val_loss": 237696.923828125, "val_acc": 44.0}
{"epoch": 4, "training_loss": 798297.189453125, "training_acc": 48.0, "val_loss": 20053.294372558594, "val_acc": 44.0}
{"epoch": 5, "training_loss": 371223.39453125, "training_acc": 44.0, "val_loss": 262214.5263671875, "val_acc": 56.0}
{"epoch": 6, "training_loss": 1146495.58203125, "training_acc": 52.0, "val_loss": 201248.681640625, "val_acc": 56.0}
{"epoch": 7, "training_loss": 700328.4580078125, "training_acc": 52.0, "val_loss": 112202.783203125, "val_acc": 44.0}
{"epoch": 8, "training_loss": 508186.623046875, "training_acc": 48.0, "val_loss": 216523.876953125, "val_acc": 44.0}
{"epoch": 9, "training_loss": 707118.1328125, "training_acc": 48.0, "val_loss": 6373.291397094727, "val_acc": 44.0}
{"epoch": 10, "training_loss": 237515.3828125, "training_acc": 52.0, "val_loss": 226747.5341796875, "val_acc": 56.0}
{"epoch": 11, "training_loss": 1014621.2734375, "training_acc": 52.0, "val_loss": 198850.4150390625, "val_acc": 56.0}
{"epoch": 12, "training_loss": 708626.9765625, "training_acc": 52.0, "val_loss": 44024.700927734375, "val_acc": 44.0}
{"epoch": 13, "training_loss": 281646.01953125, "training_acc": 48.0, "val_loss": 132765.46630859375, "val_acc": 44.0}
{"epoch": 14, "training_loss": 384169.12890625, "training_acc": 48.0, "val_loss": 62372.393798828125, "val_acc": 56.0}
{"epoch": 15, "training_loss": 330665.72265625, "training_acc": 52.0, "val_loss": 97659.375, "val_acc": 56.0}
{"epoch": 16, "training_loss": 331572.11962890625, "training_acc": 52.0, "val_loss": 84820.62377929688, "val_acc": 44.0}
{"epoch": 17, "training_loss": 376524.158203125, "training_acc": 48.0, "val_loss": 106840.4296875, "val_acc": 44.0}
{"epoch": 18, "training_loss": 259286.8466796875, "training_acc": 48.0, "val_loss": 115251.025390625, "val_acc": 56.0}
{"epoch": 19, "training_loss": 598786.572265625, "training_acc": 52.0, "val_loss": 177769.78759765625, "val_acc": 56.0}
{"epoch": 20, "training_loss": 696310.568359375, "training_acc": 52.0, "val_loss": 31984.326171875, "val_acc": 56.0}
{"epoch": 21, "training_loss": 294947.9140625, "training_acc": 46.0, "val_loss": 217524.755859375, "val_acc": 44.0}
{"epoch": 22, "training_loss": 795532.33203125, "training_acc": 48.0, "val_loss": 151146.533203125, "val_acc": 44.0}
{"epoch": 23, "training_loss": 394868.701171875, "training_acc": 48.0, "val_loss": 125022.0703125, "val_acc": 56.0}
{"epoch": 24, "training_loss": 682529.17578125, "training_acc": 52.0, "val_loss": 220672.8271484375, "val_acc": 56.0}
{"epoch": 25, "training_loss": 904746.568359375, "training_acc": 52.0, "val_loss": 101857.40966796875, "val_acc": 56.0}
{"epoch": 26, "training_loss": 242731.76330566406, "training_acc": 62.0, "val_loss": 99680.59692382812, "val_acc": 44.0}
{"epoch": 27, "training_loss": 365565.736328125, "training_acc": 48.0, "val_loss": 38213.87939453125, "val_acc": 44.0}
{"epoch": 28, "training_loss": 169565.16796875, "training_acc": 54.0, "val_loss": 110784.92431640625, "val_acc": 56.0}
