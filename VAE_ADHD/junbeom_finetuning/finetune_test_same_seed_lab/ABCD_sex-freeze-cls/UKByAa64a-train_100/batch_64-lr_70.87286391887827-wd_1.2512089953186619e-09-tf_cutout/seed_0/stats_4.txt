"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 123452.45644378662, "training_acc": 50.0, "val_loss": 22643.995666503906, "val_acc": 52.0}
{"epoch": 1, "training_loss": 154415.9052734375, "training_acc": 51.0, "val_loss": 87692.138671875, "val_acc": 48.0}
{"epoch": 2, "training_loss": 342391.8876953125, "training_acc": 47.0, "val_loss": 24561.49139404297, "val_acc": 48.0}
{"epoch": 3, "training_loss": 132342.81689453125, "training_acc": 41.0, "val_loss": 52134.08203125, "val_acc": 52.0}
{"epoch": 4, "training_loss": 198581.296875, "training_acc": 53.0, "val_loss": 39392.0654296875, "val_acc": 52.0}
{"epoch": 5, "training_loss": 112124.6640625, "training_acc": 51.0, "val_loss": 23641.281127929688, "val_acc": 48.0}
{"epoch": 6, "training_loss": 116402.6533203125, "training_acc": 47.0, "val_loss": 34759.12170410156, "val_acc": 48.0}
{"epoch": 7, "training_loss": 121206.04565429688, "training_acc": 47.0, "val_loss": 11774.117279052734, "val_acc": 52.0}
{"epoch": 8, "training_loss": 56437.8017578125, "training_acc": 52.0, "val_loss": 25500.914001464844, "val_acc": 52.0}
{"epoch": 9, "training_loss": 81017.73754882812, "training_acc": 53.0, "val_loss": 5968.319320678711, "val_acc": 48.0}
{"epoch": 10, "training_loss": 33836.386474609375, "training_acc": 48.0, "val_loss": 4151.851654052734, "val_acc": 48.0}
{"epoch": 11, "training_loss": 25542.455078125, "training_acc": 57.0, "val_loss": 16481.077575683594, "val_acc": 52.0}
{"epoch": 12, "training_loss": 52389.028076171875, "training_acc": 53.0, "val_loss": 6985.792541503906, "val_acc": 48.0}
{"epoch": 13, "training_loss": 28280.09197998047, "training_acc": 47.0, "val_loss": 1406.3166618347168, "val_acc": 64.0}
{"epoch": 14, "training_loss": 10562.023681640625, "training_acc": 61.0, "val_loss": 2604.6920776367188, "val_acc": 56.0}
{"epoch": 15, "training_loss": 19035.750122070312, "training_acc": 50.0, "val_loss": 6026.126480102539, "val_acc": 48.0}
{"epoch": 16, "training_loss": 36627.578369140625, "training_acc": 37.0, "val_loss": 11358.907318115234, "val_acc": 52.0}
{"epoch": 17, "training_loss": 28864.966552734375, "training_acc": 57.0, "val_loss": 16240.034484863281, "val_acc": 48.0}
{"epoch": 18, "training_loss": 74106.9384765625, "training_acc": 47.0, "val_loss": 13919.309997558594, "val_acc": 48.0}
{"epoch": 19, "training_loss": 38992.93615722656, "training_acc": 53.0, "val_loss": 13712.7685546875, "val_acc": 52.0}
{"epoch": 20, "training_loss": 49081.007568359375, "training_acc": 53.0, "val_loss": 2298.636245727539, "val_acc": 44.0}
{"epoch": 21, "training_loss": 18040.3427734375, "training_acc": 56.0, "val_loss": 2663.937759399414, "val_acc": 44.0}
{"epoch": 22, "training_loss": 18379.754516601562, "training_acc": 59.0, "val_loss": 10364.362335205078, "val_acc": 52.0}
{"epoch": 23, "training_loss": 24351.4208984375, "training_acc": 58.0, "val_loss": 11978.931427001953, "val_acc": 48.0}
{"epoch": 24, "training_loss": 42314.62353515625, "training_acc": 47.0, "val_loss": 2054.2421340942383, "val_acc": 52.0}
{"epoch": 25, "training_loss": 12988.800170898438, "training_acc": 60.0, "val_loss": 1340.0644302368164, "val_acc": 60.0}
{"epoch": 26, "training_loss": 9776.2880859375, "training_acc": 64.0, "val_loss": 2137.6012802124023, "val_acc": 48.0}
{"epoch": 27, "training_loss": 12858.011962890625, "training_acc": 62.0, "val_loss": 5841.804885864258, "val_acc": 52.0}
{"epoch": 28, "training_loss": 19028.309692382812, "training_acc": 55.0, "val_loss": 4471.643829345703, "val_acc": 48.0}
{"epoch": 29, "training_loss": 22384.250366210938, "training_acc": 46.0, "val_loss": 6640.592956542969, "val_acc": 52.0}
{"epoch": 30, "training_loss": 21497.891845703125, "training_acc": 52.0, "val_loss": 3954.2606353759766, "val_acc": 48.0}
{"epoch": 31, "training_loss": 16424.46600341797, "training_acc": 52.0, "val_loss": 6917.1142578125, "val_acc": 52.0}
{"epoch": 32, "training_loss": 24082.06524658203, "training_acc": 45.0, "val_loss": 1621.0809707641602, "val_acc": 56.0}
{"epoch": 33, "training_loss": 12778.426147460938, "training_acc": 57.0, "val_loss": 1804.0657043457031, "val_acc": 52.0}
{"epoch": 34, "training_loss": 13550.5966796875, "training_acc": 58.0, "val_loss": 1102.1882057189941, "val_acc": 60.0}
{"epoch": 35, "training_loss": 11389.885131835938, "training_acc": 66.0, "val_loss": 7992.814636230469, "val_acc": 52.0}
{"epoch": 36, "training_loss": 23148.241912841797, "training_acc": 51.0, "val_loss": 2392.7215576171875, "val_acc": 48.0}
{"epoch": 37, "training_loss": 15159.16162109375, "training_acc": 59.0, "val_loss": 4101.638412475586, "val_acc": 52.0}
{"epoch": 38, "training_loss": 23345.6806640625, "training_acc": 44.0, "val_loss": 4389.131164550781, "val_acc": 48.0}
{"epoch": 39, "training_loss": 20606.079711914062, "training_acc": 51.0, "val_loss": 10933.187103271484, "val_acc": 52.0}
{"epoch": 40, "training_loss": 27034.172882080078, "training_acc": 59.0, "val_loss": 11217.96646118164, "val_acc": 48.0}
{"epoch": 41, "training_loss": 46217.530517578125, "training_acc": 47.0, "val_loss": 909.5072746276855, "val_acc": 60.0}
{"epoch": 42, "training_loss": 15706.441162109375, "training_acc": 66.0, "val_loss": 10163.044738769531, "val_acc": 52.0}
{"epoch": 43, "training_loss": 24620.922912597656, "training_acc": 52.0, "val_loss": 9047.96142578125, "val_acc": 48.0}
{"epoch": 44, "training_loss": 26853.361785888672, "training_acc": 48.0, "val_loss": 12283.432006835938, "val_acc": 52.0}
{"epoch": 45, "training_loss": 46480.91455078125, "training_acc": 53.0, "val_loss": 8721.365356445312, "val_acc": 52.0}
{"epoch": 46, "training_loss": 23727.35955810547, "training_acc": 55.0, "val_loss": 10947.109985351562, "val_acc": 48.0}
{"epoch": 47, "training_loss": 29044.345336914062, "training_acc": 52.0, "val_loss": 12031.916809082031, "val_acc": 52.0}
{"epoch": 48, "training_loss": 47399.916748046875, "training_acc": 53.0, "val_loss": 5730.752944946289, "val_acc": 52.0}
{"epoch": 49, "training_loss": 26137.139892578125, "training_acc": 53.0, "val_loss": 18126.01776123047, "val_acc": 48.0}
{"epoch": 50, "training_loss": 55592.22253417969, "training_acc": 47.0, "val_loss": 8973.420715332031, "val_acc": 52.0}
{"epoch": 51, "training_loss": 40365.0302734375, "training_acc": 53.0, "val_loss": 10273.355102539062, "val_acc": 52.0}
{"epoch": 52, "training_loss": 30335.643615722656, "training_acc": 52.0, "val_loss": 7591.7083740234375, "val_acc": 48.0}
{"epoch": 53, "training_loss": 20556.443817138672, "training_acc": 52.0, "val_loss": 6499.84130859375, "val_acc": 52.0}
{"epoch": 54, "training_loss": 19027.465393066406, "training_acc": 58.0, "val_loss": 10013.802337646484, "val_acc": 48.0}
{"epoch": 55, "training_loss": 33311.34143066406, "training_acc": 51.0, "val_loss": 2561.087989807129, "val_acc": 56.0}
{"epoch": 56, "training_loss": 14042.234375, "training_acc": 61.0, "val_loss": 1498.7435340881348, "val_acc": 60.0}
{"epoch": 57, "training_loss": 16569.152099609375, "training_acc": 57.0, "val_loss": 7321.006011962891, "val_acc": 48.0}
{"epoch": 58, "training_loss": 25955.223266601562, "training_acc": 45.0, "val_loss": 7839.393615722656, "val_acc": 52.0}
{"epoch": 59, "training_loss": 15252.166015625, "training_acc": 65.0, "val_loss": 6819.194030761719, "val_acc": 48.0}
{"epoch": 60, "training_loss": 15449.537353515625, "training_acc": 57.0, "val_loss": 9317.17300415039, "val_acc": 52.0}
