"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 164907.50940704346, "training_acc": 53.0, "val_loss": 29157.60498046875, "val_acc": 52.0}
{"epoch": 1, "training_loss": 152239.7470703125, "training_acc": 53.0, "val_loss": 85904.88891601562, "val_acc": 48.0}
{"epoch": 2, "training_loss": 331010.7001953125, "training_acc": 47.0, "val_loss": 34864.14794921875, "val_acc": 48.0}
{"epoch": 3, "training_loss": 107558.90747070312, "training_acc": 53.0, "val_loss": 43237.31994628906, "val_acc": 52.0}
{"epoch": 4, "training_loss": 168979.61279296875, "training_acc": 53.0, "val_loss": 35924.51477050781, "val_acc": 52.0}
{"epoch": 5, "training_loss": 106164.89636230469, "training_acc": 53.0, "val_loss": 26947.250366210938, "val_acc": 48.0}
{"epoch": 6, "training_loss": 128373.78466796875, "training_acc": 47.0, "val_loss": 38923.45886230469, "val_acc": 48.0}
{"epoch": 7, "training_loss": 133863.88793945312, "training_acc": 47.0, "val_loss": 7842.246246337891, "val_acc": 52.0}
{"epoch": 8, "training_loss": 41055.1328125, "training_acc": 53.0, "val_loss": 19808.973693847656, "val_acc": 52.0}
{"epoch": 9, "training_loss": 57997.12841796875, "training_acc": 53.0, "val_loss": 14510.05859375, "val_acc": 48.0}
{"epoch": 10, "training_loss": 71644.5009765625, "training_acc": 47.0, "val_loss": 14049.842834472656, "val_acc": 48.0}
{"epoch": 11, "training_loss": 47006.36639404297, "training_acc": 47.0, "val_loss": 12520.659637451172, "val_acc": 52.0}
{"epoch": 12, "training_loss": 35923.97491455078, "training_acc": 52.0, "val_loss": 6916.455841064453, "val_acc": 48.0}
{"epoch": 13, "training_loss": 33519.95568847656, "training_acc": 48.0, "val_loss": 2159.8608016967773, "val_acc": 56.0}
{"epoch": 14, "training_loss": 18274.92822265625, "training_acc": 47.0, "val_loss": 5439.722442626953, "val_acc": 52.0}
{"epoch": 15, "training_loss": 21037.86572265625, "training_acc": 56.0, "val_loss": 10449.832153320312, "val_acc": 48.0}
{"epoch": 16, "training_loss": 29979.027282714844, "training_acc": 49.0, "val_loss": 10699.16000366211, "val_acc": 52.0}
{"epoch": 17, "training_loss": 39130.48095703125, "training_acc": 53.0, "val_loss": 2584.288787841797, "val_acc": 56.0}
{"epoch": 18, "training_loss": 10427.565734863281, "training_acc": 54.0, "val_loss": 3117.147636413574, "val_acc": 56.0}
{"epoch": 19, "training_loss": 11121.279754638672, "training_acc": 59.0, "val_loss": 2452.336311340332, "val_acc": 56.0}
{"epoch": 20, "training_loss": 9636.38638305664, "training_acc": 59.0, "val_loss": 2721.9953536987305, "val_acc": 56.0}
{"epoch": 21, "training_loss": 12862.680419921875, "training_acc": 52.0, "val_loss": 1322.4614143371582, "val_acc": 52.0}
{"epoch": 22, "training_loss": 15314.7021484375, "training_acc": 61.0, "val_loss": 11563.753509521484, "val_acc": 52.0}
{"epoch": 23, "training_loss": 26975.87127685547, "training_acc": 51.0, "val_loss": 12473.490142822266, "val_acc": 48.0}
{"epoch": 24, "training_loss": 55469.025390625, "training_acc": 47.0, "val_loss": 811.491584777832, "val_acc": 60.0}
{"epoch": 25, "training_loss": 24180.04248046875, "training_acc": 62.0, "val_loss": 22899.200439453125, "val_acc": 52.0}
{"epoch": 26, "training_loss": 73085.13452148438, "training_acc": 53.0, "val_loss": 1397.661018371582, "val_acc": 56.0}
{"epoch": 27, "training_loss": 25976.9794921875, "training_acc": 59.0, "val_loss": 21879.991149902344, "val_acc": 48.0}
{"epoch": 28, "training_loss": 75519.46142578125, "training_acc": 47.0, "val_loss": 8687.935638427734, "val_acc": 52.0}
{"epoch": 29, "training_loss": 42122.95654296875, "training_acc": 53.0, "val_loss": 11458.333587646484, "val_acc": 52.0}
{"epoch": 30, "training_loss": 26304.033203125, "training_acc": 56.0, "val_loss": 8528.218841552734, "val_acc": 48.0}
{"epoch": 31, "training_loss": 21649.434600830078, "training_acc": 51.0, "val_loss": 12358.126831054688, "val_acc": 52.0}
{"epoch": 32, "training_loss": 42911.30920410156, "training_acc": 53.0, "val_loss": 1507.30562210083, "val_acc": 52.0}
{"epoch": 33, "training_loss": 15501.907470703125, "training_acc": 59.0, "val_loss": 7879.307556152344, "val_acc": 48.0}
{"epoch": 34, "training_loss": 34237.76354980469, "training_acc": 41.0, "val_loss": 9384.62905883789, "val_acc": 52.0}
{"epoch": 35, "training_loss": 25789.456665039062, "training_acc": 47.0, "val_loss": 1447.7422714233398, "val_acc": 52.0}
{"epoch": 36, "training_loss": 8459.97802734375, "training_acc": 64.0, "val_loss": 4039.1441345214844, "val_acc": 52.0}
{"epoch": 37, "training_loss": 10378.284729003906, "training_acc": 63.0, "val_loss": 4211.124801635742, "val_acc": 48.0}
{"epoch": 38, "training_loss": 24121.480346679688, "training_acc": 45.0, "val_loss": 10519.197845458984, "val_acc": 52.0}
{"epoch": 39, "training_loss": 20427.27587890625, "training_acc": 58.0, "val_loss": 6660.642242431641, "val_acc": 48.0}
{"epoch": 40, "training_loss": 21965.777587890625, "training_acc": 47.0, "val_loss": 16449.472045898438, "val_acc": 52.0}
{"epoch": 41, "training_loss": 63812.662109375, "training_acc": 53.0, "val_loss": 18623.16436767578, "val_acc": 52.0}
{"epoch": 42, "training_loss": 46162.1354675293, "training_acc": 56.0, "val_loss": 19525.5615234375, "val_acc": 48.0}
{"epoch": 43, "training_loss": 92681.11572265625, "training_acc": 47.0, "val_loss": 20913.319396972656, "val_acc": 48.0}
