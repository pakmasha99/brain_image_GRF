"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 2124.2586975097656, "training_acc": 54.0, "val_loss": 56.55970573425293, "val_acc": 56.0}
{"epoch": 1, "training_loss": 2419.258514404297, "training_acc": 50.0, "val_loss": 1982.7693939208984, "val_acc": 44.0}
{"epoch": 2, "training_loss": 6348.835235595703, "training_acc": 48.0, "val_loss": 365.85583686828613, "val_acc": 44.0}
{"epoch": 3, "training_loss": 2091.6005096435547, "training_acc": 46.0, "val_loss": 1232.6298713684082, "val_acc": 56.0}
{"epoch": 4, "training_loss": 5315.010925292969, "training_acc": 52.0, "val_loss": 1095.211410522461, "val_acc": 56.0}
{"epoch": 5, "training_loss": 3839.944480895996, "training_acc": 52.0, "val_loss": 56.81383013725281, "val_acc": 48.0}
{"epoch": 6, "training_loss": 1089.617416381836, "training_acc": 52.0, "val_loss": 1032.169246673584, "val_acc": 44.0}
{"epoch": 7, "training_loss": 3567.7391815185547, "training_acc": 48.0, "val_loss": 555.2150249481201, "val_acc": 44.0}
{"epoch": 8, "training_loss": 1620.1611957550049, "training_acc": 42.0, "val_loss": 338.8395309448242, "val_acc": 56.0}
{"epoch": 9, "training_loss": 1352.5381660461426, "training_acc": 52.0, "val_loss": 127.19218730926514, "val_acc": 52.0}
{"epoch": 10, "training_loss": 743.2303466796875, "training_acc": 52.0, "val_loss": 508.8979244232178, "val_acc": 44.0}
{"epoch": 11, "training_loss": 1542.63480758667, "training_acc": 48.0, "val_loss": 67.3380434513092, "val_acc": 56.0}
{"epoch": 12, "training_loss": 435.5243148803711, "training_acc": 58.0, "val_loss": 189.3238663673401, "val_acc": 52.0}
{"epoch": 13, "training_loss": 520.3676924705505, "training_acc": 53.0, "val_loss": 173.17707538604736, "val_acc": 40.0}
{"epoch": 14, "training_loss": 412.52417516708374, "training_acc": 50.0, "val_loss": 143.276047706604, "val_acc": 56.0}
{"epoch": 15, "training_loss": 436.5697536468506, "training_acc": 55.0, "val_loss": 107.38617181777954, "val_acc": 40.0}
{"epoch": 16, "training_loss": 329.98688411712646, "training_acc": 53.0, "val_loss": 82.86518454551697, "val_acc": 52.0}
{"epoch": 17, "training_loss": 202.6284055709839, "training_acc": 64.0, "val_loss": 58.90823006629944, "val_acc": 56.0}
{"epoch": 18, "training_loss": 270.3401908874512, "training_acc": 57.0, "val_loss": 53.02720069885254, "val_acc": 56.0}
{"epoch": 19, "training_loss": 286.58329582214355, "training_acc": 62.0, "val_loss": 49.12624657154083, "val_acc": 56.0}
{"epoch": 20, "training_loss": 265.48399925231934, "training_acc": 55.0, "val_loss": 45.091795921325684, "val_acc": 56.0}
{"epoch": 21, "training_loss": 169.73568058013916, "training_acc": 62.0, "val_loss": 46.90645635128021, "val_acc": 44.0}
{"epoch": 22, "training_loss": 105.12957859039307, "training_acc": 70.0, "val_loss": 33.820536732673645, "val_acc": 60.0}
{"epoch": 23, "training_loss": 147.50704956054688, "training_acc": 63.0, "val_loss": 85.41637063026428, "val_acc": 44.0}
{"epoch": 24, "training_loss": 217.14763879776, "training_acc": 51.0, "val_loss": 23.9725261926651, "val_acc": 68.0}
{"epoch": 25, "training_loss": 71.4557728767395, "training_acc": 70.0, "val_loss": 77.10353136062622, "val_acc": 56.0}
{"epoch": 26, "training_loss": 209.23999977111816, "training_acc": 63.0, "val_loss": 227.9728889465332, "val_acc": 44.0}
{"epoch": 27, "training_loss": 791.7388401031494, "training_acc": 48.0, "val_loss": 111.37161254882812, "val_acc": 56.0}
{"epoch": 28, "training_loss": 397.4703769683838, "training_acc": 52.0, "val_loss": 43.1069552898407, "val_acc": 52.0}
{"epoch": 29, "training_loss": 163.69606828689575, "training_acc": 60.0, "val_loss": 124.06144142150879, "val_acc": 56.0}
{"epoch": 30, "training_loss": 366.84249687194824, "training_acc": 56.0, "val_loss": 237.9321575164795, "val_acc": 44.0}
{"epoch": 31, "training_loss": 1001.7762985229492, "training_acc": 48.0, "val_loss": 18.71897280216217, "val_acc": 68.0}
{"epoch": 32, "training_loss": 405.29431915283203, "training_acc": 60.0, "val_loss": 229.23660278320312, "val_acc": 56.0}
{"epoch": 33, "training_loss": 716.6371796131134, "training_acc": 59.0, "val_loss": 288.78209590911865, "val_acc": 44.0}
{"epoch": 34, "training_loss": 967.0701904296875, "training_acc": 48.0, "val_loss": 20.31657099723816, "val_acc": 64.0}
{"epoch": 35, "training_loss": 299.25141525268555, "training_acc": 62.0, "val_loss": 74.7549295425415, "val_acc": 56.0}
{"epoch": 36, "training_loss": 528.8081398010254, "training_acc": 48.0, "val_loss": 237.46232986450195, "val_acc": 44.0}
{"epoch": 37, "training_loss": 462.8512442111969, "training_acc": 61.0, "val_loss": 204.33297157287598, "val_acc": 56.0}
{"epoch": 38, "training_loss": 760.584379196167, "training_acc": 52.0, "val_loss": 40.57115912437439, "val_acc": 52.0}
{"epoch": 39, "training_loss": 131.88628339767456, "training_acc": 56.0, "val_loss": 71.34923934936523, "val_acc": 52.0}
{"epoch": 40, "training_loss": 196.0465772151947, "training_acc": 62.0, "val_loss": 60.24385690689087, "val_acc": 48.0}
{"epoch": 41, "training_loss": 260.2941360473633, "training_acc": 52.0, "val_loss": 28.263190388679504, "val_acc": 68.0}
{"epoch": 42, "training_loss": 154.7696876525879, "training_acc": 60.0, "val_loss": 46.33042812347412, "val_acc": 56.0}
{"epoch": 43, "training_loss": 136.49730610847473, "training_acc": 66.0, "val_loss": 127.40867137908936, "val_acc": 44.0}
{"epoch": 44, "training_loss": 292.94027972221375, "training_acc": 60.0, "val_loss": 132.90014266967773, "val_acc": 56.0}
{"epoch": 45, "training_loss": 411.5166645050049, "training_acc": 53.0, "val_loss": 257.9885482788086, "val_acc": 44.0}
{"epoch": 46, "training_loss": 896.9163703918457, "training_acc": 48.0, "val_loss": 25.164389610290527, "val_acc": 68.0}
{"epoch": 47, "training_loss": 315.04570960998535, "training_acc": 70.0, "val_loss": 223.0710744857788, "val_acc": 56.0}
{"epoch": 48, "training_loss": 666.7993879318237, "training_acc": 54.0, "val_loss": 371.5610980987549, "val_acc": 44.0}
{"epoch": 49, "training_loss": 1327.2645416259766, "training_acc": 48.0, "val_loss": 264.88561630249023, "val_acc": 44.0}
{"epoch": 50, "training_loss": 806.3750591278076, "training_acc": 46.0, "val_loss": 248.18930625915527, "val_acc": 56.0}
