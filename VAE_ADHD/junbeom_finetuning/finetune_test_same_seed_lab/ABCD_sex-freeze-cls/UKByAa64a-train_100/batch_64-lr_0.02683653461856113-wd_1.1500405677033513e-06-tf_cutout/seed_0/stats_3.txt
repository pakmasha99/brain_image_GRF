"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 125.69994926452637, "training_acc": 46.0, "val_loss": 20.809979736804962, "val_acc": 48.0}
{"epoch": 1, "training_loss": 95.70050525665283, "training_acc": 41.0, "val_loss": 31.6225528717041, "val_acc": 52.0}
{"epoch": 2, "training_loss": 117.64560890197754, "training_acc": 53.0, "val_loss": 24.54305589199066, "val_acc": 52.0}
{"epoch": 3, "training_loss": 83.6616780757904, "training_acc": 53.0, "val_loss": 18.586836755275726, "val_acc": 40.0}
{"epoch": 4, "training_loss": 80.1678478717804, "training_acc": 47.0, "val_loss": 23.914755880832672, "val_acc": 48.0}
{"epoch": 5, "training_loss": 92.51203155517578, "training_acc": 47.0, "val_loss": 18.280798196792603, "val_acc": 48.0}
{"epoch": 6, "training_loss": 70.79283142089844, "training_acc": 51.0, "val_loss": 21.053269505500793, "val_acc": 52.0}
{"epoch": 7, "training_loss": 81.52110576629639, "training_acc": 53.0, "val_loss": 23.147691786289215, "val_acc": 52.0}
{"epoch": 8, "training_loss": 82.70749068260193, "training_acc": 53.0, "val_loss": 18.347463011741638, "val_acc": 52.0}
{"epoch": 9, "training_loss": 71.94445538520813, "training_acc": 44.0, "val_loss": 18.638870120048523, "val_acc": 44.0}
{"epoch": 10, "training_loss": 73.11467456817627, "training_acc": 47.0, "val_loss": 18.537025153636932, "val_acc": 40.0}
{"epoch": 11, "training_loss": 72.59628415107727, "training_acc": 47.0, "val_loss": 17.68999844789505, "val_acc": 52.0}
{"epoch": 12, "training_loss": 68.83699440956116, "training_acc": 59.0, "val_loss": 19.287197291851044, "val_acc": 52.0}
{"epoch": 13, "training_loss": 71.6813132762909, "training_acc": 53.0, "val_loss": 18.560020625591278, "val_acc": 52.0}
{"epoch": 14, "training_loss": 68.74789047241211, "training_acc": 56.0, "val_loss": 17.5803005695343, "val_acc": 52.0}
{"epoch": 15, "training_loss": 68.2376697063446, "training_acc": 54.0, "val_loss": 17.824074625968933, "val_acc": 52.0}
{"epoch": 16, "training_loss": 70.07144856452942, "training_acc": 48.0, "val_loss": 17.459869384765625, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.6930022239685, "training_acc": 46.0, "val_loss": 17.764365673065186, "val_acc": 52.0}
{"epoch": 18, "training_loss": 67.57078218460083, "training_acc": 54.0, "val_loss": 17.595821619033813, "val_acc": 52.0}
{"epoch": 19, "training_loss": 67.82029509544373, "training_acc": 56.0, "val_loss": 17.348472774028778, "val_acc": 52.0}
{"epoch": 20, "training_loss": 67.15376114845276, "training_acc": 60.0, "val_loss": 17.333155870437622, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.45201587677002, "training_acc": 53.0, "val_loss": 17.30457842350006, "val_acc": 52.0}
{"epoch": 22, "training_loss": 67.41284537315369, "training_acc": 56.0, "val_loss": 17.561200261116028, "val_acc": 52.0}
{"epoch": 23, "training_loss": 68.10978055000305, "training_acc": 55.0, "val_loss": 17.753998935222626, "val_acc": 52.0}
{"epoch": 24, "training_loss": 68.0817859172821, "training_acc": 53.0, "val_loss": 17.39308536052704, "val_acc": 52.0}
{"epoch": 25, "training_loss": 67.99868822097778, "training_acc": 59.0, "val_loss": 17.418090999126434, "val_acc": 52.0}
{"epoch": 26, "training_loss": 66.5043876171112, "training_acc": 64.0, "val_loss": 17.497926950454712, "val_acc": 52.0}
{"epoch": 27, "training_loss": 67.03217625617981, "training_acc": 58.0, "val_loss": 18.032224476337433, "val_acc": 52.0}
{"epoch": 28, "training_loss": 67.83589696884155, "training_acc": 57.0, "val_loss": 17.682626843452454, "val_acc": 52.0}
{"epoch": 29, "training_loss": 66.31940579414368, "training_acc": 58.0, "val_loss": 17.56747215986252, "val_acc": 52.0}
{"epoch": 30, "training_loss": 66.8724935054779, "training_acc": 64.0, "val_loss": 17.581382393836975, "val_acc": 52.0}
{"epoch": 31, "training_loss": 68.73699831962585, "training_acc": 54.0, "val_loss": 17.858077585697174, "val_acc": 52.0}
{"epoch": 32, "training_loss": 64.9874005317688, "training_acc": 64.0, "val_loss": 17.639347910881042, "val_acc": 52.0}
{"epoch": 33, "training_loss": 65.80712175369263, "training_acc": 61.0, "val_loss": 17.51372069120407, "val_acc": 52.0}
{"epoch": 34, "training_loss": 66.79311347007751, "training_acc": 60.0, "val_loss": 17.657867074012756, "val_acc": 52.0}
{"epoch": 35, "training_loss": 65.44159817695618, "training_acc": 61.0, "val_loss": 18.220336735248566, "val_acc": 52.0}
{"epoch": 36, "training_loss": 64.64424395561218, "training_acc": 61.0, "val_loss": 18.620292842388153, "val_acc": 52.0}
{"epoch": 37, "training_loss": 65.4746823310852, "training_acc": 62.0, "val_loss": 17.955462634563446, "val_acc": 52.0}
{"epoch": 38, "training_loss": 66.277517080307, "training_acc": 62.0, "val_loss": 17.94874519109726, "val_acc": 52.0}
{"epoch": 39, "training_loss": 67.57502436637878, "training_acc": 57.0, "val_loss": 18.367639183998108, "val_acc": 52.0}
{"epoch": 40, "training_loss": 62.73633527755737, "training_acc": 68.0, "val_loss": 18.90276074409485, "val_acc": 52.0}
