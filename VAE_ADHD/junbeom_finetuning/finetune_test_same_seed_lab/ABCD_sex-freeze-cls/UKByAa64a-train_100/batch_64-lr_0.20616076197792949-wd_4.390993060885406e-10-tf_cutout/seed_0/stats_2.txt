"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 613.5735816955566, "training_acc": 53.0, "val_loss": 112.84569501876831, "val_acc": 52.0}
{"epoch": 1, "training_loss": 571.0645427703857, "training_acc": 41.0, "val_loss": 183.31283330917358, "val_acc": 48.0}
{"epoch": 2, "training_loss": 667.1768684387207, "training_acc": 47.0, "val_loss": 38.58731687068939, "val_acc": 48.0}
{"epoch": 3, "training_loss": 283.46184730529785, "training_acc": 41.0, "val_loss": 155.2775740623474, "val_acc": 52.0}
{"epoch": 4, "training_loss": 591.3778381347656, "training_acc": 53.0, "val_loss": 118.19896697998047, "val_acc": 52.0}
{"epoch": 5, "training_loss": 350.9414076805115, "training_acc": 53.0, "val_loss": 52.24207639694214, "val_acc": 48.0}
{"epoch": 6, "training_loss": 285.8312873840332, "training_acc": 47.0, "val_loss": 97.7403461933136, "val_acc": 48.0}
{"epoch": 7, "training_loss": 340.06906843185425, "training_acc": 47.0, "val_loss": 25.824475288391113, "val_acc": 52.0}
{"epoch": 8, "training_loss": 141.56099891662598, "training_acc": 53.0, "val_loss": 63.746100664138794, "val_acc": 52.0}
{"epoch": 9, "training_loss": 201.87906408309937, "training_acc": 53.0, "val_loss": 25.627899169921875, "val_acc": 48.0}
{"epoch": 10, "training_loss": 127.79617977142334, "training_acc": 47.0, "val_loss": 32.45663344860077, "val_acc": 48.0}
{"epoch": 11, "training_loss": 101.29056215286255, "training_acc": 56.0, "val_loss": 41.564205288887024, "val_acc": 52.0}
{"epoch": 12, "training_loss": 154.2871356010437, "training_acc": 53.0, "val_loss": 18.086296319961548, "val_acc": 52.0}
{"epoch": 13, "training_loss": 86.98386573791504, "training_acc": 52.0, "val_loss": 32.90731608867645, "val_acc": 48.0}
{"epoch": 14, "training_loss": 108.91358494758606, "training_acc": 54.0, "val_loss": 27.59886085987091, "val_acc": 52.0}
{"epoch": 15, "training_loss": 99.4770359992981, "training_acc": 53.0, "val_loss": 17.358554899692535, "val_acc": 60.0}
{"epoch": 16, "training_loss": 78.88748264312744, "training_acc": 58.0, "val_loss": 21.648801863193512, "val_acc": 52.0}
{"epoch": 17, "training_loss": 79.62351536750793, "training_acc": 55.0, "val_loss": 27.158716320991516, "val_acc": 52.0}
{"epoch": 18, "training_loss": 90.58901906013489, "training_acc": 56.0, "val_loss": 19.86902207136154, "val_acc": 60.0}
{"epoch": 19, "training_loss": 91.41451215744019, "training_acc": 49.0, "val_loss": 18.05841028690338, "val_acc": 52.0}
{"epoch": 20, "training_loss": 66.96911287307739, "training_acc": 57.0, "val_loss": 26.682907342910767, "val_acc": 52.0}
{"epoch": 21, "training_loss": 83.64512825012207, "training_acc": 55.0, "val_loss": 19.991466403007507, "val_acc": 56.0}
{"epoch": 22, "training_loss": 74.60640072822571, "training_acc": 53.0, "val_loss": 20.331744849681854, "val_acc": 52.0}
{"epoch": 23, "training_loss": 72.97512483596802, "training_acc": 54.0, "val_loss": 16.76671952009201, "val_acc": 56.0}
{"epoch": 24, "training_loss": 62.60509943962097, "training_acc": 68.0, "val_loss": 16.942593455314636, "val_acc": 56.0}
{"epoch": 25, "training_loss": 61.89456868171692, "training_acc": 72.0, "val_loss": 18.055659532546997, "val_acc": 52.0}
{"epoch": 26, "training_loss": 64.35956311225891, "training_acc": 62.0, "val_loss": 16.9995978474617, "val_acc": 56.0}
{"epoch": 27, "training_loss": 63.67877554893494, "training_acc": 65.0, "val_loss": 17.349380254745483, "val_acc": 52.0}
{"epoch": 28, "training_loss": 59.81155776977539, "training_acc": 73.0, "val_loss": 18.5063436627388, "val_acc": 52.0}
{"epoch": 29, "training_loss": 63.70367503166199, "training_acc": 59.0, "val_loss": 18.122631311416626, "val_acc": 56.0}
{"epoch": 30, "training_loss": 69.03320646286011, "training_acc": 57.0, "val_loss": 20.03825008869171, "val_acc": 52.0}
{"epoch": 31, "training_loss": 64.04812574386597, "training_acc": 54.0, "val_loss": 17.08489954471588, "val_acc": 64.0}
{"epoch": 32, "training_loss": 58.9706814289093, "training_acc": 72.0, "val_loss": 17.398950457572937, "val_acc": 52.0}
{"epoch": 33, "training_loss": 58.12943196296692, "training_acc": 72.0, "val_loss": 18.819423019886017, "val_acc": 52.0}
{"epoch": 34, "training_loss": 63.85437369346619, "training_acc": 64.0, "val_loss": 17.096781730651855, "val_acc": 60.0}
{"epoch": 35, "training_loss": 57.331912994384766, "training_acc": 74.0, "val_loss": 17.72983819246292, "val_acc": 52.0}
{"epoch": 36, "training_loss": 61.862107038497925, "training_acc": 69.0, "val_loss": 17.164218425750732, "val_acc": 64.0}
{"epoch": 37, "training_loss": 61.06915736198425, "training_acc": 72.0, "val_loss": 17.577677965164185, "val_acc": 56.0}
{"epoch": 38, "training_loss": 58.92794132232666, "training_acc": 67.0, "val_loss": 18.119795620441437, "val_acc": 52.0}
{"epoch": 39, "training_loss": 60.683953046798706, "training_acc": 60.0, "val_loss": 19.221962988376617, "val_acc": 52.0}
{"epoch": 40, "training_loss": 68.75294923782349, "training_acc": 59.0, "val_loss": 17.90185570716858, "val_acc": 56.0}
{"epoch": 41, "training_loss": 55.19035363197327, "training_acc": 75.0, "val_loss": 18.301279842853546, "val_acc": 52.0}
{"epoch": 42, "training_loss": 59.4261839389801, "training_acc": 71.0, "val_loss": 17.448221147060394, "val_acc": 60.0}
