"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 1235124.566871643, "training_acc": 53.0, "val_loss": 3476446.484375, "val_acc": 48.0}
{"epoch": 1, "training_loss": 14034673.0625, "training_acc": 47.0, "val_loss": 272647.36328125, "val_acc": 44.0}
{"epoch": 2, "training_loss": 2374856.921875, "training_acc": 68.0, "val_loss": 3408657.8125, "val_acc": 52.0}
{"epoch": 3, "training_loss": 10627386.5, "training_acc": 53.0, "val_loss": 949495.80078125, "val_acc": 52.0}
{"epoch": 4, "training_loss": 4542782.53125, "training_acc": 48.0, "val_loss": 2591025.78125, "val_acc": 48.0}
{"epoch": 5, "training_loss": 10509025.40625, "training_acc": 47.0, "val_loss": 710559.619140625, "val_acc": 48.0}
{"epoch": 6, "training_loss": 3634415.5625, "training_acc": 51.0, "val_loss": 2532451.953125, "val_acc": 52.0}
{"epoch": 7, "training_loss": 8801581.0625, "training_acc": 53.0, "val_loss": 1975241.9921875, "val_acc": 52.0}
{"epoch": 8, "training_loss": 4886589.07421875, "training_acc": 54.0, "val_loss": 1192732.2265625, "val_acc": 48.0}
{"epoch": 9, "training_loss": 6176377.1875, "training_acc": 47.0, "val_loss": 1725377.5390625, "val_acc": 48.0}
{"epoch": 10, "training_loss": 6054392.078125, "training_acc": 47.0, "val_loss": 871978.61328125, "val_acc": 52.0}
{"epoch": 11, "training_loss": 3128936.6875, "training_acc": 54.0, "val_loss": 1688762.890625, "val_acc": 52.0}
{"epoch": 12, "training_loss": 4692523.1171875, "training_acc": 53.0, "val_loss": 226229.2724609375, "val_acc": 48.0}
{"epoch": 13, "training_loss": 1590907.625, "training_acc": 58.0, "val_loss": 542697.998046875, "val_acc": 52.0}
{"epoch": 14, "training_loss": 2449995.306640625, "training_acc": 57.0, "val_loss": 898136.5234375, "val_acc": 52.0}
{"epoch": 15, "training_loss": 2119779.15625, "training_acc": 55.0, "val_loss": 302902.8076171875, "val_acc": 48.0}
{"epoch": 16, "training_loss": 1112682.74609375, "training_acc": 57.0, "val_loss": 309241.11328125, "val_acc": 48.0}
{"epoch": 17, "training_loss": 1359593.81640625, "training_acc": 54.0, "val_loss": 705422.216796875, "val_acc": 52.0}
{"epoch": 18, "training_loss": 1234152.12890625, "training_acc": 56.0, "val_loss": 317487.548828125, "val_acc": 52.0}
{"epoch": 19, "training_loss": 1604215.4375, "training_acc": 51.0, "val_loss": 443445.166015625, "val_acc": 48.0}
{"epoch": 20, "training_loss": 1366286.203125, "training_acc": 55.0, "val_loss": 330524.12109375, "val_acc": 48.0}
{"epoch": 21, "training_loss": 1443322.890625, "training_acc": 58.0, "val_loss": 308025.1220703125, "val_acc": 52.0}
{"epoch": 22, "training_loss": 1440070.01953125, "training_acc": 55.0, "val_loss": 661800.1953125, "val_acc": 52.0}
{"epoch": 23, "training_loss": 1626152.78515625, "training_acc": 52.0, "val_loss": 562260.7421875, "val_acc": 48.0}
{"epoch": 24, "training_loss": 2246763.890625, "training_acc": 48.0, "val_loss": 291905.0537109375, "val_acc": 52.0}
{"epoch": 25, "training_loss": 1042176.1328125, "training_acc": 58.0, "val_loss": 267119.873046875, "val_acc": 52.0}
{"epoch": 26, "training_loss": 1081231.6015625, "training_acc": 62.0, "val_loss": 405125.5859375, "val_acc": 48.0}
{"epoch": 27, "training_loss": 1650810.6875, "training_acc": 48.0, "val_loss": 530260.83984375, "val_acc": 52.0}
{"epoch": 28, "training_loss": 1085619.810546875, "training_acc": 58.0, "val_loss": 125706.23779296875, "val_acc": 52.0}
{"epoch": 29, "training_loss": 548923.314453125, "training_acc": 68.0, "val_loss": 461237.451171875, "val_acc": 52.0}
{"epoch": 30, "training_loss": 1032937.24609375, "training_acc": 49.0, "val_loss": 206455.56640625, "val_acc": 52.0}
{"epoch": 31, "training_loss": 631542.921875, "training_acc": 59.0, "val_loss": 101026.49536132812, "val_acc": 48.0}
{"epoch": 32, "training_loss": 476232.134765625, "training_acc": 64.0, "val_loss": 182864.85595703125, "val_acc": 52.0}
{"epoch": 33, "training_loss": 221691.12890625, "training_acc": 72.0, "val_loss": 119660.21728515625, "val_acc": 56.0}
{"epoch": 34, "training_loss": 440614.900390625, "training_acc": 63.0, "val_loss": 292498.583984375, "val_acc": 52.0}
{"epoch": 35, "training_loss": 730651.53125, "training_acc": 53.0, "val_loss": 78459.87548828125, "val_acc": 56.0}
{"epoch": 36, "training_loss": 345499.109375, "training_acc": 66.0, "val_loss": 150685.400390625, "val_acc": 52.0}
{"epoch": 37, "training_loss": 428521.03125, "training_acc": 62.0, "val_loss": 226255.95703125, "val_acc": 52.0}
{"epoch": 38, "training_loss": 480990.9736328125, "training_acc": 64.0, "val_loss": 303221.9970703125, "val_acc": 48.0}
{"epoch": 39, "training_loss": 1159838.2578125, "training_acc": 44.0, "val_loss": 88338.98315429688, "val_acc": 52.0}
{"epoch": 40, "training_loss": 366422.353515625, "training_acc": 65.0, "val_loss": 532472.36328125, "val_acc": 52.0}
{"epoch": 41, "training_loss": 1311703.9375, "training_acc": 54.0, "val_loss": 380731.93359375, "val_acc": 48.0}
{"epoch": 42, "training_loss": 1578274.98046875, "training_acc": 47.0, "val_loss": 600835.498046875, "val_acc": 52.0}
{"epoch": 43, "training_loss": 1738191.4921875, "training_acc": 54.0, "val_loss": 62361.02294921875, "val_acc": 52.0}
{"epoch": 44, "training_loss": 630466.75, "training_acc": 62.0, "val_loss": 252346.923828125, "val_acc": 52.0}
{"epoch": 45, "training_loss": 365195.869140625, "training_acc": 66.0, "val_loss": 155964.70947265625, "val_acc": 48.0}
{"epoch": 46, "training_loss": 573558.7841796875, "training_acc": 56.0, "val_loss": 93319.17114257812, "val_acc": 52.0}
{"epoch": 47, "training_loss": 381397.06640625, "training_acc": 70.0, "val_loss": 325912.0849609375, "val_acc": 52.0}
{"epoch": 48, "training_loss": 507463.45703125, "training_acc": 64.0, "val_loss": 276844.921875, "val_acc": 48.0}
{"epoch": 49, "training_loss": 689006.3117675781, "training_acc": 59.0, "val_loss": 444844.091796875, "val_acc": 52.0}
{"epoch": 50, "training_loss": 855575.046875, "training_acc": 59.0, "val_loss": 459351.123046875, "val_acc": 48.0}
{"epoch": 51, "training_loss": 1501090.728515625, "training_acc": 47.0, "val_loss": 862296.77734375, "val_acc": 52.0}
{"epoch": 52, "training_loss": 2804600.75, "training_acc": 53.0, "val_loss": 589501.5625, "val_acc": 52.0}
{"epoch": 53, "training_loss": 1722250.9375, "training_acc": 50.0, "val_loss": 712314.6484375, "val_acc": 48.0}
{"epoch": 54, "training_loss": 2076382.259765625, "training_acc": 50.0, "val_loss": 1226235.9375, "val_acc": 52.0}
{"epoch": 55, "training_loss": 4189633.265625, "training_acc": 54.0, "val_loss": 1441438.8671875, "val_acc": 52.0}
{"epoch": 56, "training_loss": 2931342.171875, "training_acc": 56.0, "val_loss": 1033938.0859375, "val_acc": 48.0}
{"epoch": 57, "training_loss": 5604188.15625, "training_acc": 47.0, "val_loss": 1402792.87109375, "val_acc": 48.0}
{"epoch": 58, "training_loss": 4281061.2109375, "training_acc": 47.0, "val_loss": 1451475.1953125, "val_acc": 52.0}
{"epoch": 59, "training_loss": 5872756.375, "training_acc": 54.0, "val_loss": 2505103.515625, "val_acc": 52.0}
{"epoch": 60, "training_loss": 7128422.015625, "training_acc": 53.0, "val_loss": 543891.162109375, "val_acc": 52.0}
{"epoch": 61, "training_loss": 2346708.375, "training_acc": 56.0, "val_loss": 1644771.875, "val_acc": 48.0}
{"epoch": 62, "training_loss": 6689898.953125, "training_acc": 47.0, "val_loss": 248829.00390625, "val_acc": 56.0}
