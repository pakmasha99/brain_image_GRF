"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 910816.2950134277, "training_acc": 47.0, "val_loss": 137816.9921875, "val_acc": 48.0}
{"epoch": 1, "training_loss": 826465.8046875, "training_acc": 49.0, "val_loss": 519973.779296875, "val_acc": 52.0}
{"epoch": 2, "training_loss": 1995069.7421875, "training_acc": 53.0, "val_loss": 342781.0546875, "val_acc": 52.0}
{"epoch": 3, "training_loss": 975784.619140625, "training_acc": 53.0, "val_loss": 207192.5048828125, "val_acc": 48.0}
{"epoch": 4, "training_loss": 1003868.078125, "training_acc": 47.0, "val_loss": 383360.205078125, "val_acc": 48.0}
{"epoch": 5, "training_loss": 1301661.8515625, "training_acc": 47.0, "val_loss": 131870.8251953125, "val_acc": 48.0}
{"epoch": 6, "training_loss": 478114.05859375, "training_acc": 49.0, "val_loss": 207291.7724609375, "val_acc": 52.0}
{"epoch": 7, "training_loss": 904496.56640625, "training_acc": 53.0, "val_loss": 191391.4794921875, "val_acc": 52.0}
{"epoch": 8, "training_loss": 654034.2236328125, "training_acc": 53.0, "val_loss": 87099.54223632812, "val_acc": 44.0}
{"epoch": 9, "training_loss": 428711.12890625, "training_acc": 51.0, "val_loss": 201348.84033203125, "val_acc": 48.0}
{"epoch": 10, "training_loss": 687225.37890625, "training_acc": 49.0, "val_loss": 34184.67102050781, "val_acc": 52.0}
{"epoch": 11, "training_loss": 277230.166015625, "training_acc": 56.0, "val_loss": 183470.751953125, "val_acc": 52.0}
{"epoch": 12, "training_loss": 770514.541015625, "training_acc": 53.0, "val_loss": 117785.11962890625, "val_acc": 52.0}
{"epoch": 13, "training_loss": 316962.28076171875, "training_acc": 53.0, "val_loss": 124651.5625, "val_acc": 48.0}
{"epoch": 14, "training_loss": 548027.0859375, "training_acc": 47.0, "val_loss": 127255.322265625, "val_acc": 48.0}
{"epoch": 15, "training_loss": 312072.13671875, "training_acc": 47.0, "val_loss": 101655.79223632812, "val_acc": 52.0}
{"epoch": 16, "training_loss": 493219.275390625, "training_acc": 53.0, "val_loss": 140993.49365234375, "val_acc": 52.0}
{"epoch": 17, "training_loss": 475692.21044921875, "training_acc": 53.0, "val_loss": 61972.0947265625, "val_acc": 48.0}
{"epoch": 18, "training_loss": 260183.029296875, "training_acc": 48.0, "val_loss": 73179.64477539062, "val_acc": 48.0}
{"epoch": 19, "training_loss": 245305.81640625, "training_acc": 46.0, "val_loss": 49080.853271484375, "val_acc": 52.0}
{"epoch": 20, "training_loss": 134038.93774414062, "training_acc": 55.0, "val_loss": 59873.046875, "val_acc": 48.0}
{"epoch": 21, "training_loss": 212261.0126953125, "training_acc": 48.0, "val_loss": 8289.653015136719, "val_acc": 72.0}
{"epoch": 22, "training_loss": 70883.2001953125, "training_acc": 57.0, "val_loss": 9789.6484375, "val_acc": 60.0}
{"epoch": 23, "training_loss": 62182.36572265625, "training_acc": 58.0, "val_loss": 11539.339447021484, "val_acc": 40.0}
{"epoch": 24, "training_loss": 87513.294921875, "training_acc": 58.0, "val_loss": 7702.4169921875, "val_acc": 64.0}
{"epoch": 25, "training_loss": 71966.93212890625, "training_acc": 59.0, "val_loss": 5528.072738647461, "val_acc": 68.0}
{"epoch": 26, "training_loss": 54231.4033203125, "training_acc": 63.0, "val_loss": 4865.107345581055, "val_acc": 60.0}
{"epoch": 27, "training_loss": 53396.973388671875, "training_acc": 53.0, "val_loss": 4629.945755004883, "val_acc": 60.0}
{"epoch": 28, "training_loss": 27975.642181396484, "training_acc": 65.0, "val_loss": 6525.7720947265625, "val_acc": 44.0}
{"epoch": 29, "training_loss": 27927.787719726562, "training_acc": 62.0, "val_loss": 21054.24041748047, "val_acc": 48.0}
{"epoch": 30, "training_loss": 76592.3759765625, "training_acc": 47.0, "val_loss": 2952.5814056396484, "val_acc": 64.0}
{"epoch": 31, "training_loss": 23242.13134765625, "training_acc": 68.0, "val_loss": 21470.11260986328, "val_acc": 52.0}
{"epoch": 32, "training_loss": 48752.80334472656, "training_acc": 61.0, "val_loss": 5974.346542358398, "val_acc": 48.0}
{"epoch": 33, "training_loss": 74454.1884765625, "training_acc": 53.0, "val_loss": 6415.886688232422, "val_acc": 48.0}
{"epoch": 34, "training_loss": 30608.83837890625, "training_acc": 51.0, "val_loss": 30557.852172851562, "val_acc": 48.0}
{"epoch": 35, "training_loss": 76076.64935302734, "training_acc": 54.0, "val_loss": 9172.200012207031, "val_acc": 52.0}
{"epoch": 36, "training_loss": 72075.185546875, "training_acc": 51.0, "val_loss": 19908.39385986328, "val_acc": 52.0}
{"epoch": 37, "training_loss": 43866.4443359375, "training_acc": 55.0, "val_loss": 60602.734375, "val_acc": 48.0}
{"epoch": 38, "training_loss": 230456.0986328125, "training_acc": 47.0, "val_loss": 26700.338745117188, "val_acc": 52.0}
{"epoch": 39, "training_loss": 118946.6181640625, "training_acc": 53.0, "val_loss": 11770.78857421875, "val_acc": 44.0}
{"epoch": 40, "training_loss": 36406.64123535156, "training_acc": 58.0, "val_loss": 36379.38232421875, "val_acc": 52.0}
{"epoch": 41, "training_loss": 97743.44934082031, "training_acc": 53.0, "val_loss": 63112.1337890625, "val_acc": 48.0}
{"epoch": 42, "training_loss": 232119.822265625, "training_acc": 47.0, "val_loss": 13321.531677246094, "val_acc": 52.0}
{"epoch": 43, "training_loss": 71683.27587890625, "training_acc": 56.0, "val_loss": 32682.791137695312, "val_acc": 48.0}
{"epoch": 44, "training_loss": 88459.59399414062, "training_acc": 51.0, "val_loss": 52207.611083984375, "val_acc": 52.0}
{"epoch": 45, "training_loss": 169709.1201171875, "training_acc": 53.0, "val_loss": 15901.329040527344, "val_acc": 48.0}
{"epoch": 46, "training_loss": 39620.95559692383, "training_acc": 59.0, "val_loss": 17641.19110107422, "val_acc": 52.0}
{"epoch": 47, "training_loss": 39604.587158203125, "training_acc": 60.0, "val_loss": 6478.3599853515625, "val_acc": 60.0}
{"epoch": 48, "training_loss": 32156.427001953125, "training_acc": 64.0, "val_loss": 37104.42810058594, "val_acc": 48.0}
{"epoch": 49, "training_loss": 98497.11193847656, "training_acc": 49.0, "val_loss": 62346.466064453125, "val_acc": 52.0}
