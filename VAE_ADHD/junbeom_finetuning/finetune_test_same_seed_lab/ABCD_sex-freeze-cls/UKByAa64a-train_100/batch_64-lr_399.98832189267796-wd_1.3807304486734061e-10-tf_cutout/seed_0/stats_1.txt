"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 840120.9559822083, "training_acc": 46.0, "val_loss": 175597.3876953125, "val_acc": 52.0}
{"epoch": 1, "training_loss": 852676.671875, "training_acc": 53.0, "val_loss": 454753.857421875, "val_acc": 48.0}
{"epoch": 2, "training_loss": 1711470.6796875, "training_acc": 47.0, "val_loss": 126057.03125, "val_acc": 48.0}
{"epoch": 3, "training_loss": 596367.62890625, "training_acc": 51.0, "val_loss": 332112.7685546875, "val_acc": 52.0}
{"epoch": 4, "training_loss": 1261790.75, "training_acc": 53.0, "val_loss": 296354.248046875, "val_acc": 52.0}
{"epoch": 5, "training_loss": 879556.744140625, "training_acc": 53.0, "val_loss": 51491.070556640625, "val_acc": 48.0}
{"epoch": 6, "training_loss": 474587.83203125, "training_acc": 47.0, "val_loss": 151790.36865234375, "val_acc": 48.0}
{"epoch": 7, "training_loss": 514508.4267578125, "training_acc": 47.0, "val_loss": 109987.21923828125, "val_acc": 52.0}
{"epoch": 8, "training_loss": 428868.5234375, "training_acc": 53.0, "val_loss": 190712.646484375, "val_acc": 52.0}
{"epoch": 9, "training_loss": 619271.8564453125, "training_acc": 53.0, "val_loss": 24032.5927734375, "val_acc": 52.0}
{"epoch": 10, "training_loss": 247679.388671875, "training_acc": 49.0, "val_loss": 177322.96142578125, "val_acc": 48.0}
{"epoch": 11, "training_loss": 679413.1953125, "training_acc": 47.0, "val_loss": 37614.71252441406, "val_acc": 48.0}
{"epoch": 12, "training_loss": 246165.8779296875, "training_acc": 49.0, "val_loss": 186592.88330078125, "val_acc": 52.0}
{"epoch": 13, "training_loss": 717395.26171875, "training_acc": 53.0, "val_loss": 139882.03125, "val_acc": 52.0}
{"epoch": 14, "training_loss": 375516.08251953125, "training_acc": 52.0, "val_loss": 137174.169921875, "val_acc": 48.0}
{"epoch": 15, "training_loss": 668310.40625, "training_acc": 47.0, "val_loss": 204023.583984375, "val_acc": 48.0}
{"epoch": 16, "training_loss": 688344.58203125, "training_acc": 47.0, "val_loss": 18582.36846923828, "val_acc": 60.0}
{"epoch": 17, "training_loss": 171061.9423828125, "training_acc": 54.0, "val_loss": 116206.43310546875, "val_acc": 52.0}
{"epoch": 18, "training_loss": 373209.11962890625, "training_acc": 53.0, "val_loss": 25929.025268554688, "val_acc": 48.0}
{"epoch": 19, "training_loss": 114342.51806640625, "training_acc": 50.0, "val_loss": 12740.621185302734, "val_acc": 56.0}
{"epoch": 20, "training_loss": 85270.05029296875, "training_acc": 60.0, "val_loss": 19909.906005859375, "val_acc": 60.0}
{"epoch": 21, "training_loss": 75334.17724609375, "training_acc": 56.0, "val_loss": 24325.4150390625, "val_acc": 48.0}
{"epoch": 22, "training_loss": 104868.45703125, "training_acc": 55.0, "val_loss": 55830.145263671875, "val_acc": 52.0}
{"epoch": 23, "training_loss": 106242.26336669922, "training_acc": 62.0, "val_loss": 54532.958984375, "val_acc": 48.0}
{"epoch": 24, "training_loss": 206340.2734375, "training_acc": 48.0, "val_loss": 37387.17041015625, "val_acc": 56.0}
{"epoch": 25, "training_loss": 132489.447265625, "training_acc": 57.0, "val_loss": 28539.22119140625, "val_acc": 56.0}
{"epoch": 26, "training_loss": 160963.5625, "training_acc": 46.0, "val_loss": 55245.9228515625, "val_acc": 48.0}
{"epoch": 27, "training_loss": 172258.45434570312, "training_acc": 50.0, "val_loss": 37454.742431640625, "val_acc": 56.0}
{"epoch": 28, "training_loss": 73433.49377441406, "training_acc": 56.0, "val_loss": 30145.7763671875, "val_acc": 48.0}
{"epoch": 29, "training_loss": 92145.98461914062, "training_acc": 57.0, "val_loss": 75304.08935546875, "val_acc": 52.0}
{"epoch": 30, "training_loss": 261336.9482421875, "training_acc": 53.0, "val_loss": 35819.02770996094, "val_acc": 56.0}
{"epoch": 31, "training_loss": 176340.5712890625, "training_acc": 49.0, "val_loss": 85862.9150390625, "val_acc": 48.0}
{"epoch": 32, "training_loss": 267357.6838378906, "training_acc": 49.0, "val_loss": 79878.66821289062, "val_acc": 52.0}
{"epoch": 33, "training_loss": 339633.904296875, "training_acc": 53.0, "val_loss": 88984.33227539062, "val_acc": 52.0}
{"epoch": 34, "training_loss": 177730.38696289062, "training_acc": 56.0, "val_loss": 106298.4619140625, "val_acc": 48.0}
{"epoch": 35, "training_loss": 471561.529296875, "training_acc": 47.0, "val_loss": 101529.84008789062, "val_acc": 48.0}
{"epoch": 36, "training_loss": 275294.6790161133, "training_acc": 55.0, "val_loss": 74872.39379882812, "val_acc": 52.0}
{"epoch": 37, "training_loss": 252086.1083984375, "training_acc": 53.0, "val_loss": 26913.534545898438, "val_acc": 56.0}
{"epoch": 38, "training_loss": 150157.337890625, "training_acc": 51.0, "val_loss": 89192.84057617188, "val_acc": 48.0}
