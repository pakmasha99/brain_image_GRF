"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 761496.7300109863, "training_acc": 47.0, "val_loss": 110238.18359375, "val_acc": 48.0}
{"epoch": 1, "training_loss": 807777.25390625, "training_acc": 42.0, "val_loss": 408216.796875, "val_acc": 52.0}
{"epoch": 2, "training_loss": 1513721.24609375, "training_acc": 53.0, "val_loss": 263134.7412109375, "val_acc": 52.0}
{"epoch": 3, "training_loss": 770838.34765625, "training_acc": 53.0, "val_loss": 156890.2099609375, "val_acc": 48.0}
{"epoch": 4, "training_loss": 716506.78125, "training_acc": 47.0, "val_loss": 279730.126953125, "val_acc": 48.0}
{"epoch": 5, "training_loss": 1016664.8828125, "training_acc": 47.0, "val_loss": 92527.43530273438, "val_acc": 48.0}
{"epoch": 6, "training_loss": 411483.421875, "training_acc": 43.0, "val_loss": 166479.736328125, "val_acc": 52.0}
{"epoch": 7, "training_loss": 667869.06640625, "training_acc": 53.0, "val_loss": 146904.1015625, "val_acc": 52.0}
{"epoch": 8, "training_loss": 415696.591796875, "training_acc": 53.0, "val_loss": 78107.8369140625, "val_acc": 48.0}
{"epoch": 9, "training_loss": 432317.529296875, "training_acc": 47.0, "val_loss": 143209.50927734375, "val_acc": 48.0}
{"epoch": 10, "training_loss": 491080.1142578125, "training_acc": 47.0, "val_loss": 33670.91369628906, "val_acc": 52.0}
{"epoch": 11, "training_loss": 160397.197265625, "training_acc": 53.0, "val_loss": 79795.71533203125, "val_acc": 52.0}
{"epoch": 12, "training_loss": 184356.96997070312, "training_acc": 53.0, "val_loss": 68704.638671875, "val_acc": 48.0}
{"epoch": 13, "training_loss": 350468.712890625, "training_acc": 47.0, "val_loss": 75974.45068359375, "val_acc": 48.0}
{"epoch": 14, "training_loss": 251796.44018554688, "training_acc": 48.0, "val_loss": 111385.15625, "val_acc": 52.0}
{"epoch": 15, "training_loss": 426078.642578125, "training_acc": 53.0, "val_loss": 154071.484375, "val_acc": 52.0}
{"epoch": 16, "training_loss": 455275.0048828125, "training_acc": 53.0, "val_loss": 11990.534210205078, "val_acc": 44.0}
{"epoch": 17, "training_loss": 177655.2490234375, "training_acc": 44.0, "val_loss": 75663.62915039062, "val_acc": 48.0}
{"epoch": 18, "training_loss": 244615.0380859375, "training_acc": 47.0, "val_loss": 77762.109375, "val_acc": 52.0}
{"epoch": 19, "training_loss": 300379.7890625, "training_acc": 53.0, "val_loss": 117935.55908203125, "val_acc": 52.0}
{"epoch": 20, "training_loss": 315175.490234375, "training_acc": 53.0, "val_loss": 26156.695556640625, "val_acc": 48.0}
{"epoch": 21, "training_loss": 174177.8046875, "training_acc": 48.0, "val_loss": 47829.06188964844, "val_acc": 48.0}
{"epoch": 22, "training_loss": 129317.02697753906, "training_acc": 57.0, "val_loss": 51909.613037109375, "val_acc": 52.0}
{"epoch": 23, "training_loss": 140317.66821289062, "training_acc": 56.0, "val_loss": 9027.373504638672, "val_acc": 56.0}
{"epoch": 24, "training_loss": 57450.293701171875, "training_acc": 52.0, "val_loss": 33171.82312011719, "val_acc": 52.0}
{"epoch": 25, "training_loss": 86515.61328125, "training_acc": 55.0, "val_loss": 13098.84033203125, "val_acc": 56.0}
{"epoch": 26, "training_loss": 62793.370361328125, "training_acc": 54.0, "val_loss": 31766.17431640625, "val_acc": 52.0}
{"epoch": 27, "training_loss": 77364.740234375, "training_acc": 56.0, "val_loss": 17080.74951171875, "val_acc": 52.0}
{"epoch": 28, "training_loss": 84022.07934570312, "training_acc": 52.0, "val_loss": 34999.639892578125, "val_acc": 52.0}
{"epoch": 29, "training_loss": 68586.29760742188, "training_acc": 57.0, "val_loss": 13247.811889648438, "val_acc": 40.0}
{"epoch": 30, "training_loss": 66573.57861328125, "training_acc": 57.0, "val_loss": 18312.28485107422, "val_acc": 48.0}
{"epoch": 31, "training_loss": 36129.34118652344, "training_acc": 68.0, "val_loss": 14017.185974121094, "val_acc": 36.0}
{"epoch": 32, "training_loss": 30808.049743652344, "training_acc": 62.0, "val_loss": 12447.21450805664, "val_acc": 44.0}
{"epoch": 33, "training_loss": 17821.478088378906, "training_acc": 77.0, "val_loss": 8939.656066894531, "val_acc": 48.0}
{"epoch": 34, "training_loss": 28811.264892578125, "training_acc": 65.0, "val_loss": 21904.92401123047, "val_acc": 52.0}
{"epoch": 35, "training_loss": 47253.806884765625, "training_acc": 58.0, "val_loss": 13765.097045898438, "val_acc": 48.0}
{"epoch": 36, "training_loss": 64424.505126953125, "training_acc": 50.0, "val_loss": 9272.09243774414, "val_acc": 56.0}
{"epoch": 37, "training_loss": 65343.8154296875, "training_acc": 56.0, "val_loss": 5003.740692138672, "val_acc": 40.0}
{"epoch": 38, "training_loss": 44558.12548828125, "training_acc": 67.0, "val_loss": 7546.0693359375, "val_acc": 52.0}
{"epoch": 39, "training_loss": 50249.43212890625, "training_acc": 67.0, "val_loss": 5232.312393188477, "val_acc": 36.0}
{"epoch": 40, "training_loss": 58103.13232421875, "training_acc": 63.0, "val_loss": 57486.48681640625, "val_acc": 52.0}
{"epoch": 41, "training_loss": 114519.85888671875, "training_acc": 61.0, "val_loss": 45299.920654296875, "val_acc": 48.0}
{"epoch": 42, "training_loss": 184690.46435546875, "training_acc": 47.0, "val_loss": 34437.68615722656, "val_acc": 52.0}
{"epoch": 43, "training_loss": 100597.35693359375, "training_acc": 53.0, "val_loss": 6236.815643310547, "val_acc": 48.0}
{"epoch": 44, "training_loss": 32304.089111328125, "training_acc": 66.0, "val_loss": 4769.371032714844, "val_acc": 60.0}
{"epoch": 45, "training_loss": 71903.79931640625, "training_acc": 57.0, "val_loss": 65320.892333984375, "val_acc": 52.0}
{"epoch": 46, "training_loss": 155250.822265625, "training_acc": 54.0, "val_loss": 57112.872314453125, "val_acc": 48.0}
{"epoch": 47, "training_loss": 241581.70458984375, "training_acc": 47.0, "val_loss": 10451.033782958984, "val_acc": 48.0}
{"epoch": 48, "training_loss": 140280.94140625, "training_acc": 49.0, "val_loss": 103905.712890625, "val_acc": 52.0}
{"epoch": 49, "training_loss": 337250.7392578125, "training_acc": 53.0, "val_loss": 3307.5172424316406, "val_acc": 48.0}
{"epoch": 50, "training_loss": 103782.7724609375, "training_acc": 64.0, "val_loss": 64473.49853515625, "val_acc": 48.0}
{"epoch": 51, "training_loss": 175610.88232421875, "training_acc": 47.0, "val_loss": 103625.72021484375, "val_acc": 52.0}
{"epoch": 52, "training_loss": 458572.8828125, "training_acc": 53.0, "val_loss": 155861.279296875, "val_acc": 52.0}
{"epoch": 53, "training_loss": 441523.6259765625, "training_acc": 53.0, "val_loss": 10852.659606933594, "val_acc": 56.0}
{"epoch": 54, "training_loss": 120952.24462890625, "training_acc": 54.0, "val_loss": 48594.30236816406, "val_acc": 48.0}
{"epoch": 55, "training_loss": 163617.07250976562, "training_acc": 54.0, "val_loss": 81254.78515625, "val_acc": 52.0}
{"epoch": 56, "training_loss": 223750.1025390625, "training_acc": 53.0, "val_loss": 48756.65588378906, "val_acc": 52.0}
{"epoch": 57, "training_loss": 131472.06396484375, "training_acc": 53.0, "val_loss": 49207.21130371094, "val_acc": 48.0}
{"epoch": 58, "training_loss": 163774.48486328125, "training_acc": 51.0, "val_loss": 77653.10668945312, "val_acc": 52.0}
{"epoch": 59, "training_loss": 248301.798828125, "training_acc": 53.0, "val_loss": 70873.21166992188, "val_acc": 52.0}
{"epoch": 60, "training_loss": 136332.2879638672, "training_acc": 58.0, "val_loss": 44444.47937011719, "val_acc": 48.0}
{"epoch": 61, "training_loss": 164150.48974609375, "training_acc": 47.0, "val_loss": 57110.784912109375, "val_acc": 52.0}
{"epoch": 62, "training_loss": 170876.568359375, "training_acc": 53.0, "val_loss": 51509.47265625, "val_acc": 52.0}
{"epoch": 63, "training_loss": 81683.57446289062, "training_acc": 64.0, "val_loss": 34155.18798828125, "val_acc": 48.0}
{"epoch": 64, "training_loss": 109103.76525878906, "training_acc": 59.0, "val_loss": 59528.84521484375, "val_acc": 52.0}
{"epoch": 65, "training_loss": 160619.689453125, "training_acc": 53.0, "val_loss": 16019.51904296875, "val_acc": 48.0}
{"epoch": 66, "training_loss": 97028.388671875, "training_acc": 60.0, "val_loss": 59855.023193359375, "val_acc": 48.0}
{"epoch": 67, "training_loss": 184506.10986328125, "training_acc": 52.0, "val_loss": 81841.26586914062, "val_acc": 52.0}
{"epoch": 68, "training_loss": 271503.0751953125, "training_acc": 53.0, "val_loss": 90635.96801757812, "val_acc": 52.0}
