"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 424.77853775024414, "training_acc": 46.0, "val_loss": 79.40201163291931, "val_acc": 52.0}
{"epoch": 1, "training_loss": 386.37034606933594, "training_acc": 53.0, "val_loss": 205.60498237609863, "val_acc": 48.0}
{"epoch": 2, "training_loss": 772.8046550750732, "training_acc": 47.0, "val_loss": 56.50829076766968, "val_acc": 48.0}
{"epoch": 3, "training_loss": 268.9103078842163, "training_acc": 51.0, "val_loss": 150.87859630584717, "val_acc": 52.0}
{"epoch": 4, "training_loss": 573.9184703826904, "training_acc": 53.0, "val_loss": 134.29152965545654, "val_acc": 52.0}
{"epoch": 5, "training_loss": 399.3818063735962, "training_acc": 53.0, "val_loss": 27.273467183113098, "val_acc": 48.0}
{"epoch": 6, "training_loss": 235.35647010803223, "training_acc": 47.0, "val_loss": 83.75735282897949, "val_acc": 48.0}
{"epoch": 7, "training_loss": 298.0321979522705, "training_acc": 47.0, "val_loss": 29.207244515419006, "val_acc": 52.0}
{"epoch": 8, "training_loss": 126.77735757827759, "training_acc": 53.0, "val_loss": 72.83473014831543, "val_acc": 52.0}
{"epoch": 9, "training_loss": 236.39061880111694, "training_acc": 53.0, "val_loss": 17.442943155765533, "val_acc": 52.0}
{"epoch": 10, "training_loss": 111.24162340164185, "training_acc": 53.0, "val_loss": 45.31145393848419, "val_acc": 48.0}
{"epoch": 11, "training_loss": 157.41730213165283, "training_acc": 48.0, "val_loss": 30.97408413887024, "val_acc": 52.0}
{"epoch": 12, "training_loss": 125.9741358757019, "training_acc": 53.0, "val_loss": 26.977214217185974, "val_acc": 52.0}
{"epoch": 13, "training_loss": 78.91684103012085, "training_acc": 60.0, "val_loss": 35.20531356334686, "val_acc": 48.0}
{"epoch": 14, "training_loss": 133.68161964416504, "training_acc": 47.0, "val_loss": 18.33668351173401, "val_acc": 52.0}
{"epoch": 15, "training_loss": 83.07873964309692, "training_acc": 58.0, "val_loss": 33.93665552139282, "val_acc": 52.0}
{"epoch": 16, "training_loss": 98.0876898765564, "training_acc": 56.0, "val_loss": 24.469999969005585, "val_acc": 48.0}
{"epoch": 17, "training_loss": 107.50188541412354, "training_acc": 47.0, "val_loss": 16.98654294013977, "val_acc": 56.0}
{"epoch": 18, "training_loss": 63.21316885948181, "training_acc": 63.0, "val_loss": 33.89812409877777, "val_acc": 52.0}
{"epoch": 19, "training_loss": 94.88011288642883, "training_acc": 54.0, "val_loss": 18.064625561237335, "val_acc": 56.0}
{"epoch": 20, "training_loss": 86.46878743171692, "training_acc": 50.0, "val_loss": 17.132040858268738, "val_acc": 64.0}
{"epoch": 21, "training_loss": 83.28573155403137, "training_acc": 51.0, "val_loss": 27.434253692626953, "val_acc": 52.0}
{"epoch": 22, "training_loss": 85.37771487236023, "training_acc": 52.0, "val_loss": 17.288078367710114, "val_acc": 60.0}
{"epoch": 23, "training_loss": 69.90530920028687, "training_acc": 58.0, "val_loss": 17.391671240329742, "val_acc": 56.0}
{"epoch": 24, "training_loss": 64.0690381526947, "training_acc": 62.0, "val_loss": 18.989959359169006, "val_acc": 52.0}
{"epoch": 25, "training_loss": 68.4937674999237, "training_acc": 54.0, "val_loss": 17.523060739040375, "val_acc": 56.0}
{"epoch": 26, "training_loss": 67.10263133049011, "training_acc": 58.0, "val_loss": 19.684524834156036, "val_acc": 52.0}
{"epoch": 27, "training_loss": 70.08226728439331, "training_acc": 56.0, "val_loss": 17.244143784046173, "val_acc": 64.0}
{"epoch": 28, "training_loss": 68.69909644126892, "training_acc": 54.0, "val_loss": 17.180709540843964, "val_acc": 60.0}
{"epoch": 29, "training_loss": 60.603150367736816, "training_acc": 67.0, "val_loss": 23.727943003177643, "val_acc": 52.0}
{"epoch": 30, "training_loss": 79.99363279342651, "training_acc": 52.0, "val_loss": 17.950619757175446, "val_acc": 52.0}
{"epoch": 31, "training_loss": 70.76086926460266, "training_acc": 58.0, "val_loss": 19.811545312404633, "val_acc": 52.0}
{"epoch": 32, "training_loss": 74.90171217918396, "training_acc": 55.0, "val_loss": 17.21017062664032, "val_acc": 56.0}
{"epoch": 33, "training_loss": 63.13439965248108, "training_acc": 69.0, "val_loss": 17.587508261203766, "val_acc": 56.0}
{"epoch": 34, "training_loss": 87.03007483482361, "training_acc": 41.0, "val_loss": 18.25634390115738, "val_acc": 56.0}
{"epoch": 35, "training_loss": 68.89804792404175, "training_acc": 57.0, "val_loss": 16.913577914237976, "val_acc": 64.0}
{"epoch": 36, "training_loss": 68.5824556350708, "training_acc": 55.0, "val_loss": 20.010440051555634, "val_acc": 52.0}
{"epoch": 37, "training_loss": 65.68219351768494, "training_acc": 62.0, "val_loss": 16.917434334754944, "val_acc": 68.0}
{"epoch": 38, "training_loss": 64.69178748130798, "training_acc": 62.0, "val_loss": 18.125541508197784, "val_acc": 52.0}
{"epoch": 39, "training_loss": 64.56079077720642, "training_acc": 58.0, "val_loss": 16.813701391220093, "val_acc": 64.0}
{"epoch": 40, "training_loss": 61.55711030960083, "training_acc": 66.0, "val_loss": 17.34975576400757, "val_acc": 52.0}
{"epoch": 41, "training_loss": 62.20434212684631, "training_acc": 63.0, "val_loss": 16.765712201595306, "val_acc": 64.0}
{"epoch": 42, "training_loss": 64.75286746025085, "training_acc": 65.0, "val_loss": 17.15136617422104, "val_acc": 52.0}
{"epoch": 43, "training_loss": 70.16461277008057, "training_acc": 59.0, "val_loss": 17.09046959877014, "val_acc": 52.0}
{"epoch": 44, "training_loss": 64.09004187583923, "training_acc": 57.0, "val_loss": 16.3066104054451, "val_acc": 64.0}
{"epoch": 45, "training_loss": 58.72692131996155, "training_acc": 71.0, "val_loss": 20.875883102416992, "val_acc": 52.0}
{"epoch": 46, "training_loss": 63.281023025512695, "training_acc": 65.0, "val_loss": 21.462222933769226, "val_acc": 48.0}
{"epoch": 47, "training_loss": 82.05478239059448, "training_acc": 50.0, "val_loss": 23.020678758621216, "val_acc": 52.0}
{"epoch": 48, "training_loss": 78.32974934577942, "training_acc": 54.0, "val_loss": 15.91758280992508, "val_acc": 64.0}
{"epoch": 49, "training_loss": 65.48827719688416, "training_acc": 61.0, "val_loss": 16.904351115226746, "val_acc": 52.0}
{"epoch": 50, "training_loss": 62.77163505554199, "training_acc": 60.0, "val_loss": 16.719090938568115, "val_acc": 52.0}
{"epoch": 51, "training_loss": 60.70938229560852, "training_acc": 60.0, "val_loss": 16.93340837955475, "val_acc": 68.0}
{"epoch": 52, "training_loss": 65.38568186759949, "training_acc": 60.0, "val_loss": 18.454432487487793, "val_acc": 52.0}
{"epoch": 53, "training_loss": 70.40487384796143, "training_acc": 54.0, "val_loss": 16.263003647327423, "val_acc": 64.0}
{"epoch": 54, "training_loss": 62.079976081848145, "training_acc": 60.0, "val_loss": 17.117932438850403, "val_acc": 56.0}
{"epoch": 55, "training_loss": 59.633793592453, "training_acc": 63.0, "val_loss": 16.434243321418762, "val_acc": 56.0}
{"epoch": 56, "training_loss": 57.483423471450806, "training_acc": 75.0, "val_loss": 19.70614194869995, "val_acc": 52.0}
{"epoch": 57, "training_loss": 63.541621923446655, "training_acc": 61.0, "val_loss": 17.108123004436493, "val_acc": 64.0}
{"epoch": 58, "training_loss": 62.618428230285645, "training_acc": 65.0, "val_loss": 20.624476671218872, "val_acc": 52.0}
{"epoch": 59, "training_loss": 67.43043637275696, "training_acc": 58.0, "val_loss": 19.426104426383972, "val_acc": 44.0}
{"epoch": 60, "training_loss": 71.73366737365723, "training_acc": 56.0, "val_loss": 16.864177584648132, "val_acc": 68.0}
{"epoch": 61, "training_loss": 61.960750102996826, "training_acc": 68.0, "val_loss": 17.566397786140442, "val_acc": 52.0}
{"epoch": 62, "training_loss": 60.06379771232605, "training_acc": 68.0, "val_loss": 17.33117401599884, "val_acc": 64.0}
{"epoch": 63, "training_loss": 60.70934700965881, "training_acc": 63.0, "val_loss": 27.10340917110443, "val_acc": 52.0}
{"epoch": 64, "training_loss": 83.91394472122192, "training_acc": 54.0, "val_loss": 16.91451668739319, "val_acc": 68.0}
{"epoch": 65, "training_loss": 69.62321734428406, "training_acc": 58.0, "val_loss": 18.570701777935028, "val_acc": 52.0}
{"epoch": 66, "training_loss": 64.7788770198822, "training_acc": 65.0, "val_loss": 17.598459124565125, "val_acc": 56.0}
{"epoch": 67, "training_loss": 54.7953679561615, "training_acc": 75.0, "val_loss": 18.27920526266098, "val_acc": 48.0}
