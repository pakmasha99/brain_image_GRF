"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 345.89562606811523, "training_acc": 53.0, "val_loss": 23.20515066385269, "val_acc": 52.0}
{"epoch": 1, "training_loss": 388.2012519836426, "training_acc": 49.0, "val_loss": 239.44025039672852, "val_acc": 48.0}
{"epoch": 2, "training_loss": 837.0941257476807, "training_acc": 47.0, "val_loss": 42.670226097106934, "val_acc": 48.0}
{"epoch": 3, "training_loss": 253.00965309143066, "training_acc": 49.0, "val_loss": 184.69141721725464, "val_acc": 52.0}
{"epoch": 4, "training_loss": 732.5824565887451, "training_acc": 53.0, "val_loss": 173.11359643936157, "val_acc": 52.0}
{"epoch": 5, "training_loss": 586.8645648956299, "training_acc": 53.0, "val_loss": 27.42108404636383, "val_acc": 52.0}
{"epoch": 6, "training_loss": 202.87750816345215, "training_acc": 51.0, "val_loss": 148.56526851654053, "val_acc": 48.0}
{"epoch": 7, "training_loss": 590.6797428131104, "training_acc": 47.0, "val_loss": 121.35276794433594, "val_acc": 48.0}
{"epoch": 8, "training_loss": 369.5048990249634, "training_acc": 47.0, "val_loss": 43.22556257247925, "val_acc": 52.0}
{"epoch": 9, "training_loss": 250.67983436584473, "training_acc": 53.0, "val_loss": 107.00041055679321, "val_acc": 52.0}
{"epoch": 10, "training_loss": 386.7091245651245, "training_acc": 53.0, "val_loss": 42.52957105636597, "val_acc": 52.0}
{"epoch": 11, "training_loss": 170.87942790985107, "training_acc": 47.0, "val_loss": 67.91650056838989, "val_acc": 48.0}
{"epoch": 12, "training_loss": 255.92278003692627, "training_acc": 47.0, "val_loss": 31.505215167999268, "val_acc": 48.0}
{"epoch": 13, "training_loss": 108.54074716567993, "training_acc": 50.0, "val_loss": 51.30632519721985, "val_acc": 52.0}
{"epoch": 14, "training_loss": 186.3087387084961, "training_acc": 53.0, "val_loss": 22.630248963832855, "val_acc": 52.0}
{"epoch": 15, "training_loss": 90.83117866516113, "training_acc": 53.0, "val_loss": 45.26877403259277, "val_acc": 48.0}
{"epoch": 16, "training_loss": 151.441752910614, "training_acc": 47.0, "val_loss": 19.48784738779068, "val_acc": 52.0}
{"epoch": 17, "training_loss": 105.62268877029419, "training_acc": 52.0, "val_loss": 35.62400043010712, "val_acc": 52.0}
{"epoch": 18, "training_loss": 107.709796667099, "training_acc": 55.0, "val_loss": 27.757319808006287, "val_acc": 48.0}
{"epoch": 19, "training_loss": 113.32808494567871, "training_acc": 47.0, "val_loss": 17.588622868061066, "val_acc": 56.0}
{"epoch": 20, "training_loss": 79.29732751846313, "training_acc": 56.0, "val_loss": 30.223307013511658, "val_acc": 52.0}
{"epoch": 21, "training_loss": 87.62431716918945, "training_acc": 58.0, "val_loss": 20.184995234012604, "val_acc": 44.0}
{"epoch": 22, "training_loss": 88.22309470176697, "training_acc": 48.0, "val_loss": 16.00264310836792, "val_acc": 64.0}
{"epoch": 23, "training_loss": 90.26249027252197, "training_acc": 50.0, "val_loss": 24.4740292429924, "val_acc": 52.0}
{"epoch": 24, "training_loss": 73.18005514144897, "training_acc": 62.0, "val_loss": 17.57481098175049, "val_acc": 64.0}
{"epoch": 25, "training_loss": 84.81727147102356, "training_acc": 52.0, "val_loss": 19.01187151670456, "val_acc": 52.0}
{"epoch": 26, "training_loss": 76.01774978637695, "training_acc": 57.0, "val_loss": 24.042272567749023, "val_acc": 52.0}
{"epoch": 27, "training_loss": 73.48150420188904, "training_acc": 52.0, "val_loss": 19.7844997048378, "val_acc": 52.0}
{"epoch": 28, "training_loss": 82.10219311714172, "training_acc": 50.0, "val_loss": 18.049341440200806, "val_acc": 52.0}
{"epoch": 29, "training_loss": 59.3761727809906, "training_acc": 65.0, "val_loss": 19.467203319072723, "val_acc": 52.0}
{"epoch": 30, "training_loss": 71.13356375694275, "training_acc": 58.0, "val_loss": 17.842957377433777, "val_acc": 56.0}
{"epoch": 31, "training_loss": 62.2603976726532, "training_acc": 67.0, "val_loss": 17.73305833339691, "val_acc": 52.0}
{"epoch": 32, "training_loss": 61.259438037872314, "training_acc": 65.0, "val_loss": 17.636506259441376, "val_acc": 60.0}
{"epoch": 33, "training_loss": 63.40319108963013, "training_acc": 64.0, "val_loss": 17.226752638816833, "val_acc": 56.0}
{"epoch": 34, "training_loss": 69.89313268661499, "training_acc": 57.0, "val_loss": 19.405707716941833, "val_acc": 52.0}
{"epoch": 35, "training_loss": 67.06003880500793, "training_acc": 56.0, "val_loss": 18.447931110858917, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.61265230178833, "training_acc": 50.0, "val_loss": 16.704082489013672, "val_acc": 60.0}
{"epoch": 37, "training_loss": 60.474021673202515, "training_acc": 69.0, "val_loss": 24.52828586101532, "val_acc": 52.0}
{"epoch": 38, "training_loss": 71.98070859909058, "training_acc": 58.0, "val_loss": 16.623656451702118, "val_acc": 64.0}
{"epoch": 39, "training_loss": 61.89964509010315, "training_acc": 73.0, "val_loss": 16.766665875911713, "val_acc": 64.0}
{"epoch": 40, "training_loss": 61.5949604511261, "training_acc": 74.0, "val_loss": 17.13959276676178, "val_acc": 60.0}
{"epoch": 41, "training_loss": 56.98096585273743, "training_acc": 80.0, "val_loss": 17.388159036636353, "val_acc": 60.0}
