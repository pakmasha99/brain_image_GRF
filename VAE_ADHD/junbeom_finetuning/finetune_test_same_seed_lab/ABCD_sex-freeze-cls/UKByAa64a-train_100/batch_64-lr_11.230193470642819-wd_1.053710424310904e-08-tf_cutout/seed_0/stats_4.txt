"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 24474.148780822754, "training_acc": 59.0, "val_loss": 4400.092315673828, "val_acc": 52.0}
{"epoch": 1, "training_loss": 26026.291625976562, "training_acc": 49.0, "val_loss": 12989.14794921875, "val_acc": 48.0}
{"epoch": 2, "training_loss": 49653.579833984375, "training_acc": 47.0, "val_loss": 4270.193862915039, "val_acc": 48.0}
{"epoch": 3, "training_loss": 16504.419799804688, "training_acc": 51.0, "val_loss": 8036.071014404297, "val_acc": 52.0}
{"epoch": 4, "training_loss": 31235.317626953125, "training_acc": 53.0, "val_loss": 6993.163299560547, "val_acc": 52.0}
{"epoch": 5, "training_loss": 21170.838134765625, "training_acc": 53.0, "val_loss": 2749.569320678711, "val_acc": 48.0}
{"epoch": 6, "training_loss": 15270.949951171875, "training_acc": 47.0, "val_loss": 4994.762420654297, "val_acc": 48.0}
{"epoch": 7, "training_loss": 16422.96078491211, "training_acc": 47.0, "val_loss": 2234.479522705078, "val_acc": 52.0}
{"epoch": 8, "training_loss": 10655.31884765625, "training_acc": 53.0, "val_loss": 4074.6253967285156, "val_acc": 52.0}
{"epoch": 9, "training_loss": 12307.005889892578, "training_acc": 53.0, "val_loss": 1386.466884613037, "val_acc": 48.0}
{"epoch": 10, "training_loss": 6919.136505126953, "training_acc": 47.0, "val_loss": 1306.993293762207, "val_acc": 48.0}
{"epoch": 11, "training_loss": 5629.071060180664, "training_acc": 49.0, "val_loss": 2211.3372802734375, "val_acc": 52.0}
{"epoch": 12, "training_loss": 6077.251358032227, "training_acc": 53.0, "val_loss": 1556.9403648376465, "val_acc": 48.0}
{"epoch": 13, "training_loss": 7288.492584228516, "training_acc": 47.0, "val_loss": 480.21740913391113, "val_acc": 52.0}
{"epoch": 14, "training_loss": 5702.533508300781, "training_acc": 42.0, "val_loss": 3041.628837585449, "val_acc": 52.0}
{"epoch": 15, "training_loss": 9007.388214111328, "training_acc": 52.0, "val_loss": 612.2216701507568, "val_acc": 52.0}
{"epoch": 16, "training_loss": 4110.5345458984375, "training_acc": 51.0, "val_loss": 461.586332321167, "val_acc": 48.0}
{"epoch": 17, "training_loss": 2968.6355895996094, "training_acc": 57.0, "val_loss": 2024.3703842163086, "val_acc": 52.0}
{"epoch": 18, "training_loss": 5742.731842041016, "training_acc": 52.0, "val_loss": 1321.6161727905273, "val_acc": 48.0}
{"epoch": 19, "training_loss": 5080.990371704102, "training_acc": 49.0, "val_loss": 829.9507141113281, "val_acc": 52.0}
{"epoch": 20, "training_loss": 2801.1621856689453, "training_acc": 56.0, "val_loss": 270.51424980163574, "val_acc": 44.0}
{"epoch": 21, "training_loss": 1964.945327758789, "training_acc": 54.0, "val_loss": 264.78734016418457, "val_acc": 48.0}
{"epoch": 22, "training_loss": 2163.3919830322266, "training_acc": 63.0, "val_loss": 691.1260604858398, "val_acc": 52.0}
{"epoch": 23, "training_loss": 2878.610092163086, "training_acc": 52.0, "val_loss": 452.9329299926758, "val_acc": 52.0}
{"epoch": 24, "training_loss": 3752.7537384033203, "training_acc": 50.0, "val_loss": 1285.9572410583496, "val_acc": 52.0}
{"epoch": 25, "training_loss": 3376.769271850586, "training_acc": 53.0, "val_loss": 548.719596862793, "val_acc": 48.0}
{"epoch": 26, "training_loss": 1850.2138671875, "training_acc": 56.0, "val_loss": 1021.4997291564941, "val_acc": 52.0}
{"epoch": 27, "training_loss": 1931.3012733459473, "training_acc": 63.0, "val_loss": 654.3642044067383, "val_acc": 48.0}
{"epoch": 28, "training_loss": 2276.2275581359863, "training_acc": 55.0, "val_loss": 329.8858165740967, "val_acc": 56.0}
{"epoch": 29, "training_loss": 2426.785934448242, "training_acc": 51.0, "val_loss": 140.93945026397705, "val_acc": 60.0}
{"epoch": 30, "training_loss": 1354.223014831543, "training_acc": 65.0, "val_loss": 218.66381168365479, "val_acc": 64.0}
{"epoch": 31, "training_loss": 2137.7919006347656, "training_acc": 53.0, "val_loss": 128.53553295135498, "val_acc": 60.0}
{"epoch": 32, "training_loss": 2076.1015625, "training_acc": 59.0, "val_loss": 387.53771781921387, "val_acc": 56.0}
{"epoch": 33, "training_loss": 3051.3343505859375, "training_acc": 55.0, "val_loss": 1230.1031112670898, "val_acc": 48.0}
{"epoch": 34, "training_loss": 3558.764305114746, "training_acc": 55.0, "val_loss": 1289.1630172729492, "val_acc": 52.0}
{"epoch": 35, "training_loss": 2826.626594543457, "training_acc": 56.0, "val_loss": 802.7070999145508, "val_acc": 48.0}
{"epoch": 36, "training_loss": 2156.615016937256, "training_acc": 50.0, "val_loss": 140.30065536499023, "val_acc": 60.0}
{"epoch": 37, "training_loss": 835.4194869995117, "training_acc": 70.0, "val_loss": 646.4502334594727, "val_acc": 52.0}
{"epoch": 38, "training_loss": 1545.2264518737793, "training_acc": 66.0, "val_loss": 866.6164398193359, "val_acc": 48.0}
{"epoch": 39, "training_loss": 1834.615472793579, "training_acc": 56.0, "val_loss": 753.67112159729, "val_acc": 52.0}
{"epoch": 40, "training_loss": 2263.473747253418, "training_acc": 52.0, "val_loss": 211.37902736663818, "val_acc": 56.0}
{"epoch": 41, "training_loss": 454.6916742324829, "training_acc": 73.0, "val_loss": 392.9179906845093, "val_acc": 56.0}
{"epoch": 42, "training_loss": 851.1367168426514, "training_acc": 70.0, "val_loss": 579.5568943023682, "val_acc": 48.0}
{"epoch": 43, "training_loss": 1891.9880294799805, "training_acc": 52.0, "val_loss": 392.68035888671875, "val_acc": 56.0}
{"epoch": 44, "training_loss": 2426.5680541992188, "training_acc": 51.0, "val_loss": 543.1427955627441, "val_acc": 48.0}
{"epoch": 45, "training_loss": 3891.1681213378906, "training_acc": 49.0, "val_loss": 2353.671073913574, "val_acc": 52.0}
{"epoch": 46, "training_loss": 6873.857597351074, "training_acc": 53.0, "val_loss": 2207.411003112793, "val_acc": 48.0}
{"epoch": 47, "training_loss": 8770.831268310547, "training_acc": 47.0, "val_loss": 1103.824520111084, "val_acc": 48.0}
{"epoch": 48, "training_loss": 5974.095794677734, "training_acc": 45.0, "val_loss": 3273.6576080322266, "val_acc": 52.0}
{"epoch": 49, "training_loss": 10947.323822021484, "training_acc": 53.0, "val_loss": 123.94382953643799, "val_acc": 60.0}
{"epoch": 50, "training_loss": 2477.3245391845703, "training_acc": 62.0, "val_loss": 1928.8671493530273, "val_acc": 48.0}
{"epoch": 51, "training_loss": 4678.317947387695, "training_acc": 56.0, "val_loss": 1290.7358169555664, "val_acc": 52.0}
{"epoch": 52, "training_loss": 3491.079578399658, "training_acc": 55.0, "val_loss": 1643.9104080200195, "val_acc": 48.0}
{"epoch": 53, "training_loss": 5545.909194946289, "training_acc": 47.0, "val_loss": 539.7975921630859, "val_acc": 52.0}
{"epoch": 54, "training_loss": 1955.41552734375, "training_acc": 60.0, "val_loss": 585.3874206542969, "val_acc": 48.0}
{"epoch": 55, "training_loss": 1540.9363346099854, "training_acc": 57.0, "val_loss": 497.4020481109619, "val_acc": 52.0}
{"epoch": 56, "training_loss": 1070.5103740692139, "training_acc": 63.0, "val_loss": 409.486722946167, "val_acc": 52.0}
{"epoch": 57, "training_loss": 1166.4780883789062, "training_acc": 66.0, "val_loss": 431.6380977630615, "val_acc": 60.0}
{"epoch": 58, "training_loss": 2442.912399291992, "training_acc": 51.0, "val_loss": 204.0440559387207, "val_acc": 44.0}
{"epoch": 59, "training_loss": 2551.5740356445312, "training_acc": 60.0, "val_loss": 1183.7089538574219, "val_acc": 52.0}
{"epoch": 60, "training_loss": 2902.55037689209, "training_acc": 55.0, "val_loss": 834.844970703125, "val_acc": 48.0}
{"epoch": 61, "training_loss": 3397.8330841064453, "training_acc": 45.0, "val_loss": 1164.5556449890137, "val_acc": 52.0}
{"epoch": 62, "training_loss": 2318.834300994873, "training_acc": 63.0, "val_loss": 745.5583095550537, "val_acc": 48.0}
{"epoch": 63, "training_loss": 3357.578399658203, "training_acc": 40.0, "val_loss": 371.48473262786865, "val_acc": 60.0}
{"epoch": 64, "training_loss": 2312.2254638671875, "training_acc": 59.0, "val_loss": 1133.3991050720215, "val_acc": 48.0}
{"epoch": 65, "training_loss": 2742.294448852539, "training_acc": 61.0, "val_loss": 1203.922939300537, "val_acc": 52.0}
{"epoch": 66, "training_loss": 2552.7914962768555, "training_acc": 63.0, "val_loss": 1708.2727432250977, "val_acc": 48.0}
{"epoch": 67, "training_loss": 5476.937942504883, "training_acc": 47.0, "val_loss": 1220.0693130493164, "val_acc": 52.0}
{"epoch": 68, "training_loss": 5185.9827880859375, "training_acc": 53.0, "val_loss": 529.4270038604736, "val_acc": 52.0}
