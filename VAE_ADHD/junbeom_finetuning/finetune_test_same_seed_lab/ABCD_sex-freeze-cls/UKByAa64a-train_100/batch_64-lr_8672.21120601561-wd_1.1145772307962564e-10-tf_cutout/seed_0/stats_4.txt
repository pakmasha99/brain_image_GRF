"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 16782773.929016113, "training_acc": 53.0, "val_loss": 2566598.4375, "val_acc": 52.0}
{"epoch": 1, "training_loss": 19052619.75, "training_acc": 51.0, "val_loss": 11044178.125, "val_acc": 48.0}
{"epoch": 2, "training_loss": 41339663.75, "training_acc": 47.0, "val_loss": 4036705.46875, "val_acc": 48.0}
{"epoch": 3, "training_loss": 15209196.15625, "training_acc": 45.0, "val_loss": 5608385.546875, "val_acc": 52.0}
{"epoch": 4, "training_loss": 22019129.0, "training_acc": 53.0, "val_loss": 4516196.875, "val_acc": 52.0}
{"epoch": 5, "training_loss": 14639162.390625, "training_acc": 53.0, "val_loss": 3216841.015625, "val_acc": 48.0}
{"epoch": 6, "training_loss": 14270460.0, "training_acc": 48.0, "val_loss": 4849612.5, "val_acc": 48.0}
{"epoch": 7, "training_loss": 15510592.03125, "training_acc": 48.0, "val_loss": 512886.23046875, "val_acc": 44.0}
{"epoch": 8, "training_loss": 3991618.03125, "training_acc": 62.0, "val_loss": 3199695.5078125, "val_acc": 52.0}
{"epoch": 9, "training_loss": 11481184.125, "training_acc": 53.0, "val_loss": 321856.0302734375, "val_acc": 56.0}
{"epoch": 10, "training_loss": 4861016.1875, "training_acc": 48.0, "val_loss": 2555845.5078125, "val_acc": 48.0}
{"epoch": 11, "training_loss": 7789468.0, "training_acc": 46.0, "val_loss": 1334458.0078125, "val_acc": 52.0}
{"epoch": 12, "training_loss": 6575908.59375, "training_acc": 52.0, "val_loss": 1510875.68359375, "val_acc": 52.0}
{"epoch": 13, "training_loss": 5568899.234375, "training_acc": 45.0, "val_loss": 1258200.87890625, "val_acc": 48.0}
{"epoch": 14, "training_loss": 3717841.70703125, "training_acc": 56.0, "val_loss": 887435.9375, "val_acc": 52.0}
{"epoch": 15, "training_loss": 2832783.90625, "training_acc": 54.0, "val_loss": 873008.10546875, "val_acc": 48.0}
{"epoch": 16, "training_loss": 2950654.0625, "training_acc": 49.0, "val_loss": 592744.7265625, "val_acc": 52.0}
{"epoch": 17, "training_loss": 2250474.875, "training_acc": 58.0, "val_loss": 474776.66015625, "val_acc": 44.0}
{"epoch": 18, "training_loss": 1762178.7421875, "training_acc": 54.0, "val_loss": 655433.349609375, "val_acc": 52.0}
{"epoch": 19, "training_loss": 1839886.03515625, "training_acc": 58.0, "val_loss": 674703.61328125, "val_acc": 44.0}
{"epoch": 20, "training_loss": 1808585.93359375, "training_acc": 56.0, "val_loss": 558055.322265625, "val_acc": 52.0}
{"epoch": 21, "training_loss": 1475234.01953125, "training_acc": 62.0, "val_loss": 522141.6015625, "val_acc": 48.0}
{"epoch": 22, "training_loss": 1258049.59765625, "training_acc": 59.0, "val_loss": 988187.5, "val_acc": 52.0}
{"epoch": 23, "training_loss": 2785886.515625, "training_acc": 54.0, "val_loss": 1099145.5078125, "val_acc": 48.0}
{"epoch": 24, "training_loss": 3711497.90625, "training_acc": 47.0, "val_loss": 682132.373046875, "val_acc": 52.0}
{"epoch": 25, "training_loss": 2300713.42578125, "training_acc": 53.0, "val_loss": 306832.421875, "val_acc": 44.0}
{"epoch": 26, "training_loss": 1098298.515625, "training_acc": 64.0, "val_loss": 586715.673828125, "val_acc": 52.0}
{"epoch": 27, "training_loss": 1470657.5703125, "training_acc": 61.0, "val_loss": 946707.2265625, "val_acc": 48.0}
{"epoch": 28, "training_loss": 2867625.0234375, "training_acc": 47.0, "val_loss": 1223941.6015625, "val_acc": 52.0}
{"epoch": 29, "training_loss": 5458313.5, "training_acc": 53.0, "val_loss": 298989.0869140625, "val_acc": 52.0}
{"epoch": 30, "training_loss": 3457389.9375, "training_acc": 56.0, "val_loss": 2268349.4140625, "val_acc": 48.0}
{"epoch": 31, "training_loss": 6902566.90625, "training_acc": 47.0, "val_loss": 1736719.140625, "val_acc": 52.0}
{"epoch": 32, "training_loss": 8592134.5625, "training_acc": 53.0, "val_loss": 2461139.2578125, "val_acc": 52.0}
{"epoch": 33, "training_loss": 6486768.24609375, "training_acc": 54.0, "val_loss": 2530071.875, "val_acc": 48.0}
{"epoch": 34, "training_loss": 12120196.25, "training_acc": 47.0, "val_loss": 3299351.953125, "val_acc": 48.0}
{"epoch": 35, "training_loss": 10259643.578125, "training_acc": 47.0, "val_loss": 2114012.6953125, "val_acc": 52.0}
{"epoch": 36, "training_loss": 8924634.65625, "training_acc": 53.0, "val_loss": 3719975.390625, "val_acc": 52.0}
{"epoch": 37, "training_loss": 12481685.125, "training_acc": 53.0, "val_loss": 380405.7373046875, "val_acc": 56.0}
{"epoch": 38, "training_loss": 4220549.25, "training_acc": 61.0, "val_loss": 3924509.375, "val_acc": 48.0}
{"epoch": 39, "training_loss": 14798236.9375, "training_acc": 47.0, "val_loss": 1487350.5859375, "val_acc": 48.0}
{"epoch": 40, "training_loss": 5893850.6875, "training_acc": 45.0, "val_loss": 3096345.5078125, "val_acc": 52.0}
{"epoch": 41, "training_loss": 11753103.3125, "training_acc": 53.0, "val_loss": 1656869.921875, "val_acc": 52.0}
{"epoch": 42, "training_loss": 3482518.0546875, "training_acc": 65.0, "val_loss": 2227248.4375, "val_acc": 48.0}
{"epoch": 43, "training_loss": 7553477.953125, "training_acc": 47.0, "val_loss": 247260.4248046875, "val_acc": 52.0}
{"epoch": 44, "training_loss": 1783759.90625, "training_acc": 62.0, "val_loss": 1501403.41796875, "val_acc": 52.0}
{"epoch": 45, "training_loss": 4416758.2734375, "training_acc": 56.0, "val_loss": 1647834.765625, "val_acc": 48.0}
{"epoch": 46, "training_loss": 6652425.40625, "training_acc": 47.0, "val_loss": 963599.31640625, "val_acc": 48.0}
{"epoch": 47, "training_loss": 4811915.5, "training_acc": 43.0, "val_loss": 2483387.890625, "val_acc": 52.0}
{"epoch": 48, "training_loss": 7843883.203125, "training_acc": 53.0, "val_loss": 296752.490234375, "val_acc": 64.0}
{"epoch": 49, "training_loss": 2668883.609375, "training_acc": 58.0, "val_loss": 1651182.8125, "val_acc": 48.0}
{"epoch": 50, "training_loss": 3865238.6953125, "training_acc": 54.0, "val_loss": 1220139.2578125, "val_acc": 52.0}
{"epoch": 51, "training_loss": 4643683.5, "training_acc": 53.0, "val_loss": 320944.62890625, "val_acc": 44.0}
{"epoch": 52, "training_loss": 2299651.703125, "training_acc": 60.0, "val_loss": 1010912.6953125, "val_acc": 48.0}
{"epoch": 53, "training_loss": 2546799.2578125, "training_acc": 58.0, "val_loss": 1322858.59375, "val_acc": 52.0}
{"epoch": 54, "training_loss": 3875833.6875, "training_acc": 55.0, "val_loss": 1100097.0703125, "val_acc": 48.0}
{"epoch": 55, "training_loss": 3786451.046875, "training_acc": 48.0, "val_loss": 341356.6162109375, "val_acc": 68.0}
{"epoch": 56, "training_loss": 3382469.21875, "training_acc": 45.0, "val_loss": 871756.8359375, "val_acc": 52.0}
{"epoch": 57, "training_loss": 2489263.6640625, "training_acc": 59.0, "val_loss": 617535.693359375, "val_acc": 40.0}
{"epoch": 58, "training_loss": 1719044.390625, "training_acc": 61.0, "val_loss": 1034293.359375, "val_acc": 52.0}
{"epoch": 59, "training_loss": 2142904.447265625, "training_acc": 60.0, "val_loss": 1009814.35546875, "val_acc": 48.0}
{"epoch": 60, "training_loss": 2661279.390625, "training_acc": 47.0, "val_loss": 694893.359375, "val_acc": 52.0}
{"epoch": 61, "training_loss": 1764978.81640625, "training_acc": 57.0, "val_loss": 420712.6953125, "val_acc": 44.0}
{"epoch": 62, "training_loss": 692525.2890625, "training_acc": 67.0, "val_loss": 702491.943359375, "val_acc": 52.0}
