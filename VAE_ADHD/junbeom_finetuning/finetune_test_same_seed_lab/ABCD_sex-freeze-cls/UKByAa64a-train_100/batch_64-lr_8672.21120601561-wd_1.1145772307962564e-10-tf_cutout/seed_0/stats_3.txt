"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 2596590.707496643, "training_acc": 53.0, "val_loss": 7308639.0625, "val_acc": 48.0}
{"epoch": 1, "training_loss": 29505540.125, "training_acc": 47.0, "val_loss": 573194.53125, "val_acc": 44.0}
{"epoch": 2, "training_loss": 4992965.5625, "training_acc": 68.0, "val_loss": 7166628.125, "val_acc": 52.0}
{"epoch": 3, "training_loss": 22344345.5625, "training_acc": 53.0, "val_loss": 1996999.4140625, "val_acc": 52.0}
{"epoch": 4, "training_loss": 9550843.6875, "training_acc": 48.0, "val_loss": 5446067.96875, "val_acc": 48.0}
{"epoch": 5, "training_loss": 22088905.625, "training_acc": 47.0, "val_loss": 1492563.96484375, "val_acc": 48.0}
{"epoch": 6, "training_loss": 7639190.28125, "training_acc": 51.0, "val_loss": 5325356.25, "val_acc": 52.0}
{"epoch": 7, "training_loss": 18508768.875, "training_acc": 53.0, "val_loss": 4154017.578125, "val_acc": 52.0}
{"epoch": 8, "training_loss": 10278123.6171875, "training_acc": 54.0, "val_loss": 2505945.5078125, "val_acc": 48.0}
{"epoch": 9, "training_loss": 12978554.6875, "training_acc": 47.0, "val_loss": 3625698.828125, "val_acc": 48.0}
{"epoch": 10, "training_loss": 12721936.0, "training_acc": 47.0, "val_loss": 1834737.3046875, "val_acc": 52.0}
{"epoch": 11, "training_loss": 6583583.0, "training_acc": 54.0, "val_loss": 3551927.34375, "val_acc": 52.0}
{"epoch": 12, "training_loss": 9871082.984375, "training_acc": 53.0, "val_loss": 474987.109375, "val_acc": 48.0}
{"epoch": 13, "training_loss": 3339747.390625, "training_acc": 58.0, "val_loss": 1139329.19921875, "val_acc": 52.0}
{"epoch": 14, "training_loss": 5146869.3203125, "training_acc": 57.0, "val_loss": 1889836.5234375, "val_acc": 52.0}
{"epoch": 15, "training_loss": 4462130.234375, "training_acc": 54.0, "val_loss": 596771.97265625, "val_acc": 48.0}
{"epoch": 16, "training_loss": 2376945.875, "training_acc": 56.0, "val_loss": 619808.837890625, "val_acc": 52.0}
{"epoch": 17, "training_loss": 2804076.99609375, "training_acc": 53.0, "val_loss": 1352517.67578125, "val_acc": 52.0}
{"epoch": 18, "training_loss": 2230899.9765625, "training_acc": 59.0, "val_loss": 705306.103515625, "val_acc": 56.0}
{"epoch": 19, "training_loss": 3395915.1171875, "training_acc": 51.0, "val_loss": 1064780.46875, "val_acc": 52.0}
{"epoch": 20, "training_loss": 2880997.703125, "training_acc": 53.0, "val_loss": 406708.4716796875, "val_acc": 48.0}
{"epoch": 21, "training_loss": 2240993.0625, "training_acc": 53.0, "val_loss": 322222.607421875, "val_acc": 48.0}
{"epoch": 22, "training_loss": 1781366.6640625, "training_acc": 64.0, "val_loss": 396484.5458984375, "val_acc": 44.0}
{"epoch": 23, "training_loss": 2366853.21875, "training_acc": 52.0, "val_loss": 373293.798828125, "val_acc": 56.0}
{"epoch": 24, "training_loss": 2785119.375, "training_acc": 51.0, "val_loss": 922847.265625, "val_acc": 52.0}
{"epoch": 25, "training_loss": 2411741.1484375, "training_acc": 52.0, "val_loss": 384240.5517578125, "val_acc": 52.0}
{"epoch": 26, "training_loss": 2019772.625, "training_acc": 56.0, "val_loss": 1193419.43359375, "val_acc": 52.0}
{"epoch": 27, "training_loss": 2137011.95703125, "training_acc": 57.0, "val_loss": 364574.5849609375, "val_acc": 52.0}
{"epoch": 28, "training_loss": 1807688.6328125, "training_acc": 52.0, "val_loss": 824396.58203125, "val_acc": 52.0}
{"epoch": 29, "training_loss": 2509728.4375, "training_acc": 43.0, "val_loss": 450901.220703125, "val_acc": 36.0}
{"epoch": 30, "training_loss": 862784.60546875, "training_acc": 71.0, "val_loss": 772899.853515625, "val_acc": 48.0}
{"epoch": 31, "training_loss": 1106384.2109375, "training_acc": 66.0, "val_loss": 313025.927734375, "val_acc": 40.0}
{"epoch": 32, "training_loss": 1424041.73828125, "training_acc": 55.0, "val_loss": 397353.0029296875, "val_acc": 52.0}
{"epoch": 33, "training_loss": 1281539.880859375, "training_acc": 53.0, "val_loss": 202531.201171875, "val_acc": 48.0}
{"epoch": 34, "training_loss": 1346349.1796875, "training_acc": 57.0, "val_loss": 651313.916015625, "val_acc": 52.0}
{"epoch": 35, "training_loss": 1279976.20703125, "training_acc": 64.0, "val_loss": 200411.36474609375, "val_acc": 64.0}
{"epoch": 36, "training_loss": 641025.587890625, "training_acc": 62.0, "val_loss": 435275.09765625, "val_acc": 52.0}
{"epoch": 37, "training_loss": 1192938.4921875, "training_acc": 59.0, "val_loss": 602443.9453125, "val_acc": 52.0}
{"epoch": 38, "training_loss": 1105525.37890625, "training_acc": 64.0, "val_loss": 791329.00390625, "val_acc": 48.0}
{"epoch": 39, "training_loss": 2883407.173828125, "training_acc": 48.0, "val_loss": 996082.421875, "val_acc": 52.0}
{"epoch": 40, "training_loss": 2077295.6328125, "training_acc": 58.0, "val_loss": 971488.96484375, "val_acc": 48.0}
{"epoch": 41, "training_loss": 3263517.3515625, "training_acc": 48.0, "val_loss": 1682315.4296875, "val_acc": 52.0}
{"epoch": 42, "training_loss": 5959194.0625, "training_acc": 53.0, "val_loss": 989979.00390625, "val_acc": 52.0}
{"epoch": 43, "training_loss": 4051737.53125, "training_acc": 51.0, "val_loss": 2119303.90625, "val_acc": 48.0}
{"epoch": 44, "training_loss": 7016681.515625, "training_acc": 47.0, "val_loss": 1884520.8984375, "val_acc": 52.0}
{"epoch": 45, "training_loss": 6361602.4375, "training_acc": 53.0, "val_loss": 2540221.6796875, "val_acc": 52.0}
{"epoch": 46, "training_loss": 5151577.99609375, "training_acc": 56.0, "val_loss": 1960714.2578125, "val_acc": 48.0}
{"epoch": 47, "training_loss": 9967902.4375, "training_acc": 47.0, "val_loss": 1675589.0625, "val_acc": 48.0}
{"epoch": 48, "training_loss": 6042035.0234375, "training_acc": 48.0, "val_loss": 2466367.96875, "val_acc": 52.0}
{"epoch": 49, "training_loss": 5874977.75, "training_acc": 55.0, "val_loss": 906851.953125, "val_acc": 52.0}
{"epoch": 50, "training_loss": 2723541.09375, "training_acc": 63.0, "val_loss": 1471741.015625, "val_acc": 48.0}
{"epoch": 51, "training_loss": 4896453.57421875, "training_acc": 51.0, "val_loss": 2309936.5234375, "val_acc": 52.0}
{"epoch": 52, "training_loss": 6601522.46875, "training_acc": 54.0, "val_loss": 1969666.6015625, "val_acc": 52.0}
{"epoch": 53, "training_loss": 3463857.0703125, "training_acc": 51.0, "val_loss": 570091.11328125, "val_acc": 52.0}
{"epoch": 54, "training_loss": 1890814.19921875, "training_acc": 60.0, "val_loss": 1425988.96484375, "val_acc": 52.0}
