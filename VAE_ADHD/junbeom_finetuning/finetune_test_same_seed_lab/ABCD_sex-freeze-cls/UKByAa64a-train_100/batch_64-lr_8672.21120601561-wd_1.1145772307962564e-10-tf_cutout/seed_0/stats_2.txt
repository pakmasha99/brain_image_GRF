"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 23868860.282188416, "training_acc": 53.0, "val_loss": 4693898.828125, "val_acc": 52.0}
{"epoch": 1, "training_loss": 23977057.75, "training_acc": 41.0, "val_loss": 7765029.6875, "val_acc": 48.0}
{"epoch": 2, "training_loss": 28321048.1875, "training_acc": 47.0, "val_loss": 1619677.05078125, "val_acc": 48.0}
{"epoch": 3, "training_loss": 12133795.6875, "training_acc": 41.0, "val_loss": 6808786.71875, "val_acc": 52.0}
{"epoch": 4, "training_loss": 26100106.3125, "training_acc": 53.0, "val_loss": 5580725.0, "val_acc": 52.0}
{"epoch": 5, "training_loss": 17300463.3125, "training_acc": 53.0, "val_loss": 1237800.9765625, "val_acc": 48.0}
{"epoch": 6, "training_loss": 8031298.65625, "training_acc": 47.0, "val_loss": 2948475.390625, "val_acc": 48.0}
{"epoch": 7, "training_loss": 9557998.6875, "training_acc": 47.0, "val_loss": 2139952.9296875, "val_acc": 52.0}
{"epoch": 8, "training_loss": 9661893.125, "training_acc": 53.0, "val_loss": 3198295.5078125, "val_acc": 52.0}
{"epoch": 9, "training_loss": 10043061.328125, "training_acc": 53.0, "val_loss": 1046862.59765625, "val_acc": 48.0}
{"epoch": 10, "training_loss": 5326638.03125, "training_acc": 47.0, "val_loss": 1217491.50390625, "val_acc": 48.0}
{"epoch": 11, "training_loss": 3579581.5625, "training_acc": 54.0, "val_loss": 1309370.99609375, "val_acc": 52.0}
{"epoch": 12, "training_loss": 4002339.7265625, "training_acc": 51.0, "val_loss": 1327604.98046875, "val_acc": 48.0}
{"epoch": 13, "training_loss": 5277548.15625, "training_acc": 47.0, "val_loss": 182627.9541015625, "val_acc": 52.0}
{"epoch": 14, "training_loss": 2600249.96875, "training_acc": 59.0, "val_loss": 1154533.69140625, "val_acc": 52.0}
{"epoch": 15, "training_loss": 3014748.5390625, "training_acc": 53.0, "val_loss": 558987.060546875, "val_acc": 44.0}
{"epoch": 16, "training_loss": 2116680.359375, "training_acc": 49.0, "val_loss": 274517.1875, "val_acc": 60.0}
{"epoch": 17, "training_loss": 1621095.4921875, "training_acc": 48.0, "val_loss": 336291.6748046875, "val_acc": 60.0}
{"epoch": 18, "training_loss": 869718.54296875, "training_acc": 60.0, "val_loss": 478248.6328125, "val_acc": 48.0}
{"epoch": 19, "training_loss": 1692268.806640625, "training_acc": 55.0, "val_loss": 1138862.890625, "val_acc": 52.0}
{"epoch": 20, "training_loss": 3223159.453125, "training_acc": 53.0, "val_loss": 288776.3671875, "val_acc": 44.0}
{"epoch": 21, "training_loss": 1239282.853515625, "training_acc": 54.0, "val_loss": 482373.486328125, "val_acc": 52.0}
{"epoch": 22, "training_loss": 1012352.68359375, "training_acc": 58.0, "val_loss": 426487.01171875, "val_acc": 52.0}
{"epoch": 23, "training_loss": 1349406.33203125, "training_acc": 50.0, "val_loss": 617229.345703125, "val_acc": 52.0}
{"epoch": 24, "training_loss": 1466468.3828125, "training_acc": 61.0, "val_loss": 1025300.0, "val_acc": 48.0}
{"epoch": 25, "training_loss": 3687526.015625, "training_acc": 47.0, "val_loss": 1115770.99609375, "val_acc": 52.0}
{"epoch": 26, "training_loss": 4150717.71875, "training_acc": 53.0, "val_loss": 545501.611328125, "val_acc": 52.0}
{"epoch": 27, "training_loss": 3233503.546875, "training_acc": 50.0, "val_loss": 1810959.1796875, "val_acc": 48.0}
{"epoch": 28, "training_loss": 5640012.625, "training_acc": 47.0, "val_loss": 2010360.15625, "val_acc": 52.0}
{"epoch": 29, "training_loss": 8854912.9375, "training_acc": 53.0, "val_loss": 2792072.265625, "val_acc": 52.0}
{"epoch": 30, "training_loss": 7871419.75, "training_acc": 53.0, "val_loss": 1638259.1796875, "val_acc": 48.0}
{"epoch": 31, "training_loss": 8539024.65625, "training_acc": 47.0, "val_loss": 2068203.90625, "val_acc": 48.0}
{"epoch": 32, "training_loss": 5797089.087890625, "training_acc": 56.0, "val_loss": 1858956.640625, "val_acc": 52.0}
