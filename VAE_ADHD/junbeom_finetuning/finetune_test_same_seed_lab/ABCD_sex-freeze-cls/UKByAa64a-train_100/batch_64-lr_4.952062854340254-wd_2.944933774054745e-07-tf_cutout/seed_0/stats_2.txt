"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 3204.8747940063477, "training_acc": 47.0, "val_loss": 4173.683547973633, "val_acc": 52.0}
{"epoch": 1, "training_loss": 16569.186401367188, "training_acc": 53.0, "val_loss": 2792.0793533325195, "val_acc": 52.0}
{"epoch": 2, "training_loss": 7668.035675048828, "training_acc": 52.0, "val_loss": 1813.5427474975586, "val_acc": 48.0}
{"epoch": 3, "training_loss": 7798.2698974609375, "training_acc": 47.0, "val_loss": 330.5732250213623, "val_acc": 60.0}
{"epoch": 4, "training_loss": 2055.9427032470703, "training_acc": 53.0, "val_loss": 699.03564453125, "val_acc": 52.0}
{"epoch": 5, "training_loss": 2713.894027709961, "training_acc": 50.0, "val_loss": 814.78271484375, "val_acc": 48.0}
{"epoch": 6, "training_loss": 2087.7101287841797, "training_acc": 58.0, "val_loss": 702.259635925293, "val_acc": 52.0}
{"epoch": 7, "training_loss": 2412.6645431518555, "training_acc": 53.0, "val_loss": 1123.6526489257812, "val_acc": 48.0}
{"epoch": 8, "training_loss": 4523.704391479492, "training_acc": 45.0, "val_loss": 594.0483093261719, "val_acc": 48.0}
{"epoch": 9, "training_loss": 2639.7566146850586, "training_acc": 51.0, "val_loss": 1358.5254669189453, "val_acc": 52.0}
{"epoch": 10, "training_loss": 4993.691635131836, "training_acc": 53.0, "val_loss": 291.26293659210205, "val_acc": 52.0}
{"epoch": 11, "training_loss": 2229.9641876220703, "training_acc": 45.0, "val_loss": 316.39461517333984, "val_acc": 52.0}
{"epoch": 12, "training_loss": 2189.7077407836914, "training_acc": 51.0, "val_loss": 848.7525939941406, "val_acc": 52.0}
{"epoch": 13, "training_loss": 2095.712282180786, "training_acc": 55.0, "val_loss": 648.7967491149902, "val_acc": 48.0}
{"epoch": 14, "training_loss": 2495.441680908203, "training_acc": 49.0, "val_loss": 417.70153045654297, "val_acc": 52.0}
{"epoch": 15, "training_loss": 1755.2724380493164, "training_acc": 54.0, "val_loss": 309.5738410949707, "val_acc": 60.0}
{"epoch": 16, "training_loss": 1096.3449096679688, "training_acc": 61.0, "val_loss": 556.1039447784424, "val_acc": 48.0}
{"epoch": 17, "training_loss": 2162.0199241638184, "training_acc": 49.0, "val_loss": 533.3804607391357, "val_acc": 52.0}
{"epoch": 18, "training_loss": 1382.2641716003418, "training_acc": 52.0, "val_loss": 136.3757610321045, "val_acc": 56.0}
{"epoch": 19, "training_loss": 939.9648742675781, "training_acc": 57.0, "val_loss": 136.94583177566528, "val_acc": 64.0}
{"epoch": 20, "training_loss": 483.2386245727539, "training_acc": 63.0, "val_loss": 100.12187957763672, "val_acc": 40.0}
{"epoch": 21, "training_loss": 435.1840190887451, "training_acc": 68.0, "val_loss": 95.97535729408264, "val_acc": 48.0}
{"epoch": 22, "training_loss": 523.0169048309326, "training_acc": 59.0, "val_loss": 148.77575635910034, "val_acc": 48.0}
{"epoch": 23, "training_loss": 521.4778299331665, "training_acc": 59.0, "val_loss": 93.3184564113617, "val_acc": 52.0}
{"epoch": 24, "training_loss": 334.14422702789307, "training_acc": 59.0, "val_loss": 177.64534950256348, "val_acc": 60.0}
{"epoch": 25, "training_loss": 502.3616943359375, "training_acc": 56.0, "val_loss": 349.20687675476074, "val_acc": 52.0}
{"epoch": 26, "training_loss": 577.4170551300049, "training_acc": 58.0, "val_loss": 97.20489382743835, "val_acc": 60.0}
{"epoch": 27, "training_loss": 167.19406247138977, "training_acc": 70.0, "val_loss": 117.62610673904419, "val_acc": 48.0}
{"epoch": 28, "training_loss": 1029.3850631713867, "training_acc": 54.0, "val_loss": 215.32108783721924, "val_acc": 56.0}
{"epoch": 29, "training_loss": 1668.8469848632812, "training_acc": 56.0, "val_loss": 872.1282005310059, "val_acc": 48.0}
{"epoch": 30, "training_loss": 2730.914840698242, "training_acc": 41.0, "val_loss": 276.0091304779053, "val_acc": 52.0}
{"epoch": 31, "training_loss": 1292.8889694213867, "training_acc": 55.0, "val_loss": 538.0465030670166, "val_acc": 44.0}
{"epoch": 32, "training_loss": 1521.9429893493652, "training_acc": 52.0, "val_loss": 401.45602226257324, "val_acc": 52.0}
{"epoch": 33, "training_loss": 1845.7303314208984, "training_acc": 46.0, "val_loss": 424.7478485107422, "val_acc": 44.0}
{"epoch": 34, "training_loss": 1970.3340148925781, "training_acc": 47.0, "val_loss": 795.5261707305908, "val_acc": 52.0}
{"epoch": 35, "training_loss": 1776.384822845459, "training_acc": 60.0, "val_loss": 1170.4899787902832, "val_acc": 48.0}
{"epoch": 36, "training_loss": 5008.197174072266, "training_acc": 47.0, "val_loss": 556.4795970916748, "val_acc": 48.0}
{"epoch": 37, "training_loss": 2361.813507080078, "training_acc": 54.0, "val_loss": 1798.8248825073242, "val_acc": 52.0}
{"epoch": 38, "training_loss": 6417.728240966797, "training_acc": 53.0, "val_loss": 741.0179138183594, "val_acc": 52.0}
{"epoch": 39, "training_loss": 2887.8953399658203, "training_acc": 55.0, "val_loss": 1766.6887283325195, "val_acc": 48.0}
{"epoch": 40, "training_loss": 6741.3369140625, "training_acc": 47.0, "val_loss": 227.69312858581543, "val_acc": 44.0}
{"epoch": 41, "training_loss": 1660.7136154174805, "training_acc": 64.0, "val_loss": 2553.5219192504883, "val_acc": 52.0}
{"epoch": 42, "training_loss": 9465.188079833984, "training_acc": 53.0, "val_loss": 2059.1115951538086, "val_acc": 52.0}
