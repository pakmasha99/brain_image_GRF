"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 62579.12199783325, "training_acc": 46.0, "val_loss": 13071.223449707031, "val_acc": 52.0}
{"epoch": 1, "training_loss": 63472.90869140625, "training_acc": 53.0, "val_loss": 33851.66015625, "val_acc": 48.0}
{"epoch": 2, "training_loss": 127400.708984375, "training_acc": 47.0, "val_loss": 9383.702850341797, "val_acc": 48.0}
{"epoch": 3, "training_loss": 44393.09521484375, "training_acc": 51.0, "val_loss": 24722.116088867188, "val_acc": 52.0}
{"epoch": 4, "training_loss": 93926.87841796875, "training_acc": 53.0, "val_loss": 22060.281372070312, "val_acc": 52.0}
{"epoch": 5, "training_loss": 65473.61328125, "training_acc": 53.0, "val_loss": 3833.0535888671875, "val_acc": 48.0}
{"epoch": 6, "training_loss": 35328.061767578125, "training_acc": 47.0, "val_loss": 11299.26528930664, "val_acc": 48.0}
{"epoch": 7, "training_loss": 38299.52362060547, "training_acc": 47.0, "val_loss": 8187.250518798828, "val_acc": 52.0}
{"epoch": 8, "training_loss": 31924.677612304688, "training_acc": 53.0, "val_loss": 14196.397399902344, "val_acc": 52.0}
{"epoch": 9, "training_loss": 46098.21838378906, "training_acc": 53.0, "val_loss": 1788.8532638549805, "val_acc": 52.0}
{"epoch": 10, "training_loss": 18442.08154296875, "training_acc": 49.0, "val_loss": 13206.137084960938, "val_acc": 48.0}
{"epoch": 11, "training_loss": 50603.1015625, "training_acc": 47.0, "val_loss": 2811.6622924804688, "val_acc": 48.0}
{"epoch": 12, "training_loss": 18336.12060546875, "training_acc": 49.0, "val_loss": 13875.347900390625, "val_acc": 52.0}
{"epoch": 13, "training_loss": 53343.73095703125, "training_acc": 53.0, "val_loss": 10395.32699584961, "val_acc": 52.0}
{"epoch": 14, "training_loss": 27888.373016357422, "training_acc": 52.0, "val_loss": 10241.543579101562, "val_acc": 48.0}
{"epoch": 15, "training_loss": 49882.6416015625, "training_acc": 47.0, "val_loss": 15235.469055175781, "val_acc": 48.0}
{"epoch": 16, "training_loss": 51439.92297363281, "training_acc": 47.0, "val_loss": 1335.5712890625, "val_acc": 60.0}
{"epoch": 17, "training_loss": 12960.826782226562, "training_acc": 56.0, "val_loss": 9131.468200683594, "val_acc": 52.0}
{"epoch": 18, "training_loss": 29771.368743896484, "training_acc": 53.0, "val_loss": 1055.5359840393066, "val_acc": 44.0}
{"epoch": 19, "training_loss": 7550.566101074219, "training_acc": 57.0, "val_loss": 1638.7035369873047, "val_acc": 48.0}
{"epoch": 20, "training_loss": 11736.318969726562, "training_acc": 47.0, "val_loss": 5042.013168334961, "val_acc": 52.0}
{"epoch": 21, "training_loss": 10922.562973022461, "training_acc": 59.0, "val_loss": 5262.983322143555, "val_acc": 48.0}
{"epoch": 22, "training_loss": 23299.924560546875, "training_acc": 47.0, "val_loss": 1302.1525382995605, "val_acc": 40.0}
{"epoch": 23, "training_loss": 13908.372680664062, "training_acc": 49.0, "val_loss": 10755.016326904297, "val_acc": 52.0}
{"epoch": 24, "training_loss": 35063.63513183594, "training_acc": 53.0, "val_loss": 3938.8011932373047, "val_acc": 52.0}
{"epoch": 25, "training_loss": 16525.0771484375, "training_acc": 51.0, "val_loss": 8734.121704101562, "val_acc": 48.0}
{"epoch": 26, "training_loss": 31800.964477539062, "training_acc": 47.0, "val_loss": 1198.659896850586, "val_acc": 56.0}
{"epoch": 27, "training_loss": 11854.008544921875, "training_acc": 51.0, "val_loss": 9619.257354736328, "val_acc": 52.0}
{"epoch": 28, "training_loss": 29120.492431640625, "training_acc": 53.0, "val_loss": 996.1034774780273, "val_acc": 48.0}
{"epoch": 29, "training_loss": 13145.11572265625, "training_acc": 54.0, "val_loss": 4208.995819091797, "val_acc": 48.0}
{"epoch": 30, "training_loss": 13723.128875732422, "training_acc": 50.0, "val_loss": 4436.714935302734, "val_acc": 52.0}
{"epoch": 31, "training_loss": 10492.764205932617, "training_acc": 54.0, "val_loss": 1702.5821685791016, "val_acc": 48.0}
{"epoch": 32, "training_loss": 6529.410140991211, "training_acc": 55.0, "val_loss": 3110.761070251465, "val_acc": 56.0}
{"epoch": 33, "training_loss": 9670.259765625, "training_acc": 55.0, "val_loss": 1349.079704284668, "val_acc": 48.0}
{"epoch": 34, "training_loss": 7272.645324707031, "training_acc": 52.0, "val_loss": 1224.578857421875, "val_acc": 64.0}
{"epoch": 35, "training_loss": 4314.916900634766, "training_acc": 55.0, "val_loss": 1407.3348999023438, "val_acc": 60.0}
{"epoch": 36, "training_loss": 2864.18399810791, "training_acc": 64.0, "val_loss": 1107.8669548034668, "val_acc": 44.0}
{"epoch": 37, "training_loss": 5871.309768676758, "training_acc": 59.0, "val_loss": 2019.7172164916992, "val_acc": 56.0}
{"epoch": 38, "training_loss": 5193.402328491211, "training_acc": 55.0, "val_loss": 914.5168304443359, "val_acc": 60.0}
{"epoch": 39, "training_loss": 2812.1046295166016, "training_acc": 64.0, "val_loss": 1368.1097984313965, "val_acc": 48.0}
{"epoch": 40, "training_loss": 3971.6034965515137, "training_acc": 62.0, "val_loss": 1257.499885559082, "val_acc": 60.0}
{"epoch": 41, "training_loss": 3683.1415252685547, "training_acc": 56.0, "val_loss": 774.8260498046875, "val_acc": 64.0}
{"epoch": 42, "training_loss": 3012.2966918945312, "training_acc": 55.0, "val_loss": 1539.4936561584473, "val_acc": 52.0}
{"epoch": 43, "training_loss": 3141.860263824463, "training_acc": 61.0, "val_loss": 2684.2451095581055, "val_acc": 48.0}
{"epoch": 44, "training_loss": 7068.615421295166, "training_acc": 59.0, "val_loss": 1960.2046966552734, "val_acc": 52.0}
{"epoch": 45, "training_loss": 6297.997131347656, "training_acc": 46.0, "val_loss": 1058.2331657409668, "val_acc": 52.0}
{"epoch": 46, "training_loss": 1744.6141452789307, "training_acc": 73.0, "val_loss": 1256.6128730773926, "val_acc": 48.0}
{"epoch": 47, "training_loss": 5082.34260559082, "training_acc": 52.0, "val_loss": 250.7275104522705, "val_acc": 72.0}
{"epoch": 48, "training_loss": 4433.245819091797, "training_acc": 63.0, "val_loss": 772.2933292388916, "val_acc": 60.0}
{"epoch": 49, "training_loss": 2770.4961471557617, "training_acc": 65.0, "val_loss": 2238.95263671875, "val_acc": 48.0}
{"epoch": 50, "training_loss": 4584.17276763916, "training_acc": 60.0, "val_loss": 2751.850128173828, "val_acc": 52.0}
{"epoch": 51, "training_loss": 8080.750946044922, "training_acc": 56.0, "val_loss": 4278.965759277344, "val_acc": 48.0}
{"epoch": 52, "training_loss": 14678.20654296875, "training_acc": 48.0, "val_loss": 1669.8631286621094, "val_acc": 52.0}
{"epoch": 53, "training_loss": 6348.317886352539, "training_acc": 53.0, "val_loss": 2529.4416427612305, "val_acc": 48.0}
{"epoch": 54, "training_loss": 7210.407875061035, "training_acc": 51.0, "val_loss": 4114.275360107422, "val_acc": 52.0}
{"epoch": 55, "training_loss": 14086.165161132812, "training_acc": 53.0, "val_loss": 260.1071834564209, "val_acc": 64.0}
{"epoch": 56, "training_loss": 6760.6656494140625, "training_acc": 57.0, "val_loss": 255.06744384765625, "val_acc": 68.0}
{"epoch": 57, "training_loss": 5717.9288330078125, "training_acc": 68.0, "val_loss": 2967.5153732299805, "val_acc": 52.0}
{"epoch": 58, "training_loss": 11739.98486328125, "training_acc": 45.0, "val_loss": 2449.488067626953, "val_acc": 48.0}
{"epoch": 59, "training_loss": 10476.662170410156, "training_acc": 45.0, "val_loss": 3865.859603881836, "val_acc": 52.0}
{"epoch": 60, "training_loss": 7966.5519943237305, "training_acc": 62.0, "val_loss": 6358.093643188477, "val_acc": 48.0}
{"epoch": 61, "training_loss": 24581.287719726562, "training_acc": 47.0, "val_loss": 1244.9111938476562, "val_acc": 44.0}
{"epoch": 62, "training_loss": 13823.852783203125, "training_acc": 49.0, "val_loss": 11096.046447753906, "val_acc": 52.0}
{"epoch": 63, "training_loss": 37399.68859863281, "training_acc": 53.0, "val_loss": 3304.8538208007812, "val_acc": 52.0}
{"epoch": 64, "training_loss": 15289.615905761719, "training_acc": 51.0, "val_loss": 9819.227600097656, "val_acc": 48.0}
{"epoch": 65, "training_loss": 35709.468505859375, "training_acc": 47.0, "val_loss": 891.1657333374023, "val_acc": 56.0}
{"epoch": 66, "training_loss": 9451.288452148438, "training_acc": 65.0, "val_loss": 10474.335479736328, "val_acc": 52.0}
