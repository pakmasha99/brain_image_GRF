"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 7774.7373695373535, "training_acc": 51.0, "val_loss": 28111.102294921875, "val_acc": 52.0}
{"epoch": 1, "training_loss": 101721.71508789062, "training_acc": 53.0, "val_loss": 12614.384460449219, "val_acc": 52.0}
{"epoch": 2, "training_loss": 46880.43981933594, "training_acc": 47.0, "val_loss": 15913.430786132812, "val_acc": 48.0}
{"epoch": 3, "training_loss": 53970.04150390625, "training_acc": 47.0, "val_loss": 3351.5701293945312, "val_acc": 52.0}
{"epoch": 4, "training_loss": 20322.09912109375, "training_acc": 53.0, "val_loss": 4028.447723388672, "val_acc": 52.0}
{"epoch": 5, "training_loss": 16625.25799560547, "training_acc": 57.0, "val_loss": 7425.2899169921875, "val_acc": 48.0}
{"epoch": 6, "training_loss": 23762.780334472656, "training_acc": 49.0, "val_loss": 3979.220199584961, "val_acc": 52.0}
{"epoch": 7, "training_loss": 21332.26318359375, "training_acc": 54.0, "val_loss": 3292.3095703125, "val_acc": 52.0}
{"epoch": 8, "training_loss": 17106.259765625, "training_acc": 55.0, "val_loss": 6195.030975341797, "val_acc": 48.0}
{"epoch": 9, "training_loss": 19563.777770996094, "training_acc": 50.0, "val_loss": 5458.137893676758, "val_acc": 52.0}
{"epoch": 10, "training_loss": 21513.511596679688, "training_acc": 53.0, "val_loss": 5212.565612792969, "val_acc": 52.0}
{"epoch": 11, "training_loss": 16194.934936523438, "training_acc": 47.0, "val_loss": 4044.3511962890625, "val_acc": 48.0}
{"epoch": 12, "training_loss": 14518.950668334961, "training_acc": 53.0, "val_loss": 5658.09326171875, "val_acc": 52.0}
{"epoch": 13, "training_loss": 15953.678894042969, "training_acc": 54.0, "val_loss": 1677.0471572875977, "val_acc": 48.0}
{"epoch": 14, "training_loss": 9048.643615722656, "training_acc": 55.0, "val_loss": 1511.5846633911133, "val_acc": 48.0}
{"epoch": 15, "training_loss": 8930.004821777344, "training_acc": 57.0, "val_loss": 5248.381805419922, "val_acc": 52.0}
{"epoch": 16, "training_loss": 9187.701293945312, "training_acc": 61.0, "val_loss": 3445.1717376708984, "val_acc": 48.0}
{"epoch": 17, "training_loss": 16801.57843017578, "training_acc": 48.0, "val_loss": 2968.7423706054688, "val_acc": 52.0}
{"epoch": 18, "training_loss": 7827.578063964844, "training_acc": 58.0, "val_loss": 1119.2384719848633, "val_acc": 48.0}
{"epoch": 19, "training_loss": 3715.985382080078, "training_acc": 58.0, "val_loss": 1661.3115310668945, "val_acc": 48.0}
{"epoch": 20, "training_loss": 5124.982421875, "training_acc": 53.0, "val_loss": 1087.458610534668, "val_acc": 52.0}
{"epoch": 21, "training_loss": 2540.3170471191406, "training_acc": 69.0, "val_loss": 2026.5300750732422, "val_acc": 48.0}
{"epoch": 22, "training_loss": 8510.502960205078, "training_acc": 55.0, "val_loss": 3806.779098510742, "val_acc": 52.0}
{"epoch": 23, "training_loss": 10115.718658447266, "training_acc": 53.0, "val_loss": 3079.647445678711, "val_acc": 48.0}
{"epoch": 24, "training_loss": 13589.879272460938, "training_acc": 47.0, "val_loss": 2867.0188903808594, "val_acc": 52.0}
{"epoch": 25, "training_loss": 7815.161163330078, "training_acc": 56.0, "val_loss": 845.4092025756836, "val_acc": 52.0}
{"epoch": 26, "training_loss": 5162.660614013672, "training_acc": 57.0, "val_loss": 3178.095054626465, "val_acc": 52.0}
{"epoch": 27, "training_loss": 6734.237060546875, "training_acc": 51.0, "val_loss": 1052.671718597412, "val_acc": 60.0}
{"epoch": 28, "training_loss": 4149.356147766113, "training_acc": 61.0, "val_loss": 1547.5224494934082, "val_acc": 56.0}
{"epoch": 29, "training_loss": 3112.4849014282227, "training_acc": 59.0, "val_loss": 712.9261493682861, "val_acc": 40.0}
{"epoch": 30, "training_loss": 2497.0789489746094, "training_acc": 68.0, "val_loss": 418.2063579559326, "val_acc": 48.0}
{"epoch": 31, "training_loss": 2276.7210998535156, "training_acc": 59.0, "val_loss": 2357.330322265625, "val_acc": 52.0}
{"epoch": 32, "training_loss": 4125.777381896973, "training_acc": 63.0, "val_loss": 4706.338119506836, "val_acc": 48.0}
{"epoch": 33, "training_loss": 17445.382202148438, "training_acc": 47.0, "val_loss": 2902.107810974121, "val_acc": 52.0}
{"epoch": 34, "training_loss": 12141.179870605469, "training_acc": 53.0, "val_loss": 532.6082706451416, "val_acc": 44.0}
{"epoch": 35, "training_loss": 7405.347717285156, "training_acc": 62.0, "val_loss": 4057.9498291015625, "val_acc": 48.0}
{"epoch": 36, "training_loss": 11723.427749633789, "training_acc": 51.0, "val_loss": 3321.341323852539, "val_acc": 52.0}
{"epoch": 37, "training_loss": 6702.852466583252, "training_acc": 61.0, "val_loss": 1718.6960220336914, "val_acc": 48.0}
{"epoch": 38, "training_loss": 6657.086761474609, "training_acc": 51.0, "val_loss": 1792.9410934448242, "val_acc": 52.0}
{"epoch": 39, "training_loss": 6615.024688720703, "training_acc": 61.0, "val_loss": 2697.597122192383, "val_acc": 48.0}
{"epoch": 40, "training_loss": 10303.732452392578, "training_acc": 47.0, "val_loss": 3582.9654693603516, "val_acc": 52.0}
{"epoch": 41, "training_loss": 7248.400650024414, "training_acc": 61.0, "val_loss": 1672.9753494262695, "val_acc": 52.0}
{"epoch": 42, "training_loss": 6310.8681640625, "training_acc": 51.0, "val_loss": 1724.6416091918945, "val_acc": 52.0}
{"epoch": 43, "training_loss": 7394.144958496094, "training_acc": 56.0, "val_loss": 2573.4994888305664, "val_acc": 48.0}
{"epoch": 44, "training_loss": 6493.987533569336, "training_acc": 60.0, "val_loss": 2888.4286880493164, "val_acc": 52.0}
{"epoch": 45, "training_loss": 8582.624252319336, "training_acc": 52.0, "val_loss": 711.8962287902832, "val_acc": 52.0}
{"epoch": 46, "training_loss": 4678.618240356445, "training_acc": 52.0, "val_loss": 741.0020351409912, "val_acc": 56.0}
{"epoch": 47, "training_loss": 2686.565399169922, "training_acc": 62.0, "val_loss": 3273.305892944336, "val_acc": 52.0}
{"epoch": 48, "training_loss": 5525.348793029785, "training_acc": 60.0, "val_loss": 696.4624404907227, "val_acc": 44.0}
{"epoch": 49, "training_loss": 3847.719284057617, "training_acc": 62.0, "val_loss": 833.1989288330078, "val_acc": 40.0}
