"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 299.44199562072754, "training_acc": 49.0, "val_loss": 59.56690311431885, "val_acc": 52.0}
{"epoch": 1, "training_loss": 280.83263206481934, "training_acc": 53.0, "val_loss": 140.24310111999512, "val_acc": 48.0}
{"epoch": 2, "training_loss": 518.8989639282227, "training_acc": 47.0, "val_loss": 30.247825384140015, "val_acc": 48.0}
{"epoch": 3, "training_loss": 192.71651935577393, "training_acc": 45.0, "val_loss": 111.85779571533203, "val_acc": 52.0}
{"epoch": 4, "training_loss": 396.2701110839844, "training_acc": 53.0, "val_loss": 84.98242497444153, "val_acc": 52.0}
{"epoch": 5, "training_loss": 257.2880017757416, "training_acc": 53.0, "val_loss": 33.166518807411194, "val_acc": 48.0}
{"epoch": 6, "training_loss": 183.06984043121338, "training_acc": 47.0, "val_loss": 49.635934829711914, "val_acc": 48.0}
{"epoch": 7, "training_loss": 168.20846796035767, "training_acc": 47.0, "val_loss": 38.730695843696594, "val_acc": 52.0}
{"epoch": 8, "training_loss": 149.834547996521, "training_acc": 53.0, "val_loss": 59.6676766872406, "val_acc": 52.0}
{"epoch": 9, "training_loss": 157.62784481048584, "training_acc": 54.0, "val_loss": 19.434809684753418, "val_acc": 40.0}
{"epoch": 10, "training_loss": 104.2723913192749, "training_acc": 52.0, "val_loss": 31.492018699645996, "val_acc": 48.0}
{"epoch": 11, "training_loss": 114.49230551719666, "training_acc": 48.0, "val_loss": 33.220115303993225, "val_acc": 52.0}
{"epoch": 12, "training_loss": 121.0636944770813, "training_acc": 53.0, "val_loss": 38.251909613609314, "val_acc": 52.0}
{"epoch": 13, "training_loss": 101.81603574752808, "training_acc": 59.0, "val_loss": 23.816165328025818, "val_acc": 48.0}
{"epoch": 14, "training_loss": 105.01205825805664, "training_acc": 47.0, "val_loss": 19.28613632917404, "val_acc": 40.0}
{"epoch": 15, "training_loss": 70.82832360267639, "training_acc": 60.0, "val_loss": 33.66765379905701, "val_acc": 52.0}
{"epoch": 16, "training_loss": 110.98314571380615, "training_acc": 53.0, "val_loss": 18.403370678424835, "val_acc": 52.0}
{"epoch": 17, "training_loss": 82.95824956893921, "training_acc": 50.0, "val_loss": 25.946062803268433, "val_acc": 48.0}
{"epoch": 18, "training_loss": 97.33914542198181, "training_acc": 51.0, "val_loss": 24.882027506828308, "val_acc": 52.0}
{"epoch": 19, "training_loss": 104.41652536392212, "training_acc": 53.0, "val_loss": 22.13301807641983, "val_acc": 52.0}
{"epoch": 20, "training_loss": 73.65461683273315, "training_acc": 56.0, "val_loss": 25.745409727096558, "val_acc": 48.0}
{"epoch": 21, "training_loss": 97.61456775665283, "training_acc": 47.0, "val_loss": 18.84138584136963, "val_acc": 52.0}
{"epoch": 22, "training_loss": 74.56308674812317, "training_acc": 56.0, "val_loss": 23.72947484254837, "val_acc": 52.0}
{"epoch": 23, "training_loss": 73.41903686523438, "training_acc": 55.0, "val_loss": 20.09667009115219, "val_acc": 52.0}
{"epoch": 24, "training_loss": 80.61723303794861, "training_acc": 48.0, "val_loss": 18.332961201667786, "val_acc": 52.0}
{"epoch": 25, "training_loss": 67.89941549301147, "training_acc": 63.0, "val_loss": 24.541400372982025, "val_acc": 52.0}
{"epoch": 26, "training_loss": 73.47406792640686, "training_acc": 58.0, "val_loss": 18.17687153816223, "val_acc": 48.0}
{"epoch": 27, "training_loss": 68.49436211585999, "training_acc": 53.0, "val_loss": 17.942039668560028, "val_acc": 52.0}
{"epoch": 28, "training_loss": 62.935386419296265, "training_acc": 66.0, "val_loss": 19.646228849887848, "val_acc": 52.0}
{"epoch": 29, "training_loss": 67.96037817001343, "training_acc": 58.0, "val_loss": 17.494763433933258, "val_acc": 56.0}
{"epoch": 30, "training_loss": 63.73308229446411, "training_acc": 63.0, "val_loss": 17.954787611961365, "val_acc": 52.0}
{"epoch": 31, "training_loss": 61.18130111694336, "training_acc": 70.0, "val_loss": 17.961789667606354, "val_acc": 52.0}
{"epoch": 32, "training_loss": 62.07378125190735, "training_acc": 67.0, "val_loss": 18.45008283853531, "val_acc": 52.0}
{"epoch": 33, "training_loss": 63.585612773895264, "training_acc": 58.0, "val_loss": 20.104090869426727, "val_acc": 52.0}
{"epoch": 34, "training_loss": 64.5240478515625, "training_acc": 60.0, "val_loss": 17.917747795581818, "val_acc": 52.0}
{"epoch": 35, "training_loss": 68.12053990364075, "training_acc": 55.0, "val_loss": 17.952826619148254, "val_acc": 52.0}
{"epoch": 36, "training_loss": 59.255008697509766, "training_acc": 74.0, "val_loss": 19.025275111198425, "val_acc": 52.0}
{"epoch": 37, "training_loss": 63.07487869262695, "training_acc": 66.0, "val_loss": 18.177880346775055, "val_acc": 52.0}
{"epoch": 38, "training_loss": 63.66348576545715, "training_acc": 70.0, "val_loss": 20.13053447008133, "val_acc": 52.0}
{"epoch": 39, "training_loss": 64.481290102005, "training_acc": 62.0, "val_loss": 18.232612311840057, "val_acc": 52.0}
{"epoch": 40, "training_loss": 64.22152924537659, "training_acc": 66.0, "val_loss": 18.054291605949402, "val_acc": 52.0}
{"epoch": 41, "training_loss": 63.849003314971924, "training_acc": 64.0, "val_loss": 18.853046000003815, "val_acc": 52.0}
{"epoch": 42, "training_loss": 62.17364430427551, "training_acc": 70.0, "val_loss": 18.908511102199554, "val_acc": 56.0}
{"epoch": 43, "training_loss": 69.67886400222778, "training_acc": 51.0, "val_loss": 19.592921435832977, "val_acc": 52.0}
{"epoch": 44, "training_loss": 67.28588509559631, "training_acc": 58.0, "val_loss": 19.51308399438858, "val_acc": 52.0}
{"epoch": 45, "training_loss": 62.76864695549011, "training_acc": 64.0, "val_loss": 19.07823383808136, "val_acc": 44.0}
{"epoch": 46, "training_loss": 68.43968772888184, "training_acc": 55.0, "val_loss": 22.149577736854553, "val_acc": 52.0}
{"epoch": 47, "training_loss": 67.24279522895813, "training_acc": 57.0, "val_loss": 18.418116867542267, "val_acc": 52.0}
{"epoch": 48, "training_loss": 64.91259050369263, "training_acc": 64.0, "val_loss": 18.005740642547607, "val_acc": 52.0}
