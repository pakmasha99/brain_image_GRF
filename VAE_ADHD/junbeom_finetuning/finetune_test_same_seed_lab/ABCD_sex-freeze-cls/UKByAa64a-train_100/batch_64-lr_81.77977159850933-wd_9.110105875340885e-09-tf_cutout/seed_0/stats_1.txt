"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 171802.59465408325, "training_acc": 46.0, "val_loss": 35901.64489746094, "val_acc": 52.0}
{"epoch": 1, "training_loss": 174334.53125, "training_acc": 53.0, "val_loss": 92977.06298828125, "val_acc": 48.0}
{"epoch": 2, "training_loss": 349919.6240234375, "training_acc": 47.0, "val_loss": 25773.104858398438, "val_acc": 48.0}
{"epoch": 3, "training_loss": 121930.4609375, "training_acc": 51.0, "val_loss": 67902.15454101562, "val_acc": 52.0}
{"epoch": 4, "training_loss": 257980.0322265625, "training_acc": 53.0, "val_loss": 60591.05224609375, "val_acc": 52.0}
{"epoch": 5, "training_loss": 179829.861328125, "training_acc": 53.0, "val_loss": 10527.857208251953, "val_acc": 48.0}
{"epoch": 6, "training_loss": 97032.58447265625, "training_acc": 47.0, "val_loss": 31034.561157226562, "val_acc": 48.0}
{"epoch": 7, "training_loss": 105194.23779296875, "training_acc": 47.0, "val_loss": 22487.29248046875, "val_acc": 52.0}
{"epoch": 8, "training_loss": 87684.25415039062, "training_acc": 53.0, "val_loss": 38992.010498046875, "val_acc": 52.0}
{"epoch": 9, "training_loss": 126613.14916992188, "training_acc": 53.0, "val_loss": 4913.330078125, "val_acc": 52.0}
{"epoch": 10, "training_loss": 50639.2294921875, "training_acc": 49.0, "val_loss": 36254.90417480469, "val_acc": 48.0}
{"epoch": 11, "training_loss": 138910.21801757812, "training_acc": 47.0, "val_loss": 7690.778350830078, "val_acc": 48.0}
{"epoch": 12, "training_loss": 50330.31982421875, "training_acc": 49.0, "val_loss": 38149.65515136719, "val_acc": 52.0}
{"epoch": 13, "training_loss": 146674.91259765625, "training_acc": 53.0, "val_loss": 28599.307250976562, "val_acc": 52.0}
{"epoch": 14, "training_loss": 76775.66864013672, "training_acc": 52.0, "val_loss": 28050.799560546875, "val_acc": 48.0}
{"epoch": 15, "training_loss": 136664.53125, "training_acc": 47.0, "val_loss": 41726.318359375, "val_acc": 48.0}
{"epoch": 16, "training_loss": 140789.53076171875, "training_acc": 47.0, "val_loss": 3784.6824645996094, "val_acc": 60.0}
{"epoch": 17, "training_loss": 35465.911865234375, "training_acc": 55.0, "val_loss": 24493.53790283203, "val_acc": 52.0}
{"epoch": 18, "training_loss": 79324.84606933594, "training_acc": 53.0, "val_loss": 3938.7577056884766, "val_acc": 48.0}
{"epoch": 19, "training_loss": 20660.18408203125, "training_acc": 55.0, "val_loss": 1867.4524307250977, "val_acc": 48.0}
{"epoch": 20, "training_loss": 22878.986694335938, "training_acc": 56.0, "val_loss": 9771.273040771484, "val_acc": 52.0}
{"epoch": 21, "training_loss": 22439.684661865234, "training_acc": 55.0, "val_loss": 6212.117385864258, "val_acc": 48.0}
{"epoch": 22, "training_loss": 19339.844604492188, "training_acc": 54.0, "val_loss": 8880.912017822266, "val_acc": 52.0}
{"epoch": 23, "training_loss": 17377.497131347656, "training_acc": 59.0, "val_loss": 10287.200927734375, "val_acc": 48.0}
{"epoch": 24, "training_loss": 36945.36779785156, "training_acc": 48.0, "val_loss": 8194.046783447266, "val_acc": 52.0}
{"epoch": 25, "training_loss": 29469.99755859375, "training_acc": 56.0, "val_loss": 4175.559616088867, "val_acc": 60.0}
{"epoch": 26, "training_loss": 30807.6220703125, "training_acc": 50.0, "val_loss": 10585.038757324219, "val_acc": 48.0}
{"epoch": 27, "training_loss": 33417.57946777344, "training_acc": 50.0, "val_loss": 7911.854553222656, "val_acc": 56.0}
{"epoch": 28, "training_loss": 16826.542694091797, "training_acc": 57.0, "val_loss": 5025.021743774414, "val_acc": 48.0}
{"epoch": 29, "training_loss": 13682.34114074707, "training_acc": 62.0, "val_loss": 10398.3642578125, "val_acc": 52.0}
{"epoch": 30, "training_loss": 27078.80487060547, "training_acc": 52.0, "val_loss": 8195.55435180664, "val_acc": 48.0}
{"epoch": 31, "training_loss": 32587.092041015625, "training_acc": 47.0, "val_loss": 9672.422790527344, "val_acc": 52.0}
{"epoch": 32, "training_loss": 32172.392333984375, "training_acc": 52.0, "val_loss": 4763.980865478516, "val_acc": 56.0}
{"epoch": 33, "training_loss": 18222.56494140625, "training_acc": 67.0, "val_loss": 7595.811462402344, "val_acc": 48.0}
{"epoch": 34, "training_loss": 40559.726318359375, "training_acc": 38.0, "val_loss": 8393.64242553711, "val_acc": 56.0}
{"epoch": 35, "training_loss": 19965.070434570312, "training_acc": 62.0, "val_loss": 1752.7790069580078, "val_acc": 48.0}
{"epoch": 36, "training_loss": 16920.124267578125, "training_acc": 61.0, "val_loss": 5609.579086303711, "val_acc": 56.0}
{"epoch": 37, "training_loss": 13167.173706054688, "training_acc": 53.0, "val_loss": 2313.638114929199, "val_acc": 64.0}
{"epoch": 38, "training_loss": 8887.818908691406, "training_acc": 58.0, "val_loss": 3313.2030487060547, "val_acc": 52.0}
{"epoch": 39, "training_loss": 10127.789306640625, "training_acc": 58.0, "val_loss": 3248.843765258789, "val_acc": 60.0}
{"epoch": 40, "training_loss": 10775.581359863281, "training_acc": 55.0, "val_loss": 1676.824951171875, "val_acc": 68.0}
{"epoch": 41, "training_loss": 5312.310745239258, "training_acc": 69.0, "val_loss": 966.7692184448242, "val_acc": 52.0}
{"epoch": 42, "training_loss": 3311.2034912109375, "training_acc": 72.0, "val_loss": 5228.616714477539, "val_acc": 52.0}
{"epoch": 43, "training_loss": 8886.082862854004, "training_acc": 65.0, "val_loss": 4940.591812133789, "val_acc": 48.0}
{"epoch": 44, "training_loss": 20050.076904296875, "training_acc": 46.0, "val_loss": 1511.5018844604492, "val_acc": 56.0}
{"epoch": 45, "training_loss": 14845.644287109375, "training_acc": 62.0, "val_loss": 4276.956939697266, "val_acc": 52.0}
{"epoch": 46, "training_loss": 11955.365173339844, "training_acc": 55.0, "val_loss": 8788.121795654297, "val_acc": 48.0}
{"epoch": 47, "training_loss": 35835.12316894531, "training_acc": 47.0, "val_loss": 7118.034362792969, "val_acc": 52.0}
{"epoch": 48, "training_loss": 24522.34112548828, "training_acc": 53.0, "val_loss": 1830.2913665771484, "val_acc": 56.0}
{"epoch": 49, "training_loss": 7190.328094482422, "training_acc": 58.0, "val_loss": 8254.161834716797, "val_acc": 52.0}
{"epoch": 50, "training_loss": 26736.697021484375, "training_acc": 53.0, "val_loss": 8997.196197509766, "val_acc": 48.0}
{"epoch": 51, "training_loss": 29348.013793945312, "training_acc": 48.0, "val_loss": 5831.729507446289, "val_acc": 52.0}
{"epoch": 52, "training_loss": 22801.898681640625, "training_acc": 53.0, "val_loss": 5775.563049316406, "val_acc": 48.0}
{"epoch": 53, "training_loss": 21361.523315429688, "training_acc": 48.0, "val_loss": 10345.765686035156, "val_acc": 52.0}
{"epoch": 54, "training_loss": 39076.6142578125, "training_acc": 53.0, "val_loss": 1460.0517272949219, "val_acc": 60.0}
{"epoch": 55, "training_loss": 23135.55517578125, "training_acc": 61.0, "val_loss": 9055.269622802734, "val_acc": 48.0}
{"epoch": 56, "training_loss": 26464.112243652344, "training_acc": 56.0, "val_loss": 11324.774169921875, "val_acc": 52.0}
{"epoch": 57, "training_loss": 28247.21173095703, "training_acc": 54.0, "val_loss": 10686.602020263672, "val_acc": 48.0}
{"epoch": 58, "training_loss": 48328.439208984375, "training_acc": 47.0, "val_loss": 3316.982650756836, "val_acc": 56.0}
{"epoch": 59, "training_loss": 18967.39453125, "training_acc": 61.0, "val_loss": 4096.885299682617, "val_acc": 56.0}
{"epoch": 60, "training_loss": 13708.289245605469, "training_acc": 65.0, "val_loss": 9826.549530029297, "val_acc": 48.0}
