"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 52204.80213165283, "training_acc": 47.0, "val_loss": 68946.60034179688, "val_acc": 52.0}
{"epoch": 1, "training_loss": 273704.2470703125, "training_acc": 53.0, "val_loss": 46130.49621582031, "val_acc": 52.0}
{"epoch": 2, "training_loss": 126661.98767089844, "training_acc": 52.0, "val_loss": 30488.400268554688, "val_acc": 48.0}
{"epoch": 3, "training_loss": 131585.54931640625, "training_acc": 47.0, "val_loss": 4695.421600341797, "val_acc": 60.0}
{"epoch": 4, "training_loss": 34735.6630859375, "training_acc": 56.0, "val_loss": 16078.146362304688, "val_acc": 52.0}
{"epoch": 5, "training_loss": 47254.11779785156, "training_acc": 48.0, "val_loss": 6475.5767822265625, "val_acc": 52.0}
{"epoch": 6, "training_loss": 23150.97674560547, "training_acc": 58.0, "val_loss": 5518.434524536133, "val_acc": 56.0}
{"epoch": 7, "training_loss": 26339.608154296875, "training_acc": 57.0, "val_loss": 7363.124084472656, "val_acc": 48.0}
{"epoch": 8, "training_loss": 27338.09423828125, "training_acc": 50.0, "val_loss": 5804.836273193359, "val_acc": 56.0}
{"epoch": 9, "training_loss": 31040.612182617188, "training_acc": 50.0, "val_loss": 2851.1831283569336, "val_acc": 52.0}
{"epoch": 10, "training_loss": 20802.328125, "training_acc": 52.0, "val_loss": 6611.080169677734, "val_acc": 56.0}
{"epoch": 11, "training_loss": 34491.7109375, "training_acc": 47.0, "val_loss": 7071.253204345703, "val_acc": 48.0}
{"epoch": 12, "training_loss": 31124.05859375, "training_acc": 50.0, "val_loss": 12338.442993164062, "val_acc": 52.0}
{"epoch": 13, "training_loss": 30272.60089111328, "training_acc": 55.0, "val_loss": 7456.093597412109, "val_acc": 48.0}
{"epoch": 14, "training_loss": 31994.138549804688, "training_acc": 50.0, "val_loss": 13675.627136230469, "val_acc": 52.0}
{"epoch": 15, "training_loss": 49415.870361328125, "training_acc": 52.0, "val_loss": 7922.882080078125, "val_acc": 52.0}
{"epoch": 16, "training_loss": 23891.5419921875, "training_acc": 60.0, "val_loss": 14182.748413085938, "val_acc": 48.0}
{"epoch": 17, "training_loss": 46012.28985595703, "training_acc": 49.0, "val_loss": 17202.374267578125, "val_acc": 52.0}
{"epoch": 18, "training_loss": 66195.58715820312, "training_acc": 53.0, "val_loss": 15028.610229492188, "val_acc": 52.0}
{"epoch": 19, "training_loss": 36700.41320800781, "training_acc": 53.0, "val_loss": 13874.917602539062, "val_acc": 48.0}
{"epoch": 20, "training_loss": 49170.12268066406, "training_acc": 47.0, "val_loss": 7490.126037597656, "val_acc": 52.0}
{"epoch": 21, "training_loss": 30506.69580078125, "training_acc": 54.0, "val_loss": 6958.0474853515625, "val_acc": 56.0}
{"epoch": 22, "training_loss": 25695.027587890625, "training_acc": 56.0, "val_loss": 9134.664154052734, "val_acc": 48.0}
{"epoch": 23, "training_loss": 25647.535095214844, "training_acc": 55.0, "val_loss": 9542.526245117188, "val_acc": 52.0}
{"epoch": 24, "training_loss": 27672.19549560547, "training_acc": 55.0, "val_loss": 8675.518035888672, "val_acc": 48.0}
{"epoch": 25, "training_loss": 34086.545654296875, "training_acc": 49.0, "val_loss": 6735.321807861328, "val_acc": 52.0}
{"epoch": 26, "training_loss": 22182.586059570312, "training_acc": 56.0, "val_loss": 2787.788200378418, "val_acc": 56.0}
{"epoch": 27, "training_loss": 11545.532318115234, "training_acc": 63.0, "val_loss": 4379.01611328125, "val_acc": 60.0}
{"epoch": 28, "training_loss": 11757.443359375, "training_acc": 59.0, "val_loss": 5123.640441894531, "val_acc": 48.0}
{"epoch": 29, "training_loss": 20878.421417236328, "training_acc": 57.0, "val_loss": 11139.642333984375, "val_acc": 52.0}
{"epoch": 30, "training_loss": 34384.79553222656, "training_acc": 53.0, "val_loss": 5029.07600402832, "val_acc": 44.0}
{"epoch": 31, "training_loss": 27653.11004638672, "training_acc": 47.0, "val_loss": 7527.11181640625, "val_acc": 52.0}
{"epoch": 32, "training_loss": 21203.84588623047, "training_acc": 53.0, "val_loss": 1941.653060913086, "val_acc": 64.0}
{"epoch": 33, "training_loss": 9942.112365722656, "training_acc": 61.0, "val_loss": 9026.216888427734, "val_acc": 52.0}
{"epoch": 34, "training_loss": 19948.69061279297, "training_acc": 51.0, "val_loss": 8221.38671875, "val_acc": 48.0}
{"epoch": 35, "training_loss": 35921.623046875, "training_acc": 47.0, "val_loss": 5420.956039428711, "val_acc": 56.0}
{"epoch": 36, "training_loss": 14525.848236083984, "training_acc": 55.0, "val_loss": 2172.7930068969727, "val_acc": 52.0}
{"epoch": 37, "training_loss": 9016.017990112305, "training_acc": 64.0, "val_loss": 3941.668701171875, "val_acc": 60.0}
{"epoch": 38, "training_loss": 7212.265182495117, "training_acc": 66.0, "val_loss": 1849.795913696289, "val_acc": 52.0}
{"epoch": 39, "training_loss": 11455.173217773438, "training_acc": 56.0, "val_loss": 1495.0339317321777, "val_acc": 52.0}
{"epoch": 40, "training_loss": 8585.809631347656, "training_acc": 62.0, "val_loss": 3306.7123413085938, "val_acc": 40.0}
{"epoch": 41, "training_loss": 10570.15478515625, "training_acc": 63.0, "val_loss": 7541.984558105469, "val_acc": 52.0}
{"epoch": 42, "training_loss": 14210.883773803711, "training_acc": 58.0, "val_loss": 1264.826774597168, "val_acc": 52.0}
{"epoch": 43, "training_loss": 10354.759094238281, "training_acc": 62.0, "val_loss": 3322.2366333007812, "val_acc": 60.0}
{"epoch": 44, "training_loss": 29293.622802734375, "training_acc": 50.0, "val_loss": 8977.536010742188, "val_acc": 48.0}
{"epoch": 45, "training_loss": 24206.16632080078, "training_acc": 58.0, "val_loss": 13360.797119140625, "val_acc": 52.0}
{"epoch": 46, "training_loss": 32708.945251464844, "training_acc": 53.0, "val_loss": 18911.82403564453, "val_acc": 48.0}
{"epoch": 47, "training_loss": 76319.08569335938, "training_acc": 47.0, "val_loss": 12519.37255859375, "val_acc": 48.0}
{"epoch": 48, "training_loss": 40856.128173828125, "training_acc": 51.0, "val_loss": 21614.678955078125, "val_acc": 52.0}
{"epoch": 49, "training_loss": 71388.79418945312, "training_acc": 53.0, "val_loss": 3497.211456298828, "val_acc": 48.0}
{"epoch": 50, "training_loss": 26244.696044921875, "training_acc": 53.0, "val_loss": 11602.191162109375, "val_acc": 48.0}
{"epoch": 51, "training_loss": 33463.97442626953, "training_acc": 49.0, "val_loss": 11249.569702148438, "val_acc": 52.0}
{"epoch": 52, "training_loss": 28306.097076416016, "training_acc": 57.0, "val_loss": 14803.523254394531, "val_acc": 48.0}
{"epoch": 53, "training_loss": 48211.89440917969, "training_acc": 47.0, "val_loss": 5174.693298339844, "val_acc": 52.0}
{"epoch": 54, "training_loss": 16917.817260742188, "training_acc": 55.0, "val_loss": 3864.760208129883, "val_acc": 44.0}
{"epoch": 55, "training_loss": 9318.307342529297, "training_acc": 57.0, "val_loss": 6560.320281982422, "val_acc": 52.0}
{"epoch": 56, "training_loss": 11789.796989440918, "training_acc": 64.0, "val_loss": 5843.339157104492, "val_acc": 44.0}
{"epoch": 57, "training_loss": 14069.714965820312, "training_acc": 56.0, "val_loss": 2751.5810012817383, "val_acc": 56.0}
{"epoch": 58, "training_loss": 7769.4405517578125, "training_acc": 63.0, "val_loss": 6121.171569824219, "val_acc": 52.0}
{"epoch": 59, "training_loss": 10841.691192626953, "training_acc": 59.0, "val_loss": 9569.29702758789, "val_acc": 48.0}
{"epoch": 60, "training_loss": 31823.334106445312, "training_acc": 48.0, "val_loss": 10216.378784179688, "val_acc": 52.0}
{"epoch": 61, "training_loss": 34702.94689941406, "training_acc": 53.0, "val_loss": 2422.9413986206055, "val_acc": 56.0}
