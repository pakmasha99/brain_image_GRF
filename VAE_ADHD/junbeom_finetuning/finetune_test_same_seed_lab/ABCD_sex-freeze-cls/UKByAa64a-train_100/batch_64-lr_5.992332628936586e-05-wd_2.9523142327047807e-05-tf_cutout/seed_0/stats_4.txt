"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 71.86500954627991, "training_acc": 47.0, "val_loss": 17.818202078342438, "val_acc": 52.0}
{"epoch": 1, "training_loss": 71.49799299240112, "training_acc": 47.0, "val_loss": 17.70413964986801, "val_acc": 52.0}
{"epoch": 2, "training_loss": 70.86007690429688, "training_acc": 47.0, "val_loss": 17.607267200946808, "val_acc": 52.0}
{"epoch": 3, "training_loss": 70.35170316696167, "training_acc": 47.0, "val_loss": 17.527900636196136, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.99357461929321, "training_acc": 47.0, "val_loss": 17.46576875448227, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.69945955276489, "training_acc": 47.0, "val_loss": 17.41851419210434, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.45289659500122, "training_acc": 47.0, "val_loss": 17.382831871509552, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.38392758369446, "training_acc": 51.0, "val_loss": 17.35931485891342, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.15569257736206, "training_acc": 59.0, "val_loss": 17.347192764282227, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.38264298439026, "training_acc": 53.0, "val_loss": 17.341813445091248, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.11863994598389, "training_acc": 53.0, "val_loss": 17.341701686382294, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.31791043281555, "training_acc": 53.0, "val_loss": 17.344363033771515, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.27185487747192, "training_acc": 53.0, "val_loss": 17.347709834575653, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.09178137779236, "training_acc": 53.0, "val_loss": 17.350436747074127, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.12542510032654, "training_acc": 53.0, "val_loss": 17.35401153564453, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.18524861335754, "training_acc": 53.0, "val_loss": 17.357751727104187, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.25205826759338, "training_acc": 53.0, "val_loss": 17.358331382274628, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.24662137031555, "training_acc": 53.0, "val_loss": 17.35975295305252, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.05602669715881, "training_acc": 53.0, "val_loss": 17.36018657684326, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.3036994934082, "training_acc": 53.0, "val_loss": 17.360511422157288, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.07215428352356, "training_acc": 53.0, "val_loss": 17.36166775226593, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.14109539985657, "training_acc": 53.0, "val_loss": 17.3622265458107, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.19682765007019, "training_acc": 53.0, "val_loss": 17.362697422504425, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.32156085968018, "training_acc": 53.0, "val_loss": 17.36171990633011, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.17544937133789, "training_acc": 53.0, "val_loss": 17.36004203557968, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.22800016403198, "training_acc": 53.0, "val_loss": 17.358319461345673, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.05797290802002, "training_acc": 53.0, "val_loss": 17.357070744037628, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.18235731124878, "training_acc": 53.0, "val_loss": 17.35585927963257, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.0957441329956, "training_acc": 53.0, "val_loss": 17.354291677474976, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.236013174057, "training_acc": 53.0, "val_loss": 17.35294908285141, "val_acc": 52.0}
