"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 249779.8653869629, "training_acc": 52.0, "val_loss": 82996.62475585938, "val_acc": 44.0}
{"epoch": 1, "training_loss": 254914.828125, "training_acc": 48.0, "val_loss": 87301.93481445312, "val_acc": 56.0}
{"epoch": 2, "training_loss": 404554.072265625, "training_acc": 52.0, "val_loss": 36709.97314453125, "val_acc": 56.0}
{"epoch": 3, "training_loss": 208793.302734375, "training_acc": 54.0, "val_loss": 140389.404296875, "val_acc": 44.0}
{"epoch": 4, "training_loss": 471492.8720703125, "training_acc": 48.0, "val_loss": 11843.751525878906, "val_acc": 44.0}
{"epoch": 5, "training_loss": 219253.279296875, "training_acc": 44.0, "val_loss": 154870.47119140625, "val_acc": 56.0}
{"epoch": 6, "training_loss": 677149.109375, "training_acc": 52.0, "val_loss": 118862.5244140625, "val_acc": 56.0}
{"epoch": 7, "training_loss": 413631.75, "training_acc": 52.0, "val_loss": 66269.58618164062, "val_acc": 44.0}
{"epoch": 8, "training_loss": 300146.7802734375, "training_acc": 48.0, "val_loss": 127884.14306640625, "val_acc": 44.0}
{"epoch": 9, "training_loss": 417640.4580078125, "training_acc": 48.0, "val_loss": 3764.056396484375, "val_acc": 44.0}
{"epoch": 10, "training_loss": 140282.19921875, "training_acc": 52.0, "val_loss": 133922.7783203125, "val_acc": 56.0}
{"epoch": 11, "training_loss": 599260.904296875, "training_acc": 52.0, "val_loss": 117446.03271484375, "val_acc": 56.0}
{"epoch": 12, "training_loss": 418532.9873046875, "training_acc": 52.0, "val_loss": 26001.971435546875, "val_acc": 44.0}
{"epoch": 13, "training_loss": 166346.45703125, "training_acc": 48.0, "val_loss": 78414.4287109375, "val_acc": 44.0}
{"epoch": 14, "training_loss": 226899.08056640625, "training_acc": 48.0, "val_loss": 36838.78173828125, "val_acc": 56.0}
{"epoch": 15, "training_loss": 195299.8896484375, "training_acc": 52.0, "val_loss": 57680.120849609375, "val_acc": 56.0}
{"epoch": 16, "training_loss": 195835.17431640625, "training_acc": 52.0, "val_loss": 50097.03063964844, "val_acc": 44.0}
{"epoch": 17, "training_loss": 222383.7255859375, "training_acc": 48.0, "val_loss": 63102.44140625, "val_acc": 44.0}
{"epoch": 18, "training_loss": 153140.52807617188, "training_acc": 48.0, "val_loss": 68070.18432617188, "val_acc": 56.0}
{"epoch": 19, "training_loss": 353658.59375, "training_acc": 52.0, "val_loss": 104995.27587890625, "val_acc": 56.0}
{"epoch": 20, "training_loss": 411258.5126953125, "training_acc": 52.0, "val_loss": 18890.7958984375, "val_acc": 56.0}
{"epoch": 21, "training_loss": 174203.5693359375, "training_acc": 46.0, "val_loss": 128475.35400390625, "val_acc": 44.0}
{"epoch": 22, "training_loss": 469860.26953125, "training_acc": 48.0, "val_loss": 89270.71533203125, "val_acc": 44.0}
{"epoch": 23, "training_loss": 233218.4324951172, "training_acc": 48.0, "val_loss": 73841.2109375, "val_acc": 56.0}
{"epoch": 24, "training_loss": 403118.986328125, "training_acc": 52.0, "val_loss": 130334.85107421875, "val_acc": 56.0}
{"epoch": 25, "training_loss": 534365.81640625, "training_acc": 52.0, "val_loss": 60159.539794921875, "val_acc": 56.0}
{"epoch": 26, "training_loss": 143363.38122558594, "training_acc": 62.0, "val_loss": 58873.7548828125, "val_acc": 44.0}
{"epoch": 27, "training_loss": 215911.6845703125, "training_acc": 48.0, "val_loss": 22569.993591308594, "val_acc": 44.0}
{"epoch": 28, "training_loss": 100149.1826171875, "training_acc": 54.0, "val_loss": 65432.342529296875, "val_acc": 56.0}
