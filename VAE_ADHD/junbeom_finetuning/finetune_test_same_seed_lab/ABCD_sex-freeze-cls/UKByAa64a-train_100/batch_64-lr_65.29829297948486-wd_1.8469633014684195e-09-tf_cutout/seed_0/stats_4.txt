"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 206451.11388778687, "training_acc": 44.0, "val_loss": 129288.78173828125, "val_acc": 52.0}
{"epoch": 1, "training_loss": 528644.916015625, "training_acc": 53.0, "val_loss": 52195.654296875, "val_acc": 52.0}
{"epoch": 2, "training_loss": 372859.306640625, "training_acc": 41.0, "val_loss": 144835.6689453125, "val_acc": 48.0}
{"epoch": 3, "training_loss": 518319.8251953125, "training_acc": 47.0, "val_loss": 14538.746643066406, "val_acc": 52.0}
{"epoch": 4, "training_loss": 100400.01806640625, "training_acc": 53.0, "val_loss": 21652.952575683594, "val_acc": 52.0}
{"epoch": 5, "training_loss": 154215.4921875, "training_acc": 43.0, "val_loss": 54845.623779296875, "val_acc": 48.0}
{"epoch": 6, "training_loss": 152200.46875, "training_acc": 47.0, "val_loss": 75980.52978515625, "val_acc": 52.0}
{"epoch": 7, "training_loss": 347265.955078125, "training_acc": 53.0, "val_loss": 102904.98046875, "val_acc": 52.0}
{"epoch": 8, "training_loss": 339898.3330078125, "training_acc": 53.0, "val_loss": 16016.172790527344, "val_acc": 48.0}
{"epoch": 9, "training_loss": 111653.0390625, "training_acc": 47.0, "val_loss": 30253.964233398438, "val_acc": 48.0}
{"epoch": 10, "training_loss": 95262.61474609375, "training_acc": 55.0, "val_loss": 37246.01135253906, "val_acc": 52.0}
{"epoch": 11, "training_loss": 119741.99633789062, "training_acc": 53.0, "val_loss": 36444.85168457031, "val_acc": 48.0}
{"epoch": 12, "training_loss": 158458.72802734375, "training_acc": 47.0, "val_loss": 14362.663269042969, "val_acc": 48.0}
{"epoch": 13, "training_loss": 105446.81396484375, "training_acc": 49.0, "val_loss": 71831.06079101562, "val_acc": 52.0}
{"epoch": 14, "training_loss": 265653.1357421875, "training_acc": 53.0, "val_loss": 16420.960998535156, "val_acc": 52.0}
{"epoch": 15, "training_loss": 133842.298828125, "training_acc": 49.0, "val_loss": 88989.5263671875, "val_acc": 48.0}
{"epoch": 16, "training_loss": 342632.830078125, "training_acc": 47.0, "val_loss": 24663.827514648438, "val_acc": 48.0}
{"epoch": 17, "training_loss": 135960.4306640625, "training_acc": 51.0, "val_loss": 94767.90161132812, "val_acc": 52.0}
{"epoch": 18, "training_loss": 383751.126953125, "training_acc": 53.0, "val_loss": 71326.62963867188, "val_acc": 52.0}
{"epoch": 19, "training_loss": 201869.90991210938, "training_acc": 53.0, "val_loss": 77406.16455078125, "val_acc": 48.0}
{"epoch": 20, "training_loss": 381205.359375, "training_acc": 47.0, "val_loss": 119929.35791015625, "val_acc": 48.0}
{"epoch": 21, "training_loss": 428713.0947265625, "training_acc": 47.0, "val_loss": 6459.327697753906, "val_acc": 48.0}
{"epoch": 22, "training_loss": 106312.61328125, "training_acc": 57.0, "val_loss": 145896.05712890625, "val_acc": 52.0}
{"epoch": 23, "training_loss": 614159.541015625, "training_acc": 53.0, "val_loss": 156650.0244140625, "val_acc": 52.0}
{"epoch": 24, "training_loss": 547953.65625, "training_acc": 53.0, "val_loss": 40338.5498046875, "val_acc": 52.0}
{"epoch": 25, "training_loss": 172403.8935546875, "training_acc": 55.0, "val_loss": 118709.75341796875, "val_acc": 48.0}
{"epoch": 26, "training_loss": 493191.091796875, "training_acc": 47.0, "val_loss": 107125.32958984375, "val_acc": 48.0}
{"epoch": 27, "training_loss": 347061.9736328125, "training_acc": 47.0, "val_loss": 37193.27392578125, "val_acc": 52.0}
{"epoch": 28, "training_loss": 204601.11328125, "training_acc": 53.0, "val_loss": 89861.1328125, "val_acc": 52.0}
{"epoch": 29, "training_loss": 320419.935546875, "training_acc": 53.0, "val_loss": 15106.578063964844, "val_acc": 52.0}
{"epoch": 30, "training_loss": 145201.4052734375, "training_acc": 51.0, "val_loss": 116649.79248046875, "val_acc": 48.0}
{"epoch": 31, "training_loss": 476891.78515625, "training_acc": 47.0, "val_loss": 79307.03125, "val_acc": 48.0}
{"epoch": 32, "training_loss": 217280.40362548828, "training_acc": 47.0, "val_loss": 91611.59057617188, "val_acc": 52.0}
{"epoch": 33, "training_loss": 409345.9736328125, "training_acc": 53.0, "val_loss": 164050.64697265625, "val_acc": 52.0}
{"epoch": 34, "training_loss": 626565.271484375, "training_acc": 53.0, "val_loss": 111444.7021484375, "val_acc": 52.0}
{"epoch": 35, "training_loss": 336184.7255859375, "training_acc": 53.0, "val_loss": 48970.843505859375, "val_acc": 48.0}
{"epoch": 36, "training_loss": 262129.26953125, "training_acc": 47.0, "val_loss": 109343.212890625, "val_acc": 48.0}
{"epoch": 37, "training_loss": 402542.873046875, "training_acc": 47.0, "val_loss": 15982.356262207031, "val_acc": 48.0}
{"epoch": 38, "training_loss": 179037.2744140625, "training_acc": 41.0, "val_loss": 118885.85205078125, "val_acc": 52.0}
{"epoch": 39, "training_loss": 493227.849609375, "training_acc": 53.0, "val_loss": 102758.75244140625, "val_acc": 52.0}
{"epoch": 40, "training_loss": 317699.17138671875, "training_acc": 53.0, "val_loss": 41693.68591308594, "val_acc": 48.0}
