"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 458945.60832595825, "training_acc": 46.0, "val_loss": 95921.9482421875, "val_acc": 52.0}
{"epoch": 1, "training_loss": 465784.3359375, "training_acc": 53.0, "val_loss": 248414.501953125, "val_acc": 48.0}
{"epoch": 2, "training_loss": 934910.125, "training_acc": 47.0, "val_loss": 68860.1318359375, "val_acc": 48.0}
{"epoch": 3, "training_loss": 325772.4892578125, "training_acc": 51.0, "val_loss": 181420.27587890625, "val_acc": 52.0}
{"epoch": 4, "training_loss": 689267.41796875, "training_acc": 53.0, "val_loss": 161886.77978515625, "val_acc": 52.0}
{"epoch": 5, "training_loss": 480467.775390625, "training_acc": 53.0, "val_loss": 28127.63671875, "val_acc": 48.0}
{"epoch": 6, "training_loss": 259249.0234375, "training_acc": 47.0, "val_loss": 82917.24853515625, "val_acc": 48.0}
{"epoch": 7, "training_loss": 281055.9716796875, "training_acc": 47.0, "val_loss": 60081.695556640625, "val_acc": 52.0}
{"epoch": 8, "training_loss": 234274.26171875, "training_acc": 53.0, "val_loss": 104178.86962890625, "val_acc": 52.0}
{"epoch": 9, "training_loss": 338284.25, "training_acc": 53.0, "val_loss": 13128.005981445312, "val_acc": 52.0}
{"epoch": 10, "training_loss": 135297.5751953125, "training_acc": 49.0, "val_loss": 96864.71557617188, "val_acc": 48.0}
{"epoch": 11, "training_loss": 371137.1025390625, "training_acc": 47.0, "val_loss": 20547.515869140625, "val_acc": 48.0}
{"epoch": 12, "training_loss": 134470.96875, "training_acc": 49.0, "val_loss": 101928.40576171875, "val_acc": 52.0}
{"epoch": 13, "training_loss": 391885.27734375, "training_acc": 53.0, "val_loss": 76412.06665039062, "val_acc": 52.0}
{"epoch": 14, "training_loss": 205129.91149902344, "training_acc": 52.0, "val_loss": 74932.99560546875, "val_acc": 48.0}
{"epoch": 15, "training_loss": 365072.2109375, "training_acc": 47.0, "val_loss": 111450.244140625, "val_acc": 48.0}
{"epoch": 16, "training_loss": 376016.09765625, "training_acc": 47.0, "val_loss": 10150.750732421875, "val_acc": 60.0}
{"epoch": 17, "training_loss": 93624.708984375, "training_acc": 54.0, "val_loss": 63729.656982421875, "val_acc": 52.0}
{"epoch": 18, "training_loss": 204897.87963867188, "training_acc": 53.0, "val_loss": 13673.626708984375, "val_acc": 48.0}
{"epoch": 19, "training_loss": 61898.23681640625, "training_acc": 51.0, "val_loss": 6447.812652587891, "val_acc": 56.0}
{"epoch": 20, "training_loss": 50293.73779296875, "training_acc": 58.0, "val_loss": 15950.503540039062, "val_acc": 56.0}
{"epoch": 21, "training_loss": 53017.559814453125, "training_acc": 52.0, "val_loss": 23407.16552734375, "val_acc": 48.0}
{"epoch": 22, "training_loss": 70426.29858398438, "training_acc": 52.0, "val_loss": 29655.120849609375, "val_acc": 52.0}
{"epoch": 23, "training_loss": 63462.12145996094, "training_acc": 57.0, "val_loss": 23075.328063964844, "val_acc": 48.0}
{"epoch": 24, "training_loss": 89473.91479492188, "training_acc": 48.0, "val_loss": 20274.549865722656, "val_acc": 56.0}
{"epoch": 25, "training_loss": 69437.05493164062, "training_acc": 56.0, "val_loss": 9694.800567626953, "val_acc": 56.0}
{"epoch": 26, "training_loss": 77346.36279296875, "training_acc": 53.0, "val_loss": 24504.17938232422, "val_acc": 48.0}
{"epoch": 27, "training_loss": 87146.46020507812, "training_acc": 51.0, "val_loss": 26911.334228515625, "val_acc": 52.0}
{"epoch": 28, "training_loss": 56587.55059814453, "training_acc": 56.0, "val_loss": 28708.14208984375, "val_acc": 48.0}
{"epoch": 29, "training_loss": 108787.203125, "training_acc": 49.0, "val_loss": 25766.268920898438, "val_acc": 52.0}
{"epoch": 30, "training_loss": 93324.220703125, "training_acc": 53.0, "val_loss": 17148.23760986328, "val_acc": 56.0}
{"epoch": 31, "training_loss": 77365.76318359375, "training_acc": 53.0, "val_loss": 30315.3076171875, "val_acc": 48.0}
{"epoch": 32, "training_loss": 90778.72540283203, "training_acc": 49.0, "val_loss": 17831.329345703125, "val_acc": 56.0}
{"epoch": 33, "training_loss": 39107.58752441406, "training_acc": 62.0, "val_loss": 11205.060577392578, "val_acc": 44.0}
{"epoch": 34, "training_loss": 63018.880126953125, "training_acc": 44.0, "val_loss": 8476.725769042969, "val_acc": 64.0}
{"epoch": 35, "training_loss": 45866.497802734375, "training_acc": 55.0, "val_loss": 4430.18684387207, "val_acc": 36.0}
{"epoch": 36, "training_loss": 51777.641357421875, "training_acc": 60.0, "val_loss": 28815.338134765625, "val_acc": 52.0}
{"epoch": 37, "training_loss": 61537.43395996094, "training_acc": 52.0, "val_loss": 4214.435577392578, "val_acc": 52.0}
{"epoch": 38, "training_loss": 37028.29541015625, "training_acc": 62.0, "val_loss": 13676.795959472656, "val_acc": 56.0}
{"epoch": 39, "training_loss": 48126.938720703125, "training_acc": 55.0, "val_loss": 4534.252166748047, "val_acc": 44.0}
{"epoch": 40, "training_loss": 53010.2841796875, "training_acc": 57.0, "val_loss": 40972.83935546875, "val_acc": 52.0}
{"epoch": 41, "training_loss": 106290.20227050781, "training_acc": 53.0, "val_loss": 46750.640869140625, "val_acc": 48.0}
{"epoch": 42, "training_loss": 202517.3564453125, "training_acc": 47.0, "val_loss": 34971.71936035156, "val_acc": 48.0}
{"epoch": 43, "training_loss": 144572.96533203125, "training_acc": 41.0, "val_loss": 49452.7099609375, "val_acc": 52.0}
{"epoch": 44, "training_loss": 173013.2578125, "training_acc": 53.0, "val_loss": 12414.173889160156, "val_acc": 52.0}
{"epoch": 45, "training_loss": 69091.1787109375, "training_acc": 50.0, "val_loss": 10309.204864501953, "val_acc": 52.0}
{"epoch": 46, "training_loss": 88938.21044921875, "training_acc": 45.0, "val_loss": 45672.08557128906, "val_acc": 52.0}
{"epoch": 47, "training_loss": 127768.62817382812, "training_acc": 53.0, "val_loss": 39524.68566894531, "val_acc": 48.0}
{"epoch": 48, "training_loss": 183835.89453125, "training_acc": 47.0, "val_loss": 30251.312255859375, "val_acc": 48.0}
{"epoch": 49, "training_loss": 91967.673828125, "training_acc": 54.0, "val_loss": 49705.889892578125, "val_acc": 52.0}
{"epoch": 50, "training_loss": 167067.47607421875, "training_acc": 53.0, "val_loss": 3911.870574951172, "val_acc": 44.0}
{"epoch": 51, "training_loss": 44777.58056640625, "training_acc": 59.0, "val_loss": 10386.991882324219, "val_acc": 52.0}
{"epoch": 52, "training_loss": 66509.91064453125, "training_acc": 49.0, "val_loss": 39226.74560546875, "val_acc": 52.0}
{"epoch": 53, "training_loss": 104520.37524414062, "training_acc": 54.0, "val_loss": 36944.866943359375, "val_acc": 48.0}
{"epoch": 54, "training_loss": 157794.94775390625, "training_acc": 47.0, "val_loss": 13064.682006835938, "val_acc": 48.0}
{"epoch": 55, "training_loss": 70319.14599609375, "training_acc": 56.0, "val_loss": 62017.56591796875, "val_acc": 52.0}
{"epoch": 56, "training_loss": 207229.3818359375, "training_acc": 53.0, "val_loss": 6217.569732666016, "val_acc": 64.0}
{"epoch": 57, "training_loss": 72147.7021484375, "training_acc": 64.0, "val_loss": 48711.71875, "val_acc": 48.0}
{"epoch": 58, "training_loss": 134856.51147460938, "training_acc": 47.0, "val_loss": 50331.671142578125, "val_acc": 52.0}
{"epoch": 59, "training_loss": 227223.5244140625, "training_acc": 53.0, "val_loss": 72810.57739257812, "val_acc": 52.0}
{"epoch": 60, "training_loss": 206637.03173828125, "training_acc": 53.0, "val_loss": 35972.67761230469, "val_acc": 48.0}
{"epoch": 61, "training_loss": 184260.048828125, "training_acc": 47.0, "val_loss": 56501.4892578125, "val_acc": 48.0}
{"epoch": 62, "training_loss": 152668.27111816406, "training_acc": 48.0, "val_loss": 46838.18359375, "val_acc": 52.0}
{"epoch": 63, "training_loss": 158147.9375, "training_acc": 53.0, "val_loss": 43509.82666015625, "val_acc": 52.0}
{"epoch": 64, "training_loss": 99415.19915771484, "training_acc": 56.0, "val_loss": 35453.179931640625, "val_acc": 48.0}
{"epoch": 65, "training_loss": 138970.271484375, "training_acc": 47.0, "val_loss": 11773.043823242188, "val_acc": 56.0}
{"epoch": 66, "training_loss": 47851.138916015625, "training_acc": 61.0, "val_loss": 20286.41815185547, "val_acc": 56.0}
{"epoch": 67, "training_loss": 45577.80871582031, "training_acc": 62.0, "val_loss": 22533.338928222656, "val_acc": 48.0}
{"epoch": 68, "training_loss": 72968.10858154297, "training_acc": 59.0, "val_loss": 30060.174560546875, "val_acc": 52.0}
{"epoch": 69, "training_loss": 68990.85595703125, "training_acc": 54.0, "val_loss": 19544.0185546875, "val_acc": 48.0}
