"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 557718.7755622864, "training_acc": 45.0, "val_loss": 89199.63989257812, "val_acc": 48.0}
{"epoch": 1, "training_loss": 531725.751953125, "training_acc": 45.0, "val_loss": 278271.3623046875, "val_acc": 52.0}
{"epoch": 2, "training_loss": 1039972.8359375, "training_acc": 53.0, "val_loss": 198655.77392578125, "val_acc": 52.0}
{"epoch": 3, "training_loss": 545971.361328125, "training_acc": 53.0, "val_loss": 98585.66284179688, "val_acc": 48.0}
{"epoch": 4, "training_loss": 527325.0078125, "training_acc": 47.0, "val_loss": 186948.6572265625, "val_acc": 48.0}
{"epoch": 5, "training_loss": 712962.806640625, "training_acc": 47.0, "val_loss": 50082.21740722656, "val_acc": 48.0}
{"epoch": 6, "training_loss": 214997.357421875, "training_acc": 50.0, "val_loss": 142781.2744140625, "val_acc": 52.0}
{"epoch": 7, "training_loss": 514159.341796875, "training_acc": 53.0, "val_loss": 141600.40283203125, "val_acc": 52.0}
{"epoch": 8, "training_loss": 410406.1728515625, "training_acc": 53.0, "val_loss": 12026.292419433594, "val_acc": 44.0}
{"epoch": 9, "training_loss": 117256.955078125, "training_acc": 52.0, "val_loss": 57144.012451171875, "val_acc": 48.0}
{"epoch": 10, "training_loss": 190124.06567382812, "training_acc": 47.0, "val_loss": 59985.2783203125, "val_acc": 52.0}
{"epoch": 11, "training_loss": 216501.5146484375, "training_acc": 53.0, "val_loss": 75539.07470703125, "val_acc": 52.0}
{"epoch": 12, "training_loss": 172237.4404296875, "training_acc": 57.0, "val_loss": 34360.546875, "val_acc": 48.0}
{"epoch": 13, "training_loss": 163943.0478515625, "training_acc": 47.0, "val_loss": 13413.706970214844, "val_acc": 40.0}
{"epoch": 14, "training_loss": 95308.35107421875, "training_acc": 54.0, "val_loss": 68405.95092773438, "val_acc": 52.0}
{"epoch": 15, "training_loss": 175310.95678710938, "training_acc": 53.0, "val_loss": 10886.531829833984, "val_acc": 40.0}
{"epoch": 16, "training_loss": 76230.18212890625, "training_acc": 59.0, "val_loss": 10441.754913330078, "val_acc": 36.0}
{"epoch": 17, "training_loss": 57477.06640625, "training_acc": 60.0, "val_loss": 40498.382568359375, "val_acc": 52.0}
{"epoch": 18, "training_loss": 70635.83026123047, "training_acc": 54.0, "val_loss": 23469.56787109375, "val_acc": 48.0}
{"epoch": 19, "training_loss": 101754.49230957031, "training_acc": 46.0, "val_loss": 33486.63330078125, "val_acc": 52.0}
{"epoch": 20, "training_loss": 91803.421875, "training_acc": 53.0, "val_loss": 4319.049835205078, "val_acc": 40.0}
{"epoch": 21, "training_loss": 45315.39990234375, "training_acc": 54.0, "val_loss": 4228.6865234375, "val_acc": 52.0}
{"epoch": 22, "training_loss": 51783.5849609375, "training_acc": 50.0, "val_loss": 2711.6527557373047, "val_acc": 68.0}
{"epoch": 23, "training_loss": 27346.651611328125, "training_acc": 57.0, "val_loss": 14152.838134765625, "val_acc": 52.0}
{"epoch": 24, "training_loss": 41089.82067871094, "training_acc": 59.0, "val_loss": 15101.634216308594, "val_acc": 48.0}
{"epoch": 25, "training_loss": 48857.62780761719, "training_acc": 55.0, "val_loss": 18645.73974609375, "val_acc": 52.0}
{"epoch": 26, "training_loss": 34733.02978515625, "training_acc": 58.0, "val_loss": 5053.124237060547, "val_acc": 44.0}
{"epoch": 27, "training_loss": 23784.129272460938, "training_acc": 54.0, "val_loss": 9972.328186035156, "val_acc": 52.0}
{"epoch": 28, "training_loss": 39832.76965332031, "training_acc": 54.0, "val_loss": 13132.090759277344, "val_acc": 52.0}
{"epoch": 29, "training_loss": 25287.031127929688, "training_acc": 61.0, "val_loss": 5184.151077270508, "val_acc": 52.0}
{"epoch": 30, "training_loss": 9242.113220214844, "training_acc": 76.0, "val_loss": 6723.796844482422, "val_acc": 48.0}
{"epoch": 31, "training_loss": 50640.768798828125, "training_acc": 48.0, "val_loss": 8032.756805419922, "val_acc": 52.0}
{"epoch": 32, "training_loss": 47445.90576171875, "training_acc": 62.0, "val_loss": 17327.029418945312, "val_acc": 48.0}
{"epoch": 33, "training_loss": 72078.12060546875, "training_acc": 53.0, "val_loss": 40501.318359375, "val_acc": 52.0}
{"epoch": 34, "training_loss": 94028.65814208984, "training_acc": 56.0, "val_loss": 41020.52307128906, "val_acc": 48.0}
{"epoch": 35, "training_loss": 183991.95361328125, "training_acc": 47.0, "val_loss": 12044.80209350586, "val_acc": 48.0}
{"epoch": 36, "training_loss": 84984.11865234375, "training_acc": 55.0, "val_loss": 86467.34008789062, "val_acc": 52.0}
{"epoch": 37, "training_loss": 287050.28125, "training_acc": 53.0, "val_loss": 37650.01525878906, "val_acc": 52.0}
{"epoch": 38, "training_loss": 86934.55981445312, "training_acc": 64.0, "val_loss": 59481.927490234375, "val_acc": 48.0}
{"epoch": 39, "training_loss": 228540.5517578125, "training_acc": 47.0, "val_loss": 9145.277404785156, "val_acc": 52.0}
{"epoch": 40, "training_loss": 59494.17822265625, "training_acc": 58.0, "val_loss": 29198.284912109375, "val_acc": 52.0}
{"epoch": 41, "training_loss": 72759.82397460938, "training_acc": 54.0, "val_loss": 15557.260131835938, "val_acc": 48.0}
