"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 1034.4366416931152, "training_acc": 48.0, "val_loss": 216.41991138458252, "val_acc": 52.0}
{"epoch": 1, "training_loss": 1192.804328918457, "training_acc": 45.0, "val_loss": 472.89180755615234, "val_acc": 48.0}
{"epoch": 2, "training_loss": 1694.9415245056152, "training_acc": 47.0, "val_loss": 98.60297441482544, "val_acc": 48.0}
{"epoch": 3, "training_loss": 764.789680480957, "training_acc": 38.0, "val_loss": 378.6623477935791, "val_acc": 52.0}
{"epoch": 4, "training_loss": 1533.5103645324707, "training_acc": 53.0, "val_loss": 306.52801990509033, "val_acc": 52.0}
{"epoch": 5, "training_loss": 1023.6648578643799, "training_acc": 53.0, "val_loss": 61.2653374671936, "val_acc": 48.0}
{"epoch": 6, "training_loss": 393.9918327331543, "training_acc": 47.0, "val_loss": 199.0212917327881, "val_acc": 48.0}
{"epoch": 7, "training_loss": 600.8796110153198, "training_acc": 49.0, "val_loss": 34.4446063041687, "val_acc": 52.0}
{"epoch": 8, "training_loss": 289.17945861816406, "training_acc": 53.0, "val_loss": 82.11469650268555, "val_acc": 52.0}
{"epoch": 9, "training_loss": 243.53627943992615, "training_acc": 53.0, "val_loss": 81.97826743125916, "val_acc": 48.0}
{"epoch": 10, "training_loss": 319.32124280929565, "training_acc": 45.0, "val_loss": 29.991137981414795, "val_acc": 52.0}
{"epoch": 11, "training_loss": 131.1144413948059, "training_acc": 54.0, "val_loss": 17.82260835170746, "val_acc": 44.0}
{"epoch": 12, "training_loss": 93.96276712417603, "training_acc": 57.0, "val_loss": 20.481324195861816, "val_acc": 44.0}
{"epoch": 13, "training_loss": 132.07321071624756, "training_acc": 51.0, "val_loss": 61.282879114151, "val_acc": 52.0}
{"epoch": 14, "training_loss": 145.31290912628174, "training_acc": 53.0, "val_loss": 34.97823178768158, "val_acc": 48.0}
{"epoch": 15, "training_loss": 117.93114829063416, "training_acc": 59.0, "val_loss": 71.05408310890198, "val_acc": 52.0}
{"epoch": 16, "training_loss": 158.3858551979065, "training_acc": 54.0, "val_loss": 35.66072881221771, "val_acc": 48.0}
{"epoch": 17, "training_loss": 166.81352996826172, "training_acc": 48.0, "val_loss": 56.21457099914551, "val_acc": 52.0}
{"epoch": 18, "training_loss": 120.2201075553894, "training_acc": 58.0, "val_loss": 23.781131207942963, "val_acc": 36.0}
{"epoch": 19, "training_loss": 108.58972406387329, "training_acc": 55.0, "val_loss": 36.978739500045776, "val_acc": 52.0}
{"epoch": 20, "training_loss": 98.20861959457397, "training_acc": 54.0, "val_loss": 22.3967045545578, "val_acc": 48.0}
{"epoch": 21, "training_loss": 95.30259251594543, "training_acc": 53.0, "val_loss": 43.05319786071777, "val_acc": 52.0}
{"epoch": 22, "training_loss": 119.09507179260254, "training_acc": 55.0, "val_loss": 36.05085015296936, "val_acc": 48.0}
{"epoch": 23, "training_loss": 158.66049337387085, "training_acc": 47.0, "val_loss": 56.65313005447388, "val_acc": 52.0}
{"epoch": 24, "training_loss": 198.4188642501831, "training_acc": 53.0, "val_loss": 17.742247879505157, "val_acc": 48.0}
{"epoch": 25, "training_loss": 114.13158416748047, "training_acc": 60.0, "val_loss": 20.407699048519135, "val_acc": 64.0}
{"epoch": 26, "training_loss": 141.29755401611328, "training_acc": 53.0, "val_loss": 71.21106386184692, "val_acc": 52.0}
{"epoch": 27, "training_loss": 159.8611180782318, "training_acc": 59.0, "val_loss": 40.162912011146545, "val_acc": 48.0}
{"epoch": 28, "training_loss": 129.63224339485168, "training_acc": 57.0, "val_loss": 73.22172522544861, "val_acc": 52.0}
{"epoch": 29, "training_loss": 210.07128620147705, "training_acc": 53.0, "val_loss": 26.20100975036621, "val_acc": 52.0}
{"epoch": 30, "training_loss": 123.06590223312378, "training_acc": 47.0, "val_loss": 41.64126217365265, "val_acc": 52.0}
{"epoch": 31, "training_loss": 94.71468806266785, "training_acc": 57.0, "val_loss": 20.120085775852203, "val_acc": 44.0}
{"epoch": 32, "training_loss": 78.98495554924011, "training_acc": 64.0, "val_loss": 37.19745576381683, "val_acc": 52.0}
{"epoch": 33, "training_loss": 82.76371765136719, "training_acc": 59.0, "val_loss": 20.100906491279602, "val_acc": 44.0}
{"epoch": 34, "training_loss": 65.78086996078491, "training_acc": 70.0, "val_loss": 49.04490411281586, "val_acc": 52.0}
{"epoch": 35, "training_loss": 95.714914560318, "training_acc": 55.0, "val_loss": 31.527477502822876, "val_acc": 52.0}
{"epoch": 36, "training_loss": 113.00796294212341, "training_acc": 54.0, "val_loss": 43.843480944633484, "val_acc": 52.0}
{"epoch": 37, "training_loss": 102.73535084724426, "training_acc": 52.0, "val_loss": 18.26760023832321, "val_acc": 52.0}
{"epoch": 38, "training_loss": 66.60915446281433, "training_acc": 66.0, "val_loss": 25.87733268737793, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.7196273803711, "training_acc": 58.0, "val_loss": 16.93432629108429, "val_acc": 52.0}
{"epoch": 40, "training_loss": 73.82021355628967, "training_acc": 63.0, "val_loss": 19.832879304885864, "val_acc": 56.0}
{"epoch": 41, "training_loss": 63.23238265514374, "training_acc": 67.0, "val_loss": 26.46450698375702, "val_acc": 52.0}
{"epoch": 42, "training_loss": 71.97170996665955, "training_acc": 60.0, "val_loss": 16.79846942424774, "val_acc": 48.0}
{"epoch": 43, "training_loss": 53.8706636428833, "training_acc": 75.0, "val_loss": 17.76067763566971, "val_acc": 52.0}
{"epoch": 44, "training_loss": 62.905577421188354, "training_acc": 64.0, "val_loss": 37.315770983695984, "val_acc": 52.0}
{"epoch": 45, "training_loss": 88.7710382938385, "training_acc": 57.0, "val_loss": 17.013129591941833, "val_acc": 52.0}
{"epoch": 46, "training_loss": 55.01098561286926, "training_acc": 74.0, "val_loss": 16.325338184833527, "val_acc": 52.0}
{"epoch": 47, "training_loss": 57.405190229415894, "training_acc": 71.0, "val_loss": 21.743284165859222, "val_acc": 52.0}
{"epoch": 48, "training_loss": 76.80248212814331, "training_acc": 61.0, "val_loss": 27.644872665405273, "val_acc": 52.0}
{"epoch": 49, "training_loss": 104.68563842773438, "training_acc": 47.0, "val_loss": 22.115108370780945, "val_acc": 52.0}
{"epoch": 50, "training_loss": 68.6593713760376, "training_acc": 61.0, "val_loss": 31.65087401866913, "val_acc": 48.0}
{"epoch": 51, "training_loss": 104.05723452568054, "training_acc": 51.0, "val_loss": 53.72985601425171, "val_acc": 52.0}
{"epoch": 52, "training_loss": 117.53554582595825, "training_acc": 58.0, "val_loss": 65.00006914138794, "val_acc": 48.0}
{"epoch": 53, "training_loss": 242.59244918823242, "training_acc": 47.0, "val_loss": 70.72528600692749, "val_acc": 52.0}
{"epoch": 54, "training_loss": 222.56859302520752, "training_acc": 53.0, "val_loss": 29.195299744606018, "val_acc": 52.0}
{"epoch": 55, "training_loss": 134.4224033355713, "training_acc": 58.0, "val_loss": 50.269895792007446, "val_acc": 48.0}
{"epoch": 56, "training_loss": 164.05038261413574, "training_acc": 53.0, "val_loss": 80.66306710243225, "val_acc": 52.0}
{"epoch": 57, "training_loss": 171.69227015972137, "training_acc": 62.0, "val_loss": 52.730172872543335, "val_acc": 48.0}
{"epoch": 58, "training_loss": 184.9265742301941, "training_acc": 48.0, "val_loss": 82.94549584388733, "val_acc": 52.0}
{"epoch": 59, "training_loss": 266.36126613616943, "training_acc": 53.0, "val_loss": 19.579800963401794, "val_acc": 52.0}
{"epoch": 60, "training_loss": 129.79656219482422, "training_acc": 68.0, "val_loss": 52.02588438987732, "val_acc": 48.0}
{"epoch": 61, "training_loss": 141.61453318595886, "training_acc": 63.0, "val_loss": 85.14440655708313, "val_acc": 52.0}
{"epoch": 62, "training_loss": 223.3566608428955, "training_acc": 53.0, "val_loss": 52.603745460510254, "val_acc": 48.0}
{"epoch": 63, "training_loss": 205.61827278137207, "training_acc": 48.0, "val_loss": 47.842901945114136, "val_acc": 52.0}
{"epoch": 64, "training_loss": 155.60486459732056, "training_acc": 56.0, "val_loss": 15.932095050811768, "val_acc": 48.0}
{"epoch": 65, "training_loss": 109.71726274490356, "training_acc": 61.0, "val_loss": 17.140643298625946, "val_acc": 48.0}
{"epoch": 66, "training_loss": 76.98269033432007, "training_acc": 73.0, "val_loss": 17.18451678752899, "val_acc": 48.0}
{"epoch": 67, "training_loss": 104.85089349746704, "training_acc": 61.0, "val_loss": 24.550265073776245, "val_acc": 52.0}
{"epoch": 68, "training_loss": 67.1240131855011, "training_acc": 67.0, "val_loss": 18.408969044685364, "val_acc": 48.0}
{"epoch": 69, "training_loss": 80.56079864501953, "training_acc": 64.0, "val_loss": 34.2839241027832, "val_acc": 52.0}
{"epoch": 70, "training_loss": 81.17277956008911, "training_acc": 67.0, "val_loss": 20.680975914001465, "val_acc": 68.0}
{"epoch": 71, "training_loss": 74.05922436714172, "training_acc": 63.0, "val_loss": 29.16196584701538, "val_acc": 52.0}
{"epoch": 72, "training_loss": 65.46401977539062, "training_acc": 70.0, "val_loss": 31.928259134292603, "val_acc": 48.0}
{"epoch": 73, "training_loss": 96.54812026023865, "training_acc": 62.0, "val_loss": 38.150009512901306, "val_acc": 52.0}
{"epoch": 74, "training_loss": 83.13148522377014, "training_acc": 63.0, "val_loss": 19.90758627653122, "val_acc": 72.0}
{"epoch": 75, "training_loss": 61.012813329696655, "training_acc": 63.0, "val_loss": 25.5901962518692, "val_acc": 52.0}
{"epoch": 76, "training_loss": 60.70735287666321, "training_acc": 65.0, "val_loss": 17.131923139095306, "val_acc": 56.0}
{"epoch": 77, "training_loss": 50.41647529602051, "training_acc": 78.0, "val_loss": 16.64573848247528, "val_acc": 48.0}
{"epoch": 78, "training_loss": 56.424641847610474, "training_acc": 74.0, "val_loss": 28.467729687690735, "val_acc": 52.0}
{"epoch": 79, "training_loss": 58.15590167045593, "training_acc": 66.0, "val_loss": 33.7335079908371, "val_acc": 48.0}
{"epoch": 80, "training_loss": 96.66278839111328, "training_acc": 59.0, "val_loss": 51.18109583854675, "val_acc": 52.0}
{"epoch": 81, "training_loss": 98.0044686794281, "training_acc": 63.0, "val_loss": 47.43764400482178, "val_acc": 48.0}
{"epoch": 82, "training_loss": 143.75962781906128, "training_acc": 50.0, "val_loss": 87.40441799163818, "val_acc": 52.0}
{"epoch": 83, "training_loss": 278.13318252563477, "training_acc": 53.0, "val_loss": 18.476372957229614, "val_acc": 52.0}
