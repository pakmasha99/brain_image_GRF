"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 1009.2448081970215, "training_acc": 46.0, "val_loss": 201.5385866165161, "val_acc": 52.0}
{"epoch": 1, "training_loss": 979.5798873901367, "training_acc": 53.0, "val_loss": 522.3979473114014, "val_acc": 48.0}
{"epoch": 2, "training_loss": 1965.68798828125, "training_acc": 47.0, "val_loss": 144.88887786865234, "val_acc": 48.0}
{"epoch": 3, "training_loss": 684.8273067474365, "training_acc": 51.0, "val_loss": 381.31017684936523, "val_acc": 52.0}
{"epoch": 4, "training_loss": 1449.3148345947266, "training_acc": 53.0, "val_loss": 340.2587413787842, "val_acc": 52.0}
{"epoch": 5, "training_loss": 1010.3287925720215, "training_acc": 53.0, "val_loss": 59.49374437332153, "val_acc": 48.0}
{"epoch": 6, "training_loss": 547.4773483276367, "training_acc": 47.0, "val_loss": 176.67123079299927, "val_acc": 48.0}
{"epoch": 7, "training_loss": 600.638994216919, "training_acc": 47.0, "val_loss": 122.31853008270264, "val_acc": 52.0}
{"epoch": 8, "training_loss": 477.1796054840088, "training_acc": 53.0, "val_loss": 211.95828914642334, "val_acc": 52.0}
{"epoch": 9, "training_loss": 682.8996028900146, "training_acc": 53.0, "val_loss": 23.500174283981323, "val_acc": 56.0}
{"epoch": 10, "training_loss": 258.735876083374, "training_acc": 49.0, "val_loss": 153.82452011108398, "val_acc": 48.0}
{"epoch": 11, "training_loss": 553.1416645050049, "training_acc": 47.0, "val_loss": 49.36304986476898, "val_acc": 52.0}
{"epoch": 12, "training_loss": 227.8931760787964, "training_acc": 55.0, "val_loss": 73.12314510345459, "val_acc": 52.0}
{"epoch": 13, "training_loss": 162.58322095870972, "training_acc": 62.0, "val_loss": 70.42547464370728, "val_acc": 48.0}
{"epoch": 14, "training_loss": 252.5647735595703, "training_acc": 47.0, "val_loss": 58.575111627578735, "val_acc": 52.0}
{"epoch": 15, "training_loss": 218.36203289031982, "training_acc": 54.0, "val_loss": 44.14944350719452, "val_acc": 52.0}
{"epoch": 16, "training_loss": 162.5262804031372, "training_acc": 50.0, "val_loss": 57.36393332481384, "val_acc": 48.0}
{"epoch": 17, "training_loss": 197.1259388923645, "training_acc": 52.0, "val_loss": 65.85429310798645, "val_acc": 52.0}
{"epoch": 18, "training_loss": 184.58786869049072, "training_acc": 51.0, "val_loss": 16.723470389842987, "val_acc": 60.0}
{"epoch": 19, "training_loss": 87.50435328483582, "training_acc": 63.0, "val_loss": 25.55241882801056, "val_acc": 56.0}
{"epoch": 20, "training_loss": 85.33934593200684, "training_acc": 62.0, "val_loss": 18.301159143447876, "val_acc": 60.0}
{"epoch": 21, "training_loss": 85.78910970687866, "training_acc": 55.0, "val_loss": 24.10631626844406, "val_acc": 56.0}
{"epoch": 22, "training_loss": 70.16110014915466, "training_acc": 59.0, "val_loss": 17.62373000383377, "val_acc": 56.0}
{"epoch": 23, "training_loss": 71.59358668327332, "training_acc": 62.0, "val_loss": 18.408067524433136, "val_acc": 60.0}
{"epoch": 24, "training_loss": 70.96535611152649, "training_acc": 65.0, "val_loss": 21.245931088924408, "val_acc": 60.0}
{"epoch": 25, "training_loss": 73.65306854248047, "training_acc": 58.0, "val_loss": 18.249423801898956, "val_acc": 64.0}
{"epoch": 26, "training_loss": 75.6028847694397, "training_acc": 56.0, "val_loss": 37.223684787750244, "val_acc": 52.0}
{"epoch": 27, "training_loss": 92.06552052497864, "training_acc": 52.0, "val_loss": 33.09145271778107, "val_acc": 48.0}
{"epoch": 28, "training_loss": 118.99723505973816, "training_acc": 51.0, "val_loss": 31.434524059295654, "val_acc": 52.0}
{"epoch": 29, "training_loss": 107.17293405532837, "training_acc": 42.0, "val_loss": 27.706816792488098, "val_acc": 52.0}
{"epoch": 30, "training_loss": 82.0036461353302, "training_acc": 55.0, "val_loss": 19.98710334300995, "val_acc": 40.0}
{"epoch": 31, "training_loss": 68.7437515258789, "training_acc": 58.0, "val_loss": 30.061930418014526, "val_acc": 52.0}
{"epoch": 32, "training_loss": 90.67250537872314, "training_acc": 55.0, "val_loss": 16.528083384037018, "val_acc": 68.0}
{"epoch": 33, "training_loss": 93.5606484413147, "training_acc": 53.0, "val_loss": 17.153768241405487, "val_acc": 60.0}
{"epoch": 34, "training_loss": 78.86445879936218, "training_acc": 49.0, "val_loss": 19.714972376823425, "val_acc": 56.0}
{"epoch": 35, "training_loss": 77.2828950881958, "training_acc": 51.0, "val_loss": 19.032084941864014, "val_acc": 52.0}
{"epoch": 36, "training_loss": 63.49319934844971, "training_acc": 63.0, "val_loss": 16.601070761680603, "val_acc": 60.0}
{"epoch": 37, "training_loss": 64.72466683387756, "training_acc": 61.0, "val_loss": 19.863632321357727, "val_acc": 48.0}
{"epoch": 38, "training_loss": 75.36944818496704, "training_acc": 51.0, "val_loss": 18.06475818157196, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.33086824417114, "training_acc": 63.0, "val_loss": 20.641429722309113, "val_acc": 52.0}
{"epoch": 40, "training_loss": 63.56416869163513, "training_acc": 66.0, "val_loss": 15.962974727153778, "val_acc": 60.0}
{"epoch": 41, "training_loss": 55.361674070358276, "training_acc": 72.0, "val_loss": 15.911135077476501, "val_acc": 56.0}
{"epoch": 42, "training_loss": 56.311652183532715, "training_acc": 75.0, "val_loss": 31.012624502182007, "val_acc": 52.0}
{"epoch": 43, "training_loss": 73.26260328292847, "training_acc": 64.0, "val_loss": 42.379555106163025, "val_acc": 48.0}
{"epoch": 44, "training_loss": 136.28824639320374, "training_acc": 46.0, "val_loss": 18.811173737049103, "val_acc": 52.0}
{"epoch": 45, "training_loss": 94.04964542388916, "training_acc": 57.0, "val_loss": 33.42836797237396, "val_acc": 52.0}
{"epoch": 46, "training_loss": 96.67454862594604, "training_acc": 53.0, "val_loss": 55.5323600769043, "val_acc": 48.0}
{"epoch": 47, "training_loss": 194.82555294036865, "training_acc": 47.0, "val_loss": 67.56170988082886, "val_acc": 52.0}
{"epoch": 48, "training_loss": 235.62972450256348, "training_acc": 53.0, "val_loss": 15.723706781864166, "val_acc": 68.0}
{"epoch": 49, "training_loss": 128.10790824890137, "training_acc": 61.0, "val_loss": 15.893377363681793, "val_acc": 64.0}
{"epoch": 50, "training_loss": 92.02399396896362, "training_acc": 71.0, "val_loss": 31.398773193359375, "val_acc": 52.0}
{"epoch": 51, "training_loss": 136.0595154762268, "training_acc": 52.0, "val_loss": 39.507755637168884, "val_acc": 48.0}
{"epoch": 52, "training_loss": 166.1884708404541, "training_acc": 47.0, "val_loss": 53.56869101524353, "val_acc": 52.0}
{"epoch": 53, "training_loss": 184.2574667930603, "training_acc": 49.0, "val_loss": 23.920781910419464, "val_acc": 48.0}
{"epoch": 54, "training_loss": 125.5341796875, "training_acc": 52.0, "val_loss": 37.32430934906006, "val_acc": 52.0}
{"epoch": 55, "training_loss": 137.7746958732605, "training_acc": 51.0, "val_loss": 22.524097561836243, "val_acc": 56.0}
{"epoch": 56, "training_loss": 101.70212984085083, "training_acc": 60.0, "val_loss": 64.41949605941772, "val_acc": 52.0}
{"epoch": 57, "training_loss": 160.23569345474243, "training_acc": 53.0, "val_loss": 27.190667390823364, "val_acc": 44.0}
{"epoch": 58, "training_loss": 100.60841250419617, "training_acc": 57.0, "val_loss": 39.74059522151947, "val_acc": 52.0}
{"epoch": 59, "training_loss": 96.98193001747131, "training_acc": 57.0, "val_loss": 18.838217854499817, "val_acc": 52.0}
{"epoch": 60, "training_loss": 121.53917121887207, "training_acc": 58.0, "val_loss": 20.779435336589813, "val_acc": 40.0}
{"epoch": 61, "training_loss": 84.53103995323181, "training_acc": 51.0, "val_loss": 36.382895708084106, "val_acc": 52.0}
{"epoch": 62, "training_loss": 102.99985456466675, "training_acc": 56.0, "val_loss": 50.174158811569214, "val_acc": 48.0}
{"epoch": 63, "training_loss": 162.4614667892456, "training_acc": 47.0, "val_loss": 86.24168038368225, "val_acc": 52.0}
{"epoch": 64, "training_loss": 310.8547067642212, "training_acc": 53.0, "val_loss": 49.3529736995697, "val_acc": 52.0}
{"epoch": 65, "training_loss": 209.08315658569336, "training_acc": 51.0, "val_loss": 95.07859349250793, "val_acc": 48.0}
{"epoch": 66, "training_loss": 290.1809096336365, "training_acc": 50.0, "val_loss": 122.38394021987915, "val_acc": 52.0}
{"epoch": 67, "training_loss": 479.505859375, "training_acc": 54.0, "val_loss": 118.72329711914062, "val_acc": 52.0}
