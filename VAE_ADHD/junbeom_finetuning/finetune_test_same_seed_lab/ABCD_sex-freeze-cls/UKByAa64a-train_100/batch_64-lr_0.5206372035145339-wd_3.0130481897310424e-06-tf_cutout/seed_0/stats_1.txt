"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 1137.8839988708496, "training_acc": 46.0, "val_loss": 228.42633724212646, "val_acc": 52.0}
{"epoch": 1, "training_loss": 1110.1479797363281, "training_acc": 53.0, "val_loss": 592.0345306396484, "val_acc": 48.0}
{"epoch": 2, "training_loss": 2227.7646255493164, "training_acc": 47.0, "val_loss": 164.1910195350647, "val_acc": 48.0}
{"epoch": 3, "training_loss": 776.1471214294434, "training_acc": 51.0, "val_loss": 432.1601867675781, "val_acc": 52.0}
{"epoch": 4, "training_loss": 1642.4977188110352, "training_acc": 53.0, "val_loss": 385.62371730804443, "val_acc": 52.0}
{"epoch": 5, "training_loss": 1144.9409980773926, "training_acc": 53.0, "val_loss": 67.27932691574097, "val_acc": 48.0}
{"epoch": 6, "training_loss": 619.2995338439941, "training_acc": 47.0, "val_loss": 199.09290075302124, "val_acc": 48.0}
{"epoch": 7, "training_loss": 675.7856798171997, "training_acc": 47.0, "val_loss": 140.5744194984436, "val_acc": 52.0}
{"epoch": 8, "training_loss": 548.2477340698242, "training_acc": 53.0, "val_loss": 243.22054386138916, "val_acc": 52.0}
{"epoch": 9, "training_loss": 785.8366680145264, "training_acc": 53.0, "val_loss": 27.972400188446045, "val_acc": 52.0}
{"epoch": 10, "training_loss": 302.3983554840088, "training_acc": 52.0, "val_loss": 190.62546491622925, "val_acc": 48.0}
{"epoch": 11, "training_loss": 701.8262767791748, "training_acc": 47.0, "val_loss": 28.887692093849182, "val_acc": 52.0}
{"epoch": 12, "training_loss": 185.41586112976074, "training_acc": 55.0, "val_loss": 89.25647735595703, "val_acc": 52.0}
{"epoch": 13, "training_loss": 196.62100768089294, "training_acc": 60.0, "val_loss": 100.23201704025269, "val_acc": 48.0}
{"epoch": 14, "training_loss": 422.3429927825928, "training_acc": 47.0, "val_loss": 19.130980968475342, "val_acc": 56.0}
{"epoch": 15, "training_loss": 177.0362424850464, "training_acc": 56.0, "val_loss": 148.60494136810303, "val_acc": 52.0}
{"epoch": 16, "training_loss": 461.80232524871826, "training_acc": 53.0, "val_loss": 20.690129697322845, "val_acc": 44.0}
{"epoch": 17, "training_loss": 182.19469928741455, "training_acc": 52.0, "val_loss": 26.205596327781677, "val_acc": 52.0}
{"epoch": 18, "training_loss": 117.78425979614258, "training_acc": 64.0, "val_loss": 113.45747709274292, "val_acc": 52.0}
{"epoch": 19, "training_loss": 300.74810791015625, "training_acc": 53.0, "val_loss": 32.13013410568237, "val_acc": 48.0}
{"epoch": 20, "training_loss": 211.11521911621094, "training_acc": 47.0, "val_loss": 17.56141036748886, "val_acc": 56.0}
{"epoch": 21, "training_loss": 183.44318866729736, "training_acc": 46.0, "val_loss": 85.42248606681824, "val_acc": 52.0}
{"epoch": 22, "training_loss": 211.50094509124756, "training_acc": 54.0, "val_loss": 37.050750851631165, "val_acc": 48.0}
{"epoch": 23, "training_loss": 157.0456564426422, "training_acc": 55.0, "val_loss": 56.934136152267456, "val_acc": 52.0}
{"epoch": 24, "training_loss": 143.23698616027832, "training_acc": 55.0, "val_loss": 24.126867949962616, "val_acc": 44.0}
{"epoch": 25, "training_loss": 113.94280862808228, "training_acc": 58.0, "val_loss": 44.832003116607666, "val_acc": 52.0}
{"epoch": 26, "training_loss": 148.10608291625977, "training_acc": 54.0, "val_loss": 24.402952194213867, "val_acc": 48.0}
{"epoch": 27, "training_loss": 92.93738102912903, "training_acc": 59.0, "val_loss": 25.14614760875702, "val_acc": 60.0}
{"epoch": 28, "training_loss": 81.2135169506073, "training_acc": 61.0, "val_loss": 27.098974585533142, "val_acc": 52.0}
{"epoch": 29, "training_loss": 92.47159314155579, "training_acc": 58.0, "val_loss": 66.88507199287415, "val_acc": 52.0}
{"epoch": 30, "training_loss": 199.92159843444824, "training_acc": 52.0, "val_loss": 42.04481840133667, "val_acc": 48.0}
{"epoch": 31, "training_loss": 175.9535608291626, "training_acc": 48.0, "val_loss": 62.43114471435547, "val_acc": 52.0}
{"epoch": 32, "training_loss": 204.02923011779785, "training_acc": 53.0, "val_loss": 17.788487672805786, "val_acc": 60.0}
{"epoch": 33, "training_loss": 104.17109155654907, "training_acc": 65.0, "val_loss": 18.400202691555023, "val_acc": 60.0}
{"epoch": 34, "training_loss": 160.00856971740723, "training_acc": 52.0, "val_loss": 27.422717213630676, "val_acc": 56.0}
{"epoch": 35, "training_loss": 146.3725175857544, "training_acc": 56.0, "val_loss": 32.32738375663757, "val_acc": 48.0}
{"epoch": 36, "training_loss": 189.43908405303955, "training_acc": 50.0, "val_loss": 85.01771092414856, "val_acc": 52.0}
{"epoch": 37, "training_loss": 188.80409574508667, "training_acc": 60.0, "val_loss": 58.64647030830383, "val_acc": 48.0}
{"epoch": 38, "training_loss": 210.876136302948, "training_acc": 47.0, "val_loss": 76.04387998580933, "val_acc": 52.0}
{"epoch": 39, "training_loss": 262.89098167419434, "training_acc": 53.0, "val_loss": 18.699829280376434, "val_acc": 60.0}
