"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.50476789474487, "training_acc": 53.0, "val_loss": 17.359910905361176, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.42361283302307, "training_acc": 53.0, "val_loss": 17.29246824979782, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.41640520095825, "training_acc": 52.0, "val_loss": 17.281170189380646, "val_acc": 52.0}
{"epoch": 3, "training_loss": 68.949387550354, "training_acc": 53.0, "val_loss": 17.266792058944702, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.24909734725952, "training_acc": 53.0, "val_loss": 17.285846173763275, "val_acc": 52.0}
{"epoch": 5, "training_loss": 68.9332754611969, "training_acc": 53.0, "val_loss": 17.26548671722412, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.29837727546692, "training_acc": 53.0, "val_loss": 17.256098985671997, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.01784873008728, "training_acc": 53.0, "val_loss": 17.23204255104065, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.09040760993958, "training_acc": 53.0, "val_loss": 17.239680886268616, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.00497579574585, "training_acc": 51.0, "val_loss": 17.254164814949036, "val_acc": 52.0}
{"epoch": 10, "training_loss": 68.93065619468689, "training_acc": 57.0, "val_loss": 17.266078293323517, "val_acc": 52.0}
{"epoch": 11, "training_loss": 68.94501566886902, "training_acc": 55.0, "val_loss": 17.29302555322647, "val_acc": 52.0}
{"epoch": 12, "training_loss": 68.6061282157898, "training_acc": 53.0, "val_loss": 17.379172146320343, "val_acc": 52.0}
{"epoch": 13, "training_loss": 68.7860004901886, "training_acc": 53.0, "val_loss": 17.473037540912628, "val_acc": 52.0}
{"epoch": 14, "training_loss": 68.88083982467651, "training_acc": 53.0, "val_loss": 17.470626533031464, "val_acc": 52.0}
{"epoch": 15, "training_loss": 68.64835667610168, "training_acc": 53.0, "val_loss": 17.439687252044678, "val_acc": 52.0}
{"epoch": 16, "training_loss": 68.70592832565308, "training_acc": 53.0, "val_loss": 17.417792975902557, "val_acc": 52.0}
{"epoch": 17, "training_loss": 68.50606536865234, "training_acc": 58.0, "val_loss": 17.449399828910828, "val_acc": 52.0}
{"epoch": 18, "training_loss": 68.52662205696106, "training_acc": 54.0, "val_loss": 17.52513498067856, "val_acc": 52.0}
{"epoch": 19, "training_loss": 68.52164554595947, "training_acc": 54.0, "val_loss": 17.635196447372437, "val_acc": 52.0}
{"epoch": 20, "training_loss": 68.27186250686646, "training_acc": 53.0, "val_loss": 17.744283378124237, "val_acc": 52.0}
{"epoch": 21, "training_loss": 68.71717572212219, "training_acc": 52.0, "val_loss": 17.786118388175964, "val_acc": 52.0}
{"epoch": 22, "training_loss": 67.40495228767395, "training_acc": 54.0, "val_loss": 17.76576191186905, "val_acc": 52.0}
{"epoch": 23, "training_loss": 68.59853792190552, "training_acc": 51.0, "val_loss": 17.732149362564087, "val_acc": 52.0}
{"epoch": 24, "training_loss": 68.48231863975525, "training_acc": 50.0, "val_loss": 17.701472342014313, "val_acc": 52.0}
{"epoch": 25, "training_loss": 67.26777577400208, "training_acc": 58.0, "val_loss": 17.649534344673157, "val_acc": 52.0}
{"epoch": 26, "training_loss": 67.45743894577026, "training_acc": 59.0, "val_loss": 17.64453798532486, "val_acc": 52.0}
