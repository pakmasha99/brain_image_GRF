"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 337337.3902130127, "training_acc": 54.0, "val_loss": 67486.0595703125, "val_acc": 52.0}
{"epoch": 1, "training_loss": 335205.6884765625, "training_acc": 41.0, "val_loss": 101659.44213867188, "val_acc": 48.0}
{"epoch": 2, "training_loss": 376937.357421875, "training_acc": 47.0, "val_loss": 16004.180908203125, "val_acc": 48.0}
{"epoch": 3, "training_loss": 163599.5439453125, "training_acc": 41.0, "val_loss": 100741.19873046875, "val_acc": 52.0}
{"epoch": 4, "training_loss": 395587.6552734375, "training_acc": 53.0, "val_loss": 83850.927734375, "val_acc": 52.0}
{"epoch": 5, "training_loss": 274756.7529296875, "training_acc": 53.0, "val_loss": 10054.012298583984, "val_acc": 48.0}
{"epoch": 6, "training_loss": 78824.37255859375, "training_acc": 47.0, "val_loss": 33304.901123046875, "val_acc": 48.0}
{"epoch": 7, "training_loss": 100958.39434814453, "training_acc": 47.0, "val_loss": 37145.58410644531, "val_acc": 52.0}
{"epoch": 8, "training_loss": 168286.78173828125, "training_acc": 53.0, "val_loss": 51720.03173828125, "val_acc": 52.0}
{"epoch": 9, "training_loss": 173992.1943359375, "training_acc": 53.0, "val_loss": 6636.802673339844, "val_acc": 48.0}
{"epoch": 10, "training_loss": 44044.5185546875, "training_acc": 47.0, "val_loss": 9008.33740234375, "val_acc": 48.0}
{"epoch": 11, "training_loss": 45379.808837890625, "training_acc": 53.0, "val_loss": 27027.377319335938, "val_acc": 52.0}
{"epoch": 12, "training_loss": 91895.06884765625, "training_acc": 53.0, "val_loss": 10274.370574951172, "val_acc": 48.0}
{"epoch": 13, "training_loss": 45875.25427246094, "training_acc": 47.0, "val_loss": 4015.1844024658203, "val_acc": 52.0}
{"epoch": 14, "training_loss": 13052.924987792969, "training_acc": 53.0, "val_loss": 19640.968322753906, "val_acc": 48.0}
{"epoch": 15, "training_loss": 78968.212890625, "training_acc": 47.0, "val_loss": 2317.5756454467773, "val_acc": 52.0}
{"epoch": 16, "training_loss": 10091.908203125, "training_acc": 53.0, "val_loss": 14044.757080078125, "val_acc": 48.0}
{"epoch": 17, "training_loss": 51415.13928222656, "training_acc": 47.0, "val_loss": 14123.722839355469, "val_acc": 52.0}
{"epoch": 18, "training_loss": 61776.138427734375, "training_acc": 53.0, "val_loss": 6355.5633544921875, "val_acc": 52.0}
{"epoch": 19, "training_loss": 66774.56103515625, "training_acc": 41.0, "val_loss": 30351.644897460938, "val_acc": 48.0}
{"epoch": 20, "training_loss": 99180.51245117188, "training_acc": 47.0, "val_loss": 18289.854431152344, "val_acc": 52.0}
{"epoch": 21, "training_loss": 91113.9375, "training_acc": 53.0, "val_loss": 30963.522338867188, "val_acc": 52.0}
{"epoch": 22, "training_loss": 93987.11547851562, "training_acc": 53.0, "val_loss": 21382.02362060547, "val_acc": 48.0}
{"epoch": 23, "training_loss": 111893.765625, "training_acc": 47.0, "val_loss": 26246.91162109375, "val_acc": 48.0}
{"epoch": 24, "training_loss": 74617.45791625977, "training_acc": 47.0, "val_loss": 9653.128051757812, "val_acc": 52.0}
{"epoch": 25, "training_loss": 29479.67105102539, "training_acc": 45.0, "val_loss": 8865.252685546875, "val_acc": 52.0}
{"epoch": 26, "training_loss": 25075.836807250977, "training_acc": 53.0, "val_loss": 21431.320190429688, "val_acc": 48.0}
{"epoch": 27, "training_loss": 89543.19165039062, "training_acc": 47.0, "val_loss": 3429.6646118164062, "val_acc": 48.0}
{"epoch": 28, "training_loss": 42452.48681640625, "training_acc": 55.0, "val_loss": 45712.51525878906, "val_acc": 52.0}
{"epoch": 29, "training_loss": 178091.0712890625, "training_acc": 53.0, "val_loss": 26567.7490234375, "val_acc": 52.0}
{"epoch": 30, "training_loss": 80017.71276855469, "training_acc": 49.0, "val_loss": 19365.71502685547, "val_acc": 48.0}
{"epoch": 31, "training_loss": 65810.02697753906, "training_acc": 47.0, "val_loss": 16102.764892578125, "val_acc": 52.0}
{"epoch": 32, "training_loss": 73918.01684570312, "training_acc": 53.0, "val_loss": 14804.827880859375, "val_acc": 52.0}
{"epoch": 33, "training_loss": 60855.862548828125, "training_acc": 45.0, "val_loss": 14644.757080078125, "val_acc": 48.0}
{"epoch": 34, "training_loss": 43775.72915649414, "training_acc": 47.0, "val_loss": 927.7538299560547, "val_acc": 52.0}
{"epoch": 35, "training_loss": 28917.81884765625, "training_acc": 51.0, "val_loss": 16464.76593017578, "val_acc": 48.0}
{"epoch": 36, "training_loss": 47170.61163330078, "training_acc": 51.0, "val_loss": 6344.723892211914, "val_acc": 52.0}
{"epoch": 37, "training_loss": 25791.026000976562, "training_acc": 53.0, "val_loss": 2442.953872680664, "val_acc": 48.0}
{"epoch": 38, "training_loss": 40672.327392578125, "training_acc": 45.0, "val_loss": 24785.74676513672, "val_acc": 52.0}
{"epoch": 39, "training_loss": 77743.26196289062, "training_acc": 53.0, "val_loss": 17756.504821777344, "val_acc": 48.0}
{"epoch": 40, "training_loss": 87131.24853515625, "training_acc": 47.0, "val_loss": 13519.808959960938, "val_acc": 48.0}
{"epoch": 41, "training_loss": 62140.61328125, "training_acc": 47.0, "val_loss": 27192.953491210938, "val_acc": 52.0}
{"epoch": 42, "training_loss": 95782.2470703125, "training_acc": 53.0, "val_loss": 3153.2299041748047, "val_acc": 48.0}
{"epoch": 43, "training_loss": 16201.01220703125, "training_acc": 47.0, "val_loss": 10911.444854736328, "val_acc": 52.0}
{"epoch": 44, "training_loss": 40626.63732910156, "training_acc": 53.0, "val_loss": 9002.757263183594, "val_acc": 48.0}
{"epoch": 45, "training_loss": 35004.64318847656, "training_acc": 47.0, "val_loss": 13327.590942382812, "val_acc": 52.0}
{"epoch": 46, "training_loss": 56416.422607421875, "training_acc": 53.0, "val_loss": 2133.811569213867, "val_acc": 52.0}
{"epoch": 47, "training_loss": 47604.8369140625, "training_acc": 51.0, "val_loss": 37829.412841796875, "val_acc": 48.0}
{"epoch": 48, "training_loss": 136675.923828125, "training_acc": 47.0, "val_loss": 4235.838317871094, "val_acc": 52.0}
{"epoch": 49, "training_loss": 31253.095458984375, "training_acc": 53.0, "val_loss": 6346.327972412109, "val_acc": 52.0}
{"epoch": 50, "training_loss": 39613.180908203125, "training_acc": 53.0, "val_loss": 21541.28875732422, "val_acc": 48.0}
{"epoch": 51, "training_loss": 63458.463928222656, "training_acc": 47.0, "val_loss": 27283.132934570312, "val_acc": 52.0}
{"epoch": 52, "training_loss": 132190.8291015625, "training_acc": 53.0, "val_loss": 35194.15588378906, "val_acc": 52.0}
{"epoch": 53, "training_loss": 103692.77270507812, "training_acc": 53.0, "val_loss": 25344.496154785156, "val_acc": 48.0}
