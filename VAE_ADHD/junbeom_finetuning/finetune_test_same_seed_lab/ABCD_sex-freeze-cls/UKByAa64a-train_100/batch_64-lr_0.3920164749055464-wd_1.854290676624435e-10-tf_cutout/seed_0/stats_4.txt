"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 539.5435104370117, "training_acc": 46.0, "val_loss": 160.08530855178833, "val_acc": 52.0}
{"epoch": 1, "training_loss": 704.0420379638672, "training_acc": 53.0, "val_loss": 63.05111050605774, "val_acc": 52.0}
{"epoch": 2, "training_loss": 360.17890548706055, "training_acc": 59.0, "val_loss": 268.72994899749756, "val_acc": 48.0}
{"epoch": 3, "training_loss": 927.3640079498291, "training_acc": 47.0, "val_loss": 23.01035374403, "val_acc": 64.0}
{"epoch": 4, "training_loss": 282.26372718811035, "training_acc": 50.0, "val_loss": 233.2207202911377, "val_acc": 52.0}
{"epoch": 5, "training_loss": 856.3363075256348, "training_acc": 53.0, "val_loss": 89.9809718132019, "val_acc": 52.0}
{"epoch": 6, "training_loss": 371.3462257385254, "training_acc": 52.0, "val_loss": 190.9406065940857, "val_acc": 48.0}
{"epoch": 7, "training_loss": 736.6987915039062, "training_acc": 48.0, "val_loss": 83.96874666213989, "val_acc": 48.0}
{"epoch": 8, "training_loss": 292.46623611450195, "training_acc": 48.0, "val_loss": 134.3473196029663, "val_acc": 52.0}
{"epoch": 9, "training_loss": 503.77780628204346, "training_acc": 53.0, "val_loss": 50.384289026260376, "val_acc": 52.0}
{"epoch": 10, "training_loss": 252.9559850692749, "training_acc": 47.0, "val_loss": 104.72642183303833, "val_acc": 48.0}
{"epoch": 11, "training_loss": 325.9893207550049, "training_acc": 47.0, "val_loss": 38.46682608127594, "val_acc": 52.0}
{"epoch": 12, "training_loss": 192.12106132507324, "training_acc": 52.0, "val_loss": 51.91546678543091, "val_acc": 52.0}
{"epoch": 13, "training_loss": 153.5522756576538, "training_acc": 59.0, "val_loss": 70.15631198883057, "val_acc": 48.0}
{"epoch": 14, "training_loss": 245.3585033416748, "training_acc": 50.0, "val_loss": 31.179383397102356, "val_acc": 48.0}
{"epoch": 15, "training_loss": 206.1243438720703, "training_acc": 52.0, "val_loss": 30.428031086921692, "val_acc": 48.0}
{"epoch": 16, "training_loss": 114.73698329925537, "training_acc": 55.0, "val_loss": 57.24274516105652, "val_acc": 48.0}
{"epoch": 17, "training_loss": 151.28924226760864, "training_acc": 60.0, "val_loss": 55.92932105064392, "val_acc": 52.0}
{"epoch": 18, "training_loss": 210.58730030059814, "training_acc": 53.0, "val_loss": 23.369209468364716, "val_acc": 56.0}
{"epoch": 19, "training_loss": 149.56549167633057, "training_acc": 47.0, "val_loss": 54.47246432304382, "val_acc": 48.0}
{"epoch": 20, "training_loss": 194.51205492019653, "training_acc": 51.0, "val_loss": 53.143250942230225, "val_acc": 52.0}
{"epoch": 21, "training_loss": 160.13364434242249, "training_acc": 57.0, "val_loss": 42.171308398246765, "val_acc": 52.0}
{"epoch": 22, "training_loss": 145.5353446006775, "training_acc": 52.0, "val_loss": 22.15481847524643, "val_acc": 56.0}
{"epoch": 23, "training_loss": 109.08434247970581, "training_acc": 62.0, "val_loss": 22.1428319811821, "val_acc": 56.0}
{"epoch": 24, "training_loss": 91.27025508880615, "training_acc": 61.0, "val_loss": 37.005144357681274, "val_acc": 48.0}
{"epoch": 25, "training_loss": 117.18944787979126, "training_acc": 55.0, "val_loss": 27.363571524620056, "val_acc": 52.0}
{"epoch": 26, "training_loss": 96.48740768432617, "training_acc": 58.0, "val_loss": 24.87274706363678, "val_acc": 40.0}
{"epoch": 27, "training_loss": 86.10849785804749, "training_acc": 59.0, "val_loss": 20.352010428905487, "val_acc": 56.0}
{"epoch": 28, "training_loss": 89.11153435707092, "training_acc": 63.0, "val_loss": 18.026195466518402, "val_acc": 56.0}
{"epoch": 29, "training_loss": 84.38222551345825, "training_acc": 54.0, "val_loss": 18.173205852508545, "val_acc": 56.0}
{"epoch": 30, "training_loss": 69.40505290031433, "training_acc": 71.0, "val_loss": 21.61596119403839, "val_acc": 52.0}
{"epoch": 31, "training_loss": 78.99084401130676, "training_acc": 62.0, "val_loss": 16.89203977584839, "val_acc": 64.0}
{"epoch": 32, "training_loss": 63.81652808189392, "training_acc": 62.0, "val_loss": 22.206030786037445, "val_acc": 52.0}
{"epoch": 33, "training_loss": 78.13432621955872, "training_acc": 59.0, "val_loss": 34.073248505592346, "val_acc": 48.0}
{"epoch": 34, "training_loss": 111.53421831130981, "training_acc": 49.0, "val_loss": 45.177677273750305, "val_acc": 52.0}
{"epoch": 35, "training_loss": 150.19371223449707, "training_acc": 53.0, "val_loss": 31.93410634994507, "val_acc": 48.0}
{"epoch": 36, "training_loss": 127.87510919570923, "training_acc": 47.0, "val_loss": 39.520302414894104, "val_acc": 52.0}
{"epoch": 37, "training_loss": 131.30265855789185, "training_acc": 53.0, "val_loss": 18.430134654045105, "val_acc": 60.0}
{"epoch": 38, "training_loss": 86.26165246963501, "training_acc": 57.0, "val_loss": 18.814651668071747, "val_acc": 52.0}
{"epoch": 39, "training_loss": 75.93195104598999, "training_acc": 65.0, "val_loss": 17.794619500637054, "val_acc": 48.0}
{"epoch": 40, "training_loss": 70.56157350540161, "training_acc": 54.0, "val_loss": 23.634177446365356, "val_acc": 52.0}
{"epoch": 41, "training_loss": 66.74724912643433, "training_acc": 62.0, "val_loss": 18.132907152175903, "val_acc": 48.0}
{"epoch": 42, "training_loss": 59.327507734298706, "training_acc": 62.0, "val_loss": 20.311617851257324, "val_acc": 52.0}
{"epoch": 43, "training_loss": 57.139872312545776, "training_acc": 73.0, "val_loss": 22.347623109817505, "val_acc": 44.0}
{"epoch": 44, "training_loss": 68.04186618328094, "training_acc": 62.0, "val_loss": 26.20788812637329, "val_acc": 52.0}
{"epoch": 45, "training_loss": 66.30490851402283, "training_acc": 61.0, "val_loss": 36.976078152656555, "val_acc": 48.0}
{"epoch": 46, "training_loss": 103.02862119674683, "training_acc": 56.0, "val_loss": 47.3993718624115, "val_acc": 52.0}
{"epoch": 47, "training_loss": 134.72016859054565, "training_acc": 53.0, "val_loss": 40.682998299598694, "val_acc": 48.0}
{"epoch": 48, "training_loss": 141.2466492652893, "training_acc": 46.0, "val_loss": 44.87437605857849, "val_acc": 52.0}
{"epoch": 49, "training_loss": 148.7422046661377, "training_acc": 53.0, "val_loss": 27.921298146247864, "val_acc": 48.0}
{"epoch": 50, "training_loss": 96.55875492095947, "training_acc": 49.0, "val_loss": 36.17214858531952, "val_acc": 52.0}
