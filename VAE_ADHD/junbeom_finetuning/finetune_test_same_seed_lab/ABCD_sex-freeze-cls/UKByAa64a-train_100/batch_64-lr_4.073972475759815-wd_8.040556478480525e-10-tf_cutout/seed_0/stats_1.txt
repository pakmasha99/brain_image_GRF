"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 8600.785388946533, "training_acc": 46.0, "val_loss": 1788.3649826049805, "val_acc": 52.0}
{"epoch": 1, "training_loss": 8684.981994628906, "training_acc": 53.0, "val_loss": 4631.883239746094, "val_acc": 48.0}
{"epoch": 2, "training_loss": 17431.77862548828, "training_acc": 47.0, "val_loss": 1284.0314865112305, "val_acc": 48.0}
{"epoch": 3, "training_loss": 6074.039337158203, "training_acc": 51.0, "val_loss": 3382.5096130371094, "val_acc": 52.0}
{"epoch": 4, "training_loss": 12851.697021484375, "training_acc": 53.0, "val_loss": 3018.301773071289, "val_acc": 52.0}
{"epoch": 5, "training_loss": 8958.49594116211, "training_acc": 53.0, "val_loss": 524.5586395263672, "val_acc": 48.0}
{"epoch": 6, "training_loss": 4833.897552490234, "training_acc": 47.0, "val_loss": 1546.130084991455, "val_acc": 48.0}
{"epoch": 7, "training_loss": 5240.274322509766, "training_acc": 47.0, "val_loss": 1120.1107025146484, "val_acc": 52.0}
{"epoch": 8, "training_loss": 4368.113815307617, "training_acc": 53.0, "val_loss": 1942.3168182373047, "val_acc": 52.0}
{"epoch": 9, "training_loss": 6307.456359863281, "training_acc": 53.0, "val_loss": 244.9277400970459, "val_acc": 52.0}
{"epoch": 10, "training_loss": 2554.6839752197266, "training_acc": 49.0, "val_loss": 1841.0951614379883, "val_acc": 48.0}
{"epoch": 11, "training_loss": 7080.206069946289, "training_acc": 47.0, "val_loss": 447.8111267089844, "val_acc": 48.0}
{"epoch": 12, "training_loss": 2572.1336212158203, "training_acc": 49.0, "val_loss": 1819.6929931640625, "val_acc": 52.0}
{"epoch": 13, "training_loss": 6969.642822265625, "training_acc": 53.0, "val_loss": 1327.6775360107422, "val_acc": 52.0}
{"epoch": 14, "training_loss": 3477.855405807495, "training_acc": 54.0, "val_loss": 1287.387466430664, "val_acc": 48.0}
{"epoch": 15, "training_loss": 6074.939605712891, "training_acc": 47.0, "val_loss": 1577.7968406677246, "val_acc": 48.0}
{"epoch": 16, "training_loss": 4842.299293518066, "training_acc": 47.0, "val_loss": 932.9630851745605, "val_acc": 52.0}
{"epoch": 17, "training_loss": 4312.623489379883, "training_acc": 53.0, "val_loss": 1745.1406478881836, "val_acc": 52.0}
{"epoch": 18, "training_loss": 5867.671630859375, "training_acc": 53.0, "val_loss": 174.51090812683105, "val_acc": 60.0}
{"epoch": 19, "training_loss": 1702.0621337890625, "training_acc": 59.0, "val_loss": 1663.132667541504, "val_acc": 48.0}
{"epoch": 20, "training_loss": 6283.491058349609, "training_acc": 47.0, "val_loss": 466.1007881164551, "val_acc": 48.0}
{"epoch": 21, "training_loss": 2486.810089111328, "training_acc": 47.0, "val_loss": 1572.264575958252, "val_acc": 52.0}
{"epoch": 22, "training_loss": 5326.6875, "training_acc": 53.0, "val_loss": 904.2416572570801, "val_acc": 52.0}
{"epoch": 23, "training_loss": 2047.8018646240234, "training_acc": 56.0, "val_loss": 739.2683029174805, "val_acc": 48.0}
{"epoch": 24, "training_loss": 2725.4632568359375, "training_acc": 48.0, "val_loss": 406.6017150878906, "val_acc": 56.0}
{"epoch": 25, "training_loss": 1432.8203353881836, "training_acc": 57.0, "val_loss": 489.4455909729004, "val_acc": 52.0}
{"epoch": 26, "training_loss": 1777.5492095947266, "training_acc": 44.0, "val_loss": 387.7431631088257, "val_acc": 48.0}
{"epoch": 27, "training_loss": 1482.742841720581, "training_acc": 50.0, "val_loss": 394.07286643981934, "val_acc": 56.0}
{"epoch": 28, "training_loss": 683.3043193817139, "training_acc": 59.0, "val_loss": 238.0141019821167, "val_acc": 44.0}
{"epoch": 29, "training_loss": 827.0731868743896, "training_acc": 60.0, "val_loss": 702.388334274292, "val_acc": 52.0}
{"epoch": 30, "training_loss": 2314.332290649414, "training_acc": 53.0, "val_loss": 196.06472253799438, "val_acc": 64.0}
{"epoch": 31, "training_loss": 1416.2539367675781, "training_acc": 55.0, "val_loss": 625.703763961792, "val_acc": 48.0}
{"epoch": 32, "training_loss": 1810.3036527633667, "training_acc": 52.0, "val_loss": 414.4404888153076, "val_acc": 52.0}
{"epoch": 33, "training_loss": 1084.2213172912598, "training_acc": 60.0, "val_loss": 395.93756198883057, "val_acc": 48.0}
{"epoch": 34, "training_loss": 1557.3531703948975, "training_acc": 53.0, "val_loss": 406.18739128112793, "val_acc": 52.0}
{"epoch": 35, "training_loss": 1248.1339988708496, "training_acc": 55.0, "val_loss": 136.4119529724121, "val_acc": 44.0}
{"epoch": 36, "training_loss": 675.913743019104, "training_acc": 58.0, "val_loss": 256.92667961120605, "val_acc": 56.0}
{"epoch": 37, "training_loss": 448.04335021972656, "training_acc": 62.0, "val_loss": 77.73754000663757, "val_acc": 44.0}
{"epoch": 38, "training_loss": 469.3243751525879, "training_acc": 61.0, "val_loss": 112.85195350646973, "val_acc": 64.0}
{"epoch": 39, "training_loss": 379.38201332092285, "training_acc": 63.0, "val_loss": 102.17669010162354, "val_acc": 64.0}
{"epoch": 40, "training_loss": 244.684832572937, "training_acc": 63.0, "val_loss": 52.19937562942505, "val_acc": 52.0}
{"epoch": 41, "training_loss": 233.3953447341919, "training_acc": 70.0, "val_loss": 121.18661403656006, "val_acc": 52.0}
{"epoch": 42, "training_loss": 378.5474433898926, "training_acc": 60.0, "val_loss": 45.38489282131195, "val_acc": 48.0}
{"epoch": 43, "training_loss": 360.0168876647949, "training_acc": 60.0, "val_loss": 179.65199947357178, "val_acc": 48.0}
{"epoch": 44, "training_loss": 718.4873447418213, "training_acc": 45.0, "val_loss": 75.66779851913452, "val_acc": 52.0}
{"epoch": 45, "training_loss": 311.85389614105225, "training_acc": 60.0, "val_loss": 138.50191831588745, "val_acc": 56.0}
{"epoch": 46, "training_loss": 340.59946060180664, "training_acc": 66.0, "val_loss": 68.9037561416626, "val_acc": 64.0}
{"epoch": 47, "training_loss": 203.75569343566895, "training_acc": 72.0, "val_loss": 119.3640947341919, "val_acc": 48.0}
{"epoch": 48, "training_loss": 474.3598155975342, "training_acc": 58.0, "val_loss": 87.28871941566467, "val_acc": 64.0}
{"epoch": 49, "training_loss": 797.4345169067383, "training_acc": 58.0, "val_loss": 42.27844178676605, "val_acc": 56.0}
{"epoch": 50, "training_loss": 554.4306030273438, "training_acc": 70.0, "val_loss": 316.1010980606079, "val_acc": 52.0}
{"epoch": 51, "training_loss": 1195.95845413208, "training_acc": 51.0, "val_loss": 363.9690399169922, "val_acc": 48.0}
{"epoch": 52, "training_loss": 1380.926498413086, "training_acc": 49.0, "val_loss": 480.2734851837158, "val_acc": 52.0}
{"epoch": 53, "training_loss": 1357.0977249145508, "training_acc": 51.0, "val_loss": 51.187729835510254, "val_acc": 48.0}
{"epoch": 54, "training_loss": 350.4131507873535, "training_acc": 59.0, "val_loss": 228.9961576461792, "val_acc": 48.0}
{"epoch": 55, "training_loss": 517.2408847808838, "training_acc": 60.0, "val_loss": 201.69799327850342, "val_acc": 56.0}
{"epoch": 56, "training_loss": 735.1688652038574, "training_acc": 46.0, "val_loss": 207.3124647140503, "val_acc": 56.0}
{"epoch": 57, "training_loss": 424.09330654144287, "training_acc": 62.0, "val_loss": 98.98505806922913, "val_acc": 48.0}
{"epoch": 58, "training_loss": 426.0142002105713, "training_acc": 62.0, "val_loss": 52.69845724105835, "val_acc": 52.0}
{"epoch": 59, "training_loss": 441.59894943237305, "training_acc": 72.0, "val_loss": 190.80851078033447, "val_acc": 56.0}
{"epoch": 60, "training_loss": 422.24636459350586, "training_acc": 55.0, "val_loss": 578.977108001709, "val_acc": 48.0}
{"epoch": 61, "training_loss": 2060.491050720215, "training_acc": 47.0, "val_loss": 432.80816078186035, "val_acc": 52.0}
{"epoch": 62, "training_loss": 1620.7231521606445, "training_acc": 53.0, "val_loss": 78.59652638435364, "val_acc": 64.0}
{"epoch": 63, "training_loss": 1160.7988815307617, "training_acc": 62.0, "val_loss": 152.06252336502075, "val_acc": 48.0}
{"epoch": 64, "training_loss": 1532.9106903076172, "training_acc": 51.0, "val_loss": 1139.3708229064941, "val_acc": 52.0}
{"epoch": 65, "training_loss": 3317.3703536987305, "training_acc": 53.0, "val_loss": 252.57787704467773, "val_acc": 48.0}
{"epoch": 66, "training_loss": 1713.348533630371, "training_acc": 47.0, "val_loss": 134.0200901031494, "val_acc": 60.0}
{"epoch": 67, "training_loss": 682.7976875305176, "training_acc": 64.0, "val_loss": 57.10523724555969, "val_acc": 68.0}
{"epoch": 68, "training_loss": 473.1898994445801, "training_acc": 59.0, "val_loss": 264.485502243042, "val_acc": 52.0}
