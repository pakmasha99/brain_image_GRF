"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 1966.0547103881836, "training_acc": 49.0, "val_loss": 3156.8344116210938, "val_acc": 52.0}
{"epoch": 1, "training_loss": 12189.846618652344, "training_acc": 53.0, "val_loss": 1345.150089263916, "val_acc": 52.0}
{"epoch": 2, "training_loss": 4650.178955078125, "training_acc": 57.0, "val_loss": 2255.8944702148438, "val_acc": 48.0}
{"epoch": 3, "training_loss": 7158.543273925781, "training_acc": 47.0, "val_loss": 581.5369606018066, "val_acc": 52.0}
{"epoch": 4, "training_loss": 3704.77880859375, "training_acc": 53.0, "val_loss": 591.7147159576416, "val_acc": 52.0}
{"epoch": 5, "training_loss": 1989.1747817993164, "training_acc": 60.0, "val_loss": 1253.5703659057617, "val_acc": 48.0}
{"epoch": 6, "training_loss": 3523.585609436035, "training_acc": 50.0, "val_loss": 500.1795768737793, "val_acc": 52.0}
{"epoch": 7, "training_loss": 2854.5, "training_acc": 54.0, "val_loss": 448.3027458190918, "val_acc": 52.0}
{"epoch": 8, "training_loss": 1166.628101348877, "training_acc": 64.0, "val_loss": 1010.524845123291, "val_acc": 48.0}
{"epoch": 9, "training_loss": 2874.4195556640625, "training_acc": 50.0, "val_loss": 407.7939987182617, "val_acc": 52.0}
{"epoch": 10, "training_loss": 1928.5170593261719, "training_acc": 56.0, "val_loss": 144.11906003952026, "val_acc": 56.0}
{"epoch": 11, "training_loss": 1294.1625289916992, "training_acc": 57.0, "val_loss": 725.1806259155273, "val_acc": 48.0}
{"epoch": 12, "training_loss": 1432.9274997711182, "training_acc": 54.0, "val_loss": 882.5234413146973, "val_acc": 52.0}
{"epoch": 13, "training_loss": 3320.5639877319336, "training_acc": 53.0, "val_loss": 429.93006706237793, "val_acc": 52.0}
{"epoch": 14, "training_loss": 1567.588623046875, "training_acc": 52.0, "val_loss": 990.144157409668, "val_acc": 48.0}
{"epoch": 15, "training_loss": 2848.3003425598145, "training_acc": 48.0, "val_loss": 571.4623928070068, "val_acc": 52.0}
{"epoch": 16, "training_loss": 2423.4227905273438, "training_acc": 54.0, "val_loss": 342.852258682251, "val_acc": 52.0}
{"epoch": 17, "training_loss": 1242.266944885254, "training_acc": 60.0, "val_loss": 872.5564002990723, "val_acc": 48.0}
{"epoch": 18, "training_loss": 2518.555866241455, "training_acc": 49.0, "val_loss": 404.1867733001709, "val_acc": 52.0}
{"epoch": 19, "training_loss": 1375.3928260803223, "training_acc": 55.0, "val_loss": 417.86303520202637, "val_acc": 48.0}
{"epoch": 20, "training_loss": 1480.715591430664, "training_acc": 49.0, "val_loss": 310.182785987854, "val_acc": 56.0}
{"epoch": 21, "training_loss": 1497.6970901489258, "training_acc": 55.0, "val_loss": 108.52152109146118, "val_acc": 52.0}
{"epoch": 22, "training_loss": 1242.8621520996094, "training_acc": 54.0, "val_loss": 480.3590774536133, "val_acc": 48.0}
{"epoch": 23, "training_loss": 1898.8100814819336, "training_acc": 43.0, "val_loss": 693.9727783203125, "val_acc": 52.0}
{"epoch": 24, "training_loss": 1992.3477115631104, "training_acc": 53.0, "val_loss": 679.6960353851318, "val_acc": 48.0}
{"epoch": 25, "training_loss": 2488.7488174438477, "training_acc": 47.0, "val_loss": 119.96285915374756, "val_acc": 56.0}
{"epoch": 26, "training_loss": 848.0116729736328, "training_acc": 60.0, "val_loss": 309.9902868270874, "val_acc": 56.0}
{"epoch": 27, "training_loss": 875.1387939453125, "training_acc": 60.0, "val_loss": 474.4528293609619, "val_acc": 48.0}
{"epoch": 28, "training_loss": 1410.7104988098145, "training_acc": 47.0, "val_loss": 272.1580982208252, "val_acc": 56.0}
{"epoch": 29, "training_loss": 819.2270793914795, "training_acc": 62.0, "val_loss": 323.56393337249756, "val_acc": 48.0}
{"epoch": 30, "training_loss": 935.3571853637695, "training_acc": 51.0, "val_loss": 177.77442932128906, "val_acc": 60.0}
{"epoch": 31, "training_loss": 421.97848320007324, "training_acc": 68.0, "val_loss": 218.1673765182495, "val_acc": 44.0}
{"epoch": 32, "training_loss": 889.5955696105957, "training_acc": 51.0, "val_loss": 76.57735347747803, "val_acc": 56.0}
{"epoch": 33, "training_loss": 652.2665901184082, "training_acc": 60.0, "val_loss": 216.56417846679688, "val_acc": 56.0}
{"epoch": 34, "training_loss": 499.3812861442566, "training_acc": 60.0, "val_loss": 97.73440361022949, "val_acc": 44.0}
{"epoch": 35, "training_loss": 342.0648012161255, "training_acc": 71.0, "val_loss": 101.17743015289307, "val_acc": 56.0}
{"epoch": 36, "training_loss": 302.42843532562256, "training_acc": 64.0, "val_loss": 62.37879991531372, "val_acc": 52.0}
{"epoch": 37, "training_loss": 197.652503490448, "training_acc": 75.0, "val_loss": 45.14977037906647, "val_acc": 60.0}
{"epoch": 38, "training_loss": 200.55199146270752, "training_acc": 64.0, "val_loss": 157.17294216156006, "val_acc": 56.0}
{"epoch": 39, "training_loss": 788.829273223877, "training_acc": 50.0, "val_loss": 123.61468076705933, "val_acc": 56.0}
{"epoch": 40, "training_loss": 253.15559482574463, "training_acc": 64.0, "val_loss": 388.23137283325195, "val_acc": 48.0}
{"epoch": 41, "training_loss": 997.539059638977, "training_acc": 55.0, "val_loss": 514.2493724822998, "val_acc": 52.0}
{"epoch": 42, "training_loss": 1490.4327774047852, "training_acc": 54.0, "val_loss": 556.6794872283936, "val_acc": 48.0}
{"epoch": 43, "training_loss": 2191.3812103271484, "training_acc": 47.0, "val_loss": 273.0194091796875, "val_acc": 52.0}
{"epoch": 44, "training_loss": 800.0789127349854, "training_acc": 55.0, "val_loss": 223.66340160369873, "val_acc": 48.0}
{"epoch": 45, "training_loss": 626.3209524154663, "training_acc": 57.0, "val_loss": 78.73442769050598, "val_acc": 56.0}
{"epoch": 46, "training_loss": 263.7856636047363, "training_acc": 72.0, "val_loss": 203.8517951965332, "val_acc": 52.0}
{"epoch": 47, "training_loss": 447.1518898010254, "training_acc": 58.0, "val_loss": 166.06560945510864, "val_acc": 52.0}
{"epoch": 48, "training_loss": 230.08232975006104, "training_acc": 68.0, "val_loss": 45.86502015590668, "val_acc": 48.0}
{"epoch": 49, "training_loss": 112.73436450958252, "training_acc": 77.0, "val_loss": 38.31717073917389, "val_acc": 64.0}
{"epoch": 50, "training_loss": 146.37787532806396, "training_acc": 72.0, "val_loss": 185.7887625694275, "val_acc": 52.0}
{"epoch": 51, "training_loss": 668.5145606994629, "training_acc": 54.0, "val_loss": 101.0136365890503, "val_acc": 56.0}
{"epoch": 52, "training_loss": 161.19591450691223, "training_acc": 72.0, "val_loss": 43.83838474750519, "val_acc": 64.0}
{"epoch": 53, "training_loss": 89.28274273872375, "training_acc": 81.0, "val_loss": 47.021764516830444, "val_acc": 60.0}
{"epoch": 54, "training_loss": 281.04494285583496, "training_acc": 59.0, "val_loss": 455.1309585571289, "val_acc": 52.0}
{"epoch": 55, "training_loss": 1375.5465850830078, "training_acc": 53.0, "val_loss": 522.1655368804932, "val_acc": 48.0}
{"epoch": 56, "training_loss": 1731.8784980773926, "training_acc": 47.0, "val_loss": 405.249547958374, "val_acc": 52.0}
{"epoch": 57, "training_loss": 1419.482521057129, "training_acc": 53.0, "val_loss": 318.7777042388916, "val_acc": 48.0}
{"epoch": 58, "training_loss": 872.0040302276611, "training_acc": 49.0, "val_loss": 532.5236320495605, "val_acc": 52.0}
{"epoch": 59, "training_loss": 1698.1368408203125, "training_acc": 53.0, "val_loss": 123.4650731086731, "val_acc": 48.0}
{"epoch": 60, "training_loss": 602.3557929992676, "training_acc": 59.0, "val_loss": 244.91825103759766, "val_acc": 52.0}
{"epoch": 61, "training_loss": 599.2147836685181, "training_acc": 62.0, "val_loss": 314.87345695495605, "val_acc": 48.0}
{"epoch": 62, "training_loss": 634.833001613617, "training_acc": 63.0, "val_loss": 292.1250581741333, "val_acc": 52.0}
{"epoch": 63, "training_loss": 620.7415618896484, "training_acc": 62.0, "val_loss": 341.86699390411377, "val_acc": 48.0}
{"epoch": 64, "training_loss": 844.8478746414185, "training_acc": 57.0, "val_loss": 137.00847625732422, "val_acc": 56.0}
{"epoch": 65, "training_loss": 943.793529510498, "training_acc": 51.0, "val_loss": 69.13793087005615, "val_acc": 64.0}
{"epoch": 66, "training_loss": 439.62964630126953, "training_acc": 69.0, "val_loss": 108.33638906478882, "val_acc": 48.0}
{"epoch": 67, "training_loss": 362.32797050476074, "training_acc": 60.0, "val_loss": 259.4092607498169, "val_acc": 48.0}
{"epoch": 68, "training_loss": 434.1639680862427, "training_acc": 69.0, "val_loss": 182.57900476455688, "val_acc": 56.0}
