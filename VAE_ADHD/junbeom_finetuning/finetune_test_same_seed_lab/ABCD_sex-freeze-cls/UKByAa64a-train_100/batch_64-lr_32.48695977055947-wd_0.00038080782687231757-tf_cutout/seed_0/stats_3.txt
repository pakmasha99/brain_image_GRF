"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 36174.70046234131, "training_acc": 49.0, "val_loss": 17105.50994873047, "val_acc": 52.0}
{"epoch": 1, "training_loss": 50626.64685058594, "training_acc": 53.0, "val_loss": 3633.5227966308594, "val_acc": 36.0}
{"epoch": 2, "training_loss": 27309.79931640625, "training_acc": 46.0, "val_loss": 3195.059013366699, "val_acc": 52.0}
{"epoch": 3, "training_loss": 23517.398193359375, "training_acc": 52.0, "val_loss": 15806.962585449219, "val_acc": 52.0}
{"epoch": 4, "training_loss": 41662.727478027344, "training_acc": 53.0, "val_loss": 4971.507263183594, "val_acc": 48.0}
{"epoch": 5, "training_loss": 28898.3798828125, "training_acc": 47.0, "val_loss": 1643.5611724853516, "val_acc": 44.0}
{"epoch": 6, "training_loss": 20954.53369140625, "training_acc": 49.0, "val_loss": 13782.101440429688, "val_acc": 52.0}
{"epoch": 7, "training_loss": 37648.687438964844, "training_acc": 53.0, "val_loss": 1435.6616020202637, "val_acc": 52.0}
{"epoch": 8, "training_loss": 12736.008117675781, "training_acc": 47.0, "val_loss": 1822.3556518554688, "val_acc": 44.0}
{"epoch": 9, "training_loss": 6272.685974121094, "training_acc": 52.0, "val_loss": 3378.043746948242, "val_acc": 52.0}
{"epoch": 10, "training_loss": 6626.960601806641, "training_acc": 59.0, "val_loss": 1312.1801376342773, "val_acc": 56.0}
{"epoch": 11, "training_loss": 9264.961059570312, "training_acc": 52.0, "val_loss": 5374.201202392578, "val_acc": 52.0}
{"epoch": 12, "training_loss": 11418.72925567627, "training_acc": 62.0, "val_loss": 3402.630615234375, "val_acc": 48.0}
{"epoch": 13, "training_loss": 11956.85466003418, "training_acc": 45.0, "val_loss": 1153.171157836914, "val_acc": 52.0}
{"epoch": 14, "training_loss": 2640.4049530029297, "training_acc": 58.0, "val_loss": 3679.046630859375, "val_acc": 52.0}
{"epoch": 15, "training_loss": 8392.95068359375, "training_acc": 57.0, "val_loss": 4332.099914550781, "val_acc": 48.0}
{"epoch": 16, "training_loss": 17828.95831298828, "training_acc": 47.0, "val_loss": 3290.83251953125, "val_acc": 52.0}
{"epoch": 17, "training_loss": 9262.19775390625, "training_acc": 53.0, "val_loss": 1955.5009841918945, "val_acc": 48.0}
{"epoch": 18, "training_loss": 9053.689178466797, "training_acc": 46.0, "val_loss": 5268.603897094727, "val_acc": 52.0}
{"epoch": 19, "training_loss": 12860.990844726562, "training_acc": 53.0, "val_loss": 1020.0876235961914, "val_acc": 44.0}
{"epoch": 20, "training_loss": 6470.398590087891, "training_acc": 58.0, "val_loss": 3021.001434326172, "val_acc": 52.0}
{"epoch": 21, "training_loss": 5722.113754272461, "training_acc": 55.0, "val_loss": 2204.364776611328, "val_acc": 48.0}
{"epoch": 22, "training_loss": 9292.289855957031, "training_acc": 49.0, "val_loss": 5627.166748046875, "val_acc": 52.0}
{"epoch": 23, "training_loss": 16324.83837890625, "training_acc": 53.0, "val_loss": 704.744291305542, "val_acc": 40.0}
{"epoch": 24, "training_loss": 5962.848785400391, "training_acc": 55.0, "val_loss": 611.4517688751221, "val_acc": 36.0}
{"epoch": 25, "training_loss": 4033.45263671875, "training_acc": 61.0, "val_loss": 378.6522626876831, "val_acc": 44.0}
{"epoch": 26, "training_loss": 4555.870880126953, "training_acc": 58.0, "val_loss": 3877.135467529297, "val_acc": 52.0}
{"epoch": 27, "training_loss": 10671.023864746094, "training_acc": 53.0, "val_loss": 1595.412540435791, "val_acc": 48.0}
{"epoch": 28, "training_loss": 4991.544403076172, "training_acc": 53.0, "val_loss": 4883.468246459961, "val_acc": 52.0}
{"epoch": 29, "training_loss": 16146.126342773438, "training_acc": 53.0, "val_loss": 1385.167407989502, "val_acc": 48.0}
{"epoch": 30, "training_loss": 6616.131652832031, "training_acc": 47.0, "val_loss": 4559.117126464844, "val_acc": 52.0}
{"epoch": 31, "training_loss": 12828.70361328125, "training_acc": 53.0, "val_loss": 1097.3119735717773, "val_acc": 52.0}
{"epoch": 32, "training_loss": 3976.8050575256348, "training_acc": 56.0, "val_loss": 4510.058975219727, "val_acc": 52.0}
{"epoch": 33, "training_loss": 11861.548095703125, "training_acc": 53.0, "val_loss": 2880.0092697143555, "val_acc": 48.0}
{"epoch": 34, "training_loss": 13074.135131835938, "training_acc": 47.0, "val_loss": 3274.1653442382812, "val_acc": 52.0}
{"epoch": 35, "training_loss": 10835.625854492188, "training_acc": 53.0, "val_loss": 840.3587341308594, "val_acc": 44.0}
{"epoch": 36, "training_loss": 3849.0691299438477, "training_acc": 52.0, "val_loss": 515.0874137878418, "val_acc": 56.0}
{"epoch": 37, "training_loss": 1778.8132629394531, "training_acc": 69.0, "val_loss": 3023.0308532714844, "val_acc": 52.0}
{"epoch": 38, "training_loss": 5414.198810577393, "training_acc": 58.0, "val_loss": 3052.941131591797, "val_acc": 48.0}
{"epoch": 39, "training_loss": 9993.73497390747, "training_acc": 57.0, "val_loss": 4685.46028137207, "val_acc": 52.0}
{"epoch": 40, "training_loss": 11162.072540283203, "training_acc": 54.0, "val_loss": 2343.34716796875, "val_acc": 48.0}
{"epoch": 41, "training_loss": 10614.27163696289, "training_acc": 47.0, "val_loss": 5229.814147949219, "val_acc": 52.0}
{"epoch": 42, "training_loss": 15132.707580566406, "training_acc": 53.0, "val_loss": 1656.564712524414, "val_acc": 52.0}
{"epoch": 43, "training_loss": 7524.4920654296875, "training_acc": 52.0, "val_loss": 1083.9774131774902, "val_acc": 52.0}
{"epoch": 44, "training_loss": 10607.758239746094, "training_acc": 50.0, "val_loss": 7071.269226074219, "val_acc": 52.0}
