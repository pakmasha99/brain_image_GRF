"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.1977310180664, "training_acc": 53.0, "val_loss": 17.31366366147995, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.31549978256226, "training_acc": 53.0, "val_loss": 17.311570048332214, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.182457447052, "training_acc": 53.0, "val_loss": 17.31070876121521, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.18221306800842, "training_acc": 53.0, "val_loss": 17.310020327568054, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.13112664222717, "training_acc": 53.0, "val_loss": 17.309653759002686, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.06239247322083, "training_acc": 53.0, "val_loss": 17.30959266424179, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.0950837135315, "training_acc": 53.0, "val_loss": 17.31061190366745, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.1350610256195, "training_acc": 53.0, "val_loss": 17.311832308769226, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.18979954719543, "training_acc": 53.0, "val_loss": 17.313195765018463, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.09599089622498, "training_acc": 53.0, "val_loss": 17.314930260181427, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.06731367111206, "training_acc": 53.0, "val_loss": 17.31644570827484, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.0093367099762, "training_acc": 53.0, "val_loss": 17.317405343055725, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.07566952705383, "training_acc": 53.0, "val_loss": 17.318332195281982, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.10187196731567, "training_acc": 53.0, "val_loss": 17.31969863176346, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.27065014839172, "training_acc": 53.0, "val_loss": 17.320643365383148, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.0780758857727, "training_acc": 53.0, "val_loss": 17.321552336215973, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.08633542060852, "training_acc": 53.0, "val_loss": 17.32211261987686, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.19337821006775, "training_acc": 53.0, "val_loss": 17.32233464717865, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.13170981407166, "training_acc": 53.0, "val_loss": 17.32301265001297, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.19752526283264, "training_acc": 53.0, "val_loss": 17.32337176799774, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.11199688911438, "training_acc": 53.0, "val_loss": 17.32388287782669, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.11768460273743, "training_acc": 53.0, "val_loss": 17.324496805667877, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.01765370368958, "training_acc": 53.0, "val_loss": 17.324741184711456, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.04267978668213, "training_acc": 53.0, "val_loss": 17.324909567832947, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.08491063117981, "training_acc": 53.0, "val_loss": 17.32529103755951, "val_acc": 52.0}
