"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 70.02511548995972, "training_acc": 47.0, "val_loss": 17.531977593898773, "val_acc": 52.0}
{"epoch": 1, "training_loss": 70.07690906524658, "training_acc": 47.0, "val_loss": 17.507638037204742, "val_acc": 52.0}
{"epoch": 2, "training_loss": 70.0428740978241, "training_acc": 47.0, "val_loss": 17.48259663581848, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.73999810218811, "training_acc": 47.0, "val_loss": 17.457740008831024, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.64327359199524, "training_acc": 47.0, "val_loss": 17.435559630393982, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.57957553863525, "training_acc": 46.0, "val_loss": 17.414912581443787, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.52960276603699, "training_acc": 46.0, "val_loss": 17.39741414785385, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.48841452598572, "training_acc": 49.0, "val_loss": 17.383721470832825, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.31812262535095, "training_acc": 53.0, "val_loss": 17.370684444904327, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.36230993270874, "training_acc": 47.0, "val_loss": 17.359456419944763, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.47657895088196, "training_acc": 43.0, "val_loss": 17.34956055879593, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.47572755813599, "training_acc": 53.0, "val_loss": 17.341691255569458, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.30528521537781, "training_acc": 53.0, "val_loss": 17.336255311965942, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.32984781265259, "training_acc": 54.0, "val_loss": 17.33299344778061, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.35429382324219, "training_acc": 51.0, "val_loss": 17.330074310302734, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.13801288604736, "training_acc": 53.0, "val_loss": 17.32749491930008, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.20461583137512, "training_acc": 53.0, "val_loss": 17.32533425092697, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.17000555992126, "training_acc": 53.0, "val_loss": 17.323490977287292, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.04637956619263, "training_acc": 53.0, "val_loss": 17.321841418743134, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.13210368156433, "training_acc": 53.0, "val_loss": 17.320381104946136, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.11347246170044, "training_acc": 53.0, "val_loss": 17.319270968437195, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.28070116043091, "training_acc": 53.0, "val_loss": 17.31833815574646, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.13843107223511, "training_acc": 53.0, "val_loss": 17.317795753479004, "val_acc": 52.0}
{"epoch": 23, "training_loss": 68.97402596473694, "training_acc": 53.0, "val_loss": 17.317460477352142, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.20940208435059, "training_acc": 53.0, "val_loss": 17.317132651805878, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.01043748855591, "training_acc": 53.0, "val_loss": 17.31712520122528, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.03908157348633, "training_acc": 53.0, "val_loss": 17.31726974248886, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.2865936756134, "training_acc": 53.0, "val_loss": 17.317327857017517, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.12857913970947, "training_acc": 53.0, "val_loss": 17.317433655261993, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.25992274284363, "training_acc": 53.0, "val_loss": 17.31768399477005, "val_acc": 52.0}
{"epoch": 30, "training_loss": 68.98004651069641, "training_acc": 53.0, "val_loss": 17.31797754764557, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.10979795455933, "training_acc": 53.0, "val_loss": 17.318324744701385, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.07675909996033, "training_acc": 53.0, "val_loss": 17.318615317344666, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.19905829429626, "training_acc": 53.0, "val_loss": 17.318855226039886, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.09623670578003, "training_acc": 53.0, "val_loss": 17.31906235218048, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.13615107536316, "training_acc": 53.0, "val_loss": 17.31942594051361, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.05283093452454, "training_acc": 53.0, "val_loss": 17.319756746292114, "val_acc": 52.0}
{"epoch": 37, "training_loss": 68.99191975593567, "training_acc": 53.0, "val_loss": 17.320148646831512, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.12445497512817, "training_acc": 53.0, "val_loss": 17.32032001018524, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.30863666534424, "training_acc": 53.0, "val_loss": 17.320071160793304, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.21556448936462, "training_acc": 53.0, "val_loss": 17.319926619529724, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.24258828163147, "training_acc": 53.0, "val_loss": 17.319637537002563, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.23545670509338, "training_acc": 53.0, "val_loss": 17.319726943969727, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.21189188957214, "training_acc": 53.0, "val_loss": 17.31974184513092, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.18245029449463, "training_acc": 53.0, "val_loss": 17.319709062576294, "val_acc": 52.0}
