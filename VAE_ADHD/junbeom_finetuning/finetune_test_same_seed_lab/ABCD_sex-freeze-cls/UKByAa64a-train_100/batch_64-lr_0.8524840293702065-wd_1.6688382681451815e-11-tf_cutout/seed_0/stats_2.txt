"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 302.6584224700928, "training_acc": 50.0, "val_loss": 766.8384075164795, "val_acc": 52.0}
{"epoch": 1, "training_loss": 2553.8837280273438, "training_acc": 53.0, "val_loss": 244.44570541381836, "val_acc": 48.0}
{"epoch": 2, "training_loss": 1176.977653503418, "training_acc": 47.0, "val_loss": 40.081146359443665, "val_acc": 44.0}
{"epoch": 3, "training_loss": 310.15247344970703, "training_acc": 66.0, "val_loss": 369.06726360321045, "val_acc": 52.0}
{"epoch": 4, "training_loss": 1155.8554725646973, "training_acc": 53.0, "val_loss": 149.24373626708984, "val_acc": 48.0}
{"epoch": 5, "training_loss": 753.7733268737793, "training_acc": 47.0, "val_loss": 35.98085641860962, "val_acc": 48.0}
{"epoch": 6, "training_loss": 378.93352699279785, "training_acc": 58.0, "val_loss": 359.9379301071167, "val_acc": 52.0}
{"epoch": 7, "training_loss": 1161.2815017700195, "training_acc": 53.0, "val_loss": 52.076393365859985, "val_acc": 56.0}
{"epoch": 8, "training_loss": 567.6803321838379, "training_acc": 51.0, "val_loss": 320.139217376709, "val_acc": 48.0}
{"epoch": 9, "training_loss": 1194.7966079711914, "training_acc": 47.0, "val_loss": 91.59291386604309, "val_acc": 52.0}
{"epoch": 10, "training_loss": 499.33844566345215, "training_acc": 48.0, "val_loss": 250.6453514099121, "val_acc": 52.0}
{"epoch": 11, "training_loss": 685.5036325454712, "training_acc": 52.0, "val_loss": 128.89987230300903, "val_acc": 48.0}
{"epoch": 12, "training_loss": 734.7266483306885, "training_acc": 47.0, "val_loss": 59.67280864715576, "val_acc": 44.0}
{"epoch": 13, "training_loss": 399.8623466491699, "training_acc": 52.0, "val_loss": 255.9971809387207, "val_acc": 52.0}
{"epoch": 14, "training_loss": 758.0814762115479, "training_acc": 53.0, "val_loss": 28.211602568626404, "val_acc": 64.0}
{"epoch": 15, "training_loss": 317.0256824493408, "training_acc": 60.0, "val_loss": 106.20487928390503, "val_acc": 48.0}
{"epoch": 16, "training_loss": 360.2043447494507, "training_acc": 58.0, "val_loss": 170.58961391448975, "val_acc": 52.0}
{"epoch": 17, "training_loss": 516.5553340911865, "training_acc": 53.0, "val_loss": 26.393717527389526, "val_acc": 60.0}
{"epoch": 18, "training_loss": 214.31170654296875, "training_acc": 56.0, "val_loss": 27.294179797172546, "val_acc": 60.0}
{"epoch": 19, "training_loss": 158.10940647125244, "training_acc": 61.0, "val_loss": 41.498592495918274, "val_acc": 56.0}
{"epoch": 20, "training_loss": 179.94972896575928, "training_acc": 59.0, "val_loss": 31.340360641479492, "val_acc": 60.0}
{"epoch": 21, "training_loss": 234.23508262634277, "training_acc": 55.0, "val_loss": 67.59843230247498, "val_acc": 52.0}
{"epoch": 22, "training_loss": 300.0369415283203, "training_acc": 54.0, "val_loss": 72.34286069869995, "val_acc": 48.0}
{"epoch": 23, "training_loss": 258.87128353118896, "training_acc": 50.0, "val_loss": 73.5342025756836, "val_acc": 52.0}
{"epoch": 24, "training_loss": 139.76185488700867, "training_acc": 61.0, "val_loss": 44.60357427597046, "val_acc": 44.0}
{"epoch": 25, "training_loss": 192.1184868812561, "training_acc": 53.0, "val_loss": 49.59118068218231, "val_acc": 52.0}
{"epoch": 26, "training_loss": 204.74514865875244, "training_acc": 52.0, "val_loss": 24.95296001434326, "val_acc": 56.0}
{"epoch": 27, "training_loss": 222.32184028625488, "training_acc": 49.0, "val_loss": 106.76651000976562, "val_acc": 52.0}
{"epoch": 28, "training_loss": 320.50933742523193, "training_acc": 48.0, "val_loss": 37.00777292251587, "val_acc": 52.0}
{"epoch": 29, "training_loss": 238.42219924926758, "training_acc": 49.0, "val_loss": 105.95805644989014, "val_acc": 52.0}
{"epoch": 30, "training_loss": 260.66336154937744, "training_acc": 53.0, "val_loss": 35.82908511161804, "val_acc": 52.0}
{"epoch": 31, "training_loss": 232.4982089996338, "training_acc": 55.0, "val_loss": 86.64209842681885, "val_acc": 52.0}
{"epoch": 32, "training_loss": 193.99251079559326, "training_acc": 56.0, "val_loss": 24.033664166927338, "val_acc": 56.0}
{"epoch": 33, "training_loss": 163.80231952667236, "training_acc": 59.0, "val_loss": 37.70466446876526, "val_acc": 56.0}
{"epoch": 34, "training_loss": 152.44759273529053, "training_acc": 54.0, "val_loss": 33.69930982589722, "val_acc": 44.0}
{"epoch": 35, "training_loss": 273.468542098999, "training_acc": 47.0, "val_loss": 93.90069842338562, "val_acc": 52.0}
{"epoch": 36, "training_loss": 231.12509632110596, "training_acc": 57.0, "val_loss": 82.30041265487671, "val_acc": 48.0}
{"epoch": 37, "training_loss": 221.9241795539856, "training_acc": 53.0, "val_loss": 74.09902215003967, "val_acc": 52.0}
{"epoch": 38, "training_loss": 210.90165424346924, "training_acc": 46.0, "val_loss": 30.14216125011444, "val_acc": 48.0}
{"epoch": 39, "training_loss": 172.449444770813, "training_acc": 59.0, "val_loss": 31.646400690078735, "val_acc": 56.0}
{"epoch": 40, "training_loss": 97.69369983673096, "training_acc": 65.0, "val_loss": 25.449812412261963, "val_acc": 60.0}
{"epoch": 41, "training_loss": 68.50281596183777, "training_acc": 68.0, "val_loss": 28.100726008415222, "val_acc": 56.0}
{"epoch": 42, "training_loss": 118.0278377532959, "training_acc": 67.0, "val_loss": 45.70443630218506, "val_acc": 52.0}
{"epoch": 43, "training_loss": 118.10303688049316, "training_acc": 62.0, "val_loss": 75.07659196853638, "val_acc": 48.0}
{"epoch": 44, "training_loss": 219.22960901260376, "training_acc": 48.0, "val_loss": 94.06455755233765, "val_acc": 52.0}
{"epoch": 45, "training_loss": 288.2857999801636, "training_acc": 54.0, "val_loss": 65.04835486412048, "val_acc": 44.0}
{"epoch": 46, "training_loss": 254.10656356811523, "training_acc": 49.0, "val_loss": 105.07076978683472, "val_acc": 52.0}
{"epoch": 47, "training_loss": 365.7306785583496, "training_acc": 53.0, "val_loss": 33.91262888908386, "val_acc": 52.0}
{"epoch": 48, "training_loss": 259.9503288269043, "training_acc": 57.0, "val_loss": 123.0237364768982, "val_acc": 48.0}
{"epoch": 49, "training_loss": 271.8131248950958, "training_acc": 57.0, "val_loss": 104.03871536254883, "val_acc": 52.0}
{"epoch": 50, "training_loss": 239.77498865127563, "training_acc": 60.0, "val_loss": 81.6099762916565, "val_acc": 44.0}
{"epoch": 51, "training_loss": 190.1341049671173, "training_acc": 56.0, "val_loss": 74.89842176437378, "val_acc": 52.0}
