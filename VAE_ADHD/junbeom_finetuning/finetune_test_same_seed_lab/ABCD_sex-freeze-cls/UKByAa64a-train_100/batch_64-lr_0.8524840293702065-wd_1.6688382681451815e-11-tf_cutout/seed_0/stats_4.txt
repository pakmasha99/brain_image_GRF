"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 1988.0192794799805, "training_acc": 53.0, "val_loss": 312.6727342605591, "val_acc": 52.0}
{"epoch": 1, "training_loss": 2071.511215209961, "training_acc": 47.0, "val_loss": 1012.4828338623047, "val_acc": 48.0}
{"epoch": 2, "training_loss": 3825.7270889282227, "training_acc": 47.0, "val_loss": 351.84500217437744, "val_acc": 48.0}
{"epoch": 3, "training_loss": 1285.7962646484375, "training_acc": 49.0, "val_loss": 541.1366939544678, "val_acc": 52.0}
{"epoch": 4, "training_loss": 2087.683361053467, "training_acc": 53.0, "val_loss": 421.53477668762207, "val_acc": 52.0}
{"epoch": 5, "training_loss": 1177.929765701294, "training_acc": 53.0, "val_loss": 296.3489532470703, "val_acc": 48.0}
{"epoch": 6, "training_loss": 1471.912368774414, "training_acc": 47.0, "val_loss": 482.37085342407227, "val_acc": 48.0}
{"epoch": 7, "training_loss": 1632.5402946472168, "training_acc": 47.0, "val_loss": 50.219982862472534, "val_acc": 52.0}
{"epoch": 8, "training_loss": 414.44072341918945, "training_acc": 53.0, "val_loss": 237.8086805343628, "val_acc": 52.0}
{"epoch": 9, "training_loss": 760.8491249084473, "training_acc": 53.0, "val_loss": 143.1796669960022, "val_acc": 48.0}
{"epoch": 10, "training_loss": 600.334041595459, "training_acc": 47.0, "val_loss": 118.44668388366699, "val_acc": 48.0}
{"epoch": 11, "training_loss": 405.38904190063477, "training_acc": 53.0, "val_loss": 158.48411321640015, "val_acc": 52.0}
{"epoch": 12, "training_loss": 511.8363161087036, "training_acc": 53.0, "val_loss": 116.11853837966919, "val_acc": 48.0}
{"epoch": 13, "training_loss": 411.68390464782715, "training_acc": 51.0, "val_loss": 37.679922580718994, "val_acc": 44.0}
{"epoch": 14, "training_loss": 302.47501373291016, "training_acc": 53.0, "val_loss": 145.6148386001587, "val_acc": 52.0}
{"epoch": 15, "training_loss": 414.85629510879517, "training_acc": 58.0, "val_loss": 148.04439544677734, "val_acc": 48.0}
{"epoch": 16, "training_loss": 515.2851028442383, "training_acc": 49.0, "val_loss": 44.187355041503906, "val_acc": 44.0}
{"epoch": 17, "training_loss": 323.18068504333496, "training_acc": 53.0, "val_loss": 222.1242904663086, "val_acc": 52.0}
{"epoch": 18, "training_loss": 767.2463245391846, "training_acc": 53.0, "val_loss": 51.23382210731506, "val_acc": 44.0}
{"epoch": 19, "training_loss": 319.17280197143555, "training_acc": 52.0, "val_loss": 109.61785316467285, "val_acc": 48.0}
{"epoch": 20, "training_loss": 415.38666343688965, "training_acc": 44.0, "val_loss": 116.58716201782227, "val_acc": 52.0}
{"epoch": 21, "training_loss": 348.5570983886719, "training_acc": 50.0, "val_loss": 112.9781723022461, "val_acc": 48.0}
{"epoch": 22, "training_loss": 345.26905059814453, "training_acc": 49.0, "val_loss": 56.43764138221741, "val_acc": 52.0}
{"epoch": 23, "training_loss": 250.2625551223755, "training_acc": 54.0, "val_loss": 42.57286787033081, "val_acc": 44.0}
{"epoch": 24, "training_loss": 209.10627841949463, "training_acc": 51.0, "val_loss": 47.956687211990356, "val_acc": 52.0}
{"epoch": 25, "training_loss": 195.3773069381714, "training_acc": 59.0, "val_loss": 47.22480773925781, "val_acc": 44.0}
{"epoch": 26, "training_loss": 150.02734470367432, "training_acc": 55.0, "val_loss": 53.13378572463989, "val_acc": 52.0}
{"epoch": 27, "training_loss": 186.1209373474121, "training_acc": 55.0, "val_loss": 60.07450819015503, "val_acc": 40.0}
{"epoch": 28, "training_loss": 179.2990415096283, "training_acc": 50.0, "val_loss": 43.91937851905823, "val_acc": 52.0}
{"epoch": 29, "training_loss": 115.13597655296326, "training_acc": 65.0, "val_loss": 35.452666878700256, "val_acc": 40.0}
{"epoch": 30, "training_loss": 120.69822716712952, "training_acc": 57.0, "val_loss": 23.89717996120453, "val_acc": 56.0}
{"epoch": 31, "training_loss": 74.53810429573059, "training_acc": 74.0, "val_loss": 49.4335800409317, "val_acc": 48.0}
{"epoch": 32, "training_loss": 133.6740436553955, "training_acc": 51.0, "val_loss": 30.768099427223206, "val_acc": 52.0}
{"epoch": 33, "training_loss": 148.74842262268066, "training_acc": 55.0, "val_loss": 21.011511981487274, "val_acc": 56.0}
{"epoch": 34, "training_loss": 164.21730422973633, "training_acc": 52.0, "val_loss": 28.637313842773438, "val_acc": 52.0}
{"epoch": 35, "training_loss": 265.3464813232422, "training_acc": 52.0, "val_loss": 37.51988112926483, "val_acc": 44.0}
{"epoch": 36, "training_loss": 217.71770191192627, "training_acc": 59.0, "val_loss": 172.72123098373413, "val_acc": 52.0}
{"epoch": 37, "training_loss": 456.1592082977295, "training_acc": 53.0, "val_loss": 146.58714532852173, "val_acc": 48.0}
{"epoch": 38, "training_loss": 659.3623104095459, "training_acc": 47.0, "val_loss": 81.31039142608643, "val_acc": 48.0}
{"epoch": 39, "training_loss": 442.93196296691895, "training_acc": 45.0, "val_loss": 242.31250286102295, "val_acc": 52.0}
{"epoch": 40, "training_loss": 770.9192504882812, "training_acc": 53.0, "val_loss": 23.949283361434937, "val_acc": 56.0}
{"epoch": 41, "training_loss": 193.2680206298828, "training_acc": 58.0, "val_loss": 86.49473190307617, "val_acc": 48.0}
{"epoch": 42, "training_loss": 352.7977304458618, "training_acc": 51.0, "val_loss": 145.7023024559021, "val_acc": 52.0}
{"epoch": 43, "training_loss": 417.8989691734314, "training_acc": 53.0, "val_loss": 58.02929401397705, "val_acc": 48.0}
{"epoch": 44, "training_loss": 218.2012836933136, "training_acc": 50.0, "val_loss": 73.46325516700745, "val_acc": 52.0}
{"epoch": 45, "training_loss": 152.8908212184906, "training_acc": 63.0, "val_loss": 30.340710282325745, "val_acc": 48.0}
{"epoch": 46, "training_loss": 117.95939016342163, "training_acc": 56.0, "val_loss": 20.232658088207245, "val_acc": 52.0}
{"epoch": 47, "training_loss": 86.07353568077087, "training_acc": 66.0, "val_loss": 31.12122416496277, "val_acc": 52.0}
{"epoch": 48, "training_loss": 88.90679836273193, "training_acc": 65.0, "val_loss": 25.784960389137268, "val_acc": 60.0}
{"epoch": 49, "training_loss": 89.59026908874512, "training_acc": 67.0, "val_loss": 24.92806613445282, "val_acc": 56.0}
{"epoch": 50, "training_loss": 50.09967923164368, "training_acc": 79.0, "val_loss": 36.23241484165192, "val_acc": 40.0}
{"epoch": 51, "training_loss": 143.15303945541382, "training_acc": 53.0, "val_loss": 23.308013379573822, "val_acc": 72.0}
{"epoch": 52, "training_loss": 75.84125924110413, "training_acc": 62.0, "val_loss": 56.06239438056946, "val_acc": 52.0}
{"epoch": 53, "training_loss": 126.81739377975464, "training_acc": 60.0, "val_loss": 25.52175521850586, "val_acc": 52.0}
{"epoch": 54, "training_loss": 82.2196877002716, "training_acc": 65.0, "val_loss": 27.731385827064514, "val_acc": 44.0}
{"epoch": 55, "training_loss": 90.02893495559692, "training_acc": 62.0, "val_loss": 31.944486498832703, "val_acc": 40.0}
{"epoch": 56, "training_loss": 76.97716093063354, "training_acc": 67.0, "val_loss": 34.419310092926025, "val_acc": 52.0}
{"epoch": 57, "training_loss": 145.97434902191162, "training_acc": 54.0, "val_loss": 35.66286861896515, "val_acc": 52.0}
{"epoch": 58, "training_loss": 107.35980415344238, "training_acc": 65.0, "val_loss": 39.24618363380432, "val_acc": 48.0}
{"epoch": 59, "training_loss": 139.3824553489685, "training_acc": 47.0, "val_loss": 38.470661640167236, "val_acc": 44.0}
{"epoch": 60, "training_loss": 76.94158709049225, "training_acc": 62.0, "val_loss": 37.73505687713623, "val_acc": 56.0}
{"epoch": 61, "training_loss": 85.40137910842896, "training_acc": 64.0, "val_loss": 21.934562921524048, "val_acc": 64.0}
{"epoch": 62, "training_loss": 58.137447357177734, "training_acc": 75.0, "val_loss": 24.71705973148346, "val_acc": 56.0}
{"epoch": 63, "training_loss": 59.4064564704895, "training_acc": 68.0, "val_loss": 26.412752270698547, "val_acc": 60.0}
{"epoch": 64, "training_loss": 55.43554139137268, "training_acc": 72.0, "val_loss": 30.925357341766357, "val_acc": 44.0}
{"epoch": 65, "training_loss": 97.10894632339478, "training_acc": 59.0, "val_loss": 23.817050457000732, "val_acc": 44.0}
