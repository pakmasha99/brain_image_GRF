"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 5461.932723999023, "training_acc": 53.0, "val_loss": 309.5330238342285, "val_acc": 52.0}
{"epoch": 1, "training_loss": 7000.108642578125, "training_acc": 49.0, "val_loss": 5021.520233154297, "val_acc": 48.0}
{"epoch": 2, "training_loss": 18672.02410888672, "training_acc": 47.0, "val_loss": 2382.3720932006836, "val_acc": 48.0}
{"epoch": 3, "training_loss": 6199.515087127686, "training_acc": 49.0, "val_loss": 1531.3526153564453, "val_acc": 52.0}
{"epoch": 4, "training_loss": 6310.970977783203, "training_acc": 53.0, "val_loss": 1372.8866577148438, "val_acc": 52.0}
{"epoch": 5, "training_loss": 3981.248046875, "training_acc": 53.0, "val_loss": 1331.0834884643555, "val_acc": 48.0}
{"epoch": 6, "training_loss": 5996.6202392578125, "training_acc": 47.0, "val_loss": 1831.0758590698242, "val_acc": 48.0}
{"epoch": 7, "training_loss": 5693.440811157227, "training_acc": 47.0, "val_loss": 479.1515350341797, "val_acc": 52.0}
{"epoch": 8, "training_loss": 2852.804977416992, "training_acc": 53.0, "val_loss": 1138.0585670471191, "val_acc": 52.0}
{"epoch": 9, "training_loss": 3671.9884643554688, "training_acc": 53.0, "val_loss": 446.61593437194824, "val_acc": 48.0}
{"epoch": 10, "training_loss": 2006.2480850219727, "training_acc": 48.0, "val_loss": 725.2670764923096, "val_acc": 48.0}
{"epoch": 11, "training_loss": 1640.3518753051758, "training_acc": 48.0, "val_loss": 824.119758605957, "val_acc": 52.0}
{"epoch": 12, "training_loss": 3487.189208984375, "training_acc": 53.0, "val_loss": 694.8139667510986, "val_acc": 52.0}
{"epoch": 13, "training_loss": 1887.982997894287, "training_acc": 53.0, "val_loss": 436.77196502685547, "val_acc": 48.0}
{"epoch": 14, "training_loss": 1213.0203342437744, "training_acc": 46.0, "val_loss": 556.333589553833, "val_acc": 52.0}
{"epoch": 15, "training_loss": 2204.5038986206055, "training_acc": 53.0, "val_loss": 288.3840322494507, "val_acc": 52.0}
{"epoch": 16, "training_loss": 1679.3927688598633, "training_acc": 47.0, "val_loss": 804.0095329284668, "val_acc": 48.0}
{"epoch": 17, "training_loss": 2625.7349815368652, "training_acc": 48.0, "val_loss": 482.8033447265625, "val_acc": 52.0}
{"epoch": 18, "training_loss": 1949.2823104858398, "training_acc": 53.0, "val_loss": 554.7402381896973, "val_acc": 52.0}
{"epoch": 19, "training_loss": 1370.3712463378906, "training_acc": 52.0, "val_loss": 327.82225608825684, "val_acc": 48.0}
{"epoch": 20, "training_loss": 1241.4417753219604, "training_acc": 54.0, "val_loss": 380.4095983505249, "val_acc": 52.0}
{"epoch": 21, "training_loss": 993.0308589935303, "training_acc": 57.0, "val_loss": 76.96353197097778, "val_acc": 60.0}
{"epoch": 22, "training_loss": 473.41829204559326, "training_acc": 59.0, "val_loss": 159.01153087615967, "val_acc": 60.0}
{"epoch": 23, "training_loss": 650.0998382568359, "training_acc": 56.0, "val_loss": 146.47445678710938, "val_acc": 44.0}
{"epoch": 24, "training_loss": 677.3111095428467, "training_acc": 54.0, "val_loss": 256.19804859161377, "val_acc": 52.0}
{"epoch": 25, "training_loss": 558.721529006958, "training_acc": 56.0, "val_loss": 61.99226975440979, "val_acc": 52.0}
{"epoch": 26, "training_loss": 276.52326107025146, "training_acc": 66.0, "val_loss": 91.81487560272217, "val_acc": 52.0}
{"epoch": 27, "training_loss": 271.40777492523193, "training_acc": 64.0, "val_loss": 230.42454719543457, "val_acc": 52.0}
{"epoch": 28, "training_loss": 441.49442958831787, "training_acc": 61.0, "val_loss": 285.98124980926514, "val_acc": 48.0}
{"epoch": 29, "training_loss": 864.3224277496338, "training_acc": 47.0, "val_loss": 702.9513359069824, "val_acc": 52.0}
{"epoch": 30, "training_loss": 2614.103874206543, "training_acc": 53.0, "val_loss": 620.3493118286133, "val_acc": 52.0}
{"epoch": 31, "training_loss": 1234.1522359848022, "training_acc": 62.0, "val_loss": 336.2771511077881, "val_acc": 48.0}
{"epoch": 32, "training_loss": 994.3742702007294, "training_acc": 58.0, "val_loss": 438.86327743530273, "val_acc": 52.0}
{"epoch": 33, "training_loss": 1216.7064361572266, "training_acc": 53.0, "val_loss": 370.4366683959961, "val_acc": 48.0}
{"epoch": 34, "training_loss": 1727.5336074829102, "training_acc": 47.0, "val_loss": 186.3505721092224, "val_acc": 56.0}
{"epoch": 35, "training_loss": 643.7723655700684, "training_acc": 58.0, "val_loss": 96.7017114162445, "val_acc": 60.0}
{"epoch": 36, "training_loss": 654.9298210144043, "training_acc": 59.0, "val_loss": 83.05094242095947, "val_acc": 52.0}
{"epoch": 37, "training_loss": 612.6119270324707, "training_acc": 65.0, "val_loss": 619.9014663696289, "val_acc": 52.0}
{"epoch": 38, "training_loss": 1499.6987295150757, "training_acc": 54.0, "val_loss": 639.1135215759277, "val_acc": 48.0}
{"epoch": 39, "training_loss": 2692.6928939819336, "training_acc": 47.0, "val_loss": 225.96397399902344, "val_acc": 44.0}
{"epoch": 40, "training_loss": 1591.7802505493164, "training_acc": 49.0, "val_loss": 1052.9333114624023, "val_acc": 52.0}
{"epoch": 41, "training_loss": 3402.5524826049805, "training_acc": 53.0, "val_loss": 133.74260663986206, "val_acc": 60.0}
{"epoch": 42, "training_loss": 1448.975601196289, "training_acc": 59.0, "val_loss": 1100.821590423584, "val_acc": 48.0}
{"epoch": 43, "training_loss": 3745.7147521972656, "training_acc": 47.0, "val_loss": 191.47101640701294, "val_acc": 56.0}
{"epoch": 44, "training_loss": 1027.3827476501465, "training_acc": 55.0, "val_loss": 278.8964509963989, "val_acc": 52.0}
