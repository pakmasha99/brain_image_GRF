"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 6910.256458282471, "training_acc": 46.0, "val_loss": 1435.0007057189941, "val_acc": 52.0}
{"epoch": 1, "training_loss": 6969.097686767578, "training_acc": 53.0, "val_loss": 3716.7598724365234, "val_acc": 48.0}
{"epoch": 2, "training_loss": 13987.699768066406, "training_acc": 47.0, "val_loss": 1030.3594589233398, "val_acc": 48.0}
{"epoch": 3, "training_loss": 4873.93684387207, "training_acc": 51.0, "val_loss": 2714.1830444335938, "val_acc": 52.0}
{"epoch": 4, "training_loss": 10312.53451538086, "training_acc": 53.0, "val_loss": 2421.932029724121, "val_acc": 52.0}
{"epoch": 5, "training_loss": 7188.517120361328, "training_acc": 53.0, "val_loss": 420.94225883483887, "val_acc": 48.0}
{"epoch": 6, "training_loss": 3878.865447998047, "training_acc": 47.0, "val_loss": 1240.6758308410645, "val_acc": 48.0}
{"epoch": 7, "training_loss": 4204.904197692871, "training_acc": 47.0, "val_loss": 898.776912689209, "val_acc": 52.0}
{"epoch": 8, "training_loss": 3505.0752563476562, "training_acc": 53.0, "val_loss": 1558.5284233093262, "val_acc": 52.0}
{"epoch": 9, "training_loss": 5061.231216430664, "training_acc": 53.0, "val_loss": 196.6112494468689, "val_acc": 52.0}
{"epoch": 10, "training_loss": 2053.1859588623047, "training_acc": 49.0, "val_loss": 1479.958724975586, "val_acc": 48.0}
{"epoch": 11, "training_loss": 5693.21549987793, "training_acc": 47.0, "val_loss": 364.15367126464844, "val_acc": 48.0}
{"epoch": 12, "training_loss": 2068.779655456543, "training_acc": 49.0, "val_loss": 1454.1518211364746, "val_acc": 52.0}
{"epoch": 13, "training_loss": 5567.589508056641, "training_acc": 53.0, "val_loss": 1058.1425666809082, "val_acc": 52.0}
{"epoch": 14, "training_loss": 2768.1028213500977, "training_acc": 54.0, "val_loss": 1015.4453277587891, "val_acc": 48.0}
{"epoch": 15, "training_loss": 4771.212341308594, "training_acc": 47.0, "val_loss": 1203.580093383789, "val_acc": 48.0}
{"epoch": 16, "training_loss": 3619.256790161133, "training_acc": 47.0, "val_loss": 839.0371322631836, "val_acc": 52.0}
{"epoch": 17, "training_loss": 3820.755828857422, "training_acc": 53.0, "val_loss": 1517.3436164855957, "val_acc": 52.0}
{"epoch": 18, "training_loss": 5156.873558044434, "training_acc": 53.0, "val_loss": 264.0812873840332, "val_acc": 52.0}
{"epoch": 19, "training_loss": 1505.6315460205078, "training_acc": 59.0, "val_loss": 1342.2826766967773, "val_acc": 48.0}
{"epoch": 20, "training_loss": 5162.198806762695, "training_acc": 47.0, "val_loss": 515.065336227417, "val_acc": 48.0}
{"epoch": 21, "training_loss": 2103.273078918457, "training_acc": 47.0, "val_loss": 1034.4733238220215, "val_acc": 52.0}
{"epoch": 22, "training_loss": 3323.5017318725586, "training_acc": 53.0, "val_loss": 422.0487117767334, "val_acc": 52.0}
{"epoch": 23, "training_loss": 1346.3258666992188, "training_acc": 56.0, "val_loss": 816.7543411254883, "val_acc": 48.0}
{"epoch": 24, "training_loss": 2966.74161529541, "training_acc": 47.0, "val_loss": 307.23278522491455, "val_acc": 56.0}
{"epoch": 25, "training_loss": 1175.0737686157227, "training_acc": 58.0, "val_loss": 576.1795520782471, "val_acc": 52.0}
{"epoch": 26, "training_loss": 1483.2891883850098, "training_acc": 51.0, "val_loss": 172.76989221572876, "val_acc": 36.0}
{"epoch": 27, "training_loss": 902.3865756988525, "training_acc": 48.0, "val_loss": 263.10598850250244, "val_acc": 56.0}
{"epoch": 28, "training_loss": 448.2211971282959, "training_acc": 59.0, "val_loss": 142.62667894363403, "val_acc": 36.0}
{"epoch": 29, "training_loss": 501.4324893951416, "training_acc": 63.0, "val_loss": 446.8392848968506, "val_acc": 52.0}
{"epoch": 30, "training_loss": 1284.2731628417969, "training_acc": 51.0, "val_loss": 215.49124717712402, "val_acc": 44.0}
{"epoch": 31, "training_loss": 968.6072769165039, "training_acc": 54.0, "val_loss": 288.73982429504395, "val_acc": 56.0}
{"epoch": 32, "training_loss": 961.5978775024414, "training_acc": 54.0, "val_loss": 86.35813593864441, "val_acc": 44.0}
{"epoch": 33, "training_loss": 447.35667419433594, "training_acc": 60.0, "val_loss": 99.83271360397339, "val_acc": 60.0}
{"epoch": 34, "training_loss": 806.1045722961426, "training_acc": 56.0, "val_loss": 97.92652726173401, "val_acc": 64.0}
{"epoch": 35, "training_loss": 787.7866096496582, "training_acc": 59.0, "val_loss": 135.5254054069519, "val_acc": 40.0}
{"epoch": 36, "training_loss": 1092.1033096313477, "training_acc": 53.0, "val_loss": 656.0403347015381, "val_acc": 52.0}
{"epoch": 37, "training_loss": 1519.4588260650635, "training_acc": 54.0, "val_loss": 524.0859031677246, "val_acc": 48.0}
{"epoch": 38, "training_loss": 2540.605941772461, "training_acc": 47.0, "val_loss": 244.09871101379395, "val_acc": 48.0}
{"epoch": 39, "training_loss": 1500.7729034423828, "training_acc": 52.0, "val_loss": 964.2200469970703, "val_acc": 52.0}
{"epoch": 40, "training_loss": 3020.5544052124023, "training_acc": 53.0, "val_loss": 103.86843681335449, "val_acc": 64.0}
{"epoch": 41, "training_loss": 1231.6190338134766, "training_acc": 56.0, "val_loss": 817.6685333251953, "val_acc": 48.0}
{"epoch": 42, "training_loss": 2549.946434020996, "training_acc": 47.0, "val_loss": 673.0142116546631, "val_acc": 52.0}
{"epoch": 43, "training_loss": 3057.758056640625, "training_acc": 53.0, "val_loss": 968.76220703125, "val_acc": 52.0}
{"epoch": 44, "training_loss": 2457.0330696105957, "training_acc": 53.0, "val_loss": 791.6392803192139, "val_acc": 48.0}
{"epoch": 45, "training_loss": 4104.18879699707, "training_acc": 47.0, "val_loss": 1097.4543571472168, "val_acc": 48.0}
{"epoch": 46, "training_loss": 3421.4797954559326, "training_acc": 48.0, "val_loss": 888.0910873413086, "val_acc": 52.0}
{"epoch": 47, "training_loss": 3784.316925048828, "training_acc": 53.0, "val_loss": 1392.781925201416, "val_acc": 52.0}
{"epoch": 48, "training_loss": 4553.329574584961, "training_acc": 53.0, "val_loss": 67.48016476631165, "val_acc": 52.0}
{"epoch": 49, "training_loss": 1263.55029296875, "training_acc": 60.0, "val_loss": 793.2400226593018, "val_acc": 48.0}
{"epoch": 50, "training_loss": 2116.1184043884277, "training_acc": 49.0, "val_loss": 724.4467735290527, "val_acc": 52.0}
{"epoch": 51, "training_loss": 3232.022659301758, "training_acc": 53.0, "val_loss": 1030.3640365600586, "val_acc": 52.0}
{"epoch": 52, "training_loss": 2954.9441108703613, "training_acc": 53.0, "val_loss": 589.2258644104004, "val_acc": 48.0}
{"epoch": 53, "training_loss": 2778.653289794922, "training_acc": 48.0, "val_loss": 840.4895782470703, "val_acc": 48.0}
{"epoch": 54, "training_loss": 2105.17117023468, "training_acc": 57.0, "val_loss": 686.1956596374512, "val_acc": 52.0}
{"epoch": 55, "training_loss": 2738.077781677246, "training_acc": 53.0, "val_loss": 584.8323822021484, "val_acc": 52.0}
{"epoch": 56, "training_loss": 1731.9875202178955, "training_acc": 46.0, "val_loss": 481.89849853515625, "val_acc": 48.0}
{"epoch": 57, "training_loss": 1287.363130569458, "training_acc": 53.0, "val_loss": 403.18078994750977, "val_acc": 52.0}
{"epoch": 58, "training_loss": 1333.1211051940918, "training_acc": 53.0, "val_loss": 97.86726832389832, "val_acc": 60.0}
{"epoch": 59, "training_loss": 799.7303695678711, "training_acc": 66.0, "val_loss": 247.55821228027344, "val_acc": 48.0}
{"epoch": 60, "training_loss": 1621.3852920532227, "training_acc": 39.0, "val_loss": 504.03404235839844, "val_acc": 52.0}
{"epoch": 61, "training_loss": 1095.9355373382568, "training_acc": 59.0, "val_loss": 351.4585494995117, "val_acc": 48.0}
{"epoch": 62, "training_loss": 1354.0921611785889, "training_acc": 48.0, "val_loss": 455.9072017669678, "val_acc": 52.0}
{"epoch": 63, "training_loss": 1140.4029903411865, "training_acc": 55.0, "val_loss": 215.36149978637695, "val_acc": 56.0}
{"epoch": 64, "training_loss": 736.924690246582, "training_acc": 57.0, "val_loss": 105.88018894195557, "val_acc": 44.0}
{"epoch": 65, "training_loss": 787.6370735168457, "training_acc": 57.0, "val_loss": 390.22042751312256, "val_acc": 52.0}
{"epoch": 66, "training_loss": 979.7387065887451, "training_acc": 49.0, "val_loss": 225.33142566680908, "val_acc": 44.0}
{"epoch": 67, "training_loss": 942.9467849731445, "training_acc": 45.0, "val_loss": 216.982102394104, "val_acc": 60.0}
