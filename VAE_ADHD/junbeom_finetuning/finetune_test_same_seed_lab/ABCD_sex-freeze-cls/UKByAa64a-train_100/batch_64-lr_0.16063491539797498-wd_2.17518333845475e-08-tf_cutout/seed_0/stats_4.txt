"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 242.487398147583, "training_acc": 49.0, "val_loss": 60.121726989746094, "val_acc": 52.0}
{"epoch": 1, "training_loss": 255.49508380889893, "training_acc": 53.0, "val_loss": 21.903973817825317, "val_acc": 52.0}
{"epoch": 2, "training_loss": 159.6332550048828, "training_acc": 49.0, "val_loss": 57.59682059288025, "val_acc": 48.0}
{"epoch": 3, "training_loss": 181.91127824783325, "training_acc": 48.0, "val_loss": 49.87354874610901, "val_acc": 52.0}
{"epoch": 4, "training_loss": 172.2061686515808, "training_acc": 53.0, "val_loss": 18.531160056591034, "val_acc": 60.0}
{"epoch": 5, "training_loss": 104.03699064254761, "training_acc": 47.0, "val_loss": 23.577778041362762, "val_acc": 48.0}
{"epoch": 6, "training_loss": 93.34706139564514, "training_acc": 51.0, "val_loss": 37.17438280582428, "val_acc": 52.0}
{"epoch": 7, "training_loss": 119.52956819534302, "training_acc": 53.0, "val_loss": 26.54944658279419, "val_acc": 48.0}
{"epoch": 8, "training_loss": 106.19291806221008, "training_acc": 48.0, "val_loss": 17.401710152626038, "val_acc": 52.0}
{"epoch": 9, "training_loss": 78.77674889564514, "training_acc": 55.0, "val_loss": 23.312711715698242, "val_acc": 52.0}
{"epoch": 10, "training_loss": 80.47904896736145, "training_acc": 48.0, "val_loss": 21.39906883239746, "val_acc": 48.0}
{"epoch": 11, "training_loss": 80.84175491333008, "training_acc": 54.0, "val_loss": 22.510462999343872, "val_acc": 52.0}
{"epoch": 12, "training_loss": 75.85511112213135, "training_acc": 54.0, "val_loss": 17.344142496585846, "val_acc": 48.0}
{"epoch": 13, "training_loss": 65.19158053398132, "training_acc": 62.0, "val_loss": 17.616672813892365, "val_acc": 60.0}
{"epoch": 14, "training_loss": 69.81466341018677, "training_acc": 56.0, "val_loss": 17.686381936073303, "val_acc": 52.0}
{"epoch": 15, "training_loss": 61.474550008773804, "training_acc": 66.0, "val_loss": 17.448125779628754, "val_acc": 60.0}
{"epoch": 16, "training_loss": 66.84001612663269, "training_acc": 57.0, "val_loss": 17.925044894218445, "val_acc": 64.0}
{"epoch": 17, "training_loss": 68.49792432785034, "training_acc": 57.0, "val_loss": 18.290038406848907, "val_acc": 52.0}
{"epoch": 18, "training_loss": 66.7481758594513, "training_acc": 58.0, "val_loss": 17.178916931152344, "val_acc": 48.0}
{"epoch": 19, "training_loss": 67.56879734992981, "training_acc": 55.0, "val_loss": 19.38108205795288, "val_acc": 44.0}
{"epoch": 20, "training_loss": 70.37477993965149, "training_acc": 52.0, "val_loss": 21.740786731243134, "val_acc": 52.0}
{"epoch": 21, "training_loss": 74.00508499145508, "training_acc": 54.0, "val_loss": 17.630666494369507, "val_acc": 64.0}
{"epoch": 22, "training_loss": 69.34366703033447, "training_acc": 57.0, "val_loss": 18.37954819202423, "val_acc": 52.0}
{"epoch": 23, "training_loss": 65.96496033668518, "training_acc": 61.0, "val_loss": 18.957316875457764, "val_acc": 52.0}
{"epoch": 24, "training_loss": 67.16399931907654, "training_acc": 60.0, "val_loss": 17.370547354221344, "val_acc": 52.0}
{"epoch": 25, "training_loss": 63.903602838516235, "training_acc": 61.0, "val_loss": 17.426085472106934, "val_acc": 64.0}
{"epoch": 26, "training_loss": 63.43429684638977, "training_acc": 58.0, "val_loss": 17.07525998353958, "val_acc": 52.0}
{"epoch": 27, "training_loss": 62.73679327964783, "training_acc": 65.0, "val_loss": 17.051509022712708, "val_acc": 52.0}
{"epoch": 28, "training_loss": 61.03715491294861, "training_acc": 65.0, "val_loss": 17.906829714775085, "val_acc": 60.0}
{"epoch": 29, "training_loss": 74.82625269889832, "training_acc": 46.0, "val_loss": 17.899958789348602, "val_acc": 60.0}
{"epoch": 30, "training_loss": 63.145084619522095, "training_acc": 62.0, "val_loss": 17.426955699920654, "val_acc": 52.0}
{"epoch": 31, "training_loss": 62.708470821380615, "training_acc": 68.0, "val_loss": 19.222083687782288, "val_acc": 52.0}
{"epoch": 32, "training_loss": 65.20334315299988, "training_acc": 61.0, "val_loss": 21.226350963115692, "val_acc": 48.0}
{"epoch": 33, "training_loss": 72.12467694282532, "training_acc": 49.0, "val_loss": 25.342261791229248, "val_acc": 52.0}
{"epoch": 34, "training_loss": 91.80932474136353, "training_acc": 53.0, "val_loss": 23.184435069561005, "val_acc": 48.0}
{"epoch": 35, "training_loss": 94.4026665687561, "training_acc": 47.0, "val_loss": 21.017996966838837, "val_acc": 52.0}
{"epoch": 36, "training_loss": 82.85677528381348, "training_acc": 53.0, "val_loss": 17.050160467624664, "val_acc": 48.0}
{"epoch": 37, "training_loss": 72.90490746498108, "training_acc": 68.0, "val_loss": 17.510893940925598, "val_acc": 64.0}
{"epoch": 38, "training_loss": 84.96337795257568, "training_acc": 55.0, "val_loss": 18.91140341758728, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.26593828201294, "training_acc": 58.0, "val_loss": 21.754156053066254, "val_acc": 48.0}
{"epoch": 40, "training_loss": 68.44908165931702, "training_acc": 58.0, "val_loss": 24.47243630886078, "val_acc": 52.0}
{"epoch": 41, "training_loss": 76.08769869804382, "training_acc": 61.0, "val_loss": 23.37443083524704, "val_acc": 48.0}
{"epoch": 42, "training_loss": 77.93520379066467, "training_acc": 56.0, "val_loss": 22.073763608932495, "val_acc": 52.0}
{"epoch": 43, "training_loss": 78.73407053947449, "training_acc": 54.0, "val_loss": 23.104070127010345, "val_acc": 48.0}
{"epoch": 44, "training_loss": 85.38376307487488, "training_acc": 47.0, "val_loss": 20.07347196340561, "val_acc": 52.0}
{"epoch": 45, "training_loss": 68.98393154144287, "training_acc": 54.0, "val_loss": 17.237132787704468, "val_acc": 52.0}
{"epoch": 46, "training_loss": 69.75582504272461, "training_acc": 62.0, "val_loss": 16.854633390903473, "val_acc": 56.0}
{"epoch": 47, "training_loss": 77.8810715675354, "training_acc": 63.0, "val_loss": 16.734690964221954, "val_acc": 52.0}
{"epoch": 48, "training_loss": 66.03540539741516, "training_acc": 66.0, "val_loss": 17.025944590568542, "val_acc": 64.0}
{"epoch": 49, "training_loss": 64.21384477615356, "training_acc": 61.0, "val_loss": 19.269977509975433, "val_acc": 52.0}
{"epoch": 50, "training_loss": 70.46603846549988, "training_acc": 51.0, "val_loss": 17.521417140960693, "val_acc": 64.0}
{"epoch": 51, "training_loss": 62.31616187095642, "training_acc": 61.0, "val_loss": 18.413034081459045, "val_acc": 52.0}
{"epoch": 52, "training_loss": 61.262555837631226, "training_acc": 67.0, "val_loss": 17.477279901504517, "val_acc": 60.0}
{"epoch": 53, "training_loss": 61.46246361732483, "training_acc": 63.0, "val_loss": 17.552299797534943, "val_acc": 52.0}
{"epoch": 54, "training_loss": 59.17748403549194, "training_acc": 65.0, "val_loss": 16.926896572113037, "val_acc": 60.0}
{"epoch": 55, "training_loss": 57.41643691062927, "training_acc": 75.0, "val_loss": 19.880487024784088, "val_acc": 52.0}
{"epoch": 56, "training_loss": 59.10381293296814, "training_acc": 66.0, "val_loss": 17.90201961994171, "val_acc": 68.0}
{"epoch": 57, "training_loss": 60.84336876869202, "training_acc": 63.0, "val_loss": 18.767166137695312, "val_acc": 52.0}
{"epoch": 58, "training_loss": 60.02371644973755, "training_acc": 67.0, "val_loss": 17.17877686023712, "val_acc": 60.0}
{"epoch": 59, "training_loss": 59.970661640167236, "training_acc": 67.0, "val_loss": 16.881553828716278, "val_acc": 60.0}
{"epoch": 60, "training_loss": 54.894914388656616, "training_acc": 74.0, "val_loss": 17.624902725219727, "val_acc": 52.0}
{"epoch": 61, "training_loss": 59.122215270996094, "training_acc": 67.0, "val_loss": 16.78907424211502, "val_acc": 56.0}
{"epoch": 62, "training_loss": 55.921406507492065, "training_acc": 76.0, "val_loss": 17.401038110256195, "val_acc": 52.0}
{"epoch": 63, "training_loss": 59.27233839035034, "training_acc": 69.0, "val_loss": 16.819122433662415, "val_acc": 56.0}
{"epoch": 64, "training_loss": 55.403461933135986, "training_acc": 81.0, "val_loss": 18.35242062807083, "val_acc": 52.0}
{"epoch": 65, "training_loss": 55.47211992740631, "training_acc": 76.0, "val_loss": 17.797262966632843, "val_acc": 48.0}
{"epoch": 66, "training_loss": 55.28771710395813, "training_acc": 70.0, "val_loss": 16.989928483963013, "val_acc": 56.0}
