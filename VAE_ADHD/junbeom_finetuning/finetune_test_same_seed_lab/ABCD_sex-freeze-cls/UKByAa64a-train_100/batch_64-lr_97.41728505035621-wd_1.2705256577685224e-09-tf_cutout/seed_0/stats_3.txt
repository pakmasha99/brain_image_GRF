"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 207938.00402069092, "training_acc": 54.0, "val_loss": 43280.35583496094, "val_acc": 52.0}
{"epoch": 1, "training_loss": 252357.9658203125, "training_acc": 45.0, "val_loss": 101640.22216796875, "val_acc": 48.0}
{"epoch": 2, "training_loss": 368444.2705078125, "training_acc": 47.0, "val_loss": 19628.70635986328, "val_acc": 48.0}
{"epoch": 3, "training_loss": 124957.7607421875, "training_acc": 49.0, "val_loss": 91768.3837890625, "val_acc": 52.0}
{"epoch": 4, "training_loss": 338278.763671875, "training_acc": 53.0, "val_loss": 85537.8173828125, "val_acc": 52.0}
{"epoch": 5, "training_loss": 257447.7177734375, "training_acc": 53.0, "val_loss": 8173.246002197266, "val_acc": 48.0}
{"epoch": 6, "training_loss": 65823.70849609375, "training_acc": 58.0, "val_loss": 76516.54663085938, "val_acc": 48.0}
{"epoch": 7, "training_loss": 321340.1865234375, "training_acc": 47.0, "val_loss": 61898.211669921875, "val_acc": 48.0}
{"epoch": 8, "training_loss": 213294.09228515625, "training_acc": 47.0, "val_loss": 28300.6591796875, "val_acc": 52.0}
{"epoch": 9, "training_loss": 124130.1474609375, "training_acc": 54.0, "val_loss": 64976.910400390625, "val_acc": 52.0}
{"epoch": 10, "training_loss": 204984.9619140625, "training_acc": 53.0, "val_loss": 33342.03186035156, "val_acc": 52.0}
{"epoch": 11, "training_loss": 80355.75122070312, "training_acc": 53.0, "val_loss": 23054.925537109375, "val_acc": 48.0}
{"epoch": 12, "training_loss": 110723.79150390625, "training_acc": 47.0, "val_loss": 4147.212982177734, "val_acc": 40.0}
{"epoch": 13, "training_loss": 48949.152099609375, "training_acc": 52.0, "val_loss": 40694.573974609375, "val_acc": 52.0}
{"epoch": 14, "training_loss": 127201.34521484375, "training_acc": 53.0, "val_loss": 17671.612548828125, "val_acc": 52.0}
{"epoch": 15, "training_loss": 47754.260498046875, "training_acc": 54.0, "val_loss": 27692.919921875, "val_acc": 48.0}
{"epoch": 16, "training_loss": 109735.93798828125, "training_acc": 47.0, "val_loss": 2235.4360580444336, "val_acc": 56.0}
{"epoch": 17, "training_loss": 39354.6572265625, "training_acc": 56.0, "val_loss": 32523.953247070312, "val_acc": 52.0}
{"epoch": 18, "training_loss": 98942.48681640625, "training_acc": 53.0, "val_loss": 1408.6665153503418, "val_acc": 64.0}
{"epoch": 19, "training_loss": 32285.754638671875, "training_acc": 52.0, "val_loss": 9017.459106445312, "val_acc": 48.0}
{"epoch": 20, "training_loss": 39582.611572265625, "training_acc": 51.0, "val_loss": 16217.793273925781, "val_acc": 52.0}
{"epoch": 21, "training_loss": 45473.985900878906, "training_acc": 54.0, "val_loss": 14020.419311523438, "val_acc": 48.0}
{"epoch": 22, "training_loss": 59596.857177734375, "training_acc": 46.0, "val_loss": 1484.2934608459473, "val_acc": 64.0}
{"epoch": 23, "training_loss": 38926.7255859375, "training_acc": 53.0, "val_loss": 19705.491638183594, "val_acc": 52.0}
{"epoch": 24, "training_loss": 54807.1083984375, "training_acc": 58.0, "val_loss": 16944.027709960938, "val_acc": 48.0}
{"epoch": 25, "training_loss": 80176.14794921875, "training_acc": 47.0, "val_loss": 7641.6015625, "val_acc": 48.0}
{"epoch": 26, "training_loss": 61378.80078125, "training_acc": 38.0, "val_loss": 30799.90234375, "val_acc": 52.0}
{"epoch": 27, "training_loss": 98051.95361328125, "training_acc": 53.0, "val_loss": 3649.3186950683594, "val_acc": 48.0}
{"epoch": 28, "training_loss": 34611.16552734375, "training_acc": 61.0, "val_loss": 19722.274780273438, "val_acc": 48.0}
{"epoch": 29, "training_loss": 64341.930603027344, "training_acc": 47.0, "val_loss": 24711.827087402344, "val_acc": 52.0}
{"epoch": 30, "training_loss": 86273.13232421875, "training_acc": 53.0, "val_loss": 32514.190673828125, "val_acc": 52.0}
{"epoch": 31, "training_loss": 81448.45434570312, "training_acc": 53.0, "val_loss": 12350.013732910156, "val_acc": 48.0}
{"epoch": 32, "training_loss": 83060.84130859375, "training_acc": 47.0, "val_loss": 19931.613159179688, "val_acc": 48.0}
{"epoch": 33, "training_loss": 65488.53706359863, "training_acc": 53.0, "val_loss": 19307.102966308594, "val_acc": 52.0}
{"epoch": 34, "training_loss": 53276.940673828125, "training_acc": 54.0, "val_loss": 4744.105529785156, "val_acc": 52.0}
{"epoch": 35, "training_loss": 26896.39404296875, "training_acc": 67.0, "val_loss": 16267.872619628906, "val_acc": 48.0}
{"epoch": 36, "training_loss": 43332.46176147461, "training_acc": 51.0, "val_loss": 21268.348693847656, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69853.85083007812, "training_acc": 53.0, "val_loss": 18864.892578125, "val_acc": 52.0}
