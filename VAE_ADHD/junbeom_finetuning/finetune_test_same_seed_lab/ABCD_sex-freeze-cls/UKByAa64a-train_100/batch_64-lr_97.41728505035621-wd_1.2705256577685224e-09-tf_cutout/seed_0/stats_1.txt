"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 204645.31731033325, "training_acc": 46.0, "val_loss": 42766.68701171875, "val_acc": 52.0}
{"epoch": 1, "training_loss": 207669.880859375, "training_acc": 53.0, "val_loss": 110755.55419921875, "val_acc": 48.0}
{"epoch": 2, "training_loss": 416829.328125, "training_acc": 47.0, "val_loss": 30701.318359375, "val_acc": 48.0}
{"epoch": 3, "training_loss": 145245.45166015625, "training_acc": 51.0, "val_loss": 80886.06567382812, "val_acc": 52.0}
{"epoch": 4, "training_loss": 307309.5615234375, "training_acc": 53.0, "val_loss": 72177.06298828125, "val_acc": 52.0}
{"epoch": 5, "training_loss": 214216.298828125, "training_acc": 53.0, "val_loss": 12540.765380859375, "val_acc": 48.0}
{"epoch": 6, "training_loss": 115586.1484375, "training_acc": 47.0, "val_loss": 36968.68591308594, "val_acc": 48.0}
{"epoch": 7, "training_loss": 125308.65747070312, "training_acc": 47.0, "val_loss": 26787.307739257812, "val_acc": 52.0}
{"epoch": 8, "training_loss": 104451.0234375, "training_acc": 53.0, "val_loss": 46448.01330566406, "val_acc": 52.0}
{"epoch": 9, "training_loss": 150823.84399414062, "training_acc": 53.0, "val_loss": 5853.029632568359, "val_acc": 52.0}
{"epoch": 10, "training_loss": 60322.2861328125, "training_acc": 49.0, "val_loss": 43187.17041015625, "val_acc": 48.0}
{"epoch": 11, "training_loss": 165471.3349609375, "training_acc": 47.0, "val_loss": 9161.17935180664, "val_acc": 48.0}
{"epoch": 12, "training_loss": 59953.9541015625, "training_acc": 49.0, "val_loss": 45444.64111328125, "val_acc": 52.0}
{"epoch": 13, "training_loss": 174721.8466796875, "training_acc": 53.0, "val_loss": 34068.19152832031, "val_acc": 52.0}
{"epoch": 14, "training_loss": 91457.03497314453, "training_acc": 52.0, "val_loss": 33410.94970703125, "val_acc": 48.0}
{"epoch": 15, "training_loss": 162778.314453125, "training_acc": 47.0, "val_loss": 49695.721435546875, "val_acc": 48.0}
{"epoch": 16, "training_loss": 167670.92626953125, "training_acc": 47.0, "val_loss": 4519.075393676758, "val_acc": 60.0}
{"epoch": 17, "training_loss": 42012.363525390625, "training_acc": 54.0, "val_loss": 28812.393188476562, "val_acc": 52.0}
{"epoch": 18, "training_loss": 92991.76989746094, "training_acc": 53.0, "val_loss": 5336.497116088867, "val_acc": 52.0}
{"epoch": 19, "training_loss": 26583.929321289062, "training_acc": 54.0, "val_loss": 2224.3637084960938, "val_acc": 48.0}
{"epoch": 20, "training_loss": 27717.9404296875, "training_acc": 56.0, "val_loss": 12137.403869628906, "val_acc": 52.0}
{"epoch": 21, "training_loss": 27538.94451904297, "training_acc": 56.0, "val_loss": 7279.176330566406, "val_acc": 48.0}
{"epoch": 22, "training_loss": 22769.834106445312, "training_acc": 54.0, "val_loss": 10291.341400146484, "val_acc": 52.0}
{"epoch": 23, "training_loss": 20195.85272216797, "training_acc": 60.0, "val_loss": 11745.164489746094, "val_acc": 48.0}
{"epoch": 24, "training_loss": 41786.46862792969, "training_acc": 48.0, "val_loss": 10740.823364257812, "val_acc": 52.0}
{"epoch": 25, "training_loss": 37362.11975097656, "training_acc": 56.0, "val_loss": 4606.909561157227, "val_acc": 56.0}
{"epoch": 26, "training_loss": 37005.41748046875, "training_acc": 51.0, "val_loss": 13417.301940917969, "val_acc": 48.0}
{"epoch": 27, "training_loss": 40696.91229248047, "training_acc": 51.0, "val_loss": 9223.06137084961, "val_acc": 56.0}
{"epoch": 28, "training_loss": 19866.563079833984, "training_acc": 56.0, "val_loss": 6820.777130126953, "val_acc": 48.0}
{"epoch": 29, "training_loss": 18782.850524902344, "training_acc": 60.0, "val_loss": 14456.781005859375, "val_acc": 52.0}
{"epoch": 30, "training_loss": 44418.851806640625, "training_acc": 53.0, "val_loss": 2958.426284790039, "val_acc": 36.0}
{"epoch": 31, "training_loss": 18501.83837890625, "training_acc": 60.0, "val_loss": 7716.2628173828125, "val_acc": 56.0}
{"epoch": 32, "training_loss": 19493.83868408203, "training_acc": 58.0, "val_loss": 5208.314514160156, "val_acc": 48.0}
{"epoch": 33, "training_loss": 21105.510681152344, "training_acc": 47.0, "val_loss": 9745.641326904297, "val_acc": 52.0}
{"epoch": 34, "training_loss": 21674.948364257812, "training_acc": 55.0, "val_loss": 10099.658203125, "val_acc": 48.0}
{"epoch": 35, "training_loss": 44244.10729980469, "training_acc": 47.0, "val_loss": 8787.609100341797, "val_acc": 52.0}
{"epoch": 36, "training_loss": 26073.82470703125, "training_acc": 54.0, "val_loss": 1248.7858772277832, "val_acc": 56.0}
{"epoch": 37, "training_loss": 12554.481384277344, "training_acc": 61.0, "val_loss": 8890.423583984375, "val_acc": 52.0}
{"epoch": 38, "training_loss": 20638.35076904297, "training_acc": 54.0, "val_loss": 8620.767211914062, "val_acc": 48.0}
{"epoch": 39, "training_loss": 29094.784423828125, "training_acc": 48.0, "val_loss": 14952.45361328125, "val_acc": 52.0}
{"epoch": 40, "training_loss": 54160.71826171875, "training_acc": 53.0, "val_loss": 5681.913757324219, "val_acc": 52.0}
{"epoch": 41, "training_loss": 36158.54248046875, "training_acc": 53.0, "val_loss": 20605.043029785156, "val_acc": 48.0}
{"epoch": 42, "training_loss": 54370.334228515625, "training_acc": 49.0, "val_loss": 22534.292602539062, "val_acc": 52.0}
{"epoch": 43, "training_loss": 104456.68359375, "training_acc": 53.0, "val_loss": 29020.663452148438, "val_acc": 52.0}
{"epoch": 44, "training_loss": 78934.509765625, "training_acc": 53.0, "val_loss": 25849.957275390625, "val_acc": 48.0}
{"epoch": 45, "training_loss": 126289.79833984375, "training_acc": 47.0, "val_loss": 36840.00549316406, "val_acc": 48.0}
{"epoch": 46, "training_loss": 114137.29931640625, "training_acc": 47.0, "val_loss": 22170.120239257812, "val_acc": 52.0}
{"epoch": 47, "training_loss": 100691.56884765625, "training_acc": 53.0, "val_loss": 37254.36706542969, "val_acc": 52.0}
{"epoch": 48, "training_loss": 123278.93676757812, "training_acc": 53.0, "val_loss": 4918.683624267578, "val_acc": 52.0}
{"epoch": 49, "training_loss": 37183.324951171875, "training_acc": 57.0, "val_loss": 15618.557739257812, "val_acc": 48.0}
{"epoch": 50, "training_loss": 33448.966217041016, "training_acc": 59.0, "val_loss": 12624.259185791016, "val_acc": 52.0}
{"epoch": 51, "training_loss": 39204.71875, "training_acc": 56.0, "val_loss": 10780.513763427734, "val_acc": 48.0}
{"epoch": 52, "training_loss": 42650.796875, "training_acc": 46.0, "val_loss": 2955.925941467285, "val_acc": 68.0}
{"epoch": 53, "training_loss": 16974.450561523438, "training_acc": 62.0, "val_loss": 1554.0131568908691, "val_acc": 56.0}
{"epoch": 54, "training_loss": 10595.945343017578, "training_acc": 59.0, "val_loss": 3441.849136352539, "val_acc": 64.0}
{"epoch": 55, "training_loss": 7147.179420471191, "training_acc": 67.0, "val_loss": 2831.0306549072266, "val_acc": 48.0}
