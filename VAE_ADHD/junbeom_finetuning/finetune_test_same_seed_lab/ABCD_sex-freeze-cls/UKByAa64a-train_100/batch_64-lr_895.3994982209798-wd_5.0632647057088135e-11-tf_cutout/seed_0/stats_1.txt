"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 1880609.6200447083, "training_acc": 46.0, "val_loss": 393086.181640625, "val_acc": 52.0}
{"epoch": 1, "training_loss": 1908770.9921875, "training_acc": 53.0, "val_loss": 1017995.60546875, "val_acc": 48.0}
{"epoch": 2, "training_loss": 3831236.515625, "training_acc": 47.0, "val_loss": 282186.5966796875, "val_acc": 48.0}
{"epoch": 3, "training_loss": 1335007.2890625, "training_acc": 51.0, "val_loss": 743456.005859375, "val_acc": 52.0}
{"epoch": 4, "training_loss": 2824599.5859375, "training_acc": 53.0, "val_loss": 663408.251953125, "val_acc": 52.0}
{"epoch": 5, "training_loss": 1968944.36328125, "training_acc": 53.0, "val_loss": 115265.88134765625, "val_acc": 48.0}
{"epoch": 6, "training_loss": 1062394.953125, "training_acc": 47.0, "val_loss": 339792.3095703125, "val_acc": 48.0}
{"epoch": 7, "training_loss": 1151760.1875, "training_acc": 47.0, "val_loss": 246213.623046875, "val_acc": 52.0}
{"epoch": 8, "training_loss": 960049.76953125, "training_acc": 53.0, "val_loss": 426922.65625, "val_acc": 52.0}
{"epoch": 9, "training_loss": 1386279.76953125, "training_acc": 53.0, "val_loss": 53798.712158203125, "val_acc": 52.0}
{"epoch": 10, "training_loss": 554446.42578125, "training_acc": 49.0, "val_loss": 396948.6572265625, "val_acc": 48.0}
{"epoch": 11, "training_loss": 1520909.93359375, "training_acc": 47.0, "val_loss": 84202.77709960938, "val_acc": 48.0}
{"epoch": 12, "training_loss": 551057.7890625, "training_acc": 49.0, "val_loss": 417700.390625, "val_acc": 52.0}
{"epoch": 13, "training_loss": 1605935.3125, "training_acc": 53.0, "val_loss": 313135.107421875, "val_acc": 52.0}
{"epoch": 14, "training_loss": 840616.9306640625, "training_acc": 52.0, "val_loss": 307072.998046875, "val_acc": 48.0}
{"epoch": 15, "training_loss": 1496055.53125, "training_acc": 47.0, "val_loss": 456719.7265625, "val_acc": 48.0}
{"epoch": 16, "training_loss": 1540903.1328125, "training_acc": 47.0, "val_loss": 41598.03161621094, "val_acc": 60.0}
{"epoch": 17, "training_loss": 382643.7421875, "training_acc": 54.0, "val_loss": 259732.861328125, "val_acc": 52.0}
{"epoch": 18, "training_loss": 833800.044921875, "training_acc": 53.0, "val_loss": 58831.93359375, "val_acc": 48.0}
{"epoch": 19, "training_loss": 259013.154296875, "training_acc": 50.0, "val_loss": 28132.699584960938, "val_acc": 56.0}
{"epoch": 20, "training_loss": 189179.36328125, "training_acc": 60.0, "val_loss": 43586.84387207031, "val_acc": 60.0}
{"epoch": 21, "training_loss": 162744.5673828125, "training_acc": 55.0, "val_loss": 46799.3896484375, "val_acc": 44.0}
{"epoch": 22, "training_loss": 226326.498046875, "training_acc": 55.0, "val_loss": 123556.82373046875, "val_acc": 52.0}
{"epoch": 23, "training_loss": 231423.7618408203, "training_acc": 62.0, "val_loss": 121377.63671875, "val_acc": 48.0}
{"epoch": 24, "training_loss": 449072.091796875, "training_acc": 48.0, "val_loss": 99102.734375, "val_acc": 52.0}
{"epoch": 25, "training_loss": 320697.796875, "training_acc": 55.0, "val_loss": 44237.29553222656, "val_acc": 60.0}
{"epoch": 26, "training_loss": 328430.470703125, "training_acc": 52.0, "val_loss": 108814.306640625, "val_acc": 48.0}
{"epoch": 27, "training_loss": 384540.14208984375, "training_acc": 49.0, "val_loss": 102408.0078125, "val_acc": 52.0}
{"epoch": 28, "training_loss": 205315.12573242188, "training_acc": 58.0, "val_loss": 66380.2734375, "val_acc": 48.0}
{"epoch": 29, "training_loss": 210901.99951171875, "training_acc": 58.0, "val_loss": 166864.78271484375, "val_acc": 52.0}
{"epoch": 30, "training_loss": 571695.240234375, "training_acc": 53.0, "val_loss": 77564.19067382812, "val_acc": 56.0}
{"epoch": 31, "training_loss": 371775.712890625, "training_acc": 52.0, "val_loss": 168739.9658203125, "val_acc": 48.0}
{"epoch": 32, "training_loss": 506480.7744140625, "training_acc": 51.0, "val_loss": 189949.08447265625, "val_acc": 52.0}
{"epoch": 33, "training_loss": 758161.3046875, "training_acc": 53.0, "val_loss": 173417.9443359375, "val_acc": 52.0}
{"epoch": 34, "training_loss": 321120.4470214844, "training_acc": 66.0, "val_loss": 144478.5400390625, "val_acc": 48.0}
{"epoch": 35, "training_loss": 530658.91796875, "training_acc": 47.0, "val_loss": 91306.0791015625, "val_acc": 52.0}
{"epoch": 36, "training_loss": 315840.375, "training_acc": 55.0, "val_loss": 51155.06591796875, "val_acc": 60.0}
{"epoch": 37, "training_loss": 230180.814453125, "training_acc": 49.0, "val_loss": 85356.56127929688, "val_acc": 48.0}
{"epoch": 38, "training_loss": 310844.03125, "training_acc": 48.0, "val_loss": 98900.89111328125, "val_acc": 52.0}
