"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 268136.61765289307, "training_acc": 53.0, "val_loss": 754610.546875, "val_acc": 48.0}
{"epoch": 1, "training_loss": 3046422.765625, "training_acc": 47.0, "val_loss": 59181.878662109375, "val_acc": 44.0}
{"epoch": 2, "training_loss": 513461.3984375, "training_acc": 68.0, "val_loss": 735661.279296875, "val_acc": 52.0}
{"epoch": 3, "training_loss": 2289299.6015625, "training_acc": 53.0, "val_loss": 199182.11669921875, "val_acc": 52.0}
{"epoch": 4, "training_loss": 982563.46875, "training_acc": 48.0, "val_loss": 571701.5625, "val_acc": 48.0}
{"epoch": 5, "training_loss": 2318960.1171875, "training_acc": 47.0, "val_loss": 164837.95166015625, "val_acc": 48.0}
{"epoch": 6, "training_loss": 801886.30078125, "training_acc": 51.0, "val_loss": 538849.658203125, "val_acc": 52.0}
{"epoch": 7, "training_loss": 1869146.26953125, "training_acc": 53.0, "val_loss": 417213.671875, "val_acc": 52.0}
{"epoch": 8, "training_loss": 1020103.9677734375, "training_acc": 54.0, "val_loss": 271765.673828125, "val_acc": 48.0}
{"epoch": 9, "training_loss": 1392056.140625, "training_acc": 47.0, "val_loss": 387828.8330078125, "val_acc": 48.0}
{"epoch": 10, "training_loss": 1367415.87109375, "training_acc": 47.0, "val_loss": 176458.3984375, "val_acc": 52.0}
{"epoch": 11, "training_loss": 639489.01953125, "training_acc": 55.0, "val_loss": 362223.876953125, "val_acc": 52.0}
{"epoch": 12, "training_loss": 1008155.57421875, "training_acc": 53.0, "val_loss": 47875.396728515625, "val_acc": 44.0}
{"epoch": 13, "training_loss": 335016.4140625, "training_acc": 58.0, "val_loss": 110456.591796875, "val_acc": 52.0}
{"epoch": 14, "training_loss": 512157.83447265625, "training_acc": 58.0, "val_loss": 198745.03173828125, "val_acc": 52.0}
{"epoch": 15, "training_loss": 469820.47265625, "training_acc": 55.0, "val_loss": 61636.358642578125, "val_acc": 48.0}
{"epoch": 16, "training_loss": 250370.8251953125, "training_acc": 56.0, "val_loss": 74969.96459960938, "val_acc": 52.0}
{"epoch": 17, "training_loss": 315581.84375, "training_acc": 55.0, "val_loss": 153385.55908203125, "val_acc": 52.0}
{"epoch": 18, "training_loss": 267668.099609375, "training_acc": 56.0, "val_loss": 68665.04516601562, "val_acc": 52.0}
{"epoch": 19, "training_loss": 345456.658203125, "training_acc": 51.0, "val_loss": 99317.1875, "val_acc": 48.0}
{"epoch": 20, "training_loss": 302498.4765625, "training_acc": 55.0, "val_loss": 75813.18359375, "val_acc": 48.0}
{"epoch": 21, "training_loss": 307881.40625, "training_acc": 57.0, "val_loss": 75024.91455078125, "val_acc": 52.0}
{"epoch": 22, "training_loss": 313678.9931640625, "training_acc": 55.0, "val_loss": 129333.21533203125, "val_acc": 52.0}
{"epoch": 23, "training_loss": 309066.64111328125, "training_acc": 54.0, "val_loss": 111384.326171875, "val_acc": 48.0}
{"epoch": 24, "training_loss": 420539.9423828125, "training_acc": 48.0, "val_loss": 112207.36083984375, "val_acc": 52.0}
{"epoch": 25, "training_loss": 316092.720703125, "training_acc": 54.0, "val_loss": 21841.24298095703, "val_acc": 52.0}
{"epoch": 26, "training_loss": 158482.3046875, "training_acc": 57.0, "val_loss": 23999.59259033203, "val_acc": 48.0}
{"epoch": 27, "training_loss": 167244.158203125, "training_acc": 61.0, "val_loss": 52712.70751953125, "val_acc": 52.0}
{"epoch": 28, "training_loss": 218731.4375, "training_acc": 59.0, "val_loss": 38162.14904785156, "val_acc": 52.0}
{"epoch": 29, "training_loss": 195158.7666015625, "training_acc": 61.0, "val_loss": 164449.13330078125, "val_acc": 52.0}
{"epoch": 30, "training_loss": 337879.6953125, "training_acc": 55.0, "val_loss": 70469.68383789062, "val_acc": 48.0}
{"epoch": 31, "training_loss": 239244.15979003906, "training_acc": 55.0, "val_loss": 105390.8935546875, "val_acc": 52.0}
{"epoch": 32, "training_loss": 173785.24365234375, "training_acc": 56.0, "val_loss": 150060.41259765625, "val_acc": 48.0}
{"epoch": 33, "training_loss": 634261.25390625, "training_acc": 47.0, "val_loss": 18255.911254882812, "val_acc": 48.0}
{"epoch": 34, "training_loss": 211047.837890625, "training_acc": 64.0, "val_loss": 218080.56640625, "val_acc": 52.0}
{"epoch": 35, "training_loss": 519225.72705078125, "training_acc": 53.0, "val_loss": 156524.8046875, "val_acc": 48.0}
{"epoch": 36, "training_loss": 719419.33203125, "training_acc": 47.0, "val_loss": 84211.21826171875, "val_acc": 48.0}
{"epoch": 37, "training_loss": 409647.888671875, "training_acc": 53.0, "val_loss": 280109.5947265625, "val_acc": 52.0}
{"epoch": 38, "training_loss": 934014.87890625, "training_acc": 53.0, "val_loss": 35955.81970214844, "val_acc": 52.0}
{"epoch": 39, "training_loss": 272825.419921875, "training_acc": 65.0, "val_loss": 276743.7744140625, "val_acc": 48.0}
{"epoch": 40, "training_loss": 920807.125, "training_acc": 48.0, "val_loss": 56310.546875, "val_acc": 52.0}
{"epoch": 41, "training_loss": 355629.91015625, "training_acc": 56.0, "val_loss": 81172.68676757812, "val_acc": 52.0}
{"epoch": 42, "training_loss": 316740.9658203125, "training_acc": 58.0, "val_loss": 163425.341796875, "val_acc": 48.0}
{"epoch": 43, "training_loss": 461714.623046875, "training_acc": 46.0, "val_loss": 197592.27294921875, "val_acc": 52.0}
{"epoch": 44, "training_loss": 782537.02734375, "training_acc": 53.0, "val_loss": 207712.6220703125, "val_acc": 52.0}
{"epoch": 45, "training_loss": 466291.99462890625, "training_acc": 58.0, "val_loss": 147662.744140625, "val_acc": 48.0}
{"epoch": 46, "training_loss": 561129.705078125, "training_acc": 47.0, "val_loss": 101321.34399414062, "val_acc": 52.0}
{"epoch": 47, "training_loss": 262368.669921875, "training_acc": 56.0, "val_loss": 69275.28076171875, "val_acc": 52.0}
{"epoch": 48, "training_loss": 170302.201171875, "training_acc": 66.0, "val_loss": 41355.963134765625, "val_acc": 52.0}
{"epoch": 49, "training_loss": 211114.94580078125, "training_acc": 57.0, "val_loss": 189426.67236328125, "val_acc": 52.0}
{"epoch": 50, "training_loss": 311043.92236328125, "training_acc": 58.0, "val_loss": 85510.57739257812, "val_acc": 48.0}
{"epoch": 51, "training_loss": 418542.990234375, "training_acc": 48.0, "val_loss": 109668.2861328125, "val_acc": 52.0}
{"epoch": 52, "training_loss": 236897.3125, "training_acc": 56.0, "val_loss": 107895.4345703125, "val_acc": 52.0}
