"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 2868.698684692383, "training_acc": 49.0, "val_loss": 373.76928329467773, "val_acc": 48.0}
{"epoch": 1, "training_loss": 1925.249397277832, "training_acc": 55.0, "val_loss": 1515.230655670166, "val_acc": 52.0}
{"epoch": 2, "training_loss": 5935.342056274414, "training_acc": 53.0, "val_loss": 1198.278522491455, "val_acc": 52.0}
{"epoch": 3, "training_loss": 3750.247589111328, "training_acc": 53.0, "val_loss": 95.22671103477478, "val_acc": 48.0}
{"epoch": 4, "training_loss": 943.3083724975586, "training_acc": 50.0, "val_loss": 560.1693153381348, "val_acc": 48.0}
{"epoch": 5, "training_loss": 1841.0264167785645, "training_acc": 47.0, "val_loss": 126.319420337677, "val_acc": 52.0}
{"epoch": 6, "training_loss": 664.2016067504883, "training_acc": 53.0, "val_loss": 220.82383632659912, "val_acc": 52.0}
{"epoch": 7, "training_loss": 581.5441145896912, "training_acc": 54.0, "val_loss": 267.19067096710205, "val_acc": 48.0}
{"epoch": 8, "training_loss": 1002.8768463134766, "training_acc": 47.0, "val_loss": 38.096216320991516, "val_acc": 44.0}
{"epoch": 9, "training_loss": 455.05213165283203, "training_acc": 56.0, "val_loss": 313.5737895965576, "val_acc": 52.0}
{"epoch": 10, "training_loss": 1068.5080528259277, "training_acc": 53.0, "val_loss": 102.76930332183838, "val_acc": 48.0}
{"epoch": 11, "training_loss": 477.6072483062744, "training_acc": 49.0, "val_loss": 42.56330728530884, "val_acc": 44.0}
{"epoch": 12, "training_loss": 362.18982696533203, "training_acc": 52.0, "val_loss": 188.03887367248535, "val_acc": 52.0}
{"epoch": 13, "training_loss": 517.2738661766052, "training_acc": 54.0, "val_loss": 260.7919931411743, "val_acc": 48.0}
{"epoch": 14, "training_loss": 1093.2939834594727, "training_acc": 47.0, "val_loss": 171.73559665679932, "val_acc": 48.0}
{"epoch": 15, "training_loss": 705.0284404754639, "training_acc": 42.0, "val_loss": 257.3725938796997, "val_acc": 52.0}
{"epoch": 16, "training_loss": 917.2144412994385, "training_acc": 53.0, "val_loss": 55.35665154457092, "val_acc": 44.0}
{"epoch": 17, "training_loss": 298.56970405578613, "training_acc": 55.0, "val_loss": 47.72997796535492, "val_acc": 44.0}
{"epoch": 18, "training_loss": 306.183313369751, "training_acc": 56.0, "val_loss": 156.0541033744812, "val_acc": 52.0}
{"epoch": 19, "training_loss": 460.3136134147644, "training_acc": 54.0, "val_loss": 152.02282667160034, "val_acc": 48.0}
{"epoch": 20, "training_loss": 453.6639575958252, "training_acc": 48.0, "val_loss": 132.3744297027588, "val_acc": 52.0}
{"epoch": 21, "training_loss": 534.8984680175781, "training_acc": 53.0, "val_loss": 54.714787006378174, "val_acc": 48.0}
{"epoch": 22, "training_loss": 382.61927795410156, "training_acc": 48.0, "val_loss": 199.70916509628296, "val_acc": 48.0}
{"epoch": 23, "training_loss": 585.9106774330139, "training_acc": 51.0, "val_loss": 132.80903100967407, "val_acc": 52.0}
{"epoch": 24, "training_loss": 428.7934112548828, "training_acc": 54.0, "val_loss": 93.51832270622253, "val_acc": 48.0}
{"epoch": 25, "training_loss": 350.0545988082886, "training_acc": 48.0, "val_loss": 47.21005856990814, "val_acc": 48.0}
{"epoch": 26, "training_loss": 210.070086479187, "training_acc": 61.0, "val_loss": 34.22531485557556, "val_acc": 56.0}
{"epoch": 27, "training_loss": 247.96056747436523, "training_acc": 57.0, "val_loss": 62.67735958099365, "val_acc": 48.0}
{"epoch": 28, "training_loss": 270.4648017883301, "training_acc": 57.0, "val_loss": 150.44561624526978, "val_acc": 52.0}
{"epoch": 29, "training_loss": 414.00485920906067, "training_acc": 54.0, "val_loss": 120.97622156143188, "val_acc": 48.0}
{"epoch": 30, "training_loss": 326.91836881637573, "training_acc": 50.0, "val_loss": 142.3240303993225, "val_acc": 52.0}
{"epoch": 31, "training_loss": 527.6548709869385, "training_acc": 53.0, "val_loss": 22.79650866985321, "val_acc": 56.0}
{"epoch": 32, "training_loss": 226.309720993042, "training_acc": 61.0, "val_loss": 20.07114142179489, "val_acc": 68.0}
{"epoch": 33, "training_loss": 162.64194011688232, "training_acc": 61.0, "val_loss": 24.33408349752426, "val_acc": 48.0}
{"epoch": 34, "training_loss": 104.77621650695801, "training_acc": 64.0, "val_loss": 56.46141171455383, "val_acc": 48.0}
{"epoch": 35, "training_loss": 195.5802173614502, "training_acc": 61.0, "val_loss": 71.22772932052612, "val_acc": 52.0}
{"epoch": 36, "training_loss": 319.1433095932007, "training_acc": 50.0, "val_loss": 58.73275399208069, "val_acc": 48.0}
{"epoch": 37, "training_loss": 425.49606704711914, "training_acc": 40.0, "val_loss": 137.50828504562378, "val_acc": 52.0}
{"epoch": 38, "training_loss": 271.6461420059204, "training_acc": 69.0, "val_loss": 150.62735080718994, "val_acc": 48.0}
{"epoch": 39, "training_loss": 508.16950035095215, "training_acc": 48.0, "val_loss": 164.27505016326904, "val_acc": 52.0}
{"epoch": 40, "training_loss": 682.7415542602539, "training_acc": 53.0, "val_loss": 145.63369750976562, "val_acc": 52.0}
{"epoch": 41, "training_loss": 352.0831456184387, "training_acc": 57.0, "val_loss": 141.06072187423706, "val_acc": 48.0}
{"epoch": 42, "training_loss": 323.0456397533417, "training_acc": 56.0, "val_loss": 110.11333465576172, "val_acc": 52.0}
{"epoch": 43, "training_loss": 333.0617666244507, "training_acc": 53.0, "val_loss": 134.7289800643921, "val_acc": 48.0}
{"epoch": 44, "training_loss": 465.3779468536377, "training_acc": 49.0, "val_loss": 60.434722900390625, "val_acc": 52.0}
{"epoch": 45, "training_loss": 279.4697074890137, "training_acc": 54.0, "val_loss": 44.000330567359924, "val_acc": 44.0}
{"epoch": 46, "training_loss": 157.7155909538269, "training_acc": 61.0, "val_loss": 66.27514362335205, "val_acc": 52.0}
{"epoch": 47, "training_loss": 193.9615831375122, "training_acc": 57.0, "val_loss": 133.9218258857727, "val_acc": 48.0}
{"epoch": 48, "training_loss": 416.05212211608887, "training_acc": 47.0, "val_loss": 111.75185441970825, "val_acc": 52.0}
{"epoch": 49, "training_loss": 428.4469299316406, "training_acc": 53.0, "val_loss": 31.768983602523804, "val_acc": 60.0}
{"epoch": 50, "training_loss": 207.63177394866943, "training_acc": 57.0, "val_loss": 69.13943886756897, "val_acc": 44.0}
{"epoch": 51, "training_loss": 315.9665355682373, "training_acc": 50.0, "val_loss": 160.62335968017578, "val_acc": 52.0}
