"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 246.9338150024414, "training_acc": 46.0, "val_loss": 42.312437295913696, "val_acc": 52.0}
{"epoch": 1, "training_loss": 206.81776809692383, "training_acc": 53.0, "val_loss": 106.62907361984253, "val_acc": 48.0}
{"epoch": 2, "training_loss": 394.78781509399414, "training_acc": 47.0, "val_loss": 25.700518488883972, "val_acc": 48.0}
{"epoch": 3, "training_loss": 138.11159706115723, "training_acc": 51.0, "val_loss": 83.57904553413391, "val_acc": 52.0}
{"epoch": 4, "training_loss": 315.03332901000977, "training_acc": 53.0, "val_loss": 68.20414662361145, "val_acc": 52.0}
{"epoch": 5, "training_loss": 199.46729803085327, "training_acc": 53.0, "val_loss": 25.150248408317566, "val_acc": 48.0}
{"epoch": 6, "training_loss": 163.774920463562, "training_acc": 47.0, "val_loss": 54.799485206604004, "val_acc": 48.0}
{"epoch": 7, "training_loss": 196.22942543029785, "training_acc": 47.0, "val_loss": 17.598117887973785, "val_acc": 52.0}
{"epoch": 8, "training_loss": 77.98203611373901, "training_acc": 57.0, "val_loss": 49.28875267505646, "val_acc": 52.0}
{"epoch": 9, "training_loss": 178.65041208267212, "training_acc": 53.0, "val_loss": 29.235076904296875, "val_acc": 52.0}
{"epoch": 10, "training_loss": 98.24274969100952, "training_acc": 49.0, "val_loss": 29.620295763015747, "val_acc": 48.0}
{"epoch": 11, "training_loss": 120.83651781082153, "training_acc": 47.0, "val_loss": 23.137134313583374, "val_acc": 48.0}
{"epoch": 12, "training_loss": 83.50540471076965, "training_acc": 52.0, "val_loss": 25.492969155311584, "val_acc": 52.0}
{"epoch": 13, "training_loss": 104.30667972564697, "training_acc": 53.0, "val_loss": 21.503035724163055, "val_acc": 52.0}
{"epoch": 14, "training_loss": 81.50897669792175, "training_acc": 47.0, "val_loss": 23.687182366847992, "val_acc": 48.0}
{"epoch": 15, "training_loss": 93.01906704902649, "training_acc": 47.0, "val_loss": 17.254163324832916, "val_acc": 56.0}
{"epoch": 16, "training_loss": 70.60028576850891, "training_acc": 61.0, "val_loss": 25.007078051567078, "val_acc": 52.0}
{"epoch": 17, "training_loss": 86.64034271240234, "training_acc": 53.0, "val_loss": 17.215579748153687, "val_acc": 52.0}
{"epoch": 18, "training_loss": 82.12842416763306, "training_acc": 51.0, "val_loss": 19.167576730251312, "val_acc": 36.0}
{"epoch": 19, "training_loss": 75.78231859207153, "training_acc": 43.0, "val_loss": 20.83364874124527, "val_acc": 52.0}
{"epoch": 20, "training_loss": 75.33631873130798, "training_acc": 53.0, "val_loss": 17.705313861370087, "val_acc": 52.0}
{"epoch": 21, "training_loss": 66.21540999412537, "training_acc": 56.0, "val_loss": 19.82485204935074, "val_acc": 44.0}
{"epoch": 22, "training_loss": 75.5465178489685, "training_acc": 49.0, "val_loss": 18.24425905942917, "val_acc": 52.0}
{"epoch": 23, "training_loss": 71.10439491271973, "training_acc": 58.0, "val_loss": 20.245295763015747, "val_acc": 52.0}
{"epoch": 24, "training_loss": 71.12149906158447, "training_acc": 56.0, "val_loss": 18.749502301216125, "val_acc": 44.0}
{"epoch": 25, "training_loss": 72.08691906929016, "training_acc": 52.0, "val_loss": 17.487643659114838, "val_acc": 56.0}
{"epoch": 26, "training_loss": 65.05568361282349, "training_acc": 66.0, "val_loss": 20.252399146556854, "val_acc": 52.0}
{"epoch": 27, "training_loss": 70.17531132698059, "training_acc": 54.0, "val_loss": 17.446351051330566, "val_acc": 56.0}
{"epoch": 28, "training_loss": 67.1678421497345, "training_acc": 59.0, "val_loss": 18.394818902015686, "val_acc": 52.0}
{"epoch": 29, "training_loss": 68.38222432136536, "training_acc": 57.0, "val_loss": 19.935382902622223, "val_acc": 52.0}
{"epoch": 30, "training_loss": 76.251300573349, "training_acc": 53.0, "val_loss": 19.308944046497345, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.69731140136719, "training_acc": 55.0, "val_loss": 18.357330560684204, "val_acc": 48.0}
{"epoch": 32, "training_loss": 70.52875971794128, "training_acc": 53.0, "val_loss": 17.724138498306274, "val_acc": 52.0}
{"epoch": 33, "training_loss": 68.16100788116455, "training_acc": 59.0, "val_loss": 17.517100274562836, "val_acc": 52.0}
{"epoch": 34, "training_loss": 65.95361375808716, "training_acc": 57.0, "val_loss": 18.341872096061707, "val_acc": 48.0}
{"epoch": 35, "training_loss": 74.01644158363342, "training_acc": 51.0, "val_loss": 17.080701887607574, "val_acc": 56.0}
{"epoch": 36, "training_loss": 67.10384821891785, "training_acc": 61.0, "val_loss": 19.349515438079834, "val_acc": 52.0}
{"epoch": 37, "training_loss": 67.99739027023315, "training_acc": 60.0, "val_loss": 17.139701545238495, "val_acc": 60.0}
{"epoch": 38, "training_loss": 67.85236835479736, "training_acc": 60.0, "val_loss": 16.96678251028061, "val_acc": 56.0}
{"epoch": 39, "training_loss": 65.23273539543152, "training_acc": 58.0, "val_loss": 18.21751892566681, "val_acc": 52.0}
{"epoch": 40, "training_loss": 65.36112332344055, "training_acc": 62.0, "val_loss": 17.035560309886932, "val_acc": 60.0}
{"epoch": 41, "training_loss": 65.08704662322998, "training_acc": 68.0, "val_loss": 17.012259364128113, "val_acc": 52.0}
{"epoch": 42, "training_loss": 63.79146718978882, "training_acc": 63.0, "val_loss": 17.608816921710968, "val_acc": 52.0}
{"epoch": 43, "training_loss": 65.49735379219055, "training_acc": 56.0, "val_loss": 17.09645241498947, "val_acc": 64.0}
{"epoch": 44, "training_loss": 64.35795211791992, "training_acc": 62.0, "val_loss": 17.381133139133453, "val_acc": 64.0}
{"epoch": 45, "training_loss": 63.58910155296326, "training_acc": 67.0, "val_loss": 19.196005165576935, "val_acc": 52.0}
{"epoch": 46, "training_loss": 72.32265019416809, "training_acc": 53.0, "val_loss": 16.718673706054688, "val_acc": 56.0}
{"epoch": 47, "training_loss": 69.78821063041687, "training_acc": 60.0, "val_loss": 17.807435989379883, "val_acc": 52.0}
{"epoch": 48, "training_loss": 64.50007247924805, "training_acc": 60.0, "val_loss": 21.38311117887497, "val_acc": 52.0}
{"epoch": 49, "training_loss": 74.22605633735657, "training_acc": 53.0, "val_loss": 16.710205376148224, "val_acc": 56.0}
{"epoch": 50, "training_loss": 70.7769284248352, "training_acc": 60.0, "val_loss": 17.20094382762909, "val_acc": 64.0}
{"epoch": 51, "training_loss": 64.97874093055725, "training_acc": 61.0, "val_loss": 19.28170472383499, "val_acc": 52.0}
{"epoch": 52, "training_loss": 68.64721393585205, "training_acc": 57.0, "val_loss": 16.888758540153503, "val_acc": 60.0}
{"epoch": 53, "training_loss": 67.40194392204285, "training_acc": 57.0, "val_loss": 16.792114078998566, "val_acc": 56.0}
{"epoch": 54, "training_loss": 66.02171635627747, "training_acc": 61.0, "val_loss": 18.459755182266235, "val_acc": 52.0}
{"epoch": 55, "training_loss": 64.29598736763, "training_acc": 57.0, "val_loss": 17.12118536233902, "val_acc": 64.0}
{"epoch": 56, "training_loss": 65.49217677116394, "training_acc": 58.0, "val_loss": 18.268200755119324, "val_acc": 52.0}
{"epoch": 57, "training_loss": 68.17136311531067, "training_acc": 55.0, "val_loss": 17.433197796344757, "val_acc": 52.0}
{"epoch": 58, "training_loss": 70.06877970695496, "training_acc": 51.0, "val_loss": 17.494094371795654, "val_acc": 64.0}
{"epoch": 59, "training_loss": 65.47500658035278, "training_acc": 57.0, "val_loss": 18.755893409252167, "val_acc": 52.0}
{"epoch": 60, "training_loss": 68.18329167366028, "training_acc": 53.0, "val_loss": 18.967148661613464, "val_acc": 40.0}
{"epoch": 61, "training_loss": 76.19254636764526, "training_acc": 48.0, "val_loss": 17.12723821401596, "val_acc": 60.0}
{"epoch": 62, "training_loss": 70.16514277458191, "training_acc": 61.0, "val_loss": 20.633690059185028, "val_acc": 52.0}
{"epoch": 63, "training_loss": 74.47814774513245, "training_acc": 47.0, "val_loss": 17.537882924079895, "val_acc": 64.0}
{"epoch": 64, "training_loss": 66.19823694229126, "training_acc": 61.0, "val_loss": 19.281011819839478, "val_acc": 52.0}
{"epoch": 65, "training_loss": 65.56428170204163, "training_acc": 59.0, "val_loss": 17.175476253032684, "val_acc": 56.0}
{"epoch": 66, "training_loss": 66.63552594184875, "training_acc": 63.0, "val_loss": 17.302119731903076, "val_acc": 56.0}
{"epoch": 67, "training_loss": 66.60374021530151, "training_acc": 64.0, "val_loss": 17.715339362621307, "val_acc": 52.0}
{"epoch": 68, "training_loss": 62.44142937660217, "training_acc": 66.0, "val_loss": 17.61762797832489, "val_acc": 64.0}
