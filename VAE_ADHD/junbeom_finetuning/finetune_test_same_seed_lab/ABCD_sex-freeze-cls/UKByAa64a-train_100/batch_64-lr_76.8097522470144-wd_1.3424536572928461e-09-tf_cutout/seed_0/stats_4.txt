"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 242838.65880966187, "training_acc": 44.0, "val_loss": 152081.11572265625, "val_acc": 52.0}
{"epoch": 1, "training_loss": 621839.78125, "training_acc": 53.0, "val_loss": 61397.2412109375, "val_acc": 52.0}
{"epoch": 2, "training_loss": 438590.83203125, "training_acc": 41.0, "val_loss": 170368.8232421875, "val_acc": 48.0}
{"epoch": 3, "training_loss": 609694.703125, "training_acc": 47.0, "val_loss": 17101.785278320312, "val_acc": 52.0}
{"epoch": 4, "training_loss": 118099.5234375, "training_acc": 53.0, "val_loss": 25470.147705078125, "val_acc": 52.0}
{"epoch": 5, "training_loss": 181402.193359375, "training_acc": 43.0, "val_loss": 64514.398193359375, "val_acc": 48.0}
{"epoch": 6, "training_loss": 179032.091796875, "training_acc": 47.0, "val_loss": 89375.14038085938, "val_acc": 52.0}
{"epoch": 7, "training_loss": 408485.447265625, "training_acc": 53.0, "val_loss": 121046.142578125, "val_acc": 52.0}
{"epoch": 8, "training_loss": 399819.046875, "training_acc": 53.0, "val_loss": 18839.675903320312, "val_acc": 48.0}
{"epoch": 9, "training_loss": 131336.4482421875, "training_acc": 47.0, "val_loss": 35587.46337890625, "val_acc": 48.0}
{"epoch": 10, "training_loss": 112056.5302734375, "training_acc": 55.0, "val_loss": 43812.09716796875, "val_acc": 52.0}
{"epoch": 11, "training_loss": 140851.21435546875, "training_acc": 53.0, "val_loss": 42869.769287109375, "val_acc": 48.0}
{"epoch": 12, "training_loss": 186393.6787109375, "training_acc": 47.0, "val_loss": 16894.71893310547, "val_acc": 48.0}
{"epoch": 13, "training_loss": 124036.1884765625, "training_acc": 49.0, "val_loss": 84494.140625, "val_acc": 52.0}
{"epoch": 14, "training_loss": 312485.0009765625, "training_acc": 53.0, "val_loss": 19315.768432617188, "val_acc": 52.0}
{"epoch": 15, "training_loss": 157437.375, "training_acc": 49.0, "val_loss": 104677.587890625, "val_acc": 48.0}
{"epoch": 16, "training_loss": 403035.9658203125, "training_acc": 47.0, "val_loss": 29011.892700195312, "val_acc": 48.0}
{"epoch": 17, "training_loss": 159929.0439453125, "training_acc": 51.0, "val_loss": 111474.49951171875, "val_acc": 52.0}
{"epoch": 18, "training_loss": 451402.408203125, "training_acc": 53.0, "val_loss": 83900.76293945312, "val_acc": 52.0}
{"epoch": 19, "training_loss": 237457.36572265625, "training_acc": 53.0, "val_loss": 91052.19116210938, "val_acc": 48.0}
{"epoch": 20, "training_loss": 448408.50390625, "training_acc": 47.0, "val_loss": 141071.81396484375, "val_acc": 48.0}
{"epoch": 21, "training_loss": 504291.46875, "training_acc": 47.0, "val_loss": 7598.113250732422, "val_acc": 48.0}
{"epoch": 22, "training_loss": 125054.6279296875, "training_acc": 57.0, "val_loss": 171616.064453125, "val_acc": 52.0}
{"epoch": 23, "training_loss": 722429.71875, "training_acc": 53.0, "val_loss": 184265.85693359375, "val_acc": 52.0}
{"epoch": 24, "training_loss": 644552.43359375, "training_acc": 53.0, "val_loss": 47449.798583984375, "val_acc": 52.0}
{"epoch": 25, "training_loss": 202796.97265625, "training_acc": 55.0, "val_loss": 139637.21923828125, "val_acc": 48.0}
{"epoch": 26, "training_loss": 580136.224609375, "training_acc": 47.0, "val_loss": 126010.546875, "val_acc": 48.0}
{"epoch": 27, "training_loss": 408245.9052734375, "training_acc": 47.0, "val_loss": 43750.02746582031, "val_acc": 52.0}
{"epoch": 28, "training_loss": 240670.021484375, "training_acc": 53.0, "val_loss": 105702.72216796875, "val_acc": 52.0}
{"epoch": 29, "training_loss": 376906.587890625, "training_acc": 53.0, "val_loss": 17769.667053222656, "val_acc": 52.0}
{"epoch": 30, "training_loss": 170798.9697265625, "training_acc": 51.0, "val_loss": 137214.0869140625, "val_acc": 48.0}
{"epoch": 31, "training_loss": 560963.572265625, "training_acc": 47.0, "val_loss": 93288.17138671875, "val_acc": 48.0}
{"epoch": 32, "training_loss": 255585.1904296875, "training_acc": 47.0, "val_loss": 107761.767578125, "val_acc": 52.0}
{"epoch": 33, "training_loss": 481509.400390625, "training_acc": 53.0, "val_loss": 192971.15478515625, "val_acc": 52.0}
{"epoch": 34, "training_loss": 737022.46875, "training_acc": 53.0, "val_loss": 131091.2841796875, "val_acc": 52.0}
{"epoch": 35, "training_loss": 395450.62109375, "training_acc": 53.0, "val_loss": 57603.9794921875, "val_acc": 48.0}
{"epoch": 36, "training_loss": 308340.36328125, "training_acc": 47.0, "val_loss": 128619.42138671875, "val_acc": 48.0}
{"epoch": 37, "training_loss": 473507.517578125, "training_acc": 47.0, "val_loss": 18799.960327148438, "val_acc": 48.0}
{"epoch": 38, "training_loss": 210599.9375, "training_acc": 41.0, "val_loss": 139844.2138671875, "val_acc": 52.0}
{"epoch": 39, "training_loss": 580178.84375, "training_acc": 53.0, "val_loss": 120874.072265625, "val_acc": 52.0}
{"epoch": 40, "training_loss": 373706.1708984375, "training_acc": 53.0, "val_loss": 49043.9453125, "val_acc": 48.0}
