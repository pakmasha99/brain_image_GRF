"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 3148.8786239624023, "training_acc": 47.0, "val_loss": 755.9581279754639, "val_acc": 52.0}
{"epoch": 1, "training_loss": 3164.79638671875, "training_acc": 53.0, "val_loss": 1402.0905494689941, "val_acc": 48.0}
{"epoch": 2, "training_loss": 5178.107437133789, "training_acc": 47.0, "val_loss": 221.7674732208252, "val_acc": 48.0}
{"epoch": 3, "training_loss": 1550.078369140625, "training_acc": 55.0, "val_loss": 1469.4372177124023, "val_acc": 52.0}
{"epoch": 4, "training_loss": 5914.1297607421875, "training_acc": 53.0, "val_loss": 1435.4132652282715, "val_acc": 52.0}
{"epoch": 5, "training_loss": 4752.12629699707, "training_acc": 53.0, "val_loss": 231.42850399017334, "val_acc": 52.0}
{"epoch": 6, "training_loss": 1444.603012084961, "training_acc": 55.0, "val_loss": 1271.4252471923828, "val_acc": 48.0}
{"epoch": 7, "training_loss": 5322.001220703125, "training_acc": 47.0, "val_loss": 1241.434097290039, "val_acc": 48.0}
{"epoch": 8, "training_loss": 4603.825660705566, "training_acc": 47.0, "val_loss": 129.62415218353271, "val_acc": 48.0}
{"epoch": 9, "training_loss": 1334.517219543457, "training_acc": 47.0, "val_loss": 1115.4349327087402, "val_acc": 52.0}
{"epoch": 10, "training_loss": 4348.390472412109, "training_acc": 53.0, "val_loss": 1201.4013290405273, "val_acc": 52.0}
{"epoch": 11, "training_loss": 4144.780731201172, "training_acc": 53.0, "val_loss": 475.5403995513916, "val_acc": 52.0}
{"epoch": 12, "training_loss": 1579.08154296875, "training_acc": 45.0, "val_loss": 536.6800308227539, "val_acc": 48.0}
{"epoch": 13, "training_loss": 2228.9532470703125, "training_acc": 47.0, "val_loss": 333.9834928512573, "val_acc": 48.0}
{"epoch": 14, "training_loss": 899.5615863800049, "training_acc": 52.0, "val_loss": 318.0845260620117, "val_acc": 52.0}
{"epoch": 15, "training_loss": 1153.2954216003418, "training_acc": 53.0, "val_loss": 97.8468656539917, "val_acc": 52.0}
{"epoch": 16, "training_loss": 600.6283683776855, "training_acc": 54.0, "val_loss": 411.59605979919434, "val_acc": 48.0}
{"epoch": 17, "training_loss": 1517.4846458435059, "training_acc": 47.0, "val_loss": 127.6410460472107, "val_acc": 52.0}
{"epoch": 18, "training_loss": 641.9717903137207, "training_acc": 55.0, "val_loss": 144.5241093635559, "val_acc": 52.0}
{"epoch": 19, "training_loss": 621.5396976470947, "training_acc": 46.0, "val_loss": 235.62829494476318, "val_acc": 48.0}
{"epoch": 20, "training_loss": 674.4389872550964, "training_acc": 52.0, "val_loss": 211.71293258666992, "val_acc": 52.0}
{"epoch": 21, "training_loss": 756.6024188995361, "training_acc": 53.0, "val_loss": 30.016499757766724, "val_acc": 56.0}
{"epoch": 22, "training_loss": 236.69774341583252, "training_acc": 60.0, "val_loss": 125.99225044250488, "val_acc": 48.0}
{"epoch": 23, "training_loss": 446.1976661682129, "training_acc": 51.0, "val_loss": 139.87493515014648, "val_acc": 52.0}
{"epoch": 24, "training_loss": 481.1473340988159, "training_acc": 47.0, "val_loss": 32.563316822052, "val_acc": 52.0}
{"epoch": 25, "training_loss": 199.67097187042236, "training_acc": 54.0, "val_loss": 29.047515988349915, "val_acc": 60.0}
{"epoch": 26, "training_loss": 105.3206729888916, "training_acc": 66.0, "val_loss": 30.90600073337555, "val_acc": 60.0}
{"epoch": 27, "training_loss": 136.93092393875122, "training_acc": 59.0, "val_loss": 26.696482300758362, "val_acc": 56.0}
{"epoch": 28, "training_loss": 87.06075763702393, "training_acc": 71.0, "val_loss": 50.39295554161072, "val_acc": 56.0}
{"epoch": 29, "training_loss": 171.94126224517822, "training_acc": 58.0, "val_loss": 48.153844475746155, "val_acc": 56.0}
{"epoch": 30, "training_loss": 115.38484692573547, "training_acc": 62.0, "val_loss": 45.18131911754608, "val_acc": 48.0}
{"epoch": 31, "training_loss": 237.23914051055908, "training_acc": 53.0, "val_loss": 28.19477617740631, "val_acc": 56.0}
{"epoch": 32, "training_loss": 202.74164581298828, "training_acc": 65.0, "val_loss": 22.465910017490387, "val_acc": 64.0}
{"epoch": 33, "training_loss": 165.84596157073975, "training_acc": 71.0, "val_loss": 22.686640918254852, "val_acc": 52.0}
{"epoch": 34, "training_loss": 114.10913181304932, "training_acc": 61.0, "val_loss": 169.43371295928955, "val_acc": 52.0}
{"epoch": 35, "training_loss": 541.0027408599854, "training_acc": 53.0, "val_loss": 127.80963182449341, "val_acc": 48.0}
{"epoch": 36, "training_loss": 491.818039894104, "training_acc": 47.0, "val_loss": 163.72991800308228, "val_acc": 52.0}
{"epoch": 37, "training_loss": 550.9792251586914, "training_acc": 53.0, "val_loss": 42.614296078681946, "val_acc": 48.0}
{"epoch": 38, "training_loss": 138.0531084537506, "training_acc": 57.0, "val_loss": 23.255786299705505, "val_acc": 56.0}
{"epoch": 39, "training_loss": 97.70613718032837, "training_acc": 61.0, "val_loss": 137.4078631401062, "val_acc": 52.0}
{"epoch": 40, "training_loss": 343.88641357421875, "training_acc": 53.0, "val_loss": 215.70005416870117, "val_acc": 48.0}
{"epoch": 41, "training_loss": 806.4541072845459, "training_acc": 47.0, "val_loss": 70.05420327186584, "val_acc": 52.0}
{"epoch": 42, "training_loss": 241.7785415649414, "training_acc": 54.0, "val_loss": 97.91573882102966, "val_acc": 48.0}
{"epoch": 43, "training_loss": 262.7761149406433, "training_acc": 53.0, "val_loss": 218.21389198303223, "val_acc": 52.0}
{"epoch": 44, "training_loss": 749.6815738677979, "training_acc": 53.0, "val_loss": 24.020785093307495, "val_acc": 52.0}
{"epoch": 45, "training_loss": 161.3574800491333, "training_acc": 59.0, "val_loss": 87.18423843383789, "val_acc": 52.0}
{"epoch": 46, "training_loss": 225.78167915344238, "training_acc": 55.0, "val_loss": 220.32105922698975, "val_acc": 48.0}
{"epoch": 47, "training_loss": 806.783296585083, "training_acc": 47.0, "val_loss": 62.86141276359558, "val_acc": 52.0}
{"epoch": 48, "training_loss": 216.90686225891113, "training_acc": 58.0, "val_loss": 78.85037064552307, "val_acc": 48.0}
{"epoch": 49, "training_loss": 209.15917301177979, "training_acc": 55.0, "val_loss": 140.6575083732605, "val_acc": 52.0}
{"epoch": 50, "training_loss": 340.82415866851807, "training_acc": 54.0, "val_loss": 209.6963405609131, "val_acc": 48.0}
{"epoch": 51, "training_loss": 788.6965599060059, "training_acc": 47.0, "val_loss": 72.81113266944885, "val_acc": 52.0}
