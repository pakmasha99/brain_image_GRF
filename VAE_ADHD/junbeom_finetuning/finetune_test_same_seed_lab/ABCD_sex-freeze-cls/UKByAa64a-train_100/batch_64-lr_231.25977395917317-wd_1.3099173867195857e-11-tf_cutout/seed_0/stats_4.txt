"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 601121.4233627319, "training_acc": 45.0, "val_loss": 100194.64721679688, "val_acc": 48.0}
{"epoch": 1, "training_loss": 521574.2734375, "training_acc": 49.0, "val_loss": 287612.548828125, "val_acc": 52.0}
{"epoch": 2, "training_loss": 1101398.37109375, "training_acc": 53.0, "val_loss": 201775.42724609375, "val_acc": 52.0}
{"epoch": 3, "training_loss": 605941.1376953125, "training_acc": 53.0, "val_loss": 99276.45874023438, "val_acc": 48.0}
{"epoch": 4, "training_loss": 524213.216796875, "training_acc": 47.0, "val_loss": 184441.49169921875, "val_acc": 48.0}
{"epoch": 5, "training_loss": 660481.130859375, "training_acc": 47.0, "val_loss": 26565.057373046875, "val_acc": 48.0}
{"epoch": 6, "training_loss": 235837.21484375, "training_acc": 45.0, "val_loss": 177807.861328125, "val_acc": 52.0}
{"epoch": 7, "training_loss": 707696.44140625, "training_acc": 53.0, "val_loss": 176789.12353515625, "val_acc": 52.0}
{"epoch": 8, "training_loss": 613257.150390625, "training_acc": 53.0, "val_loss": 31956.45751953125, "val_acc": 52.0}
{"epoch": 9, "training_loss": 197675.798828125, "training_acc": 51.0, "val_loss": 153306.7138671875, "val_acc": 48.0}
{"epoch": 10, "training_loss": 639755.8671875, "training_acc": 47.0, "val_loss": 135633.0078125, "val_acc": 48.0}
{"epoch": 11, "training_loss": 441486.3515625, "training_acc": 47.0, "val_loss": 42826.15661621094, "val_acc": 52.0}
{"epoch": 12, "training_loss": 251625.220703125, "training_acc": 53.0, "val_loss": 109263.02490234375, "val_acc": 52.0}
{"epoch": 13, "training_loss": 388878.4267578125, "training_acc": 53.0, "val_loss": 30849.722290039062, "val_acc": 52.0}
{"epoch": 14, "training_loss": 117030.92822265625, "training_acc": 59.0, "val_loss": 99465.6005859375, "val_acc": 48.0}
{"epoch": 15, "training_loss": 381048.4833984375, "training_acc": 47.0, "val_loss": 59923.49853515625, "val_acc": 48.0}
{"epoch": 16, "training_loss": 161716.8692626953, "training_acc": 53.0, "val_loss": 53855.426025390625, "val_acc": 52.0}
{"epoch": 17, "training_loss": 217870.7802734375, "training_acc": 53.0, "val_loss": 26469.091796875, "val_acc": 52.0}
{"epoch": 18, "training_loss": 108555.37939453125, "training_acc": 55.0, "val_loss": 65175.079345703125, "val_acc": 48.0}
{"epoch": 19, "training_loss": 222779.3466796875, "training_acc": 48.0, "val_loss": 8358.552551269531, "val_acc": 64.0}
{"epoch": 20, "training_loss": 81054.8134765625, "training_acc": 56.0, "val_loss": 41745.23010253906, "val_acc": 52.0}
{"epoch": 21, "training_loss": 128625.81408691406, "training_acc": 52.0, "val_loss": 45140.89660644531, "val_acc": 48.0}
{"epoch": 22, "training_loss": 177958.595703125, "training_acc": 47.0, "val_loss": 14306.202697753906, "val_acc": 44.0}
{"epoch": 23, "training_loss": 104895.0244140625, "training_acc": 55.0, "val_loss": 60440.216064453125, "val_acc": 52.0}
{"epoch": 24, "training_loss": 204191.5234375, "training_acc": 53.0, "val_loss": 12990.940856933594, "val_acc": 48.0}
{"epoch": 25, "training_loss": 80181.8330078125, "training_acc": 56.0, "val_loss": 34436.87438964844, "val_acc": 48.0}
{"epoch": 26, "training_loss": 117361.89428710938, "training_acc": 42.0, "val_loss": 29235.427856445312, "val_acc": 52.0}
{"epoch": 27, "training_loss": 91966.80419921875, "training_acc": 53.0, "val_loss": 28629.757690429688, "val_acc": 44.0}
{"epoch": 28, "training_loss": 113321.9853515625, "training_acc": 45.0, "val_loss": 14292.279052734375, "val_acc": 44.0}
{"epoch": 29, "training_loss": 52804.081298828125, "training_acc": 64.0, "val_loss": 42715.51513671875, "val_acc": 52.0}
{"epoch": 30, "training_loss": 132238.36962890625, "training_acc": 53.0, "val_loss": 25989.083862304688, "val_acc": 48.0}
{"epoch": 31, "training_loss": 116335.49853515625, "training_acc": 50.0, "val_loss": 18417.96417236328, "val_acc": 48.0}
{"epoch": 32, "training_loss": 119761.60888671875, "training_acc": 45.0, "val_loss": 53315.13671875, "val_acc": 52.0}
{"epoch": 33, "training_loss": 167526.37426757812, "training_acc": 54.0, "val_loss": 18863.992309570312, "val_acc": 48.0}
{"epoch": 34, "training_loss": 81064.14208984375, "training_acc": 52.0, "val_loss": 8124.30419921875, "val_acc": 48.0}
{"epoch": 35, "training_loss": 58247.47412109375, "training_acc": 57.0, "val_loss": 27708.9599609375, "val_acc": 52.0}
{"epoch": 36, "training_loss": 57720.47082519531, "training_acc": 66.0, "val_loss": 31512.07275390625, "val_acc": 48.0}
{"epoch": 37, "training_loss": 107716.57104492188, "training_acc": 47.0, "val_loss": 24536.28387451172, "val_acc": 52.0}
{"epoch": 38, "training_loss": 87179.42260742188, "training_acc": 58.0, "val_loss": 10169.064331054688, "val_acc": 56.0}
{"epoch": 39, "training_loss": 44116.817626953125, "training_acc": 62.0, "val_loss": 13364.134216308594, "val_acc": 40.0}
{"epoch": 40, "training_loss": 68092.71508789062, "training_acc": 53.0, "val_loss": 21219.639587402344, "val_acc": 52.0}
{"epoch": 41, "training_loss": 49047.48162841797, "training_acc": 65.0, "val_loss": 19996.33026123047, "val_acc": 48.0}
{"epoch": 42, "training_loss": 44396.064208984375, "training_acc": 53.0, "val_loss": 12948.863220214844, "val_acc": 52.0}
{"epoch": 43, "training_loss": 43069.196533203125, "training_acc": 45.0, "val_loss": 6324.125671386719, "val_acc": 52.0}
{"epoch": 44, "training_loss": 21049.61895751953, "training_acc": 60.0, "val_loss": 6466.423034667969, "val_acc": 40.0}
{"epoch": 45, "training_loss": 33810.32849121094, "training_acc": 49.0, "val_loss": 5687.094497680664, "val_acc": 40.0}
{"epoch": 46, "training_loss": 16555.876831054688, "training_acc": 69.0, "val_loss": 8599.562072753906, "val_acc": 56.0}
{"epoch": 47, "training_loss": 12450.695281982422, "training_acc": 73.0, "val_loss": 2545.2884674072266, "val_acc": 68.0}
{"epoch": 48, "training_loss": 15183.82861328125, "training_acc": 67.0, "val_loss": 6701.350402832031, "val_acc": 52.0}
{"epoch": 49, "training_loss": 15782.010925292969, "training_acc": 65.0, "val_loss": 4963.917541503906, "val_acc": 56.0}
{"epoch": 50, "training_loss": 17898.363830566406, "training_acc": 59.0, "val_loss": 15586.924743652344, "val_acc": 52.0}
{"epoch": 51, "training_loss": 33335.96694946289, "training_acc": 63.0, "val_loss": 13540.17333984375, "val_acc": 48.0}
{"epoch": 52, "training_loss": 40561.697998046875, "training_acc": 53.0, "val_loss": 5374.195861816406, "val_acc": 56.0}
{"epoch": 53, "training_loss": 43739.577392578125, "training_acc": 61.0, "val_loss": 2554.471778869629, "val_acc": 60.0}
{"epoch": 54, "training_loss": 38094.2119140625, "training_acc": 67.0, "val_loss": 6832.701873779297, "val_acc": 60.0}
{"epoch": 55, "training_loss": 53358.5205078125, "training_acc": 60.0, "val_loss": 26861.221313476562, "val_acc": 48.0}
{"epoch": 56, "training_loss": 80613.78576660156, "training_acc": 51.0, "val_loss": 22885.986328125, "val_acc": 52.0}
{"epoch": 57, "training_loss": 42583.35989379883, "training_acc": 65.0, "val_loss": 11163.224029541016, "val_acc": 48.0}
{"epoch": 58, "training_loss": 41398.89270019531, "training_acc": 55.0, "val_loss": 15121.420288085938, "val_acc": 52.0}
{"epoch": 59, "training_loss": 49880.628173828125, "training_acc": 52.0, "val_loss": 9081.302642822266, "val_acc": 48.0}
{"epoch": 60, "training_loss": 59248.8759765625, "training_acc": 56.0, "val_loss": 28651.113891601562, "val_acc": 52.0}
{"epoch": 61, "training_loss": 71803.42700195312, "training_acc": 51.0, "val_loss": 11835.478210449219, "val_acc": 48.0}
{"epoch": 62, "training_loss": 39700.64929199219, "training_acc": 58.0, "val_loss": 18796.36993408203, "val_acc": 52.0}
{"epoch": 63, "training_loss": 66862.82153320312, "training_acc": 45.0, "val_loss": 4000.714874267578, "val_acc": 56.0}
{"epoch": 64, "training_loss": 35152.71826171875, "training_acc": 57.0, "val_loss": 11696.537780761719, "val_acc": 52.0}
{"epoch": 65, "training_loss": 32042.124633789062, "training_acc": 66.0, "val_loss": 19349.70703125, "val_acc": 48.0}
{"epoch": 66, "training_loss": 77444.109375, "training_acc": 44.0, "val_loss": 15726.969909667969, "val_acc": 52.0}
