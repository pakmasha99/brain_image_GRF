"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 485748.31145095825, "training_acc": 46.0, "val_loss": 101524.42626953125, "val_acc": 52.0}
{"epoch": 1, "training_loss": 492989.072265625, "training_acc": 53.0, "val_loss": 262923.4375, "val_acc": 48.0}
{"epoch": 2, "training_loss": 989514.89453125, "training_acc": 47.0, "val_loss": 72882.03125, "val_acc": 48.0}
{"epoch": 3, "training_loss": 344799.6845703125, "training_acc": 51.0, "val_loss": 192016.34521484375, "val_acc": 52.0}
{"epoch": 4, "training_loss": 729524.787109375, "training_acc": 53.0, "val_loss": 171341.97998046875, "val_acc": 52.0}
{"epoch": 5, "training_loss": 508530.017578125, "training_acc": 53.0, "val_loss": 29770.425415039062, "val_acc": 48.0}
{"epoch": 6, "training_loss": 274390.689453125, "training_acc": 47.0, "val_loss": 87760.107421875, "val_acc": 48.0}
{"epoch": 7, "training_loss": 297471.4130859375, "training_acc": 47.0, "val_loss": 63590.850830078125, "val_acc": 52.0}
{"epoch": 8, "training_loss": 247957.3291015625, "training_acc": 53.0, "val_loss": 110263.58642578125, "val_acc": 52.0}
{"epoch": 9, "training_loss": 358042.1591796875, "training_acc": 53.0, "val_loss": 13894.819641113281, "val_acc": 52.0}
{"epoch": 10, "training_loss": 143199.8583984375, "training_acc": 49.0, "val_loss": 102522.16796875, "val_acc": 48.0}
{"epoch": 11, "training_loss": 392813.6875, "training_acc": 47.0, "val_loss": 21747.576904296875, "val_acc": 48.0}
{"epoch": 12, "training_loss": 142324.853515625, "training_acc": 49.0, "val_loss": 107881.70166015625, "val_acc": 52.0}
{"epoch": 13, "training_loss": 414773.85546875, "training_acc": 53.0, "val_loss": 80875.07934570312, "val_acc": 52.0}
{"epoch": 14, "training_loss": 217110.87854003906, "training_acc": 52.0, "val_loss": 79309.46655273438, "val_acc": 48.0}
{"epoch": 15, "training_loss": 386394.484375, "training_acc": 47.0, "val_loss": 117959.5703125, "val_acc": 48.0}
{"epoch": 16, "training_loss": 397977.6474609375, "training_acc": 47.0, "val_loss": 10743.649291992188, "val_acc": 60.0}
{"epoch": 17, "training_loss": 99071.3759765625, "training_acc": 54.0, "val_loss": 67421.86279296875, "val_acc": 52.0}
{"epoch": 18, "training_loss": 216741.96923828125, "training_acc": 53.0, "val_loss": 14530.966186523438, "val_acc": 48.0}
{"epoch": 19, "training_loss": 65735.88720703125, "training_acc": 51.0, "val_loss": 6796.128845214844, "val_acc": 56.0}
{"epoch": 20, "training_loss": 53131.1767578125, "training_acc": 58.0, "val_loss": 16803.366088867188, "val_acc": 56.0}
{"epoch": 21, "training_loss": 56082.6865234375, "training_acc": 52.0, "val_loss": 24881.442260742188, "val_acc": 48.0}
{"epoch": 22, "training_loss": 74731.68566894531, "training_acc": 52.0, "val_loss": 31280.084228515625, "val_acc": 52.0}
{"epoch": 23, "training_loss": 66817.28479003906, "training_acc": 58.0, "val_loss": 22279.226684570312, "val_acc": 48.0}
{"epoch": 24, "training_loss": 83937.25512695312, "training_acc": 49.0, "val_loss": 24814.540100097656, "val_acc": 52.0}
{"epoch": 25, "training_loss": 81596.43994140625, "training_acc": 56.0, "val_loss": 9441.397857666016, "val_acc": 56.0}
{"epoch": 26, "training_loss": 81373.66943359375, "training_acc": 53.0, "val_loss": 26145.358276367188, "val_acc": 48.0}
{"epoch": 27, "training_loss": 93392.39916992188, "training_acc": 50.0, "val_loss": 27297.61962890625, "val_acc": 52.0}
{"epoch": 28, "training_loss": 57394.00207519531, "training_acc": 57.0, "val_loss": 22496.71630859375, "val_acc": 48.0}
{"epoch": 29, "training_loss": 69198.97290039062, "training_acc": 51.0, "val_loss": 47118.93005371094, "val_acc": 52.0}
{"epoch": 30, "training_loss": 175901.93896484375, "training_acc": 53.0, "val_loss": 39626.568603515625, "val_acc": 52.0}
{"epoch": 31, "training_loss": 111348.92431640625, "training_acc": 48.0, "val_loss": 26856.77490234375, "val_acc": 48.0}
{"epoch": 32, "training_loss": 77679.62390136719, "training_acc": 50.0, "val_loss": 17374.10125732422, "val_acc": 56.0}
{"epoch": 33, "training_loss": 42465.60583496094, "training_acc": 61.0, "val_loss": 20267.601013183594, "val_acc": 48.0}
{"epoch": 34, "training_loss": 77596.83422851562, "training_acc": 46.0, "val_loss": 11650.031280517578, "val_acc": 60.0}
{"epoch": 35, "training_loss": 39675.667236328125, "training_acc": 56.0, "val_loss": 10351.339721679688, "val_acc": 56.0}
{"epoch": 36, "training_loss": 26048.502319335938, "training_acc": 61.0, "val_loss": 10400.698852539062, "val_acc": 44.0}
{"epoch": 37, "training_loss": 43251.76672363281, "training_acc": 60.0, "val_loss": 21113.63067626953, "val_acc": 52.0}
{"epoch": 38, "training_loss": 44675.27947998047, "training_acc": 54.0, "val_loss": 11187.071990966797, "val_acc": 48.0}
{"epoch": 39, "training_loss": 44359.47766113281, "training_acc": 53.0, "val_loss": 3022.99747467041, "val_acc": 68.0}
{"epoch": 40, "training_loss": 8142.106658935547, "training_acc": 69.0, "val_loss": 2316.6873931884766, "val_acc": 56.0}
{"epoch": 41, "training_loss": 7150.5877685546875, "training_acc": 75.0, "val_loss": 4582.744979858398, "val_acc": 64.0}
{"epoch": 42, "training_loss": 32786.73583984375, "training_acc": 53.0, "val_loss": 5266.35627746582, "val_acc": 60.0}
{"epoch": 43, "training_loss": 19342.651977539062, "training_acc": 63.0, "val_loss": 23695.846557617188, "val_acc": 48.0}
{"epoch": 44, "training_loss": 72319.60943603516, "training_acc": 49.0, "val_loss": 33112.994384765625, "val_acc": 52.0}
{"epoch": 45, "training_loss": 117139.87548828125, "training_acc": 53.0, "val_loss": 2987.5770568847656, "val_acc": 56.0}
{"epoch": 46, "training_loss": 41932.246826171875, "training_acc": 68.0, "val_loss": 26330.035400390625, "val_acc": 48.0}
{"epoch": 47, "training_loss": 87165.03491210938, "training_acc": 51.0, "val_loss": 16012.069702148438, "val_acc": 52.0}
{"epoch": 48, "training_loss": 68962.99340820312, "training_acc": 44.0, "val_loss": 7125.721740722656, "val_acc": 52.0}
{"epoch": 49, "training_loss": 52753.910888671875, "training_acc": 56.0, "val_loss": 32460.26611328125, "val_acc": 52.0}
{"epoch": 50, "training_loss": 88934.25561523438, "training_acc": 53.0, "val_loss": 18350.82550048828, "val_acc": 48.0}
{"epoch": 51, "training_loss": 43977.46252441406, "training_acc": 60.0, "val_loss": 4702.503967285156, "val_acc": 64.0}
{"epoch": 52, "training_loss": 32210.05810546875, "training_acc": 60.0, "val_loss": 3062.685012817383, "val_acc": 64.0}
{"epoch": 53, "training_loss": 18348.801391601562, "training_acc": 72.0, "val_loss": 11401.347351074219, "val_acc": 48.0}
{"epoch": 54, "training_loss": 42480.75830078125, "training_acc": 49.0, "val_loss": 3151.9771575927734, "val_acc": 64.0}
{"epoch": 55, "training_loss": 34514.517578125, "training_acc": 61.0, "val_loss": 4341.270446777344, "val_acc": 64.0}
{"epoch": 56, "training_loss": 12570.5986328125, "training_acc": 73.0, "val_loss": 5806.815338134766, "val_acc": 44.0}
{"epoch": 57, "training_loss": 26004.367065429688, "training_acc": 55.0, "val_loss": 5449.677276611328, "val_acc": 44.0}
{"epoch": 58, "training_loss": 11923.085998535156, "training_acc": 64.0, "val_loss": 3106.344223022461, "val_acc": 40.0}
{"epoch": 59, "training_loss": 7493.831115722656, "training_acc": 76.0, "val_loss": 6975.310516357422, "val_acc": 60.0}
