"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 502.72874450683594, "training_acc": 51.0, "val_loss": 88.65780234336853, "val_acc": 52.0}
{"epoch": 1, "training_loss": 666.619701385498, "training_acc": 43.0, "val_loss": 286.3373279571533, "val_acc": 48.0}
{"epoch": 2, "training_loss": 1061.4546089172363, "training_acc": 47.0, "val_loss": 58.57034921646118, "val_acc": 48.0}
{"epoch": 3, "training_loss": 301.56531715393066, "training_acc": 53.0, "val_loss": 226.8913745880127, "val_acc": 52.0}
{"epoch": 4, "training_loss": 892.8241634368896, "training_acc": 53.0, "val_loss": 210.82468032836914, "val_acc": 52.0}
{"epoch": 5, "training_loss": 701.0670585632324, "training_acc": 53.0, "val_loss": 19.78713572025299, "val_acc": 52.0}
{"epoch": 6, "training_loss": 250.51320266723633, "training_acc": 45.0, "val_loss": 178.71886491775513, "val_acc": 48.0}
{"epoch": 7, "training_loss": 693.0944538116455, "training_acc": 47.0, "val_loss": 102.6206374168396, "val_acc": 48.0}
{"epoch": 8, "training_loss": 291.15839076042175, "training_acc": 49.0, "val_loss": 91.82710647583008, "val_acc": 52.0}
{"epoch": 9, "training_loss": 398.3160972595215, "training_acc": 53.0, "val_loss": 113.40819597244263, "val_acc": 52.0}
{"epoch": 10, "training_loss": 368.8687653541565, "training_acc": 53.0, "val_loss": 29.478952288627625, "val_acc": 48.0}
{"epoch": 11, "training_loss": 179.4004364013672, "training_acc": 47.0, "val_loss": 60.411137342453, "val_acc": 48.0}
{"epoch": 12, "training_loss": 185.6241762638092, "training_acc": 54.0, "val_loss": 57.861924171447754, "val_acc": 52.0}
{"epoch": 13, "training_loss": 222.7793378829956, "training_acc": 53.0, "val_loss": 45.59539258480072, "val_acc": 52.0}
{"epoch": 14, "training_loss": 139.22969889640808, "training_acc": 53.0, "val_loss": 47.28160500526428, "val_acc": 48.0}
{"epoch": 15, "training_loss": 171.15853309631348, "training_acc": 47.0, "val_loss": 28.43291461467743, "val_acc": 52.0}
{"epoch": 16, "training_loss": 115.91029834747314, "training_acc": 57.0, "val_loss": 32.097381353378296, "val_acc": 52.0}
{"epoch": 17, "training_loss": 110.2488489151001, "training_acc": 50.0, "val_loss": 29.83444035053253, "val_acc": 48.0}
{"epoch": 18, "training_loss": 109.0175724029541, "training_acc": 48.0, "val_loss": 24.54541027545929, "val_acc": 52.0}
{"epoch": 19, "training_loss": 79.29043889045715, "training_acc": 52.0, "val_loss": 19.57056075334549, "val_acc": 56.0}
{"epoch": 20, "training_loss": 70.10445308685303, "training_acc": 61.0, "val_loss": 20.313270390033722, "val_acc": 52.0}
{"epoch": 21, "training_loss": 71.03135418891907, "training_acc": 60.0, "val_loss": 17.540113627910614, "val_acc": 56.0}
{"epoch": 22, "training_loss": 63.20856165885925, "training_acc": 66.0, "val_loss": 18.077504634857178, "val_acc": 56.0}
{"epoch": 23, "training_loss": 69.03501105308533, "training_acc": 56.0, "val_loss": 22.2549170255661, "val_acc": 52.0}
{"epoch": 24, "training_loss": 66.9442207813263, "training_acc": 64.0, "val_loss": 19.627675414085388, "val_acc": 52.0}
{"epoch": 25, "training_loss": 71.03220129013062, "training_acc": 59.0, "val_loss": 22.517183423042297, "val_acc": 52.0}
{"epoch": 26, "training_loss": 82.84051847457886, "training_acc": 49.0, "val_loss": 17.481158673763275, "val_acc": 52.0}
{"epoch": 27, "training_loss": 58.73849582672119, "training_acc": 65.0, "val_loss": 17.083585262298584, "val_acc": 52.0}
{"epoch": 28, "training_loss": 64.40526676177979, "training_acc": 59.0, "val_loss": 16.88709259033203, "val_acc": 52.0}
{"epoch": 29, "training_loss": 62.31346797943115, "training_acc": 62.0, "val_loss": 17.070049047470093, "val_acc": 52.0}
{"epoch": 30, "training_loss": 64.06249165534973, "training_acc": 66.0, "val_loss": 16.756632924079895, "val_acc": 52.0}
{"epoch": 31, "training_loss": 62.02708840370178, "training_acc": 67.0, "val_loss": 17.42352396249771, "val_acc": 52.0}
{"epoch": 32, "training_loss": 63.18061089515686, "training_acc": 62.0, "val_loss": 18.080146610736847, "val_acc": 52.0}
{"epoch": 33, "training_loss": 66.51918411254883, "training_acc": 60.0, "val_loss": 18.286718428134918, "val_acc": 52.0}
{"epoch": 34, "training_loss": 61.670295000076294, "training_acc": 60.0, "val_loss": 18.44353824853897, "val_acc": 52.0}
{"epoch": 35, "training_loss": 64.18598055839539, "training_acc": 64.0, "val_loss": 19.07491385936737, "val_acc": 52.0}
{"epoch": 36, "training_loss": 70.91799855232239, "training_acc": 52.0, "val_loss": 21.635794639587402, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.35129070281982, "training_acc": 58.0, "val_loss": 16.847442090511322, "val_acc": 52.0}
{"epoch": 38, "training_loss": 60.68777585029602, "training_acc": 73.0, "val_loss": 16.866011917591095, "val_acc": 56.0}
{"epoch": 39, "training_loss": 56.62498211860657, "training_acc": 80.0, "val_loss": 16.954657435417175, "val_acc": 56.0}
{"epoch": 40, "training_loss": 59.082797050476074, "training_acc": 67.0, "val_loss": 17.28833019733429, "val_acc": 60.0}
{"epoch": 41, "training_loss": 61.05260920524597, "training_acc": 70.0, "val_loss": 19.080032408237457, "val_acc": 52.0}
{"epoch": 42, "training_loss": 65.46129751205444, "training_acc": 61.0, "val_loss": 17.37213283777237, "val_acc": 52.0}
{"epoch": 43, "training_loss": 57.807774782180786, "training_acc": 66.0, "val_loss": 18.174472451210022, "val_acc": 56.0}
{"epoch": 44, "training_loss": 62.48508405685425, "training_acc": 61.0, "val_loss": 17.88647472858429, "val_acc": 52.0}
{"epoch": 45, "training_loss": 60.92322611808777, "training_acc": 60.0, "val_loss": 16.96496158838272, "val_acc": 56.0}
{"epoch": 46, "training_loss": 59.95584988594055, "training_acc": 63.0, "val_loss": 17.32611656188965, "val_acc": 64.0}
{"epoch": 47, "training_loss": 58.71149182319641, "training_acc": 72.0, "val_loss": 20.63681185245514, "val_acc": 52.0}
{"epoch": 48, "training_loss": 62.5629997253418, "training_acc": 65.0, "val_loss": 17.249958217144012, "val_acc": 56.0}
{"epoch": 49, "training_loss": 57.54749345779419, "training_acc": 67.0, "val_loss": 17.17788130044937, "val_acc": 56.0}
