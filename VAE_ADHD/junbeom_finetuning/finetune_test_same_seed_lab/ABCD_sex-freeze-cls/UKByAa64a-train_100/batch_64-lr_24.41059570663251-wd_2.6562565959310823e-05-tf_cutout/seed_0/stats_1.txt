"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 51312.8314704895, "training_acc": 46.0, "val_loss": 10693.563842773438, "val_acc": 52.0}
{"epoch": 1, "training_loss": 52028.286376953125, "training_acc": 53.0, "val_loss": 27772.396850585938, "val_acc": 48.0}
{"epoch": 2, "training_loss": 104489.3740234375, "training_acc": 47.0, "val_loss": 7680.0079345703125, "val_acc": 48.0}
{"epoch": 3, "training_loss": 36378.61877441406, "training_acc": 51.0, "val_loss": 20277.53448486328, "val_acc": 52.0}
{"epoch": 4, "training_loss": 77036.87817382812, "training_acc": 53.0, "val_loss": 18067.832946777344, "val_acc": 52.0}
{"epoch": 5, "training_loss": 53604.75061035156, "training_acc": 53.0, "val_loss": 3182.496452331543, "val_acc": 48.0}
{"epoch": 6, "training_loss": 29089.93408203125, "training_acc": 47.0, "val_loss": 9294.859313964844, "val_acc": 48.0}
{"epoch": 7, "training_loss": 31475.465881347656, "training_acc": 47.0, "val_loss": 6691.624450683594, "val_acc": 52.0}
{"epoch": 8, "training_loss": 26117.176147460938, "training_acc": 53.0, "val_loss": 11606.827545166016, "val_acc": 52.0}
{"epoch": 9, "training_loss": 37697.587951660156, "training_acc": 53.0, "val_loss": 1421.953010559082, "val_acc": 52.0}
{"epoch": 10, "training_loss": 15040.500732421875, "training_acc": 49.0, "val_loss": 10795.767211914062, "val_acc": 48.0}
{"epoch": 11, "training_loss": 41278.32385253906, "training_acc": 47.0, "val_loss": 2196.1313247680664, "val_acc": 48.0}
{"epoch": 12, "training_loss": 14939.522705078125, "training_acc": 49.0, "val_loss": 11520.49560546875, "val_acc": 52.0}
{"epoch": 13, "training_loss": 44360.272705078125, "training_acc": 53.0, "val_loss": 8687.771606445312, "val_acc": 52.0}
{"epoch": 14, "training_loss": 23524.33203125, "training_acc": 53.0, "val_loss": 8400.732421875, "val_acc": 48.0}
{"epoch": 15, "training_loss": 41147.53662109375, "training_acc": 47.0, "val_loss": 12822.451782226562, "val_acc": 48.0}
{"epoch": 16, "training_loss": 43631.86877441406, "training_acc": 47.0, "val_loss": 699.4660377502441, "val_acc": 60.0}
{"epoch": 17, "training_loss": 10808.613220214844, "training_acc": 58.0, "val_loss": 8951.097106933594, "val_acc": 52.0}
{"epoch": 18, "training_loss": 30688.780212402344, "training_acc": 53.0, "val_loss": 2236.899757385254, "val_acc": 52.0}
{"epoch": 19, "training_loss": 10035.855285644531, "training_acc": 59.0, "val_loss": 7396.4752197265625, "val_acc": 48.0}
{"epoch": 20, "training_loss": 27286.720275878906, "training_acc": 47.0, "val_loss": 772.2077369689941, "val_acc": 64.0}
{"epoch": 21, "training_loss": 7707.831726074219, "training_acc": 56.0, "val_loss": 5716.629409790039, "val_acc": 52.0}
{"epoch": 22, "training_loss": 13879.640121459961, "training_acc": 48.0, "val_loss": 3383.475875854492, "val_acc": 48.0}
{"epoch": 23, "training_loss": 15379.23974609375, "training_acc": 47.0, "val_loss": 696.2278842926025, "val_acc": 40.0}
{"epoch": 24, "training_loss": 8149.4241943359375, "training_acc": 56.0, "val_loss": 7185.508728027344, "val_acc": 52.0}
{"epoch": 25, "training_loss": 21096.03533935547, "training_acc": 53.0, "val_loss": 681.7925453186035, "val_acc": 48.0}
{"epoch": 26, "training_loss": 8961.014892578125, "training_acc": 57.0, "val_loss": 2561.499786376953, "val_acc": 48.0}
{"epoch": 27, "training_loss": 10855.9951171875, "training_acc": 49.0, "val_loss": 3837.2940063476562, "val_acc": 52.0}
{"epoch": 28, "training_loss": 8562.22102355957, "training_acc": 56.0, "val_loss": 3587.6644134521484, "val_acc": 48.0}
{"epoch": 29, "training_loss": 16546.4765625, "training_acc": 47.0, "val_loss": 791.7905330657959, "val_acc": 56.0}
{"epoch": 30, "training_loss": 6924.0635986328125, "training_acc": 58.0, "val_loss": 4369.968795776367, "val_acc": 52.0}
{"epoch": 31, "training_loss": 10495.709930419922, "training_acc": 53.0, "val_loss": 2659.8329544067383, "val_acc": 48.0}
{"epoch": 32, "training_loss": 7923.377410888672, "training_acc": 52.0, "val_loss": 2858.791923522949, "val_acc": 52.0}
{"epoch": 33, "training_loss": 11188.8447265625, "training_acc": 55.0, "val_loss": 714.280891418457, "val_acc": 56.0}
{"epoch": 34, "training_loss": 4468.381362915039, "training_acc": 59.0, "val_loss": 1680.2337646484375, "val_acc": 48.0}
{"epoch": 35, "training_loss": 6371.20556640625, "training_acc": 52.0, "val_loss": 2063.422393798828, "val_acc": 52.0}
{"epoch": 36, "training_loss": 4366.49690246582, "training_acc": 57.0, "val_loss": 679.2343139648438, "val_acc": 44.0}
{"epoch": 37, "training_loss": 5419.986724853516, "training_acc": 56.0, "val_loss": 1724.3959426879883, "val_acc": 56.0}
{"epoch": 38, "training_loss": 6083.659759521484, "training_acc": 54.0, "val_loss": 1101.3907432556152, "val_acc": 48.0}
{"epoch": 39, "training_loss": 6941.6256103515625, "training_acc": 54.0, "val_loss": 3297.233200073242, "val_acc": 52.0}
{"epoch": 40, "training_loss": 7600.129657745361, "training_acc": 57.0, "val_loss": 2380.9764862060547, "val_acc": 48.0}
{"epoch": 41, "training_loss": 6937.6143798828125, "training_acc": 57.0, "val_loss": 2785.2724075317383, "val_acc": 52.0}
{"epoch": 42, "training_loss": 7990.440994262695, "training_acc": 53.0, "val_loss": 2397.8532791137695, "val_acc": 48.0}
{"epoch": 43, "training_loss": 9076.902557373047, "training_acc": 47.0, "val_loss": 2345.8375930786133, "val_acc": 52.0}
{"epoch": 44, "training_loss": 8087.159637451172, "training_acc": 53.0, "val_loss": 1045.56884765625, "val_acc": 52.0}
{"epoch": 45, "training_loss": 5078.513671875, "training_acc": 50.0, "val_loss": 3257.4256896972656, "val_acc": 52.0}
{"epoch": 46, "training_loss": 11675.702819824219, "training_acc": 53.0, "val_loss": 620.551872253418, "val_acc": 64.0}
{"epoch": 47, "training_loss": 6762.2252197265625, "training_acc": 55.0, "val_loss": 2878.803253173828, "val_acc": 48.0}
{"epoch": 48, "training_loss": 7925.988182067871, "training_acc": 58.0, "val_loss": 2349.1302490234375, "val_acc": 52.0}
{"epoch": 49, "training_loss": 4855.809944152832, "training_acc": 55.0, "val_loss": 323.26812744140625, "val_acc": 56.0}
{"epoch": 50, "training_loss": 2853.3729553222656, "training_acc": 64.0, "val_loss": 755.0037860870361, "val_acc": 64.0}
{"epoch": 51, "training_loss": 2411.3547592163086, "training_acc": 60.0, "val_loss": 902.8873443603516, "val_acc": 60.0}
{"epoch": 52, "training_loss": 2505.5929412841797, "training_acc": 64.0, "val_loss": 775.3481388092041, "val_acc": 48.0}
{"epoch": 53, "training_loss": 2498.5307998657227, "training_acc": 59.0, "val_loss": 1986.088752746582, "val_acc": 52.0}
{"epoch": 54, "training_loss": 3847.382806777954, "training_acc": 63.0, "val_loss": 1179.6561241149902, "val_acc": 48.0}
{"epoch": 55, "training_loss": 3859.3901977539062, "training_acc": 58.0, "val_loss": 1384.8522186279297, "val_acc": 56.0}
{"epoch": 56, "training_loss": 6325.868194580078, "training_acc": 44.0, "val_loss": 504.75006103515625, "val_acc": 48.0}
{"epoch": 57, "training_loss": 5756.203521728516, "training_acc": 60.0, "val_loss": 3783.43505859375, "val_acc": 52.0}
{"epoch": 58, "training_loss": 9590.676963806152, "training_acc": 54.0, "val_loss": 1935.4156494140625, "val_acc": 48.0}
{"epoch": 59, "training_loss": 5554.390296936035, "training_acc": 49.0, "val_loss": 930.8218955993652, "val_acc": 60.0}
{"epoch": 60, "training_loss": 1322.3991622924805, "training_acc": 73.0, "val_loss": 616.9377326965332, "val_acc": 44.0}
{"epoch": 61, "training_loss": 3745.8466186523438, "training_acc": 57.0, "val_loss": 1360.319995880127, "val_acc": 56.0}
{"epoch": 62, "training_loss": 5377.908355712891, "training_acc": 57.0, "val_loss": 2132.2948455810547, "val_acc": 48.0}
{"epoch": 63, "training_loss": 5073.015350341797, "training_acc": 61.0, "val_loss": 3091.85791015625, "val_acc": 52.0}
{"epoch": 64, "training_loss": 6598.013702392578, "training_acc": 60.0, "val_loss": 3172.2755432128906, "val_acc": 48.0}
{"epoch": 65, "training_loss": 10409.282775878906, "training_acc": 47.0, "val_loss": 3811.014175415039, "val_acc": 52.0}
{"epoch": 66, "training_loss": 13043.012390136719, "training_acc": 53.0, "val_loss": 2268.8600540161133, "val_acc": 52.0}
{"epoch": 67, "training_loss": 7120.402740478516, "training_acc": 59.0, "val_loss": 4561.214828491211, "val_acc": 48.0}
{"epoch": 68, "training_loss": 13654.965789794922, "training_acc": 50.0, "val_loss": 5559.462356567383, "val_acc": 52.0}
