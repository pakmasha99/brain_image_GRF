"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 25450.465381622314, "training_acc": 53.0, "val_loss": 8119.402313232422, "val_acc": 48.0}
{"epoch": 1, "training_loss": 27877.31817626953, "training_acc": 47.0, "val_loss": 10689.44320678711, "val_acc": 52.0}
{"epoch": 2, "training_loss": 42432.73083496094, "training_acc": 53.0, "val_loss": 5508.801651000977, "val_acc": 52.0}
{"epoch": 3, "training_loss": 27858.4453125, "training_acc": 45.0, "val_loss": 9755.362701416016, "val_acc": 48.0}
{"epoch": 4, "training_loss": 33947.30236816406, "training_acc": 47.0, "val_loss": 5152.225494384766, "val_acc": 52.0}
{"epoch": 5, "training_loss": 20474.56866455078, "training_acc": 53.0, "val_loss": 7028.958892822266, "val_acc": 52.0}
{"epoch": 6, "training_loss": 15733.591018676758, "training_acc": 55.0, "val_loss": 5105.723571777344, "val_acc": 48.0}
{"epoch": 7, "training_loss": 22714.12615966797, "training_acc": 47.0, "val_loss": 1113.3914947509766, "val_acc": 52.0}
{"epoch": 8, "training_loss": 11868.7822265625, "training_acc": 51.0, "val_loss": 7794.528961181641, "val_acc": 52.0}
{"epoch": 9, "training_loss": 19613.768127441406, "training_acc": 52.0, "val_loss": 1709.7013473510742, "val_acc": 60.0}
{"epoch": 10, "training_loss": 13757.303039550781, "training_acc": 47.0, "val_loss": 1166.2219047546387, "val_acc": 52.0}
{"epoch": 11, "training_loss": 8483.106750488281, "training_acc": 57.0, "val_loss": 4458.025360107422, "val_acc": 52.0}
{"epoch": 12, "training_loss": 9261.509338378906, "training_acc": 53.0, "val_loss": 1178.3244132995605, "val_acc": 64.0}
{"epoch": 13, "training_loss": 5303.418037414551, "training_acc": 58.0, "val_loss": 1957.884407043457, "val_acc": 52.0}
{"epoch": 14, "training_loss": 5475.048095703125, "training_acc": 59.0, "val_loss": 561.162805557251, "val_acc": 64.0}
{"epoch": 15, "training_loss": 4191.44580078125, "training_acc": 61.0, "val_loss": 581.2161922454834, "val_acc": 68.0}
{"epoch": 16, "training_loss": 3638.869842529297, "training_acc": 60.0, "val_loss": 899.7570991516113, "val_acc": 48.0}
{"epoch": 17, "training_loss": 2449.562858581543, "training_acc": 67.0, "val_loss": 1687.376594543457, "val_acc": 48.0}
{"epoch": 18, "training_loss": 5864.254249572754, "training_acc": 52.0, "val_loss": 296.13823890686035, "val_acc": 68.0}
{"epoch": 19, "training_loss": 1531.408805847168, "training_acc": 70.0, "val_loss": 1549.6122360229492, "val_acc": 52.0}
{"epoch": 20, "training_loss": 4333.410751342773, "training_acc": 55.0, "val_loss": 1154.0419578552246, "val_acc": 52.0}
{"epoch": 21, "training_loss": 3601.510208129883, "training_acc": 53.0, "val_loss": 1891.280174255371, "val_acc": 52.0}
{"epoch": 22, "training_loss": 3165.3750534057617, "training_acc": 57.0, "val_loss": 607.2135925292969, "val_acc": 52.0}
{"epoch": 23, "training_loss": 4585.796905517578, "training_acc": 56.0, "val_loss": 2112.504768371582, "val_acc": 52.0}
{"epoch": 24, "training_loss": 8897.543151855469, "training_acc": 46.0, "val_loss": 2089.9354934692383, "val_acc": 48.0}
{"epoch": 25, "training_loss": 9028.460754394531, "training_acc": 53.0, "val_loss": 4990.576171875, "val_acc": 52.0}
{"epoch": 26, "training_loss": 9908.961364746094, "training_acc": 54.0, "val_loss": 5178.955078125, "val_acc": 48.0}
{"epoch": 27, "training_loss": 27149.016235351562, "training_acc": 47.0, "val_loss": 4255.689239501953, "val_acc": 48.0}
{"epoch": 28, "training_loss": 17541.712890625, "training_acc": 45.0, "val_loss": 6392.26188659668, "val_acc": 52.0}
{"epoch": 29, "training_loss": 16517.525817871094, "training_acc": 53.0, "val_loss": 1431.3573837280273, "val_acc": 48.0}
{"epoch": 30, "training_loss": 8804.384887695312, "training_acc": 49.0, "val_loss": 1608.335304260254, "val_acc": 52.0}
{"epoch": 31, "training_loss": 4583.222595214844, "training_acc": 62.0, "val_loss": 624.6849060058594, "val_acc": 28.0}
{"epoch": 32, "training_loss": 2610.41357421875, "training_acc": 60.0, "val_loss": 2207.349967956543, "val_acc": 52.0}
{"epoch": 33, "training_loss": 3147.589370727539, "training_acc": 64.0, "val_loss": 2810.6063842773438, "val_acc": 48.0}
{"epoch": 34, "training_loss": 8218.87158203125, "training_acc": 47.0, "val_loss": 4369.534683227539, "val_acc": 52.0}
{"epoch": 35, "training_loss": 15320.170227050781, "training_acc": 53.0, "val_loss": 1855.9967041015625, "val_acc": 52.0}
{"epoch": 36, "training_loss": 11401.274353027344, "training_acc": 51.0, "val_loss": 7465.258026123047, "val_acc": 48.0}
{"epoch": 37, "training_loss": 26425.81671142578, "training_acc": 46.0, "val_loss": 2352.11124420166, "val_acc": 52.0}
