"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 294240.11974716187, "training_acc": 44.0, "val_loss": 184285.11962890625, "val_acc": 52.0}
{"epoch": 1, "training_loss": 753505.34765625, "training_acc": 53.0, "val_loss": 74386.13891601562, "val_acc": 52.0}
{"epoch": 2, "training_loss": 531441.80078125, "training_acc": 41.0, "val_loss": 206443.75, "val_acc": 48.0}
{"epoch": 3, "training_loss": 738788.12109375, "training_acc": 47.0, "val_loss": 20729.52117919922, "val_acc": 52.0}
{"epoch": 4, "training_loss": 143128.333984375, "training_acc": 53.0, "val_loss": 30865.972900390625, "val_acc": 52.0}
{"epoch": 5, "training_loss": 219813.013671875, "training_acc": 43.0, "val_loss": 78167.24243164062, "val_acc": 48.0}
{"epoch": 6, "training_loss": 216908.62158203125, "training_acc": 47.0, "val_loss": 108305.31005859375, "val_acc": 52.0}
{"epoch": 7, "training_loss": 494992.02734375, "training_acc": 53.0, "val_loss": 146670.03173828125, "val_acc": 52.0}
{"epoch": 8, "training_loss": 484442.154296875, "training_acc": 53.0, "val_loss": 22842.198181152344, "val_acc": 48.0}
{"epoch": 9, "training_loss": 159198.9951171875, "training_acc": 47.0, "val_loss": 43131.793212890625, "val_acc": 48.0}
{"epoch": 10, "training_loss": 135797.48486328125, "training_acc": 55.0, "val_loss": 53079.022216796875, "val_acc": 52.0}
{"epoch": 11, "training_loss": 170631.62451171875, "training_acc": 53.0, "val_loss": 51959.1064453125, "val_acc": 48.0}
{"epoch": 12, "training_loss": 225909.2158203125, "training_acc": 47.0, "val_loss": 20480.003356933594, "val_acc": 48.0}
{"epoch": 13, "training_loss": 150310.4111328125, "training_acc": 49.0, "val_loss": 102372.91259765625, "val_acc": 52.0}
{"epoch": 14, "training_loss": 378595.8583984375, "training_acc": 53.0, "val_loss": 23387.79296875, "val_acc": 52.0}
{"epoch": 15, "training_loss": 190757.73046875, "training_acc": 49.0, "val_loss": 126855.70068359375, "val_acc": 48.0}
{"epoch": 16, "training_loss": 488424.599609375, "training_acc": 47.0, "val_loss": 35160.955810546875, "val_acc": 48.0}
{"epoch": 17, "training_loss": 193798.4345703125, "training_acc": 51.0, "val_loss": 135067.08984375, "val_acc": 52.0}
{"epoch": 18, "training_loss": 546924.35546875, "training_acc": 53.0, "val_loss": 101643.92700195312, "val_acc": 52.0}
{"epoch": 19, "training_loss": 287642.9494628906, "training_acc": 53.0, "val_loss": 110356.201171875, "val_acc": 48.0}
{"epoch": 20, "training_loss": 543448.947265625, "training_acc": 47.0, "val_loss": 170953.369140625, "val_acc": 48.0}
{"epoch": 21, "training_loss": 611111.607421875, "training_acc": 47.0, "val_loss": 9210.099029541016, "val_acc": 48.0}
{"epoch": 22, "training_loss": 151540.8369140625, "training_acc": 57.0, "val_loss": 207941.8212890625, "val_acc": 52.0}
{"epoch": 23, "training_loss": 875330.68359375, "training_acc": 53.0, "val_loss": 223250.4638671875, "val_acc": 52.0}
{"epoch": 24, "training_loss": 780886.197265625, "training_acc": 53.0, "val_loss": 57452.978515625, "val_acc": 52.0}
{"epoch": 25, "training_loss": 245696.4853515625, "training_acc": 55.0, "val_loss": 169245.8984375, "val_acc": 48.0}
{"epoch": 26, "training_loss": 703143.16015625, "training_acc": 47.0, "val_loss": 152719.42138671875, "val_acc": 48.0}
{"epoch": 27, "training_loss": 494799.845703125, "training_acc": 47.0, "val_loss": 52993.621826171875, "val_acc": 52.0}
{"epoch": 28, "training_loss": 291541.134765625, "training_acc": 53.0, "val_loss": 128055.078125, "val_acc": 52.0}
{"epoch": 29, "training_loss": 456586.087890625, "training_acc": 53.0, "val_loss": 21496.046447753906, "val_acc": 52.0}
{"epoch": 30, "training_loss": 206928.845703125, "training_acc": 51.0, "val_loss": 166300.3662109375, "val_acc": 48.0}
{"epoch": 31, "training_loss": 679873.80859375, "training_acc": 47.0, "val_loss": 113060.70556640625, "val_acc": 48.0}
{"epoch": 32, "training_loss": 309786.4349975586, "training_acc": 47.0, "val_loss": 130561.4013671875, "val_acc": 52.0}
{"epoch": 33, "training_loss": 583382.3671875, "training_acc": 53.0, "val_loss": 233796.19140625, "val_acc": 52.0}
{"epoch": 34, "training_loss": 892928.884765625, "training_acc": 53.0, "val_loss": 158796.8505859375, "val_acc": 52.0}
{"epoch": 35, "training_loss": 478967.5458984375, "training_acc": 53.0, "val_loss": 69863.28125, "val_acc": 48.0}
{"epoch": 36, "training_loss": 373885.59765625, "training_acc": 47.0, "val_loss": 155904.3212890625, "val_acc": 48.0}
{"epoch": 37, "training_loss": 573978.4189453125, "training_acc": 47.0, "val_loss": 22823.846435546875, "val_acc": 48.0}
{"epoch": 38, "training_loss": 255248.876953125, "training_acc": 41.0, "val_loss": 169408.49609375, "val_acc": 52.0}
{"epoch": 39, "training_loss": 702823.4296875, "training_acc": 53.0, "val_loss": 146406.97021484375, "val_acc": 52.0}
{"epoch": 40, "training_loss": 452583.9677734375, "training_acc": 53.0, "val_loss": 59501.08642578125, "val_acc": 48.0}
