"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 753845.6683654785, "training_acc": 51.0, "val_loss": 156504.6875, "val_acc": 52.0}
{"epoch": 1, "training_loss": 789104.171875, "training_acc": 49.0, "val_loss": 338929.833984375, "val_acc": 48.0}
{"epoch": 2, "training_loss": 1266516.171875, "training_acc": 47.0, "val_loss": 82019.82421875, "val_acc": 48.0}
{"epoch": 3, "training_loss": 481417.103515625, "training_acc": 45.0, "val_loss": 266317.67578125, "val_acc": 52.0}
{"epoch": 4, "training_loss": 1063319.41796875, "training_acc": 53.0, "val_loss": 224518.1640625, "val_acc": 52.0}
{"epoch": 5, "training_loss": 715640.05859375, "training_acc": 53.0, "val_loss": 58529.60205078125, "val_acc": 48.0}
{"epoch": 6, "training_loss": 342379.09765625, "training_acc": 47.0, "val_loss": 131679.58984375, "val_acc": 48.0}
{"epoch": 7, "training_loss": 434997.2607421875, "training_acc": 47.0, "val_loss": 63794.390869140625, "val_acc": 52.0}
{"epoch": 8, "training_loss": 289763.53515625, "training_acc": 53.0, "val_loss": 103274.79248046875, "val_acc": 52.0}
{"epoch": 9, "training_loss": 335252.00048828125, "training_acc": 53.0, "val_loss": 54922.39990234375, "val_acc": 48.0}
{"epoch": 10, "training_loss": 260533.0009765625, "training_acc": 47.0, "val_loss": 47781.927490234375, "val_acc": 48.0}
{"epoch": 11, "training_loss": 229705.234375, "training_acc": 39.0, "val_loss": 65527.734375, "val_acc": 52.0}
{"epoch": 12, "training_loss": 210568.74267578125, "training_acc": 53.0, "val_loss": 45549.237060546875, "val_acc": 48.0}
{"epoch": 13, "training_loss": 225019.873046875, "training_acc": 47.0, "val_loss": 24473.08349609375, "val_acc": 48.0}
{"epoch": 14, "training_loss": 139825.41796875, "training_acc": 53.0, "val_loss": 100516.32690429688, "val_acc": 52.0}
{"epoch": 15, "training_loss": 373140.62109375, "training_acc": 53.0, "val_loss": 28760.836791992188, "val_acc": 52.0}
{"epoch": 16, "training_loss": 202946.83984375, "training_acc": 47.0, "val_loss": 115584.46044921875, "val_acc": 48.0}
{"epoch": 17, "training_loss": 436495.66015625, "training_acc": 47.0, "val_loss": 16675.01983642578, "val_acc": 48.0}
{"epoch": 18, "training_loss": 238167.92578125, "training_acc": 39.0, "val_loss": 150689.306640625, "val_acc": 52.0}
{"epoch": 19, "training_loss": 584004.6640625, "training_acc": 53.0, "val_loss": 102439.44091796875, "val_acc": 52.0}
{"epoch": 20, "training_loss": 278464.1320800781, "training_acc": 53.0, "val_loss": 113146.19140625, "val_acc": 48.0}
{"epoch": 21, "training_loss": 540142.248046875, "training_acc": 47.0, "val_loss": 179328.64990234375, "val_acc": 48.0}
{"epoch": 22, "training_loss": 657944.962890625, "training_acc": 47.0, "val_loss": 32183.69140625, "val_acc": 48.0}
{"epoch": 23, "training_loss": 203730.3837890625, "training_acc": 53.0, "val_loss": 177989.02587890625, "val_acc": 52.0}
{"epoch": 24, "training_loss": 722318.001953125, "training_acc": 53.0, "val_loss": 181616.357421875, "val_acc": 52.0}
{"epoch": 25, "training_loss": 620992.33984375, "training_acc": 53.0, "val_loss": 19580.526733398438, "val_acc": 52.0}
{"epoch": 26, "training_loss": 217217.767578125, "training_acc": 53.0, "val_loss": 202402.7099609375, "val_acc": 48.0}
{"epoch": 27, "training_loss": 842146.462890625, "training_acc": 47.0, "val_loss": 178262.80517578125, "val_acc": 48.0}
{"epoch": 28, "training_loss": 586343.279296875, "training_acc": 47.0, "val_loss": 38888.22937011719, "val_acc": 52.0}
{"epoch": 29, "training_loss": 255442.783203125, "training_acc": 53.0, "val_loss": 126694.1650390625, "val_acc": 52.0}
{"epoch": 30, "training_loss": 450737.7646484375, "training_acc": 53.0, "val_loss": 26180.108642578125, "val_acc": 52.0}
{"epoch": 31, "training_loss": 228978.787109375, "training_acc": 45.0, "val_loss": 141319.59228515625, "val_acc": 48.0}
{"epoch": 32, "training_loss": 559485.833984375, "training_acc": 47.0, "val_loss": 64880.04150390625, "val_acc": 48.0}
{"epoch": 33, "training_loss": 210278.0556640625, "training_acc": 53.0, "val_loss": 92649.89013671875, "val_acc": 52.0}
{"epoch": 34, "training_loss": 360918.72265625, "training_acc": 53.0, "val_loss": 51921.044921875, "val_acc": 52.0}
{"epoch": 35, "training_loss": 134195.97485351562, "training_acc": 61.0, "val_loss": 59993.365478515625, "val_acc": 48.0}
{"epoch": 36, "training_loss": 210260.91455078125, "training_acc": 47.0, "val_loss": 37284.43603515625, "val_acc": 52.0}
