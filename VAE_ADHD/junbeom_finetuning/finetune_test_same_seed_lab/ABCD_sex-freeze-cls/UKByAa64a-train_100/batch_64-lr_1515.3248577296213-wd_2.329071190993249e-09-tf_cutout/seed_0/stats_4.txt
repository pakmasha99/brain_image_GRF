"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 2932548.2102661133, "training_acc": 53.0, "val_loss": 448465.673828125, "val_acc": 52.0}
{"epoch": 1, "training_loss": 3329126.578125, "training_acc": 51.0, "val_loss": 1929789.2578125, "val_acc": 48.0}
{"epoch": 2, "training_loss": 7223422.25, "training_acc": 47.0, "val_loss": 705340.72265625, "val_acc": 48.0}
{"epoch": 3, "training_loss": 2657548.6328125, "training_acc": 45.0, "val_loss": 979978.7109375, "val_acc": 52.0}
{"epoch": 4, "training_loss": 3847500.46875, "training_acc": 53.0, "val_loss": 789131.689453125, "val_acc": 52.0}
{"epoch": 5, "training_loss": 2557952.896484375, "training_acc": 53.0, "val_loss": 562090.91796875, "val_acc": 48.0}
{"epoch": 6, "training_loss": 2493531.1640625, "training_acc": 48.0, "val_loss": 847387.109375, "val_acc": 48.0}
{"epoch": 7, "training_loss": 2710209.1640625, "training_acc": 48.0, "val_loss": 89622.30834960938, "val_acc": 44.0}
{"epoch": 8, "training_loss": 697479.328125, "training_acc": 62.0, "val_loss": 559097.607421875, "val_acc": 52.0}
{"epoch": 9, "training_loss": 2006159.1875, "training_acc": 53.0, "val_loss": 56236.676025390625, "val_acc": 56.0}
{"epoch": 10, "training_loss": 849368.4375, "training_acc": 48.0, "val_loss": 446589.2578125, "val_acc": 48.0}
{"epoch": 11, "training_loss": 1361070.6328125, "training_acc": 46.0, "val_loss": 233178.22265625, "val_acc": 52.0}
{"epoch": 12, "training_loss": 1149040.65234375, "training_acc": 52.0, "val_loss": 264002.783203125, "val_acc": 52.0}
{"epoch": 13, "training_loss": 973066.529296875, "training_acc": 45.0, "val_loss": 219847.8759765625, "val_acc": 48.0}
{"epoch": 14, "training_loss": 649615.923828125, "training_acc": 56.0, "val_loss": 155067.24853515625, "val_acc": 52.0}
{"epoch": 15, "training_loss": 494994.62890625, "training_acc": 54.0, "val_loss": 152541.9189453125, "val_acc": 48.0}
{"epoch": 16, "training_loss": 515571.177734375, "training_acc": 49.0, "val_loss": 103574.89013671875, "val_acc": 52.0}
{"epoch": 17, "training_loss": 393233.8291015625, "training_acc": 58.0, "val_loss": 82957.21435546875, "val_acc": 44.0}
{"epoch": 18, "training_loss": 307912.185546875, "training_acc": 54.0, "val_loss": 114528.52783203125, "val_acc": 52.0}
{"epoch": 19, "training_loss": 321501.341796875, "training_acc": 58.0, "val_loss": 117891.6259765625, "val_acc": 44.0}
{"epoch": 20, "training_loss": 316020.033203125, "training_acc": 56.0, "val_loss": 97513.3544921875, "val_acc": 52.0}
{"epoch": 21, "training_loss": 257778.5966796875, "training_acc": 62.0, "val_loss": 91233.75854492188, "val_acc": 48.0}
{"epoch": 22, "training_loss": 219817.69482421875, "training_acc": 59.0, "val_loss": 172671.83837890625, "val_acc": 52.0}
{"epoch": 23, "training_loss": 486796.5166015625, "training_acc": 54.0, "val_loss": 192055.96923828125, "val_acc": 48.0}
{"epoch": 24, "training_loss": 648531.48828125, "training_acc": 47.0, "val_loss": 119194.07958984375, "val_acc": 52.0}
{"epoch": 25, "training_loss": 402019.3134765625, "training_acc": 53.0, "val_loss": 53610.9375, "val_acc": 44.0}
{"epoch": 26, "training_loss": 191893.4912109375, "training_acc": 64.0, "val_loss": 102521.37451171875, "val_acc": 52.0}
{"epoch": 27, "training_loss": 256976.4208984375, "training_acc": 61.0, "val_loss": 165419.71435546875, "val_acc": 48.0}
{"epoch": 28, "training_loss": 501072.7998046875, "training_acc": 47.0, "val_loss": 213866.30859375, "val_acc": 52.0}
{"epoch": 29, "training_loss": 953748.33984375, "training_acc": 53.0, "val_loss": 52244.647216796875, "val_acc": 52.0}
{"epoch": 30, "training_loss": 604130.87109375, "training_acc": 56.0, "val_loss": 396354.736328125, "val_acc": 48.0}
{"epoch": 31, "training_loss": 1206116.029296875, "training_acc": 47.0, "val_loss": 303466.50390625, "val_acc": 52.0}
{"epoch": 32, "training_loss": 1501354.8125, "training_acc": 53.0, "val_loss": 430044.7265625, "val_acc": 52.0}
{"epoch": 33, "training_loss": 1133461.349609375, "training_acc": 54.0, "val_loss": 442088.818359375, "val_acc": 48.0}
{"epoch": 34, "training_loss": 2117816.171875, "training_acc": 47.0, "val_loss": 576504.8828125, "val_acc": 48.0}
{"epoch": 35, "training_loss": 1792708.322265625, "training_acc": 47.0, "val_loss": 369393.359375, "val_acc": 52.0}
{"epoch": 36, "training_loss": 1559459.640625, "training_acc": 53.0, "val_loss": 650006.25, "val_acc": 52.0}
{"epoch": 37, "training_loss": 2180972.5703125, "training_acc": 53.0, "val_loss": 66468.2861328125, "val_acc": 56.0}
{"epoch": 38, "training_loss": 737469.9375, "training_acc": 61.0, "val_loss": 685743.65234375, "val_acc": 48.0}
{"epoch": 39, "training_loss": 2585767.8828125, "training_acc": 47.0, "val_loss": 259887.4267578125, "val_acc": 48.0}
{"epoch": 40, "training_loss": 1029870.453125, "training_acc": 45.0, "val_loss": 541038.232421875, "val_acc": 52.0}
{"epoch": 41, "training_loss": 2053683.359375, "training_acc": 53.0, "val_loss": 289510.986328125, "val_acc": 52.0}
{"epoch": 42, "training_loss": 608520.5283203125, "training_acc": 65.0, "val_loss": 389176.123046875, "val_acc": 48.0}
{"epoch": 43, "training_loss": 1319877.71484375, "training_acc": 47.0, "val_loss": 43198.931884765625, "val_acc": 52.0}
{"epoch": 44, "training_loss": 311662.693359375, "training_acc": 62.0, "val_loss": 262346.6796875, "val_acc": 52.0}
{"epoch": 45, "training_loss": 771747.962890625, "training_acc": 56.0, "val_loss": 287932.9833984375, "val_acc": 48.0}
{"epoch": 46, "training_loss": 1162431.1953125, "training_acc": 47.0, "val_loss": 168372.52197265625, "val_acc": 48.0}
{"epoch": 47, "training_loss": 840809.73828125, "training_acc": 43.0, "val_loss": 433932.568359375, "val_acc": 52.0}
{"epoch": 48, "training_loss": 1370624.4375, "training_acc": 53.0, "val_loss": 51846.856689453125, "val_acc": 64.0}
{"epoch": 49, "training_loss": 466339.59765625, "training_acc": 58.0, "val_loss": 288518.1884765625, "val_acc": 48.0}
{"epoch": 50, "training_loss": 675395.4477539062, "training_acc": 54.0, "val_loss": 213200.341796875, "val_acc": 52.0}
{"epoch": 51, "training_loss": 811411.90625, "training_acc": 53.0, "val_loss": 56074.407958984375, "val_acc": 44.0}
{"epoch": 52, "training_loss": 401812.39453125, "training_acc": 60.0, "val_loss": 176641.4306640625, "val_acc": 48.0}
{"epoch": 53, "training_loss": 445023.267578125, "training_acc": 58.0, "val_loss": 231148.53515625, "val_acc": 52.0}
{"epoch": 54, "training_loss": 677225.650390625, "training_acc": 55.0, "val_loss": 192225.81787109375, "val_acc": 48.0}
{"epoch": 55, "training_loss": 661651.18359375, "training_acc": 48.0, "val_loss": 59640.9423828125, "val_acc": 68.0}
{"epoch": 56, "training_loss": 591013.3828125, "training_acc": 45.0, "val_loss": 152325.54931640625, "val_acc": 52.0}
{"epoch": 57, "training_loss": 434960.123046875, "training_acc": 59.0, "val_loss": 107903.23486328125, "val_acc": 40.0}
{"epoch": 58, "training_loss": 300398.33642578125, "training_acc": 61.0, "val_loss": 180726.025390625, "val_acc": 52.0}
{"epoch": 59, "training_loss": 374431.77490234375, "training_acc": 60.0, "val_loss": 176450.9765625, "val_acc": 48.0}
{"epoch": 60, "training_loss": 465044.32373046875, "training_acc": 47.0, "val_loss": 121421.5576171875, "val_acc": 52.0}
{"epoch": 61, "training_loss": 308391.68115234375, "training_acc": 57.0, "val_loss": 73513.53149414062, "val_acc": 44.0}
{"epoch": 62, "training_loss": 121012.2138671875, "training_acc": 67.0, "val_loss": 122749.0966796875, "val_acc": 52.0}
