"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 3182609.5262947083, "training_acc": 46.0, "val_loss": 665229.98046875, "val_acc": 52.0}
{"epoch": 1, "training_loss": 3230295.90625, "training_acc": 53.0, "val_loss": 1722806.25, "val_acc": 48.0}
{"epoch": 2, "training_loss": 6483787.40625, "training_acc": 47.0, "val_loss": 477552.783203125, "val_acc": 48.0}
{"epoch": 3, "training_loss": 2259287.5234375, "training_acc": 51.0, "val_loss": 1258187.20703125, "val_acc": 52.0}
{"epoch": 4, "training_loss": 4780206.890625, "training_acc": 53.0, "val_loss": 1122709.765625, "val_acc": 52.0}
{"epoch": 5, "training_loss": 3332108.8515625, "training_acc": 53.0, "val_loss": 195082.72705078125, "val_acc": 48.0}
{"epoch": 6, "training_loss": 1797980.375, "training_acc": 47.0, "val_loss": 575056.005859375, "val_acc": 48.0}
{"epoch": 7, "training_loss": 1949200.625, "training_acc": 47.0, "val_loss": 416671.630859375, "val_acc": 52.0}
{"epoch": 8, "training_loss": 1624716.8828125, "training_acc": 53.0, "val_loss": 722490.0390625, "val_acc": 52.0}
{"epoch": 9, "training_loss": 2346032.0, "training_acc": 53.0, "val_loss": 91031.31103515625, "val_acc": 52.0}
{"epoch": 10, "training_loss": 938308.734375, "training_acc": 49.0, "val_loss": 671789.2578125, "val_acc": 48.0}
{"epoch": 11, "training_loss": 2573948.3359375, "training_acc": 47.0, "val_loss": 142511.1572265625, "val_acc": 48.0}
{"epoch": 12, "training_loss": 932596.38671875, "training_acc": 49.0, "val_loss": 706882.421875, "val_acc": 52.0}
{"epoch": 13, "training_loss": 2717770.03125, "training_acc": 53.0, "val_loss": 529916.845703125, "val_acc": 52.0}
{"epoch": 14, "training_loss": 1422569.2158203125, "training_acc": 52.0, "val_loss": 519693.603515625, "val_acc": 48.0}
{"epoch": 15, "training_loss": 2531902.96875, "training_acc": 47.0, "val_loss": 772942.87109375, "val_acc": 48.0}
{"epoch": 16, "training_loss": 2607778.7265625, "training_acc": 47.0, "val_loss": 70389.32495117188, "val_acc": 60.0}
{"epoch": 17, "training_loss": 647534.98046875, "training_acc": 54.0, "val_loss": 439529.00390625, "val_acc": 52.0}
{"epoch": 18, "training_loss": 1410984.771484375, "training_acc": 53.0, "val_loss": 99613.65966796875, "val_acc": 48.0}
{"epoch": 19, "training_loss": 438514.416015625, "training_acc": 50.0, "val_loss": 47586.273193359375, "val_acc": 56.0}
{"epoch": 20, "training_loss": 320067.6171875, "training_acc": 60.0, "val_loss": 73708.96606445312, "val_acc": 60.0}
{"epoch": 21, "training_loss": 275452.3515625, "training_acc": 55.0, "val_loss": 79274.48120117188, "val_acc": 44.0}
{"epoch": 22, "training_loss": 383083.990234375, "training_acc": 55.0, "val_loss": 209022.36328125, "val_acc": 52.0}
{"epoch": 23, "training_loss": 391458.91796875, "training_acc": 62.0, "val_loss": 205506.34765625, "val_acc": 48.0}
{"epoch": 24, "training_loss": 760323.0703125, "training_acc": 48.0, "val_loss": 167629.1015625, "val_acc": 52.0}
{"epoch": 25, "training_loss": 542441.033203125, "training_acc": 55.0, "val_loss": 74786.96899414062, "val_acc": 60.0}
{"epoch": 26, "training_loss": 555845.59375, "training_acc": 52.0, "val_loss": 184255.4443359375, "val_acc": 48.0}
{"epoch": 27, "training_loss": 650904.4375, "training_acc": 49.0, "val_loss": 173214.14794921875, "val_acc": 52.0}
{"epoch": 28, "training_loss": 347282.66357421875, "training_acc": 58.0, "val_loss": 112447.25341796875, "val_acc": 48.0}
{"epoch": 29, "training_loss": 357230.9052734375, "training_acc": 58.0, "val_loss": 282293.84765625, "val_acc": 52.0}
{"epoch": 30, "training_loss": 967150.4453125, "training_acc": 53.0, "val_loss": 131169.8974609375, "val_acc": 56.0}
{"epoch": 31, "training_loss": 629141.6171875, "training_acc": 52.0, "val_loss": 285680.3955078125, "val_acc": 48.0}
{"epoch": 32, "training_loss": 857504.3227539062, "training_acc": 51.0, "val_loss": 321356.982421875, "val_acc": 52.0}
{"epoch": 33, "training_loss": 1282721.5, "training_acc": 53.0, "val_loss": 293377.392578125, "val_acc": 52.0}
{"epoch": 34, "training_loss": 543305.0791015625, "training_acc": 66.0, "val_loss": 244626.708984375, "val_acc": 48.0}
{"epoch": 35, "training_loss": 898496.134765625, "training_acc": 47.0, "val_loss": 154415.34423828125, "val_acc": 52.0}
{"epoch": 36, "training_loss": 534168.0, "training_acc": 55.0, "val_loss": 86479.2236328125, "val_acc": 60.0}
{"epoch": 37, "training_loss": 389560.115234375, "training_acc": 49.0, "val_loss": 144572.94921875, "val_acc": 48.0}
{"epoch": 38, "training_loss": 526209.681640625, "training_acc": 48.0, "val_loss": 167266.6259765625, "val_acc": 52.0}
