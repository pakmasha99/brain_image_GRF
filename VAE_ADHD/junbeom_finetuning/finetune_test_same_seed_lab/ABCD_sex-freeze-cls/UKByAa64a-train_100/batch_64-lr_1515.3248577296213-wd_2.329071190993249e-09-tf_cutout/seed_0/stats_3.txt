"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 453748.36374664307, "training_acc": 53.0, "val_loss": 1277063.76953125, "val_acc": 48.0}
{"epoch": 1, "training_loss": 5155598.484375, "training_acc": 47.0, "val_loss": 100156.29272460938, "val_acc": 44.0}
{"epoch": 2, "training_loss": 870337.7734375, "training_acc": 68.0, "val_loss": 1247868.5546875, "val_acc": 52.0}
{"epoch": 3, "training_loss": 3886183.8046875, "training_acc": 53.0, "val_loss": 341774.3408203125, "val_acc": 52.0}
{"epoch": 4, "training_loss": 1665221.6953125, "training_acc": 48.0, "val_loss": 961223.828125, "val_acc": 48.0}
{"epoch": 5, "training_loss": 3898827.921875, "training_acc": 47.0, "val_loss": 271771.6796875, "val_acc": 48.0}
{"epoch": 6, "training_loss": 1348260.80078125, "training_acc": 51.0, "val_loss": 919281.73828125, "val_acc": 52.0}
{"epoch": 7, "training_loss": 3191302.65625, "training_acc": 53.0, "val_loss": 713892.236328125, "val_acc": 52.0}
{"epoch": 8, "training_loss": 1753899.642578125, "training_acc": 54.0, "val_loss": 451201.611328125, "val_acc": 48.0}
{"epoch": 9, "training_loss": 2321002.7109375, "training_acc": 47.0, "val_loss": 647315.966796875, "val_acc": 48.0}
{"epoch": 10, "training_loss": 2278041.66015625, "training_acc": 47.0, "val_loss": 307319.23828125, "val_acc": 52.0}
{"epoch": 11, "training_loss": 1113152.234375, "training_acc": 55.0, "val_loss": 621868.84765625, "val_acc": 52.0}
{"epoch": 12, "training_loss": 1739270.640625, "training_acc": 53.0, "val_loss": 78878.80249023438, "val_acc": 48.0}
{"epoch": 13, "training_loss": 548534.0390625, "training_acc": 59.0, "val_loss": 193951.5869140625, "val_acc": 52.0}
{"epoch": 14, "training_loss": 882397.947265625, "training_acc": 58.0, "val_loss": 316252.9296875, "val_acc": 52.0}
{"epoch": 15, "training_loss": 732911.08203125, "training_acc": 57.0, "val_loss": 103139.6728515625, "val_acc": 52.0}
{"epoch": 16, "training_loss": 413248.041015625, "training_acc": 57.0, "val_loss": 112007.080078125, "val_acc": 48.0}
{"epoch": 17, "training_loss": 498008.267578125, "training_acc": 53.0, "val_loss": 229979.541015625, "val_acc": 52.0}
{"epoch": 18, "training_loss": 372493.80078125, "training_acc": 58.0, "val_loss": 122415.10009765625, "val_acc": 52.0}
{"epoch": 19, "training_loss": 587610.224609375, "training_acc": 51.0, "val_loss": 201507.7880859375, "val_acc": 52.0}
{"epoch": 20, "training_loss": 536423.166015625, "training_acc": 54.0, "val_loss": 74572.021484375, "val_acc": 48.0}
{"epoch": 21, "training_loss": 393270.431640625, "training_acc": 53.0, "val_loss": 57013.92822265625, "val_acc": 48.0}
{"epoch": 22, "training_loss": 303560.228515625, "training_acc": 63.0, "val_loss": 76922.1923828125, "val_acc": 44.0}
{"epoch": 23, "training_loss": 384787.900390625, "training_acc": 53.0, "val_loss": 45638.20495605469, "val_acc": 64.0}
{"epoch": 24, "training_loss": 427720.6484375, "training_acc": 56.0, "val_loss": 137699.57275390625, "val_acc": 52.0}
{"epoch": 25, "training_loss": 413100.904296875, "training_acc": 53.0, "val_loss": 90273.44970703125, "val_acc": 48.0}
{"epoch": 26, "training_loss": 399219.658203125, "training_acc": 54.0, "val_loss": 207310.9130859375, "val_acc": 52.0}
{"epoch": 27, "training_loss": 375738.17919921875, "training_acc": 56.0, "val_loss": 76741.41845703125, "val_acc": 52.0}
{"epoch": 28, "training_loss": 334660.0341796875, "training_acc": 51.0, "val_loss": 134105.908203125, "val_acc": 52.0}
{"epoch": 29, "training_loss": 416567.517578125, "training_acc": 42.0, "val_loss": 79958.23974609375, "val_acc": 40.0}
{"epoch": 30, "training_loss": 157135.7353515625, "training_acc": 70.0, "val_loss": 118941.12548828125, "val_acc": 48.0}
{"epoch": 31, "training_loss": 187698.68603515625, "training_acc": 67.0, "val_loss": 52582.45849609375, "val_acc": 40.0}
{"epoch": 32, "training_loss": 295168.869140625, "training_acc": 54.0, "val_loss": 45254.522705078125, "val_acc": 64.0}
{"epoch": 33, "training_loss": 142317.70532226562, "training_acc": 57.0, "val_loss": 96812.95776367188, "val_acc": 52.0}
{"epoch": 34, "training_loss": 258875.79833984375, "training_acc": 60.0, "val_loss": 74089.05029296875, "val_acc": 52.0}
{"epoch": 35, "training_loss": 153117.84790039062, "training_acc": 70.0, "val_loss": 26508.120727539062, "val_acc": 60.0}
{"epoch": 36, "training_loss": 124844.66943359375, "training_acc": 65.0, "val_loss": 33797.84240722656, "val_acc": 48.0}
{"epoch": 37, "training_loss": 223872.12890625, "training_acc": 64.0, "val_loss": 80790.99731445312, "val_acc": 52.0}
{"epoch": 38, "training_loss": 195820.7080078125, "training_acc": 59.0, "val_loss": 108196.9970703125, "val_acc": 48.0}
{"epoch": 39, "training_loss": 435712.91015625, "training_acc": 46.0, "val_loss": 207370.361328125, "val_acc": 52.0}
{"epoch": 40, "training_loss": 444656.80908203125, "training_acc": 56.0, "val_loss": 143323.6572265625, "val_acc": 48.0}
{"epoch": 41, "training_loss": 521685.26953125, "training_acc": 48.0, "val_loss": 292568.4326171875, "val_acc": 52.0}
{"epoch": 42, "training_loss": 980348.89453125, "training_acc": 53.0, "val_loss": 145411.4990234375, "val_acc": 52.0}
{"epoch": 43, "training_loss": 692007.203125, "training_acc": 51.0, "val_loss": 428352.44140625, "val_acc": 48.0}
{"epoch": 44, "training_loss": 1504587.2734375, "training_acc": 47.0, "val_loss": 265709.1552734375, "val_acc": 52.0}
{"epoch": 45, "training_loss": 814524.48828125, "training_acc": 53.0, "val_loss": 367848.5595703125, "val_acc": 52.0}
{"epoch": 46, "training_loss": 654018.6650390625, "training_acc": 60.0, "val_loss": 156267.68798828125, "val_acc": 48.0}
{"epoch": 47, "training_loss": 687342.37890625, "training_acc": 46.0, "val_loss": 318080.615234375, "val_acc": 52.0}
{"epoch": 48, "training_loss": 790077.55078125, "training_acc": 54.0, "val_loss": 196967.431640625, "val_acc": 52.0}
{"epoch": 49, "training_loss": 611918.56640625, "training_acc": 49.0, "val_loss": 171560.3515625, "val_acc": 48.0}
{"epoch": 50, "training_loss": 591644.37890625, "training_acc": 52.0, "val_loss": 330920.6787109375, "val_acc": 52.0}
{"epoch": 51, "training_loss": 648462.291015625, "training_acc": 53.0, "val_loss": 98604.86450195312, "val_acc": 52.0}
{"epoch": 52, "training_loss": 750967.3125, "training_acc": 49.0, "val_loss": 127771.77734375, "val_acc": 52.0}
{"epoch": 53, "training_loss": 464692.7109375, "training_acc": 66.0, "val_loss": 275713.18359375, "val_acc": 52.0}
{"epoch": 54, "training_loss": 551021.1591796875, "training_acc": 50.0, "val_loss": 42370.172119140625, "val_acc": 48.0}
