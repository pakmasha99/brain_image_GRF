"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 557507.6874008179, "training_acc": 49.0, "val_loss": 155223.8037109375, "val_acc": 52.0}
{"epoch": 1, "training_loss": 605820.470703125, "training_acc": 53.0, "val_loss": 237487.0849609375, "val_acc": 48.0}
{"epoch": 2, "training_loss": 850021.212890625, "training_acc": 47.0, "val_loss": 11013.345336914062, "val_acc": 52.0}
{"epoch": 3, "training_loss": 104891.427734375, "training_acc": 53.0, "val_loss": 16144.609069824219, "val_acc": 52.0}
{"epoch": 4, "training_loss": 188006.853515625, "training_acc": 47.0, "val_loss": 103388.3544921875, "val_acc": 48.0}
{"epoch": 5, "training_loss": 335220.72314453125, "training_acc": 47.0, "val_loss": 68549.32861328125, "val_acc": 52.0}
{"epoch": 6, "training_loss": 311465.71484375, "training_acc": 53.0, "val_loss": 96643.701171875, "val_acc": 52.0}
{"epoch": 7, "training_loss": 280091.509765625, "training_acc": 53.0, "val_loss": 68950.64086914062, "val_acc": 48.0}
{"epoch": 8, "training_loss": 336755.009765625, "training_acc": 47.0, "val_loss": 85888.09204101562, "val_acc": 48.0}
{"epoch": 9, "training_loss": 243699.47705078125, "training_acc": 47.0, "val_loss": 91271.05102539062, "val_acc": 52.0}
{"epoch": 10, "training_loss": 424597.91015625, "training_acc": 53.0, "val_loss": 152038.1103515625, "val_acc": 52.0}
{"epoch": 11, "training_loss": 518650.478515625, "training_acc": 53.0, "val_loss": 40013.00354003906, "val_acc": 52.0}
{"epoch": 12, "training_loss": 204330.72265625, "training_acc": 49.0, "val_loss": 125911.63330078125, "val_acc": 48.0}
{"epoch": 13, "training_loss": 518464.32421875, "training_acc": 47.0, "val_loss": 75038.34838867188, "val_acc": 48.0}
{"epoch": 14, "training_loss": 221363.6640625, "training_acc": 46.0, "val_loss": 50354.327392578125, "val_acc": 52.0}
{"epoch": 15, "training_loss": 166164.02978515625, "training_acc": 53.0, "val_loss": 4243.951034545898, "val_acc": 52.0}
{"epoch": 16, "training_loss": 40045.876708984375, "training_acc": 58.0, "val_loss": 12785.087585449219, "val_acc": 48.0}
{"epoch": 17, "training_loss": 85355.44140625, "training_acc": 53.0, "val_loss": 26289.901733398438, "val_acc": 52.0}
{"epoch": 18, "training_loss": 77434.65600585938, "training_acc": 51.0, "val_loss": 8687.34130859375, "val_acc": 48.0}
{"epoch": 19, "training_loss": 60234.3662109375, "training_acc": 57.0, "val_loss": 38722.81799316406, "val_acc": 52.0}
{"epoch": 20, "training_loss": 86128.62976074219, "training_acc": 53.0, "val_loss": 29766.104125976562, "val_acc": 48.0}
{"epoch": 21, "training_loss": 115150.75244140625, "training_acc": 47.0, "val_loss": 28720.37353515625, "val_acc": 52.0}
{"epoch": 22, "training_loss": 80302.00927734375, "training_acc": 53.0, "val_loss": 4869.0887451171875, "val_acc": 56.0}
{"epoch": 23, "training_loss": 54610.1513671875, "training_acc": 56.0, "val_loss": 28444.198608398438, "val_acc": 52.0}
{"epoch": 24, "training_loss": 83444.509765625, "training_acc": 58.0, "val_loss": 5715.002822875977, "val_acc": 64.0}
{"epoch": 25, "training_loss": 56376.754150390625, "training_acc": 54.0, "val_loss": 32618.203735351562, "val_acc": 52.0}
{"epoch": 26, "training_loss": 80290.33276367188, "training_acc": 58.0, "val_loss": 5189.133834838867, "val_acc": 64.0}
{"epoch": 27, "training_loss": 61089.151611328125, "training_acc": 51.0, "val_loss": 42243.53942871094, "val_acc": 52.0}
{"epoch": 28, "training_loss": 143339.20947265625, "training_acc": 53.0, "val_loss": 21986.02752685547, "val_acc": 52.0}
{"epoch": 29, "training_loss": 115159.2138671875, "training_acc": 43.0, "val_loss": 46195.29724121094, "val_acc": 48.0}
{"epoch": 30, "training_loss": 124714.18518066406, "training_acc": 53.0, "val_loss": 62797.39990234375, "val_acc": 52.0}
{"epoch": 31, "training_loss": 240457.7001953125, "training_acc": 53.0, "val_loss": 69600.48217773438, "val_acc": 52.0}
{"epoch": 32, "training_loss": 167887.22216796875, "training_acc": 55.0, "val_loss": 66384.36279296875, "val_acc": 48.0}
{"epoch": 33, "training_loss": 307492.41015625, "training_acc": 47.0, "val_loss": 80355.13916015625, "val_acc": 48.0}
{"epoch": 34, "training_loss": 214633.70727539062, "training_acc": 51.0, "val_loss": 73109.27124023438, "val_acc": 52.0}
