"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 590779.7104377747, "training_acc": 44.0, "val_loss": 138191.76025390625, "val_acc": 52.0}
{"epoch": 1, "training_loss": 478370.271484375, "training_acc": 61.0, "val_loss": 303323.92578125, "val_acc": 48.0}
{"epoch": 2, "training_loss": 1171356.3046875, "training_acc": 47.0, "val_loss": 86992.90161132812, "val_acc": 48.0}
{"epoch": 3, "training_loss": 407435.33984375, "training_acc": 49.0, "val_loss": 230443.115234375, "val_acc": 52.0}
{"epoch": 4, "training_loss": 825587.48828125, "training_acc": 53.0, "val_loss": 196423.30322265625, "val_acc": 52.0}
{"epoch": 5, "training_loss": 502688.296875, "training_acc": 53.0, "val_loss": 69212.53662109375, "val_acc": 48.0}
{"epoch": 6, "training_loss": 474665.947265625, "training_acc": 47.0, "val_loss": 134131.33544921875, "val_acc": 48.0}
{"epoch": 7, "training_loss": 504512.505859375, "training_acc": 47.0, "val_loss": 56799.224853515625, "val_acc": 52.0}
{"epoch": 8, "training_loss": 238638.779296875, "training_acc": 49.0, "val_loss": 134680.13916015625, "val_acc": 52.0}
{"epoch": 9, "training_loss": 332220.0986328125, "training_acc": 53.0, "val_loss": 25641.6748046875, "val_acc": 44.0}
{"epoch": 10, "training_loss": 145094.0048828125, "training_acc": 59.0, "val_loss": 94165.37475585938, "val_acc": 48.0}
{"epoch": 11, "training_loss": 394674.8330078125, "training_acc": 47.0, "val_loss": 22187.054443359375, "val_acc": 44.0}
{"epoch": 12, "training_loss": 131904.634765625, "training_acc": 51.0, "val_loss": 90194.67163085938, "val_acc": 52.0}
{"epoch": 13, "training_loss": 224811.96630859375, "training_acc": 53.0, "val_loss": 23367.156982421875, "val_acc": 48.0}
{"epoch": 14, "training_loss": 163310.2626953125, "training_acc": 47.0, "val_loss": 8985.24169921875, "val_acc": 60.0}
{"epoch": 15, "training_loss": 120005.5419921875, "training_acc": 49.0, "val_loss": 89631.31103515625, "val_acc": 52.0}
{"epoch": 16, "training_loss": 281032.7880859375, "training_acc": 53.0, "val_loss": 8642.068481445312, "val_acc": 44.0}
{"epoch": 17, "training_loss": 107692.443359375, "training_acc": 58.0, "val_loss": 75197.4853515625, "val_acc": 48.0}
{"epoch": 18, "training_loss": 262472.8015136719, "training_acc": 49.0, "val_loss": 29147.634887695312, "val_acc": 52.0}
{"epoch": 19, "training_loss": 110261.4228515625, "training_acc": 54.0, "val_loss": 22032.997131347656, "val_acc": 52.0}
{"epoch": 20, "training_loss": 124398.12451171875, "training_acc": 50.0, "val_loss": 43332.733154296875, "val_acc": 48.0}
{"epoch": 21, "training_loss": 153489.99725341797, "training_acc": 51.0, "val_loss": 40377.61535644531, "val_acc": 52.0}
{"epoch": 22, "training_loss": 86793.138671875, "training_acc": 55.0, "val_loss": 11560.97640991211, "val_acc": 60.0}
{"epoch": 23, "training_loss": 78973.595703125, "training_acc": 55.0, "val_loss": 19887.310791015625, "val_acc": 52.0}
{"epoch": 24, "training_loss": 59158.707275390625, "training_acc": 56.0, "val_loss": 33298.13232421875, "val_acc": 52.0}
{"epoch": 25, "training_loss": 99076.6953125, "training_acc": 50.0, "val_loss": 14965.52734375, "val_acc": 44.0}
{"epoch": 26, "training_loss": 111964.70947265625, "training_acc": 43.0, "val_loss": 33924.5849609375, "val_acc": 52.0}
{"epoch": 27, "training_loss": 67634.37060546875, "training_acc": 48.0, "val_loss": 14209.980773925781, "val_acc": 40.0}
{"epoch": 28, "training_loss": 50411.253662109375, "training_acc": 53.0, "val_loss": 12120.41244506836, "val_acc": 40.0}
{"epoch": 29, "training_loss": 36250.91613769531, "training_acc": 65.0, "val_loss": 15307.354736328125, "val_acc": 52.0}
{"epoch": 30, "training_loss": 34786.26232910156, "training_acc": 62.0, "val_loss": 5820.30143737793, "val_acc": 48.0}
{"epoch": 31, "training_loss": 30507.248962402344, "training_acc": 54.0, "val_loss": 4545.967483520508, "val_acc": 56.0}
{"epoch": 32, "training_loss": 23620.419799804688, "training_acc": 61.0, "val_loss": 5012.200546264648, "val_acc": 36.0}
{"epoch": 33, "training_loss": 22665.2255859375, "training_acc": 71.0, "val_loss": 17442.69561767578, "val_acc": 52.0}
{"epoch": 34, "training_loss": 23486.875061035156, "training_acc": 65.0, "val_loss": 6203.833770751953, "val_acc": 36.0}
{"epoch": 35, "training_loss": 13835.193786621094, "training_acc": 66.0, "val_loss": 9088.396453857422, "val_acc": 48.0}
{"epoch": 36, "training_loss": 11930.104553222656, "training_acc": 64.0, "val_loss": 21417.034912109375, "val_acc": 52.0}
{"epoch": 37, "training_loss": 38327.28729248047, "training_acc": 60.0, "val_loss": 39027.18811035156, "val_acc": 48.0}
{"epoch": 38, "training_loss": 139775.82861328125, "training_acc": 47.0, "val_loss": 40052.03857421875, "val_acc": 52.0}
{"epoch": 39, "training_loss": 155139.25390625, "training_acc": 53.0, "val_loss": 23996.109008789062, "val_acc": 52.0}
{"epoch": 40, "training_loss": 118398.13525390625, "training_acc": 53.0, "val_loss": 72046.8017578125, "val_acc": 48.0}
{"epoch": 41, "training_loss": 231266.0166015625, "training_acc": 47.0, "val_loss": 48976.91650390625, "val_acc": 52.0}
{"epoch": 42, "training_loss": 206620.6162109375, "training_acc": 53.0, "val_loss": 60818.46923828125, "val_acc": 52.0}
{"epoch": 43, "training_loss": 139536.34045410156, "training_acc": 53.0, "val_loss": 15021.824645996094, "val_acc": 48.0}
{"epoch": 44, "training_loss": 87128.375, "training_acc": 41.0, "val_loss": 16343.247985839844, "val_acc": 52.0}
{"epoch": 45, "training_loss": 77855.23291015625, "training_acc": 54.0, "val_loss": 42868.51501464844, "val_acc": 48.0}
{"epoch": 46, "training_loss": 122407.06396484375, "training_acc": 50.0, "val_loss": 17080.1025390625, "val_acc": 52.0}
{"epoch": 47, "training_loss": 61107.60693359375, "training_acc": 49.0, "val_loss": 6010.066223144531, "val_acc": 52.0}
{"epoch": 48, "training_loss": 71167.83837890625, "training_acc": 51.0, "val_loss": 41539.6240234375, "val_acc": 52.0}
{"epoch": 49, "training_loss": 118381.85375976562, "training_acc": 45.0, "val_loss": 2702.2079467773438, "val_acc": 56.0}
{"epoch": 50, "training_loss": 19730.540283203125, "training_acc": 68.0, "val_loss": 5517.32063293457, "val_acc": 56.0}
{"epoch": 51, "training_loss": 37644.72119140625, "training_acc": 51.0, "val_loss": 4879.651260375977, "val_acc": 48.0}
{"epoch": 52, "training_loss": 17582.706665039062, "training_acc": 66.0, "val_loss": 6007.23991394043, "val_acc": 52.0}
{"epoch": 53, "training_loss": 11293.2333984375, "training_acc": 77.0, "val_loss": 14168.115234375, "val_acc": 52.0}
{"epoch": 54, "training_loss": 31786.777587890625, "training_acc": 58.0, "val_loss": 5225.308609008789, "val_acc": 56.0}
{"epoch": 55, "training_loss": 12489.585998535156, "training_acc": 74.0, "val_loss": 21822.691345214844, "val_acc": 48.0}
{"epoch": 56, "training_loss": 66380.45310974121, "training_acc": 54.0, "val_loss": 14135.502624511719, "val_acc": 52.0}
{"epoch": 57, "training_loss": 47816.330322265625, "training_acc": 51.0, "val_loss": 3478.7342071533203, "val_acc": 56.0}
{"epoch": 58, "training_loss": 12847.084716796875, "training_acc": 68.0, "val_loss": 29291.720581054688, "val_acc": 48.0}
{"epoch": 59, "training_loss": 93927.25048828125, "training_acc": 50.0, "val_loss": 44363.580322265625, "val_acc": 52.0}
{"epoch": 60, "training_loss": 136363.06396484375, "training_acc": 53.0, "val_loss": 7120.5810546875, "val_acc": 44.0}
{"epoch": 61, "training_loss": 38249.71484375, "training_acc": 61.0, "val_loss": 5965.493011474609, "val_acc": 44.0}
{"epoch": 62, "training_loss": 46096.0283203125, "training_acc": 67.0, "val_loss": 45249.5849609375, "val_acc": 52.0}
{"epoch": 63, "training_loss": 73127.09216308594, "training_acc": 64.0, "val_loss": 23231.382751464844, "val_acc": 48.0}
{"epoch": 64, "training_loss": 70104.47003173828, "training_acc": 53.0, "val_loss": 23598.941040039062, "val_acc": 52.0}
{"epoch": 65, "training_loss": 36456.56140136719, "training_acc": 56.0, "val_loss": 8674.749755859375, "val_acc": 44.0}
{"epoch": 66, "training_loss": 10808.03091430664, "training_acc": 73.0, "val_loss": 2705.360794067383, "val_acc": 52.0}
{"epoch": 67, "training_loss": 7553.024841308594, "training_acc": 80.0, "val_loss": 3170.938491821289, "val_acc": 60.0}
{"epoch": 68, "training_loss": 15230.914672851562, "training_acc": 67.0, "val_loss": 23176.304626464844, "val_acc": 52.0}
