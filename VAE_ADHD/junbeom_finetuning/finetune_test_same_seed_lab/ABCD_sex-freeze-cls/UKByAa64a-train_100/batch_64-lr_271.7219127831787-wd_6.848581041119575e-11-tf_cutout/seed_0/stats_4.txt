"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 595252.8920898438, "training_acc": 47.0, "val_loss": 165039.70947265625, "val_acc": 52.0}
{"epoch": 1, "training_loss": 707016.244140625, "training_acc": 45.0, "val_loss": 199787.8662109375, "val_acc": 48.0}
{"epoch": 2, "training_loss": 694676.01953125, "training_acc": 47.0, "val_loss": 41098.91357421875, "val_acc": 52.0}
{"epoch": 3, "training_loss": 220484.5869140625, "training_acc": 53.0, "val_loss": 62390.2587890625, "val_acc": 52.0}
{"epoch": 4, "training_loss": 149254.10974121094, "training_acc": 59.0, "val_loss": 31684.671020507812, "val_acc": 48.0}
{"epoch": 5, "training_loss": 112862.14965820312, "training_acc": 49.0, "val_loss": 14664.671325683594, "val_acc": 52.0}
{"epoch": 6, "training_loss": 117076.6064453125, "training_acc": 48.0, "val_loss": 34116.26892089844, "val_acc": 48.0}
{"epoch": 7, "training_loss": 117218.072265625, "training_acc": 53.0, "val_loss": 46236.47155761719, "val_acc": 52.0}
{"epoch": 8, "training_loss": 125210.5087890625, "training_acc": 54.0, "val_loss": 58487.847900390625, "val_acc": 48.0}
{"epoch": 9, "training_loss": 251274.802734375, "training_acc": 47.0, "val_loss": 35092.56896972656, "val_acc": 48.0}
{"epoch": 10, "training_loss": 162625.6162109375, "training_acc": 47.0, "val_loss": 79166.77856445312, "val_acc": 52.0}
{"epoch": 11, "training_loss": 270026.6962890625, "training_acc": 53.0, "val_loss": 9788.934326171875, "val_acc": 56.0}
{"epoch": 12, "training_loss": 102448.3603515625, "training_acc": 51.0, "val_loss": 78157.60498046875, "val_acc": 48.0}
{"epoch": 13, "training_loss": 244615.05322265625, "training_acc": 48.0, "val_loss": 38049.725341796875, "val_acc": 52.0}
{"epoch": 14, "training_loss": 183059.0986328125, "training_acc": 53.0, "val_loss": 49109.124755859375, "val_acc": 52.0}
{"epoch": 15, "training_loss": 132788.48657226562, "training_acc": 51.0, "val_loss": 35411.31896972656, "val_acc": 48.0}
{"epoch": 16, "training_loss": 115569.77124023438, "training_acc": 50.0, "val_loss": 34274.462890625, "val_acc": 52.0}
{"epoch": 17, "training_loss": 148629.92822265625, "training_acc": 53.0, "val_loss": 13098.794555664062, "val_acc": 56.0}
{"epoch": 18, "training_loss": 64229.28857421875, "training_acc": 52.0, "val_loss": 33447.78137207031, "val_acc": 48.0}
{"epoch": 19, "training_loss": 90498.3173828125, "training_acc": 53.0, "val_loss": 23025.436401367188, "val_acc": 52.0}
{"epoch": 20, "training_loss": 52201.00231933594, "training_acc": 63.0, "val_loss": 24808.616638183594, "val_acc": 44.0}
{"epoch": 21, "training_loss": 70287.04382324219, "training_acc": 58.0, "val_loss": 26309.915161132812, "val_acc": 52.0}
{"epoch": 22, "training_loss": 79940.75756835938, "training_acc": 55.0, "val_loss": 22386.599731445312, "val_acc": 48.0}
{"epoch": 23, "training_loss": 85737.25, "training_acc": 50.0, "val_loss": 17311.672973632812, "val_acc": 52.0}
{"epoch": 24, "training_loss": 57456.686279296875, "training_acc": 57.0, "val_loss": 19933.792114257812, "val_acc": 48.0}
{"epoch": 25, "training_loss": 64294.906005859375, "training_acc": 51.0, "val_loss": 31659.365844726562, "val_acc": 52.0}
{"epoch": 26, "training_loss": 103577.75512695312, "training_acc": 55.0, "val_loss": 5280.979537963867, "val_acc": 52.0}
{"epoch": 27, "training_loss": 46778.4794921875, "training_acc": 59.0, "val_loss": 8595.226287841797, "val_acc": 56.0}
{"epoch": 28, "training_loss": 37622.064697265625, "training_acc": 59.0, "val_loss": 24139.09454345703, "val_acc": 48.0}
{"epoch": 29, "training_loss": 73909.07513427734, "training_acc": 51.0, "val_loss": 32762.539672851562, "val_acc": 52.0}
{"epoch": 30, "training_loss": 106150.45629882812, "training_acc": 53.0, "val_loss": 17307.717895507812, "val_acc": 48.0}
{"epoch": 31, "training_loss": 68852.03540039062, "training_acc": 49.0, "val_loss": 33076.324462890625, "val_acc": 52.0}
{"epoch": 32, "training_loss": 119257.169921875, "training_acc": 53.0, "val_loss": 3670.008087158203, "val_acc": 64.0}
{"epoch": 33, "training_loss": 84946.09521484375, "training_acc": 55.0, "val_loss": 32191.790771484375, "val_acc": 48.0}
{"epoch": 34, "training_loss": 93261.39074707031, "training_acc": 55.0, "val_loss": 34347.381591796875, "val_acc": 52.0}
{"epoch": 35, "training_loss": 84747.7626953125, "training_acc": 50.0, "val_loss": 21463.870239257812, "val_acc": 48.0}
{"epoch": 36, "training_loss": 71434.46520996094, "training_acc": 49.0, "val_loss": 26219.93408203125, "val_acc": 52.0}
{"epoch": 37, "training_loss": 57971.51557922363, "training_acc": 60.0, "val_loss": 11792.754364013672, "val_acc": 48.0}
{"epoch": 38, "training_loss": 55147.4921875, "training_acc": 49.0, "val_loss": 7950.053405761719, "val_acc": 52.0}
{"epoch": 39, "training_loss": 17337.925048828125, "training_acc": 70.0, "val_loss": 3404.1770935058594, "val_acc": 56.0}
{"epoch": 40, "training_loss": 37332.9013671875, "training_acc": 70.0, "val_loss": 17310.952758789062, "val_acc": 52.0}
{"epoch": 41, "training_loss": 81371.08056640625, "training_acc": 50.0, "val_loss": 27616.278076171875, "val_acc": 48.0}
{"epoch": 42, "training_loss": 137639.3232421875, "training_acc": 37.0, "val_loss": 30547.671508789062, "val_acc": 52.0}
{"epoch": 43, "training_loss": 59762.9345703125, "training_acc": 64.0, "val_loss": 22772.076416015625, "val_acc": 48.0}
{"epoch": 44, "training_loss": 59319.7109375, "training_acc": 61.0, "val_loss": 26716.250610351562, "val_acc": 52.0}
{"epoch": 45, "training_loss": 67521.76287841797, "training_acc": 58.0, "val_loss": 11444.924926757812, "val_acc": 48.0}
{"epoch": 46, "training_loss": 58155.299072265625, "training_acc": 45.0, "val_loss": 3781.340789794922, "val_acc": 64.0}
{"epoch": 47, "training_loss": 50689.0107421875, "training_acc": 59.0, "val_loss": 9258.335876464844, "val_acc": 48.0}
{"epoch": 48, "training_loss": 71351.09033203125, "training_acc": 57.0, "val_loss": 47864.12353515625, "val_acc": 52.0}
{"epoch": 49, "training_loss": 128599.89245605469, "training_acc": 57.0, "val_loss": 57628.363037109375, "val_acc": 48.0}
{"epoch": 50, "training_loss": 226124.2421875, "training_acc": 47.0, "val_loss": 27980.117797851562, "val_acc": 48.0}
{"epoch": 51, "training_loss": 125453.4404296875, "training_acc": 52.0, "val_loss": 81076.6845703125, "val_acc": 52.0}
{"epoch": 52, "training_loss": 293789.9033203125, "training_acc": 53.0, "val_loss": 12453.245544433594, "val_acc": 52.0}
{"epoch": 53, "training_loss": 153268.6435546875, "training_acc": 53.0, "val_loss": 116013.330078125, "val_acc": 48.0}
{"epoch": 54, "training_loss": 417626.224609375, "training_acc": 47.0, "val_loss": 25138.685607910156, "val_acc": 48.0}
{"epoch": 55, "training_loss": 188937.826171875, "training_acc": 43.0, "val_loss": 130811.5966796875, "val_acc": 52.0}
{"epoch": 56, "training_loss": 493312.626953125, "training_acc": 53.0, "val_loss": 92937.841796875, "val_acc": 52.0}
{"epoch": 57, "training_loss": 243535.94061279297, "training_acc": 56.0, "val_loss": 93557.57446289062, "val_acc": 48.0}
{"epoch": 58, "training_loss": 450816.783203125, "training_acc": 47.0, "val_loss": 135327.7587890625, "val_acc": 48.0}
