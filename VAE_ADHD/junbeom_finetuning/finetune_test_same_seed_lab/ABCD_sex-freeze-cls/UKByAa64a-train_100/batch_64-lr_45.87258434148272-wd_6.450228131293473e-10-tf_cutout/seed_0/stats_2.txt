"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 126700.10411071777, "training_acc": 43.0, "val_loss": 23667.543029785156, "val_acc": 48.0}
{"epoch": 1, "training_loss": 114825.81103515625, "training_acc": 45.0, "val_loss": 49915.35949707031, "val_acc": 52.0}
{"epoch": 2, "training_loss": 190752.1162109375, "training_acc": 53.0, "val_loss": 31141.476440429688, "val_acc": 52.0}
{"epoch": 3, "training_loss": 90437.57611083984, "training_acc": 52.0, "val_loss": 30389.181518554688, "val_acc": 48.0}
{"epoch": 4, "training_loss": 136777.16015625, "training_acc": 47.0, "val_loss": 46681.50634765625, "val_acc": 48.0}
{"epoch": 5, "training_loss": 156885.68994140625, "training_acc": 47.0, "val_loss": 16633.615112304688, "val_acc": 48.0}
{"epoch": 6, "training_loss": 46500.91955566406, "training_acc": 50.0, "val_loss": 23253.1982421875, "val_acc": 52.0}
{"epoch": 7, "training_loss": 98519.337890625, "training_acc": 53.0, "val_loss": 21523.62518310547, "val_acc": 52.0}
{"epoch": 8, "training_loss": 70052.22131347656, "training_acc": 54.0, "val_loss": 11552.721405029297, "val_acc": 48.0}
{"epoch": 9, "training_loss": 48679.432373046875, "training_acc": 50.0, "val_loss": 22946.47979736328, "val_acc": 48.0}
{"epoch": 10, "training_loss": 69031.6181640625, "training_acc": 48.0, "val_loss": 3833.9515686035156, "val_acc": 48.0}
{"epoch": 11, "training_loss": 33189.67370605469, "training_acc": 51.0, "val_loss": 17145.225524902344, "val_acc": 52.0}
{"epoch": 12, "training_loss": 66085.294921875, "training_acc": 53.0, "val_loss": 3912.1177673339844, "val_acc": 48.0}
{"epoch": 13, "training_loss": 24556.158325195312, "training_acc": 55.0, "val_loss": 19772.84698486328, "val_acc": 48.0}
{"epoch": 14, "training_loss": 64498.57080078125, "training_acc": 48.0, "val_loss": 7781.073760986328, "val_acc": 48.0}
{"epoch": 15, "training_loss": 31548.056640625, "training_acc": 45.0, "val_loss": 10764.151763916016, "val_acc": 52.0}
{"epoch": 16, "training_loss": 38053.39709472656, "training_acc": 53.0, "val_loss": 4006.2767028808594, "val_acc": 44.0}
{"epoch": 17, "training_loss": 20244.33740234375, "training_acc": 52.0, "val_loss": 5947.421646118164, "val_acc": 48.0}
{"epoch": 18, "training_loss": 16390.508209228516, "training_acc": 55.0, "val_loss": 7656.761169433594, "val_acc": 52.0}
{"epoch": 19, "training_loss": 23264.27374267578, "training_acc": 53.0, "val_loss": 7257.625579833984, "val_acc": 48.0}
{"epoch": 20, "training_loss": 28048.566528320312, "training_acc": 47.0, "val_loss": 2656.270980834961, "val_acc": 44.0}
{"epoch": 21, "training_loss": 13614.907287597656, "training_acc": 58.0, "val_loss": 12549.773406982422, "val_acc": 52.0}
{"epoch": 22, "training_loss": 42050.637939453125, "training_acc": 53.0, "val_loss": 853.8152694702148, "val_acc": 52.0}
{"epoch": 23, "training_loss": 10635.065795898438, "training_acc": 70.0, "val_loss": 9933.472442626953, "val_acc": 48.0}
{"epoch": 24, "training_loss": 28657.200408935547, "training_acc": 46.0, "val_loss": 10468.289947509766, "val_acc": 52.0}
{"epoch": 25, "training_loss": 44748.179443359375, "training_acc": 53.0, "val_loss": 12404.874420166016, "val_acc": 52.0}
{"epoch": 26, "training_loss": 35630.986877441406, "training_acc": 53.0, "val_loss": 10266.0400390625, "val_acc": 48.0}
{"epoch": 27, "training_loss": 42641.621337890625, "training_acc": 47.0, "val_loss": 6882.313537597656, "val_acc": 48.0}
{"epoch": 28, "training_loss": 18817.349517822266, "training_acc": 55.0, "val_loss": 10992.874908447266, "val_acc": 52.0}
{"epoch": 29, "training_loss": 35939.8076171875, "training_acc": 53.0, "val_loss": 1310.6240272521973, "val_acc": 52.0}
{"epoch": 30, "training_loss": 17507.981811523438, "training_acc": 49.0, "val_loss": 7536.273193359375, "val_acc": 48.0}
{"epoch": 31, "training_loss": 23503.423828125, "training_acc": 51.0, "val_loss": 5379.920959472656, "val_acc": 52.0}
{"epoch": 32, "training_loss": 15006.547149658203, "training_acc": 60.0, "val_loss": 5595.945739746094, "val_acc": 48.0}
{"epoch": 33, "training_loss": 15973.591613769531, "training_acc": 54.0, "val_loss": 1596.267032623291, "val_acc": 48.0}
{"epoch": 34, "training_loss": 11443.949890136719, "training_acc": 59.0, "val_loss": 2083.4203720092773, "val_acc": 56.0}
{"epoch": 35, "training_loss": 10427.96435546875, "training_acc": 60.0, "val_loss": 2129.9373626708984, "val_acc": 44.0}
{"epoch": 36, "training_loss": 14297.007629394531, "training_acc": 51.0, "val_loss": 7206.679534912109, "val_acc": 52.0}
{"epoch": 37, "training_loss": 17623.2275390625, "training_acc": 56.0, "val_loss": 2191.560935974121, "val_acc": 52.0}
{"epoch": 38, "training_loss": 8273.60285949707, "training_acc": 56.0, "val_loss": 2812.637710571289, "val_acc": 52.0}
{"epoch": 39, "training_loss": 6239.103958129883, "training_acc": 64.0, "val_loss": 1438.3434295654297, "val_acc": 56.0}
{"epoch": 40, "training_loss": 9136.100494384766, "training_acc": 53.0, "val_loss": 2108.6551666259766, "val_acc": 60.0}
{"epoch": 41, "training_loss": 11834.865356445312, "training_acc": 56.0, "val_loss": 5498.291015625, "val_acc": 48.0}
