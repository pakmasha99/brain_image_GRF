"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 2433.329345703125, "training_acc": 45.0, "val_loss": 579.4899940490723, "val_acc": 52.0}
{"epoch": 1, "training_loss": 2161.1384658813477, "training_acc": 55.0, "val_loss": 1005.0933837890625, "val_acc": 48.0}
{"epoch": 2, "training_loss": 3745.919189453125, "training_acc": 47.0, "val_loss": 143.89245510101318, "val_acc": 48.0}
{"epoch": 3, "training_loss": 1369.5538177490234, "training_acc": 47.0, "val_loss": 1030.495834350586, "val_acc": 52.0}
{"epoch": 4, "training_loss": 3763.2171173095703, "training_acc": 53.0, "val_loss": 886.264705657959, "val_acc": 52.0}
{"epoch": 5, "training_loss": 2681.746696472168, "training_acc": 53.0, "val_loss": 74.63393807411194, "val_acc": 44.0}
{"epoch": 6, "training_loss": 812.6147232055664, "training_acc": 47.0, "val_loss": 328.5818815231323, "val_acc": 48.0}
{"epoch": 7, "training_loss": 1084.213770866394, "training_acc": 47.0, "val_loss": 348.50006103515625, "val_acc": 52.0}
{"epoch": 8, "training_loss": 1122.994773864746, "training_acc": 55.0, "val_loss": 503.45325469970703, "val_acc": 52.0}
{"epoch": 9, "training_loss": 1211.791696548462, "training_acc": 56.0, "val_loss": 74.9755322933197, "val_acc": 36.0}
{"epoch": 10, "training_loss": 848.3338928222656, "training_acc": 46.0, "val_loss": 216.30675792694092, "val_acc": 48.0}
{"epoch": 11, "training_loss": 774.008544921875, "training_acc": 53.0, "val_loss": 314.7982120513916, "val_acc": 52.0}
{"epoch": 12, "training_loss": 857.9375495910645, "training_acc": 54.0, "val_loss": 368.7805414199829, "val_acc": 52.0}
{"epoch": 13, "training_loss": 739.9617977142334, "training_acc": 55.0, "val_loss": 124.23423528671265, "val_acc": 48.0}
{"epoch": 14, "training_loss": 760.2488212585449, "training_acc": 46.0, "val_loss": 50.40801167488098, "val_acc": 32.0}
{"epoch": 15, "training_loss": 279.7090034484863, "training_acc": 63.0, "val_loss": 280.95388412475586, "val_acc": 52.0}
{"epoch": 16, "training_loss": 687.6160898208618, "training_acc": 53.0, "val_loss": 103.77844572067261, "val_acc": 48.0}
{"epoch": 17, "training_loss": 571.6040420532227, "training_acc": 47.0, "val_loss": 54.25000190734863, "val_acc": 52.0}
{"epoch": 18, "training_loss": 262.96861457824707, "training_acc": 57.0, "val_loss": 56.22031092643738, "val_acc": 52.0}
{"epoch": 19, "training_loss": 302.7433376312256, "training_acc": 51.0, "val_loss": 89.05852437019348, "val_acc": 48.0}
{"epoch": 20, "training_loss": 380.7576541900635, "training_acc": 52.0, "val_loss": 156.30706548690796, "val_acc": 52.0}
{"epoch": 21, "training_loss": 389.48820543289185, "training_acc": 54.0, "val_loss": 40.33752381801605, "val_acc": 52.0}
{"epoch": 22, "training_loss": 292.6466426849365, "training_acc": 45.0, "val_loss": 42.6195502281189, "val_acc": 52.0}
{"epoch": 23, "training_loss": 283.70948028564453, "training_acc": 52.0, "val_loss": 46.49253785610199, "val_acc": 48.0}
{"epoch": 24, "training_loss": 348.66977310180664, "training_acc": 50.0, "val_loss": 229.5266628265381, "val_acc": 52.0}
{"epoch": 25, "training_loss": 550.9402465820312, "training_acc": 53.0, "val_loss": 179.3919324874878, "val_acc": 48.0}
{"epoch": 26, "training_loss": 896.7520866394043, "training_acc": 47.0, "val_loss": 151.6731858253479, "val_acc": 48.0}
{"epoch": 27, "training_loss": 614.7395668029785, "training_acc": 43.0, "val_loss": 235.2123737335205, "val_acc": 52.0}
{"epoch": 28, "training_loss": 559.5158157348633, "training_acc": 53.0, "val_loss": 115.68692922592163, "val_acc": 48.0}
{"epoch": 29, "training_loss": 588.9410800933838, "training_acc": 47.0, "val_loss": 43.17342936992645, "val_acc": 32.0}
{"epoch": 30, "training_loss": 313.7877769470215, "training_acc": 58.0, "val_loss": 270.52531242370605, "val_acc": 52.0}
{"epoch": 31, "training_loss": 564.6422777175903, "training_acc": 54.0, "val_loss": 108.20223093032837, "val_acc": 48.0}
{"epoch": 32, "training_loss": 586.6585807800293, "training_acc": 47.0, "val_loss": 54.26079034805298, "val_acc": 44.0}
{"epoch": 33, "training_loss": 307.8807792663574, "training_acc": 59.0, "val_loss": 186.0585331916809, "val_acc": 52.0}
{"epoch": 34, "training_loss": 314.56663179397583, "training_acc": 55.0, "val_loss": 76.54827237129211, "val_acc": 48.0}
{"epoch": 35, "training_loss": 281.07816433906555, "training_acc": 52.0, "val_loss": 112.32304573059082, "val_acc": 52.0}
{"epoch": 36, "training_loss": 153.4775266647339, "training_acc": 64.0, "val_loss": 63.72936964035034, "val_acc": 48.0}
{"epoch": 37, "training_loss": 234.59736824035645, "training_acc": 50.0, "val_loss": 114.46045637130737, "val_acc": 52.0}
{"epoch": 38, "training_loss": 234.19398069381714, "training_acc": 55.0, "val_loss": 40.95157980918884, "val_acc": 52.0}
{"epoch": 39, "training_loss": 121.51438212394714, "training_acc": 60.0, "val_loss": 114.72575664520264, "val_acc": 52.0}
{"epoch": 40, "training_loss": 178.1478157043457, "training_acc": 64.0, "val_loss": 71.27450108528137, "val_acc": 48.0}
