"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 71.7451651096344, "training_acc": 44.0, "val_loss": 17.51284748315811, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.78524494171143, "training_acc": 47.0, "val_loss": 17.571356892585754, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.62989926338196, "training_acc": 53.0, "val_loss": 17.84524917602539, "val_acc": 52.0}
{"epoch": 3, "training_loss": 70.26127171516418, "training_acc": 53.0, "val_loss": 17.695464193820953, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.79440379142761, "training_acc": 53.0, "val_loss": 17.48208999633789, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.12679290771484, "training_acc": 53.0, "val_loss": 17.384250462055206, "val_acc": 52.0}
{"epoch": 6, "training_loss": 68.98774671554565, "training_acc": 53.0, "val_loss": 17.4045592546463, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.13253498077393, "training_acc": 57.0, "val_loss": 17.42599755525589, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.50497364997864, "training_acc": 47.0, "val_loss": 17.419955134391785, "val_acc": 52.0}
{"epoch": 9, "training_loss": 68.9324562549591, "training_acc": 64.0, "val_loss": 17.41722673177719, "val_acc": 52.0}
{"epoch": 10, "training_loss": 68.77651023864746, "training_acc": 53.0, "val_loss": 17.570747435092926, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.28857469558716, "training_acc": 53.0, "val_loss": 17.73359626531601, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.44921088218689, "training_acc": 53.0, "val_loss": 17.709262669086456, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.61549687385559, "training_acc": 53.0, "val_loss": 17.542918026447296, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.18278861045837, "training_acc": 53.0, "val_loss": 17.417827248573303, "val_acc": 52.0}
{"epoch": 15, "training_loss": 68.84777092933655, "training_acc": 53.0, "val_loss": 17.39410310983658, "val_acc": 52.0}
{"epoch": 16, "training_loss": 68.71845865249634, "training_acc": 56.0, "val_loss": 17.392826080322266, "val_acc": 52.0}
{"epoch": 17, "training_loss": 68.80518698692322, "training_acc": 55.0, "val_loss": 17.387087643146515, "val_acc": 52.0}
{"epoch": 18, "training_loss": 68.46631932258606, "training_acc": 60.0, "val_loss": 17.381474375724792, "val_acc": 52.0}
{"epoch": 19, "training_loss": 68.50978469848633, "training_acc": 53.0, "val_loss": 17.39986538887024, "val_acc": 52.0}
{"epoch": 20, "training_loss": 68.91164231300354, "training_acc": 53.0, "val_loss": 17.470788955688477, "val_acc": 52.0}
{"epoch": 21, "training_loss": 68.39206290245056, "training_acc": 53.0, "val_loss": 17.42059886455536, "val_acc": 52.0}
{"epoch": 22, "training_loss": 68.47449469566345, "training_acc": 53.0, "val_loss": 17.38516390323639, "val_acc": 52.0}
{"epoch": 23, "training_loss": 68.40188479423523, "training_acc": 53.0, "val_loss": 17.338809370994568, "val_acc": 52.0}
{"epoch": 24, "training_loss": 68.72528958320618, "training_acc": 57.0, "val_loss": 17.317041754722595, "val_acc": 52.0}
{"epoch": 25, "training_loss": 68.10443806648254, "training_acc": 75.0, "val_loss": 17.325030267238617, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.08871364593506, "training_acc": 53.0, "val_loss": 17.32819676399231, "val_acc": 52.0}
{"epoch": 27, "training_loss": 68.58971333503723, "training_acc": 55.0, "val_loss": 17.312876880168915, "val_acc": 52.0}
{"epoch": 28, "training_loss": 68.65120339393616, "training_acc": 57.0, "val_loss": 17.31407940387726, "val_acc": 52.0}
{"epoch": 29, "training_loss": 68.7382595539093, "training_acc": 56.0, "val_loss": 17.356444895267487, "val_acc": 52.0}
{"epoch": 30, "training_loss": 67.84571170806885, "training_acc": 55.0, "val_loss": 17.428794503211975, "val_acc": 52.0}
{"epoch": 31, "training_loss": 68.60565161705017, "training_acc": 53.0, "val_loss": 17.486076056957245, "val_acc": 52.0}
{"epoch": 32, "training_loss": 68.0653121471405, "training_acc": 53.0, "val_loss": 17.447637021541595, "val_acc": 52.0}
{"epoch": 33, "training_loss": 68.11787915229797, "training_acc": 53.0, "val_loss": 17.409412562847137, "val_acc": 52.0}
{"epoch": 34, "training_loss": 67.9724690914154, "training_acc": 58.0, "val_loss": 17.40858405828476, "val_acc": 52.0}
{"epoch": 35, "training_loss": 68.33896946907043, "training_acc": 58.0, "val_loss": 17.41631031036377, "val_acc": 52.0}
{"epoch": 36, "training_loss": 68.4533314704895, "training_acc": 57.0, "val_loss": 17.43650585412979, "val_acc": 52.0}
{"epoch": 37, "training_loss": 67.72269558906555, "training_acc": 60.0, "val_loss": 17.52869337797165, "val_acc": 52.0}
{"epoch": 38, "training_loss": 67.81885266304016, "training_acc": 52.0, "val_loss": 17.721417546272278, "val_acc": 52.0}
{"epoch": 39, "training_loss": 68.35742974281311, "training_acc": 53.0, "val_loss": 17.778147757053375, "val_acc": 52.0}
{"epoch": 40, "training_loss": 67.71409201622009, "training_acc": 53.0, "val_loss": 17.613036930561066, "val_acc": 52.0}
{"epoch": 41, "training_loss": 68.0617139339447, "training_acc": 59.0, "val_loss": 17.520295083522797, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.10379362106323, "training_acc": 55.0, "val_loss": 17.521314322948456, "val_acc": 52.0}
{"epoch": 43, "training_loss": 67.06731486320496, "training_acc": 70.0, "val_loss": 17.550136148929596, "val_acc": 52.0}
{"epoch": 44, "training_loss": 68.2928991317749, "training_acc": 59.0, "val_loss": 17.60779470205307, "val_acc": 52.0}
{"epoch": 45, "training_loss": 66.73287606239319, "training_acc": 55.0, "val_loss": 17.72506684064865, "val_acc": 52.0}
{"epoch": 46, "training_loss": 68.14762926101685, "training_acc": 52.0, "val_loss": 17.758245766162872, "val_acc": 52.0}
