"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 254.2306079864502, "training_acc": 46.0, "val_loss": 44.34541165828705, "val_acc": 52.0}
{"epoch": 1, "training_loss": 214.2868013381958, "training_acc": 53.0, "val_loss": 110.5074405670166, "val_acc": 48.0}
{"epoch": 2, "training_loss": 410.86838722229004, "training_acc": 47.0, "val_loss": 27.48485803604126, "val_acc": 48.0}
{"epoch": 3, "training_loss": 144.19919061660767, "training_acc": 51.0, "val_loss": 86.31672263145447, "val_acc": 52.0}
{"epoch": 4, "training_loss": 326.38310527801514, "training_acc": 53.0, "val_loss": 72.56124019622803, "val_acc": 52.0}
{"epoch": 5, "training_loss": 214.02029657363892, "training_acc": 53.0, "val_loss": 23.584847152233124, "val_acc": 48.0}
{"epoch": 6, "training_loss": 161.85033226013184, "training_acc": 47.0, "val_loss": 56.577068567276, "val_acc": 48.0}
{"epoch": 7, "training_loss": 205.4876103401184, "training_acc": 47.0, "val_loss": 17.35328435897827, "val_acc": 52.0}
{"epoch": 8, "training_loss": 77.26520133018494, "training_acc": 59.0, "val_loss": 51.38055086135864, "val_acc": 52.0}
{"epoch": 9, "training_loss": 186.55169820785522, "training_acc": 53.0, "val_loss": 32.09128379821777, "val_acc": 52.0}
{"epoch": 10, "training_loss": 103.99538731575012, "training_acc": 56.0, "val_loss": 29.65829372406006, "val_acc": 48.0}
{"epoch": 11, "training_loss": 123.46084833145142, "training_acc": 47.0, "val_loss": 24.885186553001404, "val_acc": 48.0}
{"epoch": 12, "training_loss": 87.65837073326111, "training_acc": 49.0, "val_loss": 25.965696573257446, "val_acc": 52.0}
{"epoch": 13, "training_loss": 107.88556814193726, "training_acc": 53.0, "val_loss": 23.26662540435791, "val_acc": 52.0}
{"epoch": 14, "training_loss": 84.76895475387573, "training_acc": 48.0, "val_loss": 23.88746440410614, "val_acc": 48.0}
{"epoch": 15, "training_loss": 95.5823450088501, "training_acc": 47.0, "val_loss": 17.519794404506683, "val_acc": 60.0}
{"epoch": 16, "training_loss": 70.95029377937317, "training_acc": 56.0, "val_loss": 25.828132033348083, "val_acc": 52.0}
{"epoch": 17, "training_loss": 89.37244725227356, "training_acc": 53.0, "val_loss": 17.33900010585785, "val_acc": 52.0}
{"epoch": 18, "training_loss": 82.56634664535522, "training_acc": 46.0, "val_loss": 19.69773769378662, "val_acc": 44.0}
{"epoch": 19, "training_loss": 76.68363857269287, "training_acc": 44.0, "val_loss": 20.743072032928467, "val_acc": 52.0}
{"epoch": 20, "training_loss": 75.37051486968994, "training_acc": 53.0, "val_loss": 18.023504316806793, "val_acc": 52.0}
{"epoch": 21, "training_loss": 65.62934994697571, "training_acc": 55.0, "val_loss": 19.977882504463196, "val_acc": 40.0}
{"epoch": 22, "training_loss": 76.00442504882812, "training_acc": 49.0, "val_loss": 18.2021826505661, "val_acc": 52.0}
{"epoch": 23, "training_loss": 70.7060935497284, "training_acc": 61.0, "val_loss": 20.705993473529816, "val_acc": 52.0}
{"epoch": 24, "training_loss": 71.40036797523499, "training_acc": 55.0, "val_loss": 18.76187026500702, "val_acc": 44.0}
{"epoch": 25, "training_loss": 72.04580783843994, "training_acc": 52.0, "val_loss": 17.54959374666214, "val_acc": 60.0}
{"epoch": 26, "training_loss": 64.63918352127075, "training_acc": 63.0, "val_loss": 20.629265904426575, "val_acc": 52.0}
{"epoch": 27, "training_loss": 70.24651455879211, "training_acc": 57.0, "val_loss": 17.503145337104797, "val_acc": 56.0}
{"epoch": 28, "training_loss": 66.77192711830139, "training_acc": 61.0, "val_loss": 18.488436937332153, "val_acc": 48.0}
{"epoch": 29, "training_loss": 68.34142327308655, "training_acc": 58.0, "val_loss": 20.355749130249023, "val_acc": 52.0}
{"epoch": 30, "training_loss": 77.17942261695862, "training_acc": 53.0, "val_loss": 19.568149745464325, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.61665225028992, "training_acc": 54.0, "val_loss": 18.471908569335938, "val_acc": 52.0}
{"epoch": 32, "training_loss": 70.70735311508179, "training_acc": 53.0, "val_loss": 17.929160594940186, "val_acc": 52.0}
{"epoch": 33, "training_loss": 68.1031723022461, "training_acc": 59.0, "val_loss": 17.61224865913391, "val_acc": 56.0}
{"epoch": 34, "training_loss": 65.66502857208252, "training_acc": 57.0, "val_loss": 18.38737279176712, "val_acc": 48.0}
{"epoch": 35, "training_loss": 74.43580675125122, "training_acc": 50.0, "val_loss": 17.111065983772278, "val_acc": 56.0}
{"epoch": 36, "training_loss": 66.92104840278625, "training_acc": 60.0, "val_loss": 19.640707969665527, "val_acc": 52.0}
{"epoch": 37, "training_loss": 67.74618864059448, "training_acc": 59.0, "val_loss": 17.127034068107605, "val_acc": 64.0}
{"epoch": 38, "training_loss": 67.72796702384949, "training_acc": 61.0, "val_loss": 16.946113109588623, "val_acc": 56.0}
{"epoch": 39, "training_loss": 64.86231350898743, "training_acc": 58.0, "val_loss": 18.3257058262825, "val_acc": 52.0}
{"epoch": 40, "training_loss": 64.47979354858398, "training_acc": 63.0, "val_loss": 17.009322345256805, "val_acc": 60.0}
{"epoch": 41, "training_loss": 64.6193699836731, "training_acc": 70.0, "val_loss": 16.995929181575775, "val_acc": 52.0}
{"epoch": 42, "training_loss": 62.977678537368774, "training_acc": 63.0, "val_loss": 17.63470023870468, "val_acc": 52.0}
{"epoch": 43, "training_loss": 64.40848183631897, "training_acc": 59.0, "val_loss": 17.019516229629517, "val_acc": 64.0}
{"epoch": 44, "training_loss": 63.458476305007935, "training_acc": 65.0, "val_loss": 17.207814753055573, "val_acc": 64.0}
{"epoch": 45, "training_loss": 62.36892819404602, "training_acc": 69.0, "val_loss": 19.336625933647156, "val_acc": 52.0}
{"epoch": 46, "training_loss": 71.27412366867065, "training_acc": 53.0, "val_loss": 16.615666449069977, "val_acc": 56.0}
{"epoch": 47, "training_loss": 68.99419283866882, "training_acc": 61.0, "val_loss": 17.54094511270523, "val_acc": 56.0}
{"epoch": 48, "training_loss": 63.417813539505005, "training_acc": 60.0, "val_loss": 21.574123203754425, "val_acc": 52.0}
{"epoch": 49, "training_loss": 73.04070711135864, "training_acc": 53.0, "val_loss": 16.575397551059723, "val_acc": 56.0}
{"epoch": 50, "training_loss": 70.02724003791809, "training_acc": 62.0, "val_loss": 16.924968361854553, "val_acc": 64.0}
{"epoch": 51, "training_loss": 63.88639950752258, "training_acc": 63.0, "val_loss": 19.410361349582672, "val_acc": 52.0}
{"epoch": 52, "training_loss": 67.55984473228455, "training_acc": 60.0, "val_loss": 16.79048240184784, "val_acc": 64.0}
{"epoch": 53, "training_loss": 66.536137342453, "training_acc": 58.0, "val_loss": 16.730213165283203, "val_acc": 56.0}
{"epoch": 54, "training_loss": 65.16547155380249, "training_acc": 60.0, "val_loss": 18.39800477027893, "val_acc": 52.0}
{"epoch": 55, "training_loss": 62.88468074798584, "training_acc": 61.0, "val_loss": 17.033500969409943, "val_acc": 68.0}
{"epoch": 56, "training_loss": 64.01891255378723, "training_acc": 61.0, "val_loss": 18.515951931476593, "val_acc": 52.0}
{"epoch": 57, "training_loss": 67.3764762878418, "training_acc": 54.0, "val_loss": 17.27507710456848, "val_acc": 56.0}
{"epoch": 58, "training_loss": 69.20682311058044, "training_acc": 53.0, "val_loss": 17.368943989276886, "val_acc": 64.0}
{"epoch": 59, "training_loss": 64.27918648719788, "training_acc": 62.0, "val_loss": 19.005592167377472, "val_acc": 52.0}
{"epoch": 60, "training_loss": 67.39465880393982, "training_acc": 54.0, "val_loss": 18.976546823978424, "val_acc": 40.0}
{"epoch": 61, "training_loss": 75.92498135566711, "training_acc": 49.0, "val_loss": 17.025573551654816, "val_acc": 60.0}
{"epoch": 62, "training_loss": 69.63242697715759, "training_acc": 61.0, "val_loss": 21.004992723464966, "val_acc": 52.0}
{"epoch": 63, "training_loss": 73.06710696220398, "training_acc": 51.0, "val_loss": 17.530567944049835, "val_acc": 60.0}
{"epoch": 64, "training_loss": 65.33228635787964, "training_acc": 64.0, "val_loss": 19.336768984794617, "val_acc": 52.0}
{"epoch": 65, "training_loss": 64.19634962081909, "training_acc": 60.0, "val_loss": 17.070811986923218, "val_acc": 56.0}
{"epoch": 66, "training_loss": 65.20365071296692, "training_acc": 63.0, "val_loss": 17.13445484638214, "val_acc": 56.0}
{"epoch": 67, "training_loss": 64.77826428413391, "training_acc": 64.0, "val_loss": 17.77527928352356, "val_acc": 56.0}
{"epoch": 68, "training_loss": 60.690964698791504, "training_acc": 69.0, "val_loss": 17.41562932729721, "val_acc": 68.0}
