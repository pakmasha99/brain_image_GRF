"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 4272.284717559814, "training_acc": 46.0, "val_loss": 883.5975646972656, "val_acc": 52.0}
{"epoch": 1, "training_loss": 4291.560348510742, "training_acc": 53.0, "val_loss": 2288.760757446289, "val_acc": 48.0}
{"epoch": 2, "training_loss": 8613.413330078125, "training_acc": 47.0, "val_loss": 634.5215797424316, "val_acc": 48.0}
{"epoch": 3, "training_loss": 3001.2494049072266, "training_acc": 51.0, "val_loss": 1671.2955474853516, "val_acc": 52.0}
{"epoch": 4, "training_loss": 6350.312042236328, "training_acc": 53.0, "val_loss": 1491.3331985473633, "val_acc": 52.0}
{"epoch": 5, "training_loss": 4426.573226928711, "training_acc": 53.0, "val_loss": 259.25121307373047, "val_acc": 48.0}
{"epoch": 6, "training_loss": 2388.5814666748047, "training_acc": 47.0, "val_loss": 764.0296936035156, "val_acc": 48.0}
{"epoch": 7, "training_loss": 2589.2646446228027, "training_acc": 47.0, "val_loss": 553.3999443054199, "val_acc": 52.0}
{"epoch": 8, "training_loss": 2158.300453186035, "training_acc": 53.0, "val_loss": 959.5537185668945, "val_acc": 52.0}
{"epoch": 9, "training_loss": 3116.1100845336914, "training_acc": 53.0, "val_loss": 121.08213901519775, "val_acc": 52.0}
{"epoch": 10, "training_loss": 1264.5027389526367, "training_acc": 49.0, "val_loss": 908.2693099975586, "val_acc": 48.0}
{"epoch": 11, "training_loss": 3492.0405197143555, "training_acc": 47.0, "val_loss": 218.35260391235352, "val_acc": 48.0}
{"epoch": 12, "training_loss": 1268.0305557250977, "training_acc": 49.0, "val_loss": 902.8525352478027, "val_acc": 52.0}
{"epoch": 13, "training_loss": 3457.87890625, "training_acc": 53.0, "val_loss": 660.5371475219727, "val_acc": 52.0}
{"epoch": 14, "training_loss": 1734.4975881576538, "training_acc": 53.0, "val_loss": 635.6348514556885, "val_acc": 48.0}
{"epoch": 15, "training_loss": 3008.1170196533203, "training_acc": 47.0, "val_loss": 787.4898910522461, "val_acc": 48.0}
{"epoch": 16, "training_loss": 2430.236301422119, "training_acc": 47.0, "val_loss": 447.50399589538574, "val_acc": 52.0}
{"epoch": 17, "training_loss": 2074.7806854248047, "training_acc": 53.0, "val_loss": 843.8569068908691, "val_acc": 52.0}
{"epoch": 18, "training_loss": 2825.7556076049805, "training_acc": 53.0, "val_loss": 71.71263098716736, "val_acc": 60.0}
{"epoch": 19, "training_loss": 774.6963806152344, "training_acc": 60.0, "val_loss": 728.0258178710938, "val_acc": 48.0}
{"epoch": 20, "training_loss": 2682.06990814209, "training_acc": 47.0, "val_loss": 57.938170433044434, "val_acc": 44.0}
{"epoch": 21, "training_loss": 946.240234375, "training_acc": 54.0, "val_loss": 834.1331481933594, "val_acc": 52.0}
{"epoch": 22, "training_loss": 2780.0370483398438, "training_acc": 53.0, "val_loss": 399.3139982223511, "val_acc": 52.0}
{"epoch": 23, "training_loss": 1031.1090335845947, "training_acc": 53.0, "val_loss": 455.8117389678955, "val_acc": 48.0}
{"epoch": 24, "training_loss": 1716.7290840148926, "training_acc": 47.0, "val_loss": 163.49321603775024, "val_acc": 56.0}
{"epoch": 25, "training_loss": 639.4744338989258, "training_acc": 60.0, "val_loss": 326.83794498443604, "val_acc": 52.0}
{"epoch": 26, "training_loss": 872.3531627655029, "training_acc": 49.0, "val_loss": 98.8552987575531, "val_acc": 36.0}
{"epoch": 27, "training_loss": 563.5187721252441, "training_acc": 50.0, "val_loss": 150.5983829498291, "val_acc": 56.0}
{"epoch": 28, "training_loss": 278.90629863739014, "training_acc": 60.0, "val_loss": 70.5836832523346, "val_acc": 44.0}
{"epoch": 29, "training_loss": 246.95464992523193, "training_acc": 64.0, "val_loss": 189.38720226287842, "val_acc": 56.0}
{"epoch": 30, "training_loss": 477.66537857055664, "training_acc": 58.0, "val_loss": 106.6401481628418, "val_acc": 44.0}
{"epoch": 31, "training_loss": 364.155065536499, "training_acc": 59.0, "val_loss": 199.64441061019897, "val_acc": 52.0}
{"epoch": 32, "training_loss": 397.07777404785156, "training_acc": 60.0, "val_loss": 229.02183532714844, "val_acc": 48.0}
{"epoch": 33, "training_loss": 750.7899112701416, "training_acc": 48.0, "val_loss": 222.1769094467163, "val_acc": 52.0}
{"epoch": 34, "training_loss": 755.0758361816406, "training_acc": 52.0, "val_loss": 87.41605877876282, "val_acc": 40.0}
{"epoch": 35, "training_loss": 527.2094478607178, "training_acc": 51.0, "val_loss": 96.57478332519531, "val_acc": 60.0}
{"epoch": 36, "training_loss": 284.0751647949219, "training_acc": 63.0, "val_loss": 45.32085359096527, "val_acc": 44.0}
{"epoch": 37, "training_loss": 253.57676887512207, "training_acc": 62.0, "val_loss": 171.53244018554688, "val_acc": 52.0}
{"epoch": 38, "training_loss": 351.97964572906494, "training_acc": 56.0, "val_loss": 186.49001121520996, "val_acc": 48.0}
{"epoch": 39, "training_loss": 511.55886793136597, "training_acc": 55.0, "val_loss": 260.53662300109863, "val_acc": 52.0}
{"epoch": 40, "training_loss": 853.2971591949463, "training_acc": 53.0, "val_loss": 116.44060611724854, "val_acc": 52.0}
{"epoch": 41, "training_loss": 320.9965934753418, "training_acc": 52.0, "val_loss": 189.91117477416992, "val_acc": 52.0}
{"epoch": 42, "training_loss": 590.9101314544678, "training_acc": 54.0, "val_loss": 225.2103567123413, "val_acc": 48.0}
{"epoch": 43, "training_loss": 702.032395362854, "training_acc": 48.0, "val_loss": 228.69679927825928, "val_acc": 52.0}
{"epoch": 44, "training_loss": 862.0664863586426, "training_acc": 53.0, "val_loss": 60.63299775123596, "val_acc": 48.0}
{"epoch": 45, "training_loss": 300.7317924499512, "training_acc": 53.0, "val_loss": 254.5016050338745, "val_acc": 52.0}
{"epoch": 46, "training_loss": 928.4514083862305, "training_acc": 53.0, "val_loss": 24.39323365688324, "val_acc": 56.0}
{"epoch": 47, "training_loss": 343.7982807159424, "training_acc": 57.0, "val_loss": 44.80429291725159, "val_acc": 64.0}
{"epoch": 48, "training_loss": 152.41574573516846, "training_acc": 70.0, "val_loss": 41.369301080703735, "val_acc": 48.0}
{"epoch": 49, "training_loss": 128.52697730064392, "training_acc": 66.0, "val_loss": 29.405778646469116, "val_acc": 68.0}
{"epoch": 50, "training_loss": 166.40037631988525, "training_acc": 61.0, "val_loss": 192.44335889816284, "val_acc": 52.0}
{"epoch": 51, "training_loss": 528.4364776611328, "training_acc": 54.0, "val_loss": 228.24571132659912, "val_acc": 48.0}
{"epoch": 52, "training_loss": 835.5615177154541, "training_acc": 48.0, "val_loss": 176.43446922302246, "val_acc": 52.0}
{"epoch": 53, "training_loss": 608.827070236206, "training_acc": 53.0, "val_loss": 91.46328568458557, "val_acc": 44.0}
{"epoch": 54, "training_loss": 273.27778482437134, "training_acc": 55.0, "val_loss": 182.23333358764648, "val_acc": 52.0}
{"epoch": 55, "training_loss": 473.5539321899414, "training_acc": 59.0, "val_loss": 203.75030040740967, "val_acc": 48.0}
{"epoch": 56, "training_loss": 657.7802085876465, "training_acc": 47.0, "val_loss": 348.2426643371582, "val_acc": 52.0}
{"epoch": 57, "training_loss": 1371.442771911621, "training_acc": 53.0, "val_loss": 267.83244609832764, "val_acc": 52.0}
{"epoch": 58, "training_loss": 1053.2896118164062, "training_acc": 45.0, "val_loss": 339.76714611053467, "val_acc": 48.0}
{"epoch": 59, "training_loss": 930.8854718208313, "training_acc": 49.0, "val_loss": 415.65494537353516, "val_acc": 52.0}
{"epoch": 60, "training_loss": 1678.567527770996, "training_acc": 53.0, "val_loss": 262.2906684875488, "val_acc": 52.0}
{"epoch": 61, "training_loss": 1046.1846961975098, "training_acc": 49.0, "val_loss": 474.7615337371826, "val_acc": 48.0}
{"epoch": 62, "training_loss": 1594.110652923584, "training_acc": 47.0, "val_loss": 310.5323791503906, "val_acc": 52.0}
{"epoch": 63, "training_loss": 1017.4180297851562, "training_acc": 53.0, "val_loss": 325.8816719055176, "val_acc": 52.0}
{"epoch": 64, "training_loss": 714.3556814193726, "training_acc": 57.0, "val_loss": 176.62922143936157, "val_acc": 48.0}
{"epoch": 65, "training_loss": 534.7618708610535, "training_acc": 54.0, "val_loss": 172.48409986495972, "val_acc": 52.0}
