"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 5115.085601806641, "training_acc": 48.0, "val_loss": 1103.1325340270996, "val_acc": 52.0}
{"epoch": 1, "training_loss": 3772.47216796875, "training_acc": 59.0, "val_loss": 2003.9495468139648, "val_acc": 48.0}
{"epoch": 2, "training_loss": 7601.6729736328125, "training_acc": 47.0, "val_loss": 544.3708419799805, "val_acc": 48.0}
{"epoch": 3, "training_loss": 2550.794631958008, "training_acc": 51.0, "val_loss": 1613.3176803588867, "val_acc": 52.0}
{"epoch": 4, "training_loss": 6563.466796875, "training_acc": 53.0, "val_loss": 1351.9767761230469, "val_acc": 52.0}
{"epoch": 5, "training_loss": 3923.4178466796875, "training_acc": 54.0, "val_loss": 586.7185115814209, "val_acc": 48.0}
{"epoch": 6, "training_loss": 3427.162582397461, "training_acc": 47.0, "val_loss": 1142.5415992736816, "val_acc": 48.0}
{"epoch": 7, "training_loss": 4115.012855529785, "training_acc": 47.0, "val_loss": 113.40715885162354, "val_acc": 52.0}
{"epoch": 8, "training_loss": 890.9790534973145, "training_acc": 55.0, "val_loss": 643.377161026001, "val_acc": 52.0}
{"epoch": 9, "training_loss": 2057.0757637023926, "training_acc": 53.0, "val_loss": 105.15018701553345, "val_acc": 48.0}
{"epoch": 10, "training_loss": 584.3412761688232, "training_acc": 48.0, "val_loss": 103.89136075973511, "val_acc": 52.0}
{"epoch": 11, "training_loss": 283.251914024353, "training_acc": 55.0, "val_loss": 236.64355278015137, "val_acc": 48.0}
{"epoch": 12, "training_loss": 754.4220266342163, "training_acc": 49.0, "val_loss": 239.32716846466064, "val_acc": 52.0}
{"epoch": 13, "training_loss": 939.0124359130859, "training_acc": 53.0, "val_loss": 77.6703953742981, "val_acc": 52.0}
{"epoch": 14, "training_loss": 336.146502494812, "training_acc": 55.0, "val_loss": 33.755484223365784, "val_acc": 56.0}
{"epoch": 15, "training_loss": 314.8437099456787, "training_acc": 51.0, "val_loss": 50.29296278953552, "val_acc": 48.0}
{"epoch": 16, "training_loss": 204.0034532546997, "training_acc": 53.0, "val_loss": 201.4688014984131, "val_acc": 52.0}
{"epoch": 17, "training_loss": 601.516056060791, "training_acc": 56.0, "val_loss": 216.39890670776367, "val_acc": 48.0}
{"epoch": 18, "training_loss": 878.2683601379395, "training_acc": 47.0, "val_loss": 189.22265768051147, "val_acc": 52.0}
{"epoch": 19, "training_loss": 796.5738716125488, "training_acc": 54.0, "val_loss": 44.0068393945694, "val_acc": 64.0}
{"epoch": 20, "training_loss": 519.3819847106934, "training_acc": 56.0, "val_loss": 94.98594403266907, "val_acc": 40.0}
{"epoch": 21, "training_loss": 599.6557426452637, "training_acc": 55.0, "val_loss": 361.7791414260864, "val_acc": 52.0}
{"epoch": 22, "training_loss": 1013.0173015594482, "training_acc": 53.0, "val_loss": 326.01470947265625, "val_acc": 48.0}
{"epoch": 23, "training_loss": 1175.0631942749023, "training_acc": 47.0, "val_loss": 37.12511360645294, "val_acc": 68.0}
{"epoch": 24, "training_loss": 617.1526718139648, "training_acc": 56.0, "val_loss": 281.30650520324707, "val_acc": 52.0}
{"epoch": 25, "training_loss": 834.027684211731, "training_acc": 46.0, "val_loss": 124.47994947433472, "val_acc": 48.0}
{"epoch": 26, "training_loss": 457.6299076080322, "training_acc": 49.0, "val_loss": 89.5168662071228, "val_acc": 52.0}
{"epoch": 27, "training_loss": 471.0159320831299, "training_acc": 58.0, "val_loss": 228.08055877685547, "val_acc": 48.0}
{"epoch": 28, "training_loss": 827.4622230529785, "training_acc": 40.0, "val_loss": 130.35001754760742, "val_acc": 52.0}
{"epoch": 29, "training_loss": 534.6986351013184, "training_acc": 49.0, "val_loss": 171.07762098312378, "val_acc": 48.0}
{"epoch": 30, "training_loss": 394.2396697998047, "training_acc": 61.0, "val_loss": 127.21649408340454, "val_acc": 52.0}
{"epoch": 31, "training_loss": 524.6475791931152, "training_acc": 49.0, "val_loss": 56.03981018066406, "val_acc": 48.0}
{"epoch": 32, "training_loss": 387.1566619873047, "training_acc": 54.0, "val_loss": 36.272501945495605, "val_acc": 60.0}
{"epoch": 33, "training_loss": 354.7454471588135, "training_acc": 61.0, "val_loss": 107.36767053604126, "val_acc": 48.0}
