"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 3660.376235961914, "training_acc": 51.0, "val_loss": 739.8852348327637, "val_acc": 52.0}
{"epoch": 1, "training_loss": 5093.05419921875, "training_acc": 45.0, "val_loss": 2272.992706298828, "val_acc": 48.0}
{"epoch": 2, "training_loss": 8346.719757080078, "training_acc": 47.0, "val_loss": 429.8971652984619, "val_acc": 48.0}
{"epoch": 3, "training_loss": 2470.967315673828, "training_acc": 51.0, "val_loss": 1818.7623977661133, "val_acc": 52.0}
{"epoch": 4, "training_loss": 7133.095642089844, "training_acc": 53.0, "val_loss": 1641.6934967041016, "val_acc": 52.0}
{"epoch": 5, "training_loss": 5383.429885864258, "training_acc": 53.0, "val_loss": 50.98661780357361, "val_acc": 52.0}
{"epoch": 6, "training_loss": 1357.3253326416016, "training_acc": 50.0, "val_loss": 878.6226272583008, "val_acc": 48.0}
{"epoch": 7, "training_loss": 2912.812271118164, "training_acc": 47.0, "val_loss": 214.78188037872314, "val_acc": 52.0}
{"epoch": 8, "training_loss": 1163.4684143066406, "training_acc": 53.0, "val_loss": 349.05285835266113, "val_acc": 52.0}
{"epoch": 9, "training_loss": 1059.3719501495361, "training_acc": 51.0, "val_loss": 193.98261308670044, "val_acc": 48.0}
{"epoch": 10, "training_loss": 591.5498943328857, "training_acc": 50.0, "val_loss": 65.46491980552673, "val_acc": 56.0}
{"epoch": 11, "training_loss": 349.9214973449707, "training_acc": 57.0, "val_loss": 37.906673550605774, "val_acc": 56.0}
{"epoch": 12, "training_loss": 269.21833896636963, "training_acc": 54.0, "val_loss": 62.108421325683594, "val_acc": 52.0}
{"epoch": 13, "training_loss": 251.3890619277954, "training_acc": 56.0, "val_loss": 127.55279541015625, "val_acc": 52.0}
{"epoch": 14, "training_loss": 289.47880458831787, "training_acc": 56.0, "val_loss": 36.0512912273407, "val_acc": 56.0}
{"epoch": 15, "training_loss": 448.1213626861572, "training_acc": 60.0, "val_loss": 164.31597471237183, "val_acc": 52.0}
{"epoch": 16, "training_loss": 580.0848731994629, "training_acc": 52.0, "val_loss": 113.17846775054932, "val_acc": 44.0}
{"epoch": 17, "training_loss": 768.4904594421387, "training_acc": 47.0, "val_loss": 325.35133361816406, "val_acc": 52.0}
{"epoch": 18, "training_loss": 825.8760666847229, "training_acc": 53.0, "val_loss": 238.46125602722168, "val_acc": 48.0}
{"epoch": 19, "training_loss": 961.1447105407715, "training_acc": 47.0, "val_loss": 321.5332508087158, "val_acc": 52.0}
{"epoch": 20, "training_loss": 1118.2620697021484, "training_acc": 53.0, "val_loss": 172.4212408065796, "val_acc": 52.0}
{"epoch": 21, "training_loss": 755.5596809387207, "training_acc": 52.0, "val_loss": 402.46548652648926, "val_acc": 48.0}
{"epoch": 22, "training_loss": 1399.0390014648438, "training_acc": 48.0, "val_loss": 463.1298065185547, "val_acc": 52.0}
{"epoch": 23, "training_loss": 1891.8709030151367, "training_acc": 53.0, "val_loss": 547.6377964019775, "val_acc": 52.0}
{"epoch": 24, "training_loss": 1277.8139085769653, "training_acc": 55.0, "val_loss": 506.50973320007324, "val_acc": 48.0}
{"epoch": 25, "training_loss": 2392.609977722168, "training_acc": 47.0, "val_loss": 566.1937236785889, "val_acc": 48.0}
{"epoch": 26, "training_loss": 1731.3299589157104, "training_acc": 49.0, "val_loss": 557.6300144195557, "val_acc": 52.0}
{"epoch": 27, "training_loss": 2334.9845962524414, "training_acc": 53.0, "val_loss": 755.1780700683594, "val_acc": 52.0}
{"epoch": 28, "training_loss": 2347.72700881958, "training_acc": 53.0, "val_loss": 252.1824836730957, "val_acc": 48.0}
{"epoch": 29, "training_loss": 1401.1971740722656, "training_acc": 49.0, "val_loss": 351.10020637512207, "val_acc": 48.0}
{"epoch": 30, "training_loss": 1217.2684230804443, "training_acc": 42.0, "val_loss": 278.58922481536865, "val_acc": 52.0}
{"epoch": 31, "training_loss": 812.2944030761719, "training_acc": 55.0, "val_loss": 212.0506763458252, "val_acc": 48.0}
{"epoch": 32, "training_loss": 794.1173248291016, "training_acc": 51.0, "val_loss": 217.2908067703247, "val_acc": 52.0}
{"epoch": 33, "training_loss": 912.498363494873, "training_acc": 55.0, "val_loss": 62.93169856071472, "val_acc": 56.0}
