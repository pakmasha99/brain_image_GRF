"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 571879.4457778931, "training_acc": 53.0, "val_loss": 1609571.97265625, "val_acc": 48.0}
{"epoch": 1, "training_loss": 6497969.5, "training_acc": 47.0, "val_loss": 126233.88671875, "val_acc": 44.0}
{"epoch": 2, "training_loss": 1098703.75, "training_acc": 68.0, "val_loss": 1576442.578125, "val_acc": 52.0}
{"epoch": 3, "training_loss": 4913192.515625, "training_acc": 53.0, "val_loss": 436765.234375, "val_acc": 52.0}
{"epoch": 4, "training_loss": 2101833.140625, "training_acc": 48.0, "val_loss": 1203449.51171875, "val_acc": 48.0}
{"epoch": 5, "training_loss": 4881193.78125, "training_acc": 47.0, "val_loss": 333352.1728515625, "val_acc": 48.0}
{"epoch": 6, "training_loss": 1688061.6328125, "training_acc": 51.0, "val_loss": 1168039.74609375, "val_acc": 52.0}
{"epoch": 7, "training_loss": 4058035.984375, "training_acc": 53.0, "val_loss": 909776.85546875, "val_acc": 52.0}
{"epoch": 8, "training_loss": 2245745.58984375, "training_acc": 54.0, "val_loss": 557520.068359375, "val_acc": 48.0}
{"epoch": 9, "training_loss": 2880773.203125, "training_acc": 47.0, "val_loss": 804317.626953125, "val_acc": 48.0}
{"epoch": 10, "training_loss": 2825066.25, "training_acc": 47.0, "val_loss": 398444.53125, "val_acc": 52.0}
{"epoch": 11, "training_loss": 1429503.625, "training_acc": 54.0, "val_loss": 776508.349609375, "val_acc": 52.0}
{"epoch": 12, "training_loss": 2152495.140625, "training_acc": 53.0, "val_loss": 107161.4990234375, "val_acc": 56.0}
{"epoch": 13, "training_loss": 753219.484375, "training_acc": 58.0, "val_loss": 256687.7197265625, "val_acc": 52.0}
{"epoch": 14, "training_loss": 1147660.451171875, "training_acc": 57.0, "val_loss": 410247.802734375, "val_acc": 52.0}
{"epoch": 15, "training_loss": 975127.39453125, "training_acc": 55.0, "val_loss": 137493.5546875, "val_acc": 48.0}
{"epoch": 16, "training_loss": 526967.46875, "training_acc": 57.0, "val_loss": 159062.353515625, "val_acc": 52.0}
{"epoch": 17, "training_loss": 672090.923828125, "training_acc": 55.0, "val_loss": 354759.1796875, "val_acc": 52.0}
{"epoch": 18, "training_loss": 676843.474609375, "training_acc": 56.0, "val_loss": 113956.0546875, "val_acc": 56.0}
{"epoch": 19, "training_loss": 639881.10546875, "training_acc": 54.0, "val_loss": 159173.779296875, "val_acc": 48.0}
{"epoch": 20, "training_loss": 577731.76953125, "training_acc": 51.0, "val_loss": 176578.84521484375, "val_acc": 48.0}
{"epoch": 21, "training_loss": 655542.62109375, "training_acc": 56.0, "val_loss": 144139.0625, "val_acc": 52.0}
{"epoch": 22, "training_loss": 645476.068359375, "training_acc": 55.0, "val_loss": 260905.859375, "val_acc": 52.0}
{"epoch": 23, "training_loss": 604370.2783203125, "training_acc": 50.0, "val_loss": 124060.63232421875, "val_acc": 48.0}
{"epoch": 24, "training_loss": 479291.22705078125, "training_acc": 59.0, "val_loss": 130127.0751953125, "val_acc": 52.0}
{"epoch": 25, "training_loss": 229301.31640625, "training_acc": 64.0, "val_loss": 85331.21948242188, "val_acc": 52.0}
{"epoch": 26, "training_loss": 312853.0734863281, "training_acc": 61.0, "val_loss": 195584.130859375, "val_acc": 52.0}
{"epoch": 27, "training_loss": 328079.6484375, "training_acc": 62.0, "val_loss": 83081.640625, "val_acc": 52.0}
{"epoch": 28, "training_loss": 371559.384765625, "training_acc": 51.0, "val_loss": 91269.85473632812, "val_acc": 52.0}
{"epoch": 29, "training_loss": 476169.6640625, "training_acc": 47.0, "val_loss": 76275.81787109375, "val_acc": 44.0}
{"epoch": 30, "training_loss": 190867.541015625, "training_acc": 71.0, "val_loss": 88732.76977539062, "val_acc": 48.0}
{"epoch": 31, "training_loss": 228764.7900390625, "training_acc": 64.0, "val_loss": 95618.97583007812, "val_acc": 48.0}
{"epoch": 32, "training_loss": 303232.33203125, "training_acc": 61.0, "val_loss": 179760.498046875, "val_acc": 48.0}
{"epoch": 33, "training_loss": 634127.041015625, "training_acc": 47.0, "val_loss": 315466.1376953125, "val_acc": 52.0}
{"epoch": 34, "training_loss": 967536.35546875, "training_acc": 53.0, "val_loss": 61272.125244140625, "val_acc": 52.0}
{"epoch": 35, "training_loss": 447927.06640625, "training_acc": 61.0, "val_loss": 106695.64208984375, "val_acc": 48.0}
{"epoch": 36, "training_loss": 668508.73828125, "training_acc": 50.0, "val_loss": 466919.287109375, "val_acc": 52.0}
{"epoch": 37, "training_loss": 1275192.466796875, "training_acc": 53.0, "val_loss": 299464.0869140625, "val_acc": 48.0}
{"epoch": 38, "training_loss": 1309371.0234375, "training_acc": 47.0, "val_loss": 101487.2314453125, "val_acc": 48.0}
{"epoch": 39, "training_loss": 1102546.6640625, "training_acc": 41.0, "val_loss": 594977.734375, "val_acc": 52.0}
{"epoch": 40, "training_loss": 1918389.5703125, "training_acc": 53.0, "val_loss": 79570.25756835938, "val_acc": 48.0}
{"epoch": 41, "training_loss": 661594.69921875, "training_acc": 48.0, "val_loss": 121873.01025390625, "val_acc": 48.0}
{"epoch": 42, "training_loss": 802168.90234375, "training_acc": 48.0, "val_loss": 459418.65234375, "val_acc": 52.0}
{"epoch": 43, "training_loss": 1227544.251953125, "training_acc": 53.0, "val_loss": 280638.623046875, "val_acc": 48.0}
{"epoch": 44, "training_loss": 1395758.3515625, "training_acc": 47.0, "val_loss": 115871.875, "val_acc": 52.0}
{"epoch": 45, "training_loss": 749524.26953125, "training_acc": 53.0, "val_loss": 687372.0703125, "val_acc": 52.0}
{"epoch": 46, "training_loss": 1955408.65625, "training_acc": 53.0, "val_loss": 208848.046875, "val_acc": 52.0}
{"epoch": 47, "training_loss": 868786.8671875, "training_acc": 56.0, "val_loss": 504246.533203125, "val_acc": 48.0}
{"epoch": 48, "training_loss": 1918894.41015625, "training_acc": 47.0, "val_loss": 314536.2548828125, "val_acc": 52.0}
{"epoch": 49, "training_loss": 770603.52734375, "training_acc": 56.0, "val_loss": 493129.443359375, "val_acc": 52.0}
{"epoch": 50, "training_loss": 847448.3837890625, "training_acc": 59.0, "val_loss": 210643.5791015625, "val_acc": 48.0}
{"epoch": 51, "training_loss": 1059625.64453125, "training_acc": 47.0, "val_loss": 196709.65576171875, "val_acc": 52.0}
{"epoch": 52, "training_loss": 519869.791015625, "training_acc": 60.0, "val_loss": 342748.3642578125, "val_acc": 52.0}
{"epoch": 53, "training_loss": 474609.6767578125, "training_acc": 54.0, "val_loss": 100685.04638671875, "val_acc": 56.0}
