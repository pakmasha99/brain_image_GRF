"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 4011255.8387947083, "training_acc": 46.0, "val_loss": 838445.5078125, "val_acc": 52.0}
{"epoch": 1, "training_loss": 4071373.65625, "training_acc": 53.0, "val_loss": 2171366.40625, "val_acc": 48.0}
{"epoch": 2, "training_loss": 8171959.90625, "training_acc": 47.0, "val_loss": 601898.73046875, "val_acc": 48.0}
{"epoch": 3, "training_loss": 2847546.4921875, "training_acc": 51.0, "val_loss": 1585778.3203125, "val_acc": 52.0}
{"epoch": 4, "training_loss": 6024819.546875, "training_acc": 53.0, "val_loss": 1415037.6953125, "val_acc": 52.0}
{"epoch": 5, "training_loss": 4199721.65625, "training_acc": 53.0, "val_loss": 245860.4736328125, "val_acc": 48.0}
{"epoch": 6, "training_loss": 2266071.03125, "training_acc": 47.0, "val_loss": 724771.09375, "val_acc": 48.0}
{"epoch": 7, "training_loss": 2456685.0625, "training_acc": 47.0, "val_loss": 525169.091796875, "val_acc": 52.0}
{"epoch": 8, "training_loss": 2047768.0078125, "training_acc": 53.0, "val_loss": 910618.06640625, "val_acc": 52.0}
{"epoch": 9, "training_loss": 2956908.421875, "training_acc": 53.0, "val_loss": 114751.220703125, "val_acc": 52.0}
{"epoch": 10, "training_loss": 1182624.1796875, "training_acc": 49.0, "val_loss": 846684.86328125, "val_acc": 48.0}
{"epoch": 11, "training_loss": 3244075.6015625, "training_acc": 47.0, "val_loss": 179603.30810546875, "val_acc": 48.0}
{"epoch": 12, "training_loss": 1175396.7890625, "training_acc": 49.0, "val_loss": 890947.0703125, "val_acc": 52.0}
{"epoch": 13, "training_loss": 3425429.953125, "training_acc": 53.0, "val_loss": 667911.1328125, "val_acc": 52.0}
{"epoch": 14, "training_loss": 1793019.10546875, "training_acc": 52.0, "val_loss": 654981.73828125, "val_acc": 48.0}
{"epoch": 15, "training_loss": 3191061.234375, "training_acc": 47.0, "val_loss": 974175.390625, "val_acc": 48.0}
{"epoch": 16, "training_loss": 3286720.0078125, "training_acc": 47.0, "val_loss": 88727.72216796875, "val_acc": 60.0}
{"epoch": 17, "training_loss": 816155.734375, "training_acc": 54.0, "val_loss": 553983.056640625, "val_acc": 52.0}
{"epoch": 18, "training_loss": 1778388.345703125, "training_acc": 53.0, "val_loss": 125531.40869140625, "val_acc": 48.0}
{"epoch": 19, "training_loss": 552640.375, "training_acc": 50.0, "val_loss": 59984.979248046875, "val_acc": 56.0}
{"epoch": 20, "training_loss": 403421.515625, "training_acc": 60.0, "val_loss": 92915.26489257812, "val_acc": 60.0}
{"epoch": 21, "training_loss": 347167.083984375, "training_acc": 55.0, "val_loss": 99899.18823242188, "val_acc": 44.0}
{"epoch": 22, "training_loss": 482821.125, "training_acc": 55.0, "val_loss": 263461.3037109375, "val_acc": 52.0}
{"epoch": 23, "training_loss": 493406.849609375, "training_acc": 62.0, "val_loss": 258994.7998046875, "val_acc": 48.0}
{"epoch": 24, "training_loss": 958240.25, "training_acc": 48.0, "val_loss": 211289.84375, "val_acc": 52.0}
{"epoch": 25, "training_loss": 683698.4296875, "training_acc": 55.0, "val_loss": 94275.48828125, "val_acc": 60.0}
{"epoch": 26, "training_loss": 700573.04296875, "training_acc": 52.0, "val_loss": 232211.3037109375, "val_acc": 48.0}
{"epoch": 27, "training_loss": 820365.53515625, "training_acc": 49.0, "val_loss": 218329.78515625, "val_acc": 52.0}
{"epoch": 28, "training_loss": 437717.9716796875, "training_acc": 58.0, "val_loss": 141705.77392578125, "val_acc": 48.0}
{"epoch": 29, "training_loss": 450215.408203125, "training_acc": 58.0, "val_loss": 355810.8154296875, "val_acc": 52.0}
{"epoch": 30, "training_loss": 1219000.51171875, "training_acc": 53.0, "val_loss": 165341.796875, "val_acc": 56.0}
{"epoch": 31, "training_loss": 792954.828125, "training_acc": 52.0, "val_loss": 360041.9921875, "val_acc": 48.0}
{"epoch": 32, "training_loss": 1080731.0908203125, "training_acc": 51.0, "val_loss": 405045.654296875, "val_acc": 52.0}
{"epoch": 33, "training_loss": 1616721.9921875, "training_acc": 53.0, "val_loss": 369784.1796875, "val_acc": 52.0}
{"epoch": 34, "training_loss": 684780.1713867188, "training_acc": 66.0, "val_loss": 308295.5810546875, "val_acc": 48.0}
{"epoch": 35, "training_loss": 1132370.29296875, "training_acc": 47.0, "val_loss": 194639.92919921875, "val_acc": 52.0}
{"epoch": 36, "training_loss": 673283.140625, "training_acc": 55.0, "val_loss": 109017.5537109375, "val_acc": 60.0}
{"epoch": 37, "training_loss": 491003.640625, "training_acc": 49.0, "val_loss": 182190.90576171875, "val_acc": 48.0}
{"epoch": 38, "training_loss": 663193.591796875, "training_acc": 48.0, "val_loss": 210838.5009765625, "val_acc": 52.0}
