"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 3431016.2048797607, "training_acc": 51.0, "val_loss": 643367.28515625, "val_acc": 52.0}
{"epoch": 1, "training_loss": 3788273.15625, "training_acc": 55.0, "val_loss": 2408150.390625, "val_acc": 48.0}
{"epoch": 2, "training_loss": 9138554.03125, "training_acc": 47.0, "val_loss": 728711.23046875, "val_acc": 48.0}
{"epoch": 3, "training_loss": 2716625.484375, "training_acc": 53.0, "val_loss": 1518347.16796875, "val_acc": 52.0}
{"epoch": 4, "training_loss": 6021711.40625, "training_acc": 53.0, "val_loss": 1353756.25, "val_acc": 52.0}
{"epoch": 5, "training_loss": 4105963.8046875, "training_acc": 53.0, "val_loss": 402956.8603515625, "val_acc": 48.0}
{"epoch": 6, "training_loss": 2660639.71875, "training_acc": 47.0, "val_loss": 850594.62890625, "val_acc": 48.0}
{"epoch": 7, "training_loss": 2818261.48046875, "training_acc": 47.0, "val_loss": 470848.73046875, "val_acc": 52.0}
{"epoch": 8, "training_loss": 2179201.90625, "training_acc": 53.0, "val_loss": 785376.123046875, "val_acc": 52.0}
{"epoch": 9, "training_loss": 2474623.73828125, "training_acc": 53.0, "val_loss": 185158.87451171875, "val_acc": 48.0}
{"epoch": 10, "training_loss": 1035694.8984375, "training_acc": 48.0, "val_loss": 216142.5048828125, "val_acc": 48.0}
{"epoch": 11, "training_loss": 1007084.453125, "training_acc": 48.0, "val_loss": 426370.654296875, "val_acc": 52.0}
{"epoch": 12, "training_loss": 1318290.18359375, "training_acc": 52.0, "val_loss": 163632.26318359375, "val_acc": 48.0}
{"epoch": 13, "training_loss": 963424.83203125, "training_acc": 49.0, "val_loss": 86977.71606445312, "val_acc": 44.0}
{"epoch": 14, "training_loss": 411161.125, "training_acc": 61.0, "val_loss": 383447.4365234375, "val_acc": 52.0}
{"epoch": 15, "training_loss": 1097375.0703125, "training_acc": 53.0, "val_loss": 281201.46484375, "val_acc": 48.0}
{"epoch": 16, "training_loss": 1203268.60546875, "training_acc": 48.0, "val_loss": 140904.65087890625, "val_acc": 48.0}
{"epoch": 17, "training_loss": 663512.44921875, "training_acc": 57.0, "val_loss": 390166.4794921875, "val_acc": 52.0}
{"epoch": 18, "training_loss": 1112546.912109375, "training_acc": 57.0, "val_loss": 222969.7998046875, "val_acc": 48.0}
{"epoch": 19, "training_loss": 997192.8828125, "training_acc": 49.0, "val_loss": 64144.317626953125, "val_acc": 48.0}
{"epoch": 20, "training_loss": 560520.0703125, "training_acc": 59.0, "val_loss": 308607.8125, "val_acc": 52.0}
{"epoch": 21, "training_loss": 860729.8935546875, "training_acc": 51.0, "val_loss": 258973.681640625, "val_acc": 48.0}
{"epoch": 22, "training_loss": 886208.08984375, "training_acc": 50.0, "val_loss": 158871.142578125, "val_acc": 52.0}
{"epoch": 23, "training_loss": 565326.91796875, "training_acc": 56.0, "val_loss": 49144.63806152344, "val_acc": 52.0}
{"epoch": 24, "training_loss": 353222.537109375, "training_acc": 54.0, "val_loss": 71555.52368164062, "val_acc": 56.0}
{"epoch": 25, "training_loss": 431287.884765625, "training_acc": 55.0, "val_loss": 45903.997802734375, "val_acc": 56.0}
{"epoch": 26, "training_loss": 475397.61328125, "training_acc": 54.0, "val_loss": 51409.149169921875, "val_acc": 60.0}
{"epoch": 27, "training_loss": 334739.017578125, "training_acc": 58.0, "val_loss": 46248.80065917969, "val_acc": 52.0}
{"epoch": 28, "training_loss": 182061.52490234375, "training_acc": 63.0, "val_loss": 166375.40283203125, "val_acc": 52.0}
{"epoch": 29, "training_loss": 405150.0517578125, "training_acc": 58.0, "val_loss": 224768.2373046875, "val_acc": 48.0}
{"epoch": 30, "training_loss": 746791.244140625, "training_acc": 48.0, "val_loss": 122704.931640625, "val_acc": 52.0}
{"epoch": 31, "training_loss": 355464.9150390625, "training_acc": 57.0, "val_loss": 175408.94775390625, "val_acc": 48.0}
{"epoch": 32, "training_loss": 508361.51171875, "training_acc": 48.0, "val_loss": 317811.8896484375, "val_acc": 52.0}
{"epoch": 33, "training_loss": 1336755.5, "training_acc": 53.0, "val_loss": 172685.65673828125, "val_acc": 52.0}
{"epoch": 34, "training_loss": 904596.9375, "training_acc": 51.0, "val_loss": 552648.92578125, "val_acc": 48.0}
{"epoch": 35, "training_loss": 1833323.10546875, "training_acc": 47.0, "val_loss": 157442.63916015625, "val_acc": 52.0}
{"epoch": 36, "training_loss": 755867.42578125, "training_acc": 55.0, "val_loss": 228926.3427734375, "val_acc": 52.0}
{"epoch": 37, "training_loss": 954746.82421875, "training_acc": 42.0, "val_loss": 248230.5908203125, "val_acc": 48.0}
{"epoch": 38, "training_loss": 577873.8115234375, "training_acc": 55.0, "val_loss": 217729.98046875, "val_acc": 52.0}
{"epoch": 39, "training_loss": 604959.015625, "training_acc": 54.0, "val_loss": 218541.0888671875, "val_acc": 48.0}
{"epoch": 40, "training_loss": 682832.05078125, "training_acc": 49.0, "val_loss": 132935.65673828125, "val_acc": 52.0}
{"epoch": 41, "training_loss": 542567.23828125, "training_acc": 56.0, "val_loss": 61005.99365234375, "val_acc": 48.0}
{"epoch": 42, "training_loss": 252081.1181640625, "training_acc": 58.0, "val_loss": 50635.37292480469, "val_acc": 60.0}
{"epoch": 43, "training_loss": 330859.837890625, "training_acc": 60.0, "val_loss": 124606.18896484375, "val_acc": 48.0}
{"epoch": 44, "training_loss": 359925.34765625, "training_acc": 58.0, "val_loss": 140784.423828125, "val_acc": 52.0}
