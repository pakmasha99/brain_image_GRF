"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 185734.26289367676, "training_acc": 41.0, "val_loss": 32837.127685546875, "val_acc": 48.0}
{"epoch": 1, "training_loss": 155985.78955078125, "training_acc": 47.0, "val_loss": 72405.86547851562, "val_acc": 52.0}
{"epoch": 2, "training_loss": 277055.71826171875, "training_acc": 53.0, "val_loss": 49292.73376464844, "val_acc": 52.0}
{"epoch": 3, "training_loss": 140683.5625, "training_acc": 53.0, "val_loss": 27583.059692382812, "val_acc": 48.0}
{"epoch": 4, "training_loss": 148914.6484375, "training_acc": 47.0, "val_loss": 50657.06481933594, "val_acc": 48.0}
{"epoch": 5, "training_loss": 182486.22900390625, "training_acc": 47.0, "val_loss": 6290.930557250977, "val_acc": 48.0}
{"epoch": 6, "training_loss": 49499.46923828125, "training_acc": 53.0, "val_loss": 48756.280517578125, "val_acc": 52.0}
{"epoch": 7, "training_loss": 197038.34228515625, "training_acc": 53.0, "val_loss": 49608.52966308594, "val_acc": 52.0}
{"epoch": 8, "training_loss": 171929.8046875, "training_acc": 53.0, "val_loss": 10286.865997314453, "val_acc": 52.0}
{"epoch": 9, "training_loss": 46513.023193359375, "training_acc": 59.0, "val_loss": 41626.30615234375, "val_acc": 48.0}
{"epoch": 10, "training_loss": 167063.75830078125, "training_acc": 47.0, "val_loss": 38494.622802734375, "val_acc": 48.0}
{"epoch": 11, "training_loss": 120767.06201171875, "training_acc": 47.0, "val_loss": 7638.294982910156, "val_acc": 52.0}
{"epoch": 12, "training_loss": 52867.673583984375, "training_acc": 53.0, "val_loss": 25244.00177001953, "val_acc": 52.0}
{"epoch": 13, "training_loss": 89194.89013671875, "training_acc": 53.0, "val_loss": 2583.2977294921875, "val_acc": 52.0}
{"epoch": 14, "training_loss": 26844.59375, "training_acc": 58.0, "val_loss": 30876.760864257812, "val_acc": 48.0}
{"epoch": 15, "training_loss": 116196.79052734375, "training_acc": 47.0, "val_loss": 15921.337890625, "val_acc": 48.0}
{"epoch": 16, "training_loss": 38144.020446777344, "training_acc": 58.0, "val_loss": 16013.908386230469, "val_acc": 52.0}
{"epoch": 17, "training_loss": 61165.13732910156, "training_acc": 53.0, "val_loss": 6718.443298339844, "val_acc": 52.0}
{"epoch": 18, "training_loss": 24953.317504882812, "training_acc": 61.0, "val_loss": 17976.22833251953, "val_acc": 48.0}
{"epoch": 19, "training_loss": 63364.616943359375, "training_acc": 47.0, "val_loss": 1631.348991394043, "val_acc": 60.0}
{"epoch": 20, "training_loss": 24044.767578125, "training_acc": 56.0, "val_loss": 10845.785522460938, "val_acc": 52.0}
{"epoch": 21, "training_loss": 35106.55822753906, "training_acc": 52.0, "val_loss": 9334.120178222656, "val_acc": 48.0}
{"epoch": 22, "training_loss": 29568.72509765625, "training_acc": 52.0, "val_loss": 4026.5487670898438, "val_acc": 52.0}
{"epoch": 23, "training_loss": 16872.878845214844, "training_acc": 53.0, "val_loss": 1781.2175750732422, "val_acc": 44.0}
{"epoch": 24, "training_loss": 8016.372283935547, "training_acc": 59.0, "val_loss": 3768.920135498047, "val_acc": 52.0}
{"epoch": 25, "training_loss": 10576.8720703125, "training_acc": 58.0, "val_loss": 3959.634017944336, "val_acc": 44.0}
{"epoch": 26, "training_loss": 13443.811492919922, "training_acc": 55.0, "val_loss": 6864.096832275391, "val_acc": 52.0}
{"epoch": 27, "training_loss": 20576.35821533203, "training_acc": 54.0, "val_loss": 3831.8809509277344, "val_acc": 40.0}
{"epoch": 28, "training_loss": 14198.746276855469, "training_acc": 54.0, "val_loss": 5142.747116088867, "val_acc": 52.0}
{"epoch": 29, "training_loss": 14704.465850830078, "training_acc": 56.0, "val_loss": 3803.5587310791016, "val_acc": 48.0}
{"epoch": 30, "training_loss": 10912.228088378906, "training_acc": 50.0, "val_loss": 4134.220123291016, "val_acc": 52.0}
{"epoch": 31, "training_loss": 12126.838287353516, "training_acc": 53.0, "val_loss": 899.323558807373, "val_acc": 64.0}
{"epoch": 32, "training_loss": 5550.048370361328, "training_acc": 61.0, "val_loss": 2779.855728149414, "val_acc": 52.0}
{"epoch": 33, "training_loss": 8945.427856445312, "training_acc": 53.0, "val_loss": 2406.433868408203, "val_acc": 52.0}
{"epoch": 34, "training_loss": 6961.998268127441, "training_acc": 63.0, "val_loss": 1211.6397857666016, "val_acc": 64.0}
{"epoch": 35, "training_loss": 2939.8071479797363, "training_acc": 74.0, "val_loss": 1486.263370513916, "val_acc": 56.0}
{"epoch": 36, "training_loss": 9091.009948730469, "training_acc": 55.0, "val_loss": 4724.970626831055, "val_acc": 52.0}
{"epoch": 37, "training_loss": 12575.224594116211, "training_acc": 54.0, "val_loss": 8339.433288574219, "val_acc": 48.0}
{"epoch": 38, "training_loss": 29503.95831298828, "training_acc": 47.0, "val_loss": 7045.381927490234, "val_acc": 52.0}
{"epoch": 39, "training_loss": 26778.888916015625, "training_acc": 53.0, "val_loss": 1607.2612762451172, "val_acc": 44.0}
{"epoch": 40, "training_loss": 6266.097640991211, "training_acc": 55.0, "val_loss": 7326.197052001953, "val_acc": 52.0}
{"epoch": 41, "training_loss": 21697.12109375, "training_acc": 53.0, "val_loss": 4585.528182983398, "val_acc": 48.0}
{"epoch": 42, "training_loss": 13733.79719543457, "training_acc": 52.0, "val_loss": 7811.594390869141, "val_acc": 52.0}
{"epoch": 43, "training_loss": 23458.380981445312, "training_acc": 54.0, "val_loss": 4028.8169860839844, "val_acc": 48.0}
{"epoch": 44, "training_loss": 13239.919921875, "training_acc": 53.0, "val_loss": 7787.626647949219, "val_acc": 52.0}
{"epoch": 45, "training_loss": 24511.510681152344, "training_acc": 53.0, "val_loss": 2457.672882080078, "val_acc": 44.0}
{"epoch": 46, "training_loss": 9856.305694580078, "training_acc": 54.0, "val_loss": 5831.605911254883, "val_acc": 52.0}
{"epoch": 47, "training_loss": 14239.751708984375, "training_acc": 57.0, "val_loss": 5425.659942626953, "val_acc": 48.0}
{"epoch": 48, "training_loss": 14656.325012207031, "training_acc": 55.0, "val_loss": 7115.090179443359, "val_acc": 52.0}
{"epoch": 49, "training_loss": 23479.843383789062, "training_acc": 54.0, "val_loss": 6526.401519775391, "val_acc": 48.0}
{"epoch": 50, "training_loss": 26103.488403320312, "training_acc": 47.0, "val_loss": 3566.3806915283203, "val_acc": 52.0}
