"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 135233.71379470825, "training_acc": 46.0, "val_loss": 28257.852172851562, "val_acc": 52.0}
{"epoch": 1, "training_loss": 137217.1943359375, "training_acc": 53.0, "val_loss": 73181.37817382812, "val_acc": 48.0}
{"epoch": 2, "training_loss": 275418.482421875, "training_acc": 47.0, "val_loss": 20285.806274414062, "val_acc": 48.0}
{"epoch": 3, "training_loss": 95970.34008789062, "training_acc": 51.0, "val_loss": 53445.1416015625, "val_acc": 52.0}
{"epoch": 4, "training_loss": 203053.740234375, "training_acc": 53.0, "val_loss": 47690.66162109375, "val_acc": 52.0}
{"epoch": 5, "training_loss": 141542.548828125, "training_acc": 53.0, "val_loss": 8286.346435546875, "val_acc": 48.0}
{"epoch": 6, "training_loss": 76373.29736328125, "training_acc": 47.0, "val_loss": 24426.99737548828, "val_acc": 48.0}
{"epoch": 7, "training_loss": 82797.26550292969, "training_acc": 47.0, "val_loss": 17699.549865722656, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69015.5791015625, "training_acc": 53.0, "val_loss": 30690.274047851562, "val_acc": 52.0}
{"epoch": 9, "training_loss": 99656.21557617188, "training_acc": 53.0, "val_loss": 3867.2718048095703, "val_acc": 52.0}
{"epoch": 10, "training_loss": 39857.771728515625, "training_acc": 49.0, "val_loss": 28535.992431640625, "val_acc": 48.0}
{"epoch": 11, "training_loss": 109335.32373046875, "training_acc": 47.0, "val_loss": 6053.549575805664, "val_acc": 48.0}
{"epoch": 12, "training_loss": 39614.736083984375, "training_acc": 49.0, "val_loss": 30026.971435546875, "val_acc": 52.0}
{"epoch": 13, "training_loss": 115445.46044921875, "training_acc": 53.0, "val_loss": 22509.94110107422, "val_acc": 52.0}
{"epoch": 14, "training_loss": 60428.339263916016, "training_acc": 52.0, "val_loss": 22085.394287109375, "val_acc": 48.0}
{"epoch": 15, "training_loss": 107603.78515625, "training_acc": 47.0, "val_loss": 32860.809326171875, "val_acc": 48.0}
{"epoch": 16, "training_loss": 110893.40234375, "training_acc": 47.0, "val_loss": 2957.561492919922, "val_acc": 60.0}
{"epoch": 17, "training_loss": 28170.643798828125, "training_acc": 56.0, "val_loss": 19708.168029785156, "val_acc": 52.0}
{"epoch": 18, "training_loss": 64207.37536621094, "training_acc": 53.0, "val_loss": 2387.47501373291, "val_acc": 44.0}
{"epoch": 19, "training_loss": 16743.227416992188, "training_acc": 57.0, "val_loss": 3762.5137329101562, "val_acc": 48.0}
{"epoch": 20, "training_loss": 25498.778564453125, "training_acc": 47.0, "val_loss": 10638.614654541016, "val_acc": 52.0}
{"epoch": 21, "training_loss": 22780.62059020996, "training_acc": 59.0, "val_loss": 11713.587951660156, "val_acc": 48.0}
{"epoch": 22, "training_loss": 51677.981689453125, "training_acc": 47.0, "val_loss": 3159.57088470459, "val_acc": 44.0}
{"epoch": 23, "training_loss": 30681.7421875, "training_acc": 49.0, "val_loss": 23175.328063964844, "val_acc": 52.0}
{"epoch": 24, "training_loss": 75662.35278320312, "training_acc": 53.0, "val_loss": 8671.633911132812, "val_acc": 52.0}
{"epoch": 25, "training_loss": 35675.448486328125, "training_acc": 51.0, "val_loss": 18481.32781982422, "val_acc": 48.0}
{"epoch": 26, "training_loss": 67147.59741210938, "training_acc": 47.0, "val_loss": 2904.042625427246, "val_acc": 56.0}
{"epoch": 27, "training_loss": 25034.708740234375, "training_acc": 52.0, "val_loss": 19743.597412109375, "val_acc": 52.0}
{"epoch": 28, "training_loss": 57754.61364746094, "training_acc": 53.0, "val_loss": 2393.5136795043945, "val_acc": 32.0}
{"epoch": 29, "training_loss": 28214.840576171875, "training_acc": 51.0, "val_loss": 4068.300247192383, "val_acc": 48.0}
{"epoch": 30, "training_loss": 25675.257202148438, "training_acc": 52.0, "val_loss": 16158.631896972656, "val_acc": 52.0}
{"epoch": 31, "training_loss": 44325.670166015625, "training_acc": 53.0, "val_loss": 2673.5511779785156, "val_acc": 36.0}
{"epoch": 32, "training_loss": 18903.622314453125, "training_acc": 54.0, "val_loss": 1883.8844299316406, "val_acc": 52.0}
{"epoch": 33, "training_loss": 13463.517944335938, "training_acc": 57.0, "val_loss": 4095.2102661132812, "val_acc": 60.0}
{"epoch": 34, "training_loss": 10239.000427246094, "training_acc": 64.0, "val_loss": 4024.801254272461, "val_acc": 48.0}
{"epoch": 35, "training_loss": 15358.635192871094, "training_acc": 55.0, "val_loss": 7473.905181884766, "val_acc": 52.0}
{"epoch": 36, "training_loss": 17754.112365722656, "training_acc": 58.0, "val_loss": 4979.788970947266, "val_acc": 48.0}
{"epoch": 37, "training_loss": 24405.684692382812, "training_acc": 47.0, "val_loss": 8256.898498535156, "val_acc": 52.0}
{"epoch": 38, "training_loss": 27058.348876953125, "training_acc": 54.0, "val_loss": 2745.16544342041, "val_acc": 60.0}
{"epoch": 39, "training_loss": 15380.884887695312, "training_acc": 56.0, "val_loss": 2812.619972229004, "val_acc": 48.0}
{"epoch": 40, "training_loss": 18557.307556152344, "training_acc": 55.0, "val_loss": 11476.671600341797, "val_acc": 52.0}
{"epoch": 41, "training_loss": 27335.974731445312, "training_acc": 59.0, "val_loss": 7409.845733642578, "val_acc": 48.0}
{"epoch": 42, "training_loss": 33627.45935058594, "training_acc": 47.0, "val_loss": 3242.822265625, "val_acc": 56.0}
{"epoch": 43, "training_loss": 16400.243286132812, "training_acc": 57.0, "val_loss": 3457.619857788086, "val_acc": 56.0}
{"epoch": 44, "training_loss": 16386.869506835938, "training_acc": 55.0, "val_loss": 10130.156707763672, "val_acc": 48.0}
{"epoch": 45, "training_loss": 26354.02049255371, "training_acc": 52.0, "val_loss": 14944.984436035156, "val_acc": 52.0}
{"epoch": 46, "training_loss": 62587.893798828125, "training_acc": 53.0, "val_loss": 15221.957397460938, "val_acc": 52.0}
{"epoch": 47, "training_loss": 36595.282302856445, "training_acc": 55.0, "val_loss": 10649.923706054688, "val_acc": 48.0}
{"epoch": 48, "training_loss": 41980.51818847656, "training_acc": 47.0, "val_loss": 2815.079116821289, "val_acc": 60.0}
{"epoch": 49, "training_loss": 12566.110900878906, "training_acc": 59.0, "val_loss": 2514.3165588378906, "val_acc": 60.0}
{"epoch": 50, "training_loss": 19469.55029296875, "training_acc": 49.0, "val_loss": 5678.896331787109, "val_acc": 48.0}
{"epoch": 51, "training_loss": 22684.5263671875, "training_acc": 50.0, "val_loss": 9996.77505493164, "val_acc": 52.0}
