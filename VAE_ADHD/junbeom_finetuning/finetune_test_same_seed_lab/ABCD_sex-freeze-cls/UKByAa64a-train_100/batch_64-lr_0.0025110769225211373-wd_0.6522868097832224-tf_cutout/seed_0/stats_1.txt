"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 80.29089117050171, "training_acc": 47.0, "val_loss": 17.877952754497528, "val_acc": 52.0}
{"epoch": 1, "training_loss": 71.25916528701782, "training_acc": 49.0, "val_loss": 19.38544660806656, "val_acc": 52.0}
{"epoch": 2, "training_loss": 79.61211347579956, "training_acc": 53.0, "val_loss": 19.751912355422974, "val_acc": 52.0}
{"epoch": 3, "training_loss": 76.64101147651672, "training_acc": 53.0, "val_loss": 17.39727407693863, "val_acc": 52.0}
{"epoch": 4, "training_loss": 70.23883938789368, "training_acc": 47.0, "val_loss": 17.866884171962738, "val_acc": 52.0}
{"epoch": 5, "training_loss": 72.45077252388, "training_acc": 47.0, "val_loss": 18.154995143413544, "val_acc": 52.0}
{"epoch": 6, "training_loss": 72.68334197998047, "training_acc": 47.0, "val_loss": 17.419937252998352, "val_acc": 52.0}
{"epoch": 7, "training_loss": 68.69355368614197, "training_acc": 58.0, "val_loss": 17.434997856616974, "val_acc": 52.0}
{"epoch": 8, "training_loss": 71.00036191940308, "training_acc": 53.0, "val_loss": 18.274161219596863, "val_acc": 52.0}
{"epoch": 9, "training_loss": 72.7064368724823, "training_acc": 53.0, "val_loss": 17.931421101093292, "val_acc": 52.0}
{"epoch": 10, "training_loss": 70.40729522705078, "training_acc": 53.0, "val_loss": 17.291301488876343, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.76133179664612, "training_acc": 47.0, "val_loss": 17.609520256519318, "val_acc": 52.0}
{"epoch": 12, "training_loss": 70.75592470169067, "training_acc": 47.0, "val_loss": 17.649170756340027, "val_acc": 52.0}
{"epoch": 13, "training_loss": 70.64395356178284, "training_acc": 47.0, "val_loss": 17.334197461605072, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.07573390007019, "training_acc": 51.0, "val_loss": 17.39792227745056, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.24793362617493, "training_acc": 53.0, "val_loss": 17.69671142101288, "val_acc": 52.0}
{"epoch": 16, "training_loss": 70.53124570846558, "training_acc": 53.0, "val_loss": 17.653274536132812, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.77914094924927, "training_acc": 53.0, "val_loss": 17.317505180835724, "val_acc": 52.0}
{"epoch": 18, "training_loss": 70.18203639984131, "training_acc": 42.0, "val_loss": 17.367665469646454, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.62954759597778, "training_acc": 47.0, "val_loss": 17.335177958011627, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.29011988639832, "training_acc": 53.0, "val_loss": 17.29717254638672, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.29271793365479, "training_acc": 53.0, "val_loss": 17.444413900375366, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.74715852737427, "training_acc": 53.0, "val_loss": 17.420437932014465, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.13087320327759, "training_acc": 53.0, "val_loss": 17.289718985557556, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.07927370071411, "training_acc": 53.0, "val_loss": 17.502956092357635, "val_acc": 52.0}
{"epoch": 25, "training_loss": 70.2625617980957, "training_acc": 47.0, "val_loss": 17.513182759284973, "val_acc": 52.0}
{"epoch": 26, "training_loss": 70.01336622238159, "training_acc": 47.0, "val_loss": 17.294178903102875, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.2008547782898, "training_acc": 53.0, "val_loss": 17.43330955505371, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.4124801158905, "training_acc": 53.0, "val_loss": 17.53971576690674, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.77825856208801, "training_acc": 53.0, "val_loss": 17.38211363554001, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.26716566085815, "training_acc": 53.0, "val_loss": 17.29186773300171, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.1938145160675, "training_acc": 54.0, "val_loss": 17.301926016807556, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.44192290306091, "training_acc": 54.0, "val_loss": 17.290952801704407, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.12895655632019, "training_acc": 53.0, "val_loss": 17.29196459054947, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.19926905632019, "training_acc": 55.0, "val_loss": 17.29246824979782, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.73585557937622, "training_acc": 53.0, "val_loss": 17.317557334899902, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.09185457229614, "training_acc": 53.0, "val_loss": 17.297954857349396, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.33047842979431, "training_acc": 53.0, "val_loss": 17.295154929161072, "val_acc": 52.0}
{"epoch": 38, "training_loss": 68.96326875686646, "training_acc": 53.0, "val_loss": 17.34490692615509, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.05893898010254, "training_acc": 53.0, "val_loss": 17.39836484193802, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.2852554321289, "training_acc": 53.0, "val_loss": 17.388685047626495, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.1704454421997, "training_acc": 53.0, "val_loss": 17.299167811870575, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.07834601402283, "training_acc": 53.0, "val_loss": 17.303694784641266, "val_acc": 52.0}
