"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 416711.35051345825, "training_acc": 46.0, "val_loss": 87093.9208984375, "val_acc": 52.0}
{"epoch": 1, "training_loss": 422916.583984375, "training_acc": 53.0, "val_loss": 225552.05078125, "val_acc": 48.0}
{"epoch": 2, "training_loss": 848867.0703125, "training_acc": 47.0, "val_loss": 62522.71728515625, "val_acc": 48.0}
{"epoch": 3, "training_loss": 295790.4873046875, "training_acc": 51.0, "val_loss": 164723.49853515625, "val_acc": 52.0}
{"epoch": 4, "training_loss": 625831.59375, "training_acc": 53.0, "val_loss": 146987.744140625, "val_acc": 52.0}
{"epoch": 5, "training_loss": 436248.5146484375, "training_acc": 53.0, "val_loss": 25538.967895507812, "val_acc": 48.0}
{"epoch": 6, "training_loss": 235389.443359375, "training_acc": 47.0, "val_loss": 75286.10229492188, "val_acc": 48.0}
{"epoch": 7, "training_loss": 255189.44873046875, "training_acc": 47.0, "val_loss": 54552.130126953125, "val_acc": 52.0}
{"epoch": 8, "training_loss": 212713.09765625, "training_acc": 53.0, "val_loss": 94590.88745117188, "val_acc": 52.0}
{"epoch": 9, "training_loss": 307150.6650390625, "training_acc": 53.0, "val_loss": 11919.766235351562, "val_acc": 52.0}
{"epoch": 10, "training_loss": 122845.6552734375, "training_acc": 49.0, "val_loss": 87949.94506835938, "val_acc": 48.0}
{"epoch": 11, "training_loss": 336980.083984375, "training_acc": 47.0, "val_loss": 18656.480407714844, "val_acc": 48.0}
{"epoch": 12, "training_loss": 122095.16845703125, "training_acc": 49.0, "val_loss": 92547.54638671875, "val_acc": 52.0}
{"epoch": 13, "training_loss": 355818.599609375, "training_acc": 53.0, "val_loss": 69379.57153320312, "val_acc": 52.0}
{"epoch": 14, "training_loss": 186251.03161621094, "training_acc": 52.0, "val_loss": 68036.669921875, "val_acc": 48.0}
{"epoch": 15, "training_loss": 331473.4375, "training_acc": 47.0, "val_loss": 101193.115234375, "val_acc": 48.0}
{"epoch": 16, "training_loss": 341410.125, "training_acc": 47.0, "val_loss": 9216.49398803711, "val_acc": 60.0}
{"epoch": 17, "training_loss": 85041.2255859375, "training_acc": 54.0, "val_loss": 57910.58349609375, "val_acc": 52.0}
{"epoch": 18, "training_loss": 186229.95239257812, "training_acc": 53.0, "val_loss": 12324.77798461914, "val_acc": 48.0}
{"epoch": 19, "training_loss": 55857.364501953125, "training_acc": 51.0, "val_loss": 5898.944091796875, "val_acc": 56.0}
{"epoch": 20, "training_loss": 45823.981689453125, "training_acc": 58.0, "val_loss": 14607.48291015625, "val_acc": 56.0}
{"epoch": 21, "training_loss": 48187.3447265625, "training_acc": 52.0, "val_loss": 21081.8359375, "val_acc": 48.0}
{"epoch": 22, "training_loss": 63635.728759765625, "training_acc": 52.0, "val_loss": 27097.317504882812, "val_acc": 52.0}
{"epoch": 23, "training_loss": 58198.104736328125, "training_acc": 56.0, "val_loss": 22633.843994140625, "val_acc": 48.0}
{"epoch": 24, "training_loss": 89951.02758789062, "training_acc": 48.0, "val_loss": 14241.110229492188, "val_acc": 56.0}
{"epoch": 25, "training_loss": 51849.77490234375, "training_acc": 60.0, "val_loss": 9045.56655883789, "val_acc": 56.0}
{"epoch": 26, "training_loss": 68271.69775390625, "training_acc": 51.0, "val_loss": 19736.036682128906, "val_acc": 48.0}
{"epoch": 27, "training_loss": 76681.74926757812, "training_acc": 50.0, "val_loss": 26714.996337890625, "val_acc": 52.0}
{"epoch": 28, "training_loss": 58552.667907714844, "training_acc": 55.0, "val_loss": 30262.420654296875, "val_acc": 48.0}
{"epoch": 29, "training_loss": 123225.5244140625, "training_acc": 48.0, "val_loss": 12979.574584960938, "val_acc": 56.0}
{"epoch": 30, "training_loss": 50596.333251953125, "training_acc": 52.0, "val_loss": 13399.961853027344, "val_acc": 56.0}
{"epoch": 31, "training_loss": 56848.0263671875, "training_acc": 52.0, "val_loss": 14046.189880371094, "val_acc": 48.0}
{"epoch": 32, "training_loss": 71283.62426757812, "training_acc": 49.0, "val_loss": 29253.436279296875, "val_acc": 52.0}
{"epoch": 33, "training_loss": 62883.75964355469, "training_acc": 59.0, "val_loss": 29532.119750976562, "val_acc": 48.0}
{"epoch": 34, "training_loss": 118325.74243164062, "training_acc": 47.0, "val_loss": 11586.945343017578, "val_acc": 56.0}
{"epoch": 35, "training_loss": 38423.564697265625, "training_acc": 54.0, "val_loss": 4177.620697021484, "val_acc": 48.0}
{"epoch": 36, "training_loss": 25852.610595703125, "training_acc": 65.0, "val_loss": 6174.724578857422, "val_acc": 60.0}
{"epoch": 37, "training_loss": 23968.955444335938, "training_acc": 61.0, "val_loss": 3152.1739959716797, "val_acc": 52.0}
{"epoch": 38, "training_loss": 24311.765014648438, "training_acc": 61.0, "val_loss": 17290.3076171875, "val_acc": 52.0}
{"epoch": 39, "training_loss": 46362.385009765625, "training_acc": 57.0, "val_loss": 22410.003662109375, "val_acc": 48.0}
{"epoch": 40, "training_loss": 73103.802734375, "training_acc": 47.0, "val_loss": 21801.59454345703, "val_acc": 52.0}
{"epoch": 41, "training_loss": 83756.65161132812, "training_acc": 53.0, "val_loss": 4660.964584350586, "val_acc": 60.0}
{"epoch": 42, "training_loss": 69902.0966796875, "training_acc": 54.0, "val_loss": 40696.96350097656, "val_acc": 48.0}
{"epoch": 43, "training_loss": 110789.93365478516, "training_acc": 48.0, "val_loss": 28316.891479492188, "val_acc": 52.0}
{"epoch": 44, "training_loss": 114535.31494140625, "training_acc": 53.0, "val_loss": 4910.577011108398, "val_acc": 48.0}
{"epoch": 45, "training_loss": 48042.549072265625, "training_acc": 58.0, "val_loss": 6120.269393920898, "val_acc": 48.0}
{"epoch": 46, "training_loss": 75599.6279296875, "training_acc": 45.0, "val_loss": 41969.091796875, "val_acc": 52.0}
{"epoch": 47, "training_loss": 111398.08972167969, "training_acc": 54.0, "val_loss": 37572.943115234375, "val_acc": 48.0}
{"epoch": 48, "training_loss": 182468.373046875, "training_acc": 47.0, "val_loss": 27989.376831054688, "val_acc": 48.0}
{"epoch": 49, "training_loss": 96766.30981445312, "training_acc": 53.0, "val_loss": 48353.411865234375, "val_acc": 52.0}
{"epoch": 50, "training_loss": 156491.11401367188, "training_acc": 53.0, "val_loss": 2337.232208251953, "val_acc": 60.0}
{"epoch": 51, "training_loss": 41148.65673828125, "training_acc": 62.0, "val_loss": 10532.698059082031, "val_acc": 48.0}
{"epoch": 52, "training_loss": 66909.38134765625, "training_acc": 51.0, "val_loss": 38131.536865234375, "val_acc": 52.0}
{"epoch": 53, "training_loss": 105886.55358886719, "training_acc": 54.0, "val_loss": 33380.401611328125, "val_acc": 48.0}
{"epoch": 54, "training_loss": 142826.75244140625, "training_acc": 47.0, "val_loss": 16098.521423339844, "val_acc": 48.0}
{"epoch": 55, "training_loss": 66315.63403320312, "training_acc": 54.0, "val_loss": 50481.46057128906, "val_acc": 52.0}
{"epoch": 56, "training_loss": 171514.62475585938, "training_acc": 53.0, "val_loss": 5082.661056518555, "val_acc": 40.0}
{"epoch": 57, "training_loss": 42412.135498046875, "training_acc": 59.0, "val_loss": 9972.269439697266, "val_acc": 48.0}
{"epoch": 58, "training_loss": 51153.6142578125, "training_acc": 60.0, "val_loss": 38669.683837890625, "val_acc": 52.0}
{"epoch": 59, "training_loss": 111622.62768554688, "training_acc": 53.0, "val_loss": 30911.404418945312, "val_acc": 48.0}
{"epoch": 60, "training_loss": 119507.04150390625, "training_acc": 48.0, "val_loss": 23570.90301513672, "val_acc": 48.0}
{"epoch": 61, "training_loss": 81253.2158203125, "training_acc": 51.0, "val_loss": 36725.05798339844, "val_acc": 52.0}
{"epoch": 62, "training_loss": 111747.82446289062, "training_acc": 53.0, "val_loss": 26672.784423828125, "val_acc": 48.0}
{"epoch": 63, "training_loss": 120849.23291015625, "training_acc": 47.0, "val_loss": 8055.060577392578, "val_acc": 44.0}
{"epoch": 64, "training_loss": 74128.27783203125, "training_acc": 58.0, "val_loss": 59131.8603515625, "val_acc": 52.0}
{"epoch": 65, "training_loss": 184611.35302734375, "training_acc": 53.0, "val_loss": 4650.111389160156, "val_acc": 56.0}
{"epoch": 66, "training_loss": 64866.4970703125, "training_acc": 58.0, "val_loss": 24037.88299560547, "val_acc": 48.0}
{"epoch": 67, "training_loss": 91900.63623046875, "training_acc": 43.0, "val_loss": 29505.23681640625, "val_acc": 52.0}
{"epoch": 68, "training_loss": 55764.428955078125, "training_acc": 62.0, "val_loss": 20916.552734375, "val_acc": 48.0}
{"epoch": 69, "training_loss": 71090.70959472656, "training_acc": 49.0, "val_loss": 21385.067749023438, "val_acc": 52.0}
