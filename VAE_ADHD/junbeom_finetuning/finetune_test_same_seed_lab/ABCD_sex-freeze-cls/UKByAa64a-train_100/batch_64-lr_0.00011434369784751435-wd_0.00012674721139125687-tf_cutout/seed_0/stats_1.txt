"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 70.02751231193542, "training_acc": 47.0, "val_loss": 17.52474308013916, "val_acc": 52.0}
{"epoch": 1, "training_loss": 70.04620790481567, "training_acc": 47.0, "val_loss": 17.494972050189972, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.9866590499878, "training_acc": 47.0, "val_loss": 17.465020716190338, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.66558313369751, "training_acc": 47.0, "val_loss": 17.436082661151886, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.55264544487, "training_acc": 48.0, "val_loss": 17.41122454404831, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.48834204673767, "training_acc": 45.0, "val_loss": 17.389000952243805, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.4375569820404, "training_acc": 44.0, "val_loss": 17.371268570423126, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.37787699699402, "training_acc": 46.0, "val_loss": 17.358385026454926, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.22201204299927, "training_acc": 48.0, "val_loss": 17.346736788749695, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.25995230674744, "training_acc": 52.0, "val_loss": 17.337536811828613, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.38711643218994, "training_acc": 53.0, "val_loss": 17.330175638198853, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.39746141433716, "training_acc": 52.0, "val_loss": 17.325054109096527, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.24336528778076, "training_acc": 53.0, "val_loss": 17.322073876857758, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.26044178009033, "training_acc": 53.0, "val_loss": 17.32073575258255, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.29165577888489, "training_acc": 53.0, "val_loss": 17.319591343402863, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.0837652683258, "training_acc": 53.0, "val_loss": 17.318731546401978, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.16505885124207, "training_acc": 53.0, "val_loss": 17.318174242973328, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.13616871833801, "training_acc": 53.0, "val_loss": 17.317765951156616, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.02025628089905, "training_acc": 53.0, "val_loss": 17.317412793636322, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.10360360145569, "training_acc": 53.0, "val_loss": 17.317236959934235, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.09323287010193, "training_acc": 53.0, "val_loss": 17.31724888086319, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.26145553588867, "training_acc": 53.0, "val_loss": 17.317359149456024, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.12104272842407, "training_acc": 53.0, "val_loss": 17.317305505275726, "val_acc": 52.0}
{"epoch": 23, "training_loss": 68.97504997253418, "training_acc": 53.0, "val_loss": 17.31712371110916, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.18845319747925, "training_acc": 53.0, "val_loss": 17.31714904308319, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.01034879684448, "training_acc": 53.0, "val_loss": 17.3171266913414, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.03889393806458, "training_acc": 53.0, "val_loss": 17.317290604114532, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.27204608917236, "training_acc": 53.0, "val_loss": 17.317530512809753, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.11994528770447, "training_acc": 53.0, "val_loss": 17.317840456962585, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.24964547157288, "training_acc": 53.0, "val_loss": 17.318379878997803, "val_acc": 52.0}
{"epoch": 30, "training_loss": 68.97634840011597, "training_acc": 53.0, "val_loss": 17.318835854530334, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.10625123977661, "training_acc": 53.0, "val_loss": 17.319251596927643, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.07228755950928, "training_acc": 53.0, "val_loss": 17.319558560848236, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.18888831138611, "training_acc": 53.0, "val_loss": 17.31974333524704, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.08972215652466, "training_acc": 53.0, "val_loss": 17.319907248020172, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.12742185592651, "training_acc": 53.0, "val_loss": 17.320340871810913, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.04794788360596, "training_acc": 53.0, "val_loss": 17.32065975666046, "val_acc": 52.0}
{"epoch": 37, "training_loss": 68.99102854728699, "training_acc": 53.0, "val_loss": 17.321081459522247, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.1143651008606, "training_acc": 53.0, "val_loss": 17.321136593818665, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.29909062385559, "training_acc": 53.0, "val_loss": 17.32056438922882, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.20830965042114, "training_acc": 53.0, "val_loss": 17.320185899734497, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.23208332061768, "training_acc": 53.0, "val_loss": 17.319664359092712, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.22008180618286, "training_acc": 53.0, "val_loss": 17.319701611995697, "val_acc": 52.0}
