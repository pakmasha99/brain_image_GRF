"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.23486304283142, "training_acc": 55.0, "val_loss": 17.38765239715576, "val_acc": 56.0}
{"epoch": 1, "training_loss": 69.35609197616577, "training_acc": 46.0, "val_loss": 17.35805869102478, "val_acc": 56.0}
{"epoch": 2, "training_loss": 69.43106937408447, "training_acc": 47.0, "val_loss": 17.338936030864716, "val_acc": 56.0}
{"epoch": 3, "training_loss": 69.12868428230286, "training_acc": 55.0, "val_loss": 17.327407002449036, "val_acc": 56.0}
{"epoch": 4, "training_loss": 69.13892030715942, "training_acc": 52.0, "val_loss": 17.320911586284637, "val_acc": 56.0}
{"epoch": 5, "training_loss": 69.36290454864502, "training_acc": 52.0, "val_loss": 17.313840985298157, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.25273895263672, "training_acc": 51.0, "val_loss": 17.311908304691315, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.32723736763, "training_acc": 51.0, "val_loss": 17.309026420116425, "val_acc": 56.0}
{"epoch": 8, "training_loss": 69.27331852912903, "training_acc": 52.0, "val_loss": 17.30428636074066, "val_acc": 56.0}
{"epoch": 9, "training_loss": 69.26415038108826, "training_acc": 52.0, "val_loss": 17.30414479970932, "val_acc": 56.0}
{"epoch": 10, "training_loss": 69.23494338989258, "training_acc": 51.0, "val_loss": 17.304274439811707, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.3613076210022, "training_acc": 52.0, "val_loss": 17.303065955638885, "val_acc": 56.0}
{"epoch": 12, "training_loss": 69.15740489959717, "training_acc": 52.0, "val_loss": 17.302492260932922, "val_acc": 56.0}
{"epoch": 13, "training_loss": 69.27266192436218, "training_acc": 52.0, "val_loss": 17.30348914861679, "val_acc": 56.0}
{"epoch": 14, "training_loss": 69.11865282058716, "training_acc": 52.0, "val_loss": 17.303085327148438, "val_acc": 56.0}
{"epoch": 15, "training_loss": 69.22552585601807, "training_acc": 52.0, "val_loss": 17.301282286643982, "val_acc": 56.0}
{"epoch": 16, "training_loss": 69.24566721916199, "training_acc": 52.0, "val_loss": 17.297084629535675, "val_acc": 56.0}
{"epoch": 17, "training_loss": 69.38686680793762, "training_acc": 52.0, "val_loss": 17.29448139667511, "val_acc": 56.0}
{"epoch": 18, "training_loss": 69.23983335494995, "training_acc": 52.0, "val_loss": 17.29211062192917, "val_acc": 56.0}
{"epoch": 19, "training_loss": 68.99463677406311, "training_acc": 52.0, "val_loss": 17.289607226848602, "val_acc": 56.0}
{"epoch": 20, "training_loss": 69.20351672172546, "training_acc": 52.0, "val_loss": 17.289233207702637, "val_acc": 56.0}
{"epoch": 21, "training_loss": 69.18954944610596, "training_acc": 52.0, "val_loss": 17.28847026824951, "val_acc": 56.0}
{"epoch": 22, "training_loss": 69.2810046672821, "training_acc": 52.0, "val_loss": 17.28636920452118, "val_acc": 56.0}
{"epoch": 23, "training_loss": 69.11282348632812, "training_acc": 52.0, "val_loss": 17.287108302116394, "val_acc": 56.0}
{"epoch": 24, "training_loss": 69.32436871528625, "training_acc": 52.0, "val_loss": 17.289678752422333, "val_acc": 56.0}
{"epoch": 25, "training_loss": 69.25225448608398, "training_acc": 52.0, "val_loss": 17.293904721736908, "val_acc": 56.0}
{"epoch": 26, "training_loss": 68.92030787467957, "training_acc": 52.0, "val_loss": 17.295360565185547, "val_acc": 56.0}
{"epoch": 27, "training_loss": 69.29132437705994, "training_acc": 52.0, "val_loss": 17.301496863365173, "val_acc": 56.0}
{"epoch": 28, "training_loss": 69.09040117263794, "training_acc": 52.0, "val_loss": 17.30782985687256, "val_acc": 56.0}
{"epoch": 29, "training_loss": 69.19621157646179, "training_acc": 52.0, "val_loss": 17.31169819831848, "val_acc": 56.0}
{"epoch": 30, "training_loss": 69.09105324745178, "training_acc": 52.0, "val_loss": 17.313213646411896, "val_acc": 56.0}
{"epoch": 31, "training_loss": 69.25094890594482, "training_acc": 52.0, "val_loss": 17.318107187747955, "val_acc": 56.0}
{"epoch": 32, "training_loss": 69.29337978363037, "training_acc": 52.0, "val_loss": 17.31894016265869, "val_acc": 56.0}
{"epoch": 33, "training_loss": 68.86396527290344, "training_acc": 52.0, "val_loss": 17.31995791196823, "val_acc": 56.0}
{"epoch": 34, "training_loss": 69.11958312988281, "training_acc": 52.0, "val_loss": 17.319507896900177, "val_acc": 56.0}
{"epoch": 35, "training_loss": 69.19477105140686, "training_acc": 52.0, "val_loss": 17.32034683227539, "val_acc": 56.0}
{"epoch": 36, "training_loss": 69.12792134284973, "training_acc": 52.0, "val_loss": 17.320431768894196, "val_acc": 56.0}
{"epoch": 37, "training_loss": 69.19255828857422, "training_acc": 52.0, "val_loss": 17.319199442863464, "val_acc": 56.0}
{"epoch": 38, "training_loss": 69.08609366416931, "training_acc": 52.0, "val_loss": 17.315135896205902, "val_acc": 56.0}
{"epoch": 39, "training_loss": 69.07946348190308, "training_acc": 52.0, "val_loss": 17.30893701314926, "val_acc": 56.0}
{"epoch": 40, "training_loss": 69.06644320487976, "training_acc": 52.0, "val_loss": 17.306071519851685, "val_acc": 56.0}
{"epoch": 41, "training_loss": 69.07801723480225, "training_acc": 52.0, "val_loss": 17.30448454618454, "val_acc": 56.0}
