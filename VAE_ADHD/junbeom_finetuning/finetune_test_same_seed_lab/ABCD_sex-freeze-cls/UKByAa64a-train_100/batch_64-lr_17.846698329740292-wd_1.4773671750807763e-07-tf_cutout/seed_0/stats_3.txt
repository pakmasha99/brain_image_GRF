"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 40849.92300415039, "training_acc": 45.0, "val_loss": 9911.390686035156, "val_acc": 52.0}
{"epoch": 1, "training_loss": 36907.61779785156, "training_acc": 55.0, "val_loss": 17142.210388183594, "val_acc": 48.0}
{"epoch": 2, "training_loss": 63887.120849609375, "training_acc": 47.0, "val_loss": 2438.6390686035156, "val_acc": 48.0}
{"epoch": 3, "training_loss": 23358.26220703125, "training_acc": 47.0, "val_loss": 17612.210083007812, "val_acc": 52.0}
{"epoch": 4, "training_loss": 64311.39013671875, "training_acc": 53.0, "val_loss": 15150.343322753906, "val_acc": 52.0}
{"epoch": 5, "training_loss": 45847.789978027344, "training_acc": 53.0, "val_loss": 1227.973461151123, "val_acc": 44.0}
{"epoch": 6, "training_loss": 13525.961059570312, "training_acc": 46.0, "val_loss": 5331.476974487305, "val_acc": 48.0}
{"epoch": 7, "training_loss": 17340.211853027344, "training_acc": 48.0, "val_loss": 5868.242645263672, "val_acc": 52.0}
{"epoch": 8, "training_loss": 18352.249267578125, "training_acc": 55.0, "val_loss": 7715.027618408203, "val_acc": 52.0}
{"epoch": 9, "training_loss": 17316.121551513672, "training_acc": 56.0, "val_loss": 1809.549903869629, "val_acc": 44.0}
{"epoch": 10, "training_loss": 14232.318542480469, "training_acc": 46.0, "val_loss": 1346.8473434448242, "val_acc": 40.0}
{"epoch": 11, "training_loss": 7621.251922607422, "training_acc": 57.0, "val_loss": 6578.184509277344, "val_acc": 52.0}
{"epoch": 12, "training_loss": 16418.921325683594, "training_acc": 52.0, "val_loss": 3321.7525482177734, "val_acc": 52.0}
{"epoch": 13, "training_loss": 8539.094360351562, "training_acc": 49.0, "val_loss": 1846.4351654052734, "val_acc": 48.0}
{"epoch": 14, "training_loss": 7898.279342651367, "training_acc": 51.0, "val_loss": 3374.9088287353516, "val_acc": 52.0}
{"epoch": 15, "training_loss": 6501.6356201171875, "training_acc": 55.0, "val_loss": 621.4717388153076, "val_acc": 48.0}
{"epoch": 16, "training_loss": 3114.0070571899414, "training_acc": 55.0, "val_loss": 2115.8769607543945, "val_acc": 52.0}
{"epoch": 17, "training_loss": 4384.645294189453, "training_acc": 54.0, "val_loss": 1736.418342590332, "val_acc": 48.0}
{"epoch": 18, "training_loss": 6365.275726318359, "training_acc": 49.0, "val_loss": 1865.2412414550781, "val_acc": 52.0}
{"epoch": 19, "training_loss": 5589.321319580078, "training_acc": 52.0, "val_loss": 1416.9660568237305, "val_acc": 48.0}
{"epoch": 20, "training_loss": 6885.646942138672, "training_acc": 44.0, "val_loss": 2094.9771881103516, "val_acc": 52.0}
{"epoch": 21, "training_loss": 6146.43701171875, "training_acc": 53.0, "val_loss": 299.9295234680176, "val_acc": 64.0}
{"epoch": 22, "training_loss": 2184.461067199707, "training_acc": 56.0, "val_loss": 354.7105312347412, "val_acc": 60.0}
{"epoch": 23, "training_loss": 3040.5822830200195, "training_acc": 56.0, "val_loss": 1273.8032341003418, "val_acc": 52.0}
{"epoch": 24, "training_loss": 3566.6265869140625, "training_acc": 56.0, "val_loss": 637.0758533477783, "val_acc": 36.0}
{"epoch": 25, "training_loss": 5089.764923095703, "training_acc": 52.0, "val_loss": 1936.3788604736328, "val_acc": 52.0}
{"epoch": 26, "training_loss": 4743.623718261719, "training_acc": 55.0, "val_loss": 1323.0984687805176, "val_acc": 48.0}
{"epoch": 27, "training_loss": 6687.448577880859, "training_acc": 44.0, "val_loss": 3001.848602294922, "val_acc": 52.0}
{"epoch": 28, "training_loss": 4807.483779907227, "training_acc": 59.0, "val_loss": 813.0053520202637, "val_acc": 44.0}
{"epoch": 29, "training_loss": 4995.265609741211, "training_acc": 47.0, "val_loss": 1899.3419647216797, "val_acc": 52.0}
{"epoch": 30, "training_loss": 3320.3450927734375, "training_acc": 57.0, "val_loss": 910.9062194824219, "val_acc": 44.0}
{"epoch": 31, "training_loss": 1755.6041564941406, "training_acc": 65.0, "val_loss": 396.22459411621094, "val_acc": 48.0}
{"epoch": 32, "training_loss": 1765.45166015625, "training_acc": 61.0, "val_loss": 252.26712226867676, "val_acc": 44.0}
{"epoch": 33, "training_loss": 1455.2756881713867, "training_acc": 63.0, "val_loss": 441.31007194519043, "val_acc": 48.0}
{"epoch": 34, "training_loss": 1757.2887153625488, "training_acc": 56.0, "val_loss": 255.71584701538086, "val_acc": 64.0}
{"epoch": 35, "training_loss": 1784.4103088378906, "training_acc": 54.0, "val_loss": 385.24935245513916, "val_acc": 48.0}
{"epoch": 36, "training_loss": 2010.2221603393555, "training_acc": 56.0, "val_loss": 519.2431449890137, "val_acc": 52.0}
{"epoch": 37, "training_loss": 3005.6276245117188, "training_acc": 47.0, "val_loss": 320.38018703460693, "val_acc": 48.0}
{"epoch": 38, "training_loss": 1982.4885711669922, "training_acc": 56.0, "val_loss": 2896.5227127075195, "val_acc": 52.0}
{"epoch": 39, "training_loss": 7742.664749145508, "training_acc": 53.0, "val_loss": 293.674373626709, "val_acc": 56.0}
{"epoch": 40, "training_loss": 2641.105712890625, "training_acc": 61.0, "val_loss": 798.8514423370361, "val_acc": 52.0}
{"epoch": 41, "training_loss": 2588.8915252685547, "training_acc": 57.0, "val_loss": 709.2004299163818, "val_acc": 48.0}
{"epoch": 42, "training_loss": 2929.0452041625977, "training_acc": 53.0, "val_loss": 1593.4916496276855, "val_acc": 52.0}
{"epoch": 43, "training_loss": 3422.638702392578, "training_acc": 48.0, "val_loss": 570.5994129180908, "val_acc": 48.0}
{"epoch": 44, "training_loss": 758.479658126831, "training_acc": 74.0, "val_loss": 252.70168781280518, "val_acc": 48.0}
{"epoch": 45, "training_loss": 769.778865814209, "training_acc": 67.0, "val_loss": 195.86986303329468, "val_acc": 64.0}
{"epoch": 46, "training_loss": 630.5258102416992, "training_acc": 75.0, "val_loss": 326.131272315979, "val_acc": 52.0}
{"epoch": 47, "training_loss": 2236.8455200195312, "training_acc": 48.0, "val_loss": 527.7657985687256, "val_acc": 48.0}
{"epoch": 48, "training_loss": 2409.3567428588867, "training_acc": 48.0, "val_loss": 909.9527359008789, "val_acc": 48.0}
{"epoch": 49, "training_loss": 1759.383557319641, "training_acc": 66.0, "val_loss": 1584.7498893737793, "val_acc": 52.0}
{"epoch": 50, "training_loss": 3338.067211151123, "training_acc": 62.0, "val_loss": 746.0037708282471, "val_acc": 48.0}
{"epoch": 51, "training_loss": 4265.806213378906, "training_acc": 46.0, "val_loss": 1266.3200378417969, "val_acc": 52.0}
{"epoch": 52, "training_loss": 6471.760711669922, "training_acc": 49.0, "val_loss": 2510.3090286254883, "val_acc": 48.0}
{"epoch": 53, "training_loss": 6287.556396484375, "training_acc": 58.0, "val_loss": 1983.9672088623047, "val_acc": 52.0}
{"epoch": 54, "training_loss": 4038.893238067627, "training_acc": 58.0, "val_loss": 302.8806686401367, "val_acc": 56.0}
{"epoch": 55, "training_loss": 2395.316940307617, "training_acc": 60.0, "val_loss": 595.3557014465332, "val_acc": 52.0}
{"epoch": 56, "training_loss": 1852.6106414794922, "training_acc": 65.0, "val_loss": 933.8223457336426, "val_acc": 52.0}
{"epoch": 57, "training_loss": 1159.4613704681396, "training_acc": 71.0, "val_loss": 1065.2986526489258, "val_acc": 48.0}
{"epoch": 58, "training_loss": 2907.8079652786255, "training_acc": 62.0, "val_loss": 1259.571361541748, "val_acc": 52.0}
{"epoch": 59, "training_loss": 2665.0018157958984, "training_acc": 54.0, "val_loss": 509.60354804992676, "val_acc": 48.0}
{"epoch": 60, "training_loss": 653.2612705230713, "training_acc": 74.0, "val_loss": 172.92003631591797, "val_acc": 60.0}
{"epoch": 61, "training_loss": 1059.8275299072266, "training_acc": 71.0, "val_loss": 291.51604175567627, "val_acc": 56.0}
{"epoch": 62, "training_loss": 1656.6349411010742, "training_acc": 59.0, "val_loss": 846.4043617248535, "val_acc": 48.0}
{"epoch": 63, "training_loss": 1950.0158233642578, "training_acc": 56.0, "val_loss": 157.2697401046753, "val_acc": 76.0}
{"epoch": 64, "training_loss": 861.8994827270508, "training_acc": 73.0, "val_loss": 1163.6314392089844, "val_acc": 52.0}
{"epoch": 65, "training_loss": 2268.345600128174, "training_acc": 69.0, "val_loss": 1710.060691833496, "val_acc": 48.0}
{"epoch": 66, "training_loss": 4161.149948120117, "training_acc": 56.0, "val_loss": 3043.522071838379, "val_acc": 52.0}
{"epoch": 67, "training_loss": 9015.710479736328, "training_acc": 53.0, "val_loss": 427.5469779968262, "val_acc": 56.0}
{"epoch": 68, "training_loss": 2039.6153030395508, "training_acc": 55.0, "val_loss": 346.8335151672363, "val_acc": 48.0}
{"epoch": 69, "training_loss": 708.2957992553711, "training_acc": 76.0, "val_loss": 457.7925205230713, "val_acc": 52.0}
{"epoch": 70, "training_loss": 2526.6697540283203, "training_acc": 54.0, "val_loss": 720.4587459564209, "val_acc": 52.0}
{"epoch": 71, "training_loss": 3478.152069091797, "training_acc": 60.0, "val_loss": 1019.0656661987305, "val_acc": 48.0}
{"epoch": 72, "training_loss": 4405.788177490234, "training_acc": 59.0, "val_loss": 3403.3275604248047, "val_acc": 52.0}
{"epoch": 73, "training_loss": 9048.775024414062, "training_acc": 53.0, "val_loss": 3688.7107849121094, "val_acc": 48.0}
{"epoch": 74, "training_loss": 17345.544006347656, "training_acc": 47.0, "val_loss": 3242.4388885498047, "val_acc": 48.0}
{"epoch": 75, "training_loss": 12450.33740234375, "training_acc": 41.0, "val_loss": 3754.593276977539, "val_acc": 52.0}
{"epoch": 76, "training_loss": 11348.388687133789, "training_acc": 53.0, "val_loss": 2287.312126159668, "val_acc": 48.0}
{"epoch": 77, "training_loss": 9105.231353759766, "training_acc": 47.0, "val_loss": 191.36208295822144, "val_acc": 76.0}
{"epoch": 78, "training_loss": 5519.6988525390625, "training_acc": 65.0, "val_loss": 3807.6358795166016, "val_acc": 52.0}
{"epoch": 79, "training_loss": 10249.235229492188, "training_acc": 54.0, "val_loss": 3858.205795288086, "val_acc": 48.0}
{"epoch": 80, "training_loss": 17077.3740234375, "training_acc": 47.0, "val_loss": 2548.7627029418945, "val_acc": 48.0}
{"epoch": 81, "training_loss": 9598.15786743164, "training_acc": 49.0, "val_loss": 4907.6416015625, "val_acc": 52.0}
{"epoch": 82, "training_loss": 15147.564758300781, "training_acc": 53.0, "val_loss": 226.125168800354, "val_acc": 72.0}
