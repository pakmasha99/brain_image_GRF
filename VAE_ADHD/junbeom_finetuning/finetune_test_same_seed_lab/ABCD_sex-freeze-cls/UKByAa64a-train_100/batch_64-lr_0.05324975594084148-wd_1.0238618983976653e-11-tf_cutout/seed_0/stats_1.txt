"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 156.33355808258057, "training_acc": 46.0, "val_loss": 26.56051516532898, "val_acc": 52.0}
{"epoch": 1, "training_loss": 120.88307809829712, "training_acc": 53.0, "val_loss": 54.45524454116821, "val_acc": 48.0}
{"epoch": 2, "training_loss": 197.66282606124878, "training_acc": 47.0, "val_loss": 17.557668685913086, "val_acc": 52.0}
{"epoch": 3, "training_loss": 88.31241846084595, "training_acc": 51.0, "val_loss": 45.92757225036621, "val_acc": 52.0}
{"epoch": 4, "training_loss": 168.93235635757446, "training_acc": 53.0, "val_loss": 31.373780965805054, "val_acc": 52.0}
{"epoch": 5, "training_loss": 97.55416440963745, "training_acc": 51.0, "val_loss": 23.82971942424774, "val_acc": 48.0}
{"epoch": 6, "training_loss": 119.9516830444336, "training_acc": 47.0, "val_loss": 31.459161639213562, "val_acc": 48.0}
{"epoch": 7, "training_loss": 111.77058506011963, "training_acc": 47.0, "val_loss": 18.109722435474396, "val_acc": 52.0}
{"epoch": 8, "training_loss": 73.26358318328857, "training_acc": 53.0, "val_loss": 31.423848867416382, "val_acc": 52.0}
{"epoch": 9, "training_loss": 116.84119129180908, "training_acc": 53.0, "val_loss": 22.1851646900177, "val_acc": 52.0}
{"epoch": 10, "training_loss": 80.77109098434448, "training_acc": 58.0, "val_loss": 20.842471718788147, "val_acc": 48.0}
{"epoch": 11, "training_loss": 85.24835705757141, "training_acc": 47.0, "val_loss": 22.29268252849579, "val_acc": 48.0}
{"epoch": 12, "training_loss": 82.96826934814453, "training_acc": 46.0, "val_loss": 17.669551074504852, "val_acc": 52.0}
{"epoch": 13, "training_loss": 77.15408372879028, "training_acc": 54.0, "val_loss": 21.55904322862625, "val_acc": 52.0}
{"epoch": 14, "training_loss": 80.06964087486267, "training_acc": 53.0, "val_loss": 17.220179736614227, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.99227261543274, "training_acc": 60.0, "val_loss": 18.957698345184326, "val_acc": 48.0}
{"epoch": 16, "training_loss": 73.12636089324951, "training_acc": 47.0, "val_loss": 17.285674810409546, "val_acc": 52.0}
{"epoch": 17, "training_loss": 68.4684681892395, "training_acc": 56.0, "val_loss": 19.35214251279831, "val_acc": 52.0}
{"epoch": 18, "training_loss": 73.29480051994324, "training_acc": 53.0, "val_loss": 17.58132129907608, "val_acc": 52.0}
{"epoch": 19, "training_loss": 65.83116269111633, "training_acc": 55.0, "val_loss": 17.241521179676056, "val_acc": 52.0}
{"epoch": 20, "training_loss": 66.30648803710938, "training_acc": 65.0, "val_loss": 17.350146174430847, "val_acc": 56.0}
{"epoch": 21, "training_loss": 67.75541687011719, "training_acc": 62.0, "val_loss": 17.32485145330429, "val_acc": 52.0}
{"epoch": 22, "training_loss": 67.22424840927124, "training_acc": 56.0, "val_loss": 17.94828027486801, "val_acc": 52.0}
{"epoch": 23, "training_loss": 66.45755362510681, "training_acc": 56.0, "val_loss": 17.57020503282547, "val_acc": 52.0}
{"epoch": 24, "training_loss": 66.27411079406738, "training_acc": 59.0, "val_loss": 17.44500696659088, "val_acc": 56.0}
{"epoch": 25, "training_loss": 66.06133246421814, "training_acc": 65.0, "val_loss": 17.437779903411865, "val_acc": 52.0}
{"epoch": 26, "training_loss": 65.84089589118958, "training_acc": 63.0, "val_loss": 18.00820380449295, "val_acc": 52.0}
{"epoch": 27, "training_loss": 66.45119524002075, "training_acc": 58.0, "val_loss": 17.572852969169617, "val_acc": 52.0}
{"epoch": 28, "training_loss": 66.24329113960266, "training_acc": 58.0, "val_loss": 17.612621188163757, "val_acc": 60.0}
{"epoch": 29, "training_loss": 69.23815965652466, "training_acc": 54.0, "val_loss": 17.408040165901184, "val_acc": 52.0}
{"epoch": 30, "training_loss": 68.13999032974243, "training_acc": 55.0, "val_loss": 19.130612909793854, "val_acc": 52.0}
{"epoch": 31, "training_loss": 68.95682954788208, "training_acc": 53.0, "val_loss": 17.383193969726562, "val_acc": 52.0}
{"epoch": 32, "training_loss": 65.33036303520203, "training_acc": 60.0, "val_loss": 17.640873789787292, "val_acc": 64.0}
{"epoch": 33, "training_loss": 67.54839444160461, "training_acc": 56.0, "val_loss": 17.25824326276779, "val_acc": 56.0}
