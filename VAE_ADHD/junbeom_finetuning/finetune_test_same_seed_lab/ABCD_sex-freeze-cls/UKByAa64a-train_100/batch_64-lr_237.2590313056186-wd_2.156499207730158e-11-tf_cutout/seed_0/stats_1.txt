"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 498348.31145095825, "training_acc": 46.0, "val_loss": 104158.16650390625, "val_acc": 52.0}
{"epoch": 1, "training_loss": 505778.0078125, "training_acc": 53.0, "val_loss": 269744.0673828125, "val_acc": 48.0}
{"epoch": 2, "training_loss": 1015184.25390625, "training_acc": 47.0, "val_loss": 74772.66845703125, "val_acc": 48.0}
{"epoch": 3, "training_loss": 353744.32421875, "training_acc": 51.0, "val_loss": 196997.59521484375, "val_acc": 52.0}
{"epoch": 4, "training_loss": 748450.041015625, "training_acc": 53.0, "val_loss": 175786.9384765625, "val_acc": 52.0}
{"epoch": 5, "training_loss": 521722.36328125, "training_acc": 53.0, "val_loss": 30542.669677734375, "val_acc": 48.0}
{"epoch": 6, "training_loss": 281508.619140625, "training_acc": 47.0, "val_loss": 90036.70043945312, "val_acc": 48.0}
{"epoch": 7, "training_loss": 305188.13916015625, "training_acc": 47.0, "val_loss": 65240.53955078125, "val_acc": 52.0}
{"epoch": 8, "training_loss": 254389.8759765625, "training_acc": 53.0, "val_loss": 113124.0478515625, "val_acc": 52.0}
{"epoch": 9, "training_loss": 367330.5146484375, "training_acc": 53.0, "val_loss": 14255.307006835938, "val_acc": 52.0}
{"epoch": 10, "training_loss": 146914.6943359375, "training_acc": 49.0, "val_loss": 105181.72607421875, "val_acc": 48.0}
{"epoch": 11, "training_loss": 403003.8466796875, "training_acc": 47.0, "val_loss": 22311.721801757812, "val_acc": 48.0}
{"epoch": 12, "training_loss": 146016.96484375, "training_acc": 49.0, "val_loss": 110680.35888671875, "val_acc": 52.0}
{"epoch": 13, "training_loss": 425533.876953125, "training_acc": 53.0, "val_loss": 82973.13232421875, "val_acc": 52.0}
{"epoch": 14, "training_loss": 222743.1934814453, "training_acc": 52.0, "val_loss": 81366.85791015625, "val_acc": 48.0}
{"epoch": 15, "training_loss": 396418.111328125, "training_acc": 47.0, "val_loss": 121019.61669921875, "val_acc": 48.0}
{"epoch": 16, "training_loss": 408301.736328125, "training_acc": 47.0, "val_loss": 11022.381591796875, "val_acc": 60.0}
{"epoch": 17, "training_loss": 101631.2783203125, "training_acc": 54.0, "val_loss": 69156.68334960938, "val_acc": 52.0}
{"epoch": 18, "training_loss": 222306.30590820312, "training_acc": 53.0, "val_loss": 14935.743713378906, "val_acc": 48.0}
{"epoch": 19, "training_loss": 67542.11376953125, "training_acc": 51.0, "val_loss": 6961.5478515625, "val_acc": 56.0}
{"epoch": 20, "training_loss": 54475.2666015625, "training_acc": 58.0, "val_loss": 17211.976623535156, "val_acc": 56.0}
{"epoch": 21, "training_loss": 57524.7529296875, "training_acc": 52.0, "val_loss": 25560.890197753906, "val_acc": 48.0}
{"epoch": 22, "training_loss": 76724.91857910156, "training_acc": 52.0, "val_loss": 32059.088134765625, "val_acc": 52.0}
{"epoch": 23, "training_loss": 68451.17639160156, "training_acc": 58.0, "val_loss": 22887.179565429688, "val_acc": 48.0}
{"epoch": 24, "training_loss": 86210.96337890625, "training_acc": 49.0, "val_loss": 25439.878845214844, "val_acc": 52.0}
{"epoch": 25, "training_loss": 83666.4697265625, "training_acc": 56.0, "val_loss": 9680.171966552734, "val_acc": 56.0}
{"epoch": 26, "training_loss": 83312.134765625, "training_acc": 54.0, "val_loss": 26632.400512695312, "val_acc": 48.0}
{"epoch": 27, "training_loss": 95694.57666015625, "training_acc": 50.0, "val_loss": 28326.876831054688, "val_acc": 52.0}
{"epoch": 28, "training_loss": 59621.24822998047, "training_acc": 57.0, "val_loss": 22594.43359375, "val_acc": 48.0}
{"epoch": 29, "training_loss": 69107.02880859375, "training_acc": 52.0, "val_loss": 47055.3466796875, "val_acc": 52.0}
{"epoch": 30, "training_loss": 173050.54833984375, "training_acc": 53.0, "val_loss": 36216.44592285156, "val_acc": 52.0}
{"epoch": 31, "training_loss": 109095.8837890625, "training_acc": 49.0, "val_loss": 28439.566040039062, "val_acc": 48.0}
{"epoch": 32, "training_loss": 84494.01971435547, "training_acc": 50.0, "val_loss": 17757.8369140625, "val_acc": 56.0}
{"epoch": 33, "training_loss": 41558.806884765625, "training_acc": 60.0, "val_loss": 13240.214538574219, "val_acc": 48.0}
{"epoch": 34, "training_loss": 74713.22265625, "training_acc": 42.0, "val_loss": 8976.75552368164, "val_acc": 64.0}
{"epoch": 35, "training_loss": 50762.80029296875, "training_acc": 55.0, "val_loss": 4983.116149902344, "val_acc": 48.0}
{"epoch": 36, "training_loss": 56255.30224609375, "training_acc": 58.0, "val_loss": 32010.018920898438, "val_acc": 52.0}
{"epoch": 37, "training_loss": 67983.08459472656, "training_acc": 54.0, "val_loss": 11287.002563476562, "val_acc": 44.0}
{"epoch": 38, "training_loss": 53766.402099609375, "training_acc": 53.0, "val_loss": 19045.953369140625, "val_acc": 56.0}
{"epoch": 39, "training_loss": 56150.918701171875, "training_acc": 53.0, "val_loss": 4214.634323120117, "val_acc": 48.0}
{"epoch": 40, "training_loss": 50195.017333984375, "training_acc": 60.0, "val_loss": 35295.562744140625, "val_acc": 52.0}
{"epoch": 41, "training_loss": 79383.98477172852, "training_acc": 59.0, "val_loss": 28228.024291992188, "val_acc": 48.0}
{"epoch": 42, "training_loss": 86356.09204101562, "training_acc": 47.0, "val_loss": 42421.92077636719, "val_acc": 52.0}
{"epoch": 43, "training_loss": 179872.0634765625, "training_acc": 53.0, "val_loss": 35624.79553222656, "val_acc": 52.0}
{"epoch": 44, "training_loss": 98568.9267578125, "training_acc": 55.0, "val_loss": 37780.218505859375, "val_acc": 48.0}
{"epoch": 45, "training_loss": 100751.32421875, "training_acc": 49.0, "val_loss": 48647.39685058594, "val_acc": 52.0}
{"epoch": 46, "training_loss": 219805.9404296875, "training_acc": 53.0, "val_loss": 53702.63671875, "val_acc": 52.0}
{"epoch": 47, "training_loss": 134438.83966064453, "training_acc": 56.0, "val_loss": 36234.44519042969, "val_acc": 48.0}
{"epoch": 48, "training_loss": 129635.15673828125, "training_acc": 47.0, "val_loss": 19317.96875, "val_acc": 52.0}
{"epoch": 49, "training_loss": 73201.24682617188, "training_acc": 53.0, "val_loss": 6407.964324951172, "val_acc": 64.0}
{"epoch": 50, "training_loss": 74385.5400390625, "training_acc": 50.0, "val_loss": 27115.573120117188, "val_acc": 48.0}
{"epoch": 51, "training_loss": 87681.3896484375, "training_acc": 50.0, "val_loss": 28426.358032226562, "val_acc": 52.0}
{"epoch": 52, "training_loss": 61625.927307128906, "training_acc": 65.0, "val_loss": 19484.202575683594, "val_acc": 48.0}
{"epoch": 53, "training_loss": 58509.084045410156, "training_acc": 50.0, "val_loss": 32168.588256835938, "val_acc": 52.0}
{"epoch": 54, "training_loss": 98674.57153320312, "training_acc": 53.0, "val_loss": 8374.419403076172, "val_acc": 44.0}
{"epoch": 55, "training_loss": 46967.515380859375, "training_acc": 54.0, "val_loss": 17642.874145507812, "val_acc": 56.0}
{"epoch": 56, "training_loss": 40948.0185546875, "training_acc": 53.0, "val_loss": 22415.12451171875, "val_acc": 48.0}
{"epoch": 57, "training_loss": 74581.87329101562, "training_acc": 49.0, "val_loss": 30853.115844726562, "val_acc": 52.0}
{"epoch": 58, "training_loss": 107451.0322265625, "training_acc": 53.0, "val_loss": 5034.521865844727, "val_acc": 64.0}
