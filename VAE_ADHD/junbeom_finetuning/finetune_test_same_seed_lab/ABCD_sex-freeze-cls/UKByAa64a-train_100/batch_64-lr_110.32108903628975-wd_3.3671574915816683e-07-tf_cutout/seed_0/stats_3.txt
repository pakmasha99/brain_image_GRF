"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 893557.6605529785, "training_acc": 51.0, "val_loss": 185514.794921875, "val_acc": 52.0}
{"epoch": 1, "training_loss": 935360.44140625, "training_acc": 49.0, "val_loss": 401745.5322265625, "val_acc": 48.0}
{"epoch": 2, "training_loss": 1501248.89453125, "training_acc": 47.0, "val_loss": 97222.8271484375, "val_acc": 48.0}
{"epoch": 3, "training_loss": 570645.44921875, "training_acc": 45.0, "val_loss": 315677.392578125, "val_acc": 52.0}
{"epoch": 4, "training_loss": 1260399.1328125, "training_acc": 53.0, "val_loss": 266134.130859375, "val_acc": 52.0}
{"epoch": 5, "training_loss": 848293.08984375, "training_acc": 53.0, "val_loss": 69372.10083007812, "val_acc": 48.0}
{"epoch": 6, "training_loss": 405814.525390625, "training_acc": 47.0, "val_loss": 156081.21337890625, "val_acc": 48.0}
{"epoch": 7, "training_loss": 515604.58984375, "training_acc": 47.0, "val_loss": 75621.13647460938, "val_acc": 52.0}
{"epoch": 8, "training_loss": 343481.0458984375, "training_acc": 53.0, "val_loss": 122420.05615234375, "val_acc": 52.0}
{"epoch": 9, "training_loss": 397405.07177734375, "training_acc": 53.0, "val_loss": 65096.56982421875, "val_acc": 48.0}
{"epoch": 10, "training_loss": 308799.814453125, "training_acc": 47.0, "val_loss": 56633.544921875, "val_acc": 48.0}
{"epoch": 11, "training_loss": 272273.2607421875, "training_acc": 39.0, "val_loss": 77676.98364257812, "val_acc": 52.0}
{"epoch": 12, "training_loss": 249613.1103515625, "training_acc": 53.0, "val_loss": 53986.3037109375, "val_acc": 48.0}
{"epoch": 13, "training_loss": 266705.1708984375, "training_acc": 47.0, "val_loss": 29004.5654296875, "val_acc": 48.0}
{"epoch": 14, "training_loss": 165735.1611328125, "training_acc": 53.0, "val_loss": 119150.732421875, "val_acc": 52.0}
{"epoch": 15, "training_loss": 442317.72265625, "training_acc": 53.0, "val_loss": 34097.039794921875, "val_acc": 52.0}
{"epoch": 16, "training_loss": 240566.39453125, "training_acc": 47.0, "val_loss": 137001.5380859375, "val_acc": 48.0}
{"epoch": 17, "training_loss": 517374.8427734375, "training_acc": 47.0, "val_loss": 19761.216735839844, "val_acc": 48.0}
{"epoch": 18, "training_loss": 282304.796875, "training_acc": 39.0, "val_loss": 178623.22998046875, "val_acc": 52.0}
{"epoch": 19, "training_loss": 692265.537109375, "training_acc": 53.0, "val_loss": 121432.48291015625, "val_acc": 52.0}
{"epoch": 20, "training_loss": 330102.1745605469, "training_acc": 53.0, "val_loss": 134109.1796875, "val_acc": 48.0}
{"epoch": 21, "training_loss": 640222.78515625, "training_acc": 47.0, "val_loss": 212559.9609375, "val_acc": 48.0}
{"epoch": 22, "training_loss": 779867.66015625, "training_acc": 47.0, "val_loss": 38144.65637207031, "val_acc": 48.0}
{"epoch": 23, "training_loss": 241484.814453125, "training_acc": 53.0, "val_loss": 210982.3974609375, "val_acc": 52.0}
{"epoch": 24, "training_loss": 856212.6796875, "training_acc": 53.0, "val_loss": 215284.423828125, "val_acc": 52.0}
{"epoch": 25, "training_loss": 736116.658203125, "training_acc": 53.0, "val_loss": 23218.25714111328, "val_acc": 52.0}
{"epoch": 26, "training_loss": 257485.24609375, "training_acc": 53.0, "val_loss": 239908.0810546875, "val_acc": 48.0}
{"epoch": 27, "training_loss": 998199.1875, "training_acc": 47.0, "val_loss": 211296.6796875, "val_acc": 48.0}
{"epoch": 28, "training_loss": 694995.818359375, "training_acc": 47.0, "val_loss": 46099.810791015625, "val_acc": 52.0}
{"epoch": 29, "training_loss": 302802.66796875, "training_acc": 53.0, "val_loss": 150180.9814453125, "val_acc": 52.0}
{"epoch": 30, "training_loss": 534298.767578125, "training_acc": 53.0, "val_loss": 31038.720703125, "val_acc": 52.0}
{"epoch": 31, "training_loss": 271423.642578125, "training_acc": 45.0, "val_loss": 167505.8837890625, "val_acc": 48.0}
{"epoch": 32, "training_loss": 663158.3515625, "training_acc": 47.0, "val_loss": 76900.5126953125, "val_acc": 48.0}
{"epoch": 33, "training_loss": 249244.9912109375, "training_acc": 53.0, "val_loss": 109826.123046875, "val_acc": 52.0}
{"epoch": 34, "training_loss": 427828.998046875, "training_acc": 53.0, "val_loss": 61549.664306640625, "val_acc": 52.0}
{"epoch": 35, "training_loss": 159072.49291992188, "training_acc": 61.0, "val_loss": 71106.78100585938, "val_acc": 48.0}
{"epoch": 36, "training_loss": 249207.65869140625, "training_acc": 47.0, "val_loss": 44199.853515625, "val_acc": 52.0}
