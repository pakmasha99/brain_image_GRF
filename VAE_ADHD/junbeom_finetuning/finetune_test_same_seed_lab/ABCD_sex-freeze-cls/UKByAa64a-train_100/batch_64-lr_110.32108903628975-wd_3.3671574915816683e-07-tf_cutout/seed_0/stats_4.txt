"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 348767.44591903687, "training_acc": 44.0, "val_loss": 218440.087890625, "val_acc": 52.0}
{"epoch": 1, "training_loss": 893159.98046875, "training_acc": 53.0, "val_loss": 88174.560546875, "val_acc": 52.0}
{"epoch": 2, "training_loss": 629941.44140625, "training_acc": 41.0, "val_loss": 244705.76171875, "val_acc": 48.0}
{"epoch": 3, "training_loss": 875715.220703125, "training_acc": 47.0, "val_loss": 24570.53680419922, "val_acc": 52.0}
{"epoch": 4, "training_loss": 169652.2822265625, "training_acc": 53.0, "val_loss": 36586.260986328125, "val_acc": 52.0}
{"epoch": 5, "training_loss": 260552.974609375, "training_acc": 43.0, "val_loss": 92655.810546875, "val_acc": 48.0}
{"epoch": 6, "training_loss": 257115.09790039062, "training_acc": 47.0, "val_loss": 128377.69775390625, "val_acc": 52.0}
{"epoch": 7, "training_loss": 586731.53515625, "training_acc": 53.0, "val_loss": 173854.77294921875, "val_acc": 52.0}
{"epoch": 8, "training_loss": 574233.734375, "training_acc": 53.0, "val_loss": 27073.703002929688, "val_acc": 48.0}
{"epoch": 9, "training_loss": 188696.703125, "training_acc": 47.0, "val_loss": 51124.420166015625, "val_acc": 48.0}
{"epoch": 10, "training_loss": 160963.9609375, "training_acc": 55.0, "val_loss": 62918.17626953125, "val_acc": 52.0}
{"epoch": 11, "training_loss": 202263.07666015625, "training_acc": 53.0, "val_loss": 61587.310791015625, "val_acc": 48.0}
{"epoch": 12, "training_loss": 267771.6591796875, "training_acc": 47.0, "val_loss": 24274.51171875, "val_acc": 48.0}
{"epoch": 13, "training_loss": 178167.2333984375, "training_acc": 49.0, "val_loss": 121348.49853515625, "val_acc": 52.0}
{"epoch": 14, "training_loss": 448772.96875, "training_acc": 53.0, "val_loss": 27725.225830078125, "val_acc": 52.0}
{"epoch": 15, "training_loss": 226115.06640625, "training_acc": 49.0, "val_loss": 150364.9658203125, "val_acc": 48.0}
{"epoch": 16, "training_loss": 578941.3046875, "training_acc": 47.0, "val_loss": 41676.708984375, "val_acc": 48.0}
{"epoch": 17, "training_loss": 229715.9853515625, "training_acc": 51.0, "val_loss": 160102.03857421875, "val_acc": 52.0}
{"epoch": 18, "training_loss": 648299.78125, "training_acc": 53.0, "val_loss": 120485.9130859375, "val_acc": 52.0}
{"epoch": 19, "training_loss": 340968.8137207031, "training_acc": 53.0, "val_loss": 130805.79833984375, "val_acc": 48.0}
{"epoch": 20, "training_loss": 644157.140625, "training_acc": 47.0, "val_loss": 202636.07177734375, "val_acc": 48.0}
{"epoch": 21, "training_loss": 724368.423828125, "training_acc": 47.0, "val_loss": 10916.595458984375, "val_acc": 48.0}
{"epoch": 22, "training_loss": 179626.2578125, "training_acc": 57.0, "val_loss": 246483.49609375, "val_acc": 52.0}
{"epoch": 23, "training_loss": 1037574.04296875, "training_acc": 53.0, "val_loss": 264632.421875, "val_acc": 52.0}
{"epoch": 24, "training_loss": 925637.05078125, "training_acc": 53.0, "val_loss": 68107.9345703125, "val_acc": 52.0}
{"epoch": 25, "training_loss": 291239.9384765625, "training_acc": 55.0, "val_loss": 200607.45849609375, "val_acc": 48.0}
{"epoch": 26, "training_loss": 833437.763671875, "training_acc": 47.0, "val_loss": 181020.21484375, "val_acc": 48.0}
{"epoch": 27, "training_loss": 586488.7861328125, "training_acc": 47.0, "val_loss": 62818.46923828125, "val_acc": 52.0}
{"epoch": 28, "training_loss": 345588.517578125, "training_acc": 53.0, "val_loss": 151793.15185546875, "val_acc": 52.0}
{"epoch": 29, "training_loss": 541228.640625, "training_acc": 53.0, "val_loss": 25485.595703125, "val_acc": 52.0}
{"epoch": 30, "training_loss": 245286.267578125, "training_acc": 51.0, "val_loss": 197117.46826171875, "val_acc": 48.0}
{"epoch": 31, "training_loss": 805861.36328125, "training_acc": 47.0, "val_loss": 134012.34130859375, "val_acc": 48.0}
{"epoch": 32, "training_loss": 367189.4879760742, "training_acc": 47.0, "val_loss": 154762.2802734375, "val_acc": 52.0}
{"epoch": 33, "training_loss": 691518.46875, "training_acc": 53.0, "val_loss": 277133.0810546875, "val_acc": 52.0}
{"epoch": 34, "training_loss": 1058446.62890625, "training_acc": 53.0, "val_loss": 188235.99853515625, "val_acc": 52.0}
{"epoch": 35, "training_loss": 567771.73828125, "training_acc": 53.0, "val_loss": 82802.28881835938, "val_acc": 48.0}
{"epoch": 36, "training_loss": 443142.447265625, "training_acc": 47.0, "val_loss": 184791.8212890625, "val_acc": 48.0}
{"epoch": 37, "training_loss": 680327.435546875, "training_acc": 47.0, "val_loss": 27047.47314453125, "val_acc": 48.0}
{"epoch": 38, "training_loss": 302548.0859375, "training_acc": 41.0, "val_loss": 200813.58642578125, "val_acc": 52.0}
{"epoch": 39, "training_loss": 833114.8984375, "training_acc": 53.0, "val_loss": 173551.1962890625, "val_acc": 52.0}
{"epoch": 40, "training_loss": 536503.54296875, "training_acc": 53.0, "val_loss": 70517.99926757812, "val_acc": 48.0}
