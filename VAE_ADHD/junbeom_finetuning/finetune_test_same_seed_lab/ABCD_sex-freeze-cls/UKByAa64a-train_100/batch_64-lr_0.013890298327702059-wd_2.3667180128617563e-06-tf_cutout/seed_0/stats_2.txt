"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 76.29141569137573, "training_acc": 53.0, "val_loss": 17.479953169822693, "val_acc": 52.0}
{"epoch": 1, "training_loss": 73.21403431892395, "training_acc": 50.0, "val_loss": 20.10897845029831, "val_acc": 48.0}
{"epoch": 2, "training_loss": 75.89298915863037, "training_acc": 47.0, "val_loss": 17.59454756975174, "val_acc": 52.0}
{"epoch": 3, "training_loss": 73.71753311157227, "training_acc": 53.0, "val_loss": 19.75008100271225, "val_acc": 52.0}
{"epoch": 4, "training_loss": 76.55184578895569, "training_acc": 53.0, "val_loss": 17.544327676296234, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.84084105491638, "training_acc": 49.0, "val_loss": 17.92566478252411, "val_acc": 52.0}
{"epoch": 6, "training_loss": 71.5284960269928, "training_acc": 47.0, "val_loss": 17.796197533607483, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.73960614204407, "training_acc": 48.0, "val_loss": 17.246930301189423, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.62609815597534, "training_acc": 53.0, "val_loss": 18.055452406406403, "val_acc": 52.0}
{"epoch": 9, "training_loss": 71.40549802780151, "training_acc": 53.0, "val_loss": 17.481784522533417, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.11341834068298, "training_acc": 55.0, "val_loss": 17.36951768398285, "val_acc": 52.0}
{"epoch": 11, "training_loss": 68.1352117061615, "training_acc": 54.0, "val_loss": 17.598263919353485, "val_acc": 52.0}
{"epoch": 12, "training_loss": 70.04061436653137, "training_acc": 47.0, "val_loss": 17.298991978168488, "val_acc": 52.0}
{"epoch": 13, "training_loss": 67.90231966972351, "training_acc": 59.0, "val_loss": 17.33257621526718, "val_acc": 52.0}
{"epoch": 14, "training_loss": 68.36901116371155, "training_acc": 54.0, "val_loss": 17.341248691082, "val_acc": 52.0}
{"epoch": 15, "training_loss": 68.68646550178528, "training_acc": 54.0, "val_loss": 17.286302149295807, "val_acc": 52.0}
{"epoch": 16, "training_loss": 67.29016613960266, "training_acc": 64.0, "val_loss": 17.31140911579132, "val_acc": 52.0}
{"epoch": 17, "training_loss": 67.47991180419922, "training_acc": 59.0, "val_loss": 17.23088026046753, "val_acc": 52.0}
{"epoch": 18, "training_loss": 67.13315320014954, "training_acc": 67.0, "val_loss": 17.21320152282715, "val_acc": 52.0}
{"epoch": 19, "training_loss": 67.68705797195435, "training_acc": 55.0, "val_loss": 17.22906082868576, "val_acc": 52.0}
{"epoch": 20, "training_loss": 67.72942423820496, "training_acc": 54.0, "val_loss": 17.210914194583893, "val_acc": 52.0}
{"epoch": 21, "training_loss": 66.7940423488617, "training_acc": 61.0, "val_loss": 17.23332107067108, "val_acc": 52.0}
{"epoch": 22, "training_loss": 67.78178834915161, "training_acc": 55.0, "val_loss": 17.27166771888733, "val_acc": 52.0}
{"epoch": 23, "training_loss": 67.15928363800049, "training_acc": 63.0, "val_loss": 17.29249507188797, "val_acc": 52.0}
{"epoch": 24, "training_loss": 67.00228238105774, "training_acc": 59.0, "val_loss": 17.326097190380096, "val_acc": 52.0}
{"epoch": 25, "training_loss": 64.59373116493225, "training_acc": 70.0, "val_loss": 17.376819252967834, "val_acc": 52.0}
{"epoch": 26, "training_loss": 68.76172709465027, "training_acc": 61.0, "val_loss": 17.481324076652527, "val_acc": 52.0}
{"epoch": 27, "training_loss": 66.89634013175964, "training_acc": 63.0, "val_loss": 17.52248704433441, "val_acc": 52.0}
{"epoch": 28, "training_loss": 65.97804260253906, "training_acc": 62.0, "val_loss": 17.32006072998047, "val_acc": 52.0}
{"epoch": 29, "training_loss": 66.47104692459106, "training_acc": 62.0, "val_loss": 17.43122637271881, "val_acc": 52.0}
{"epoch": 30, "training_loss": 67.92119789123535, "training_acc": 55.0, "val_loss": 17.415648698806763, "val_acc": 52.0}
{"epoch": 31, "training_loss": 66.77540326118469, "training_acc": 58.0, "val_loss": 17.329546809196472, "val_acc": 52.0}
{"epoch": 32, "training_loss": 68.47157907485962, "training_acc": 55.0, "val_loss": 17.50759482383728, "val_acc": 52.0}
{"epoch": 33, "training_loss": 66.98198437690735, "training_acc": 60.0, "val_loss": 17.31707602739334, "val_acc": 52.0}
{"epoch": 34, "training_loss": 68.15614008903503, "training_acc": 53.0, "val_loss": 17.830607295036316, "val_acc": 52.0}
{"epoch": 35, "training_loss": 66.9308774471283, "training_acc": 54.0, "val_loss": 17.115437984466553, "val_acc": 52.0}
{"epoch": 36, "training_loss": 66.18745684623718, "training_acc": 63.0, "val_loss": 17.613959312438965, "val_acc": 52.0}
{"epoch": 37, "training_loss": 68.68379354476929, "training_acc": 50.0, "val_loss": 17.065581679344177, "val_acc": 52.0}
{"epoch": 38, "training_loss": 65.83298993110657, "training_acc": 67.0, "val_loss": 18.6822310090065, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.21992039680481, "training_acc": 53.0, "val_loss": 18.08111220598221, "val_acc": 52.0}
{"epoch": 40, "training_loss": 67.82088804244995, "training_acc": 53.0, "val_loss": 16.96513742208481, "val_acc": 52.0}
{"epoch": 41, "training_loss": 68.14370465278625, "training_acc": 59.0, "val_loss": 17.07378327846527, "val_acc": 52.0}
{"epoch": 42, "training_loss": 67.26602435112, "training_acc": 56.0, "val_loss": 17.06397235393524, "val_acc": 52.0}
{"epoch": 43, "training_loss": 66.67848563194275, "training_acc": 57.0, "val_loss": 17.17909276485443, "val_acc": 52.0}
{"epoch": 44, "training_loss": 64.87691497802734, "training_acc": 60.0, "val_loss": 16.97421669960022, "val_acc": 52.0}
{"epoch": 45, "training_loss": 63.5407829284668, "training_acc": 77.0, "val_loss": 16.97290390729904, "val_acc": 52.0}
{"epoch": 46, "training_loss": 65.27403211593628, "training_acc": 68.0, "val_loss": 16.97283387184143, "val_acc": 52.0}
{"epoch": 47, "training_loss": 65.36129093170166, "training_acc": 69.0, "val_loss": 17.05583930015564, "val_acc": 52.0}
{"epoch": 48, "training_loss": 65.23094964027405, "training_acc": 60.0, "val_loss": 17.027828097343445, "val_acc": 52.0}
{"epoch": 49, "training_loss": 65.38734221458435, "training_acc": 56.0, "val_loss": 16.996818780899048, "val_acc": 52.0}
{"epoch": 50, "training_loss": 66.51753091812134, "training_acc": 62.0, "val_loss": 17.014870047569275, "val_acc": 52.0}
{"epoch": 51, "training_loss": 65.66123580932617, "training_acc": 62.0, "val_loss": 17.018400132656097, "val_acc": 52.0}
{"epoch": 52, "training_loss": 64.41032457351685, "training_acc": 68.0, "val_loss": 17.052462697029114, "val_acc": 52.0}
{"epoch": 53, "training_loss": 65.79339146614075, "training_acc": 65.0, "val_loss": 17.077626287937164, "val_acc": 52.0}
{"epoch": 54, "training_loss": 64.49232864379883, "training_acc": 67.0, "val_loss": 17.055633664131165, "val_acc": 52.0}
{"epoch": 55, "training_loss": 65.42681455612183, "training_acc": 66.0, "val_loss": 17.07586944103241, "val_acc": 52.0}
{"epoch": 56, "training_loss": 64.30248379707336, "training_acc": 69.0, "val_loss": 17.121095955371857, "val_acc": 52.0}
{"epoch": 57, "training_loss": 66.27669930458069, "training_acc": 68.0, "val_loss": 17.15552657842636, "val_acc": 52.0}
{"epoch": 58, "training_loss": 64.45842337608337, "training_acc": 63.0, "val_loss": 17.41243004798889, "val_acc": 52.0}
{"epoch": 59, "training_loss": 65.61597037315369, "training_acc": 54.0, "val_loss": 17.29625314474106, "val_acc": 52.0}
