"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 207738.5874862671, "training_acc": 45.0, "val_loss": 29303.106689453125, "val_acc": 48.0}
{"epoch": 1, "training_loss": 201172.6298828125, "training_acc": 45.0, "val_loss": 111490.14892578125, "val_acc": 52.0}
{"epoch": 2, "training_loss": 411351.064453125, "training_acc": 53.0, "val_loss": 76968.85986328125, "val_acc": 52.0}
{"epoch": 3, "training_loss": 228079.70947265625, "training_acc": 53.0, "val_loss": 32615.643310546875, "val_acc": 48.0}
{"epoch": 4, "training_loss": 186613.408203125, "training_acc": 47.0, "val_loss": 65476.446533203125, "val_acc": 48.0}
{"epoch": 5, "training_loss": 235725.82666015625, "training_acc": 47.0, "val_loss": 7352.909851074219, "val_acc": 48.0}
{"epoch": 6, "training_loss": 60709.4609375, "training_acc": 52.0, "val_loss": 60861.41357421875, "val_acc": 52.0}
{"epoch": 7, "training_loss": 244374.7392578125, "training_acc": 53.0, "val_loss": 58146.42333984375, "val_acc": 52.0}
{"epoch": 8, "training_loss": 188394.69580078125, "training_acc": 53.0, "val_loss": 1485.595703125, "val_acc": 52.0}
{"epoch": 9, "training_loss": 36679.951904296875, "training_acc": 51.0, "val_loss": 28536.2548828125, "val_acc": 48.0}
{"epoch": 10, "training_loss": 96206.40161132812, "training_acc": 47.0, "val_loss": 10786.053466796875, "val_acc": 52.0}
{"epoch": 11, "training_loss": 45065.509521484375, "training_acc": 53.0, "val_loss": 16088.018798828125, "val_acc": 52.0}
{"epoch": 12, "training_loss": 36409.41069030762, "training_acc": 58.0, "val_loss": 9570.072937011719, "val_acc": 48.0}
{"epoch": 13, "training_loss": 31123.684112548828, "training_acc": 50.0, "val_loss": 14807.244873046875, "val_acc": 52.0}
{"epoch": 14, "training_loss": 43531.05993652344, "training_acc": 53.0, "val_loss": 4347.539138793945, "val_acc": 52.0}
{"epoch": 15, "training_loss": 22830.169921875, "training_acc": 57.0, "val_loss": 5963.738250732422, "val_acc": 48.0}
{"epoch": 16, "training_loss": 42178.324951171875, "training_acc": 38.0, "val_loss": 16994.20928955078, "val_acc": 52.0}
{"epoch": 17, "training_loss": 34472.59419250488, "training_acc": 54.0, "val_loss": 6603.556060791016, "val_acc": 48.0}
{"epoch": 18, "training_loss": 28656.8994140625, "training_acc": 52.0, "val_loss": 11174.443054199219, "val_acc": 52.0}
{"epoch": 19, "training_loss": 20539.960571289062, "training_acc": 57.0, "val_loss": 3832.1094512939453, "val_acc": 52.0}
{"epoch": 20, "training_loss": 16696.404998779297, "training_acc": 52.0, "val_loss": 10659.697723388672, "val_acc": 52.0}
{"epoch": 21, "training_loss": 21599.008850097656, "training_acc": 54.0, "val_loss": 8149.2340087890625, "val_acc": 48.0}
{"epoch": 22, "training_loss": 37340.85217285156, "training_acc": 47.0, "val_loss": 7205.590057373047, "val_acc": 52.0}
{"epoch": 23, "training_loss": 25351.402709960938, "training_acc": 50.0, "val_loss": 1473.0403900146484, "val_acc": 52.0}
{"epoch": 24, "training_loss": 17606.120727539062, "training_acc": 62.0, "val_loss": 3235.862350463867, "val_acc": 48.0}
{"epoch": 25, "training_loss": 26386.333374023438, "training_acc": 46.0, "val_loss": 14489.707946777344, "val_acc": 52.0}
{"epoch": 26, "training_loss": 31851.378448486328, "training_acc": 61.0, "val_loss": 9390.154266357422, "val_acc": 48.0}
{"epoch": 27, "training_loss": 34488.31414794922, "training_acc": 47.0, "val_loss": 13542.515563964844, "val_acc": 52.0}
{"epoch": 28, "training_loss": 41435.814453125, "training_acc": 53.0, "val_loss": 7043.526458740234, "val_acc": 52.0}
{"epoch": 29, "training_loss": 29253.160888671875, "training_acc": 51.0, "val_loss": 14865.525817871094, "val_acc": 48.0}
{"epoch": 30, "training_loss": 47299.64669799805, "training_acc": 52.0, "val_loss": 17543.521118164062, "val_acc": 52.0}
{"epoch": 31, "training_loss": 51465.388916015625, "training_acc": 53.0, "val_loss": 9896.19140625, "val_acc": 52.0}
{"epoch": 32, "training_loss": 35157.434814453125, "training_acc": 51.0, "val_loss": 14313.381958007812, "val_acc": 48.0}
{"epoch": 33, "training_loss": 45545.698638916016, "training_acc": 51.0, "val_loss": 19929.339599609375, "val_acc": 52.0}
{"epoch": 34, "training_loss": 63636.39599609375, "training_acc": 53.0, "val_loss": 15870.358276367188, "val_acc": 52.0}
{"epoch": 35, "training_loss": 29732.226684570312, "training_acc": 59.0, "val_loss": 9840.579986572266, "val_acc": 48.0}
{"epoch": 36, "training_loss": 32652.52359008789, "training_acc": 52.0, "val_loss": 14818.515014648438, "val_acc": 52.0}
{"epoch": 37, "training_loss": 47873.7421875, "training_acc": 53.0, "val_loss": 3644.1123962402344, "val_acc": 48.0}
{"epoch": 38, "training_loss": 21549.910034179688, "training_acc": 68.0, "val_loss": 19369.764709472656, "val_acc": 48.0}
{"epoch": 39, "training_loss": 59079.613525390625, "training_acc": 50.0, "val_loss": 14678.582763671875, "val_acc": 52.0}
{"epoch": 40, "training_loss": 57316.26123046875, "training_acc": 53.0, "val_loss": 15969.488525390625, "val_acc": 52.0}
{"epoch": 41, "training_loss": 41003.47979736328, "training_acc": 46.0, "val_loss": 7069.939422607422, "val_acc": 48.0}
{"epoch": 42, "training_loss": 20523.830200195312, "training_acc": 62.0, "val_loss": 10080.7861328125, "val_acc": 52.0}
