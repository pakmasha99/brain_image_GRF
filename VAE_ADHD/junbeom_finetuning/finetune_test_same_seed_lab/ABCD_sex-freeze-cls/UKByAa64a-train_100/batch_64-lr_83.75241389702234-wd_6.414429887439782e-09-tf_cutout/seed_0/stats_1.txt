"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 175945.61809158325, "training_acc": 46.0, "val_loss": 36767.66357421875, "val_acc": 52.0}
{"epoch": 1, "training_loss": 178539.7138671875, "training_acc": 53.0, "val_loss": 95219.76318359375, "val_acc": 48.0}
{"epoch": 2, "training_loss": 358360.11328125, "training_acc": 47.0, "val_loss": 26394.793701171875, "val_acc": 48.0}
{"epoch": 3, "training_loss": 124871.6103515625, "training_acc": 51.0, "val_loss": 69540.05126953125, "val_acc": 52.0}
{"epoch": 4, "training_loss": 264202.857421875, "training_acc": 53.0, "val_loss": 62052.618408203125, "val_acc": 52.0}
{"epoch": 5, "training_loss": 184167.6708984375, "training_acc": 53.0, "val_loss": 10781.763458251953, "val_acc": 48.0}
{"epoch": 6, "training_loss": 99373.025390625, "training_acc": 47.0, "val_loss": 31783.126831054688, "val_acc": 48.0}
{"epoch": 7, "training_loss": 107731.59692382812, "training_acc": 47.0, "val_loss": 23029.74395751953, "val_acc": 52.0}
{"epoch": 8, "training_loss": 89799.38037109375, "training_acc": 53.0, "val_loss": 39932.58056640625, "val_acc": 52.0}
{"epoch": 9, "training_loss": 129667.32446289062, "training_acc": 53.0, "val_loss": 5031.891632080078, "val_acc": 52.0}
{"epoch": 10, "training_loss": 51860.7333984375, "training_acc": 49.0, "val_loss": 37129.37316894531, "val_acc": 48.0}
{"epoch": 11, "training_loss": 142260.78466796875, "training_acc": 47.0, "val_loss": 7876.2451171875, "val_acc": 48.0}
{"epoch": 12, "training_loss": 51544.29443359375, "training_acc": 49.0, "val_loss": 39069.927978515625, "val_acc": 52.0}
{"epoch": 13, "training_loss": 150213.0478515625, "training_acc": 53.0, "val_loss": 29289.230346679688, "val_acc": 52.0}
{"epoch": 14, "training_loss": 78627.78686523438, "training_acc": 52.0, "val_loss": 28726.8310546875, "val_acc": 48.0}
{"epoch": 15, "training_loss": 139958.07861328125, "training_acc": 47.0, "val_loss": 42731.341552734375, "val_acc": 48.0}
{"epoch": 16, "training_loss": 144179.30493164062, "training_acc": 47.0, "val_loss": 3877.6611328125, "val_acc": 60.0}
{"epoch": 17, "training_loss": 36286.93798828125, "training_acc": 55.0, "val_loss": 25030.55419921875, "val_acc": 52.0}
{"epoch": 18, "training_loss": 81016.68627929688, "training_acc": 53.0, "val_loss": 4124.727630615234, "val_acc": 48.0}
{"epoch": 19, "training_loss": 21535.678955078125, "training_acc": 55.0, "val_loss": 1882.4758529663086, "val_acc": 48.0}
{"epoch": 20, "training_loss": 24057.41162109375, "training_acc": 55.0, "val_loss": 10656.333923339844, "val_acc": 52.0}
{"epoch": 21, "training_loss": 23737.65609741211, "training_acc": 56.0, "val_loss": 5742.984771728516, "val_acc": 48.0}
{"epoch": 22, "training_loss": 18908.631805419922, "training_acc": 53.0, "val_loss": 8820.884704589844, "val_acc": 52.0}
{"epoch": 23, "training_loss": 16652.745346069336, "training_acc": 62.0, "val_loss": 9826.405334472656, "val_acc": 48.0}
{"epoch": 24, "training_loss": 33776.60400390625, "training_acc": 49.0, "val_loss": 10315.266418457031, "val_acc": 52.0}
{"epoch": 25, "training_loss": 35655.49035644531, "training_acc": 55.0, "val_loss": 4124.117279052734, "val_acc": 60.0}
{"epoch": 26, "training_loss": 33230.790771484375, "training_acc": 50.0, "val_loss": 12942.4560546875, "val_acc": 48.0}
{"epoch": 27, "training_loss": 36084.77391052246, "training_acc": 54.0, "val_loss": 7845.00732421875, "val_acc": 56.0}
{"epoch": 28, "training_loss": 17466.297943115234, "training_acc": 54.0, "val_loss": 9644.00863647461, "val_acc": 48.0}
{"epoch": 29, "training_loss": 33911.475830078125, "training_acc": 49.0, "val_loss": 13067.015075683594, "val_acc": 52.0}
{"epoch": 30, "training_loss": 48897.489501953125, "training_acc": 52.0, "val_loss": 10304.949188232422, "val_acc": 52.0}
{"epoch": 31, "training_loss": 36177.81494140625, "training_acc": 49.0, "val_loss": 11642.835998535156, "val_acc": 48.0}
{"epoch": 32, "training_loss": 33209.8641204834, "training_acc": 53.0, "val_loss": 8799.508666992188, "val_acc": 52.0}
{"epoch": 33, "training_loss": 23470.256103515625, "training_acc": 56.0, "val_loss": 10279.33349609375, "val_acc": 48.0}
{"epoch": 34, "training_loss": 39680.86877441406, "training_acc": 47.0, "val_loss": 6242.0867919921875, "val_acc": 56.0}
{"epoch": 35, "training_loss": 20308.767028808594, "training_acc": 54.0, "val_loss": 2922.022819519043, "val_acc": 40.0}
{"epoch": 36, "training_loss": 12057.139144897461, "training_acc": 58.0, "val_loss": 3244.834518432617, "val_acc": 60.0}
{"epoch": 37, "training_loss": 6079.970291137695, "training_acc": 65.0, "val_loss": 1347.6615905761719, "val_acc": 48.0}
{"epoch": 38, "training_loss": 8481.959533691406, "training_acc": 62.0, "val_loss": 2164.66064453125, "val_acc": 64.0}
{"epoch": 39, "training_loss": 8996.292907714844, "training_acc": 62.0, "val_loss": 5354.806137084961, "val_acc": 52.0}
{"epoch": 40, "training_loss": 10773.66056060791, "training_acc": 59.0, "val_loss": 4151.056671142578, "val_acc": 48.0}
{"epoch": 41, "training_loss": 16064.583862304688, "training_acc": 51.0, "val_loss": 2332.0802688598633, "val_acc": 60.0}
{"epoch": 42, "training_loss": 20593.639282226562, "training_acc": 52.0, "val_loss": 4788.64631652832, "val_acc": 48.0}
{"epoch": 43, "training_loss": 37434.319091796875, "training_acc": 41.0, "val_loss": 17330.032348632812, "val_acc": 52.0}
{"epoch": 44, "training_loss": 50159.38659667969, "training_acc": 53.0, "val_loss": 15023.513793945312, "val_acc": 48.0}
{"epoch": 45, "training_loss": 73034.27685546875, "training_acc": 47.0, "val_loss": 12085.58349609375, "val_acc": 48.0}
{"epoch": 46, "training_loss": 53823.012451171875, "training_acc": 41.0, "val_loss": 18621.995544433594, "val_acc": 52.0}
{"epoch": 47, "training_loss": 60652.22814941406, "training_acc": 53.0, "val_loss": 5747.688293457031, "val_acc": 52.0}
{"epoch": 48, "training_loss": 30597.498046875, "training_acc": 46.0, "val_loss": 655.3313255310059, "val_acc": 56.0}
{"epoch": 49, "training_loss": 13818.003295898438, "training_acc": 63.0, "val_loss": 3934.4104766845703, "val_acc": 52.0}
{"epoch": 50, "training_loss": 30899.571533203125, "training_acc": 44.0, "val_loss": 11084.728240966797, "val_acc": 48.0}
{"epoch": 51, "training_loss": 29840.55877685547, "training_acc": 54.0, "val_loss": 6650.6866455078125, "val_acc": 52.0}
{"epoch": 52, "training_loss": 17400.27342224121, "training_acc": 58.0, "val_loss": 3770.7916259765625, "val_acc": 52.0}
{"epoch": 53, "training_loss": 12314.694702148438, "training_acc": 54.0, "val_loss": 3339.142608642578, "val_acc": 56.0}
{"epoch": 54, "training_loss": 14800.663146972656, "training_acc": 55.0, "val_loss": 935.808277130127, "val_acc": 64.0}
{"epoch": 55, "training_loss": 10215.18017578125, "training_acc": 68.0, "val_loss": 5072.5311279296875, "val_acc": 56.0}
{"epoch": 56, "training_loss": 27592.695068359375, "training_acc": 45.0, "val_loss": 7037.101745605469, "val_acc": 48.0}
{"epoch": 57, "training_loss": 29408.790771484375, "training_acc": 49.0, "val_loss": 11791.751861572266, "val_acc": 52.0}
{"epoch": 58, "training_loss": 30131.92559814453, "training_acc": 54.0, "val_loss": 8426.79672241211, "val_acc": 48.0}
{"epoch": 59, "training_loss": 22886.264587402344, "training_acc": 48.0, "val_loss": 11600.975036621094, "val_acc": 52.0}
{"epoch": 60, "training_loss": 41025.07958984375, "training_acc": 53.0, "val_loss": 2917.43221282959, "val_acc": 40.0}
{"epoch": 61, "training_loss": 13332.90771484375, "training_acc": 51.0, "val_loss": 5636.703872680664, "val_acc": 56.0}
{"epoch": 62, "training_loss": 14146.601440429688, "training_acc": 56.0, "val_loss": 8575.8056640625, "val_acc": 48.0}
{"epoch": 63, "training_loss": 29119.713989257812, "training_acc": 47.0, "val_loss": 10921.241760253906, "val_acc": 52.0}
{"epoch": 64, "training_loss": 36740.648193359375, "training_acc": 53.0, "val_loss": 7127.947235107422, "val_acc": 56.0}
{"epoch": 65, "training_loss": 26763.814208984375, "training_acc": 54.0, "val_loss": 12541.61376953125, "val_acc": 48.0}
{"epoch": 66, "training_loss": 35654.64373779297, "training_acc": 54.0, "val_loss": 15941.032409667969, "val_acc": 52.0}
{"epoch": 67, "training_loss": 53286.90283203125, "training_acc": 55.0, "val_loss": 7830.833435058594, "val_acc": 56.0}
