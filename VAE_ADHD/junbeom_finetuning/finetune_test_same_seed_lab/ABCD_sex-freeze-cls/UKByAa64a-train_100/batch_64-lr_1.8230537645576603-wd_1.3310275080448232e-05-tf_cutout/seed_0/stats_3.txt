"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 3124.460792541504, "training_acc": 50.0, "val_loss": 538.7702465057373, "val_acc": 52.0}
{"epoch": 1, "training_loss": 3737.9287109375, "training_acc": 53.0, "val_loss": 2420.353889465332, "val_acc": 48.0}
{"epoch": 2, "training_loss": 9155.766540527344, "training_acc": 47.0, "val_loss": 801.6134262084961, "val_acc": 48.0}
{"epoch": 3, "training_loss": 2554.487003326416, "training_acc": 55.0, "val_loss": 1372.3075866699219, "val_acc": 52.0}
{"epoch": 4, "training_loss": 5038.923049926758, "training_acc": 53.0, "val_loss": 1216.1774635314941, "val_acc": 52.0}
{"epoch": 5, "training_loss": 3203.9270210266113, "training_acc": 53.0, "val_loss": 467.045259475708, "val_acc": 48.0}
{"epoch": 6, "training_loss": 3074.7274627685547, "training_acc": 47.0, "val_loss": 809.6057891845703, "val_acc": 48.0}
{"epoch": 7, "training_loss": 2881.7757873535156, "training_acc": 47.0, "val_loss": 527.1369934082031, "val_acc": 52.0}
{"epoch": 8, "training_loss": 2131.1134338378906, "training_acc": 50.0, "val_loss": 998.3997344970703, "val_acc": 52.0}
{"epoch": 9, "training_loss": 2975.9117736816406, "training_acc": 53.0, "val_loss": 221.03338241577148, "val_acc": 52.0}
{"epoch": 10, "training_loss": 815.5383338928223, "training_acc": 60.0, "val_loss": 730.3360939025879, "val_acc": 48.0}
{"epoch": 11, "training_loss": 3112.410675048828, "training_acc": 47.0, "val_loss": 176.07386112213135, "val_acc": 48.0}
{"epoch": 12, "training_loss": 1097.6319274902344, "training_acc": 54.0, "val_loss": 912.9287719726562, "val_acc": 52.0}
{"epoch": 13, "training_loss": 3084.324172973633, "training_acc": 53.0, "val_loss": 711.9655609130859, "val_acc": 52.0}
{"epoch": 14, "training_loss": 1477.3990440368652, "training_acc": 56.0, "val_loss": 484.7686767578125, "val_acc": 48.0}
{"epoch": 15, "training_loss": 2799.2884216308594, "training_acc": 47.0, "val_loss": 590.8445835113525, "val_acc": 48.0}
{"epoch": 16, "training_loss": 2007.7697372436523, "training_acc": 48.0, "val_loss": 562.0640754699707, "val_acc": 52.0}
{"epoch": 17, "training_loss": 1926.2429809570312, "training_acc": 52.0, "val_loss": 862.0143890380859, "val_acc": 52.0}
{"epoch": 18, "training_loss": 2409.693099975586, "training_acc": 53.0, "val_loss": 112.90909051895142, "val_acc": 44.0}
{"epoch": 19, "training_loss": 966.3569564819336, "training_acc": 55.0, "val_loss": 523.1280326843262, "val_acc": 48.0}
{"epoch": 20, "training_loss": 2025.5822677612305, "training_acc": 47.0, "val_loss": 274.1022825241089, "val_acc": 52.0}
{"epoch": 21, "training_loss": 952.8503952026367, "training_acc": 53.0, "val_loss": 390.30537605285645, "val_acc": 52.0}
{"epoch": 22, "training_loss": 805.4499101638794, "training_acc": 57.0, "val_loss": 186.94692850112915, "val_acc": 48.0}
{"epoch": 23, "training_loss": 788.5984477996826, "training_acc": 49.0, "val_loss": 297.95477390289307, "val_acc": 52.0}
{"epoch": 24, "training_loss": 899.9924659729004, "training_acc": 53.0, "val_loss": 124.20146465301514, "val_acc": 52.0}
{"epoch": 25, "training_loss": 548.6825294494629, "training_acc": 54.0, "val_loss": 295.4029083251953, "val_acc": 48.0}
{"epoch": 26, "training_loss": 936.8755178451538, "training_acc": 50.0, "val_loss": 317.2677278518677, "val_acc": 52.0}
{"epoch": 27, "training_loss": 1038.0141868591309, "training_acc": 53.0, "val_loss": 70.04242539405823, "val_acc": 52.0}
{"epoch": 28, "training_loss": 566.8139266967773, "training_acc": 59.0, "val_loss": 267.82281398773193, "val_acc": 48.0}
{"epoch": 29, "training_loss": 1012.624397277832, "training_acc": 38.0, "val_loss": 164.76635932922363, "val_acc": 52.0}
{"epoch": 30, "training_loss": 459.7924175262451, "training_acc": 51.0, "val_loss": 28.832808136940002, "val_acc": 64.0}
{"epoch": 31, "training_loss": 216.7958059310913, "training_acc": 64.0, "val_loss": 47.655096650123596, "val_acc": 56.0}
{"epoch": 32, "training_loss": 208.46830081939697, "training_acc": 60.0, "val_loss": 70.27868628501892, "val_acc": 52.0}
{"epoch": 33, "training_loss": 200.7454195022583, "training_acc": 59.0, "val_loss": 184.03109312057495, "val_acc": 52.0}
{"epoch": 34, "training_loss": 326.05907821655273, "training_acc": 56.0, "val_loss": 135.5312943458557, "val_acc": 48.0}
{"epoch": 35, "training_loss": 494.6406126022339, "training_acc": 47.0, "val_loss": 301.2677192687988, "val_acc": 52.0}
{"epoch": 36, "training_loss": 810.3146667480469, "training_acc": 53.0, "val_loss": 69.42061185836792, "val_acc": 44.0}
{"epoch": 37, "training_loss": 312.03835678100586, "training_acc": 63.0, "val_loss": 68.19199919700623, "val_acc": 44.0}
{"epoch": 38, "training_loss": 360.69784355163574, "training_acc": 55.0, "val_loss": 127.02599763870239, "val_acc": 52.0}
{"epoch": 39, "training_loss": 387.80809783935547, "training_acc": 56.0, "val_loss": 79.2349100112915, "val_acc": 52.0}
{"epoch": 40, "training_loss": 678.9868354797363, "training_acc": 49.0, "val_loss": 356.5974712371826, "val_acc": 52.0}
{"epoch": 41, "training_loss": 845.7866561412811, "training_acc": 57.0, "val_loss": 274.1846799850464, "val_acc": 48.0}
{"epoch": 42, "training_loss": 1108.28511428833, "training_acc": 47.0, "val_loss": 90.75332283973694, "val_acc": 52.0}
{"epoch": 43, "training_loss": 403.04472732543945, "training_acc": 58.0, "val_loss": 66.21066927909851, "val_acc": 52.0}
{"epoch": 44, "training_loss": 493.65402603149414, "training_acc": 57.0, "val_loss": 194.88147497177124, "val_acc": 48.0}
{"epoch": 45, "training_loss": 674.5453872680664, "training_acc": 51.0, "val_loss": 229.25615310668945, "val_acc": 52.0}
{"epoch": 46, "training_loss": 416.03865146636963, "training_acc": 62.0, "val_loss": 221.9433307647705, "val_acc": 48.0}
{"epoch": 47, "training_loss": 762.7583904266357, "training_acc": 48.0, "val_loss": 297.9769229888916, "val_acc": 52.0}
{"epoch": 48, "training_loss": 1082.841941833496, "training_acc": 53.0, "val_loss": 175.5970597267151, "val_acc": 52.0}
{"epoch": 49, "training_loss": 808.1806526184082, "training_acc": 49.0, "val_loss": 472.9811668395996, "val_acc": 48.0}
