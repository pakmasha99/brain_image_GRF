"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 100864.80559158325, "training_acc": 46.0, "val_loss": 21073.907470703125, "val_acc": 52.0}
{"epoch": 1, "training_loss": 102332.81884765625, "training_acc": 53.0, "val_loss": 54576.641845703125, "val_acc": 48.0}
{"epoch": 2, "training_loss": 205399.400390625, "training_acc": 47.0, "val_loss": 15128.633117675781, "val_acc": 48.0}
{"epoch": 3, "training_loss": 71571.9892578125, "training_acc": 51.0, "val_loss": 39857.843017578125, "val_acc": 52.0}
{"epoch": 4, "training_loss": 151431.767578125, "training_acc": 53.0, "val_loss": 35566.34216308594, "val_acc": 52.0}
{"epoch": 5, "training_loss": 105558.53857421875, "training_acc": 53.0, "val_loss": 6179.717254638672, "val_acc": 48.0}
{"epoch": 6, "training_loss": 56956.984375, "training_acc": 47.0, "val_loss": 18216.97235107422, "val_acc": 48.0}
{"epoch": 7, "training_loss": 61747.79455566406, "training_acc": 47.0, "val_loss": 13199.810791015625, "val_acc": 52.0}
{"epoch": 8, "training_loss": 51469.9287109375, "training_acc": 53.0, "val_loss": 22887.940979003906, "val_acc": 52.0}
{"epoch": 9, "training_loss": 74320.90246582031, "training_acc": 53.0, "val_loss": 2884.109306335449, "val_acc": 52.0}
{"epoch": 10, "training_loss": 29725.4375, "training_acc": 49.0, "val_loss": 21282.138061523438, "val_acc": 48.0}
{"epoch": 11, "training_loss": 81542.6796875, "training_acc": 47.0, "val_loss": 4516.057205200195, "val_acc": 48.0}
{"epoch": 12, "training_loss": 29545.051025390625, "training_acc": 49.0, "val_loss": 22391.42303466797, "val_acc": 52.0}
{"epoch": 13, "training_loss": 86088.6083984375, "training_acc": 53.0, "val_loss": 16785.073852539062, "val_acc": 52.0}
{"epoch": 14, "training_loss": 45057.661529541016, "training_acc": 52.0, "val_loss": 16482.760620117188, "val_acc": 48.0}
{"epoch": 15, "training_loss": 80309.044921875, "training_acc": 47.0, "val_loss": 24535.60028076172, "val_acc": 48.0}
{"epoch": 16, "training_loss": 82824.90185546875, "training_acc": 47.0, "val_loss": 2172.9896545410156, "val_acc": 60.0}
{"epoch": 17, "training_loss": 20965.544555664062, "training_acc": 56.0, "val_loss": 14749.310302734375, "val_acc": 52.0}
{"epoch": 18, "training_loss": 48106.122009277344, "training_acc": 53.0, "val_loss": 1676.2001037597656, "val_acc": 44.0}
{"epoch": 19, "training_loss": 12080.111267089844, "training_acc": 57.0, "val_loss": 2606.4916610717773, "val_acc": 48.0}
{"epoch": 20, "training_loss": 18893.632568359375, "training_acc": 47.0, "val_loss": 8162.684631347656, "val_acc": 52.0}
{"epoch": 21, "training_loss": 17719.30519104004, "training_acc": 59.0, "val_loss": 8445.4833984375, "val_acc": 48.0}
{"epoch": 22, "training_loss": 37406.23229980469, "training_acc": 47.0, "val_loss": 2062.849998474121, "val_acc": 40.0}
{"epoch": 23, "training_loss": 21984.010009765625, "training_acc": 49.0, "val_loss": 16853.305053710938, "val_acc": 52.0}
{"epoch": 24, "training_loss": 54431.97692871094, "training_acc": 53.0, "val_loss": 5415.628814697266, "val_acc": 52.0}
{"epoch": 25, "training_loss": 26094.175048828125, "training_acc": 50.0, "val_loss": 15294.454956054688, "val_acc": 48.0}
{"epoch": 26, "training_loss": 55880.889892578125, "training_acc": 47.0, "val_loss": 1522.066593170166, "val_acc": 48.0}
{"epoch": 27, "training_loss": 22264.96435546875, "training_acc": 52.0, "val_loss": 19334.605407714844, "val_acc": 52.0}
{"epoch": 28, "training_loss": 65045.83203125, "training_acc": 53.0, "val_loss": 8246.006774902344, "val_acc": 52.0}
{"epoch": 29, "training_loss": 35268.83642578125, "training_acc": 41.0, "val_loss": 14261.976623535156, "val_acc": 48.0}
{"epoch": 30, "training_loss": 53182.398681640625, "training_acc": 47.0, "val_loss": 1908.187484741211, "val_acc": 48.0}
{"epoch": 31, "training_loss": 15108.114013671875, "training_acc": 66.0, "val_loss": 15846.772766113281, "val_acc": 52.0}
{"epoch": 32, "training_loss": 50233.93420410156, "training_acc": 53.0, "val_loss": 2927.35538482666, "val_acc": 60.0}
{"epoch": 33, "training_loss": 19192.095947265625, "training_acc": 65.0, "val_loss": 18175.78887939453, "val_acc": 48.0}
{"epoch": 34, "training_loss": 71126.02319335938, "training_acc": 47.0, "val_loss": 3503.9451599121094, "val_acc": 48.0}
{"epoch": 35, "training_loss": 23816.973266601562, "training_acc": 47.0, "val_loss": 18452.53448486328, "val_acc": 52.0}
{"epoch": 36, "training_loss": 60812.07373046875, "training_acc": 53.0, "val_loss": 9584.849548339844, "val_acc": 52.0}
{"epoch": 37, "training_loss": 25099.05938720703, "training_acc": 51.0, "val_loss": 9958.831787109375, "val_acc": 48.0}
{"epoch": 38, "training_loss": 39009.54748535156, "training_acc": 47.0, "val_loss": 5281.817245483398, "val_acc": 56.0}
{"epoch": 39, "training_loss": 16673.499572753906, "training_acc": 55.0, "val_loss": 6059.661865234375, "val_acc": 56.0}
{"epoch": 40, "training_loss": 14887.874938964844, "training_acc": 49.0, "val_loss": 5008.132553100586, "val_acc": 48.0}
{"epoch": 41, "training_loss": 17739.820922851562, "training_acc": 53.0, "val_loss": 6094.033432006836, "val_acc": 56.0}
{"epoch": 42, "training_loss": 13522.073440551758, "training_acc": 52.0, "val_loss": 1526.0908126831055, "val_acc": 52.0}
{"epoch": 43, "training_loss": 6687.981658935547, "training_acc": 66.0, "val_loss": 1360.3190422058105, "val_acc": 52.0}
{"epoch": 44, "training_loss": 4789.5450439453125, "training_acc": 70.0, "val_loss": 1414.2669677734375, "val_acc": 56.0}
{"epoch": 45, "training_loss": 3604.652503967285, "training_acc": 69.0, "val_loss": 1055.0938606262207, "val_acc": 56.0}
{"epoch": 46, "training_loss": 5849.525588989258, "training_acc": 67.0, "val_loss": 1103.4738540649414, "val_acc": 40.0}
{"epoch": 47, "training_loss": 5659.700088500977, "training_acc": 65.0, "val_loss": 3601.2882232666016, "val_acc": 56.0}
{"epoch": 48, "training_loss": 9026.57209777832, "training_acc": 54.0, "val_loss": 1049.742317199707, "val_acc": 64.0}
{"epoch": 49, "training_loss": 1941.9333686828613, "training_acc": 73.0, "val_loss": 523.5239028930664, "val_acc": 64.0}
{"epoch": 50, "training_loss": 2845.1157455444336, "training_acc": 73.0, "val_loss": 3396.19140625, "val_acc": 52.0}
{"epoch": 51, "training_loss": 6546.1003494262695, "training_acc": 63.0, "val_loss": 3640.715789794922, "val_acc": 48.0}
{"epoch": 52, "training_loss": 11723.313472747803, "training_acc": 59.0, "val_loss": 4144.834899902344, "val_acc": 52.0}
{"epoch": 53, "training_loss": 9667.916534423828, "training_acc": 51.0, "val_loss": 640.9936904907227, "val_acc": 64.0}
{"epoch": 54, "training_loss": 2572.2974700927734, "training_acc": 66.0, "val_loss": 1697.878074645996, "val_acc": 44.0}
{"epoch": 55, "training_loss": 5489.159767150879, "training_acc": 62.0, "val_loss": 1297.7544784545898, "val_acc": 60.0}
{"epoch": 56, "training_loss": 7085.43212890625, "training_acc": 59.0, "val_loss": 2471.6686248779297, "val_acc": 52.0}
{"epoch": 57, "training_loss": 6063.273651123047, "training_acc": 59.0, "val_loss": 2613.72013092041, "val_acc": 48.0}
{"epoch": 58, "training_loss": 7016.800308227539, "training_acc": 55.0, "val_loss": 447.7743625640869, "val_acc": 72.0}
{"epoch": 59, "training_loss": 4691.356842041016, "training_acc": 68.0, "val_loss": 1578.412914276123, "val_acc": 56.0}
{"epoch": 60, "training_loss": 4688.2830810546875, "training_acc": 63.0, "val_loss": 7257.963562011719, "val_acc": 48.0}
{"epoch": 61, "training_loss": 26800.240600585938, "training_acc": 47.0, "val_loss": 3162.0399475097656, "val_acc": 52.0}
{"epoch": 62, "training_loss": 11303.372314453125, "training_acc": 53.0, "val_loss": 1620.8196640014648, "val_acc": 48.0}
{"epoch": 63, "training_loss": 5199.3104248046875, "training_acc": 57.0, "val_loss": 6432.9742431640625, "val_acc": 52.0}
{"epoch": 64, "training_loss": 18030.28399658203, "training_acc": 53.0, "val_loss": 421.494722366333, "val_acc": 68.0}
{"epoch": 65, "training_loss": 4786.697052001953, "training_acc": 63.0, "val_loss": 4654.7088623046875, "val_acc": 52.0}
{"epoch": 66, "training_loss": 10822.960174560547, "training_acc": 50.0, "val_loss": 3309.383773803711, "val_acc": 48.0}
{"epoch": 67, "training_loss": 10607.547874450684, "training_acc": 53.0, "val_loss": 1971.8681335449219, "val_acc": 56.0}
{"epoch": 68, "training_loss": 4380.368560791016, "training_acc": 63.0, "val_loss": 1913.3403778076172, "val_acc": 56.0}
{"epoch": 69, "training_loss": 3208.3380012512207, "training_acc": 68.0, "val_loss": 2861.1488342285156, "val_acc": 48.0}
{"epoch": 70, "training_loss": 5992.8622970581055, "training_acc": 57.0, "val_loss": 906.9217681884766, "val_acc": 60.0}
{"epoch": 71, "training_loss": 8165.147033691406, "training_acc": 56.0, "val_loss": 653.6762714385986, "val_acc": 52.0}
{"epoch": 72, "training_loss": 6253.757751464844, "training_acc": 66.0, "val_loss": 605.0354957580566, "val_acc": 52.0}
{"epoch": 73, "training_loss": 6214.258605957031, "training_acc": 68.0, "val_loss": 1059.149169921875, "val_acc": 64.0}
{"epoch": 74, "training_loss": 2328.3517990112305, "training_acc": 67.0, "val_loss": 1178.9155006408691, "val_acc": 48.0}
{"epoch": 75, "training_loss": 6402.4105224609375, "training_acc": 55.0, "val_loss": 515.8137321472168, "val_acc": 68.0}
{"epoch": 76, "training_loss": 2913.210647583008, "training_acc": 66.0, "val_loss": 2948.3190536499023, "val_acc": 52.0}
{"epoch": 77, "training_loss": 5282.151176452637, "training_acc": 66.0, "val_loss": 2746.548843383789, "val_acc": 48.0}
{"epoch": 78, "training_loss": 6968.338333129883, "training_acc": 56.0, "val_loss": 964.309024810791, "val_acc": 60.0}
{"epoch": 79, "training_loss": 4625.3046875, "training_acc": 67.0, "val_loss": 1921.5030670166016, "val_acc": 56.0}
{"epoch": 80, "training_loss": 3558.3033714294434, "training_acc": 66.0, "val_loss": 2795.8852767944336, "val_acc": 48.0}
{"epoch": 81, "training_loss": 10715.242797851562, "training_acc": 47.0, "val_loss": 2034.2536926269531, "val_acc": 60.0}
{"epoch": 82, "training_loss": 13856.145263671875, "training_acc": 50.0, "val_loss": 4865.40412902832, "val_acc": 48.0}
{"epoch": 83, "training_loss": 17794.44757080078, "training_acc": 49.0, "val_loss": 7247.574615478516, "val_acc": 52.0}
