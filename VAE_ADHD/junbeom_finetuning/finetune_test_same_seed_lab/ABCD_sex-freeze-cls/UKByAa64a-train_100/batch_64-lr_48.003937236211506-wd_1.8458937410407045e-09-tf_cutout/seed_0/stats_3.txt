"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 80255.85828781128, "training_acc": 49.0, "val_loss": 5667.911148071289, "val_acc": 48.0}
{"epoch": 1, "training_loss": 46933.11279296875, "training_acc": 52.0, "val_loss": 16652.64434814453, "val_acc": 52.0}
{"epoch": 2, "training_loss": 63290.27783203125, "training_acc": 47.0, "val_loss": 16374.397277832031, "val_acc": 48.0}
{"epoch": 3, "training_loss": 44912.067443847656, "training_acc": 47.0, "val_loss": 20306.182861328125, "val_acc": 52.0}
{"epoch": 4, "training_loss": 79377.97314453125, "training_acc": 53.0, "val_loss": 17124.163818359375, "val_acc": 52.0}
{"epoch": 5, "training_loss": 46674.8489074707, "training_acc": 45.0, "val_loss": 4075.7164001464844, "val_acc": 48.0}
{"epoch": 6, "training_loss": 14555.848114013672, "training_acc": 54.0, "val_loss": 3736.355972290039, "val_acc": 52.0}
{"epoch": 7, "training_loss": 8548.905395507812, "training_acc": 65.0, "val_loss": 3445.1614379882812, "val_acc": 48.0}
{"epoch": 8, "training_loss": 13949.004943847656, "training_acc": 54.0, "val_loss": 7956.120300292969, "val_acc": 52.0}
{"epoch": 9, "training_loss": 15203.474090576172, "training_acc": 54.0, "val_loss": 5218.046188354492, "val_acc": 48.0}
{"epoch": 10, "training_loss": 15936.716613769531, "training_acc": 47.0, "val_loss": 11385.404205322266, "val_acc": 52.0}
{"epoch": 11, "training_loss": 35549.340576171875, "training_acc": 53.0, "val_loss": 5132.418441772461, "val_acc": 52.0}
{"epoch": 12, "training_loss": 18021.25341796875, "training_acc": 57.0, "val_loss": 11464.897918701172, "val_acc": 48.0}
{"epoch": 13, "training_loss": 39947.45587158203, "training_acc": 47.0, "val_loss": 11790.24887084961, "val_acc": 52.0}
{"epoch": 14, "training_loss": 44947.33837890625, "training_acc": 53.0, "val_loss": 14669.038391113281, "val_acc": 52.0}
{"epoch": 15, "training_loss": 26803.45066833496, "training_acc": 57.0, "val_loss": 9938.048553466797, "val_acc": 48.0}
{"epoch": 16, "training_loss": 47846.05261230469, "training_acc": 47.0, "val_loss": 4533.732223510742, "val_acc": 48.0}
{"epoch": 17, "training_loss": 23498.099243164062, "training_acc": 50.0, "val_loss": 16228.717041015625, "val_acc": 52.0}
{"epoch": 18, "training_loss": 41142.875244140625, "training_acc": 53.0, "val_loss": 3705.8998107910156, "val_acc": 40.0}
{"epoch": 19, "training_loss": 23862.667236328125, "training_acc": 53.0, "val_loss": 9096.837615966797, "val_acc": 48.0}
{"epoch": 20, "training_loss": 32435.26284790039, "training_acc": 53.0, "val_loss": 13360.861206054688, "val_acc": 52.0}
{"epoch": 21, "training_loss": 41865.149169921875, "training_acc": 53.0, "val_loss": 12178.300476074219, "val_acc": 52.0}
{"epoch": 22, "training_loss": 26094.515747070312, "training_acc": 50.0, "val_loss": 7558.409118652344, "val_acc": 48.0}
{"epoch": 23, "training_loss": 31652.422119140625, "training_acc": 48.0, "val_loss": 6458.219909667969, "val_acc": 52.0}
{"epoch": 24, "training_loss": 17660.683227539062, "training_acc": 54.0, "val_loss": 5899.347686767578, "val_acc": 52.0}
{"epoch": 25, "training_loss": 15545.863708496094, "training_acc": 53.0, "val_loss": 5905.973052978516, "val_acc": 48.0}
{"epoch": 26, "training_loss": 25235.452758789062, "training_acc": 44.0, "val_loss": 7827.204132080078, "val_acc": 52.0}
{"epoch": 27, "training_loss": 16753.85984802246, "training_acc": 57.0, "val_loss": 3074.3785858154297, "val_acc": 48.0}
{"epoch": 28, "training_loss": 16943.05551147461, "training_acc": 49.0, "val_loss": 4524.351119995117, "val_acc": 52.0}
{"epoch": 29, "training_loss": 11401.494323730469, "training_acc": 56.0, "val_loss": 3387.9146575927734, "val_acc": 48.0}
{"epoch": 30, "training_loss": 15170.086120605469, "training_acc": 50.0, "val_loss": 5252.816390991211, "val_acc": 52.0}
{"epoch": 31, "training_loss": 15820.269287109375, "training_acc": 53.0, "val_loss": 822.8403091430664, "val_acc": 60.0}
{"epoch": 32, "training_loss": 11395.874389648438, "training_acc": 58.0, "val_loss": 858.5664749145508, "val_acc": 60.0}
{"epoch": 33, "training_loss": 9672.097595214844, "training_acc": 62.0, "val_loss": 4675.437545776367, "val_acc": 52.0}
{"epoch": 34, "training_loss": 9467.664672851562, "training_acc": 60.0, "val_loss": 1530.8627128601074, "val_acc": 64.0}
{"epoch": 35, "training_loss": 12810.276916503906, "training_acc": 55.0, "val_loss": 7126.389312744141, "val_acc": 52.0}
{"epoch": 36, "training_loss": 11538.927322387695, "training_acc": 66.0, "val_loss": 1651.2014389038086, "val_acc": 48.0}
{"epoch": 37, "training_loss": 9222.129592895508, "training_acc": 68.0, "val_loss": 7754.408264160156, "val_acc": 52.0}
{"epoch": 38, "training_loss": 13064.929260253906, "training_acc": 50.0, "val_loss": 3484.531784057617, "val_acc": 48.0}
{"epoch": 39, "training_loss": 16632.018188476562, "training_acc": 51.0, "val_loss": 6162.144470214844, "val_acc": 52.0}
{"epoch": 40, "training_loss": 11669.166442871094, "training_acc": 60.0, "val_loss": 1442.43745803833, "val_acc": 40.0}
{"epoch": 41, "training_loss": 8384.37109375, "training_acc": 58.0, "val_loss": 4940.283966064453, "val_acc": 52.0}
{"epoch": 42, "training_loss": 8189.451705932617, "training_acc": 60.0, "val_loss": 1330.744743347168, "val_acc": 44.0}
{"epoch": 43, "training_loss": 5961.33967590332, "training_acc": 66.0, "val_loss": 3474.86572265625, "val_acc": 52.0}
{"epoch": 44, "training_loss": 4431.8135986328125, "training_acc": 64.0, "val_loss": 835.1162910461426, "val_acc": 36.0}
{"epoch": 45, "training_loss": 5155.478050231934, "training_acc": 58.0, "val_loss": 634.9559783935547, "val_acc": 64.0}
{"epoch": 46, "training_loss": 3883.190170288086, "training_acc": 64.0, "val_loss": 633.9715003967285, "val_acc": 64.0}
{"epoch": 47, "training_loss": 3721.7675552368164, "training_acc": 65.0, "val_loss": 3110.915946960449, "val_acc": 52.0}
{"epoch": 48, "training_loss": 9633.753997802734, "training_acc": 51.0, "val_loss": 1526.8820762634277, "val_acc": 48.0}
{"epoch": 49, "training_loss": 4397.535064697266, "training_acc": 65.0, "val_loss": 1063.608741760254, "val_acc": 48.0}
{"epoch": 50, "training_loss": 2182.157424926758, "training_acc": 74.0, "val_loss": 1236.3187789916992, "val_acc": 56.0}
{"epoch": 51, "training_loss": 9773.482177734375, "training_acc": 47.0, "val_loss": 1318.3212280273438, "val_acc": 44.0}
{"epoch": 52, "training_loss": 6460.410339355469, "training_acc": 59.0, "val_loss": 4521.148300170898, "val_acc": 52.0}
{"epoch": 53, "training_loss": 8286.697311401367, "training_acc": 57.0, "val_loss": 2274.9441146850586, "val_acc": 52.0}
{"epoch": 54, "training_loss": 8951.609024047852, "training_acc": 53.0, "val_loss": 2490.4586791992188, "val_acc": 52.0}
{"epoch": 55, "training_loss": 5651.645324707031, "training_acc": 61.0, "val_loss": 3044.8665618896484, "val_acc": 52.0}
{"epoch": 56, "training_loss": 4103.816635131836, "training_acc": 68.0, "val_loss": 3885.1959228515625, "val_acc": 48.0}
{"epoch": 57, "training_loss": 11354.98078918457, "training_acc": 54.0, "val_loss": 7866.944885253906, "val_acc": 52.0}
{"epoch": 58, "training_loss": 22987.007385253906, "training_acc": 53.0, "val_loss": 1602.928352355957, "val_acc": 48.0}
{"epoch": 59, "training_loss": 4977.303970336914, "training_acc": 56.0, "val_loss": 4679.536056518555, "val_acc": 52.0}
{"epoch": 60, "training_loss": 11153.034141540527, "training_acc": 59.0, "val_loss": 4188.753890991211, "val_acc": 48.0}
{"epoch": 61, "training_loss": 11280.759521484375, "training_acc": 57.0, "val_loss": 6997.673034667969, "val_acc": 52.0}
{"epoch": 62, "training_loss": 18607.26348876953, "training_acc": 53.0, "val_loss": 3231.8504333496094, "val_acc": 48.0}
{"epoch": 63, "training_loss": 11099.714630126953, "training_acc": 50.0, "val_loss": 6669.745635986328, "val_acc": 52.0}
{"epoch": 64, "training_loss": 15098.196228027344, "training_acc": 53.0, "val_loss": 3622.0252990722656, "val_acc": 48.0}
{"epoch": 65, "training_loss": 17263.238647460938, "training_acc": 47.0, "val_loss": 6773.53515625, "val_acc": 52.0}
