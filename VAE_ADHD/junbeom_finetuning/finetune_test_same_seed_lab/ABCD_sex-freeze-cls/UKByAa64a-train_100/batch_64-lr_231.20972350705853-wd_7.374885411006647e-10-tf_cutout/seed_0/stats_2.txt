"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 362385.6132659912, "training_acc": 53.0, "val_loss": 12486.019134521484, "val_acc": 56.0}
{"epoch": 1, "training_loss": 219943.611328125, "training_acc": 51.0, "val_loss": 44221.32263183594, "val_acc": 48.0}
{"epoch": 2, "training_loss": 231588.095703125, "training_acc": 55.0, "val_loss": 169558.837890625, "val_acc": 52.0}
{"epoch": 3, "training_loss": 609203.994140625, "training_acc": 53.0, "val_loss": 47260.75744628906, "val_acc": 52.0}
{"epoch": 4, "training_loss": 271713.99609375, "training_acc": 49.0, "val_loss": 152840.8447265625, "val_acc": 48.0}
{"epoch": 5, "training_loss": 602983.462890625, "training_acc": 47.0, "val_loss": 61726.2939453125, "val_acc": 48.0}
{"epoch": 6, "training_loss": 233403.015625, "training_acc": 49.0, "val_loss": 105885.94970703125, "val_acc": 52.0}
{"epoch": 7, "training_loss": 402795.4833984375, "training_acc": 53.0, "val_loss": 69866.30859375, "val_acc": 52.0}
{"epoch": 8, "training_loss": 170964.7481689453, "training_acc": 55.0, "val_loss": 46010.870361328125, "val_acc": 48.0}
{"epoch": 9, "training_loss": 183702.81396484375, "training_acc": 47.0, "val_loss": 8714.369201660156, "val_acc": 60.0}
{"epoch": 10, "training_loss": 52392.84912109375, "training_acc": 58.0, "val_loss": 9908.267974853516, "val_acc": 60.0}
{"epoch": 11, "training_loss": 71753.04150390625, "training_acc": 58.0, "val_loss": 42951.03759765625, "val_acc": 48.0}
{"epoch": 12, "training_loss": 123823.41882324219, "training_acc": 51.0, "val_loss": 32561.517333984375, "val_acc": 52.0}
{"epoch": 13, "training_loss": 111595.92602539062, "training_acc": 53.0, "val_loss": 20786.785888671875, "val_acc": 48.0}
{"epoch": 14, "training_loss": 86594.91650390625, "training_acc": 47.0, "val_loss": 15647.895812988281, "val_acc": 52.0}
{"epoch": 15, "training_loss": 64169.384033203125, "training_acc": 53.0, "val_loss": 17683.859252929688, "val_acc": 48.0}
{"epoch": 16, "training_loss": 60179.719482421875, "training_acc": 50.0, "val_loss": 23998.585510253906, "val_acc": 52.0}
{"epoch": 17, "training_loss": 75570.08129882812, "training_acc": 55.0, "val_loss": 6261.265182495117, "val_acc": 52.0}
{"epoch": 18, "training_loss": 36045.409240722656, "training_acc": 51.0, "val_loss": 15194.261169433594, "val_acc": 52.0}
{"epoch": 19, "training_loss": 31135.417907714844, "training_acc": 58.0, "val_loss": 6354.154205322266, "val_acc": 64.0}
{"epoch": 20, "training_loss": 15438.259765625, "training_acc": 65.0, "val_loss": 13335.102844238281, "val_acc": 48.0}
{"epoch": 21, "training_loss": 41648.12365722656, "training_acc": 52.0, "val_loss": 6691.801452636719, "val_acc": 56.0}
{"epoch": 22, "training_loss": 22466.339721679688, "training_acc": 61.0, "val_loss": 6103.332901000977, "val_acc": 52.0}
{"epoch": 23, "training_loss": 18630.087524414062, "training_acc": 66.0, "val_loss": 18388.870239257812, "val_acc": 44.0}
{"epoch": 24, "training_loss": 48042.678466796875, "training_acc": 53.0, "val_loss": 16969.813537597656, "val_acc": 52.0}
{"epoch": 25, "training_loss": 31445.079345703125, "training_acc": 60.0, "val_loss": 7519.218444824219, "val_acc": 48.0}
{"epoch": 26, "training_loss": 50634.017822265625, "training_acc": 48.0, "val_loss": 4640.95458984375, "val_acc": 60.0}
{"epoch": 27, "training_loss": 39268.865234375, "training_acc": 62.0, "val_loss": 6615.312957763672, "val_acc": 44.0}
{"epoch": 28, "training_loss": 76903.95263671875, "training_acc": 52.0, "val_loss": 57252.01416015625, "val_acc": 52.0}
{"epoch": 29, "training_loss": 169356.42529296875, "training_acc": 52.0, "val_loss": 30994.216918945312, "val_acc": 48.0}
{"epoch": 30, "training_loss": 148345.61376953125, "training_acc": 47.0, "val_loss": 4318.784713745117, "val_acc": 56.0}
{"epoch": 31, "training_loss": 107054.43359375, "training_acc": 48.0, "val_loss": 72779.11376953125, "val_acc": 52.0}
{"epoch": 32, "training_loss": 219483.16137695312, "training_acc": 53.0, "val_loss": 13493.824768066406, "val_acc": 48.0}
{"epoch": 33, "training_loss": 85317.216796875, "training_acc": 47.0, "val_loss": 8969.107055664062, "val_acc": 56.0}
{"epoch": 34, "training_loss": 44377.44580078125, "training_acc": 60.0, "val_loss": 6808.661651611328, "val_acc": 48.0}
{"epoch": 35, "training_loss": 17640.986267089844, "training_acc": 62.0, "val_loss": 13430.622863769531, "val_acc": 52.0}
{"epoch": 36, "training_loss": 30158.458862304688, "training_acc": 57.0, "val_loss": 7251.240539550781, "val_acc": 52.0}
{"epoch": 37, "training_loss": 12823.14566040039, "training_acc": 69.0, "val_loss": 7439.208984375, "val_acc": 52.0}
{"epoch": 38, "training_loss": 21161.910034179688, "training_acc": 67.0, "val_loss": 17357.994079589844, "val_acc": 52.0}
{"epoch": 39, "training_loss": 34339.81558227539, "training_acc": 60.0, "val_loss": 3933.354949951172, "val_acc": 60.0}
{"epoch": 40, "training_loss": 14764.893371582031, "training_acc": 70.0, "val_loss": 4839.101409912109, "val_acc": 60.0}
{"epoch": 41, "training_loss": 22112.449829101562, "training_acc": 66.0, "val_loss": 28440.167236328125, "val_acc": 52.0}
{"epoch": 42, "training_loss": 77297.93237304688, "training_acc": 54.0, "val_loss": 28149.359130859375, "val_acc": 48.0}
{"epoch": 43, "training_loss": 103978.09399414062, "training_acc": 47.0, "val_loss": 13710.298156738281, "val_acc": 52.0}
{"epoch": 44, "training_loss": 32366.268920898438, "training_acc": 55.0, "val_loss": 17464.686584472656, "val_acc": 48.0}
{"epoch": 45, "training_loss": 40102.11798095703, "training_acc": 53.0, "val_loss": 33820.306396484375, "val_acc": 52.0}
{"epoch": 46, "training_loss": 110761.12060546875, "training_acc": 53.0, "val_loss": 9387.857055664062, "val_acc": 40.0}
{"epoch": 47, "training_loss": 27799.408325195312, "training_acc": 54.0, "val_loss": 19717.36297607422, "val_acc": 52.0}
{"epoch": 48, "training_loss": 48698.21496582031, "training_acc": 59.0, "val_loss": 26787.646484375, "val_acc": 48.0}
{"epoch": 49, "training_loss": 71424.04260253906, "training_acc": 49.0, "val_loss": 37403.51867675781, "val_acc": 52.0}
{"epoch": 50, "training_loss": 144873.94384765625, "training_acc": 53.0, "val_loss": 12174.308013916016, "val_acc": 56.0}
{"epoch": 51, "training_loss": 75117.697265625, "training_acc": 58.0, "val_loss": 65376.28173828125, "val_acc": 48.0}
{"epoch": 52, "training_loss": 196218.7509765625, "training_acc": 47.0, "val_loss": 36426.116943359375, "val_acc": 52.0}
{"epoch": 53, "training_loss": 166895.189453125, "training_acc": 53.0, "val_loss": 45559.375, "val_acc": 52.0}
{"epoch": 54, "training_loss": 113948.74829101562, "training_acc": 54.0, "val_loss": 28481.982421875, "val_acc": 48.0}
{"epoch": 55, "training_loss": 74543.5913696289, "training_acc": 50.0, "val_loss": 13091.708374023438, "val_acc": 56.0}
{"epoch": 56, "training_loss": 31339.228515625, "training_acc": 59.0, "val_loss": 5587.583541870117, "val_acc": 56.0}
{"epoch": 57, "training_loss": 11075.725555419922, "training_acc": 73.0, "val_loss": 4748.121643066406, "val_acc": 60.0}
{"epoch": 58, "training_loss": 11065.336486816406, "training_acc": 68.0, "val_loss": 4749.538040161133, "val_acc": 56.0}
