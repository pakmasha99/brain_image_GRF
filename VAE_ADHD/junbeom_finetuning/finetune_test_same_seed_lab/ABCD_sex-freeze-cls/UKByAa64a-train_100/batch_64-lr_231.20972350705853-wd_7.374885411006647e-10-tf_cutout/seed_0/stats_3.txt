"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 500780.4211425781, "training_acc": 47.0, "val_loss": 128653.82080078125, "val_acc": 52.0}
{"epoch": 1, "training_loss": 532889.603515625, "training_acc": 51.0, "val_loss": 208511.9384765625, "val_acc": 48.0}
{"epoch": 2, "training_loss": 743802.259765625, "training_acc": 47.0, "val_loss": 5140.7958984375, "val_acc": 44.0}
{"epoch": 3, "training_loss": 238191.134765625, "training_acc": 52.0, "val_loss": 253278.22265625, "val_acc": 52.0}
{"epoch": 4, "training_loss": 973808.53125, "training_acc": 53.0, "val_loss": 226561.7431640625, "val_acc": 52.0}
{"epoch": 5, "training_loss": 711631.029296875, "training_acc": 53.0, "val_loss": 10544.487762451172, "val_acc": 36.0}
{"epoch": 6, "training_loss": 216819.4716796875, "training_acc": 55.0, "val_loss": 163151.123046875, "val_acc": 48.0}
{"epoch": 7, "training_loss": 652715.6298828125, "training_acc": 47.0, "val_loss": 86016.5283203125, "val_acc": 48.0}
{"epoch": 8, "training_loss": 269774.94104003906, "training_acc": 47.0, "val_loss": 70070.21484375, "val_acc": 52.0}
{"epoch": 9, "training_loss": 248301.912109375, "training_acc": 53.0, "val_loss": 47047.10388183594, "val_acc": 52.0}
{"epoch": 10, "training_loss": 125007.19897460938, "training_acc": 51.0, "val_loss": 35145.97473144531, "val_acc": 48.0}
{"epoch": 11, "training_loss": 102686.08911132812, "training_acc": 47.0, "val_loss": 64554.266357421875, "val_acc": 52.0}
{"epoch": 12, "training_loss": 288142.44921875, "training_acc": 53.0, "val_loss": 83144.873046875, "val_acc": 52.0}
{"epoch": 13, "training_loss": 229686.98901367188, "training_acc": 53.0, "val_loss": 52418.115234375, "val_acc": 48.0}
{"epoch": 14, "training_loss": 255026.17578125, "training_acc": 47.0, "val_loss": 71171.12426757812, "val_acc": 48.0}
{"epoch": 15, "training_loss": 229002.9970703125, "training_acc": 47.0, "val_loss": 60537.091064453125, "val_acc": 52.0}
{"epoch": 16, "training_loss": 259029.81640625, "training_acc": 53.0, "val_loss": 98644.29321289062, "val_acc": 52.0}
{"epoch": 17, "training_loss": 294848.3271484375, "training_acc": 53.0, "val_loss": 3804.65087890625, "val_acc": 44.0}
{"epoch": 18, "training_loss": 77440.44140625, "training_acc": 50.0, "val_loss": 10496.91162109375, "val_acc": 48.0}
{"epoch": 19, "training_loss": 86186.50390625, "training_acc": 51.0, "val_loss": 63034.814453125, "val_acc": 52.0}
{"epoch": 20, "training_loss": 170273.751953125, "training_acc": 53.0, "val_loss": 7734.056091308594, "val_acc": 52.0}
{"epoch": 21, "training_loss": 78367.20068359375, "training_acc": 47.0, "val_loss": 14646.334838867188, "val_acc": 52.0}
{"epoch": 22, "training_loss": 37554.73193359375, "training_acc": 61.0, "val_loss": 12620.844268798828, "val_acc": 52.0}
{"epoch": 23, "training_loss": 22424.034545898438, "training_acc": 67.0, "val_loss": 3459.971237182617, "val_acc": 52.0}
{"epoch": 24, "training_loss": 39171.289794921875, "training_acc": 58.0, "val_loss": 24456.431579589844, "val_acc": 52.0}
{"epoch": 25, "training_loss": 52518.81579589844, "training_acc": 56.0, "val_loss": 3791.465377807617, "val_acc": 56.0}
{"epoch": 26, "training_loss": 57488.78515625, "training_acc": 51.0, "val_loss": 32445.547485351562, "val_acc": 52.0}
{"epoch": 27, "training_loss": 77601.47766113281, "training_acc": 51.0, "val_loss": 3840.771484375, "val_acc": 60.0}
{"epoch": 28, "training_loss": 54805.3583984375, "training_acc": 51.0, "val_loss": 30534.515380859375, "val_acc": 52.0}
{"epoch": 29, "training_loss": 67779.34985351562, "training_acc": 51.0, "val_loss": 5643.891906738281, "val_acc": 64.0}
{"epoch": 30, "training_loss": 80395.97607421875, "training_acc": 42.0, "val_loss": 42747.20153808594, "val_acc": 52.0}
{"epoch": 31, "training_loss": 73408.88897705078, "training_acc": 58.0, "val_loss": 35096.65832519531, "val_acc": 48.0}
{"epoch": 32, "training_loss": 155876.5068359375, "training_acc": 47.0, "val_loss": 14385.203552246094, "val_acc": 48.0}
{"epoch": 33, "training_loss": 45317.343505859375, "training_acc": 62.0, "val_loss": 28023.931884765625, "val_acc": 52.0}
{"epoch": 34, "training_loss": 57772.15625, "training_acc": 53.0, "val_loss": 7702.9022216796875, "val_acc": 52.0}
{"epoch": 35, "training_loss": 43467.5234375, "training_acc": 60.0, "val_loss": 34934.64660644531, "val_acc": 52.0}
{"epoch": 36, "training_loss": 65288.277587890625, "training_acc": 52.0, "val_loss": 7397.054290771484, "val_acc": 44.0}
{"epoch": 37, "training_loss": 37691.682373046875, "training_acc": 60.0, "val_loss": 6855.438995361328, "val_acc": 48.0}
{"epoch": 38, "training_loss": 28802.666015625, "training_acc": 69.0, "val_loss": 10925.462341308594, "val_acc": 52.0}
{"epoch": 39, "training_loss": 19912.462768554688, "training_acc": 63.0, "val_loss": 5908.000946044922, "val_acc": 48.0}
{"epoch": 40, "training_loss": 38480.76037597656, "training_acc": 48.0, "val_loss": 7730.865478515625, "val_acc": 52.0}
{"epoch": 41, "training_loss": 24801.496459960938, "training_acc": 59.0, "val_loss": 13970.951843261719, "val_acc": 52.0}
{"epoch": 42, "training_loss": 19699.351623535156, "training_acc": 64.0, "val_loss": 12697.454833984375, "val_acc": 48.0}
