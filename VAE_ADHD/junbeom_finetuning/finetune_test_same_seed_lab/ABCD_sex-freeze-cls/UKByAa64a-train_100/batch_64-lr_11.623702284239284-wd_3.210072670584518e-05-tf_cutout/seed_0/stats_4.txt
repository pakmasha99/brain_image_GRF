"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 36786.64928817749, "training_acc": 44.0, "val_loss": 23022.450256347656, "val_acc": 52.0}
{"epoch": 1, "training_loss": 94122.37109375, "training_acc": 53.0, "val_loss": 9280.94482421875, "val_acc": 52.0}
{"epoch": 2, "training_loss": 66369.82275390625, "training_acc": 41.0, "val_loss": 25789.263916015625, "val_acc": 48.0}
{"epoch": 3, "training_loss": 92283.35620117188, "training_acc": 47.0, "val_loss": 2595.8444595336914, "val_acc": 52.0}
{"epoch": 4, "training_loss": 17900.970947265625, "training_acc": 53.0, "val_loss": 3858.314895629883, "val_acc": 52.0}
{"epoch": 5, "training_loss": 27458.863525390625, "training_acc": 43.0, "val_loss": 9757.172393798828, "val_acc": 48.0}
{"epoch": 6, "training_loss": 27064.4462890625, "training_acc": 47.0, "val_loss": 13534.785461425781, "val_acc": 52.0}
{"epoch": 7, "training_loss": 61846.09765625, "training_acc": 53.0, "val_loss": 18315.2099609375, "val_acc": 52.0}
{"epoch": 8, "training_loss": 60481.05126953125, "training_acc": 53.0, "val_loss": 2866.452407836914, "val_acc": 48.0}
{"epoch": 9, "training_loss": 19938.463012695312, "training_acc": 47.0, "val_loss": 5396.695327758789, "val_acc": 48.0}
{"epoch": 10, "training_loss": 16976.969268798828, "training_acc": 55.0, "val_loss": 6620.814514160156, "val_acc": 52.0}
{"epoch": 11, "training_loss": 21272.50323486328, "training_acc": 53.0, "val_loss": 6502.348327636719, "val_acc": 48.0}
{"epoch": 12, "training_loss": 28266.841552734375, "training_acc": 47.0, "val_loss": 2566.097068786621, "val_acc": 48.0}
{"epoch": 13, "training_loss": 18786.5185546875, "training_acc": 49.0, "val_loss": 12776.319885253906, "val_acc": 52.0}
{"epoch": 14, "training_loss": 47239.67370605469, "training_acc": 53.0, "val_loss": 2904.071617126465, "val_acc": 52.0}
{"epoch": 15, "training_loss": 23813.587524414062, "training_acc": 49.0, "val_loss": 15859.683227539062, "val_acc": 48.0}
{"epoch": 16, "training_loss": 61060.508056640625, "training_acc": 47.0, "val_loss": 4398.104476928711, "val_acc": 48.0}
{"epoch": 17, "training_loss": 24214.367797851562, "training_acc": 51.0, "val_loss": 16861.34490966797, "val_acc": 52.0}
{"epoch": 18, "training_loss": 68263.51953125, "training_acc": 53.0, "val_loss": 12675.579071044922, "val_acc": 52.0}
{"epoch": 19, "training_loss": 35840.372955322266, "training_acc": 53.0, "val_loss": 13809.231567382812, "val_acc": 48.0}
{"epoch": 20, "training_loss": 67977.28466796875, "training_acc": 47.0, "val_loss": 21365.548706054688, "val_acc": 48.0}
{"epoch": 21, "training_loss": 76377.62915039062, "training_acc": 47.0, "val_loss": 1153.526782989502, "val_acc": 48.0}
{"epoch": 22, "training_loss": 18936.450805664062, "training_acc": 57.0, "val_loss": 25963.43994140625, "val_acc": 52.0}
{"epoch": 23, "training_loss": 109278.83251953125, "training_acc": 53.0, "val_loss": 27856.695556640625, "val_acc": 52.0}
{"epoch": 24, "training_loss": 97406.0048828125, "training_acc": 53.0, "val_loss": 7134.2254638671875, "val_acc": 52.0}
{"epoch": 25, "training_loss": 30652.054443359375, "training_acc": 55.0, "val_loss": 21182.26776123047, "val_acc": 48.0}
{"epoch": 26, "training_loss": 87997.55053710938, "training_acc": 47.0, "val_loss": 19103.67889404297, "val_acc": 48.0}
{"epoch": 27, "training_loss": 61916.65100097656, "training_acc": 47.0, "val_loss": 6600.3875732421875, "val_acc": 52.0}
{"epoch": 28, "training_loss": 36333.026123046875, "training_acc": 53.0, "val_loss": 15967.953491210938, "val_acc": 52.0}
{"epoch": 29, "training_loss": 56912.635986328125, "training_acc": 53.0, "val_loss": 2650.1598358154297, "val_acc": 52.0}
{"epoch": 30, "training_loss": 25814.8017578125, "training_acc": 51.0, "val_loss": 20805.120849609375, "val_acc": 48.0}
{"epoch": 31, "training_loss": 85054.3564453125, "training_acc": 47.0, "val_loss": 14142.07763671875, "val_acc": 48.0}
{"epoch": 32, "training_loss": 38777.525062561035, "training_acc": 47.0, "val_loss": 16292.074584960938, "val_acc": 52.0}
{"epoch": 33, "training_loss": 72795.3056640625, "training_acc": 53.0, "val_loss": 29171.44775390625, "val_acc": 52.0}
{"epoch": 34, "training_loss": 111395.90283203125, "training_acc": 53.0, "val_loss": 19786.268615722656, "val_acc": 52.0}
{"epoch": 35, "training_loss": 59622.253662109375, "training_acc": 53.0, "val_loss": 8787.287139892578, "val_acc": 48.0}
{"epoch": 36, "training_loss": 46953.028564453125, "training_acc": 47.0, "val_loss": 19524.269104003906, "val_acc": 48.0}
{"epoch": 37, "training_loss": 71902.17785644531, "training_acc": 47.0, "val_loss": 2892.679214477539, "val_acc": 48.0}
{"epoch": 38, "training_loss": 31938.974365234375, "training_acc": 41.0, "val_loss": 21117.15545654297, "val_acc": 52.0}
{"epoch": 39, "training_loss": 87599.01416015625, "training_acc": 53.0, "val_loss": 18229.710388183594, "val_acc": 52.0}
{"epoch": 40, "training_loss": 56293.27404785156, "training_acc": 53.0, "val_loss": 7502.748107910156, "val_acc": 48.0}
