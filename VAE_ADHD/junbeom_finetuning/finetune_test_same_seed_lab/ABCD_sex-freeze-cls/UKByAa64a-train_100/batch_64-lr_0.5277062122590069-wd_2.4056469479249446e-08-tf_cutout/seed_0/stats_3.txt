"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 888.3278274536133, "training_acc": 51.0, "val_loss": 130.4360032081604, "val_acc": 52.0}
{"epoch": 1, "training_loss": 1234.712547302246, "training_acc": 48.0, "val_loss": 669.7057723999023, "val_acc": 48.0}
{"epoch": 2, "training_loss": 2572.6185607910156, "training_acc": 47.0, "val_loss": 151.1987328529358, "val_acc": 48.0}
{"epoch": 3, "training_loss": 798.9547538757324, "training_acc": 49.0, "val_loss": 499.44186210632324, "val_acc": 52.0}
{"epoch": 4, "training_loss": 1795.2041015625, "training_acc": 53.0, "val_loss": 467.3184871673584, "val_acc": 52.0}
{"epoch": 5, "training_loss": 1404.2597160339355, "training_acc": 53.0, "val_loss": 33.850061893463135, "val_acc": 40.0}
{"epoch": 6, "training_loss": 569.2859878540039, "training_acc": 49.0, "val_loss": 247.26910591125488, "val_acc": 48.0}
{"epoch": 7, "training_loss": 923.0775508880615, "training_acc": 47.0, "val_loss": 60.31624674797058, "val_acc": 52.0}
{"epoch": 8, "training_loss": 267.80513763427734, "training_acc": 61.0, "val_loss": 183.09932947158813, "val_acc": 52.0}
{"epoch": 9, "training_loss": 463.4668321609497, "training_acc": 54.0, "val_loss": 48.66412580013275, "val_acc": 48.0}
{"epoch": 10, "training_loss": 291.7339096069336, "training_acc": 47.0, "val_loss": 24.12232905626297, "val_acc": 40.0}
{"epoch": 11, "training_loss": 143.48480701446533, "training_acc": 61.0, "val_loss": 118.36026906967163, "val_acc": 52.0}
{"epoch": 12, "training_loss": 276.7008652687073, "training_acc": 52.0, "val_loss": 69.98868584632874, "val_acc": 48.0}
{"epoch": 13, "training_loss": 332.44203090667725, "training_acc": 47.0, "val_loss": 41.17588698863983, "val_acc": 52.0}
{"epoch": 14, "training_loss": 111.19893026351929, "training_acc": 56.0, "val_loss": 24.660587310791016, "val_acc": 56.0}
{"epoch": 15, "training_loss": 122.29786014556885, "training_acc": 58.0, "val_loss": 19.276587665081024, "val_acc": 48.0}
{"epoch": 16, "training_loss": 103.26155233383179, "training_acc": 59.0, "val_loss": 57.667380571365356, "val_acc": 52.0}
{"epoch": 17, "training_loss": 116.29737544059753, "training_acc": 62.0, "val_loss": 47.01451361179352, "val_acc": 48.0}
{"epoch": 18, "training_loss": 143.6925847530365, "training_acc": 58.0, "val_loss": 63.562458753585815, "val_acc": 52.0}
{"epoch": 19, "training_loss": 137.8754677772522, "training_acc": 56.0, "val_loss": 36.5146279335022, "val_acc": 48.0}
{"epoch": 20, "training_loss": 147.28790998458862, "training_acc": 46.0, "val_loss": 34.15008187294006, "val_acc": 52.0}
{"epoch": 21, "training_loss": 76.9995470046997, "training_acc": 64.0, "val_loss": 24.139192700386047, "val_acc": 36.0}
{"epoch": 22, "training_loss": 97.75630044937134, "training_acc": 49.0, "val_loss": 25.267955660820007, "val_acc": 48.0}
{"epoch": 23, "training_loss": 75.71096205711365, "training_acc": 62.0, "val_loss": 35.869938135147095, "val_acc": 52.0}
{"epoch": 24, "training_loss": 72.48933291435242, "training_acc": 63.0, "val_loss": 25.27523636817932, "val_acc": 40.0}
{"epoch": 25, "training_loss": 93.88530540466309, "training_acc": 57.0, "val_loss": 38.81069719791412, "val_acc": 52.0}
{"epoch": 26, "training_loss": 89.34858679771423, "training_acc": 55.0, "val_loss": 25.07610321044922, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.49150919914246, "training_acc": 66.0, "val_loss": 21.439258754253387, "val_acc": 60.0}
{"epoch": 28, "training_loss": 78.80306553840637, "training_acc": 52.0, "val_loss": 23.889346420764923, "val_acc": 52.0}
{"epoch": 29, "training_loss": 59.54491662979126, "training_acc": 65.0, "val_loss": 22.40283489227295, "val_acc": 52.0}
{"epoch": 30, "training_loss": 60.96303367614746, "training_acc": 70.0, "val_loss": 16.93519502878189, "val_acc": 52.0}
{"epoch": 31, "training_loss": 58.975670337677, "training_acc": 68.0, "val_loss": 17.74151772260666, "val_acc": 68.0}
{"epoch": 32, "training_loss": 62.06689214706421, "training_acc": 63.0, "val_loss": 16.71362817287445, "val_acc": 60.0}
{"epoch": 33, "training_loss": 71.77267169952393, "training_acc": 59.0, "val_loss": 63.506412506103516, "val_acc": 52.0}
{"epoch": 34, "training_loss": 165.5822114944458, "training_acc": 53.0, "val_loss": 44.777169823646545, "val_acc": 48.0}
{"epoch": 35, "training_loss": 145.66308760643005, "training_acc": 57.0, "val_loss": 63.34499716758728, "val_acc": 52.0}
{"epoch": 36, "training_loss": 133.29899215698242, "training_acc": 56.0, "val_loss": 80.94883561134338, "val_acc": 48.0}
{"epoch": 37, "training_loss": 329.97108364105225, "training_acc": 47.0, "val_loss": 63.48050236701965, "val_acc": 52.0}
{"epoch": 38, "training_loss": 169.83042764663696, "training_acc": 55.0, "val_loss": 17.985732853412628, "val_acc": 52.0}
{"epoch": 39, "training_loss": 107.11829328536987, "training_acc": 64.0, "val_loss": 39.2969936132431, "val_acc": 52.0}
{"epoch": 40, "training_loss": 94.93830895423889, "training_acc": 57.0, "val_loss": 29.76604700088501, "val_acc": 48.0}
{"epoch": 41, "training_loss": 105.14790940284729, "training_acc": 59.0, "val_loss": 56.264448165893555, "val_acc": 52.0}
{"epoch": 42, "training_loss": 104.95748543739319, "training_acc": 57.0, "val_loss": 43.93332600593567, "val_acc": 48.0}
{"epoch": 43, "training_loss": 162.56461453437805, "training_acc": 46.0, "val_loss": 20.662161707878113, "val_acc": 52.0}
{"epoch": 44, "training_loss": 84.2670259475708, "training_acc": 61.0, "val_loss": 34.60741639137268, "val_acc": 52.0}
{"epoch": 45, "training_loss": 84.51313805580139, "training_acc": 63.0, "val_loss": 28.098422288894653, "val_acc": 48.0}
{"epoch": 46, "training_loss": 113.00118637084961, "training_acc": 47.0, "val_loss": 18.680816888809204, "val_acc": 48.0}
{"epoch": 47, "training_loss": 78.33026266098022, "training_acc": 66.0, "val_loss": 38.22595179080963, "val_acc": 52.0}
{"epoch": 48, "training_loss": 69.26516461372375, "training_acc": 61.0, "val_loss": 27.966085076332092, "val_acc": 52.0}
{"epoch": 49, "training_loss": 102.11220097541809, "training_acc": 58.0, "val_loss": 34.31040048599243, "val_acc": 52.0}
{"epoch": 50, "training_loss": 70.58396911621094, "training_acc": 63.0, "val_loss": 21.18639647960663, "val_acc": 52.0}
{"epoch": 51, "training_loss": 57.33122205734253, "training_acc": 74.0, "val_loss": 19.148296117782593, "val_acc": 44.0}
