"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 1290.9926452636719, "training_acc": 42.0, "val_loss": 208.68923664093018, "val_acc": 48.0}
{"epoch": 1, "training_loss": 994.9849739074707, "training_acc": 49.0, "val_loss": 528.1355857849121, "val_acc": 52.0}
{"epoch": 2, "training_loss": 2000.6570205688477, "training_acc": 53.0, "val_loss": 391.695237159729, "val_acc": 52.0}
{"epoch": 3, "training_loss": 1125.0968379974365, "training_acc": 53.0, "val_loss": 158.52093696594238, "val_acc": 48.0}
{"epoch": 4, "training_loss": 909.8367881774902, "training_acc": 47.0, "val_loss": 325.47008991241455, "val_acc": 48.0}
{"epoch": 5, "training_loss": 1172.651969909668, "training_acc": 47.0, "val_loss": 33.41783881187439, "val_acc": 48.0}
{"epoch": 6, "training_loss": 354.55224990844727, "training_acc": 49.0, "val_loss": 330.87072372436523, "val_acc": 52.0}
{"epoch": 7, "training_loss": 1207.898136138916, "training_acc": 53.0, "val_loss": 314.7164821624756, "val_acc": 52.0}
{"epoch": 8, "training_loss": 979.0447444915771, "training_acc": 53.0, "val_loss": 37.23700940608978, "val_acc": 52.0}
{"epoch": 9, "training_loss": 352.62520599365234, "training_acc": 46.0, "val_loss": 249.6849536895752, "val_acc": 48.0}
{"epoch": 10, "training_loss": 1011.7516212463379, "training_acc": 47.0, "val_loss": 136.0689640045166, "val_acc": 48.0}
{"epoch": 11, "training_loss": 495.13258361816406, "training_acc": 34.0, "val_loss": 137.37324476242065, "val_acc": 52.0}
{"epoch": 12, "training_loss": 456.51934242248535, "training_acc": 53.0, "val_loss": 96.57790064811707, "val_acc": 52.0}
{"epoch": 13, "training_loss": 260.7356071472168, "training_acc": 45.0, "val_loss": 69.28751468658447, "val_acc": 48.0}
{"epoch": 14, "training_loss": 256.2437734603882, "training_acc": 48.0, "val_loss": 62.205666303634644, "val_acc": 52.0}
{"epoch": 15, "training_loss": 184.80613803863525, "training_acc": 53.0, "val_loss": 24.213974177837372, "val_acc": 48.0}
{"epoch": 16, "training_loss": 140.40518379211426, "training_acc": 50.0, "val_loss": 29.009398818016052, "val_acc": 52.0}
{"epoch": 17, "training_loss": 132.96166276931763, "training_acc": 49.0, "val_loss": 61.98685169219971, "val_acc": 52.0}
{"epoch": 18, "training_loss": 155.56402921676636, "training_acc": 48.0, "val_loss": 32.18032121658325, "val_acc": 48.0}
{"epoch": 19, "training_loss": 153.75531101226807, "training_acc": 39.0, "val_loss": 25.84371268749237, "val_acc": 52.0}
{"epoch": 20, "training_loss": 82.69166946411133, "training_acc": 55.0, "val_loss": 18.60891729593277, "val_acc": 52.0}
{"epoch": 21, "training_loss": 97.765860080719, "training_acc": 57.0, "val_loss": 21.22870534658432, "val_acc": 56.0}
{"epoch": 22, "training_loss": 93.61014151573181, "training_acc": 49.0, "val_loss": 41.50263965129852, "val_acc": 52.0}
{"epoch": 23, "training_loss": 111.88858079910278, "training_acc": 55.0, "val_loss": 29.742971062660217, "val_acc": 48.0}
{"epoch": 24, "training_loss": 118.92870330810547, "training_acc": 49.0, "val_loss": 51.19967460632324, "val_acc": 52.0}
{"epoch": 25, "training_loss": 119.97467136383057, "training_acc": 54.0, "val_loss": 22.164100408554077, "val_acc": 32.0}
{"epoch": 26, "training_loss": 87.56961107254028, "training_acc": 58.0, "val_loss": 36.08943819999695, "val_acc": 52.0}
{"epoch": 27, "training_loss": 84.79133033752441, "training_acc": 59.0, "val_loss": 22.046685218811035, "val_acc": 40.0}
{"epoch": 28, "training_loss": 79.23032212257385, "training_acc": 51.0, "val_loss": 50.52539110183716, "val_acc": 52.0}
{"epoch": 29, "training_loss": 120.25021386146545, "training_acc": 53.0, "val_loss": 36.16071343421936, "val_acc": 48.0}
{"epoch": 30, "training_loss": 130.9817135334015, "training_acc": 47.0, "val_loss": 27.479976415634155, "val_acc": 52.0}
{"epoch": 31, "training_loss": 77.30455350875854, "training_acc": 59.0, "val_loss": 16.91865175962448, "val_acc": 52.0}
{"epoch": 32, "training_loss": 64.01385807991028, "training_acc": 62.0, "val_loss": 17.161083221435547, "val_acc": 52.0}
{"epoch": 33, "training_loss": 59.762195110321045, "training_acc": 65.0, "val_loss": 17.545390129089355, "val_acc": 44.0}
{"epoch": 34, "training_loss": 56.34374666213989, "training_acc": 74.0, "val_loss": 25.679907202720642, "val_acc": 52.0}
{"epoch": 35, "training_loss": 55.88429594039917, "training_acc": 72.0, "val_loss": 19.835704565048218, "val_acc": 36.0}
{"epoch": 36, "training_loss": 65.49425554275513, "training_acc": 61.0, "val_loss": 23.15874993801117, "val_acc": 52.0}
{"epoch": 37, "training_loss": 52.10541212558746, "training_acc": 76.0, "val_loss": 19.691307842731476, "val_acc": 36.0}
{"epoch": 38, "training_loss": 68.59845304489136, "training_acc": 59.0, "val_loss": 20.465384423732758, "val_acc": 52.0}
{"epoch": 39, "training_loss": 62.05985164642334, "training_acc": 63.0, "val_loss": 20.492611825466156, "val_acc": 52.0}
{"epoch": 40, "training_loss": 63.53627014160156, "training_acc": 69.0, "val_loss": 18.07159036397934, "val_acc": 48.0}
{"epoch": 41, "training_loss": 60.896788120269775, "training_acc": 66.0, "val_loss": 17.98189878463745, "val_acc": 48.0}
{"epoch": 42, "training_loss": 60.255579710006714, "training_acc": 66.0, "val_loss": 46.12058401107788, "val_acc": 52.0}
{"epoch": 43, "training_loss": 106.29265546798706, "training_acc": 56.0, "val_loss": 37.67935037612915, "val_acc": 48.0}
{"epoch": 44, "training_loss": 115.32210755348206, "training_acc": 54.0, "val_loss": 54.08954620361328, "val_acc": 52.0}
{"epoch": 45, "training_loss": 114.40661692619324, "training_acc": 57.0, "val_loss": 37.93732225894928, "val_acc": 48.0}
{"epoch": 46, "training_loss": 130.32979321479797, "training_acc": 53.0, "val_loss": 76.0797381401062, "val_acc": 52.0}
{"epoch": 47, "training_loss": 218.42763710021973, "training_acc": 53.0, "val_loss": 39.500829577445984, "val_acc": 48.0}
{"epoch": 48, "training_loss": 170.1839919090271, "training_acc": 47.0, "val_loss": 39.77822661399841, "val_acc": 52.0}
{"epoch": 49, "training_loss": 139.35049438476562, "training_acc": 54.0, "val_loss": 24.400265514850616, "val_acc": 52.0}
{"epoch": 50, "training_loss": 107.84801626205444, "training_acc": 49.0, "val_loss": 46.09059393405914, "val_acc": 52.0}
