"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 2917006.3075447083, "training_acc": 46.0, "val_loss": 609719.23828125, "val_acc": 52.0}
{"epoch": 1, "training_loss": 2960711.984375, "training_acc": 53.0, "val_loss": 1579022.55859375, "val_acc": 48.0}
{"epoch": 2, "training_loss": 5942667.0625, "training_acc": 47.0, "val_loss": 437701.904296875, "val_acc": 48.0}
{"epoch": 3, "training_loss": 2070741.7734375, "training_acc": 51.0, "val_loss": 1153181.73828125, "val_acc": 52.0}
{"epoch": 4, "training_loss": 4381263.25, "training_acc": 53.0, "val_loss": 1029018.5546875, "val_acc": 52.0}
{"epoch": 5, "training_loss": 3054046.6640625, "training_acc": 53.0, "val_loss": 178790.58837890625, "val_acc": 48.0}
{"epoch": 6, "training_loss": 1647892.7265625, "training_acc": 47.0, "val_loss": 527055.419921875, "val_acc": 48.0}
{"epoch": 7, "training_loss": 1786507.09375, "training_acc": 47.0, "val_loss": 381904.0283203125, "val_acc": 52.0}
{"epoch": 8, "training_loss": 1489141.390625, "training_acc": 53.0, "val_loss": 662203.3203125, "val_acc": 52.0}
{"epoch": 9, "training_loss": 2150269.92578125, "training_acc": 53.0, "val_loss": 83446.96655273438, "val_acc": 52.0}
{"epoch": 10, "training_loss": 860006.6796875, "training_acc": 49.0, "val_loss": 615711.474609375, "val_acc": 48.0}
{"epoch": 11, "training_loss": 2359099.9296875, "training_acc": 47.0, "val_loss": 130608.21533203125, "val_acc": 48.0}
{"epoch": 12, "training_loss": 854751.61328125, "training_acc": 49.0, "val_loss": 647898.486328125, "val_acc": 52.0}
{"epoch": 13, "training_loss": 2490979.765625, "training_acc": 53.0, "val_loss": 485706.005859375, "val_acc": 52.0}
{"epoch": 14, "training_loss": 1303886.3115234375, "training_acc": 52.0, "val_loss": 476304.78515625, "val_acc": 48.0}
{"epoch": 15, "training_loss": 2320548.484375, "training_acc": 47.0, "val_loss": 708422.900390625, "val_acc": 48.0}
{"epoch": 16, "training_loss": 2390111.140625, "training_acc": 47.0, "val_loss": 64522.711181640625, "val_acc": 60.0}
{"epoch": 17, "training_loss": 593510.15625, "training_acc": 54.0, "val_loss": 402857.763671875, "val_acc": 52.0}
{"epoch": 18, "training_loss": 1293249.345703125, "training_acc": 53.0, "val_loss": 91286.41357421875, "val_acc": 48.0}
{"epoch": 19, "training_loss": 401879.375, "training_acc": 50.0, "val_loss": 43621.3623046875, "val_acc": 56.0}
{"epoch": 20, "training_loss": 293370.291015625, "training_acc": 60.0, "val_loss": 67568.76220703125, "val_acc": 60.0}
{"epoch": 21, "training_loss": 252460.181640625, "training_acc": 55.0, "val_loss": 72645.95336914062, "val_acc": 44.0}
{"epoch": 22, "training_loss": 351107.443359375, "training_acc": 55.0, "val_loss": 191590.71044921875, "val_acc": 52.0}
{"epoch": 23, "training_loss": 358809.8515625, "training_acc": 62.0, "val_loss": 188340.380859375, "val_acc": 48.0}
{"epoch": 24, "training_loss": 696828.93359375, "training_acc": 48.0, "val_loss": 153651.72119140625, "val_acc": 52.0}
{"epoch": 25, "training_loss": 497192.671875, "training_acc": 55.0, "val_loss": 68558.35571289062, "val_acc": 60.0}
{"epoch": 26, "training_loss": 509457.5625, "training_acc": 52.0, "val_loss": 168863.0859375, "val_acc": 48.0}
{"epoch": 27, "training_loss": 596569.4130859375, "training_acc": 49.0, "val_loss": 158771.32568359375, "val_acc": 52.0}
{"epoch": 28, "training_loss": 318313.0537109375, "training_acc": 58.0, "val_loss": 103047.15576171875, "val_acc": 48.0}
{"epoch": 29, "training_loss": 327391.7158203125, "training_acc": 58.0, "val_loss": 258747.94921875, "val_acc": 52.0}
{"epoch": 30, "training_loss": 886466.61328125, "training_acc": 53.0, "val_loss": 120238.22021484375, "val_acc": 56.0}
{"epoch": 31, "training_loss": 576639.07421875, "training_acc": 52.0, "val_loss": 261821.7041015625, "val_acc": 48.0}
{"epoch": 32, "training_loss": 785903.6967773438, "training_acc": 51.0, "val_loss": 294551.66015625, "val_acc": 52.0}
{"epoch": 33, "training_loss": 1175691.1328125, "training_acc": 53.0, "val_loss": 268909.375, "val_acc": 52.0}
{"epoch": 34, "training_loss": 497976.25244140625, "training_acc": 66.0, "val_loss": 224191.6259765625, "val_acc": 48.0}
{"epoch": 35, "training_loss": 823455.18359375, "training_acc": 47.0, "val_loss": 141544.15283203125, "val_acc": 52.0}
{"epoch": 36, "training_loss": 489619.373046875, "training_acc": 55.0, "val_loss": 79278.94897460938, "val_acc": 60.0}
{"epoch": 37, "training_loss": 357058.0234375, "training_acc": 49.0, "val_loss": 132488.00048828125, "val_acc": 48.0}
{"epoch": 38, "training_loss": 482273.4580078125, "training_acc": 48.0, "val_loss": 153323.73046875, "val_acc": 52.0}
