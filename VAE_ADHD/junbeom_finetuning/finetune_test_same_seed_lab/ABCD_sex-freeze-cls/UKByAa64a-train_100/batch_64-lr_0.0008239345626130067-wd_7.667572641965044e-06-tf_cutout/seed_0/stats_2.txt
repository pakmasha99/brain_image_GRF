"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 68.81675434112549, "training_acc": 52.0, "val_loss": 17.331576347351074, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.50331735610962, "training_acc": 53.0, "val_loss": 17.33313500881195, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.3093912601471, "training_acc": 53.0, "val_loss": 17.35161542892456, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.1093544960022, "training_acc": 53.0, "val_loss": 17.338252067565918, "val_acc": 52.0}
{"epoch": 4, "training_loss": 68.92885327339172, "training_acc": 53.0, "val_loss": 17.316417396068573, "val_acc": 52.0}
{"epoch": 5, "training_loss": 68.88566303253174, "training_acc": 53.0, "val_loss": 17.311327159404755, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.06917119026184, "training_acc": 53.0, "val_loss": 17.312151193618774, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.1445541381836, "training_acc": 53.0, "val_loss": 17.309436202049255, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.34393382072449, "training_acc": 53.0, "val_loss": 17.303888499736786, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.17068815231323, "training_acc": 53.0, "val_loss": 17.29949712753296, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.06650948524475, "training_acc": 53.0, "val_loss": 17.294126749038696, "val_acc": 52.0}
{"epoch": 11, "training_loss": 68.95349860191345, "training_acc": 53.0, "val_loss": 17.28505641222, "val_acc": 52.0}
{"epoch": 12, "training_loss": 68.55202913284302, "training_acc": 53.0, "val_loss": 17.277200520038605, "val_acc": 52.0}
{"epoch": 13, "training_loss": 68.92052888870239, "training_acc": 53.0, "val_loss": 17.272120714187622, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.01098704338074, "training_acc": 52.0, "val_loss": 17.266827821731567, "val_acc": 52.0}
{"epoch": 15, "training_loss": 68.75328421592712, "training_acc": 54.0, "val_loss": 17.262950539588928, "val_acc": 52.0}
{"epoch": 16, "training_loss": 68.74710893630981, "training_acc": 54.0, "val_loss": 17.260272800922394, "val_acc": 52.0}
{"epoch": 17, "training_loss": 68.53683567047119, "training_acc": 53.0, "val_loss": 17.25761592388153, "val_acc": 52.0}
{"epoch": 18, "training_loss": 68.86860656738281, "training_acc": 53.0, "val_loss": 17.25504994392395, "val_acc": 52.0}
{"epoch": 19, "training_loss": 68.74247550964355, "training_acc": 55.0, "val_loss": 17.253179848194122, "val_acc": 52.0}
{"epoch": 20, "training_loss": 68.86508512496948, "training_acc": 54.0, "val_loss": 17.256712913513184, "val_acc": 52.0}
{"epoch": 21, "training_loss": 68.99667429924011, "training_acc": 53.0, "val_loss": 17.26541668176651, "val_acc": 52.0}
{"epoch": 22, "training_loss": 68.81979250907898, "training_acc": 53.0, "val_loss": 17.26401448249817, "val_acc": 52.0}
{"epoch": 23, "training_loss": 68.75536060333252, "training_acc": 53.0, "val_loss": 17.267417907714844, "val_acc": 52.0}
{"epoch": 24, "training_loss": 68.75994920730591, "training_acc": 53.0, "val_loss": 17.266330122947693, "val_acc": 52.0}
{"epoch": 25, "training_loss": 68.78659772872925, "training_acc": 53.0, "val_loss": 17.26222187280655, "val_acc": 52.0}
{"epoch": 26, "training_loss": 68.74293446540833, "training_acc": 53.0, "val_loss": 17.253535985946655, "val_acc": 52.0}
{"epoch": 27, "training_loss": 68.64099931716919, "training_acc": 53.0, "val_loss": 17.251449823379517, "val_acc": 52.0}
{"epoch": 28, "training_loss": 68.71502995491028, "training_acc": 53.0, "val_loss": 17.25439727306366, "val_acc": 52.0}
{"epoch": 29, "training_loss": 68.6624903678894, "training_acc": 55.0, "val_loss": 17.257730662822723, "val_acc": 52.0}
{"epoch": 30, "training_loss": 68.53799176216125, "training_acc": 53.0, "val_loss": 17.26132184267044, "val_acc": 52.0}
{"epoch": 31, "training_loss": 68.59465765953064, "training_acc": 53.0, "val_loss": 17.26652830839157, "val_acc": 52.0}
{"epoch": 32, "training_loss": 68.56873393058777, "training_acc": 52.0, "val_loss": 17.27319061756134, "val_acc": 52.0}
{"epoch": 33, "training_loss": 68.68701267242432, "training_acc": 53.0, "val_loss": 17.278923094272614, "val_acc": 52.0}
{"epoch": 34, "training_loss": 68.1467592716217, "training_acc": 53.0, "val_loss": 17.285102605819702, "val_acc": 52.0}
{"epoch": 35, "training_loss": 68.85523533821106, "training_acc": 57.0, "val_loss": 17.28954315185547, "val_acc": 52.0}
{"epoch": 36, "training_loss": 68.7086455821991, "training_acc": 54.0, "val_loss": 17.2932431101799, "val_acc": 52.0}
{"epoch": 37, "training_loss": 68.46741437911987, "training_acc": 54.0, "val_loss": 17.29636937379837, "val_acc": 52.0}
{"epoch": 38, "training_loss": 68.21898937225342, "training_acc": 50.0, "val_loss": 17.30228364467621, "val_acc": 52.0}
{"epoch": 39, "training_loss": 68.60221409797668, "training_acc": 54.0, "val_loss": 17.318204045295715, "val_acc": 52.0}
{"epoch": 40, "training_loss": 68.16979646682739, "training_acc": 54.0, "val_loss": 17.33531653881073, "val_acc": 52.0}
{"epoch": 41, "training_loss": 68.7955551147461, "training_acc": 53.0, "val_loss": 17.336858808994293, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.079350233078, "training_acc": 53.0, "val_loss": 17.33597368001938, "val_acc": 52.0}
{"epoch": 43, "training_loss": 68.60624265670776, "training_acc": 53.0, "val_loss": 17.322060465812683, "val_acc": 52.0}
{"epoch": 44, "training_loss": 68.90203762054443, "training_acc": 53.0, "val_loss": 17.313367128372192, "val_acc": 52.0}
{"epoch": 45, "training_loss": 68.52098035812378, "training_acc": 53.0, "val_loss": 17.305341362953186, "val_acc": 52.0}
{"epoch": 46, "training_loss": 68.93467044830322, "training_acc": 53.0, "val_loss": 17.29726344347, "val_acc": 52.0}
