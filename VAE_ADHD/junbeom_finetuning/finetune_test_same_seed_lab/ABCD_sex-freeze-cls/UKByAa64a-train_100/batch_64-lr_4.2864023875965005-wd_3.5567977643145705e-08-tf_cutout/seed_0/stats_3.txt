"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 11868.292343139648, "training_acc": 43.0, "val_loss": 2486.195945739746, "val_acc": 48.0}
{"epoch": 1, "training_loss": 9196.366516113281, "training_acc": 53.0, "val_loss": 4640.903091430664, "val_acc": 52.0}
{"epoch": 2, "training_loss": 17517.35418701172, "training_acc": 53.0, "val_loss": 3027.520179748535, "val_acc": 52.0}
{"epoch": 3, "training_loss": 7445.562789916992, "training_acc": 53.0, "val_loss": 2702.0774841308594, "val_acc": 48.0}
{"epoch": 4, "training_loss": 12632.8408203125, "training_acc": 47.0, "val_loss": 4589.600372314453, "val_acc": 48.0}
{"epoch": 5, "training_loss": 16973.910552978516, "training_acc": 47.0, "val_loss": 1975.0688552856445, "val_acc": 48.0}
{"epoch": 6, "training_loss": 6205.710556030273, "training_acc": 45.0, "val_loss": 1585.3105545043945, "val_acc": 52.0}
{"epoch": 7, "training_loss": 6795.742095947266, "training_acc": 53.0, "val_loss": 1308.535099029541, "val_acc": 52.0}
{"epoch": 8, "training_loss": 3689.5986347198486, "training_acc": 54.0, "val_loss": 1555.6849479675293, "val_acc": 48.0}
{"epoch": 9, "training_loss": 7031.611785888672, "training_acc": 46.0, "val_loss": 1849.4531631469727, "val_acc": 48.0}
{"epoch": 10, "training_loss": 5604.479965209961, "training_acc": 48.0, "val_loss": 977.6408195495605, "val_acc": 52.0}
{"epoch": 11, "training_loss": 5104.715270996094, "training_acc": 53.0, "val_loss": 1901.740837097168, "val_acc": 52.0}
{"epoch": 12, "training_loss": 6316.316711425781, "training_acc": 53.0, "val_loss": 109.90133285522461, "val_acc": 48.0}
{"epoch": 13, "training_loss": 1914.2450714111328, "training_acc": 58.0, "val_loss": 1523.4328269958496, "val_acc": 48.0}
{"epoch": 14, "training_loss": 5398.958358764648, "training_acc": 48.0, "val_loss": 170.68886756896973, "val_acc": 48.0}
{"epoch": 15, "training_loss": 1582.1017303466797, "training_acc": 49.0, "val_loss": 1111.9646072387695, "val_acc": 52.0}
{"epoch": 16, "training_loss": 2941.835796356201, "training_acc": 53.0, "val_loss": 697.2677230834961, "val_acc": 48.0}
{"epoch": 17, "training_loss": 3358.292999267578, "training_acc": 47.0, "val_loss": 354.08992767333984, "val_acc": 48.0}
{"epoch": 18, "training_loss": 2192.679801940918, "training_acc": 47.0, "val_loss": 1393.0461883544922, "val_acc": 52.0}
{"epoch": 19, "training_loss": 4223.476181030273, "training_acc": 53.0, "val_loss": 334.34770107269287, "val_acc": 52.0}
{"epoch": 20, "training_loss": 1715.1007766723633, "training_acc": 52.0, "val_loss": 1159.5650672912598, "val_acc": 48.0}
{"epoch": 21, "training_loss": 4219.052734375, "training_acc": 47.0, "val_loss": 548.8713264465332, "val_acc": 52.0}
{"epoch": 22, "training_loss": 1719.798728942871, "training_acc": 52.0, "val_loss": 910.2006912231445, "val_acc": 52.0}
{"epoch": 23, "training_loss": 1806.3251399993896, "training_acc": 53.0, "val_loss": 715.6047344207764, "val_acc": 48.0}
{"epoch": 24, "training_loss": 3289.041976928711, "training_acc": 46.0, "val_loss": 159.7952127456665, "val_acc": 40.0}
{"epoch": 25, "training_loss": 1597.9099884033203, "training_acc": 60.0, "val_loss": 1086.4873886108398, "val_acc": 52.0}
{"epoch": 26, "training_loss": 2693.441635131836, "training_acc": 53.0, "val_loss": 634.0922832489014, "val_acc": 48.0}
{"epoch": 27, "training_loss": 3312.1075439453125, "training_acc": 47.0, "val_loss": 634.2767238616943, "val_acc": 48.0}
{"epoch": 28, "training_loss": 2259.4130821228027, "training_acc": 49.0, "val_loss": 799.3340969085693, "val_acc": 52.0}
{"epoch": 29, "training_loss": 2243.9686889648438, "training_acc": 53.0, "val_loss": 470.625638961792, "val_acc": 48.0}
{"epoch": 30, "training_loss": 2217.739517211914, "training_acc": 47.0, "val_loss": 86.33837699890137, "val_acc": 48.0}
{"epoch": 31, "training_loss": 960.6575775146484, "training_acc": 59.0, "val_loss": 872.1722602844238, "val_acc": 52.0}
{"epoch": 32, "training_loss": 2129.812479019165, "training_acc": 53.0, "val_loss": 764.3962383270264, "val_acc": 48.0}
{"epoch": 33, "training_loss": 3420.875961303711, "training_acc": 47.0, "val_loss": 456.7643165588379, "val_acc": 48.0}
{"epoch": 34, "training_loss": 2028.380111694336, "training_acc": 48.0, "val_loss": 1149.094295501709, "val_acc": 52.0}
{"epoch": 35, "training_loss": 3291.733139038086, "training_acc": 53.0, "val_loss": 120.44912576675415, "val_acc": 44.0}
{"epoch": 36, "training_loss": 1143.1684875488281, "training_acc": 51.0, "val_loss": 129.99626398086548, "val_acc": 40.0}
{"epoch": 37, "training_loss": 1295.8418045043945, "training_acc": 56.0, "val_loss": 865.1774406433105, "val_acc": 52.0}
{"epoch": 38, "training_loss": 1831.6236782073975, "training_acc": 55.0, "val_loss": 564.4862651824951, "val_acc": 48.0}
{"epoch": 39, "training_loss": 2586.706443786621, "training_acc": 47.0, "val_loss": 167.60188341140747, "val_acc": 52.0}
{"epoch": 40, "training_loss": 1233.8596954345703, "training_acc": 61.0, "val_loss": 885.4707717895508, "val_acc": 52.0}
{"epoch": 41, "training_loss": 1918.2516269683838, "training_acc": 54.0, "val_loss": 543.5410499572754, "val_acc": 48.0}
{"epoch": 42, "training_loss": 2457.111473083496, "training_acc": 47.0, "val_loss": 306.99729919433594, "val_acc": 52.0}
{"epoch": 43, "training_loss": 1297.2828979492188, "training_acc": 55.0, "val_loss": 412.531042098999, "val_acc": 52.0}
{"epoch": 44, "training_loss": 1220.406982421875, "training_acc": 55.0, "val_loss": 454.95924949645996, "val_acc": 48.0}
{"epoch": 45, "training_loss": 1417.4448642730713, "training_acc": 53.0, "val_loss": 963.0023002624512, "val_acc": 52.0}
{"epoch": 46, "training_loss": 3079.266860961914, "training_acc": 53.0, "val_loss": 358.40017795562744, "val_acc": 52.0}
{"epoch": 47, "training_loss": 1213.2030715942383, "training_acc": 59.0, "val_loss": 948.5135078430176, "val_acc": 48.0}
{"epoch": 48, "training_loss": 3288.8427658081055, "training_acc": 47.0, "val_loss": 781.50315284729, "val_acc": 52.0}
{"epoch": 49, "training_loss": 2430.903045654297, "training_acc": 53.0, "val_loss": 996.4216232299805, "val_acc": 52.0}
