"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 9802.037200927734, "training_acc": 47.0, "val_loss": 1475.1635551452637, "val_acc": 48.0}
{"epoch": 1, "training_loss": 8854.599304199219, "training_acc": 49.0, "val_loss": 5573.806381225586, "val_acc": 52.0}
{"epoch": 2, "training_loss": 21385.99346923828, "training_acc": 53.0, "val_loss": 3674.9523162841797, "val_acc": 52.0}
{"epoch": 3, "training_loss": 10463.056564331055, "training_acc": 53.0, "val_loss": 2218.61515045166, "val_acc": 48.0}
{"epoch": 4, "training_loss": 10750.856384277344, "training_acc": 47.0, "val_loss": 4106.484222412109, "val_acc": 48.0}
{"epoch": 5, "training_loss": 13942.254211425781, "training_acc": 47.0, "val_loss": 1411.4429473876953, "val_acc": 48.0}
{"epoch": 6, "training_loss": 5131.891799926758, "training_acc": 49.0, "val_loss": 2235.7025146484375, "val_acc": 52.0}
{"epoch": 7, "training_loss": 9757.667510986328, "training_acc": 53.0, "val_loss": 2077.0477294921875, "val_acc": 52.0}
{"epoch": 8, "training_loss": 7118.503555297852, "training_acc": 53.0, "val_loss": 897.0510482788086, "val_acc": 44.0}
{"epoch": 9, "training_loss": 4494.478561401367, "training_acc": 52.0, "val_loss": 2159.087371826172, "val_acc": 48.0}
{"epoch": 10, "training_loss": 7397.035064697266, "training_acc": 49.0, "val_loss": 390.9229278564453, "val_acc": 56.0}
{"epoch": 11, "training_loss": 2956.813217163086, "training_acc": 56.0, "val_loss": 1906.4630508422852, "val_acc": 52.0}
{"epoch": 12, "training_loss": 8007.634826660156, "training_acc": 53.0, "val_loss": 1179.5819282531738, "val_acc": 52.0}
{"epoch": 13, "training_loss": 3177.956901550293, "training_acc": 53.0, "val_loss": 1231.6580772399902, "val_acc": 48.0}
{"epoch": 14, "training_loss": 5175.9921875, "training_acc": 47.0, "val_loss": 912.5334739685059, "val_acc": 48.0}
{"epoch": 15, "training_loss": 2448.792537689209, "training_acc": 54.0, "val_loss": 649.5785236358643, "val_acc": 52.0}
{"epoch": 16, "training_loss": 2394.5134048461914, "training_acc": 53.0, "val_loss": 509.82675552368164, "val_acc": 48.0}
{"epoch": 17, "training_loss": 2089.4652404785156, "training_acc": 49.0, "val_loss": 87.29528784751892, "val_acc": 56.0}
{"epoch": 18, "training_loss": 1168.481300354004, "training_acc": 55.0, "val_loss": 249.17254447937012, "val_acc": 52.0}
{"epoch": 19, "training_loss": 1078.9750366210938, "training_acc": 56.0, "val_loss": 579.2064189910889, "val_acc": 48.0}
{"epoch": 20, "training_loss": 1776.8532962799072, "training_acc": 46.0, "val_loss": 307.96940326690674, "val_acc": 52.0}
{"epoch": 21, "training_loss": 779.7597389221191, "training_acc": 62.0, "val_loss": 577.0244121551514, "val_acc": 48.0}
{"epoch": 22, "training_loss": 1798.5409507751465, "training_acc": 47.0, "val_loss": 620.5183029174805, "val_acc": 52.0}
{"epoch": 23, "training_loss": 2474.8424224853516, "training_acc": 53.0, "val_loss": 378.95469665527344, "val_acc": 52.0}
{"epoch": 24, "training_loss": 1220.4949531555176, "training_acc": 63.0, "val_loss": 897.9162216186523, "val_acc": 48.0}
{"epoch": 25, "training_loss": 2561.833683013916, "training_acc": 47.0, "val_loss": 626.1034965515137, "val_acc": 52.0}
{"epoch": 26, "training_loss": 2789.4817657470703, "training_acc": 53.0, "val_loss": 531.8695545196533, "val_acc": 52.0}
{"epoch": 27, "training_loss": 1658.4254341125488, "training_acc": 54.0, "val_loss": 707.108211517334, "val_acc": 48.0}
{"epoch": 28, "training_loss": 1819.2781066894531, "training_acc": 54.0, "val_loss": 688.4222984313965, "val_acc": 52.0}
{"epoch": 29, "training_loss": 3072.3885650634766, "training_acc": 53.0, "val_loss": 620.687198638916, "val_acc": 52.0}
{"epoch": 30, "training_loss": 1581.2771606445312, "training_acc": 55.0, "val_loss": 502.0415782928467, "val_acc": 48.0}
{"epoch": 31, "training_loss": 1478.884241104126, "training_acc": 52.0, "val_loss": 181.4857006072998, "val_acc": 52.0}
{"epoch": 32, "training_loss": 552.5402011871338, "training_acc": 65.0, "val_loss": 73.75518679618835, "val_acc": 56.0}
{"epoch": 33, "training_loss": 686.4618644714355, "training_acc": 52.0, "val_loss": 265.2448892593384, "val_acc": 48.0}
{"epoch": 34, "training_loss": 815.2599811553955, "training_acc": 52.0, "val_loss": 86.50041222572327, "val_acc": 60.0}
{"epoch": 35, "training_loss": 338.0412302017212, "training_acc": 71.0, "val_loss": 133.48467350006104, "val_acc": 44.0}
{"epoch": 36, "training_loss": 276.27078914642334, "training_acc": 66.0, "val_loss": 194.20686960220337, "val_acc": 52.0}
{"epoch": 37, "training_loss": 424.12380599975586, "training_acc": 64.0, "val_loss": 45.32287120819092, "val_acc": 64.0}
{"epoch": 38, "training_loss": 210.70505619049072, "training_acc": 68.0, "val_loss": 59.75319743156433, "val_acc": 56.0}
{"epoch": 39, "training_loss": 245.31013011932373, "training_acc": 69.0, "val_loss": 46.113321185112, "val_acc": 56.0}
{"epoch": 40, "training_loss": 116.47123527526855, "training_acc": 72.0, "val_loss": 113.87319564819336, "val_acc": 52.0}
{"epoch": 41, "training_loss": 576.697826385498, "training_acc": 59.0, "val_loss": 227.70135402679443, "val_acc": 52.0}
{"epoch": 42, "training_loss": 502.07337045669556, "training_acc": 58.0, "val_loss": 234.83521938323975, "val_acc": 48.0}
{"epoch": 43, "training_loss": 1260.9798736572266, "training_acc": 41.0, "val_loss": 135.8642339706421, "val_acc": 52.0}
{"epoch": 44, "training_loss": 951.73779296875, "training_acc": 54.0, "val_loss": 531.4542770385742, "val_acc": 48.0}
{"epoch": 45, "training_loss": 1459.9567222595215, "training_acc": 55.0, "val_loss": 374.3141174316406, "val_acc": 52.0}
{"epoch": 46, "training_loss": 909.8685054779053, "training_acc": 53.0, "val_loss": 74.96780753135681, "val_acc": 56.0}
{"epoch": 47, "training_loss": 132.07317399978638, "training_acc": 71.0, "val_loss": 375.7582426071167, "val_acc": 48.0}
{"epoch": 48, "training_loss": 1230.359094619751, "training_acc": 45.0, "val_loss": 35.53005754947662, "val_acc": 60.0}
{"epoch": 49, "training_loss": 143.97636890411377, "training_acc": 71.0, "val_loss": 140.44113159179688, "val_acc": 48.0}
{"epoch": 50, "training_loss": 537.6070861816406, "training_acc": 57.0, "val_loss": 124.82056617736816, "val_acc": 52.0}
{"epoch": 51, "training_loss": 1067.7806930541992, "training_acc": 56.0, "val_loss": 308.27693939208984, "val_acc": 48.0}
{"epoch": 52, "training_loss": 1949.3087615966797, "training_acc": 40.0, "val_loss": 821.9734191894531, "val_acc": 52.0}
{"epoch": 53, "training_loss": 2098.4856309890747, "training_acc": 56.0, "val_loss": 1010.4238510131836, "val_acc": 48.0}
{"epoch": 54, "training_loss": 4284.386825561523, "training_acc": 47.0, "val_loss": 611.2492561340332, "val_acc": 48.0}
{"epoch": 55, "training_loss": 2084.019287109375, "training_acc": 55.0, "val_loss": 1156.695556640625, "val_acc": 52.0}
{"epoch": 56, "training_loss": 3773.863052368164, "training_acc": 53.0, "val_loss": 138.6166214942932, "val_acc": 60.0}
{"epoch": 57, "training_loss": 1443.162338256836, "training_acc": 58.0, "val_loss": 821.8441009521484, "val_acc": 48.0}
{"epoch": 58, "training_loss": 2292.6387367248535, "training_acc": 54.0, "val_loss": 595.8627700805664, "val_acc": 52.0}
{"epoch": 59, "training_loss": 1534.6815605163574, "training_acc": 56.0, "val_loss": 274.910831451416, "val_acc": 48.0}
{"epoch": 60, "training_loss": 1040.2418518066406, "training_acc": 48.0, "val_loss": 622.5146293640137, "val_acc": 52.0}
{"epoch": 61, "training_loss": 2047.5258178710938, "training_acc": 53.0, "val_loss": 128.86126041412354, "val_acc": 56.0}
{"epoch": 62, "training_loss": 967.0604553222656, "training_acc": 55.0, "val_loss": 482.8321933746338, "val_acc": 48.0}
{"epoch": 63, "training_loss": 1567.374397277832, "training_acc": 51.0, "val_loss": 522.7397441864014, "val_acc": 52.0}
{"epoch": 64, "training_loss": 950.5336925983429, "training_acc": 67.0, "val_loss": 420.1232433319092, "val_acc": 48.0}
{"epoch": 65, "training_loss": 1129.0904550552368, "training_acc": 54.0, "val_loss": 649.1900444030762, "val_acc": 52.0}
{"epoch": 66, "training_loss": 2149.439224243164, "training_acc": 53.0, "val_loss": 83.56732130050659, "val_acc": 60.0}
{"epoch": 67, "training_loss": 626.8590698242188, "training_acc": 56.0, "val_loss": 79.4357419013977, "val_acc": 56.0}
