"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 2375.439254760742, "training_acc": 54.0, "val_loss": 62.6262366771698, "val_acc": 56.0}
{"epoch": 1, "training_loss": 2714.6024169921875, "training_acc": 50.0, "val_loss": 2230.0790786743164, "val_acc": 44.0}
{"epoch": 2, "training_loss": 7148.129196166992, "training_acc": 48.0, "val_loss": 425.27685165405273, "val_acc": 44.0}
{"epoch": 3, "training_loss": 2357.1826782226562, "training_acc": 46.0, "val_loss": 1365.8733367919922, "val_acc": 56.0}
{"epoch": 4, "training_loss": 5889.958084106445, "training_acc": 52.0, "val_loss": 1209.9924087524414, "val_acc": 56.0}
{"epoch": 5, "training_loss": 4228.187210083008, "training_acc": 52.0, "val_loss": 74.67764616012573, "val_acc": 40.0}
{"epoch": 6, "training_loss": 1164.1020050048828, "training_acc": 52.0, "val_loss": 1045.8806991577148, "val_acc": 44.0}
{"epoch": 7, "training_loss": 3519.590072631836, "training_acc": 48.0, "val_loss": 415.58451652526855, "val_acc": 44.0}
{"epoch": 8, "training_loss": 1641.6713027954102, "training_acc": 42.0, "val_loss": 579.027795791626, "val_acc": 56.0}
{"epoch": 9, "training_loss": 2373.340103149414, "training_acc": 52.0, "val_loss": 347.62158393859863, "val_acc": 56.0}
{"epoch": 10, "training_loss": 1076.6460628509521, "training_acc": 55.0, "val_loss": 396.35229110717773, "val_acc": 44.0}
{"epoch": 11, "training_loss": 1200.6012153625488, "training_acc": 48.0, "val_loss": 82.02848434448242, "val_acc": 56.0}
{"epoch": 12, "training_loss": 391.30239486694336, "training_acc": 56.0, "val_loss": 106.66302442550659, "val_acc": 56.0}
{"epoch": 13, "training_loss": 412.0393371582031, "training_acc": 50.0, "val_loss": 134.45810079574585, "val_acc": 36.0}
{"epoch": 14, "training_loss": 406.6123867034912, "training_acc": 57.0, "val_loss": 172.23491668701172, "val_acc": 52.0}
{"epoch": 15, "training_loss": 526.1416463851929, "training_acc": 52.0, "val_loss": 170.71337699890137, "val_acc": 40.0}
{"epoch": 16, "training_loss": 452.70560455322266, "training_acc": 56.0, "val_loss": 124.5867133140564, "val_acc": 56.0}
{"epoch": 17, "training_loss": 287.41501665115356, "training_acc": 62.0, "val_loss": 97.35209345817566, "val_acc": 40.0}
{"epoch": 18, "training_loss": 219.53263092041016, "training_acc": 58.0, "val_loss": 82.93097019195557, "val_acc": 56.0}
{"epoch": 19, "training_loss": 235.81169319152832, "training_acc": 56.0, "val_loss": 54.120904207229614, "val_acc": 60.0}
{"epoch": 20, "training_loss": 203.3752408027649, "training_acc": 66.0, "val_loss": 55.0598680973053, "val_acc": 60.0}
{"epoch": 21, "training_loss": 206.91740322113037, "training_acc": 54.0, "val_loss": 62.82870173454285, "val_acc": 56.0}
{"epoch": 22, "training_loss": 171.397141456604, "training_acc": 63.0, "val_loss": 187.9370093345642, "val_acc": 44.0}
{"epoch": 23, "training_loss": 503.6812915802002, "training_acc": 54.0, "val_loss": 147.41787910461426, "val_acc": 56.0}
{"epoch": 24, "training_loss": 601.8068332672119, "training_acc": 52.0, "val_loss": 203.5414457321167, "val_acc": 44.0}
{"epoch": 25, "training_loss": 599.3961772918701, "training_acc": 50.0, "val_loss": 49.422988295555115, "val_acc": 64.0}
{"epoch": 26, "training_loss": 452.66929054260254, "training_acc": 56.0, "val_loss": 124.42542314529419, "val_acc": 44.0}
{"epoch": 27, "training_loss": 334.0349464416504, "training_acc": 51.0, "val_loss": 122.18190431594849, "val_acc": 56.0}
{"epoch": 28, "training_loss": 455.65769958496094, "training_acc": 53.0, "val_loss": 171.60362005233765, "val_acc": 44.0}
{"epoch": 29, "training_loss": 470.6467685699463, "training_acc": 49.0, "val_loss": 157.56210088729858, "val_acc": 56.0}
{"epoch": 30, "training_loss": 762.7482872009277, "training_acc": 52.0, "val_loss": 30.72914481163025, "val_acc": 56.0}
{"epoch": 31, "training_loss": 432.00925064086914, "training_acc": 50.0, "val_loss": 70.35346627235413, "val_acc": 44.0}
{"epoch": 32, "training_loss": 496.1580276489258, "training_acc": 52.0, "val_loss": 290.2195930480957, "val_acc": 56.0}
{"epoch": 33, "training_loss": 942.5944366455078, "training_acc": 51.0, "val_loss": 317.93150901794434, "val_acc": 44.0}
{"epoch": 34, "training_loss": 1238.2649307250977, "training_acc": 48.0, "val_loss": 207.29763507843018, "val_acc": 44.0}
{"epoch": 35, "training_loss": 711.1502170562744, "training_acc": 50.0, "val_loss": 332.2965621948242, "val_acc": 56.0}
{"epoch": 36, "training_loss": 1166.0212116241455, "training_acc": 52.0, "val_loss": 110.7886791229248, "val_acc": 40.0}
{"epoch": 37, "training_loss": 492.2609748840332, "training_acc": 50.0, "val_loss": 41.726118326187134, "val_acc": 56.0}
{"epoch": 38, "training_loss": 185.34830284118652, "training_acc": 67.0, "val_loss": 136.53663396835327, "val_acc": 52.0}
{"epoch": 39, "training_loss": 357.0571451187134, "training_acc": 59.0, "val_loss": 135.12667417526245, "val_acc": 44.0}
{"epoch": 40, "training_loss": 367.2353210449219, "training_acc": 53.0, "val_loss": 109.02736186981201, "val_acc": 56.0}
{"epoch": 41, "training_loss": 258.51362466812134, "training_acc": 63.0, "val_loss": 89.37084674835205, "val_acc": 44.0}
{"epoch": 42, "training_loss": 285.64881706237793, "training_acc": 57.0, "val_loss": 58.72960686683655, "val_acc": 56.0}
{"epoch": 43, "training_loss": 199.15479946136475, "training_acc": 61.0, "val_loss": 61.405014991760254, "val_acc": 48.0}
{"epoch": 44, "training_loss": 279.458571434021, "training_acc": 56.0, "val_loss": 150.05522966384888, "val_acc": 52.0}
{"epoch": 45, "training_loss": 318.9125361442566, "training_acc": 65.0, "val_loss": 130.08651733398438, "val_acc": 44.0}
{"epoch": 46, "training_loss": 357.8441848754883, "training_acc": 54.0, "val_loss": 128.9853811264038, "val_acc": 52.0}
{"epoch": 47, "training_loss": 315.4256753921509, "training_acc": 60.0, "val_loss": 59.52795743942261, "val_acc": 52.0}
{"epoch": 48, "training_loss": 276.57491874694824, "training_acc": 59.0, "val_loss": 75.99931955337524, "val_acc": 56.0}
{"epoch": 49, "training_loss": 296.024866104126, "training_acc": 58.0, "val_loss": 73.17720651626587, "val_acc": 52.0}
