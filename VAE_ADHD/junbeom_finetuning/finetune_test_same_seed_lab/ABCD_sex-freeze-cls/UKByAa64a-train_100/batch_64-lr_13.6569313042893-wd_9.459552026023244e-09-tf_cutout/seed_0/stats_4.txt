"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 31275.36920928955, "training_acc": 45.0, "val_loss": 7715.657806396484, "val_acc": 52.0}
{"epoch": 1, "training_loss": 28519.871215820312, "training_acc": 55.0, "val_loss": 12540.675354003906, "val_acc": 48.0}
{"epoch": 2, "training_loss": 45825.20178222656, "training_acc": 47.0, "val_loss": 1077.651023864746, "val_acc": 48.0}
{"epoch": 3, "training_loss": 15246.630004882812, "training_acc": 49.0, "val_loss": 13535.435485839844, "val_acc": 52.0}
{"epoch": 4, "training_loss": 53123.12890625, "training_acc": 53.0, "val_loss": 11458.243560791016, "val_acc": 52.0}
{"epoch": 5, "training_loss": 37480.75469970703, "training_acc": 53.0, "val_loss": 1115.6569480895996, "val_acc": 48.0}
{"epoch": 6, "training_loss": 11057.310668945312, "training_acc": 47.0, "val_loss": 4315.107727050781, "val_acc": 48.0}
{"epoch": 7, "training_loss": 12409.235580444336, "training_acc": 47.0, "val_loss": 4718.545913696289, "val_acc": 52.0}
{"epoch": 8, "training_loss": 22064.9697265625, "training_acc": 53.0, "val_loss": 7170.117950439453, "val_acc": 52.0}
{"epoch": 9, "training_loss": 22563.635620117188, "training_acc": 53.0, "val_loss": 591.9511795043945, "val_acc": 60.0}
{"epoch": 10, "training_loss": 9317.675659179688, "training_acc": 51.0, "val_loss": 6980.3314208984375, "val_acc": 48.0}
{"epoch": 11, "training_loss": 27776.935913085938, "training_acc": 47.0, "val_loss": 3002.3569107055664, "val_acc": 48.0}
{"epoch": 12, "training_loss": 12263.232116699219, "training_acc": 45.0, "val_loss": 4798.687362670898, "val_acc": 52.0}
{"epoch": 13, "training_loss": 17771.76446533203, "training_acc": 53.0, "val_loss": 2907.5199127197266, "val_acc": 52.0}
{"epoch": 14, "training_loss": 10139.034637451172, "training_acc": 42.0, "val_loss": 2286.7889404296875, "val_acc": 48.0}
{"epoch": 15, "training_loss": 7041.972644805908, "training_acc": 52.0, "val_loss": 1642.8340911865234, "val_acc": 52.0}
{"epoch": 16, "training_loss": 6287.567047119141, "training_acc": 53.0, "val_loss": 430.3456783294678, "val_acc": 48.0}
{"epoch": 17, "training_loss": 2785.5422973632812, "training_acc": 54.0, "val_loss": 712.528133392334, "val_acc": 52.0}
{"epoch": 18, "training_loss": 1966.3928604125977, "training_acc": 59.0, "val_loss": 996.1709022521973, "val_acc": 48.0}
{"epoch": 19, "training_loss": 2511.7095642089844, "training_acc": 54.0, "val_loss": 1517.6711082458496, "val_acc": 52.0}
{"epoch": 20, "training_loss": 5716.196517944336, "training_acc": 53.0, "val_loss": 925.0778198242188, "val_acc": 44.0}
{"epoch": 21, "training_loss": 3219.049072265625, "training_acc": 55.0, "val_loss": 477.3714542388916, "val_acc": 52.0}
{"epoch": 22, "training_loss": 2191.6459045410156, "training_acc": 57.0, "val_loss": 267.7715301513672, "val_acc": 48.0}
{"epoch": 23, "training_loss": 1138.2541236877441, "training_acc": 60.0, "val_loss": 338.8987064361572, "val_acc": 44.0}
{"epoch": 24, "training_loss": 1300.7935237884521, "training_acc": 63.0, "val_loss": 237.21861839294434, "val_acc": 60.0}
{"epoch": 25, "training_loss": 1009.1409873962402, "training_acc": 69.0, "val_loss": 273.14815521240234, "val_acc": 52.0}
{"epoch": 26, "training_loss": 1667.7891006469727, "training_acc": 58.0, "val_loss": 321.5672492980957, "val_acc": 40.0}
{"epoch": 27, "training_loss": 1726.8946380615234, "training_acc": 54.0, "val_loss": 195.93361616134644, "val_acc": 48.0}
{"epoch": 28, "training_loss": 597.382532119751, "training_acc": 66.0, "val_loss": 211.12589836120605, "val_acc": 44.0}
{"epoch": 29, "training_loss": 792.0012092590332, "training_acc": 67.0, "val_loss": 790.3968334197998, "val_acc": 48.0}
{"epoch": 30, "training_loss": 2690.647247314453, "training_acc": 47.0, "val_loss": 595.4577922821045, "val_acc": 44.0}
{"epoch": 31, "training_loss": 1794.9200077056885, "training_acc": 59.0, "val_loss": 195.80118656158447, "val_acc": 56.0}
{"epoch": 32, "training_loss": 1350.437843322754, "training_acc": 64.0, "val_loss": 659.6895694732666, "val_acc": 52.0}
{"epoch": 33, "training_loss": 1575.5269317626953, "training_acc": 55.0, "val_loss": 624.315595626831, "val_acc": 52.0}
{"epoch": 34, "training_loss": 2983.3819122314453, "training_acc": 43.0, "val_loss": 613.505220413208, "val_acc": 52.0}
{"epoch": 35, "training_loss": 1178.6867752075195, "training_acc": 62.0, "val_loss": 585.4411602020264, "val_acc": 48.0}
{"epoch": 36, "training_loss": 3636.2283630371094, "training_acc": 46.0, "val_loss": 557.7502250671387, "val_acc": 52.0}
{"epoch": 37, "training_loss": 2946.6588745117188, "training_acc": 58.0, "val_loss": 1248.6167907714844, "val_acc": 48.0}
{"epoch": 38, "training_loss": 6366.125762939453, "training_acc": 41.0, "val_loss": 1842.54150390625, "val_acc": 52.0}
{"epoch": 39, "training_loss": 4722.548572540283, "training_acc": 51.0, "val_loss": 265.9514904022217, "val_acc": 44.0}
{"epoch": 40, "training_loss": 2251.0152435302734, "training_acc": 57.0, "val_loss": 762.2120380401611, "val_acc": 52.0}
{"epoch": 41, "training_loss": 2397.9754638671875, "training_acc": 64.0, "val_loss": 1802.962875366211, "val_acc": 48.0}
{"epoch": 42, "training_loss": 4624.210803985596, "training_acc": 52.0, "val_loss": 627.7605533599854, "val_acc": 52.0}
{"epoch": 43, "training_loss": 2931.9361572265625, "training_acc": 50.0, "val_loss": 247.23742008209229, "val_acc": 60.0}
{"epoch": 44, "training_loss": 3242.805419921875, "training_acc": 53.0, "val_loss": 1477.4127960205078, "val_acc": 52.0}
{"epoch": 45, "training_loss": 6653.444976806641, "training_acc": 41.0, "val_loss": 886.6876602172852, "val_acc": 44.0}
{"epoch": 46, "training_loss": 3189.2565002441406, "training_acc": 56.0, "val_loss": 1688.5414123535156, "val_acc": 52.0}
{"epoch": 47, "training_loss": 3831.511833190918, "training_acc": 58.0, "val_loss": 1868.9802169799805, "val_acc": 48.0}
{"epoch": 48, "training_loss": 5878.388488769531, "training_acc": 49.0, "val_loss": 1918.2621002197266, "val_acc": 52.0}
{"epoch": 49, "training_loss": 7205.611572265625, "training_acc": 53.0, "val_loss": 499.6386528015137, "val_acc": 48.0}
{"epoch": 50, "training_loss": 3737.0189208984375, "training_acc": 64.0, "val_loss": 3639.716339111328, "val_acc": 48.0}
