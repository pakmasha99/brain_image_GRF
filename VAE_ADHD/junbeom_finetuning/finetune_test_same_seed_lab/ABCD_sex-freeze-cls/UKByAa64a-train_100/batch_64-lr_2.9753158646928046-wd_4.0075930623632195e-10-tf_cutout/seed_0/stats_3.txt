"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 6317.348854064941, "training_acc": 46.0, "val_loss": 1471.5497970581055, "val_acc": 52.0}
{"epoch": 1, "training_loss": 5135.220855712891, "training_acc": 61.0, "val_loss": 3394.3191528320312, "val_acc": 48.0}
{"epoch": 2, "training_loss": 12757.751922607422, "training_acc": 47.0, "val_loss": 929.5888900756836, "val_acc": 48.0}
{"epoch": 3, "training_loss": 4184.51579284668, "training_acc": 53.0, "val_loss": 2660.442543029785, "val_acc": 52.0}
{"epoch": 4, "training_loss": 9596.803161621094, "training_acc": 53.0, "val_loss": 2391.1834716796875, "val_acc": 52.0}
{"epoch": 5, "training_loss": 6023.523880004883, "training_acc": 52.0, "val_loss": 599.4072914123535, "val_acc": 48.0}
{"epoch": 6, "training_loss": 4638.8963623046875, "training_acc": 47.0, "val_loss": 1421.261215209961, "val_acc": 48.0}
{"epoch": 7, "training_loss": 5803.612930297852, "training_acc": 47.0, "val_loss": 577.2914409637451, "val_acc": 52.0}
{"epoch": 8, "training_loss": 2367.61181640625, "training_acc": 49.0, "val_loss": 1450.9212493896484, "val_acc": 52.0}
{"epoch": 9, "training_loss": 4099.637481689453, "training_acc": 54.0, "val_loss": 424.26276206970215, "val_acc": 48.0}
{"epoch": 10, "training_loss": 1987.8636169433594, "training_acc": 57.0, "val_loss": 931.7994117736816, "val_acc": 48.0}
{"epoch": 11, "training_loss": 3740.7626571655273, "training_acc": 47.0, "val_loss": 394.28157806396484, "val_acc": 52.0}
{"epoch": 12, "training_loss": 1276.2250747680664, "training_acc": 58.0, "val_loss": 666.6220664978027, "val_acc": 52.0}
{"epoch": 13, "training_loss": 1266.557131767273, "training_acc": 51.0, "val_loss": 414.9430274963379, "val_acc": 48.0}
{"epoch": 14, "training_loss": 1808.4391059875488, "training_acc": 47.0, "val_loss": 350.0943183898926, "val_acc": 52.0}
{"epoch": 15, "training_loss": 1073.112533569336, "training_acc": 53.0, "val_loss": 51.29201412200928, "val_acc": 44.0}
{"epoch": 16, "training_loss": 401.1179733276367, "training_acc": 56.0, "val_loss": 163.60520124435425, "val_acc": 52.0}
{"epoch": 17, "training_loss": 438.96881675720215, "training_acc": 57.0, "val_loss": 211.01880073547363, "val_acc": 48.0}
{"epoch": 18, "training_loss": 697.2088871002197, "training_acc": 49.0, "val_loss": 497.8830337524414, "val_acc": 52.0}
{"epoch": 19, "training_loss": 1752.0853118896484, "training_acc": 53.0, "val_loss": 118.98863315582275, "val_acc": 52.0}
{"epoch": 20, "training_loss": 592.1069755554199, "training_acc": 66.0, "val_loss": 556.6686153411865, "val_acc": 48.0}
{"epoch": 21, "training_loss": 1700.1201133728027, "training_acc": 50.0, "val_loss": 776.9858837127686, "val_acc": 52.0}
{"epoch": 22, "training_loss": 3072.174270629883, "training_acc": 53.0, "val_loss": 966.7022705078125, "val_acc": 52.0}
{"epoch": 23, "training_loss": 2231.1595878601074, "training_acc": 53.0, "val_loss": 759.9239349365234, "val_acc": 48.0}
{"epoch": 24, "training_loss": 3962.6315002441406, "training_acc": 47.0, "val_loss": 1016.7681694030762, "val_acc": 48.0}
{"epoch": 25, "training_loss": 3418.152530670166, "training_acc": 47.0, "val_loss": 920.2582359313965, "val_acc": 52.0}
{"epoch": 26, "training_loss": 3404.9226837158203, "training_acc": 53.0, "val_loss": 1492.6297187805176, "val_acc": 52.0}
{"epoch": 27, "training_loss": 4061.644790649414, "training_acc": 53.0, "val_loss": 244.60468292236328, "val_acc": 40.0}
{"epoch": 28, "training_loss": 1516.8650436401367, "training_acc": 50.0, "val_loss": 1050.1898765563965, "val_acc": 48.0}
{"epoch": 29, "training_loss": 4331.822463989258, "training_acc": 47.0, "val_loss": 189.94824886322021, "val_acc": 36.0}
{"epoch": 30, "training_loss": 1302.6462326049805, "training_acc": 59.0, "val_loss": 1205.5713653564453, "val_acc": 52.0}
{"epoch": 31, "training_loss": 3106.9618911743164, "training_acc": 53.0, "val_loss": 360.58831214904785, "val_acc": 52.0}
{"epoch": 32, "training_loss": 1670.268325805664, "training_acc": 52.0, "val_loss": 770.7223415374756, "val_acc": 48.0}
{"epoch": 33, "training_loss": 2968.6081771850586, "training_acc": 47.0, "val_loss": 564.4827365875244, "val_acc": 52.0}
{"epoch": 34, "training_loss": 1944.049087524414, "training_acc": 56.0, "val_loss": 813.6842727661133, "val_acc": 52.0}
