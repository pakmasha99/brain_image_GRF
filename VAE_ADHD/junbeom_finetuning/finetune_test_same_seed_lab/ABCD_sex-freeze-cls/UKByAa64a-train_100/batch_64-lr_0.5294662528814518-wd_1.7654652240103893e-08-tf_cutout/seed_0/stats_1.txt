"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 1156.4270668029785, "training_acc": 46.0, "val_loss": 232.30345249176025, "val_acc": 52.0}
{"epoch": 1, "training_loss": 1128.9698104858398, "training_acc": 53.0, "val_loss": 602.0715236663818, "val_acc": 48.0}
{"epoch": 2, "training_loss": 2265.5405883789062, "training_acc": 47.0, "val_loss": 166.97425842285156, "val_acc": 48.0}
{"epoch": 3, "training_loss": 789.3118381500244, "training_acc": 51.0, "val_loss": 439.4897937774658, "val_acc": 52.0}
{"epoch": 4, "training_loss": 1670.3442687988281, "training_acc": 53.0, "val_loss": 392.16461181640625, "val_acc": 52.0}
{"epoch": 5, "training_loss": 1164.3521881103516, "training_acc": 53.0, "val_loss": 68.40351223945618, "val_acc": 48.0}
{"epoch": 6, "training_loss": 629.6760482788086, "training_acc": 47.0, "val_loss": 202.35164165496826, "val_acc": 48.0}
{"epoch": 7, "training_loss": 686.7397165298462, "training_acc": 47.0, "val_loss": 143.15991401672363, "val_acc": 52.0}
{"epoch": 8, "training_loss": 558.3178539276123, "training_acc": 53.0, "val_loss": 247.6668119430542, "val_acc": 52.0}
{"epoch": 9, "training_loss": 800.4402122497559, "training_acc": 53.0, "val_loss": 28.62231433391571, "val_acc": 52.0}
{"epoch": 10, "training_loss": 308.62596702575684, "training_acc": 53.0, "val_loss": 195.7269549369812, "val_acc": 48.0}
{"epoch": 11, "training_loss": 722.3259716033936, "training_acc": 47.0, "val_loss": 26.736578345298767, "val_acc": 52.0}
{"epoch": 12, "training_loss": 185.8965721130371, "training_acc": 54.0, "val_loss": 96.97843790054321, "val_acc": 52.0}
{"epoch": 13, "training_loss": 220.36020708084106, "training_acc": 57.0, "val_loss": 113.4329080581665, "val_acc": 48.0}
{"epoch": 14, "training_loss": 506.7033233642578, "training_acc": 47.0, "val_loss": 57.19066858291626, "val_acc": 48.0}
{"epoch": 15, "training_loss": 255.87102127075195, "training_acc": 53.0, "val_loss": 162.60977983474731, "val_acc": 52.0}
{"epoch": 16, "training_loss": 551.2163047790527, "training_acc": 53.0, "val_loss": 47.04669117927551, "val_acc": 52.0}
{"epoch": 17, "training_loss": 256.29968452453613, "training_acc": 52.0, "val_loss": 157.96536207199097, "val_acc": 48.0}
{"epoch": 18, "training_loss": 601.6031341552734, "training_acc": 47.0, "val_loss": 31.176000833511353, "val_acc": 56.0}
{"epoch": 19, "training_loss": 247.0381202697754, "training_acc": 57.0, "val_loss": 144.77965831756592, "val_acc": 52.0}
{"epoch": 20, "training_loss": 370.82087898254395, "training_acc": 53.0, "val_loss": 62.78870105743408, "val_acc": 48.0}
{"epoch": 21, "training_loss": 356.6534729003906, "training_acc": 47.0, "val_loss": 61.181557178497314, "val_acc": 48.0}
{"epoch": 22, "training_loss": 246.8954701423645, "training_acc": 51.0, "val_loss": 110.49495935440063, "val_acc": 52.0}
{"epoch": 23, "training_loss": 318.1640930175781, "training_acc": 53.0, "val_loss": 26.656275987625122, "val_acc": 36.0}
{"epoch": 24, "training_loss": 179.21786880493164, "training_acc": 53.0, "val_loss": 20.931796729564667, "val_acc": 60.0}
{"epoch": 25, "training_loss": 108.14093589782715, "training_acc": 65.0, "val_loss": 52.63853073120117, "val_acc": 52.0}
{"epoch": 26, "training_loss": 192.18239068984985, "training_acc": 49.0, "val_loss": 28.70590388774872, "val_acc": 36.0}
{"epoch": 27, "training_loss": 143.30253744125366, "training_acc": 50.0, "val_loss": 38.56855034828186, "val_acc": 56.0}
{"epoch": 28, "training_loss": 107.56791615486145, "training_acc": 59.0, "val_loss": 30.05066215991974, "val_acc": 40.0}
{"epoch": 29, "training_loss": 92.57623767852783, "training_acc": 62.0, "val_loss": 60.99240779876709, "val_acc": 52.0}
{"epoch": 30, "training_loss": 169.38016891479492, "training_acc": 51.0, "val_loss": 40.820980072021484, "val_acc": 48.0}
{"epoch": 31, "training_loss": 155.06125831604004, "training_acc": 54.0, "val_loss": 59.95781421661377, "val_acc": 52.0}
{"epoch": 32, "training_loss": 182.78396034240723, "training_acc": 52.0, "val_loss": 19.962866604328156, "val_acc": 52.0}
{"epoch": 33, "training_loss": 105.7229437828064, "training_acc": 58.0, "val_loss": 22.512076795101166, "val_acc": 56.0}
{"epoch": 34, "training_loss": 148.27064418792725, "training_acc": 56.0, "val_loss": 23.630619049072266, "val_acc": 56.0}
{"epoch": 35, "training_loss": 140.3619089126587, "training_acc": 56.0, "val_loss": 23.967143893241882, "val_acc": 44.0}
{"epoch": 36, "training_loss": 161.0025224685669, "training_acc": 54.0, "val_loss": 78.5294234752655, "val_acc": 52.0}
{"epoch": 37, "training_loss": 169.39120602607727, "training_acc": 56.0, "val_loss": 41.60907566547394, "val_acc": 48.0}
{"epoch": 38, "training_loss": 141.9543855190277, "training_acc": 50.0, "val_loss": 52.44317650794983, "val_acc": 52.0}
{"epoch": 39, "training_loss": 132.0338580608368, "training_acc": 55.0, "val_loss": 47.340354323387146, "val_acc": 48.0}
{"epoch": 40, "training_loss": 139.18049120903015, "training_acc": 53.0, "val_loss": 57.4336051940918, "val_acc": 52.0}
{"epoch": 41, "training_loss": 181.7651243209839, "training_acc": 54.0, "val_loss": 39.890849590301514, "val_acc": 48.0}
{"epoch": 42, "training_loss": 142.19621992111206, "training_acc": 52.0, "val_loss": 37.37463653087616, "val_acc": 52.0}
{"epoch": 43, "training_loss": 153.44985389709473, "training_acc": 52.0, "val_loss": 33.069199323654175, "val_acc": 52.0}
{"epoch": 44, "training_loss": 105.19816541671753, "training_acc": 51.0, "val_loss": 26.547744870185852, "val_acc": 52.0}
{"epoch": 45, "training_loss": 80.69284725189209, "training_acc": 61.0, "val_loss": 21.436236798763275, "val_acc": 44.0}
{"epoch": 46, "training_loss": 79.4428722858429, "training_acc": 58.0, "val_loss": 16.48813635110855, "val_acc": 64.0}
{"epoch": 47, "training_loss": 59.18370032310486, "training_acc": 69.0, "val_loss": 21.460506319999695, "val_acc": 56.0}
{"epoch": 48, "training_loss": 71.48025918006897, "training_acc": 65.0, "val_loss": 20.301446318626404, "val_acc": 56.0}
{"epoch": 49, "training_loss": 55.81901478767395, "training_acc": 72.0, "val_loss": 16.426044702529907, "val_acc": 68.0}
{"epoch": 50, "training_loss": 61.517115354537964, "training_acc": 69.0, "val_loss": 28.69889736175537, "val_acc": 52.0}
{"epoch": 51, "training_loss": 76.42589354515076, "training_acc": 66.0, "val_loss": 28.516462445259094, "val_acc": 44.0}
{"epoch": 52, "training_loss": 94.63961911201477, "training_acc": 51.0, "val_loss": 20.47792822122574, "val_acc": 56.0}
{"epoch": 53, "training_loss": 107.02841997146606, "training_acc": 63.0, "val_loss": 20.66996544599533, "val_acc": 56.0}
{"epoch": 54, "training_loss": 73.16650199890137, "training_acc": 60.0, "val_loss": 28.814831376075745, "val_acc": 44.0}
{"epoch": 55, "training_loss": 81.69896626472473, "training_acc": 57.0, "val_loss": 47.802865505218506, "val_acc": 52.0}
{"epoch": 56, "training_loss": 123.6044921875, "training_acc": 56.0, "val_loss": 24.627147614955902, "val_acc": 44.0}
{"epoch": 57, "training_loss": 76.81740856170654, "training_acc": 57.0, "val_loss": 19.565747678279877, "val_acc": 60.0}
{"epoch": 58, "training_loss": 88.38751649856567, "training_acc": 59.0, "val_loss": 27.895572781562805, "val_acc": 52.0}
{"epoch": 59, "training_loss": 82.02684926986694, "training_acc": 59.0, "val_loss": 42.88436770439148, "val_acc": 48.0}
{"epoch": 60, "training_loss": 133.5780050754547, "training_acc": 51.0, "val_loss": 17.84948706626892, "val_acc": 64.0}
{"epoch": 61, "training_loss": 58.80199670791626, "training_acc": 71.0, "val_loss": 24.569831788539886, "val_acc": 52.0}
{"epoch": 62, "training_loss": 60.571388244628906, "training_acc": 71.0, "val_loss": 19.999507069587708, "val_acc": 44.0}
{"epoch": 63, "training_loss": 51.11998760700226, "training_acc": 72.0, "val_loss": 36.9690477848053, "val_acc": 52.0}
{"epoch": 64, "training_loss": 92.66171360015869, "training_acc": 59.0, "val_loss": 17.428576946258545, "val_acc": 68.0}
{"epoch": 65, "training_loss": 83.08345699310303, "training_acc": 62.0, "val_loss": 15.952752530574799, "val_acc": 64.0}
{"epoch": 66, "training_loss": 71.90726947784424, "training_acc": 67.0, "val_loss": 40.47907888889313, "val_acc": 52.0}
{"epoch": 67, "training_loss": 108.31707429885864, "training_acc": 56.0, "val_loss": 69.14355158805847, "val_acc": 48.0}
{"epoch": 68, "training_loss": 225.12425088882446, "training_acc": 47.0, "val_loss": 67.93626546859741, "val_acc": 52.0}
{"epoch": 69, "training_loss": 278.55368995666504, "training_acc": 53.0, "val_loss": 16.29498302936554, "val_acc": 60.0}
{"epoch": 70, "training_loss": 188.53930473327637, "training_acc": 62.0, "val_loss": 47.27441370487213, "val_acc": 48.0}
{"epoch": 71, "training_loss": 192.99921989440918, "training_acc": 53.0, "val_loss": 122.33984470367432, "val_acc": 52.0}
{"epoch": 72, "training_loss": 335.4054265022278, "training_acc": 53.0, "val_loss": 84.09655690193176, "val_acc": 48.0}
{"epoch": 73, "training_loss": 364.92440605163574, "training_acc": 47.0, "val_loss": 27.557846903800964, "val_acc": 44.0}
{"epoch": 74, "training_loss": 180.8001127243042, "training_acc": 57.0, "val_loss": 168.9238429069519, "val_acc": 52.0}
{"epoch": 75, "training_loss": 524.0472888946533, "training_acc": 53.0, "val_loss": 19.13124769926071, "val_acc": 60.0}
{"epoch": 76, "training_loss": 190.2426300048828, "training_acc": 62.0, "val_loss": 44.80661749839783, "val_acc": 48.0}
{"epoch": 77, "training_loss": 278.39597034454346, "training_acc": 41.0, "val_loss": 125.47522783279419, "val_acc": 52.0}
{"epoch": 78, "training_loss": 317.7802200317383, "training_acc": 53.0, "val_loss": 81.87114000320435, "val_acc": 48.0}
{"epoch": 79, "training_loss": 350.69032287597656, "training_acc": 47.0, "val_loss": 18.26501041650772, "val_acc": 56.0}
{"epoch": 80, "training_loss": 142.98851203918457, "training_acc": 68.0, "val_loss": 113.66863250732422, "val_acc": 52.0}
{"epoch": 81, "training_loss": 270.9485557079315, "training_acc": 56.0, "val_loss": 101.87556743621826, "val_acc": 48.0}
{"epoch": 82, "training_loss": 429.8735980987549, "training_acc": 47.0, "val_loss": 25.924143195152283, "val_acc": 44.0}
{"epoch": 83, "training_loss": 219.79136085510254, "training_acc": 56.0, "val_loss": 171.22478485107422, "val_acc": 52.0}
{"epoch": 84, "training_loss": 542.6616859436035, "training_acc": 53.0, "val_loss": 27.060958743095398, "val_acc": 44.0}
