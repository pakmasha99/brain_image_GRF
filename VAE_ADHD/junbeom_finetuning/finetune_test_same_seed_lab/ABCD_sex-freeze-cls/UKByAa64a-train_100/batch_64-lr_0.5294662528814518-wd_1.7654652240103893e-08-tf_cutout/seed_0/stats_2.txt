"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 1232.9895477294922, "training_acc": 51.0, "val_loss": 231.860613822937, "val_acc": 52.0}
{"epoch": 1, "training_loss": 1199.9780731201172, "training_acc": 51.0, "val_loss": 596.7642784118652, "val_acc": 48.0}
{"epoch": 2, "training_loss": 2237.427070617676, "training_acc": 47.0, "val_loss": 180.1121950149536, "val_acc": 48.0}
{"epoch": 3, "training_loss": 803.5791244506836, "training_acc": 47.0, "val_loss": 393.8856363296509, "val_acc": 52.0}
{"epoch": 4, "training_loss": 1560.030906677246, "training_acc": 53.0, "val_loss": 325.7662296295166, "val_acc": 52.0}
{"epoch": 5, "training_loss": 979.8775882720947, "training_acc": 53.0, "val_loss": 151.60690546035767, "val_acc": 48.0}
{"epoch": 6, "training_loss": 738.9795112609863, "training_acc": 47.0, "val_loss": 277.05156803131104, "val_acc": 48.0}
{"epoch": 7, "training_loss": 948.7558555603027, "training_acc": 47.0, "val_loss": 35.71905791759491, "val_acc": 52.0}
{"epoch": 8, "training_loss": 247.34238052368164, "training_acc": 53.0, "val_loss": 104.70479726791382, "val_acc": 52.0}
{"epoch": 9, "training_loss": 319.6611931324005, "training_acc": 53.0, "val_loss": 88.06712031364441, "val_acc": 48.0}
{"epoch": 10, "training_loss": 285.78857612609863, "training_acc": 49.0, "val_loss": 51.170384883880615, "val_acc": 52.0}
{"epoch": 11, "training_loss": 230.3923854827881, "training_acc": 53.0, "val_loss": 19.11999136209488, "val_acc": 44.0}
{"epoch": 12, "training_loss": 190.78632831573486, "training_acc": 50.0, "val_loss": 33.55681896209717, "val_acc": 48.0}
{"epoch": 13, "training_loss": 154.05977249145508, "training_acc": 55.0, "val_loss": 89.98822569847107, "val_acc": 52.0}
{"epoch": 14, "training_loss": 249.8402967453003, "training_acc": 53.0, "val_loss": 102.46459245681763, "val_acc": 48.0}
{"epoch": 15, "training_loss": 459.1553440093994, "training_acc": 47.0, "val_loss": 66.04980826377869, "val_acc": 48.0}
{"epoch": 16, "training_loss": 284.0881748199463, "training_acc": 49.0, "val_loss": 120.02158164978027, "val_acc": 52.0}
{"epoch": 17, "training_loss": 388.6388792991638, "training_acc": 53.0, "val_loss": 28.713491559028625, "val_acc": 44.0}
{"epoch": 18, "training_loss": 167.1053590774536, "training_acc": 48.0, "val_loss": 32.606786489486694, "val_acc": 52.0}
{"epoch": 19, "training_loss": 122.56476736068726, "training_acc": 58.0, "val_loss": 18.21160912513733, "val_acc": 64.0}
{"epoch": 20, "training_loss": 134.29027557373047, "training_acc": 46.0, "val_loss": 23.32763671875, "val_acc": 52.0}
{"epoch": 21, "training_loss": 75.01780724525452, "training_acc": 60.0, "val_loss": 17.69413650035858, "val_acc": 52.0}
{"epoch": 22, "training_loss": 65.42873406410217, "training_acc": 63.0, "val_loss": 19.234909117221832, "val_acc": 56.0}
{"epoch": 23, "training_loss": 73.55448150634766, "training_acc": 65.0, "val_loss": 19.259902834892273, "val_acc": 44.0}
{"epoch": 24, "training_loss": 64.19324493408203, "training_acc": 65.0, "val_loss": 20.966407656669617, "val_acc": 56.0}
{"epoch": 25, "training_loss": 80.9724509716034, "training_acc": 60.0, "val_loss": 18.72841566801071, "val_acc": 56.0}
{"epoch": 26, "training_loss": 57.04990744590759, "training_acc": 73.0, "val_loss": 17.6808163523674, "val_acc": 56.0}
{"epoch": 27, "training_loss": 49.45753836631775, "training_acc": 79.0, "val_loss": 21.691671013832092, "val_acc": 52.0}
{"epoch": 28, "training_loss": 67.74290013313293, "training_acc": 62.0, "val_loss": 18.76198947429657, "val_acc": 60.0}
{"epoch": 29, "training_loss": 62.32119703292847, "training_acc": 68.0, "val_loss": 23.409579694271088, "val_acc": 48.0}
{"epoch": 30, "training_loss": 100.76251268386841, "training_acc": 48.0, "val_loss": 17.36786663532257, "val_acc": 52.0}
{"epoch": 31, "training_loss": 55.30316495895386, "training_acc": 73.0, "val_loss": 20.01158893108368, "val_acc": 52.0}
{"epoch": 32, "training_loss": 58.45269846916199, "training_acc": 70.0, "val_loss": 18.183191120624542, "val_acc": 56.0}
{"epoch": 33, "training_loss": 56.396183013916016, "training_acc": 73.0, "val_loss": 18.656909465789795, "val_acc": 56.0}
{"epoch": 34, "training_loss": 58.78364932537079, "training_acc": 66.0, "val_loss": 20.727895200252533, "val_acc": 52.0}
{"epoch": 35, "training_loss": 53.68375897407532, "training_acc": 73.0, "val_loss": 22.253045439720154, "val_acc": 56.0}
{"epoch": 36, "training_loss": 72.43510127067566, "training_acc": 60.0, "val_loss": 18.409812450408936, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.61416602134705, "training_acc": 60.0, "val_loss": 50.37781596183777, "val_acc": 52.0}
{"epoch": 38, "training_loss": 127.8376395702362, "training_acc": 58.0, "val_loss": 33.22426676750183, "val_acc": 48.0}
{"epoch": 39, "training_loss": 142.896249294281, "training_acc": 45.0, "val_loss": 20.55933326482773, "val_acc": 52.0}
{"epoch": 40, "training_loss": 114.69116973876953, "training_acc": 57.0, "val_loss": 18.948964774608612, "val_acc": 64.0}
{"epoch": 41, "training_loss": 89.07247638702393, "training_acc": 62.0, "val_loss": 20.264019072055817, "val_acc": 56.0}
{"epoch": 42, "training_loss": 71.01720523834229, "training_acc": 53.0, "val_loss": 60.52674651145935, "val_acc": 52.0}
{"epoch": 43, "training_loss": 163.0865774154663, "training_acc": 53.0, "val_loss": 73.21898341178894, "val_acc": 48.0}
{"epoch": 44, "training_loss": 235.07727479934692, "training_acc": 48.0, "val_loss": 77.63899564743042, "val_acc": 52.0}
{"epoch": 45, "training_loss": 294.82958793640137, "training_acc": 53.0, "val_loss": 19.900256395339966, "val_acc": 64.0}
{"epoch": 46, "training_loss": 184.93694496154785, "training_acc": 58.0, "val_loss": 36.727118492126465, "val_acc": 44.0}
{"epoch": 47, "training_loss": 165.9035587310791, "training_acc": 56.0, "val_loss": 130.70052862167358, "val_acc": 52.0}
{"epoch": 48, "training_loss": 375.03929805755615, "training_acc": 53.0, "val_loss": 92.57007241249084, "val_acc": 48.0}
{"epoch": 49, "training_loss": 416.6690616607666, "training_acc": 47.0, "val_loss": 50.783294439315796, "val_acc": 48.0}
