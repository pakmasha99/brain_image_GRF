"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 405457.8247680664, "training_acc": 49.0, "val_loss": 400519.3115234375, "val_acc": 52.0}
{"epoch": 1, "training_loss": 1299521.5234375, "training_acc": 52.0, "val_loss": 239217.9931640625, "val_acc": 52.0}
{"epoch": 2, "training_loss": 717785.94140625, "training_acc": 52.0, "val_loss": 290410.8642578125, "val_acc": 48.0}
{"epoch": 3, "training_loss": 1195779.07421875, "training_acc": 47.0, "val_loss": 93726.86767578125, "val_acc": 40.0}
{"epoch": 4, "training_loss": 382599.322265625, "training_acc": 61.0, "val_loss": 342662.3779296875, "val_acc": 52.0}
{"epoch": 5, "training_loss": 883027.79296875, "training_acc": 52.0, "val_loss": 124080.62744140625, "val_acc": 48.0}
{"epoch": 6, "training_loss": 385997.3203125, "training_acc": 46.0, "val_loss": 127087.8662109375, "val_acc": 48.0}
{"epoch": 7, "training_loss": 537802.1103515625, "training_acc": 46.0, "val_loss": 158121.74072265625, "val_acc": 52.0}
{"epoch": 8, "training_loss": 463467.84375, "training_acc": 50.0, "val_loss": 163264.24560546875, "val_acc": 52.0}
{"epoch": 9, "training_loss": 384712.8984375, "training_acc": 50.0, "val_loss": 68649.9267578125, "val_acc": 48.0}
{"epoch": 10, "training_loss": 300808.5703125, "training_acc": 49.0, "val_loss": 93220.49560546875, "val_acc": 52.0}
{"epoch": 11, "training_loss": 223977.6298828125, "training_acc": 54.0, "val_loss": 17803.839111328125, "val_acc": 48.0}
{"epoch": 12, "training_loss": 119381.72412109375, "training_acc": 52.0, "val_loss": 55151.397705078125, "val_acc": 52.0}
{"epoch": 13, "training_loss": 117951.45190429688, "training_acc": 54.0, "val_loss": 37001.66015625, "val_acc": 48.0}
{"epoch": 14, "training_loss": 132912.03979492188, "training_acc": 51.0, "val_loss": 71297.8515625, "val_acc": 52.0}
{"epoch": 15, "training_loss": 284399.8779296875, "training_acc": 53.0, "val_loss": 12062.69302368164, "val_acc": 56.0}
{"epoch": 16, "training_loss": 134582.97998046875, "training_acc": 52.0, "val_loss": 41165.65246582031, "val_acc": 52.0}
{"epoch": 17, "training_loss": 106795.9482421875, "training_acc": 61.0, "val_loss": 9338.02719116211, "val_acc": 64.0}
{"epoch": 18, "training_loss": 60989.08996582031, "training_acc": 62.0, "val_loss": 37696.343994140625, "val_acc": 52.0}
{"epoch": 19, "training_loss": 96510.6015625, "training_acc": 60.0, "val_loss": 17229.598999023438, "val_acc": 36.0}
{"epoch": 20, "training_loss": 86224.90185546875, "training_acc": 58.0, "val_loss": 32807.77282714844, "val_acc": 48.0}
{"epoch": 21, "training_loss": 116489.18359375, "training_acc": 53.0, "val_loss": 30566.754150390625, "val_acc": 48.0}
{"epoch": 22, "training_loss": 66958.73583984375, "training_acc": 67.0, "val_loss": 16539.878845214844, "val_acc": 56.0}
{"epoch": 23, "training_loss": 96781.900390625, "training_acc": 52.0, "val_loss": 100953.63159179688, "val_acc": 52.0}
{"epoch": 24, "training_loss": 262935.701171875, "training_acc": 53.0, "val_loss": 14071.612548828125, "val_acc": 36.0}
{"epoch": 25, "training_loss": 58594.235595703125, "training_acc": 62.0, "val_loss": 35436.48681640625, "val_acc": 52.0}
{"epoch": 26, "training_loss": 49119.43615722656, "training_acc": 69.0, "val_loss": 9206.85806274414, "val_acc": 44.0}
{"epoch": 27, "training_loss": 71195.83520507812, "training_acc": 58.0, "val_loss": 8013.501739501953, "val_acc": 44.0}
{"epoch": 28, "training_loss": 55451.262451171875, "training_acc": 48.0, "val_loss": 49753.41796875, "val_acc": 48.0}
{"epoch": 29, "training_loss": 181620.52734375, "training_acc": 47.0, "val_loss": 84929.26635742188, "val_acc": 52.0}
{"epoch": 30, "training_loss": 296860.4462890625, "training_acc": 53.0, "val_loss": 15728.036499023438, "val_acc": 56.0}
{"epoch": 31, "training_loss": 146494.4228515625, "training_acc": 59.0, "val_loss": 81427.04467773438, "val_acc": 48.0}
{"epoch": 32, "training_loss": 210521.63092041016, "training_acc": 58.0, "val_loss": 85957.81860351562, "val_acc": 52.0}
{"epoch": 33, "training_loss": 259979.9423828125, "training_acc": 53.0, "val_loss": 37864.34020996094, "val_acc": 48.0}
{"epoch": 34, "training_loss": 194700.5859375, "training_acc": 47.0, "val_loss": 45688.75427246094, "val_acc": 52.0}
{"epoch": 35, "training_loss": 120505.0322265625, "training_acc": 56.0, "val_loss": 11142.941284179688, "val_acc": 52.0}
{"epoch": 36, "training_loss": 117119.9072265625, "training_acc": 57.0, "val_loss": 69548.66943359375, "val_acc": 52.0}
{"epoch": 37, "training_loss": 165394.91357421875, "training_acc": 50.0, "val_loss": 24359.67559814453, "val_acc": 48.0}
{"epoch": 38, "training_loss": 111025.39013671875, "training_acc": 49.0, "val_loss": 81244.7509765625, "val_acc": 52.0}
{"epoch": 39, "training_loss": 208799.44775390625, "training_acc": 53.0, "val_loss": 9757.28988647461, "val_acc": 56.0}
{"epoch": 40, "training_loss": 72600.71435546875, "training_acc": 52.0, "val_loss": 77418.5546875, "val_acc": 52.0}
{"epoch": 41, "training_loss": 237391.4892578125, "training_acc": 53.0, "val_loss": 5910.6781005859375, "val_acc": 68.0}
{"epoch": 42, "training_loss": 98964.736328125, "training_acc": 52.0, "val_loss": 39108.75549316406, "val_acc": 52.0}
{"epoch": 43, "training_loss": 73863.5224609375, "training_acc": 59.0, "val_loss": 37011.95983886719, "val_acc": 48.0}
{"epoch": 44, "training_loss": 128879.62097167969, "training_acc": 48.0, "val_loss": 32146.994018554688, "val_acc": 52.0}
{"epoch": 45, "training_loss": 88198.962890625, "training_acc": 50.0, "val_loss": 31575.277709960938, "val_acc": 52.0}
{"epoch": 46, "training_loss": 43385.30090332031, "training_acc": 68.0, "val_loss": 18701.312255859375, "val_acc": 52.0}
{"epoch": 47, "training_loss": 66057.78540039062, "training_acc": 58.0, "val_loss": 13751.283264160156, "val_acc": 48.0}
{"epoch": 48, "training_loss": 43238.848876953125, "training_acc": 67.0, "val_loss": 47324.67956542969, "val_acc": 52.0}
{"epoch": 49, "training_loss": 85907.40356445312, "training_acc": 57.0, "val_loss": 62294.964599609375, "val_acc": 48.0}
{"epoch": 50, "training_loss": 234280.32958984375, "training_acc": 47.0, "val_loss": 56531.2744140625, "val_acc": 52.0}
{"epoch": 51, "training_loss": 201586.4052734375, "training_acc": 53.0, "val_loss": 6097.575378417969, "val_acc": 68.0}
{"epoch": 52, "training_loss": 84498.5087890625, "training_acc": 56.0, "val_loss": 16841.9677734375, "val_acc": 52.0}
{"epoch": 53, "training_loss": 58744.489501953125, "training_acc": 60.0, "val_loss": 30905.584716796875, "val_acc": 48.0}
{"epoch": 54, "training_loss": 93874.18249511719, "training_acc": 61.0, "val_loss": 55654.852294921875, "val_acc": 52.0}
{"epoch": 55, "training_loss": 85773.65069580078, "training_acc": 62.0, "val_loss": 30322.537231445312, "val_acc": 52.0}
{"epoch": 56, "training_loss": 85286.70629882812, "training_acc": 55.0, "val_loss": 94259.35668945312, "val_acc": 52.0}
{"epoch": 57, "training_loss": 230453.06103515625, "training_acc": 53.0, "val_loss": 15618.307495117188, "val_acc": 52.0}
{"epoch": 58, "training_loss": 97259.7890625, "training_acc": 61.0, "val_loss": 39885.68420410156, "val_acc": 52.0}
{"epoch": 59, "training_loss": 49091.04846191406, "training_acc": 66.0, "val_loss": 18198.690795898438, "val_acc": 64.0}
{"epoch": 60, "training_loss": 76171.35424804688, "training_acc": 58.0, "val_loss": 96738.232421875, "val_acc": 52.0}
