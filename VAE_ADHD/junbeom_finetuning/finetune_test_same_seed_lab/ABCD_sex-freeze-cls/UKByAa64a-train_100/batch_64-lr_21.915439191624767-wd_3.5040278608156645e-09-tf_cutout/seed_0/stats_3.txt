"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 177542.37539672852, "training_acc": 51.0, "val_loss": 36857.32421875, "val_acc": 52.0}
{"epoch": 1, "training_loss": 185812.52734375, "training_acc": 49.0, "val_loss": 79803.15551757812, "val_acc": 48.0}
{"epoch": 2, "training_loss": 298212.841796875, "training_acc": 47.0, "val_loss": 19314.564514160156, "val_acc": 48.0}
{"epoch": 3, "training_loss": 113359.90966796875, "training_acc": 45.0, "val_loss": 62709.539794921875, "val_acc": 52.0}
{"epoch": 4, "training_loss": 250382.0498046875, "training_acc": 53.0, "val_loss": 52872.4853515625, "val_acc": 52.0}
{"epoch": 5, "training_loss": 168534.84838867188, "training_acc": 53.0, "val_loss": 13772.83935546875, "val_acc": 48.0}
{"epoch": 6, "training_loss": 80583.7294921875, "training_acc": 47.0, "val_loss": 30999.398803710938, "val_acc": 48.0}
{"epoch": 7, "training_loss": 102401.32373046875, "training_acc": 47.0, "val_loss": 15026.640319824219, "val_acc": 52.0}
{"epoch": 8, "training_loss": 68250.45288085938, "training_acc": 53.0, "val_loss": 24324.783325195312, "val_acc": 52.0}
{"epoch": 9, "training_loss": 78968.73986816406, "training_acc": 53.0, "val_loss": 12923.849487304688, "val_acc": 48.0}
{"epoch": 10, "training_loss": 61313.0166015625, "training_acc": 47.0, "val_loss": 11243.850708007812, "val_acc": 48.0}
{"epoch": 11, "training_loss": 54078.19921875, "training_acc": 39.0, "val_loss": 15436.732482910156, "val_acc": 52.0}
{"epoch": 12, "training_loss": 49610.247802734375, "training_acc": 53.0, "val_loss": 10716.974639892578, "val_acc": 48.0}
{"epoch": 13, "training_loss": 52951.533935546875, "training_acc": 47.0, "val_loss": 5755.317306518555, "val_acc": 48.0}
{"epoch": 14, "training_loss": 32914.798828125, "training_acc": 53.0, "val_loss": 23675.88348388672, "val_acc": 52.0}
{"epoch": 15, "training_loss": 87892.87524414062, "training_acc": 53.0, "val_loss": 6781.4849853515625, "val_acc": 52.0}
{"epoch": 16, "training_loss": 47795.767578125, "training_acc": 47.0, "val_loss": 27207.327270507812, "val_acc": 48.0}
{"epoch": 17, "training_loss": 102745.12231445312, "training_acc": 47.0, "val_loss": 3919.1612243652344, "val_acc": 48.0}
{"epoch": 18, "training_loss": 56071.466796875, "training_acc": 39.0, "val_loss": 35490.54870605469, "val_acc": 52.0}
{"epoch": 19, "training_loss": 137547.2744140625, "training_acc": 53.0, "val_loss": 24132.130432128906, "val_acc": 52.0}
{"epoch": 20, "training_loss": 65612.68872070312, "training_acc": 53.0, "val_loss": 26629.776000976562, "val_acc": 48.0}
{"epoch": 21, "training_loss": 127137.01611328125, "training_acc": 47.0, "val_loss": 42216.63513183594, "val_acc": 48.0}
{"epoch": 22, "training_loss": 154888.84228515625, "training_acc": 47.0, "val_loss": 7571.501159667969, "val_acc": 48.0}
{"epoch": 23, "training_loss": 47963.13720703125, "training_acc": 53.0, "val_loss": 41918.20068359375, "val_acc": 52.0}
{"epoch": 24, "training_loss": 170113.59375, "training_acc": 53.0, "val_loss": 42776.10778808594, "val_acc": 52.0}
{"epoch": 25, "training_loss": 146269.3193359375, "training_acc": 53.0, "val_loss": 4624.524307250977, "val_acc": 52.0}
{"epoch": 26, "training_loss": 51160.45361328125, "training_acc": 53.0, "val_loss": 47645.904541015625, "val_acc": 48.0}
{"epoch": 27, "training_loss": 198246.5322265625, "training_acc": 47.0, "val_loss": 41965.850830078125, "val_acc": 48.0}
{"epoch": 28, "training_loss": 138029.9228515625, "training_acc": 47.0, "val_loss": 9163.597106933594, "val_acc": 52.0}
{"epoch": 29, "training_loss": 60174.3876953125, "training_acc": 53.0, "val_loss": 29840.762329101562, "val_acc": 52.0}
{"epoch": 30, "training_loss": 106167.48510742188, "training_acc": 53.0, "val_loss": 6174.942779541016, "val_acc": 52.0}
{"epoch": 31, "training_loss": 53926.1455078125, "training_acc": 45.0, "val_loss": 33266.19873046875, "val_acc": 48.0}
{"epoch": 32, "training_loss": 131702.3203125, "training_acc": 47.0, "val_loss": 15269.674682617188, "val_acc": 48.0}
{"epoch": 33, "training_loss": 49503.22375488281, "training_acc": 53.0, "val_loss": 21823.30780029297, "val_acc": 52.0}
{"epoch": 34, "training_loss": 85012.5205078125, "training_acc": 53.0, "val_loss": 12234.754943847656, "val_acc": 52.0}
{"epoch": 35, "training_loss": 31606.186645507812, "training_acc": 61.0, "val_loss": 14116.893005371094, "val_acc": 48.0}
{"epoch": 36, "training_loss": 49471.60021972656, "training_acc": 47.0, "val_loss": 8787.657928466797, "val_acc": 52.0}
