"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69318.65002059937, "training_acc": 44.0, "val_loss": 43391.92199707031, "val_acc": 52.0}
{"epoch": 1, "training_loss": 177424.0078125, "training_acc": 53.0, "val_loss": 17517.898559570312, "val_acc": 52.0}
{"epoch": 2, "training_loss": 125139.0869140625, "training_acc": 41.0, "val_loss": 48609.83581542969, "val_acc": 48.0}
{"epoch": 3, "training_loss": 173958.60107421875, "training_acc": 47.0, "val_loss": 4879.466247558594, "val_acc": 52.0}
{"epoch": 4, "training_loss": 33696.3115234375, "training_acc": 53.0, "val_loss": 7267.139434814453, "val_acc": 52.0}
{"epoch": 5, "training_loss": 51757.796875, "training_acc": 43.0, "val_loss": 18407.337951660156, "val_acc": 48.0}
{"epoch": 6, "training_loss": 51081.513244628906, "training_acc": 47.0, "val_loss": 25500.57830810547, "val_acc": 52.0}
{"epoch": 7, "training_loss": 116549.50927734375, "training_acc": 53.0, "val_loss": 34536.98425292969, "val_acc": 52.0}
{"epoch": 8, "training_loss": 114076.78076171875, "training_acc": 53.0, "val_loss": 5375.376510620117, "val_acc": 48.0}
{"epoch": 9, "training_loss": 37473.012451171875, "training_acc": 47.0, "val_loss": 10153.871154785156, "val_acc": 48.0}
{"epoch": 10, "training_loss": 31972.034301757812, "training_acc": 55.0, "val_loss": 12500.489807128906, "val_acc": 52.0}
{"epoch": 11, "training_loss": 40187.86657714844, "training_acc": 53.0, "val_loss": 12231.661224365234, "val_acc": 48.0}
{"epoch": 12, "training_loss": 53181.923828125, "training_acc": 47.0, "val_loss": 4820.430374145508, "val_acc": 48.0}
{"epoch": 13, "training_loss": 35389.97705078125, "training_acc": 49.0, "val_loss": 24107.9345703125, "val_acc": 52.0}
{"epoch": 14, "training_loss": 89158.59985351562, "training_acc": 53.0, "val_loss": 5511.176681518555, "val_acc": 52.0}
{"epoch": 15, "training_loss": 44920.093017578125, "training_acc": 49.0, "val_loss": 29866.729736328125, "val_acc": 48.0}
{"epoch": 16, "training_loss": 114994.5634765625, "training_acc": 47.0, "val_loss": 8277.713012695312, "val_acc": 48.0}
{"epoch": 17, "training_loss": 45631.077392578125, "training_acc": 51.0, "val_loss": 31806.005859375, "val_acc": 52.0}
{"epoch": 18, "training_loss": 128794.7421875, "training_acc": 53.0, "val_loss": 23938.63983154297, "val_acc": 52.0}
{"epoch": 19, "training_loss": 67751.63446044922, "training_acc": 53.0, "val_loss": 25979.11376953125, "val_acc": 48.0}
{"epoch": 20, "training_loss": 127940.30517578125, "training_acc": 47.0, "val_loss": 40250.76599121094, "val_acc": 48.0}
{"epoch": 21, "training_loss": 143884.6923828125, "training_acc": 47.0, "val_loss": 2167.9101943969727, "val_acc": 48.0}
{"epoch": 22, "training_loss": 35680.53076171875, "training_acc": 57.0, "val_loss": 48965.65856933594, "val_acc": 52.0}
{"epoch": 23, "training_loss": 206124.4111328125, "training_acc": 53.0, "val_loss": 52574.91455078125, "val_acc": 52.0}
{"epoch": 24, "training_loss": 183904.388671875, "training_acc": 53.0, "val_loss": 13538.412475585938, "val_acc": 52.0}
{"epoch": 25, "training_loss": 57862.30126953125, "training_acc": 55.0, "val_loss": 39841.436767578125, "val_acc": 48.0}
{"epoch": 26, "training_loss": 165524.82861328125, "training_acc": 47.0, "val_loss": 35953.472900390625, "val_acc": 48.0}
{"epoch": 27, "training_loss": 116481.03051757812, "training_acc": 47.0, "val_loss": 12482.79037475586, "val_acc": 52.0}
{"epoch": 28, "training_loss": 68668.34838867188, "training_acc": 53.0, "val_loss": 30159.197998046875, "val_acc": 52.0}
{"epoch": 29, "training_loss": 107539.537109375, "training_acc": 53.0, "val_loss": 5070.046615600586, "val_acc": 52.0}
{"epoch": 30, "training_loss": 48732.541015625, "training_acc": 51.0, "val_loss": 39150.0732421875, "val_acc": 48.0}
{"epoch": 31, "training_loss": 160054.49267578125, "training_acc": 47.0, "val_loss": 26617.086791992188, "val_acc": 48.0}
{"epoch": 32, "training_loss": 72923.677734375, "training_acc": 47.0, "val_loss": 30746.685791015625, "val_acc": 52.0}
{"epoch": 33, "training_loss": 137384.82275390625, "training_acc": 53.0, "val_loss": 55058.7158203125, "val_acc": 52.0}
{"epoch": 34, "training_loss": 210288.1044921875, "training_acc": 53.0, "val_loss": 37403.08837890625, "val_acc": 52.0}
{"epoch": 35, "training_loss": 112830.51538085938, "training_acc": 53.0, "val_loss": 16435.63232421875, "val_acc": 48.0}
{"epoch": 36, "training_loss": 87975.90869140625, "training_acc": 47.0, "val_loss": 36697.83630371094, "val_acc": 48.0}
{"epoch": 37, "training_loss": 135101.56982421875, "training_acc": 47.0, "val_loss": 5364.028167724609, "val_acc": 48.0}
{"epoch": 38, "training_loss": 60088.44677734375, "training_acc": 41.0, "val_loss": 39900.49133300781, "val_acc": 52.0}
{"epoch": 39, "training_loss": 165537.4296875, "training_acc": 53.0, "val_loss": 34487.90588378906, "val_acc": 52.0}
{"epoch": 40, "training_loss": 106626.45458984375, "training_acc": 53.0, "val_loss": 13993.267822265625, "val_acc": 48.0}
