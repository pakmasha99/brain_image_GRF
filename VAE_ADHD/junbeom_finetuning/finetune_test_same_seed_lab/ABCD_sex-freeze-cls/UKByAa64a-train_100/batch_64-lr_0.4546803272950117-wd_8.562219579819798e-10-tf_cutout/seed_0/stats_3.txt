"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 1170.7742538452148, "training_acc": 45.0, "val_loss": 158.4716558456421, "val_acc": 48.0}
{"epoch": 1, "training_loss": 1090.7798309326172, "training_acc": 45.0, "val_loss": 605.6761264801025, "val_acc": 52.0}
{"epoch": 2, "training_loss": 2236.0886306762695, "training_acc": 53.0, "val_loss": 418.2619094848633, "val_acc": 52.0}
{"epoch": 3, "training_loss": 1241.1018390655518, "training_acc": 53.0, "val_loss": 176.4622926712036, "val_acc": 48.0}
{"epoch": 4, "training_loss": 1010.0903778076172, "training_acc": 47.0, "val_loss": 354.86137866973877, "val_acc": 48.0}
{"epoch": 5, "training_loss": 1276.422269821167, "training_acc": 47.0, "val_loss": 40.584394335746765, "val_acc": 48.0}
{"epoch": 6, "training_loss": 326.0482769012451, "training_acc": 52.0, "val_loss": 316.70544147491455, "val_acc": 52.0}
{"epoch": 7, "training_loss": 1265.337013244629, "training_acc": 53.0, "val_loss": 289.23168182373047, "val_acc": 52.0}
{"epoch": 8, "training_loss": 917.2574195861816, "training_acc": 53.0, "val_loss": 47.605764865875244, "val_acc": 48.0}
{"epoch": 9, "training_loss": 313.98091888427734, "training_acc": 48.0, "val_loss": 149.14324283599854, "val_acc": 48.0}
{"epoch": 10, "training_loss": 470.38583850860596, "training_acc": 47.0, "val_loss": 90.77590703964233, "val_acc": 52.0}
{"epoch": 11, "training_loss": 396.0670166015625, "training_acc": 53.0, "val_loss": 141.39471054077148, "val_acc": 52.0}
{"epoch": 12, "training_loss": 410.4851598739624, "training_acc": 53.0, "val_loss": 80.50763010978699, "val_acc": 48.0}
{"epoch": 13, "training_loss": 399.3910446166992, "training_acc": 47.0, "val_loss": 88.2694959640503, "val_acc": 48.0}
{"epoch": 14, "training_loss": 240.12547302246094, "training_acc": 56.0, "val_loss": 103.63103151321411, "val_acc": 52.0}
{"epoch": 15, "training_loss": 346.46994495391846, "training_acc": 53.0, "val_loss": 47.43471145629883, "val_acc": 52.0}
{"epoch": 16, "training_loss": 131.45299768447876, "training_acc": 63.0, "val_loss": 96.08585238456726, "val_acc": 48.0}
{"epoch": 17, "training_loss": 342.1899194717407, "training_acc": 47.0, "val_loss": 74.40226078033447, "val_acc": 52.0}
{"epoch": 18, "training_loss": 266.6994171142578, "training_acc": 53.0, "val_loss": 93.31451058387756, "val_acc": 52.0}
{"epoch": 19, "training_loss": 188.94361209869385, "training_acc": 60.0, "val_loss": 55.71911334991455, "val_acc": 48.0}
{"epoch": 20, "training_loss": 212.6086483001709, "training_acc": 50.0, "val_loss": 70.89787125587463, "val_acc": 52.0}
{"epoch": 21, "training_loss": 200.76647758483887, "training_acc": 51.0, "val_loss": 49.47327673435211, "val_acc": 52.0}
{"epoch": 22, "training_loss": 138.53985118865967, "training_acc": 59.0, "val_loss": 37.67164647579193, "val_acc": 52.0}
{"epoch": 23, "training_loss": 176.14591264724731, "training_acc": 43.0, "val_loss": 59.668755531311035, "val_acc": 52.0}
{"epoch": 24, "training_loss": 113.80660963058472, "training_acc": 56.0, "val_loss": 25.14650523662567, "val_acc": 40.0}
{"epoch": 25, "training_loss": 95.04588294029236, "training_acc": 55.0, "val_loss": 37.85615563392639, "val_acc": 52.0}
{"epoch": 26, "training_loss": 86.43300580978394, "training_acc": 62.0, "val_loss": 19.49475407600403, "val_acc": 60.0}
{"epoch": 27, "training_loss": 75.79539585113525, "training_acc": 67.0, "val_loss": 24.150671064853668, "val_acc": 52.0}
{"epoch": 28, "training_loss": 104.07049608230591, "training_acc": 50.0, "val_loss": 27.9516339302063, "val_acc": 52.0}
{"epoch": 29, "training_loss": 77.21619820594788, "training_acc": 58.0, "val_loss": 18.72813254594803, "val_acc": 52.0}
{"epoch": 30, "training_loss": 79.14405417442322, "training_acc": 61.0, "val_loss": 20.07286250591278, "val_acc": 56.0}
{"epoch": 31, "training_loss": 66.31791806221008, "training_acc": 65.0, "val_loss": 26.38457715511322, "val_acc": 52.0}
{"epoch": 32, "training_loss": 70.63356423377991, "training_acc": 62.0, "val_loss": 27.187258005142212, "val_acc": 52.0}
{"epoch": 33, "training_loss": 56.77394413948059, "training_acc": 69.0, "val_loss": 19.415132701396942, "val_acc": 40.0}
{"epoch": 34, "training_loss": 69.2720537185669, "training_acc": 63.0, "val_loss": 23.673558235168457, "val_acc": 52.0}
{"epoch": 35, "training_loss": 57.108195066452026, "training_acc": 69.0, "val_loss": 17.958715558052063, "val_acc": 44.0}
{"epoch": 36, "training_loss": 64.82545256614685, "training_acc": 59.0, "val_loss": 18.05851310491562, "val_acc": 52.0}
{"epoch": 37, "training_loss": 56.94097065925598, "training_acc": 73.0, "val_loss": 22.81622290611267, "val_acc": 52.0}
{"epoch": 38, "training_loss": 102.97625637054443, "training_acc": 45.0, "val_loss": 17.181818187236786, "val_acc": 52.0}
{"epoch": 39, "training_loss": 64.78532457351685, "training_acc": 59.0, "val_loss": 35.25944948196411, "val_acc": 52.0}
{"epoch": 40, "training_loss": 83.93016910552979, "training_acc": 64.0, "val_loss": 27.0119845867157, "val_acc": 48.0}
{"epoch": 41, "training_loss": 76.27130472660065, "training_acc": 61.0, "val_loss": 36.374473571777344, "val_acc": 52.0}
{"epoch": 42, "training_loss": 94.10981369018555, "training_acc": 54.0, "val_loss": 17.52055734395981, "val_acc": 52.0}
{"epoch": 43, "training_loss": 81.10771083831787, "training_acc": 60.0, "val_loss": 18.367987871170044, "val_acc": 60.0}
{"epoch": 44, "training_loss": 61.000081062316895, "training_acc": 61.0, "val_loss": 34.69183146953583, "val_acc": 52.0}
{"epoch": 45, "training_loss": 73.27526473999023, "training_acc": 65.0, "val_loss": 24.679678678512573, "val_acc": 48.0}
{"epoch": 46, "training_loss": 78.0538022518158, "training_acc": 63.0, "val_loss": 34.53149199485779, "val_acc": 52.0}
{"epoch": 47, "training_loss": 109.83540391921997, "training_acc": 49.0, "val_loss": 16.785910725593567, "val_acc": 48.0}
{"epoch": 48, "training_loss": 64.05576729774475, "training_acc": 68.0, "val_loss": 17.123229801654816, "val_acc": 52.0}
{"epoch": 49, "training_loss": 54.88948857784271, "training_acc": 71.0, "val_loss": 19.693435728549957, "val_acc": 52.0}
{"epoch": 50, "training_loss": 58.176413774490356, "training_acc": 65.0, "val_loss": 31.31224811077118, "val_acc": 52.0}
{"epoch": 51, "training_loss": 81.38878726959229, "training_acc": 55.0, "val_loss": 26.9415020942688, "val_acc": 52.0}
{"epoch": 52, "training_loss": 59.06754541397095, "training_acc": 68.0, "val_loss": 22.428733110427856, "val_acc": 56.0}
{"epoch": 53, "training_loss": 89.89326763153076, "training_acc": 51.0, "val_loss": 19.235865771770477, "val_acc": 48.0}
{"epoch": 54, "training_loss": 52.53221356868744, "training_acc": 74.0, "val_loss": 18.38538348674774, "val_acc": 52.0}
{"epoch": 55, "training_loss": 46.95497930049896, "training_acc": 77.0, "val_loss": 17.143291234970093, "val_acc": 68.0}
{"epoch": 56, "training_loss": 51.42845952510834, "training_acc": 71.0, "val_loss": 20.056986808776855, "val_acc": 52.0}
{"epoch": 57, "training_loss": 79.58956003189087, "training_acc": 60.0, "val_loss": 42.814627289772034, "val_acc": 52.0}
{"epoch": 58, "training_loss": 97.65945029258728, "training_acc": 61.0, "val_loss": 21.01038694381714, "val_acc": 56.0}
{"epoch": 59, "training_loss": 80.20633435249329, "training_acc": 56.0, "val_loss": 22.052639722824097, "val_acc": 52.0}
{"epoch": 60, "training_loss": 48.14240312576294, "training_acc": 76.0, "val_loss": 17.383037507534027, "val_acc": 44.0}
{"epoch": 61, "training_loss": 54.62944030761719, "training_acc": 78.0, "val_loss": 26.95479989051819, "val_acc": 52.0}
{"epoch": 62, "training_loss": 85.62952756881714, "training_acc": 59.0, "val_loss": 29.82659935951233, "val_acc": 52.0}
{"epoch": 63, "training_loss": 67.8809244632721, "training_acc": 61.0, "val_loss": 37.26586699485779, "val_acc": 48.0}
{"epoch": 64, "training_loss": 112.70943021774292, "training_acc": 57.0, "val_loss": 37.34103441238403, "val_acc": 52.0}
{"epoch": 65, "training_loss": 79.85765171051025, "training_acc": 63.0, "val_loss": 26.023438572883606, "val_acc": 48.0}
{"epoch": 66, "training_loss": 71.60075879096985, "training_acc": 58.0, "val_loss": 25.024399161338806, "val_acc": 52.0}
