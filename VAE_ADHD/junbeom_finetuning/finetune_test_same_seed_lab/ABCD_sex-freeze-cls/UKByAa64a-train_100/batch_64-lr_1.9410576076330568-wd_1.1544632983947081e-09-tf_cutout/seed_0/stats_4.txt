"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 4049.4020919799805, "training_acc": 47.0, "val_loss": 293.77801418304443, "val_acc": 48.0}
{"epoch": 1, "training_loss": 4130.048980712891, "training_acc": 46.0, "val_loss": 2880.110740661621, "val_acc": 52.0}
{"epoch": 2, "training_loss": 11174.718719482422, "training_acc": 53.0, "val_loss": 2083.851432800293, "val_acc": 52.0}
{"epoch": 3, "training_loss": 6276.089172363281, "training_acc": 53.0, "val_loss": 460.35876274108887, "val_acc": 48.0}
{"epoch": 4, "training_loss": 2313.432113647461, "training_acc": 48.0, "val_loss": 1350.7513999938965, "val_acc": 48.0}
{"epoch": 5, "training_loss": 4881.159683227539, "training_acc": 47.0, "val_loss": 345.21379470825195, "val_acc": 48.0}
{"epoch": 6, "training_loss": 1268.2694244384766, "training_acc": 60.0, "val_loss": 1086.9709014892578, "val_acc": 52.0}
{"epoch": 7, "training_loss": 4541.57844543457, "training_acc": 53.0, "val_loss": 968.5207366943359, "val_acc": 52.0}
{"epoch": 8, "training_loss": 3119.6373176574707, "training_acc": 53.0, "val_loss": 467.18997955322266, "val_acc": 48.0}
{"epoch": 9, "training_loss": 2249.826644897461, "training_acc": 47.0, "val_loss": 782.9764366149902, "val_acc": 48.0}
{"epoch": 10, "training_loss": 2427.6328506469727, "training_acc": 47.0, "val_loss": 291.6520595550537, "val_acc": 52.0}
{"epoch": 11, "training_loss": 1499.206398010254, "training_acc": 53.0, "val_loss": 579.6923637390137, "val_acc": 52.0}
{"epoch": 12, "training_loss": 1752.5251502990723, "training_acc": 54.0, "val_loss": 292.8325414657593, "val_acc": 48.0}
{"epoch": 13, "training_loss": 1317.0461311340332, "training_acc": 49.0, "val_loss": 322.76508808135986, "val_acc": 48.0}
{"epoch": 14, "training_loss": 1082.4634246826172, "training_acc": 45.0, "val_loss": 234.6376657485962, "val_acc": 52.0}
{"epoch": 15, "training_loss": 743.1922969818115, "training_acc": 50.0, "val_loss": 242.64345169067383, "val_acc": 48.0}
{"epoch": 16, "training_loss": 803.0924072265625, "training_acc": 50.0, "val_loss": 138.59151601791382, "val_acc": 48.0}
{"epoch": 17, "training_loss": 588.7977390289307, "training_acc": 54.0, "val_loss": 94.48071122169495, "val_acc": 44.0}
{"epoch": 18, "training_loss": 368.15833139419556, "training_acc": 56.0, "val_loss": 53.8731575012207, "val_acc": 52.0}
{"epoch": 19, "training_loss": 188.06905460357666, "training_acc": 58.0, "val_loss": 49.84119236469269, "val_acc": 60.0}
{"epoch": 20, "training_loss": 291.1040344238281, "training_acc": 50.0, "val_loss": 139.45014476776123, "val_acc": 52.0}
{"epoch": 21, "training_loss": 397.47991275787354, "training_acc": 55.0, "val_loss": 199.21468496322632, "val_acc": 48.0}
{"epoch": 22, "training_loss": 614.7772088050842, "training_acc": 51.0, "val_loss": 189.0621304512024, "val_acc": 52.0}
{"epoch": 23, "training_loss": 593.213116645813, "training_acc": 54.0, "val_loss": 185.50423383712769, "val_acc": 48.0}
{"epoch": 24, "training_loss": 600.59792137146, "training_acc": 50.0, "val_loss": 231.60159587860107, "val_acc": 52.0}
{"epoch": 25, "training_loss": 807.4037380218506, "training_acc": 53.0, "val_loss": 58.45794677734375, "val_acc": 48.0}
{"epoch": 26, "training_loss": 228.0330991744995, "training_acc": 50.0, "val_loss": 160.67346334457397, "val_acc": 52.0}
{"epoch": 27, "training_loss": 503.3457384109497, "training_acc": 55.0, "val_loss": 201.9883394241333, "val_acc": 48.0}
{"epoch": 28, "training_loss": 653.6974630355835, "training_acc": 46.0, "val_loss": 233.35494995117188, "val_acc": 52.0}
{"epoch": 29, "training_loss": 899.3082389831543, "training_acc": 53.0, "val_loss": 29.575428366661072, "val_acc": 60.0}
{"epoch": 30, "training_loss": 673.8558731079102, "training_acc": 54.0, "val_loss": 352.5644540786743, "val_acc": 48.0}
{"epoch": 31, "training_loss": 961.5296492576599, "training_acc": 52.0, "val_loss": 198.9982008934021, "val_acc": 52.0}
{"epoch": 32, "training_loss": 539.1465320587158, "training_acc": 57.0, "val_loss": 226.3777732849121, "val_acc": 48.0}
{"epoch": 33, "training_loss": 805.8804092407227, "training_acc": 47.0, "val_loss": 224.34051036834717, "val_acc": 52.0}
{"epoch": 34, "training_loss": 913.6040458679199, "training_acc": 53.0, "val_loss": 49.26244616508484, "val_acc": 56.0}
{"epoch": 35, "training_loss": 680.9586410522461, "training_acc": 56.0, "val_loss": 404.4673442840576, "val_acc": 48.0}
{"epoch": 36, "training_loss": 1070.4181275367737, "training_acc": 54.0, "val_loss": 390.8504009246826, "val_acc": 52.0}
{"epoch": 37, "training_loss": 1590.2088623046875, "training_acc": 53.0, "val_loss": 305.71205615997314, "val_acc": 52.0}
{"epoch": 38, "training_loss": 942.3628787994385, "training_acc": 53.0, "val_loss": 335.5381488800049, "val_acc": 48.0}
{"epoch": 39, "training_loss": 985.3735599517822, "training_acc": 48.0, "val_loss": 357.8298568725586, "val_acc": 52.0}
{"epoch": 40, "training_loss": 1364.219440460205, "training_acc": 53.0, "val_loss": 373.2471227645874, "val_acc": 52.0}
{"epoch": 41, "training_loss": 939.2677989006042, "training_acc": 57.0, "val_loss": 306.92033767700195, "val_acc": 48.0}
{"epoch": 42, "training_loss": 1040.9992752075195, "training_acc": 48.0, "val_loss": 155.02346754074097, "val_acc": 52.0}
{"epoch": 43, "training_loss": 608.8382606506348, "training_acc": 53.0, "val_loss": 38.33381533622742, "val_acc": 68.0}
{"epoch": 44, "training_loss": 359.9555549621582, "training_acc": 61.0, "val_loss": 42.03660488128662, "val_acc": 64.0}
{"epoch": 45, "training_loss": 277.24930572509766, "training_acc": 67.0, "val_loss": 115.61591625213623, "val_acc": 52.0}
{"epoch": 46, "training_loss": 347.5218210220337, "training_acc": 60.0, "val_loss": 137.47925758361816, "val_acc": 48.0}
{"epoch": 47, "training_loss": 493.9936122894287, "training_acc": 56.0, "val_loss": 193.16930770874023, "val_acc": 52.0}
{"epoch": 48, "training_loss": 541.9913940429688, "training_acc": 50.0, "val_loss": 66.04330539703369, "val_acc": 48.0}
