"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 106429.74992752075, "training_acc": 46.0, "val_loss": 22237.120056152344, "val_acc": 52.0}
{"epoch": 1, "training_loss": 107981.2275390625, "training_acc": 53.0, "val_loss": 57589.0869140625, "val_acc": 48.0}
{"epoch": 2, "training_loss": 216736.716796875, "training_acc": 47.0, "val_loss": 15963.671875, "val_acc": 48.0}
{"epoch": 3, "training_loss": 75522.52709960938, "training_acc": 51.0, "val_loss": 42057.867431640625, "val_acc": 52.0}
{"epoch": 4, "training_loss": 159790.279296875, "training_acc": 53.0, "val_loss": 37529.4921875, "val_acc": 52.0}
{"epoch": 5, "training_loss": 111385.00830078125, "training_acc": 53.0, "val_loss": 6520.809173583984, "val_acc": 48.0}
{"epoch": 6, "training_loss": 60100.80419921875, "training_acc": 47.0, "val_loss": 19222.48077392578, "val_acc": 48.0}
{"epoch": 7, "training_loss": 65156.06896972656, "training_acc": 47.0, "val_loss": 13928.402709960938, "val_acc": 52.0}
{"epoch": 8, "training_loss": 54310.88720703125, "training_acc": 53.0, "val_loss": 24151.283264160156, "val_acc": 52.0}
{"epoch": 9, "training_loss": 78423.16076660156, "training_acc": 53.0, "val_loss": 3043.3090209960938, "val_acc": 52.0}
{"epoch": 10, "training_loss": 31365.96044921875, "training_acc": 49.0, "val_loss": 22456.55059814453, "val_acc": 48.0}
{"epoch": 11, "training_loss": 86042.30541992188, "training_acc": 47.0, "val_loss": 4764.802169799805, "val_acc": 48.0}
{"epoch": 12, "training_loss": 31175.308349609375, "training_acc": 49.0, "val_loss": 23628.0029296875, "val_acc": 52.0}
{"epoch": 13, "training_loss": 90843.01171875, "training_acc": 53.0, "val_loss": 17712.335205078125, "val_acc": 52.0}
{"epoch": 14, "training_loss": 47547.570709228516, "training_acc": 52.0, "val_loss": 17389.81475830078, "val_acc": 48.0}
{"epoch": 15, "training_loss": 84728.4765625, "training_acc": 47.0, "val_loss": 25883.926391601562, "val_acc": 48.0}
{"epoch": 16, "training_loss": 87371.25146484375, "training_acc": 47.0, "val_loss": 2299.497413635254, "val_acc": 60.0}
{"epoch": 17, "training_loss": 22143.224731445312, "training_acc": 56.0, "val_loss": 15569.435119628906, "val_acc": 52.0}
{"epoch": 18, "training_loss": 50784.04229736328, "training_acc": 53.0, "val_loss": 1764.9335861206055, "val_acc": 44.0}
{"epoch": 19, "training_loss": 12733.965515136719, "training_acc": 57.0, "val_loss": 2746.8446731567383, "val_acc": 48.0}
{"epoch": 20, "training_loss": 19932.641967773438, "training_acc": 47.0, "val_loss": 8615.432739257812, "val_acc": 52.0}
{"epoch": 21, "training_loss": 18704.475296020508, "training_acc": 59.0, "val_loss": 8909.992218017578, "val_acc": 48.0}
{"epoch": 22, "training_loss": 39464.01574707031, "training_acc": 47.0, "val_loss": 2175.6860733032227, "val_acc": 40.0}
{"epoch": 23, "training_loss": 23176.185913085938, "training_acc": 49.0, "val_loss": 17758.419799804688, "val_acc": 52.0}
{"epoch": 24, "training_loss": 57328.71923828125, "training_acc": 53.0, "val_loss": 5667.048263549805, "val_acc": 52.0}
{"epoch": 25, "training_loss": 27348.234985351562, "training_acc": 50.0, "val_loss": 15991.094970703125, "val_acc": 48.0}
{"epoch": 26, "training_loss": 58240.3134765625, "training_acc": 47.0, "val_loss": 1674.9059677124023, "val_acc": 52.0}
{"epoch": 27, "training_loss": 22531.544677734375, "training_acc": 51.0, "val_loss": 19389.75372314453, "val_acc": 52.0}
{"epoch": 28, "training_loss": 63677.034423828125, "training_acc": 53.0, "val_loss": 6600.962066650391, "val_acc": 52.0}
{"epoch": 29, "training_loss": 35858.835693359375, "training_acc": 42.0, "val_loss": 17795.835876464844, "val_acc": 48.0}
{"epoch": 30, "training_loss": 67717.82543945312, "training_acc": 47.0, "val_loss": 2270.6499099731445, "val_acc": 36.0}
{"epoch": 31, "training_loss": 20518.53662109375, "training_acc": 60.0, "val_loss": 21988.48419189453, "val_acc": 52.0}
{"epoch": 32, "training_loss": 77185.81225585938, "training_acc": 53.0, "val_loss": 14552.772521972656, "val_acc": 52.0}
{"epoch": 33, "training_loss": 30028.453887939453, "training_acc": 57.0, "val_loss": 12354.315948486328, "val_acc": 48.0}
{"epoch": 34, "training_loss": 55708.806884765625, "training_acc": 47.0, "val_loss": 9202.17056274414, "val_acc": 48.0}
{"epoch": 35, "training_loss": 29820.72918701172, "training_acc": 48.0, "val_loss": 10956.077575683594, "val_acc": 52.0}
{"epoch": 36, "training_loss": 30597.048828125, "training_acc": 54.0, "val_loss": 3414.2223358154297, "val_acc": 56.0}
{"epoch": 37, "training_loss": 11442.999389648438, "training_acc": 54.0, "val_loss": 1483.1403732299805, "val_acc": 48.0}
{"epoch": 38, "training_loss": 11087.80859375, "training_acc": 60.0, "val_loss": 5287.820816040039, "val_acc": 56.0}
{"epoch": 39, "training_loss": 12910.013702392578, "training_acc": 56.0, "val_loss": 1893.1499481201172, "val_acc": 44.0}
{"epoch": 40, "training_loss": 10944.604125976562, "training_acc": 57.0, "val_loss": 5591.139602661133, "val_acc": 52.0}
{"epoch": 41, "training_loss": 12633.242950439453, "training_acc": 58.0, "val_loss": 4195.050811767578, "val_acc": 48.0}
{"epoch": 42, "training_loss": 11783.854934692383, "training_acc": 57.0, "val_loss": 8302.484893798828, "val_acc": 52.0}
{"epoch": 43, "training_loss": 27753.361206054688, "training_acc": 53.0, "val_loss": 1277.7030944824219, "val_acc": 64.0}
{"epoch": 44, "training_loss": 13034.970703125, "training_acc": 64.0, "val_loss": 9621.913146972656, "val_acc": 48.0}
{"epoch": 45, "training_loss": 26097.588928222656, "training_acc": 50.0, "val_loss": 12403.585815429688, "val_acc": 52.0}
{"epoch": 46, "training_loss": 54154.229248046875, "training_acc": 53.0, "val_loss": 16053.880310058594, "val_acc": 52.0}
{"epoch": 47, "training_loss": 42385.20733642578, "training_acc": 54.0, "val_loss": 11084.334564208984, "val_acc": 48.0}
{"epoch": 48, "training_loss": 59030.8466796875, "training_acc": 47.0, "val_loss": 15107.406616210938, "val_acc": 48.0}
{"epoch": 49, "training_loss": 39284.21487426758, "training_acc": 49.0, "val_loss": 14824.713134765625, "val_acc": 52.0}
{"epoch": 50, "training_loss": 64254.90283203125, "training_acc": 53.0, "val_loss": 24362.542724609375, "val_acc": 52.0}
{"epoch": 51, "training_loss": 81914.0947265625, "training_acc": 53.0, "val_loss": 4679.452133178711, "val_acc": 52.0}
{"epoch": 52, "training_loss": 29907.364501953125, "training_acc": 52.0, "val_loss": 23485.499572753906, "val_acc": 48.0}
{"epoch": 53, "training_loss": 90462.6259765625, "training_acc": 47.0, "val_loss": 12713.697814941406, "val_acc": 48.0}
{"epoch": 54, "training_loss": 37337.986267089844, "training_acc": 49.0, "val_loss": 11745.917510986328, "val_acc": 52.0}
{"epoch": 55, "training_loss": 42736.47106933594, "training_acc": 53.0, "val_loss": 3295.8370208740234, "val_acc": 60.0}
{"epoch": 56, "training_loss": 23534.380859375, "training_acc": 47.0, "val_loss": 12731.883239746094, "val_acc": 48.0}
{"epoch": 57, "training_loss": 38457.10626220703, "training_acc": 49.0, "val_loss": 5989.106750488281, "val_acc": 52.0}
{"epoch": 58, "training_loss": 25819.348876953125, "training_acc": 52.0, "val_loss": 5893.973541259766, "val_acc": 52.0}
{"epoch": 59, "training_loss": 17864.81219482422, "training_acc": 55.0, "val_loss": 4593.323135375977, "val_acc": 48.0}
{"epoch": 60, "training_loss": 19160.522521972656, "training_acc": 45.0, "val_loss": 2603.981399536133, "val_acc": 60.0}
{"epoch": 61, "training_loss": 7840.381378173828, "training_acc": 55.0, "val_loss": 1055.18798828125, "val_acc": 44.0}
{"epoch": 62, "training_loss": 12010.220031738281, "training_acc": 54.0, "val_loss": 3347.268295288086, "val_acc": 56.0}
{"epoch": 63, "training_loss": 14575.63232421875, "training_acc": 49.0, "val_loss": 1107.5387954711914, "val_acc": 52.0}
{"epoch": 64, "training_loss": 15051.197631835938, "training_acc": 54.0, "val_loss": 9435.98403930664, "val_acc": 52.0}
{"epoch": 65, "training_loss": 20861.475494384766, "training_acc": 54.0, "val_loss": 6592.380523681641, "val_acc": 48.0}
{"epoch": 66, "training_loss": 34891.34814453125, "training_acc": 47.0, "val_loss": 892.2236442565918, "val_acc": 56.0}
{"epoch": 67, "training_loss": 19546.075927734375, "training_acc": 55.0, "val_loss": 12967.153930664062, "val_acc": 52.0}
{"epoch": 68, "training_loss": 31613.07666015625, "training_acc": 54.0, "val_loss": 3171.2812423706055, "val_acc": 48.0}
{"epoch": 69, "training_loss": 16253.818176269531, "training_acc": 47.0, "val_loss": 1964.9925231933594, "val_acc": 64.0}
{"epoch": 70, "training_loss": 7075.851989746094, "training_acc": 67.0, "val_loss": 2573.512649536133, "val_acc": 60.0}
{"epoch": 71, "training_loss": 10604.251953125, "training_acc": 56.0, "val_loss": 1637.6104354858398, "val_acc": 48.0}
{"epoch": 72, "training_loss": 13566.554443359375, "training_acc": 54.0, "val_loss": 6930.72509765625, "val_acc": 52.0}
{"epoch": 73, "training_loss": 14183.262718200684, "training_acc": 59.0, "val_loss": 4390.191268920898, "val_acc": 48.0}
{"epoch": 74, "training_loss": 11270.208755493164, "training_acc": 56.0, "val_loss": 5768.882751464844, "val_acc": 52.0}
{"epoch": 75, "training_loss": 17663.80438232422, "training_acc": 53.0, "val_loss": 3792.224884033203, "val_acc": 48.0}
{"epoch": 76, "training_loss": 13627.198181152344, "training_acc": 47.0, "val_loss": 3882.492446899414, "val_acc": 56.0}
{"epoch": 77, "training_loss": 12750.407958984375, "training_acc": 56.0, "val_loss": 1916.6913986206055, "val_acc": 52.0}
{"epoch": 78, "training_loss": 9750.887268066406, "training_acc": 55.0, "val_loss": 3282.6011657714844, "val_acc": 60.0}
{"epoch": 79, "training_loss": 8408.300018310547, "training_acc": 61.0, "val_loss": 1697.4735260009766, "val_acc": 52.0}
{"epoch": 80, "training_loss": 5903.2835693359375, "training_acc": 62.0, "val_loss": 3501.797103881836, "val_acc": 60.0}
{"epoch": 81, "training_loss": 7982.788040161133, "training_acc": 65.0, "val_loss": 1646.1158752441406, "val_acc": 52.0}
{"epoch": 82, "training_loss": 8592.808578491211, "training_acc": 61.0, "val_loss": 3636.7393493652344, "val_acc": 60.0}
{"epoch": 83, "training_loss": 6493.965507507324, "training_acc": 63.0, "val_loss": 2218.5035705566406, "val_acc": 52.0}
{"epoch": 84, "training_loss": 7746.41520690918, "training_acc": 59.0, "val_loss": 1455.7462692260742, "val_acc": 64.0}
{"epoch": 85, "training_loss": 6446.035980224609, "training_acc": 61.0, "val_loss": 1766.6765213012695, "val_acc": 64.0}
