"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 5209.903755187988, "training_acc": 53.0, "val_loss": 914.5343780517578, "val_acc": 52.0}
{"epoch": 1, "training_loss": 4768.799713134766, "training_acc": 53.0, "val_loss": 2688.656234741211, "val_acc": 48.0}
{"epoch": 2, "training_loss": 10358.125671386719, "training_acc": 47.0, "val_loss": 1089.7727012634277, "val_acc": 48.0}
{"epoch": 3, "training_loss": 3365.5867919921875, "training_acc": 53.0, "val_loss": 1356.041145324707, "val_acc": 52.0}
{"epoch": 4, "training_loss": 5300.089248657227, "training_acc": 53.0, "val_loss": 1126.7523765563965, "val_acc": 52.0}
{"epoch": 5, "training_loss": 3331.749988555908, "training_acc": 53.0, "val_loss": 842.2656059265137, "val_acc": 48.0}
{"epoch": 6, "training_loss": 4012.565200805664, "training_acc": 47.0, "val_loss": 1217.1002388000488, "val_acc": 48.0}
{"epoch": 7, "training_loss": 4183.064491271973, "training_acc": 47.0, "val_loss": 247.56052494049072, "val_acc": 52.0}
{"epoch": 8, "training_loss": 1295.6763381958008, "training_acc": 53.0, "val_loss": 624.3432521820068, "val_acc": 52.0}
{"epoch": 9, "training_loss": 1833.7391242980957, "training_acc": 53.0, "val_loss": 448.76513481140137, "val_acc": 48.0}
{"epoch": 10, "training_loss": 2218.209991455078, "training_acc": 47.0, "val_loss": 432.8670024871826, "val_acc": 48.0}
{"epoch": 11, "training_loss": 1463.4878215789795, "training_acc": 47.0, "val_loss": 401.0042667388916, "val_acc": 52.0}
{"epoch": 12, "training_loss": 1163.0819969177246, "training_acc": 53.0, "val_loss": 217.3318862915039, "val_acc": 48.0}
{"epoch": 13, "training_loss": 1062.789463043213, "training_acc": 48.0, "val_loss": 63.09429407119751, "val_acc": 56.0}
{"epoch": 14, "training_loss": 597.3926773071289, "training_acc": 51.0, "val_loss": 203.38757038116455, "val_acc": 52.0}
{"epoch": 15, "training_loss": 669.7424392700195, "training_acc": 55.0, "val_loss": 268.43109130859375, "val_acc": 48.0}
{"epoch": 16, "training_loss": 813.1902742385864, "training_acc": 45.0, "val_loss": 225.8472204208374, "val_acc": 52.0}
{"epoch": 17, "training_loss": 672.9869699478149, "training_acc": 53.0, "val_loss": 190.9629464149475, "val_acc": 48.0}
{"epoch": 18, "training_loss": 563.5721349716187, "training_acc": 49.0, "val_loss": 232.24921226501465, "val_acc": 52.0}
{"epoch": 19, "training_loss": 764.506799697876, "training_acc": 54.0, "val_loss": 273.35777282714844, "val_acc": 48.0}
{"epoch": 20, "training_loss": 1045.4225883483887, "training_acc": 46.0, "val_loss": 123.48915338516235, "val_acc": 52.0}
{"epoch": 21, "training_loss": 443.9454154968262, "training_acc": 54.0, "val_loss": 109.96158123016357, "val_acc": 48.0}
{"epoch": 22, "training_loss": 330.50709342956543, "training_acc": 51.0, "val_loss": 244.63233947753906, "val_acc": 52.0}
{"epoch": 23, "training_loss": 724.2799911499023, "training_acc": 53.0, "val_loss": 232.80110359191895, "val_acc": 48.0}
{"epoch": 24, "training_loss": 1011.216423034668, "training_acc": 47.0, "val_loss": 146.18346691131592, "val_acc": 52.0}
{"epoch": 25, "training_loss": 394.7930450439453, "training_acc": 54.0, "val_loss": 113.7952446937561, "val_acc": 48.0}
{"epoch": 26, "training_loss": 368.84595489501953, "training_acc": 53.0, "val_loss": 224.50153827667236, "val_acc": 52.0}
{"epoch": 27, "training_loss": 523.6933107376099, "training_acc": 56.0, "val_loss": 292.6054000854492, "val_acc": 48.0}
{"epoch": 28, "training_loss": 1217.5406684875488, "training_acc": 47.0, "val_loss": 114.99934196472168, "val_acc": 52.0}
{"epoch": 29, "training_loss": 393.1964874267578, "training_acc": 53.0, "val_loss": 145.1521873474121, "val_acc": 48.0}
{"epoch": 30, "training_loss": 439.4200930595398, "training_acc": 54.0, "val_loss": 237.75033950805664, "val_acc": 52.0}
{"epoch": 31, "training_loss": 704.5544700622559, "training_acc": 54.0, "val_loss": 179.83815670013428, "val_acc": 48.0}
{"epoch": 32, "training_loss": 688.2139015197754, "training_acc": 49.0, "val_loss": 203.04319858551025, "val_acc": 52.0}
