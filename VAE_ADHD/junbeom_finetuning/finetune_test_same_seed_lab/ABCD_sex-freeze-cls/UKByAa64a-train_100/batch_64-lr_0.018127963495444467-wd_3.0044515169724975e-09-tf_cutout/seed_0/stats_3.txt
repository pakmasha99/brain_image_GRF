"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 75.87998080253601, "training_acc": 52.0, "val_loss": 17.693662643432617, "val_acc": 52.0}
{"epoch": 1, "training_loss": 77.87228798866272, "training_acc": 52.0, "val_loss": 18.890036642551422, "val_acc": 60.0}
{"epoch": 2, "training_loss": 75.10487580299377, "training_acc": 46.0, "val_loss": 19.77139562368393, "val_acc": 52.0}
{"epoch": 3, "training_loss": 78.29979729652405, "training_acc": 53.0, "val_loss": 19.132886826992035, "val_acc": 52.0}
{"epoch": 4, "training_loss": 71.56173014640808, "training_acc": 50.0, "val_loss": 17.95165240764618, "val_acc": 48.0}
{"epoch": 5, "training_loss": 72.62167286872864, "training_acc": 47.0, "val_loss": 18.282467126846313, "val_acc": 44.0}
{"epoch": 6, "training_loss": 72.0826301574707, "training_acc": 47.0, "val_loss": 17.557303607463837, "val_acc": 52.0}
{"epoch": 7, "training_loss": 68.04426980018616, "training_acc": 52.0, "val_loss": 18.896791338920593, "val_acc": 52.0}
{"epoch": 8, "training_loss": 71.51861929893494, "training_acc": 53.0, "val_loss": 18.295781314373016, "val_acc": 52.0}
{"epoch": 9, "training_loss": 68.7630398273468, "training_acc": 51.0, "val_loss": 17.66335666179657, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.63827180862427, "training_acc": 58.0, "val_loss": 17.671486735343933, "val_acc": 52.0}
{"epoch": 11, "training_loss": 66.83541798591614, "training_acc": 60.0, "val_loss": 17.865173518657684, "val_acc": 52.0}
{"epoch": 12, "training_loss": 68.89994931221008, "training_acc": 49.0, "val_loss": 18.16776543855667, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.1290991306305, "training_acc": 53.0, "val_loss": 17.878341674804688, "val_acc": 52.0}
{"epoch": 14, "training_loss": 66.69488477706909, "training_acc": 57.0, "val_loss": 17.79758781194687, "val_acc": 52.0}
{"epoch": 15, "training_loss": 67.82819318771362, "training_acc": 57.0, "val_loss": 17.699803411960602, "val_acc": 52.0}
{"epoch": 16, "training_loss": 66.90269899368286, "training_acc": 64.0, "val_loss": 17.733778059482574, "val_acc": 52.0}
{"epoch": 17, "training_loss": 68.00758051872253, "training_acc": 54.0, "val_loss": 17.95983910560608, "val_acc": 52.0}
{"epoch": 18, "training_loss": 68.71440935134888, "training_acc": 55.0, "val_loss": 17.626620829105377, "val_acc": 52.0}
{"epoch": 19, "training_loss": 66.61701250076294, "training_acc": 61.0, "val_loss": 17.463262379169464, "val_acc": 52.0}
{"epoch": 20, "training_loss": 66.81557178497314, "training_acc": 59.0, "val_loss": 17.28779524564743, "val_acc": 52.0}
{"epoch": 21, "training_loss": 66.95527911186218, "training_acc": 65.0, "val_loss": 17.195312678813934, "val_acc": 52.0}
{"epoch": 22, "training_loss": 66.81715106964111, "training_acc": 65.0, "val_loss": 17.172439396381378, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.16949391365051, "training_acc": 56.0, "val_loss": 17.369253933429718, "val_acc": 52.0}
{"epoch": 24, "training_loss": 67.3418664932251, "training_acc": 58.0, "val_loss": 16.885283589363098, "val_acc": 52.0}
{"epoch": 25, "training_loss": 66.673898935318, "training_acc": 60.0, "val_loss": 16.941744089126587, "val_acc": 52.0}
{"epoch": 26, "training_loss": 66.0313868522644, "training_acc": 67.0, "val_loss": 16.904348134994507, "val_acc": 52.0}
{"epoch": 27, "training_loss": 65.99377298355103, "training_acc": 61.0, "val_loss": 17.42490381002426, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.81923508644104, "training_acc": 54.0, "val_loss": 17.098236083984375, "val_acc": 52.0}
{"epoch": 29, "training_loss": 66.41163372993469, "training_acc": 60.0, "val_loss": 17.291098833084106, "val_acc": 52.0}
{"epoch": 30, "training_loss": 71.43758702278137, "training_acc": 49.0, "val_loss": 16.971099376678467, "val_acc": 52.0}
{"epoch": 31, "training_loss": 67.64904832839966, "training_acc": 56.0, "val_loss": 17.071044445037842, "val_acc": 52.0}
{"epoch": 32, "training_loss": 65.776695728302, "training_acc": 58.0, "val_loss": 17.645980417728424, "val_acc": 52.0}
{"epoch": 33, "training_loss": 65.61533784866333, "training_acc": 57.0, "val_loss": 17.09512323141098, "val_acc": 52.0}
{"epoch": 34, "training_loss": 64.29452061653137, "training_acc": 68.0, "val_loss": 17.183834314346313, "val_acc": 52.0}
{"epoch": 35, "training_loss": 66.00707936286926, "training_acc": 67.0, "val_loss": 17.43040233850479, "val_acc": 52.0}
{"epoch": 36, "training_loss": 65.75997686386108, "training_acc": 60.0, "val_loss": 18.314139544963837, "val_acc": 52.0}
{"epoch": 37, "training_loss": 65.45298314094543, "training_acc": 55.0, "val_loss": 17.904959619045258, "val_acc": 52.0}
{"epoch": 38, "training_loss": 66.34662795066833, "training_acc": 60.0, "val_loss": 17.69731044769287, "val_acc": 52.0}
{"epoch": 39, "training_loss": 67.22412300109863, "training_acc": 58.0, "val_loss": 17.771659791469574, "val_acc": 52.0}
{"epoch": 40, "training_loss": 64.35400629043579, "training_acc": 61.0, "val_loss": 17.77384877204895, "val_acc": 52.0}
{"epoch": 41, "training_loss": 66.52044796943665, "training_acc": 63.0, "val_loss": 18.338699638843536, "val_acc": 52.0}
{"epoch": 42, "training_loss": 64.46298336982727, "training_acc": 62.0, "val_loss": 18.175867199897766, "val_acc": 52.0}
{"epoch": 43, "training_loss": 65.29381394386292, "training_acc": 57.0, "val_loss": 17.74687170982361, "val_acc": 52.0}
