"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 73.6420910358429, "training_acc": 45.0, "val_loss": 19.153237342834473, "val_acc": 52.0}
{"epoch": 1, "training_loss": 77.43139505386353, "training_acc": 53.0, "val_loss": 18.262703716754913, "val_acc": 52.0}
{"epoch": 2, "training_loss": 70.67494201660156, "training_acc": 50.0, "val_loss": 20.21024525165558, "val_acc": 44.0}
{"epoch": 3, "training_loss": 72.93710660934448, "training_acc": 53.0, "val_loss": 18.087396025657654, "val_acc": 52.0}
{"epoch": 4, "training_loss": 70.54290127754211, "training_acc": 53.0, "val_loss": 18.251658976078033, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.00783252716064, "training_acc": 53.0, "val_loss": 17.792771756649017, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.74818134307861, "training_acc": 52.0, "val_loss": 18.36165338754654, "val_acc": 40.0}
{"epoch": 7, "training_loss": 70.32305312156677, "training_acc": 48.0, "val_loss": 17.744433879852295, "val_acc": 52.0}
{"epoch": 8, "training_loss": 68.24362373352051, "training_acc": 52.0, "val_loss": 17.594896256923676, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.76311898231506, "training_acc": 57.0, "val_loss": 17.45118647813797, "val_acc": 52.0}
{"epoch": 10, "training_loss": 66.027583360672, "training_acc": 61.0, "val_loss": 17.41442233324051, "val_acc": 52.0}
{"epoch": 11, "training_loss": 68.24993443489075, "training_acc": 52.0, "val_loss": 17.301812767982483, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.51596927642822, "training_acc": 50.0, "val_loss": 17.39569753408432, "val_acc": 52.0}
{"epoch": 13, "training_loss": 68.9224681854248, "training_acc": 54.0, "val_loss": 17.138341069221497, "val_acc": 52.0}
{"epoch": 14, "training_loss": 67.48746871948242, "training_acc": 61.0, "val_loss": 17.0793816447258, "val_acc": 52.0}
{"epoch": 15, "training_loss": 67.68200421333313, "training_acc": 65.0, "val_loss": 17.048507928848267, "val_acc": 52.0}
{"epoch": 16, "training_loss": 67.15141701698303, "training_acc": 60.0, "val_loss": 17.09272861480713, "val_acc": 52.0}
{"epoch": 17, "training_loss": 67.36452555656433, "training_acc": 57.0, "val_loss": 17.07487255334854, "val_acc": 52.0}
{"epoch": 18, "training_loss": 67.51868319511414, "training_acc": 60.0, "val_loss": 17.081613838672638, "val_acc": 52.0}
{"epoch": 19, "training_loss": 66.23699069023132, "training_acc": 67.0, "val_loss": 17.134074866771698, "val_acc": 52.0}
{"epoch": 20, "training_loss": 66.81770753860474, "training_acc": 63.0, "val_loss": 17.08500385284424, "val_acc": 52.0}
{"epoch": 21, "training_loss": 66.9204113483429, "training_acc": 61.0, "val_loss": 17.39801913499832, "val_acc": 52.0}
{"epoch": 22, "training_loss": 67.9534182548523, "training_acc": 54.0, "val_loss": 17.262642085552216, "val_acc": 52.0}
{"epoch": 23, "training_loss": 65.89048075675964, "training_acc": 59.0, "val_loss": 17.41688996553421, "val_acc": 56.0}
{"epoch": 24, "training_loss": 68.26340055465698, "training_acc": 48.0, "val_loss": 17.387035489082336, "val_acc": 52.0}
{"epoch": 25, "training_loss": 66.54155659675598, "training_acc": 57.0, "val_loss": 18.251407146453857, "val_acc": 52.0}
{"epoch": 26, "training_loss": 71.26237630844116, "training_acc": 56.0, "val_loss": 17.994891107082367, "val_acc": 52.0}
{"epoch": 27, "training_loss": 65.67275738716125, "training_acc": 62.0, "val_loss": 18.18537265062332, "val_acc": 48.0}
{"epoch": 28, "training_loss": 73.0804226398468, "training_acc": 50.0, "val_loss": 17.930862307548523, "val_acc": 56.0}
{"epoch": 29, "training_loss": 66.80399179458618, "training_acc": 58.0, "val_loss": 17.934486269950867, "val_acc": 52.0}
{"epoch": 30, "training_loss": 70.16334462165833, "training_acc": 54.0, "val_loss": 19.617992639541626, "val_acc": 52.0}
{"epoch": 31, "training_loss": 71.98009896278381, "training_acc": 53.0, "val_loss": 17.126765847206116, "val_acc": 52.0}
{"epoch": 32, "training_loss": 68.64106273651123, "training_acc": 58.0, "val_loss": 17.69850105047226, "val_acc": 52.0}
{"epoch": 33, "training_loss": 67.49599385261536, "training_acc": 54.0, "val_loss": 17.16471016407013, "val_acc": 52.0}
{"epoch": 34, "training_loss": 66.59892988204956, "training_acc": 55.0, "val_loss": 18.373791873455048, "val_acc": 52.0}
