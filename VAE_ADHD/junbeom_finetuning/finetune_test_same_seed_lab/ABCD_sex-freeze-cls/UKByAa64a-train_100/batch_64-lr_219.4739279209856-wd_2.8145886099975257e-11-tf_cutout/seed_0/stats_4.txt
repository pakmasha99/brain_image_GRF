"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 431314.0743751526, "training_acc": 47.0, "val_loss": 101742.27905273438, "val_acc": 52.0}
{"epoch": 1, "training_loss": 424408.181640625, "training_acc": 57.0, "val_loss": 238499.609375, "val_acc": 48.0}
{"epoch": 2, "training_loss": 903786.064453125, "training_acc": 47.0, "val_loss": 45322.03674316406, "val_acc": 48.0}
{"epoch": 3, "training_loss": 328766.234375, "training_acc": 45.0, "val_loss": 199369.61669921875, "val_acc": 52.0}
{"epoch": 4, "training_loss": 785234.015625, "training_acc": 53.0, "val_loss": 166694.7265625, "val_acc": 52.0}
{"epoch": 5, "training_loss": 512666.505859375, "training_acc": 53.0, "val_loss": 44483.60900878906, "val_acc": 48.0}
{"epoch": 6, "training_loss": 307216.734375, "training_acc": 47.0, "val_loss": 104877.001953125, "val_acc": 48.0}
{"epoch": 7, "training_loss": 389224.9833984375, "training_acc": 47.0, "val_loss": 29172.189331054688, "val_acc": 52.0}
{"epoch": 8, "training_loss": 139903.1181640625, "training_acc": 56.0, "val_loss": 63597.27783203125, "val_acc": 52.0}
{"epoch": 9, "training_loss": 176380.56958007812, "training_acc": 52.0, "val_loss": 48675.60729980469, "val_acc": 48.0}
{"epoch": 10, "training_loss": 224373.1923828125, "training_acc": 47.0, "val_loss": 44930.08117675781, "val_acc": 48.0}
{"epoch": 11, "training_loss": 126386.2998046875, "training_acc": 53.0, "val_loss": 56586.68212890625, "val_acc": 52.0}
{"epoch": 12, "training_loss": 205535.20166015625, "training_acc": 53.0, "val_loss": 34784.771728515625, "val_acc": 52.0}
{"epoch": 13, "training_loss": 103262.599609375, "training_acc": 55.0, "val_loss": 36384.70153808594, "val_acc": 48.0}
{"epoch": 14, "training_loss": 107211.765625, "training_acc": 49.0, "val_loss": 31307.342529296875, "val_acc": 52.0}
{"epoch": 15, "training_loss": 119823.77197265625, "training_acc": 53.0, "val_loss": 4650.323486328125, "val_acc": 56.0}
{"epoch": 16, "training_loss": 36928.22314453125, "training_acc": 59.0, "val_loss": 13476.689147949219, "val_acc": 48.0}
{"epoch": 17, "training_loss": 89426.4951171875, "training_acc": 42.0, "val_loss": 20044.378662109375, "val_acc": 52.0}
{"epoch": 18, "training_loss": 58226.10986328125, "training_acc": 56.0, "val_loss": 21800.018310546875, "val_acc": 48.0}
{"epoch": 19, "training_loss": 64347.17980957031, "training_acc": 60.0, "val_loss": 29179.318237304688, "val_acc": 52.0}
{"epoch": 20, "training_loss": 99365.48559570312, "training_acc": 53.0, "val_loss": 27457.321166992188, "val_acc": 48.0}
{"epoch": 21, "training_loss": 110185.1865234375, "training_acc": 48.0, "val_loss": 3822.745132446289, "val_acc": 52.0}
{"epoch": 22, "training_loss": 63164.1689453125, "training_acc": 58.0, "val_loss": 21913.284301757812, "val_acc": 52.0}
{"epoch": 23, "training_loss": 93229.42504882812, "training_acc": 48.0, "val_loss": 24393.264770507812, "val_acc": 48.0}
{"epoch": 24, "training_loss": 66584.65197753906, "training_acc": 53.0, "val_loss": 15793.794250488281, "val_acc": 52.0}
{"epoch": 25, "training_loss": 48664.23046875, "training_acc": 57.0, "val_loss": 4119.715118408203, "val_acc": 48.0}
{"epoch": 26, "training_loss": 25321.270141601562, "training_acc": 57.0, "val_loss": 5343.568420410156, "val_acc": 52.0}
{"epoch": 27, "training_loss": 29375.404052734375, "training_acc": 52.0, "val_loss": 4822.188949584961, "val_acc": 52.0}
{"epoch": 28, "training_loss": 15329.388610839844, "training_acc": 61.0, "val_loss": 11902.232360839844, "val_acc": 52.0}
{"epoch": 29, "training_loss": 30905.416259765625, "training_acc": 59.0, "val_loss": 3863.180923461914, "val_acc": 60.0}
{"epoch": 30, "training_loss": 10887.489715576172, "training_acc": 67.0, "val_loss": 6554.892730712891, "val_acc": 52.0}
{"epoch": 31, "training_loss": 14180.027282714844, "training_acc": 65.0, "val_loss": 2810.6122970581055, "val_acc": 68.0}
{"epoch": 32, "training_loss": 14089.185729980469, "training_acc": 61.0, "val_loss": 21902.276611328125, "val_acc": 52.0}
{"epoch": 33, "training_loss": 59758.60467529297, "training_acc": 55.0, "val_loss": 31646.359252929688, "val_acc": 48.0}
{"epoch": 34, "training_loss": 114145.1962890625, "training_acc": 47.0, "val_loss": 18780.624389648438, "val_acc": 52.0}
{"epoch": 35, "training_loss": 90143.90625, "training_acc": 53.0, "val_loss": 2912.0046615600586, "val_acc": 56.0}
{"epoch": 36, "training_loss": 35650.654541015625, "training_acc": 56.0, "val_loss": 7658.040618896484, "val_acc": 52.0}
{"epoch": 37, "training_loss": 21369.146881103516, "training_acc": 57.0, "val_loss": 18280.178833007812, "val_acc": 48.0}
{"epoch": 38, "training_loss": 41189.14100646973, "training_acc": 60.0, "val_loss": 16105.293273925781, "val_acc": 52.0}
{"epoch": 39, "training_loss": 38109.16339111328, "training_acc": 55.0, "val_loss": 3896.810531616211, "val_acc": 56.0}
{"epoch": 40, "training_loss": 12189.151672363281, "training_acc": 72.0, "val_loss": 5894.514846801758, "val_acc": 56.0}
{"epoch": 41, "training_loss": 18317.904907226562, "training_acc": 60.0, "val_loss": 16050.048828125, "val_acc": 52.0}
{"epoch": 42, "training_loss": 33572.836486816406, "training_acc": 52.0, "val_loss": 25584.686279296875, "val_acc": 48.0}
{"epoch": 43, "training_loss": 72587.80334472656, "training_acc": 48.0, "val_loss": 43350.27160644531, "val_acc": 52.0}
{"epoch": 44, "training_loss": 180912.03125, "training_acc": 53.0, "val_loss": 33588.46130371094, "val_acc": 52.0}
{"epoch": 45, "training_loss": 101349.43774414062, "training_acc": 53.0, "val_loss": 36161.90185546875, "val_acc": 48.0}
{"epoch": 46, "training_loss": 94858.13269042969, "training_acc": 49.0, "val_loss": 50074.114990234375, "val_acc": 52.0}
{"epoch": 47, "training_loss": 215299.5341796875, "training_acc": 53.0, "val_loss": 47430.30090332031, "val_acc": 52.0}
{"epoch": 48, "training_loss": 133653.27490234375, "training_acc": 47.0, "val_loss": 23559.725952148438, "val_acc": 48.0}
{"epoch": 49, "training_loss": 61839.669372558594, "training_acc": 56.0, "val_loss": 10111.150360107422, "val_acc": 52.0}
{"epoch": 50, "training_loss": 31826.98876953125, "training_acc": 58.0, "val_loss": 4521.764373779297, "val_acc": 64.0}
