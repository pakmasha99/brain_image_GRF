"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 2043.6005554199219, "training_acc": 53.0, "val_loss": 398.73924255371094, "val_acc": 52.0}
{"epoch": 1, "training_loss": 2095.3617401123047, "training_acc": 51.0, "val_loss": 1068.0390357971191, "val_acc": 48.0}
{"epoch": 2, "training_loss": 3918.153335571289, "training_acc": 47.0, "val_loss": 332.07390308380127, "val_acc": 48.0}
{"epoch": 3, "training_loss": 1365.657169342041, "training_acc": 49.0, "val_loss": 714.8720741271973, "val_acc": 52.0}
{"epoch": 4, "training_loss": 2838.5618743896484, "training_acc": 53.0, "val_loss": 616.5630340576172, "val_acc": 52.0}
{"epoch": 5, "training_loss": 2166.652256011963, "training_acc": 53.0, "val_loss": 155.29314279556274, "val_acc": 48.0}
{"epoch": 6, "training_loss": 776.4563217163086, "training_acc": 49.0, "val_loss": 270.42229175567627, "val_acc": 48.0}
{"epoch": 7, "training_loss": 690.325852394104, "training_acc": 48.0, "val_loss": 217.77000427246094, "val_acc": 52.0}
{"epoch": 8, "training_loss": 881.0146541595459, "training_acc": 53.0, "val_loss": 108.21478366851807, "val_acc": 52.0}
{"epoch": 9, "training_loss": 455.9971752166748, "training_acc": 53.0, "val_loss": 240.00964164733887, "val_acc": 48.0}
{"epoch": 10, "training_loss": 696.6951770782471, "training_acc": 47.0, "val_loss": 142.14723110198975, "val_acc": 52.0}
{"epoch": 11, "training_loss": 672.6284980773926, "training_acc": 53.0, "val_loss": 165.70576429367065, "val_acc": 52.0}
{"epoch": 12, "training_loss": 477.7083263397217, "training_acc": 49.0, "val_loss": 118.08197498321533, "val_acc": 48.0}
{"epoch": 13, "training_loss": 276.2208504676819, "training_acc": 50.0, "val_loss": 142.50766038894653, "val_acc": 52.0}
{"epoch": 14, "training_loss": 558.703784942627, "training_acc": 53.0, "val_loss": 36.18795573711395, "val_acc": 56.0}
{"epoch": 15, "training_loss": 365.5227699279785, "training_acc": 51.0, "val_loss": 226.45421028137207, "val_acc": 48.0}
{"epoch": 16, "training_loss": 662.5412578582764, "training_acc": 47.0, "val_loss": 197.6884126663208, "val_acc": 52.0}
{"epoch": 17, "training_loss": 905.4277877807617, "training_acc": 53.0, "val_loss": 274.0931987762451, "val_acc": 52.0}
{"epoch": 18, "training_loss": 816.6291465759277, "training_acc": 53.0, "val_loss": 202.9277801513672, "val_acc": 48.0}
{"epoch": 19, "training_loss": 911.3507766723633, "training_acc": 47.0, "val_loss": 223.84741306304932, "val_acc": 48.0}
{"epoch": 20, "training_loss": 584.1888761520386, "training_acc": 57.0, "val_loss": 201.96232795715332, "val_acc": 52.0}
{"epoch": 21, "training_loss": 801.5690040588379, "training_acc": 53.0, "val_loss": 136.38442754745483, "val_acc": 52.0}
{"epoch": 22, "training_loss": 410.9032897949219, "training_acc": 54.0, "val_loss": 168.58885288238525, "val_acc": 48.0}
{"epoch": 23, "training_loss": 496.3917155265808, "training_acc": 49.0, "val_loss": 129.74761724472046, "val_acc": 52.0}
{"epoch": 24, "training_loss": 491.60598945617676, "training_acc": 53.0, "val_loss": 58.88007879257202, "val_acc": 52.0}
{"epoch": 25, "training_loss": 373.3380699157715, "training_acc": 49.0, "val_loss": 194.7078824043274, "val_acc": 48.0}
{"epoch": 26, "training_loss": 561.6025505065918, "training_acc": 48.0, "val_loss": 155.56542873382568, "val_acc": 52.0}
{"epoch": 27, "training_loss": 655.0722007751465, "training_acc": 53.0, "val_loss": 105.12253046035767, "val_acc": 52.0}
{"epoch": 28, "training_loss": 386.5011615753174, "training_acc": 52.0, "val_loss": 176.75188779830933, "val_acc": 48.0}
{"epoch": 29, "training_loss": 567.9456787109375, "training_acc": 49.0, "val_loss": 152.30846405029297, "val_acc": 52.0}
{"epoch": 30, "training_loss": 579.9023399353027, "training_acc": 53.0, "val_loss": 125.71531534194946, "val_acc": 52.0}
{"epoch": 31, "training_loss": 317.35654163360596, "training_acc": 52.0, "val_loss": 121.31311893463135, "val_acc": 48.0}
{"epoch": 32, "training_loss": 365.4924416542053, "training_acc": 53.0, "val_loss": 159.05146598815918, "val_acc": 52.0}
{"epoch": 33, "training_loss": 563.6215229034424, "training_acc": 54.0, "val_loss": 74.6918797492981, "val_acc": 52.0}
