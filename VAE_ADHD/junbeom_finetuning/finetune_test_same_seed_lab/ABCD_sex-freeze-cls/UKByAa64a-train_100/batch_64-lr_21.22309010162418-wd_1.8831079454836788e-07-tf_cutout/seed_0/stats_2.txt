"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 38174.09797668457, "training_acc": 51.0, "val_loss": 7806.459045410156, "val_acc": 52.0}
{"epoch": 1, "training_loss": 53701.638427734375, "training_acc": 45.0, "val_loss": 23956.494140625, "val_acc": 48.0}
{"epoch": 2, "training_loss": 87974.63232421875, "training_acc": 47.0, "val_loss": 4524.834060668945, "val_acc": 48.0}
{"epoch": 3, "training_loss": 26045.516479492188, "training_acc": 51.0, "val_loss": 19181.076049804688, "val_acc": 52.0}
{"epoch": 4, "training_loss": 75225.73168945312, "training_acc": 53.0, "val_loss": 17314.100646972656, "val_acc": 52.0}
{"epoch": 5, "training_loss": 56779.646728515625, "training_acc": 53.0, "val_loss": 498.46363067626953, "val_acc": 52.0}
{"epoch": 6, "training_loss": 13768.615234375, "training_acc": 50.0, "val_loss": 8726.58462524414, "val_acc": 48.0}
{"epoch": 7, "training_loss": 28324.835388183594, "training_acc": 47.0, "val_loss": 3080.154800415039, "val_acc": 52.0}
{"epoch": 8, "training_loss": 15583.513732910156, "training_acc": 53.0, "val_loss": 4760.842514038086, "val_acc": 52.0}
{"epoch": 9, "training_loss": 12728.354949951172, "training_acc": 51.0, "val_loss": 2949.965476989746, "val_acc": 48.0}
{"epoch": 10, "training_loss": 8840.768615722656, "training_acc": 47.0, "val_loss": 2500.9700775146484, "val_acc": 52.0}
{"epoch": 11, "training_loss": 10216.26025390625, "training_acc": 54.0, "val_loss": 622.8190898895264, "val_acc": 48.0}
{"epoch": 12, "training_loss": 4300.728240966797, "training_acc": 51.0, "val_loss": 970.8043098449707, "val_acc": 52.0}
{"epoch": 13, "training_loss": 2880.3477478027344, "training_acc": 56.0, "val_loss": 662.4953269958496, "val_acc": 44.0}
{"epoch": 14, "training_loss": 5043.026428222656, "training_acc": 46.0, "val_loss": 702.7975082397461, "val_acc": 56.0}
{"epoch": 15, "training_loss": 2899.096450805664, "training_acc": 52.0, "val_loss": 1083.7848663330078, "val_acc": 52.0}
{"epoch": 16, "training_loss": 2723.3469467163086, "training_acc": 56.0, "val_loss": 1140.1761054992676, "val_acc": 48.0}
{"epoch": 17, "training_loss": 4788.998817443848, "training_acc": 50.0, "val_loss": 481.57644271850586, "val_acc": 60.0}
{"epoch": 18, "training_loss": 2753.2664337158203, "training_acc": 54.0, "val_loss": 1662.5682830810547, "val_acc": 52.0}
{"epoch": 19, "training_loss": 4109.237579345703, "training_acc": 52.0, "val_loss": 3147.3880767822266, "val_acc": 48.0}
{"epoch": 20, "training_loss": 13001.273956298828, "training_acc": 47.0, "val_loss": 810.153865814209, "val_acc": 52.0}
{"epoch": 21, "training_loss": 3329.0230255126953, "training_acc": 60.0, "val_loss": 261.15081310272217, "val_acc": 64.0}
{"epoch": 22, "training_loss": 2616.798828125, "training_acc": 56.0, "val_loss": 1298.3613967895508, "val_acc": 52.0}
{"epoch": 23, "training_loss": 3020.9918518066406, "training_acc": 60.0, "val_loss": 2491.830062866211, "val_acc": 48.0}
{"epoch": 24, "training_loss": 8090.417953491211, "training_acc": 47.0, "val_loss": 3522.1752166748047, "val_acc": 52.0}
{"epoch": 25, "training_loss": 14706.046081542969, "training_acc": 53.0, "val_loss": 2039.1828536987305, "val_acc": 52.0}
{"epoch": 26, "training_loss": 8815.296630859375, "training_acc": 55.0, "val_loss": 5722.980499267578, "val_acc": 48.0}
{"epoch": 27, "training_loss": 19948.228759765625, "training_acc": 47.0, "val_loss": 2339.8393630981445, "val_acc": 52.0}
{"epoch": 28, "training_loss": 8959.907836914062, "training_acc": 53.0, "val_loss": 2429.6289443969727, "val_acc": 52.0}
{"epoch": 29, "training_loss": 7457.476348876953, "training_acc": 53.0, "val_loss": 2379.852294921875, "val_acc": 48.0}
{"epoch": 30, "training_loss": 8903.317169189453, "training_acc": 43.0, "val_loss": 1323.7343788146973, "val_acc": 52.0}
{"epoch": 31, "training_loss": 6403.725067138672, "training_acc": 44.0, "val_loss": 430.62267303466797, "val_acc": 56.0}
{"epoch": 32, "training_loss": 4710.2845458984375, "training_acc": 56.0, "val_loss": 4477.724075317383, "val_acc": 52.0}
{"epoch": 33, "training_loss": 12206.86538696289, "training_acc": 54.0, "val_loss": 2821.08097076416, "val_acc": 48.0}
{"epoch": 34, "training_loss": 13438.355529785156, "training_acc": 47.0, "val_loss": 1555.8355331420898, "val_acc": 48.0}
{"epoch": 35, "training_loss": 8631.37158203125, "training_acc": 49.0, "val_loss": 5739.383697509766, "val_acc": 52.0}
{"epoch": 36, "training_loss": 18640.16455078125, "training_acc": 53.0, "val_loss": 469.7263717651367, "val_acc": 52.0}
{"epoch": 37, "training_loss": 3322.9391174316406, "training_acc": 54.0, "val_loss": 803.4136772155762, "val_acc": 56.0}
{"epoch": 38, "training_loss": 2854.114486694336, "training_acc": 61.0, "val_loss": 422.25985527038574, "val_acc": 60.0}
{"epoch": 39, "training_loss": 3038.7061767578125, "training_acc": 60.0, "val_loss": 1540.6044960021973, "val_acc": 52.0}
{"epoch": 40, "training_loss": 3712.122673034668, "training_acc": 60.0, "val_loss": 1465.0689125061035, "val_acc": 48.0}
