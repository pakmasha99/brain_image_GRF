"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 446.3971939086914, "training_acc": 46.0, "val_loss": 83.91084671020508, "val_acc": 52.0}
{"epoch": 1, "training_loss": 408.30215644836426, "training_acc": 53.0, "val_loss": 217.40736961364746, "val_acc": 48.0}
{"epoch": 2, "training_loss": 817.3750190734863, "training_acc": 47.0, "val_loss": 59.92337465286255, "val_acc": 48.0}
{"epoch": 3, "training_loss": 284.47762966156006, "training_acc": 51.0, "val_loss": 159.2639684677124, "val_acc": 52.0}
{"epoch": 4, "training_loss": 605.7931537628174, "training_acc": 53.0, "val_loss": 141.86197519302368, "val_acc": 52.0}
{"epoch": 5, "training_loss": 421.8117322921753, "training_acc": 53.0, "val_loss": 28.232550621032715, "val_acc": 48.0}
{"epoch": 6, "training_loss": 246.13457870483398, "training_acc": 47.0, "val_loss": 86.92423701286316, "val_acc": 48.0}
{"epoch": 7, "training_loss": 308.11596393585205, "training_acc": 47.0, "val_loss": 32.380884885787964, "val_acc": 52.0}
{"epoch": 8, "training_loss": 137.62791395187378, "training_acc": 53.0, "val_loss": 75.81182718276978, "val_acc": 52.0}
{"epoch": 9, "training_loss": 243.4160304069519, "training_acc": 53.0, "val_loss": 17.17063933610916, "val_acc": 56.0}
{"epoch": 10, "training_loss": 114.82733154296875, "training_acc": 56.0, "val_loss": 43.68174970149994, "val_acc": 48.0}
{"epoch": 11, "training_loss": 151.82618713378906, "training_acc": 50.0, "val_loss": 34.38001871109009, "val_acc": 52.0}
{"epoch": 12, "training_loss": 132.2280135154724, "training_acc": 53.0, "val_loss": 22.764664888381958, "val_acc": 52.0}
{"epoch": 13, "training_loss": 73.99758076667786, "training_acc": 58.0, "val_loss": 35.04911661148071, "val_acc": 48.0}
{"epoch": 14, "training_loss": 123.36191415786743, "training_acc": 47.0, "val_loss": 24.6661975979805, "val_acc": 52.0}
{"epoch": 15, "training_loss": 99.56036520004272, "training_acc": 54.0, "val_loss": 28.182682394981384, "val_acc": 52.0}
{"epoch": 16, "training_loss": 86.43594741821289, "training_acc": 54.0, "val_loss": 25.92136263847351, "val_acc": 48.0}
{"epoch": 17, "training_loss": 102.4612946510315, "training_acc": 49.0, "val_loss": 23.41483384370804, "val_acc": 52.0}
{"epoch": 18, "training_loss": 79.11054158210754, "training_acc": 56.0, "val_loss": 24.094806611537933, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.00071907043457, "training_acc": 59.0, "val_loss": 18.886084854602814, "val_acc": 44.0}
{"epoch": 20, "training_loss": 84.02889060974121, "training_acc": 50.0, "val_loss": 18.49917620420456, "val_acc": 56.0}
{"epoch": 21, "training_loss": 71.71861696243286, "training_acc": 57.0, "val_loss": 17.693527042865753, "val_acc": 56.0}
{"epoch": 22, "training_loss": 71.8714759349823, "training_acc": 58.0, "val_loss": 16.781088709831238, "val_acc": 56.0}
{"epoch": 23, "training_loss": 65.95143818855286, "training_acc": 57.0, "val_loss": 18.18244904279709, "val_acc": 56.0}
{"epoch": 24, "training_loss": 66.7462990283966, "training_acc": 60.0, "val_loss": 17.459361255168915, "val_acc": 56.0}
{"epoch": 25, "training_loss": 65.5955319404602, "training_acc": 60.0, "val_loss": 18.93942952156067, "val_acc": 52.0}
{"epoch": 26, "training_loss": 74.11291527748108, "training_acc": 54.0, "val_loss": 17.19089150428772, "val_acc": 60.0}
{"epoch": 27, "training_loss": 62.72635364532471, "training_acc": 60.0, "val_loss": 17.217890918254852, "val_acc": 60.0}
{"epoch": 28, "training_loss": 63.84900665283203, "training_acc": 65.0, "val_loss": 17.15301126241684, "val_acc": 60.0}
{"epoch": 29, "training_loss": 63.796913385391235, "training_acc": 61.0, "val_loss": 22.309213876724243, "val_acc": 52.0}
{"epoch": 30, "training_loss": 76.9387435913086, "training_acc": 51.0, "val_loss": 17.11633801460266, "val_acc": 64.0}
{"epoch": 31, "training_loss": 68.53986096382141, "training_acc": 60.0, "val_loss": 18.99188905954361, "val_acc": 52.0}
{"epoch": 32, "training_loss": 73.10644006729126, "training_acc": 57.0, "val_loss": 17.13956594467163, "val_acc": 56.0}
{"epoch": 33, "training_loss": 62.83295941352844, "training_acc": 72.0, "val_loss": 17.150458693504333, "val_acc": 60.0}
{"epoch": 34, "training_loss": 85.97808003425598, "training_acc": 41.0, "val_loss": 17.096322774887085, "val_acc": 56.0}
{"epoch": 35, "training_loss": 69.4897530078888, "training_acc": 59.0, "val_loss": 16.543984413146973, "val_acc": 64.0}
{"epoch": 36, "training_loss": 68.7097327709198, "training_acc": 62.0, "val_loss": 19.145318865776062, "val_acc": 52.0}
{"epoch": 37, "training_loss": 66.48529362678528, "training_acc": 54.0, "val_loss": 17.019714415073395, "val_acc": 68.0}
{"epoch": 38, "training_loss": 65.38532662391663, "training_acc": 60.0, "val_loss": 18.533648550510406, "val_acc": 52.0}
{"epoch": 39, "training_loss": 65.87672328948975, "training_acc": 57.0, "val_loss": 17.193371057510376, "val_acc": 56.0}
{"epoch": 40, "training_loss": 61.45288825035095, "training_acc": 68.0, "val_loss": 18.63173246383667, "val_acc": 52.0}
{"epoch": 41, "training_loss": 63.91237950325012, "training_acc": 63.0, "val_loss": 17.675647139549255, "val_acc": 48.0}
{"epoch": 42, "training_loss": 63.474162340164185, "training_acc": 68.0, "val_loss": 19.8164701461792, "val_acc": 52.0}
{"epoch": 43, "training_loss": 71.58213925361633, "training_acc": 54.0, "val_loss": 21.09440714120865, "val_acc": 52.0}
{"epoch": 44, "training_loss": 75.39014410972595, "training_acc": 47.0, "val_loss": 17.467723786830902, "val_acc": 52.0}
{"epoch": 45, "training_loss": 61.96808648109436, "training_acc": 62.0, "val_loss": 16.926610469818115, "val_acc": 52.0}
{"epoch": 46, "training_loss": 57.1372013092041, "training_acc": 73.0, "val_loss": 17.898303270339966, "val_acc": 64.0}
{"epoch": 47, "training_loss": 67.27201461791992, "training_acc": 61.0, "val_loss": 20.464429259300232, "val_acc": 52.0}
{"epoch": 48, "training_loss": 70.9254002571106, "training_acc": 58.0, "val_loss": 16.152241826057434, "val_acc": 68.0}
{"epoch": 49, "training_loss": 58.07482552528381, "training_acc": 74.0, "val_loss": 18.361656367778778, "val_acc": 52.0}
{"epoch": 50, "training_loss": 66.35084533691406, "training_acc": 57.0, "val_loss": 16.169656813144684, "val_acc": 64.0}
{"epoch": 51, "training_loss": 58.19017720222473, "training_acc": 73.0, "val_loss": 16.24833643436432, "val_acc": 64.0}
{"epoch": 52, "training_loss": 60.99062466621399, "training_acc": 70.0, "val_loss": 16.739200055599213, "val_acc": 56.0}
{"epoch": 53, "training_loss": 63.12125372886658, "training_acc": 65.0, "val_loss": 16.803088784217834, "val_acc": 56.0}
{"epoch": 54, "training_loss": 60.302106857299805, "training_acc": 66.0, "val_loss": 16.431136429309845, "val_acc": 64.0}
{"epoch": 55, "training_loss": 57.38742804527283, "training_acc": 71.0, "val_loss": 18.42511147260666, "val_acc": 56.0}
{"epoch": 56, "training_loss": 61.48415493965149, "training_acc": 73.0, "val_loss": 17.31402575969696, "val_acc": 56.0}
{"epoch": 57, "training_loss": 58.461257219314575, "training_acc": 70.0, "val_loss": 16.59175455570221, "val_acc": 64.0}
{"epoch": 58, "training_loss": 59.87436604499817, "training_acc": 72.0, "val_loss": 18.24909597635269, "val_acc": 52.0}
{"epoch": 59, "training_loss": 61.25719404220581, "training_acc": 64.0, "val_loss": 18.450504541397095, "val_acc": 40.0}
{"epoch": 60, "training_loss": 71.0841007232666, "training_acc": 54.0, "val_loss": 17.778396606445312, "val_acc": 52.0}
{"epoch": 61, "training_loss": 64.7111029624939, "training_acc": 63.0, "val_loss": 19.34627890586853, "val_acc": 52.0}
{"epoch": 62, "training_loss": 64.00604200363159, "training_acc": 58.0, "val_loss": 18.608054518699646, "val_acc": 48.0}
{"epoch": 63, "training_loss": 68.29202699661255, "training_acc": 53.0, "val_loss": 32.219356298446655, "val_acc": 52.0}
{"epoch": 64, "training_loss": 100.2201075553894, "training_acc": 52.0, "val_loss": 16.42884612083435, "val_acc": 60.0}
{"epoch": 65, "training_loss": 74.75938057899475, "training_acc": 56.0, "val_loss": 17.94206351041794, "val_acc": 56.0}
{"epoch": 66, "training_loss": 66.3731541633606, "training_acc": 67.0, "val_loss": 19.34850811958313, "val_acc": 52.0}
{"epoch": 67, "training_loss": 57.14620876312256, "training_acc": 66.0, "val_loss": 19.820155203342438, "val_acc": 40.0}
