"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 1510445.5262947083, "training_acc": 46.0, "val_loss": 315712.0849609375, "val_acc": 52.0}
{"epoch": 1, "training_loss": 1533055.0859375, "training_acc": 53.0, "val_loss": 817617.236328125, "val_acc": 48.0}
{"epoch": 2, "training_loss": 3077110.125, "training_acc": 47.0, "val_loss": 226641.845703125, "val_acc": 48.0}
{"epoch": 3, "training_loss": 1072228.88671875, "training_acc": 51.0, "val_loss": 597116.748046875, "val_acc": 52.0}
{"epoch": 4, "training_loss": 2268615.5625, "training_acc": 53.0, "val_loss": 532824.90234375, "val_acc": 52.0}
{"epoch": 5, "training_loss": 1581382.9765625, "training_acc": 53.0, "val_loss": 92578.01513671875, "val_acc": 48.0}
{"epoch": 6, "training_loss": 853278.87890625, "training_acc": 47.0, "val_loss": 272909.27734375, "val_acc": 48.0}
{"epoch": 7, "training_loss": 925052.89453125, "training_acc": 47.0, "val_loss": 197749.4384765625, "val_acc": 52.0}
{"epoch": 8, "training_loss": 771075.9375, "training_acc": 53.0, "val_loss": 342888.18359375, "val_acc": 52.0}
{"epoch": 9, "training_loss": 1113407.71484375, "training_acc": 53.0, "val_loss": 43208.41979980469, "val_acc": 52.0}
{"epoch": 10, "training_loss": 445310.8515625, "training_acc": 49.0, "val_loss": 318815.4296875, "val_acc": 48.0}
{"epoch": 11, "training_loss": 1221541.49609375, "training_acc": 47.0, "val_loss": 67629.16870117188, "val_acc": 48.0}
{"epoch": 12, "training_loss": 442590.349609375, "training_acc": 49.0, "val_loss": 335481.1279296875, "val_acc": 52.0}
{"epoch": 13, "training_loss": 1289827.234375, "training_acc": 53.0, "val_loss": 251497.900390625, "val_acc": 52.0}
{"epoch": 14, "training_loss": 675150.6962890625, "training_acc": 52.0, "val_loss": 246630.7861328125, "val_acc": 48.0}
{"epoch": 15, "training_loss": 1201580.109375, "training_acc": 47.0, "val_loss": 366821.435546875, "val_acc": 48.0}
{"epoch": 16, "training_loss": 1237599.3203125, "training_acc": 47.0, "val_loss": 33409.54284667969, "val_acc": 60.0}
{"epoch": 17, "training_loss": 307341.62109375, "training_acc": 54.0, "val_loss": 208631.1767578125, "val_acc": 52.0}
{"epoch": 18, "training_loss": 669774.3447265625, "training_acc": 53.0, "val_loss": 47205.85021972656, "val_acc": 48.0}
{"epoch": 19, "training_loss": 207851.7314453125, "training_acc": 50.0, "val_loss": 22617.730712890625, "val_acc": 56.0}
{"epoch": 20, "training_loss": 152042.0205078125, "training_acc": 60.0, "val_loss": 35064.990234375, "val_acc": 60.0}
{"epoch": 21, "training_loss": 130672.3720703125, "training_acc": 55.0, "val_loss": 37506.27746582031, "val_acc": 44.0}
{"epoch": 22, "training_loss": 181702.0361328125, "training_acc": 55.0, "val_loss": 99324.3896484375, "val_acc": 52.0}
{"epoch": 23, "training_loss": 186100.48010253906, "training_acc": 62.0, "val_loss": 97381.94580078125, "val_acc": 48.0}
{"epoch": 24, "training_loss": 360277.8017578125, "training_acc": 48.0, "val_loss": 79695.88623046875, "val_acc": 52.0}
{"epoch": 25, "training_loss": 257939.544921875, "training_acc": 55.0, "val_loss": 35616.3818359375, "val_acc": 60.0}
{"epoch": 26, "training_loss": 263744.06640625, "training_acc": 52.0, "val_loss": 87276.6845703125, "val_acc": 48.0}
{"epoch": 27, "training_loss": 308690.90576171875, "training_acc": 49.0, "val_loss": 82361.38916015625, "val_acc": 52.0}
{"epoch": 28, "training_loss": 165131.19409179688, "training_acc": 58.0, "val_loss": 53189.434814453125, "val_acc": 48.0}
{"epoch": 29, "training_loss": 169000.34375, "training_acc": 58.0, "val_loss": 134135.0341796875, "val_acc": 52.0}
{"epoch": 30, "training_loss": 459606.41796875, "training_acc": 53.0, "val_loss": 62403.790283203125, "val_acc": 56.0}
{"epoch": 31, "training_loss": 299026.6875, "training_acc": 52.0, "val_loss": 135872.98583984375, "val_acc": 48.0}
{"epoch": 32, "training_loss": 408212.10986328125, "training_acc": 49.0, "val_loss": 163881.4697265625, "val_acc": 52.0}
{"epoch": 33, "training_loss": 670937.73046875, "training_acc": 53.0, "val_loss": 171267.88330078125, "val_acc": 52.0}
{"epoch": 34, "training_loss": 344017.84716796875, "training_acc": 56.0, "val_loss": 201518.37158203125, "val_acc": 48.0}
{"epoch": 35, "training_loss": 916995.8828125, "training_acc": 47.0, "val_loss": 223839.892578125, "val_acc": 48.0}
{"epoch": 36, "training_loss": 640356.1513671875, "training_acc": 47.0, "val_loss": 223812.646484375, "val_acc": 52.0}
{"epoch": 37, "training_loss": 956560.28125, "training_acc": 53.0, "val_loss": 375951.3671875, "val_acc": 52.0}
{"epoch": 38, "training_loss": 1237155.35546875, "training_acc": 53.0, "val_loss": 95899.169921875, "val_acc": 52.0}
