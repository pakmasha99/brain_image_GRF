"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 24205.93955230713, "training_acc": 59.0, "val_loss": 4351.793670654297, "val_acc": 52.0}
{"epoch": 1, "training_loss": 25740.550415039062, "training_acc": 49.0, "val_loss": 12846.522521972656, "val_acc": 48.0}
{"epoch": 2, "training_loss": 49108.36779785156, "training_acc": 47.0, "val_loss": 4223.296737670898, "val_acc": 48.0}
{"epoch": 3, "training_loss": 16323.203369140625, "training_acc": 51.0, "val_loss": 7947.852325439453, "val_acc": 52.0}
{"epoch": 4, "training_loss": 30892.4169921875, "training_acc": 53.0, "val_loss": 6916.396331787109, "val_acc": 52.0}
{"epoch": 5, "training_loss": 20938.448303222656, "training_acc": 53.0, "val_loss": 2719.3666458129883, "val_acc": 48.0}
{"epoch": 6, "training_loss": 15103.235168457031, "training_acc": 47.0, "val_loss": 4939.908981323242, "val_acc": 48.0}
{"epoch": 7, "training_loss": 16242.593658447266, "training_acc": 47.0, "val_loss": 2209.9592208862305, "val_acc": 52.0}
{"epoch": 8, "training_loss": 10538.373962402344, "training_acc": 53.0, "val_loss": 4029.901123046875, "val_acc": 52.0}
{"epoch": 9, "training_loss": 12171.928680419922, "training_acc": 53.0, "val_loss": 1371.2303161621094, "val_acc": 48.0}
{"epoch": 10, "training_loss": 6843.117614746094, "training_acc": 47.0, "val_loss": 1292.6291465759277, "val_acc": 48.0}
{"epoch": 11, "training_loss": 5567.250732421875, "training_acc": 49.0, "val_loss": 2187.0712280273438, "val_acc": 52.0}
{"epoch": 12, "training_loss": 6010.578590393066, "training_acc": 53.0, "val_loss": 1539.5891189575195, "val_acc": 48.0}
{"epoch": 13, "training_loss": 7207.087341308594, "training_acc": 47.0, "val_loss": 474.38769340515137, "val_acc": 52.0}
{"epoch": 14, "training_loss": 5639.033630371094, "training_acc": 42.0, "val_loss": 3008.488655090332, "val_acc": 52.0}
{"epoch": 15, "training_loss": 8909.289627075195, "training_acc": 52.0, "val_loss": 605.5380821228027, "val_acc": 52.0}
{"epoch": 16, "training_loss": 4065.4026641845703, "training_acc": 51.0, "val_loss": 456.5490245819092, "val_acc": 48.0}
{"epoch": 17, "training_loss": 2935.941925048828, "training_acc": 57.0, "val_loss": 2001.9729614257812, "val_acc": 52.0}
{"epoch": 18, "training_loss": 5679.310005187988, "training_acc": 52.0, "val_loss": 1306.075382232666, "val_acc": 48.0}
{"epoch": 19, "training_loss": 5019.202987670898, "training_acc": 49.0, "val_loss": 823.9739418029785, "val_acc": 52.0}
{"epoch": 20, "training_loss": 2779.1547088623047, "training_acc": 56.0, "val_loss": 267.4829959869385, "val_acc": 44.0}
{"epoch": 21, "training_loss": 1945.4631118774414, "training_acc": 55.0, "val_loss": 262.10784912109375, "val_acc": 48.0}
{"epoch": 22, "training_loss": 2142.8595581054688, "training_acc": 63.0, "val_loss": 687.8032207489014, "val_acc": 52.0}
{"epoch": 23, "training_loss": 2859.421859741211, "training_acc": 52.0, "val_loss": 456.07829093933105, "val_acc": 52.0}
{"epoch": 24, "training_loss": 3741.743667602539, "training_acc": 49.0, "val_loss": 1292.5929069519043, "val_acc": 52.0}
{"epoch": 25, "training_loss": 3344.6335372924805, "training_acc": 54.0, "val_loss": 561.4859580993652, "val_acc": 44.0}
{"epoch": 26, "training_loss": 1800.7818641662598, "training_acc": 56.0, "val_loss": 937.2466087341309, "val_acc": 52.0}
{"epoch": 27, "training_loss": 1783.7429084777832, "training_acc": 62.0, "val_loss": 449.08032417297363, "val_acc": 44.0}
{"epoch": 28, "training_loss": 2069.3599243164062, "training_acc": 56.0, "val_loss": 288.8545751571655, "val_acc": 56.0}
{"epoch": 29, "training_loss": 2350.244842529297, "training_acc": 54.0, "val_loss": 159.27027463912964, "val_acc": 60.0}
{"epoch": 30, "training_loss": 1256.4594955444336, "training_acc": 68.0, "val_loss": 166.22118949890137, "val_acc": 60.0}
{"epoch": 31, "training_loss": 1911.8446960449219, "training_acc": 53.0, "val_loss": 321.0933446884155, "val_acc": 52.0}
{"epoch": 32, "training_loss": 1519.6827850341797, "training_acc": 63.0, "val_loss": 480.9597969055176, "val_acc": 48.0}
{"epoch": 33, "training_loss": 1333.1804580688477, "training_acc": 59.0, "val_loss": 1180.3617477416992, "val_acc": 52.0}
{"epoch": 34, "training_loss": 3008.407325744629, "training_acc": 55.0, "val_loss": 1332.3115348815918, "val_acc": 48.0}
{"epoch": 35, "training_loss": 4483.89533996582, "training_acc": 47.0, "val_loss": 1471.1539268493652, "val_acc": 52.0}
{"epoch": 36, "training_loss": 5886.5396728515625, "training_acc": 53.0, "val_loss": 416.50853157043457, "val_acc": 56.0}
{"epoch": 37, "training_loss": 4092.069091796875, "training_acc": 57.0, "val_loss": 2991.035270690918, "val_acc": 48.0}
{"epoch": 38, "training_loss": 9094.333526611328, "training_acc": 47.0, "val_loss": 1857.478141784668, "val_acc": 52.0}
{"epoch": 39, "training_loss": 8465.356994628906, "training_acc": 53.0, "val_loss": 2479.092788696289, "val_acc": 52.0}
{"epoch": 40, "training_loss": 6454.496055603027, "training_acc": 58.0, "val_loss": 2558.9752197265625, "val_acc": 48.0}
{"epoch": 41, "training_loss": 10401.905212402344, "training_acc": 47.0, "val_loss": 1458.305835723877, "val_acc": 48.0}
{"epoch": 42, "training_loss": 6472.519866943359, "training_acc": 45.0, "val_loss": 3180.1570892333984, "val_acc": 52.0}
{"epoch": 43, "training_loss": 10914.128814697266, "training_acc": 53.0, "val_loss": 353.01692485809326, "val_acc": 56.0}
{"epoch": 44, "training_loss": 4575.818939208984, "training_acc": 57.0, "val_loss": 3515.066146850586, "val_acc": 48.0}
{"epoch": 45, "training_loss": 11638.253326416016, "training_acc": 47.0, "val_loss": 1087.272834777832, "val_acc": 52.0}
{"epoch": 46, "training_loss": 4717.659957885742, "training_acc": 53.0, "val_loss": 1481.9560050964355, "val_acc": 52.0}
{"epoch": 47, "training_loss": 3445.8494453430176, "training_acc": 61.0, "val_loss": 1532.2840690612793, "val_acc": 48.0}
{"epoch": 48, "training_loss": 3928.348014831543, "training_acc": 54.0, "val_loss": 1395.9299087524414, "val_acc": 52.0}
