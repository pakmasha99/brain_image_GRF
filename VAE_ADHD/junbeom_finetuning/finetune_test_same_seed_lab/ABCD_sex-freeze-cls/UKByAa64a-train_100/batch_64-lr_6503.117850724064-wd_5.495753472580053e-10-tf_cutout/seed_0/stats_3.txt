"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 1947143.113746643, "training_acc": 53.0, "val_loss": 5480607.8125, "val_acc": 48.0}
{"epoch": 1, "training_loss": 22125608.3125, "training_acc": 47.0, "val_loss": 429827.5390625, "val_acc": 44.0}
{"epoch": 2, "training_loss": 3744131.34375, "training_acc": 68.0, "val_loss": 5374100.0, "val_acc": 52.0}
{"epoch": 3, "training_loss": 16755544.96875, "training_acc": 53.0, "val_loss": 1497462.20703125, "val_acc": 52.0}
{"epoch": 4, "training_loss": 7161981.84375, "training_acc": 48.0, "val_loss": 4083951.171875, "val_acc": 48.0}
{"epoch": 5, "training_loss": 16564199.1875, "training_acc": 47.0, "val_loss": 1119284.1796875, "val_acc": 48.0}
{"epoch": 6, "training_loss": 5728527.25, "training_acc": 51.0, "val_loss": 3993330.078125, "val_acc": 52.0}
{"epoch": 7, "training_loss": 13879214.96875, "training_acc": 53.0, "val_loss": 3114941.9921875, "val_acc": 52.0}
{"epoch": 8, "training_loss": 7707158.015625, "training_acc": 54.0, "val_loss": 1879245.8984375, "val_acc": 48.0}
{"epoch": 9, "training_loss": 9732655.75, "training_acc": 47.0, "val_loss": 2718916.015625, "val_acc": 48.0}
{"epoch": 10, "training_loss": 9540165.25, "training_acc": 47.0, "val_loss": 1375765.234375, "val_acc": 52.0}
{"epoch": 11, "training_loss": 4936769.96875, "training_acc": 54.0, "val_loss": 2663440.4296875, "val_acc": 52.0}
{"epoch": 12, "training_loss": 7401936.203125, "training_acc": 53.0, "val_loss": 356199.8779296875, "val_acc": 48.0}
{"epoch": 13, "training_loss": 2504600.734375, "training_acc": 58.0, "val_loss": 854439.35546875, "val_acc": 52.0}
{"epoch": 14, "training_loss": 3859615.109375, "training_acc": 57.0, "val_loss": 1417062.01171875, "val_acc": 52.0}
{"epoch": 15, "training_loss": 3345861.5078125, "training_acc": 54.0, "val_loss": 447444.53125, "val_acc": 48.0}
{"epoch": 16, "training_loss": 1782433.4921875, "training_acc": 56.0, "val_loss": 464823.4375, "val_acc": 52.0}
{"epoch": 17, "training_loss": 2102783.4921875, "training_acc": 53.0, "val_loss": 1014128.22265625, "val_acc": 52.0}
{"epoch": 18, "training_loss": 1672708.015625, "training_acc": 59.0, "val_loss": 528972.75390625, "val_acc": 56.0}
{"epoch": 19, "training_loss": 2546812.6171875, "training_acc": 51.0, "val_loss": 798356.73828125, "val_acc": 52.0}
{"epoch": 20, "training_loss": 2160159.921875, "training_acc": 53.0, "val_loss": 304908.59375, "val_acc": 48.0}
{"epoch": 21, "training_loss": 1680474.75, "training_acc": 53.0, "val_loss": 241553.3935546875, "val_acc": 48.0}
{"epoch": 22, "training_loss": 1335641.3125, "training_acc": 64.0, "val_loss": 297212.9638671875, "val_acc": 44.0}
{"epoch": 23, "training_loss": 1774798.1015625, "training_acc": 52.0, "val_loss": 280005.95703125, "val_acc": 56.0}
{"epoch": 24, "training_loss": 2088553.140625, "training_acc": 51.0, "val_loss": 691916.064453125, "val_acc": 52.0}
{"epoch": 25, "training_loss": 1808495.5, "training_acc": 52.0, "val_loss": 288233.740234375, "val_acc": 52.0}
{"epoch": 26, "training_loss": 1514698.25390625, "training_acc": 56.0, "val_loss": 894810.83984375, "val_acc": 52.0}
{"epoch": 27, "training_loss": 1602408.822265625, "training_acc": 57.0, "val_loss": 273466.30859375, "val_acc": 52.0}
{"epoch": 28, "training_loss": 1355629.24609375, "training_acc": 52.0, "val_loss": 618084.228515625, "val_acc": 52.0}
{"epoch": 29, "training_loss": 1881986.15625, "training_acc": 43.0, "val_loss": 338035.107421875, "val_acc": 36.0}
{"epoch": 30, "training_loss": 646874.69140625, "training_acc": 71.0, "val_loss": 579484.423828125, "val_acc": 48.0}
{"epoch": 31, "training_loss": 829569.62109375, "training_acc": 66.0, "val_loss": 234666.943359375, "val_acc": 40.0}
{"epoch": 32, "training_loss": 1067829.82421875, "training_acc": 55.0, "val_loss": 298075.0244140625, "val_acc": 52.0}
{"epoch": 33, "training_loss": 961196.994140625, "training_acc": 53.0, "val_loss": 151785.9619140625, "val_acc": 48.0}
{"epoch": 34, "training_loss": 1009680.046875, "training_acc": 57.0, "val_loss": 488287.5, "val_acc": 52.0}
{"epoch": 35, "training_loss": 959655.455078125, "training_acc": 64.0, "val_loss": 150333.544921875, "val_acc": 64.0}
{"epoch": 36, "training_loss": 480798.0361328125, "training_acc": 62.0, "val_loss": 326282.12890625, "val_acc": 52.0}
{"epoch": 37, "training_loss": 894542.9921875, "training_acc": 59.0, "val_loss": 451638.671875, "val_acc": 52.0}
{"epoch": 38, "training_loss": 828832.3046875, "training_acc": 64.0, "val_loss": 593531.34765625, "val_acc": 48.0}
{"epoch": 39, "training_loss": 2162397.71484375, "training_acc": 48.0, "val_loss": 746819.775390625, "val_acc": 52.0}
{"epoch": 40, "training_loss": 1557546.560546875, "training_acc": 58.0, "val_loss": 728631.591796875, "val_acc": 48.0}
{"epoch": 41, "training_loss": 2447651.51171875, "training_acc": 48.0, "val_loss": 1261412.5, "val_acc": 52.0}
{"epoch": 42, "training_loss": 4468490.78125, "training_acc": 53.0, "val_loss": 742235.3515625, "val_acc": 52.0}
{"epoch": 43, "training_loss": 3038381.78125, "training_acc": 51.0, "val_loss": 1589359.66796875, "val_acc": 48.0}
{"epoch": 44, "training_loss": 5262085.484375, "training_acc": 47.0, "val_loss": 1413041.30859375, "val_acc": 52.0}
{"epoch": 45, "training_loss": 4770335.703125, "training_acc": 53.0, "val_loss": 1904728.7109375, "val_acc": 52.0}
{"epoch": 46, "training_loss": 3862944.9453125, "training_acc": 56.0, "val_loss": 1470447.94921875, "val_acc": 48.0}
{"epoch": 47, "training_loss": 7475161.0, "training_acc": 47.0, "val_loss": 1256629.00390625, "val_acc": 48.0}
{"epoch": 48, "training_loss": 4531036.05078125, "training_acc": 48.0, "val_loss": 1849350.0, "val_acc": 52.0}
{"epoch": 49, "training_loss": 4405446.0703125, "training_acc": 55.0, "val_loss": 679890.966796875, "val_acc": 52.0}
{"epoch": 50, "training_loss": 2042338.0625, "training_acc": 63.0, "val_loss": 1103775.0, "val_acc": 48.0}
{"epoch": 51, "training_loss": 3672012.578125, "training_acc": 51.0, "val_loss": 1732038.671875, "val_acc": 52.0}
{"epoch": 52, "training_loss": 4950203.703125, "training_acc": 54.0, "val_loss": 1476867.1875, "val_acc": 52.0}
{"epoch": 53, "training_loss": 2597594.8046875, "training_acc": 51.0, "val_loss": 427635.15625, "val_acc": 52.0}
{"epoch": 54, "training_loss": 1418088.0703125, "training_acc": 60.0, "val_loss": 1069170.3125, "val_acc": 52.0}
