"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 353738.36388778687, "training_acc": 44.0, "val_loss": 221546.3623046875, "val_acc": 52.0}
{"epoch": 1, "training_loss": 905874.015625, "training_acc": 53.0, "val_loss": 89441.3818359375, "val_acc": 52.0}
{"epoch": 2, "training_loss": 638923.43359375, "training_acc": 41.0, "val_loss": 248187.0849609375, "val_acc": 48.0}
{"epoch": 3, "training_loss": 888181.22265625, "training_acc": 47.0, "val_loss": 24913.308715820312, "val_acc": 52.0}
{"epoch": 4, "training_loss": 172043.2255859375, "training_acc": 53.0, "val_loss": 37104.06799316406, "val_acc": 52.0}
{"epoch": 5, "training_loss": 264260.26953125, "training_acc": 43.0, "val_loss": 93982.19604492188, "val_acc": 48.0}
{"epoch": 6, "training_loss": 260807.38049316406, "training_acc": 47.0, "val_loss": 130198.54736328125, "val_acc": 52.0}
{"epoch": 7, "training_loss": 595067.15625, "training_acc": 53.0, "val_loss": 176335.7421875, "val_acc": 52.0}
{"epoch": 8, "training_loss": 582442.43359375, "training_acc": 53.0, "val_loss": 27444.82421875, "val_acc": 48.0}
{"epoch": 9, "training_loss": 191325.7314453125, "training_acc": 47.0, "val_loss": 51842.4072265625, "val_acc": 48.0}
{"epoch": 10, "training_loss": 163239.78857421875, "training_acc": 55.0, "val_loss": 63824.0234375, "val_acc": 52.0}
{"epoch": 11, "training_loss": 205187.44775390625, "training_acc": 53.0, "val_loss": 62450.96435546875, "val_acc": 48.0}
{"epoch": 12, "training_loss": 271531.0380859375, "training_acc": 47.0, "val_loss": 24611.460876464844, "val_acc": 48.0}
{"epoch": 13, "training_loss": 180691.3544921875, "training_acc": 49.0, "val_loss": 123088.18359375, "val_acc": 52.0}
{"epoch": 14, "training_loss": 455217.4404296875, "training_acc": 53.0, "val_loss": 28138.714599609375, "val_acc": 52.0}
{"epoch": 15, "training_loss": 229349.3203125, "training_acc": 49.0, "val_loss": 152490.34423828125, "val_acc": 48.0}
{"epoch": 16, "training_loss": 587127.650390625, "training_acc": 47.0, "val_loss": 42263.31481933594, "val_acc": 48.0}
{"epoch": 17, "training_loss": 232978.712890625, "training_acc": 51.0, "val_loss": 162392.2119140625, "val_acc": 52.0}
{"epoch": 18, "training_loss": 657587.470703125, "training_acc": 53.0, "val_loss": 122223.828125, "val_acc": 52.0}
{"epoch": 19, "training_loss": 345920.1555175781, "training_acc": 53.0, "val_loss": 132641.30859375, "val_acc": 48.0}
{"epoch": 20, "training_loss": 653224.4609375, "training_acc": 47.0, "val_loss": 205508.1298828125, "val_acc": 48.0}
{"epoch": 21, "training_loss": 734633.03125, "training_acc": 47.0, "val_loss": 11068.489837646484, "val_acc": 48.0}
{"epoch": 22, "training_loss": 182174.986328125, "training_acc": 57.0, "val_loss": 250004.296875, "val_acc": 52.0}
{"epoch": 23, "training_loss": 1052410.36328125, "training_acc": 53.0, "val_loss": 268432.12890625, "val_acc": 52.0}
{"epoch": 24, "training_loss": 938961.67578125, "training_acc": 53.0, "val_loss": 69123.44360351562, "val_acc": 52.0}
{"epoch": 25, "training_loss": 295427.5478515625, "training_acc": 55.0, "val_loss": 203418.15185546875, "val_acc": 48.0}
{"epoch": 26, "training_loss": 845120.474609375, "training_acc": 47.0, "val_loss": 183567.3828125, "val_acc": 48.0}
{"epoch": 27, "training_loss": 594717.0302734375, "training_acc": 47.0, "val_loss": 63733.685302734375, "val_acc": 52.0}
{"epoch": 28, "training_loss": 350600.21875, "training_acc": 53.0, "val_loss": 153984.16748046875, "val_acc": 52.0}
{"epoch": 29, "training_loss": 549064.876953125, "training_acc": 53.0, "val_loss": 25886.495971679688, "val_acc": 52.0}
{"epoch": 30, "training_loss": 248814.0703125, "training_acc": 51.0, "val_loss": 199888.28125, "val_acc": 48.0}
{"epoch": 31, "training_loss": 817190.54296875, "training_acc": 47.0, "val_loss": 135898.6083984375, "val_acc": 48.0}
{"epoch": 32, "training_loss": 372326.41009521484, "training_acc": 47.0, "val_loss": 156983.67919921875, "val_acc": 52.0}
{"epoch": 33, "training_loss": 701446.505859375, "training_acc": 53.0, "val_loss": 281113.720703125, "val_acc": 52.0}
{"epoch": 34, "training_loss": 1073668.953125, "training_acc": 53.0, "val_loss": 190969.39697265625, "val_acc": 52.0}
{"epoch": 35, "training_loss": 576079.484375, "training_acc": 53.0, "val_loss": 83915.02685546875, "val_acc": 48.0}
{"epoch": 36, "training_loss": 449177.6953125, "training_acc": 47.0, "val_loss": 187367.81005859375, "val_acc": 48.0}
{"epoch": 37, "training_loss": 689787.4140625, "training_acc": 47.0, "val_loss": 27386.77978515625, "val_acc": 48.0}
{"epoch": 38, "training_loss": 306794.056640625, "training_acc": 41.0, "val_loss": 203720.37353515625, "val_acc": 52.0}
{"epoch": 39, "training_loss": 845184.9140625, "training_acc": 53.0, "val_loss": 176085.3759765625, "val_acc": 52.0}
{"epoch": 40, "training_loss": 544403.037109375, "training_acc": 53.0, "val_loss": 71445.03784179688, "val_acc": 48.0}
