"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 49013.21912384033, "training_acc": 47.0, "val_loss": 17272.959899902344, "val_acc": 48.0}
{"epoch": 1, "training_loss": 53657.067932128906, "training_acc": 49.0, "val_loss": 10178.752899169922, "val_acc": 52.0}
{"epoch": 2, "training_loss": 37839.958068847656, "training_acc": 53.0, "val_loss": 6465.9881591796875, "val_acc": 48.0}
{"epoch": 3, "training_loss": 38565.901611328125, "training_acc": 47.0, "val_loss": 925.1948356628418, "val_acc": 52.0}
{"epoch": 4, "training_loss": 12845.664611816406, "training_acc": 53.0, "val_loss": 1993.316650390625, "val_acc": 48.0}
{"epoch": 5, "training_loss": 9373.909973144531, "training_acc": 45.0, "val_loss": 3811.3338470458984, "val_acc": 52.0}
{"epoch": 6, "training_loss": 10963.767578125, "training_acc": 51.0, "val_loss": 2663.2015228271484, "val_acc": 48.0}
{"epoch": 7, "training_loss": 4729.6373291015625, "training_acc": 57.0, "val_loss": 2318.861770629883, "val_acc": 52.0}
{"epoch": 8, "training_loss": 8350.366088867188, "training_acc": 51.0, "val_loss": 4386.423873901367, "val_acc": 48.0}
{"epoch": 9, "training_loss": 12811.300415039062, "training_acc": 49.0, "val_loss": 2625.038528442383, "val_acc": 52.0}
{"epoch": 10, "training_loss": 9492.488342285156, "training_acc": 49.0, "val_loss": 3379.4925689697266, "val_acc": 52.0}
{"epoch": 11, "training_loss": 19311.756118774414, "training_acc": 53.0, "val_loss": 482.89804458618164, "val_acc": 48.0}
{"epoch": 12, "training_loss": 2992.9973754882812, "training_acc": 49.0, "val_loss": 1500.5656242370605, "val_acc": 48.0}
{"epoch": 13, "training_loss": 7571.6300048828125, "training_acc": 45.0, "val_loss": 2158.6097717285156, "val_acc": 52.0}
{"epoch": 14, "training_loss": 8935.855758666992, "training_acc": 47.0, "val_loss": 3760.443878173828, "val_acc": 52.0}
{"epoch": 15, "training_loss": 19514.55223083496, "training_acc": 53.0, "val_loss": 366.2848472595215, "val_acc": 48.0}
{"epoch": 16, "training_loss": 3341.9485778808594, "training_acc": 47.0, "val_loss": 3423.589324951172, "val_acc": 48.0}
{"epoch": 17, "training_loss": 17549.54998779297, "training_acc": 47.0, "val_loss": 4851.91535949707, "val_acc": 52.0}
{"epoch": 18, "training_loss": 28276.8203125, "training_acc": 53.0, "val_loss": 5120.686340332031, "val_acc": 52.0}
{"epoch": 19, "training_loss": 16037.534851074219, "training_acc": 51.0, "val_loss": 10200.697326660156, "val_acc": 48.0}
{"epoch": 20, "training_loss": 31841.85124206543, "training_acc": 47.0, "val_loss": 3185.2752685546875, "val_acc": 52.0}
{"epoch": 21, "training_loss": 10965.4423828125, "training_acc": 49.0, "val_loss": 1945.4689025878906, "val_acc": 48.0}
{"epoch": 22, "training_loss": 6601.820508956909, "training_acc": 51.0, "val_loss": 2350.5844116210938, "val_acc": 48.0}
{"epoch": 23, "training_loss": 7289.073535919189, "training_acc": 45.0, "val_loss": 586.0421657562256, "val_acc": 48.0}
{"epoch": 24, "training_loss": 1870.548683166504, "training_acc": 53.0, "val_loss": 1125.6178855895996, "val_acc": 52.0}
{"epoch": 25, "training_loss": 1969.6547775268555, "training_acc": 60.0, "val_loss": 153.2442569732666, "val_acc": 52.0}
{"epoch": 26, "training_loss": 1716.6753273010254, "training_acc": 53.0, "val_loss": 637.4565124511719, "val_acc": 52.0}
{"epoch": 27, "training_loss": 6132.012878417969, "training_acc": 47.0, "val_loss": 1273.2688903808594, "val_acc": 52.0}
{"epoch": 28, "training_loss": 4281.5751953125, "training_acc": 62.0, "val_loss": 358.6275577545166, "val_acc": 48.0}
{"epoch": 29, "training_loss": 18880.473266601562, "training_acc": 39.0, "val_loss": 2409.8268508911133, "val_acc": 52.0}
{"epoch": 30, "training_loss": 14914.650756835938, "training_acc": 51.0, "val_loss": 3367.996597290039, "val_acc": 48.0}
{"epoch": 31, "training_loss": 15988.308959960938, "training_acc": 41.0, "val_loss": 324.02093410491943, "val_acc": 52.0}
{"epoch": 32, "training_loss": 19254.42105102539, "training_acc": 51.0, "val_loss": 4688.93928527832, "val_acc": 48.0}
{"epoch": 33, "training_loss": 11557.557739257812, "training_acc": 53.0, "val_loss": 1196.5938568115234, "val_acc": 52.0}
{"epoch": 34, "training_loss": 11460.206176757812, "training_acc": 53.0, "val_loss": 2754.6005249023438, "val_acc": 48.0}
{"epoch": 35, "training_loss": 5272.539245605469, "training_acc": 49.0, "val_loss": 2276.529312133789, "val_acc": 52.0}
{"epoch": 36, "training_loss": 7591.48876953125, "training_acc": 55.0, "val_loss": 4876.274490356445, "val_acc": 48.0}
{"epoch": 37, "training_loss": 12699.76937866211, "training_acc": 49.0, "val_loss": 7664.813232421875, "val_acc": 52.0}
{"epoch": 38, "training_loss": 28355.513137817383, "training_acc": 53.0, "val_loss": 1773.7360000610352, "val_acc": 48.0}
{"epoch": 39, "training_loss": 9357.921272277832, "training_acc": 49.0, "val_loss": 3042.7597045898438, "val_acc": 52.0}
{"epoch": 40, "training_loss": 6699.85368347168, "training_acc": 58.0, "val_loss": 5693.315124511719, "val_acc": 48.0}
{"epoch": 41, "training_loss": 16423.231567382812, "training_acc": 47.0, "val_loss": 6258.144378662109, "val_acc": 52.0}
{"epoch": 42, "training_loss": 22193.260184288025, "training_acc": 53.0, "val_loss": 5078.1341552734375, "val_acc": 48.0}
{"epoch": 43, "training_loss": 24524.4267578125, "training_acc": 47.0, "val_loss": 2043.0809020996094, "val_acc": 52.0}
{"epoch": 44, "training_loss": 14399.462829589844, "training_acc": 53.0, "val_loss": 2119.4774627685547, "val_acc": 48.0}
