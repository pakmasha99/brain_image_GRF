"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 44105.032318115234, "training_acc": 48.0, "val_loss": 18419.593811035156, "val_acc": 56.0}
{"epoch": 1, "training_loss": 66066.11712646484, "training_acc": 52.0, "val_loss": 11383.405303955078, "val_acc": 44.0}
{"epoch": 2, "training_loss": 64479.114013671875, "training_acc": 48.0, "val_loss": 9166.999053955078, "val_acc": 44.0}
{"epoch": 3, "training_loss": 24046.6181640625, "training_acc": 50.0, "val_loss": 5610.948181152344, "val_acc": 56.0}
{"epoch": 4, "training_loss": 19639.305541992188, "training_acc": 50.0, "val_loss": 9979.642486572266, "val_acc": 44.0}
{"epoch": 5, "training_loss": 21522.809692382812, "training_acc": 44.0, "val_loss": 2558.2685470581055, "val_acc": 56.0}
{"epoch": 6, "training_loss": 11160.224021911621, "training_acc": 50.0, "val_loss": 5657.577896118164, "val_acc": 44.0}
{"epoch": 7, "training_loss": 13418.972778320312, "training_acc": 52.0, "val_loss": 5650.875473022461, "val_acc": 56.0}
{"epoch": 8, "training_loss": 13997.751403808594, "training_acc": 52.0, "val_loss": 9706.849670410156, "val_acc": 44.0}
{"epoch": 9, "training_loss": 35791.4423828125, "training_acc": 48.0, "val_loss": 1431.796932220459, "val_acc": 56.0}
{"epoch": 10, "training_loss": 9575.181106567383, "training_acc": 50.0, "val_loss": 307.6362133026123, "val_acc": 44.0}
{"epoch": 11, "training_loss": 7500.889953613281, "training_acc": 54.0, "val_loss": 1019.4438934326172, "val_acc": 44.0}
{"epoch": 12, "training_loss": 3232.2362937927246, "training_acc": 43.0, "val_loss": 241.61674976348877, "val_acc": 44.0}
{"epoch": 13, "training_loss": 3437.6312561035156, "training_acc": 56.0, "val_loss": 241.5182113647461, "val_acc": 44.0}
{"epoch": 14, "training_loss": 10141.517272949219, "training_acc": 50.0, "val_loss": 910.899543762207, "val_acc": 44.0}
{"epoch": 15, "training_loss": 3321.4228515625, "training_acc": 52.0, "val_loss": 6279.475402832031, "val_acc": 56.0}
{"epoch": 16, "training_loss": 27514.54345703125, "training_acc": 52.0, "val_loss": 4179.317474365234, "val_acc": 44.0}
{"epoch": 17, "training_loss": 26811.903411865234, "training_acc": 48.0, "val_loss": 3730.2894592285156, "val_acc": 44.0}
{"epoch": 18, "training_loss": 13986.924072265625, "training_acc": 46.0, "val_loss": 586.7854595184326, "val_acc": 56.0}
{"epoch": 19, "training_loss": 18547.287109375, "training_acc": 48.0, "val_loss": 4171.1151123046875, "val_acc": 44.0}
{"epoch": 20, "training_loss": 12985.985778808594, "training_acc": 52.0, "val_loss": 2505.898666381836, "val_acc": 56.0}
{"epoch": 21, "training_loss": 9947.492462158203, "training_acc": 48.0, "val_loss": 1832.2111129760742, "val_acc": 56.0}
{"epoch": 22, "training_loss": 9567.766494750977, "training_acc": 54.0, "val_loss": 3264.97802734375, "val_acc": 44.0}
{"epoch": 23, "training_loss": 8908.357421875, "training_acc": 46.0, "val_loss": 1675.0003814697266, "val_acc": 56.0}
{"epoch": 24, "training_loss": 7240.893371582031, "training_acc": 54.0, "val_loss": 5367.991256713867, "val_acc": 44.0}
{"epoch": 25, "training_loss": 12928.040954589844, "training_acc": 52.0, "val_loss": 3781.8740844726562, "val_acc": 56.0}
{"epoch": 26, "training_loss": 8544.889892578125, "training_acc": 62.0, "val_loss": 3862.2974395751953, "val_acc": 44.0}
{"epoch": 27, "training_loss": 12493.017486572266, "training_acc": 50.0, "val_loss": 5867.679214477539, "val_acc": 56.0}
{"epoch": 28, "training_loss": 15774.560913085938, "training_acc": 46.0, "val_loss": 1260.7264518737793, "val_acc": 44.0}
{"epoch": 29, "training_loss": 11999.455810546875, "training_acc": 48.0, "val_loss": 1835.268211364746, "val_acc": 56.0}
{"epoch": 30, "training_loss": 4745.464424133301, "training_acc": 54.0, "val_loss": 1198.2833862304688, "val_acc": 56.0}
{"epoch": 31, "training_loss": 3581.4222412109375, "training_acc": 50.0, "val_loss": 4494.761657714844, "val_acc": 56.0}
{"epoch": 32, "training_loss": 17195.97723388672, "training_acc": 50.0, "val_loss": 1843.1806564331055, "val_acc": 44.0}
