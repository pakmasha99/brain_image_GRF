"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 40811.60617828369, "training_acc": 53.0, "val_loss": 17740.951538085938, "val_acc": 48.0}
{"epoch": 1, "training_loss": 48223.986572265625, "training_acc": 47.0, "val_loss": 12734.718322753906, "val_acc": 52.0}
{"epoch": 2, "training_loss": 47497.765625, "training_acc": 53.0, "val_loss": 4390.4876708984375, "val_acc": 48.0}
{"epoch": 3, "training_loss": 29698.158203125, "training_acc": 47.0, "val_loss": 1426.2651443481445, "val_acc": 48.0}
{"epoch": 4, "training_loss": 12241.5673828125, "training_acc": 59.0, "val_loss": 4275.004196166992, "val_acc": 52.0}
{"epoch": 5, "training_loss": 16901.017028808594, "training_acc": 45.0, "val_loss": 7642.671966552734, "val_acc": 48.0}
{"epoch": 6, "training_loss": 18570.848205566406, "training_acc": 47.0, "val_loss": 3999.050521850586, "val_acc": 52.0}
{"epoch": 7, "training_loss": 8168.211090087891, "training_acc": 61.0, "val_loss": 813.3755683898926, "val_acc": 48.0}
{"epoch": 8, "training_loss": 14202.763916015625, "training_acc": 47.0, "val_loss": 418.94259452819824, "val_acc": 52.0}
{"epoch": 9, "training_loss": 19846.17401123047, "training_acc": 53.0, "val_loss": 3777.9857635498047, "val_acc": 48.0}
{"epoch": 10, "training_loss": 16181.7177734375, "training_acc": 51.0, "val_loss": 4587.397384643555, "val_acc": 52.0}
{"epoch": 11, "training_loss": 15368.361938476562, "training_acc": 51.0, "val_loss": 8700.187683105469, "val_acc": 48.0}
{"epoch": 12, "training_loss": 18791.886108398438, "training_acc": 51.0, "val_loss": 12381.895446777344, "val_acc": 52.0}
{"epoch": 13, "training_loss": 58176.29248046875, "training_acc": 53.0, "val_loss": 10484.320831298828, "val_acc": 52.0}
{"epoch": 14, "training_loss": 16947.46209716797, "training_acc": 61.0, "val_loss": 9495.9716796875, "val_acc": 48.0}
{"epoch": 15, "training_loss": 34411.894134521484, "training_acc": 47.0, "val_loss": 5240.203094482422, "val_acc": 52.0}
{"epoch": 16, "training_loss": 30011.7900390625, "training_acc": 53.0, "val_loss": 2943.446731567383, "val_acc": 52.0}
{"epoch": 17, "training_loss": 19579.29150390625, "training_acc": 43.0, "val_loss": 3839.3619537353516, "val_acc": 48.0}
{"epoch": 18, "training_loss": 8742.474792480469, "training_acc": 57.0, "val_loss": 592.387580871582, "val_acc": 52.0}
{"epoch": 19, "training_loss": 11437.188293457031, "training_acc": 51.0, "val_loss": 2583.8125228881836, "val_acc": 52.0}
{"epoch": 20, "training_loss": 14343.590591430664, "training_acc": 53.0, "val_loss": 2053.7763595581055, "val_acc": 48.0}
{"epoch": 21, "training_loss": 5804.33553314209, "training_acc": 53.0, "val_loss": 1386.65132522583, "val_acc": 48.0}
{"epoch": 22, "training_loss": 6225.598648071289, "training_acc": 47.0, "val_loss": 1379.9407958984375, "val_acc": 52.0}
{"epoch": 23, "training_loss": 5152.504547119141, "training_acc": 55.0, "val_loss": 984.4215393066406, "val_acc": 48.0}
{"epoch": 24, "training_loss": 2559.242233276367, "training_acc": 63.0, "val_loss": 2440.6225204467773, "val_acc": 48.0}
{"epoch": 25, "training_loss": 7204.380752563477, "training_acc": 51.0, "val_loss": 2219.308853149414, "val_acc": 48.0}
{"epoch": 26, "training_loss": 6947.546783447266, "training_acc": 50.0, "val_loss": 1888.1752014160156, "val_acc": 48.0}
{"epoch": 27, "training_loss": 7590.253845214844, "training_acc": 49.0, "val_loss": 6075.410842895508, "val_acc": 52.0}
