"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 52754.888748168945, "training_acc": 51.0, "val_loss": 9544.07730102539, "val_acc": 48.0}
{"epoch": 1, "training_loss": 25912.125732421875, "training_acc": 49.0, "val_loss": 6964.571380615234, "val_acc": 52.0}
{"epoch": 2, "training_loss": 26321.5078125, "training_acc": 41.0, "val_loss": 6022.555923461914, "val_acc": 48.0}
{"epoch": 3, "training_loss": 12815.796936035156, "training_acc": 63.0, "val_loss": 13275.201416015625, "val_acc": 52.0}
{"epoch": 4, "training_loss": 43738.90478515625, "training_acc": 53.0, "val_loss": 3859.2544555664062, "val_acc": 48.0}
{"epoch": 5, "training_loss": 27413.2001953125, "training_acc": 47.0, "val_loss": 2735.256004333496, "val_acc": 48.0}
{"epoch": 6, "training_loss": 13221.993774414062, "training_acc": 51.0, "val_loss": 2455.6861877441406, "val_acc": 52.0}
{"epoch": 7, "training_loss": 11001.333801269531, "training_acc": 47.0, "val_loss": 2257.573127746582, "val_acc": 52.0}
{"epoch": 8, "training_loss": 10525.532293319702, "training_acc": 53.0, "val_loss": 4750.157165527344, "val_acc": 48.0}
{"epoch": 9, "training_loss": 22870.45196533203, "training_acc": 47.0, "val_loss": 3044.072914123535, "val_acc": 52.0}
{"epoch": 10, "training_loss": 17489.772705078125, "training_acc": 53.0, "val_loss": 2248.196220397949, "val_acc": 48.0}
{"epoch": 11, "training_loss": 20203.9814453125, "training_acc": 47.0, "val_loss": 1091.1843299865723, "val_acc": 52.0}
{"epoch": 12, "training_loss": 11690.133972167969, "training_acc": 53.0, "val_loss": 1837.4931335449219, "val_acc": 48.0}
{"epoch": 13, "training_loss": 9517.933937072754, "training_acc": 49.0, "val_loss": 3363.174057006836, "val_acc": 52.0}
{"epoch": 14, "training_loss": 9870.532287597656, "training_acc": 53.0, "val_loss": 5653.666687011719, "val_acc": 48.0}
{"epoch": 15, "training_loss": 15247.519439697266, "training_acc": 49.0, "val_loss": 7596.529388427734, "val_acc": 52.0}
{"epoch": 16, "training_loss": 28681.74462890625, "training_acc": 53.0, "val_loss": 1539.1798973083496, "val_acc": 48.0}
{"epoch": 17, "training_loss": 10093.37776184082, "training_acc": 45.0, "val_loss": 386.59117221832275, "val_acc": 48.0}
{"epoch": 18, "training_loss": 2608.027633666992, "training_acc": 55.0, "val_loss": 3083.803939819336, "val_acc": 48.0}
{"epoch": 19, "training_loss": 8819.439819335938, "training_acc": 51.0, "val_loss": 4054.736328125, "val_acc": 52.0}
{"epoch": 20, "training_loss": 8740.73112487793, "training_acc": 53.0, "val_loss": 633.1453800201416, "val_acc": 52.0}
{"epoch": 21, "training_loss": 3853.8666076660156, "training_acc": 55.0, "val_loss": 172.15548753738403, "val_acc": 56.0}
{"epoch": 22, "training_loss": 1943.8550720214844, "training_acc": 60.0, "val_loss": 965.7764434814453, "val_acc": 48.0}
{"epoch": 23, "training_loss": 5331.052230834961, "training_acc": 57.0, "val_loss": 3301.4907836914062, "val_acc": 48.0}
{"epoch": 24, "training_loss": 13464.83504486084, "training_acc": 49.0, "val_loss": 3979.962158203125, "val_acc": 52.0}
{"epoch": 25, "training_loss": 11799.260375976562, "training_acc": 55.0, "val_loss": 5354.89387512207, "val_acc": 48.0}
{"epoch": 26, "training_loss": 15175.24526977539, "training_acc": 49.0, "val_loss": 6358.20198059082, "val_acc": 52.0}
{"epoch": 27, "training_loss": 23119.784645080566, "training_acc": 53.0, "val_loss": 3691.7659759521484, "val_acc": 48.0}
{"epoch": 28, "training_loss": 18367.940223693848, "training_acc": 47.0, "val_loss": 3556.601333618164, "val_acc": 52.0}
{"epoch": 29, "training_loss": 16701.41259765625, "training_acc": 53.0, "val_loss": 3311.396026611328, "val_acc": 48.0}
{"epoch": 30, "training_loss": 16069.464294433594, "training_acc": 47.0, "val_loss": 6262.40119934082, "val_acc": 52.0}
{"epoch": 31, "training_loss": 38371.82177734375, "training_acc": 53.0, "val_loss": 7814.171600341797, "val_acc": 52.0}
{"epoch": 32, "training_loss": 21179.9375, "training_acc": 53.0, "val_loss": 9752.388763427734, "val_acc": 48.0}
{"epoch": 33, "training_loss": 22339.643920898438, "training_acc": 51.0, "val_loss": 12513.968658447266, "val_acc": 52.0}
{"epoch": 34, "training_loss": 57768.64111328125, "training_acc": 53.0, "val_loss": 11552.0263671875, "val_acc": 52.0}
{"epoch": 35, "training_loss": 20214.619873046875, "training_acc": 62.0, "val_loss": 6703.251647949219, "val_acc": 48.0}
{"epoch": 36, "training_loss": 17302.100524902344, "training_acc": 49.0, "val_loss": 9217.659759521484, "val_acc": 52.0}
{"epoch": 37, "training_loss": 37020.335693359375, "training_acc": 53.0, "val_loss": 1526.4443397521973, "val_acc": 52.0}
{"epoch": 38, "training_loss": 25792.363037109375, "training_acc": 47.0, "val_loss": 7940.096282958984, "val_acc": 48.0}
{"epoch": 39, "training_loss": 22346.143188476562, "training_acc": 47.0, "val_loss": 11264.580535888672, "val_acc": 52.0}
{"epoch": 40, "training_loss": 41033.15283203125, "training_acc": 53.0, "val_loss": 719.4081783294678, "val_acc": 48.0}
