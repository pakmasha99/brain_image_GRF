"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 49047.52346420288, "training_acc": 45.0, "val_loss": 16429.940795898438, "val_acc": 52.0}
{"epoch": 1, "training_loss": 46716.46350097656, "training_acc": 53.0, "val_loss": 8093.072509765625, "val_acc": 48.0}
{"epoch": 2, "training_loss": 22641.21044921875, "training_acc": 51.0, "val_loss": 10287.224578857422, "val_acc": 52.0}
{"epoch": 3, "training_loss": 43769.538330078125, "training_acc": 53.0, "val_loss": 1648.654556274414, "val_acc": 52.0}
{"epoch": 4, "training_loss": 27157.929931640625, "training_acc": 51.0, "val_loss": 9828.021240234375, "val_acc": 48.0}
{"epoch": 5, "training_loss": 18756.310607910156, "training_acc": 63.0, "val_loss": 6705.323791503906, "val_acc": 52.0}
{"epoch": 6, "training_loss": 17112.62579345703, "training_acc": 53.0, "val_loss": 7631.358337402344, "val_acc": 48.0}
{"epoch": 7, "training_loss": 24637.363677978516, "training_acc": 49.0, "val_loss": 5220.044326782227, "val_acc": 52.0}
{"epoch": 8, "training_loss": 20293.166374206543, "training_acc": 53.0, "val_loss": 5442.198944091797, "val_acc": 48.0}
{"epoch": 9, "training_loss": 28781.99169921875, "training_acc": 47.0, "val_loss": 2444.2163467407227, "val_acc": 52.0}
{"epoch": 10, "training_loss": 19231.651641845703, "training_acc": 53.0, "val_loss": 177.785062789917, "val_acc": 52.0}
{"epoch": 11, "training_loss": 22406.871292114258, "training_acc": 43.0, "val_loss": 5485.470962524414, "val_acc": 48.0}
{"epoch": 12, "training_loss": 14523.123046875, "training_acc": 51.0, "val_loss": 10700.094604492188, "val_acc": 52.0}
{"epoch": 13, "training_loss": 35013.51770019531, "training_acc": 53.0, "val_loss": 5415.660858154297, "val_acc": 48.0}
{"epoch": 14, "training_loss": 30401.78631591797, "training_acc": 47.0, "val_loss": 2995.334815979004, "val_acc": 48.0}
{"epoch": 15, "training_loss": 11911.004974365234, "training_acc": 61.0, "val_loss": 3884.1175079345703, "val_acc": 52.0}
{"epoch": 16, "training_loss": 9071.042304992676, "training_acc": 42.0, "val_loss": 192.17150211334229, "val_acc": 52.0}
{"epoch": 17, "training_loss": 6852.970932006836, "training_acc": 49.0, "val_loss": 1553.7678718566895, "val_acc": 48.0}
{"epoch": 18, "training_loss": 5538.001220703125, "training_acc": 53.0, "val_loss": 413.8056755065918, "val_acc": 48.0}
{"epoch": 19, "training_loss": 4222.905456542969, "training_acc": 47.0, "val_loss": 1683.8136672973633, "val_acc": 52.0}
{"epoch": 20, "training_loss": 4223.909698486328, "training_acc": 49.0, "val_loss": 3474.353790283203, "val_acc": 52.0}
{"epoch": 21, "training_loss": 6332.825546264648, "training_acc": 63.0, "val_loss": 4750.789260864258, "val_acc": 48.0}
{"epoch": 22, "training_loss": 11947.079467773438, "training_acc": 47.0, "val_loss": 1382.6580047607422, "val_acc": 52.0}
{"epoch": 23, "training_loss": 9436.542510986328, "training_acc": 35.0, "val_loss": 148.0212926864624, "val_acc": 48.0}
{"epoch": 24, "training_loss": 5106.028350830078, "training_acc": 55.0, "val_loss": 2023.3747482299805, "val_acc": 52.0}
{"epoch": 25, "training_loss": 4432.748840332031, "training_acc": 41.0, "val_loss": 918.4151649475098, "val_acc": 52.0}
{"epoch": 26, "training_loss": 5136.386093139648, "training_acc": 49.0, "val_loss": 344.06447410583496, "val_acc": 48.0}
{"epoch": 27, "training_loss": 7698.103851318359, "training_acc": 61.0, "val_loss": 472.5635528564453, "val_acc": 52.0}
{"epoch": 28, "training_loss": 13852.591217041016, "training_acc": 49.0, "val_loss": 1164.1736030578613, "val_acc": 48.0}
{"epoch": 29, "training_loss": 8856.343872070312, "training_acc": 65.0, "val_loss": 1935.586166381836, "val_acc": 52.0}
{"epoch": 30, "training_loss": 9520.479568481445, "training_acc": 53.0, "val_loss": 200.3551721572876, "val_acc": 56.0}
{"epoch": 31, "training_loss": 2077.293888568878, "training_acc": 59.0, "val_loss": 1601.6643524169922, "val_acc": 52.0}
{"epoch": 32, "training_loss": 7366.158142089844, "training_acc": 45.0, "val_loss": 301.8282413482666, "val_acc": 52.0}
{"epoch": 33, "training_loss": 1844.4716567993164, "training_acc": 59.0, "val_loss": 2500.51212310791, "val_acc": 52.0}
{"epoch": 34, "training_loss": 9049.01318359375, "training_acc": 51.0, "val_loss": 3816.7442321777344, "val_acc": 48.0}
{"epoch": 35, "training_loss": 9188.983764648438, "training_acc": 57.0, "val_loss": 4352.105712890625, "val_acc": 52.0}
{"epoch": 36, "training_loss": 14018.333251953125, "training_acc": 47.0, "val_loss": 3926.3427734375, "val_acc": 48.0}
{"epoch": 37, "training_loss": 6230.14436340332, "training_acc": 59.0, "val_loss": 770.3634262084961, "val_acc": 48.0}
{"epoch": 38, "training_loss": 2284.452102661133, "training_acc": 49.0, "val_loss": 438.3737564086914, "val_acc": 48.0}
{"epoch": 39, "training_loss": 2047.2559356689453, "training_acc": 53.0, "val_loss": 491.0102844238281, "val_acc": 52.0}
{"epoch": 40, "training_loss": 1827.7479023262858, "training_acc": 60.0, "val_loss": 202.21569538116455, "val_acc": 52.0}
{"epoch": 41, "training_loss": 2115.7122230529785, "training_acc": 56.0, "val_loss": 953.7545204162598, "val_acc": 48.0}
{"epoch": 42, "training_loss": 2121.978317260742, "training_acc": 56.0, "val_loss": 2711.6018295288086, "val_acc": 52.0}
