"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 61376.466400146484, "training_acc": 48.0, "val_loss": 25622.695922851562, "val_acc": 56.0}
{"epoch": 1, "training_loss": 91939.87585449219, "training_acc": 52.0, "val_loss": 15816.983032226562, "val_acc": 44.0}
{"epoch": 2, "training_loss": 89665.650390625, "training_acc": 48.0, "val_loss": 12767.32177734375, "val_acc": 44.0}
{"epoch": 3, "training_loss": 33437.3388671875, "training_acc": 50.0, "val_loss": 7802.363586425781, "val_acc": 56.0}
{"epoch": 4, "training_loss": 27320.466430664062, "training_acc": 50.0, "val_loss": 13897.357177734375, "val_acc": 44.0}
{"epoch": 5, "training_loss": 29956.044372558594, "training_acc": 44.0, "val_loss": 3546.219253540039, "val_acc": 56.0}
{"epoch": 6, "training_loss": 15523.771697998047, "training_acc": 50.0, "val_loss": 7912.931060791016, "val_acc": 44.0}
{"epoch": 7, "training_loss": 18711.6337890625, "training_acc": 52.0, "val_loss": 7817.441558837891, "val_acc": 56.0}
{"epoch": 8, "training_loss": 19296.0625, "training_acc": 52.0, "val_loss": 13571.200561523438, "val_acc": 44.0}
{"epoch": 9, "training_loss": 50070.1943359375, "training_acc": 48.0, "val_loss": 1922.5194931030273, "val_acc": 56.0}
{"epoch": 10, "training_loss": 13043.757720947266, "training_acc": 50.0, "val_loss": 517.5780296325684, "val_acc": 44.0}
{"epoch": 11, "training_loss": 10332.622497558594, "training_acc": 54.0, "val_loss": 1507.3243141174316, "val_acc": 44.0}
{"epoch": 12, "training_loss": 4612.075443267822, "training_acc": 44.0, "val_loss": 3564.9463653564453, "val_acc": 56.0}
{"epoch": 13, "training_loss": 12138.260375976562, "training_acc": 50.0, "val_loss": 5751.365661621094, "val_acc": 44.0}
{"epoch": 14, "training_loss": 14972.663208007812, "training_acc": 52.0, "val_loss": 5263.407135009766, "val_acc": 56.0}
{"epoch": 15, "training_loss": 20044.373779296875, "training_acc": 46.0, "val_loss": 5208.997344970703, "val_acc": 44.0}
{"epoch": 16, "training_loss": 18627.166870117188, "training_acc": 46.0, "val_loss": 305.16295433044434, "val_acc": 56.0}
{"epoch": 17, "training_loss": 26075.221069335938, "training_acc": 50.0, "val_loss": 9483.63265991211, "val_acc": 44.0}
{"epoch": 18, "training_loss": 23708.10693359375, "training_acc": 50.0, "val_loss": 7314.626312255859, "val_acc": 56.0}
{"epoch": 19, "training_loss": 19189.800048828125, "training_acc": 52.0, "val_loss": 3334.3910217285156, "val_acc": 44.0}
{"epoch": 20, "training_loss": 11092.885833740234, "training_acc": 52.0, "val_loss": 2512.921905517578, "val_acc": 44.0}
{"epoch": 21, "training_loss": 8468.210083007812, "training_acc": 48.0, "val_loss": 6210.813903808594, "val_acc": 56.0}
{"epoch": 22, "training_loss": 18438.213256835938, "training_acc": 54.0, "val_loss": 9848.396301269531, "val_acc": 44.0}
{"epoch": 23, "training_loss": 31510.642379760742, "training_acc": 48.0, "val_loss": 9341.232299804688, "val_acc": 56.0}
{"epoch": 24, "training_loss": 68214.4423828125, "training_acc": 52.0, "val_loss": 14977.827453613281, "val_acc": 56.0}
{"epoch": 25, "training_loss": 33955.71415710449, "training_acc": 55.0, "val_loss": 11237.02621459961, "val_acc": 44.0}
{"epoch": 26, "training_loss": 32752.551513671875, "training_acc": 48.0, "val_loss": 5209.270477294922, "val_acc": 56.0}
{"epoch": 27, "training_loss": 17117.47705078125, "training_acc": 52.0, "val_loss": 7457.209014892578, "val_acc": 44.0}
{"epoch": 28, "training_loss": 17295.221923828125, "training_acc": 50.0, "val_loss": 9289.959716796875, "val_acc": 56.0}
{"epoch": 29, "training_loss": 41318.408203125, "training_acc": 52.0, "val_loss": 462.29467391967773, "val_acc": 44.0}
{"epoch": 30, "training_loss": 6131.048782348633, "training_acc": 40.0, "val_loss": 1564.907169342041, "val_acc": 44.0}
{"epoch": 31, "training_loss": 4992.3035888671875, "training_acc": 58.0, "val_loss": 3173.5397338867188, "val_acc": 56.0}
{"epoch": 32, "training_loss": 11952.121337890625, "training_acc": 52.0, "val_loss": 3441.668701171875, "val_acc": 44.0}
{"epoch": 33, "training_loss": 12415.812377929688, "training_acc": 56.0, "val_loss": 1145.6452369689941, "val_acc": 44.0}
{"epoch": 34, "training_loss": 4166.587371826172, "training_acc": 48.0, "val_loss": 142.00360774993896, "val_acc": 44.0}
{"epoch": 35, "training_loss": 3345.2174072265625, "training_acc": 58.0, "val_loss": 4354.683685302734, "val_acc": 56.0}
{"epoch": 36, "training_loss": 22151.57544708252, "training_acc": 52.0, "val_loss": 5059.018325805664, "val_acc": 44.0}
{"epoch": 37, "training_loss": 20817.889450073242, "training_acc": 48.0, "val_loss": 1348.4315872192383, "val_acc": 56.0}
{"epoch": 38, "training_loss": 3128.057403564453, "training_acc": 57.0, "val_loss": 4587.932968139648, "val_acc": 44.0}
{"epoch": 39, "training_loss": 15120.100830078125, "training_acc": 46.0, "val_loss": 4624.602890014648, "val_acc": 56.0}
{"epoch": 40, "training_loss": 15531.788818359375, "training_acc": 50.0, "val_loss": 2369.565200805664, "val_acc": 44.0}
{"epoch": 41, "training_loss": 15543.89111328125, "training_acc": 54.0, "val_loss": 627.3630142211914, "val_acc": 56.0}
{"epoch": 42, "training_loss": 22446.234375, "training_acc": 54.0, "val_loss": 7698.027038574219, "val_acc": 44.0}
{"epoch": 43, "training_loss": 27788.389038085938, "training_acc": 40.0, "val_loss": 12702.210998535156, "val_acc": 56.0}
{"epoch": 44, "training_loss": 37377.95886230469, "training_acc": 52.0, "val_loss": 10045.999145507812, "val_acc": 44.0}
{"epoch": 45, "training_loss": 32845.486640930176, "training_acc": 48.0, "val_loss": 7169.152069091797, "val_acc": 56.0}
{"epoch": 46, "training_loss": 44365.91662597656, "training_acc": 52.0, "val_loss": 6547.489166259766, "val_acc": 56.0}
{"epoch": 47, "training_loss": 25152.110595703125, "training_acc": 46.0, "val_loss": 16248.992919921875, "val_acc": 44.0}
{"epoch": 48, "training_loss": 44240.41711425781, "training_acc": 46.0, "val_loss": 5041.2811279296875, "val_acc": 56.0}
{"epoch": 49, "training_loss": 15624.762908935547, "training_acc": 54.0, "val_loss": 8831.756591796875, "val_acc": 44.0}
{"epoch": 50, "training_loss": 28433.20819091797, "training_acc": 46.0, "val_loss": 2526.4129638671875, "val_acc": 56.0}
{"epoch": 51, "training_loss": 10140.067443847656, "training_acc": 52.0, "val_loss": 5364.833068847656, "val_acc": 44.0}
{"epoch": 52, "training_loss": 17075.73974609375, "training_acc": 46.0, "val_loss": 7991.596221923828, "val_acc": 56.0}
{"epoch": 53, "training_loss": 18258.604064941406, "training_acc": 52.0, "val_loss": 1873.5727310180664, "val_acc": 44.0}
