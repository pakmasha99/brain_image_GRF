"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 70015.61511611938, "training_acc": 49.0, "val_loss": 13146.832275390625, "val_acc": 48.0}
{"epoch": 1, "training_loss": 39780.4658203125, "training_acc": 47.0, "val_loss": 12715.170288085938, "val_acc": 52.0}
{"epoch": 2, "training_loss": 41472.255859375, "training_acc": 41.0, "val_loss": 9802.867126464844, "val_acc": 48.0}
{"epoch": 3, "training_loss": 22755.69873046875, "training_acc": 55.0, "val_loss": 9846.946716308594, "val_acc": 52.0}
{"epoch": 4, "training_loss": 23514.674194335938, "training_acc": 51.0, "val_loss": 10635.85205078125, "val_acc": 48.0}
{"epoch": 5, "training_loss": 34445.970153808594, "training_acc": 47.0, "val_loss": 5085.263442993164, "val_acc": 52.0}
{"epoch": 6, "training_loss": 15882.189086914062, "training_acc": 53.0, "val_loss": 5477.794647216797, "val_acc": 48.0}
{"epoch": 7, "training_loss": 11856.373748779297, "training_acc": 55.0, "val_loss": 757.6265811920166, "val_acc": 52.0}
{"epoch": 8, "training_loss": 8663.832473754883, "training_acc": 49.0, "val_loss": 541.6204929351807, "val_acc": 52.0}
{"epoch": 9, "training_loss": 6394.119049072266, "training_acc": 49.0, "val_loss": 2567.7892684936523, "val_acc": 52.0}
{"epoch": 10, "training_loss": 12796.70980834961, "training_acc": 47.0, "val_loss": 2639.6669387817383, "val_acc": 48.0}
{"epoch": 11, "training_loss": 21844.6005859375, "training_acc": 51.0, "val_loss": 6499.320983886719, "val_acc": 52.0}
{"epoch": 12, "training_loss": 12797.178527832031, "training_acc": 61.0, "val_loss": 9733.867645263672, "val_acc": 48.0}
{"epoch": 13, "training_loss": 29045.324951171875, "training_acc": 45.0, "val_loss": 6701.7608642578125, "val_acc": 52.0}
{"epoch": 14, "training_loss": 19926.839813232422, "training_acc": 53.0, "val_loss": 5744.258117675781, "val_acc": 48.0}
{"epoch": 15, "training_loss": 15746.99250793457, "training_acc": 53.0, "val_loss": 7125.648498535156, "val_acc": 52.0}
{"epoch": 16, "training_loss": 19249.25048828125, "training_acc": 51.0, "val_loss": 6888.631439208984, "val_acc": 48.0}
{"epoch": 17, "training_loss": 14913.822250366211, "training_acc": 53.0, "val_loss": 1973.5673904418945, "val_acc": 52.0}
{"epoch": 18, "training_loss": 6435.0791015625, "training_acc": 47.0, "val_loss": 4269.226837158203, "val_acc": 52.0}
{"epoch": 19, "training_loss": 10189.897369384766, "training_acc": 51.0, "val_loss": 490.0093078613281, "val_acc": 52.0}
{"epoch": 20, "training_loss": 2634.7469177246094, "training_acc": 56.0, "val_loss": 1546.0206985473633, "val_acc": 52.0}
{"epoch": 21, "training_loss": 6568.9931640625, "training_acc": 55.0, "val_loss": 1771.333885192871, "val_acc": 48.0}
{"epoch": 22, "training_loss": 8204.294876098633, "training_acc": 55.0, "val_loss": 3905.656051635742, "val_acc": 48.0}
{"epoch": 23, "training_loss": 16231.12646484375, "training_acc": 45.0, "val_loss": 3957.986068725586, "val_acc": 52.0}
{"epoch": 24, "training_loss": 12910.6025390625, "training_acc": 51.0, "val_loss": 6456.005096435547, "val_acc": 48.0}
{"epoch": 25, "training_loss": 13923.984741210938, "training_acc": 55.0, "val_loss": 3465.9027099609375, "val_acc": 52.0}
{"epoch": 26, "training_loss": 12344.764892578125, "training_acc": 51.0, "val_loss": 4579.93049621582, "val_acc": 48.0}
{"epoch": 27, "training_loss": 14169.926696777344, "training_acc": 53.0, "val_loss": 10679.073333740234, "val_acc": 52.0}
{"epoch": 28, "training_loss": 23647.917205810547, "training_acc": 54.0, "val_loss": 13318.894958496094, "val_acc": 48.0}
{"epoch": 29, "training_loss": 48927.92175292969, "training_acc": 47.0, "val_loss": 4534.114074707031, "val_acc": 52.0}
{"epoch": 30, "training_loss": 26904.66552734375, "training_acc": 53.0, "val_loss": 958.4897994995117, "val_acc": 48.0}
{"epoch": 31, "training_loss": 6357.829528808594, "training_acc": 49.0, "val_loss": 6004.457473754883, "val_acc": 52.0}
{"epoch": 32, "training_loss": 18750.098846435547, "training_acc": 55.0, "val_loss": 6749.6246337890625, "val_acc": 48.0}
{"epoch": 33, "training_loss": 23283.205932617188, "training_acc": 47.0, "val_loss": 5745.815658569336, "val_acc": 52.0}
{"epoch": 34, "training_loss": 16986.8037109375, "training_acc": 51.0, "val_loss": 6815.702819824219, "val_acc": 48.0}
{"epoch": 35, "training_loss": 17122.160186767578, "training_acc": 49.0, "val_loss": 3792.4549102783203, "val_acc": 52.0}
{"epoch": 36, "training_loss": 16519.206298828125, "training_acc": 45.0, "val_loss": 5246.931838989258, "val_acc": 48.0}
{"epoch": 37, "training_loss": 17615.79754638672, "training_acc": 41.0, "val_loss": 4797.585678100586, "val_acc": 48.0}
{"epoch": 38, "training_loss": 27900.95068359375, "training_acc": 47.0, "val_loss": 2581.5792083740234, "val_acc": 52.0}
