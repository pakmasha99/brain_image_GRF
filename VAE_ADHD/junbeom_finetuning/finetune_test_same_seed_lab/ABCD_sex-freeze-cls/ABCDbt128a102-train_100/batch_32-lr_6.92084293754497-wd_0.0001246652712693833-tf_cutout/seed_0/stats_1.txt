"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 67795.9504776001, "training_acc": 51.0, "val_loss": 24452.305603027344, "val_acc": 48.0}
{"epoch": 1, "training_loss": 73979.26712036133, "training_acc": 49.0, "val_loss": 15246.109008789062, "val_acc": 52.0}
{"epoch": 2, "training_loss": 52314.749267578125, "training_acc": 53.0, "val_loss": 10328.558349609375, "val_acc": 48.0}
{"epoch": 3, "training_loss": 62743.6484375, "training_acc": 47.0, "val_loss": 6609.381866455078, "val_acc": 48.0}
{"epoch": 4, "training_loss": 23939.07989501953, "training_acc": 59.0, "val_loss": 10270.291137695312, "val_acc": 52.0}
{"epoch": 5, "training_loss": 24796.60577392578, "training_acc": 49.0, "val_loss": 5224.87678527832, "val_acc": 48.0}
{"epoch": 6, "training_loss": 14605.851440429688, "training_acc": 53.0, "val_loss": 8777.436065673828, "val_acc": 52.0}
{"epoch": 7, "training_loss": 19568.58990097046, "training_acc": 60.0, "val_loss": 10382.992553710938, "val_acc": 48.0}
{"epoch": 8, "training_loss": 37151.36932373047, "training_acc": 47.0, "val_loss": 5648.462677001953, "val_acc": 52.0}
{"epoch": 9, "training_loss": 17903.571731567383, "training_acc": 55.0, "val_loss": 7574.382019042969, "val_acc": 48.0}
{"epoch": 10, "training_loss": 23470.952697753906, "training_acc": 47.0, "val_loss": 8335.501861572266, "val_acc": 52.0}
{"epoch": 11, "training_loss": 24796.8984375, "training_acc": 53.0, "val_loss": 6984.147644042969, "val_acc": 48.0}
{"epoch": 12, "training_loss": 19748.565307617188, "training_acc": 41.0, "val_loss": 95.99347710609436, "val_acc": 52.0}
{"epoch": 13, "training_loss": 8957.529157161713, "training_acc": 62.0, "val_loss": 2308.462333679199, "val_acc": 52.0}
{"epoch": 14, "training_loss": 12622.573486328125, "training_acc": 43.0, "val_loss": 1268.4321403503418, "val_acc": 48.0}
{"epoch": 15, "training_loss": 17985.904663085938, "training_acc": 53.0, "val_loss": 4263.119888305664, "val_acc": 52.0}
{"epoch": 16, "training_loss": 15118.503479003906, "training_acc": 43.0, "val_loss": 5879.087829589844, "val_acc": 52.0}
{"epoch": 17, "training_loss": 30801.325073242188, "training_acc": 53.0, "val_loss": 941.7355537414551, "val_acc": 48.0}
{"epoch": 18, "training_loss": 6740.3853759765625, "training_acc": 45.0, "val_loss": 3990.7615661621094, "val_acc": 52.0}
{"epoch": 19, "training_loss": 15641.029418945312, "training_acc": 49.0, "val_loss": 7002.074432373047, "val_acc": 48.0}
{"epoch": 20, "training_loss": 20788.037109375, "training_acc": 49.0, "val_loss": 10944.204711914062, "val_acc": 52.0}
{"epoch": 21, "training_loss": 28381.057861328125, "training_acc": 51.0, "val_loss": 10104.999542236328, "val_acc": 48.0}
{"epoch": 22, "training_loss": 30472.7412109375, "training_acc": 45.0, "val_loss": 5903.357315063477, "val_acc": 52.0}
{"epoch": 23, "training_loss": 15379.091613769531, "training_acc": 53.0, "val_loss": 9372.14126586914, "val_acc": 48.0}
{"epoch": 24, "training_loss": 27898.773986816406, "training_acc": 49.0, "val_loss": 8086.6455078125, "val_acc": 52.0}
{"epoch": 25, "training_loss": 31661.686462402344, "training_acc": 53.0, "val_loss": 5049.723434448242, "val_acc": 48.0}
{"epoch": 26, "training_loss": 27088.59553527832, "training_acc": 47.0, "val_loss": 2999.2435455322266, "val_acc": 52.0}
{"epoch": 27, "training_loss": 11885.592590332031, "training_acc": 53.0, "val_loss": 3200.747299194336, "val_acc": 48.0}
{"epoch": 28, "training_loss": 12824.288757324219, "training_acc": 47.0, "val_loss": 8161.968231201172, "val_acc": 52.0}
{"epoch": 29, "training_loss": 23604.62841796875, "training_acc": 39.0, "val_loss": 1025.4233360290527, "val_acc": 52.0}
{"epoch": 30, "training_loss": 4354.0380859375, "training_acc": 57.0, "val_loss": 8435.028839111328, "val_acc": 48.0}
{"epoch": 31, "training_loss": 27845.753967285156, "training_acc": 49.0, "val_loss": 5886.441421508789, "val_acc": 52.0}
