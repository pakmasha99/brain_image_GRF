"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 55466.67102241516, "training_acc": 53.0, "val_loss": 27908.2763671875, "val_acc": 48.0}
{"epoch": 1, "training_loss": 79040.66693115234, "training_acc": 49.0, "val_loss": 17365.80047607422, "val_acc": 52.0}
{"epoch": 2, "training_loss": 72176.88916015625, "training_acc": 53.0, "val_loss": 807.8344345092773, "val_acc": 52.0}
{"epoch": 3, "training_loss": 39191.91564941406, "training_acc": 55.0, "val_loss": 16947.866821289062, "val_acc": 48.0}
{"epoch": 4, "training_loss": 40568.723083496094, "training_acc": 49.0, "val_loss": 19604.33807373047, "val_acc": 52.0}
{"epoch": 5, "training_loss": 85276.69189453125, "training_acc": 53.0, "val_loss": 12527.802276611328, "val_acc": 52.0}
{"epoch": 6, "training_loss": 34506.44909667969, "training_acc": 51.0, "val_loss": 19594.546508789062, "val_acc": 48.0}
{"epoch": 7, "training_loss": 67412.82037353516, "training_acc": 47.0, "val_loss": 7529.689025878906, "val_acc": 52.0}
{"epoch": 8, "training_loss": 48860.564453125, "training_acc": 53.0, "val_loss": 12849.198913574219, "val_acc": 52.0}
{"epoch": 9, "training_loss": 27681.099731445312, "training_acc": 49.0, "val_loss": 1179.183292388916, "val_acc": 48.0}
{"epoch": 10, "training_loss": 18307.26397705078, "training_acc": 51.0, "val_loss": 2140.413475036621, "val_acc": 52.0}
{"epoch": 11, "training_loss": 21003.655395507812, "training_acc": 45.0, "val_loss": 1813.8830184936523, "val_acc": 52.0}
{"epoch": 12, "training_loss": 14643.582656860352, "training_acc": 53.0, "val_loss": 7059.514617919922, "val_acc": 48.0}
{"epoch": 13, "training_loss": 32872.384521484375, "training_acc": 47.0, "val_loss": 6319.887161254883, "val_acc": 52.0}
{"epoch": 14, "training_loss": 37705.75341796875, "training_acc": 53.0, "val_loss": 5054.888534545898, "val_acc": 52.0}
{"epoch": 15, "training_loss": 11947.15478515625, "training_acc": 59.0, "val_loss": 134.07244682312012, "val_acc": 52.0}
{"epoch": 16, "training_loss": 24440.555633544922, "training_acc": 46.0, "val_loss": 993.5024261474609, "val_acc": 52.0}
{"epoch": 17, "training_loss": 29069.4091796875, "training_acc": 51.0, "val_loss": 9908.525085449219, "val_acc": 48.0}
{"epoch": 18, "training_loss": 34345.34130859375, "training_acc": 41.0, "val_loss": 12648.341369628906, "val_acc": 52.0}
{"epoch": 19, "training_loss": 33009.333251953125, "training_acc": 51.0, "val_loss": 9657.286071777344, "val_acc": 48.0}
{"epoch": 20, "training_loss": 31367.332275390625, "training_acc": 45.0, "val_loss": 4810.605239868164, "val_acc": 52.0}
{"epoch": 21, "training_loss": 9385.229797363281, "training_acc": 57.0, "val_loss": 13186.399841308594, "val_acc": 48.0}
{"epoch": 22, "training_loss": 56171.7353515625, "training_acc": 47.0, "val_loss": 2430.4452896118164, "val_acc": 52.0}
{"epoch": 23, "training_loss": 29364.57568359375, "training_acc": 53.0, "val_loss": 5901.273345947266, "val_acc": 52.0}
{"epoch": 24, "training_loss": 14635.208251953125, "training_acc": 53.0, "val_loss": 456.4857006072998, "val_acc": 52.0}
{"epoch": 25, "training_loss": 4679.998340606689, "training_acc": 43.0, "val_loss": 333.00185203552246, "val_acc": 52.0}
{"epoch": 26, "training_loss": 7418.837837219238, "training_acc": 51.0, "val_loss": 3323.502731323242, "val_acc": 52.0}
{"epoch": 27, "training_loss": 8883.755477905273, "training_acc": 55.0, "val_loss": 685.6761932373047, "val_acc": 48.0}
{"epoch": 28, "training_loss": 14727.435363769531, "training_acc": 49.0, "val_loss": 1036.4547729492188, "val_acc": 48.0}
{"epoch": 29, "training_loss": 5192.43395614624, "training_acc": 45.0, "val_loss": 2323.14395904541, "val_acc": 48.0}
{"epoch": 30, "training_loss": 11987.960510253906, "training_acc": 41.0, "val_loss": 2577.8078079223633, "val_acc": 52.0}
{"epoch": 31, "training_loss": 10567.667572021484, "training_acc": 49.0, "val_loss": 4459.309768676758, "val_acc": 52.0}
{"epoch": 32, "training_loss": 18553.846809864044, "training_acc": 54.0, "val_loss": 5840.1947021484375, "val_acc": 48.0}
{"epoch": 33, "training_loss": 28816.48974609375, "training_acc": 47.0, "val_loss": 8850.189971923828, "val_acc": 52.0}
{"epoch": 34, "training_loss": 56542.790283203125, "training_acc": 53.0, "val_loss": 16531.82830810547, "val_acc": 52.0}
