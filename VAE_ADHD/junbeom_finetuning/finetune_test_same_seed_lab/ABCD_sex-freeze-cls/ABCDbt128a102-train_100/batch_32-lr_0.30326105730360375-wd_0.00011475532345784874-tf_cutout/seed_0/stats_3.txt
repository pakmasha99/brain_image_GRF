"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 2063.059917449951, "training_acc": 42.0, "val_loss": 360.05232334136963, "val_acc": 52.0}
{"epoch": 1, "training_loss": 803.3246536254883, "training_acc": 63.0, "val_loss": 371.08373641967773, "val_acc": 48.0}
{"epoch": 2, "training_loss": 817.3003692626953, "training_acc": 64.0, "val_loss": 320.5411672592163, "val_acc": 44.0}
{"epoch": 3, "training_loss": 710.755443572998, "training_acc": 62.0, "val_loss": 433.89782905578613, "val_acc": 36.0}
{"epoch": 4, "training_loss": 636.5159888267517, "training_acc": 61.0, "val_loss": 334.37178134918213, "val_acc": 28.0}
{"epoch": 5, "training_loss": 668.1835556030273, "training_acc": 71.0, "val_loss": 290.53049087524414, "val_acc": 40.0}
{"epoch": 6, "training_loss": 454.7282199859619, "training_acc": 67.0, "val_loss": 260.95354557037354, "val_acc": 52.0}
{"epoch": 7, "training_loss": 535.5799331665039, "training_acc": 61.0, "val_loss": 218.40074062347412, "val_acc": 44.0}
{"epoch": 8, "training_loss": 1000.551070405636, "training_acc": 61.0, "val_loss": 440.4886245727539, "val_acc": 44.0}
{"epoch": 9, "training_loss": 550.7626367807356, "training_acc": 71.0, "val_loss": 414.3251419067383, "val_acc": 44.0}
{"epoch": 10, "training_loss": 913.2789631783962, "training_acc": 63.0, "val_loss": 342.19658374786377, "val_acc": 36.0}
{"epoch": 11, "training_loss": 552.0485134124756, "training_acc": 66.0, "val_loss": 435.3625774383545, "val_acc": 48.0}
{"epoch": 12, "training_loss": 488.875, "training_acc": 69.0, "val_loss": 428.56030464172363, "val_acc": 28.0}
{"epoch": 13, "training_loss": 368.1556625366211, "training_acc": 76.0, "val_loss": 496.5007781982422, "val_acc": 28.0}
{"epoch": 14, "training_loss": 342.0112762451172, "training_acc": 70.0, "val_loss": 431.3663959503174, "val_acc": 28.0}
{"epoch": 15, "training_loss": 244.9068946838379, "training_acc": 78.0, "val_loss": 418.8910961151123, "val_acc": 44.0}
{"epoch": 16, "training_loss": 451.08641815185547, "training_acc": 73.0, "val_loss": 487.24255561828613, "val_acc": 32.0}
{"epoch": 17, "training_loss": 654.5114135742188, "training_acc": 72.0, "val_loss": 587.8977298736572, "val_acc": 44.0}
{"epoch": 18, "training_loss": 754.266544342041, "training_acc": 68.0, "val_loss": 758.7610721588135, "val_acc": 48.0}
{"epoch": 19, "training_loss": 1103.4854221343994, "training_acc": 64.0, "val_loss": 560.8119964599609, "val_acc": 60.0}
{"epoch": 20, "training_loss": 920.9060020446777, "training_acc": 69.0, "val_loss": 396.769642829895, "val_acc": 56.0}
{"epoch": 21, "training_loss": 501.43561363220215, "training_acc": 73.0, "val_loss": 479.11572456359863, "val_acc": 40.0}
{"epoch": 22, "training_loss": 919.9284057617188, "training_acc": 67.0, "val_loss": 474.58910942077637, "val_acc": 44.0}
{"epoch": 23, "training_loss": 669.9262962341309, "training_acc": 69.0, "val_loss": 427.58684158325195, "val_acc": 44.0}
{"epoch": 24, "training_loss": 443.9054260253906, "training_acc": 76.0, "val_loss": 417.3020362854004, "val_acc": 40.0}
{"epoch": 25, "training_loss": 398.8330879211426, "training_acc": 75.0, "val_loss": 470.0052261352539, "val_acc": 36.0}
{"epoch": 26, "training_loss": 608.5668277740479, "training_acc": 73.0, "val_loss": 511.4109992980957, "val_acc": 36.0}
