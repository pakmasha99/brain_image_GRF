"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 1166.5043563842773, "training_acc": 37.0, "val_loss": 536.8977069854736, "val_acc": 56.0}
{"epoch": 1, "training_loss": 1684.8512802124023, "training_acc": 57.0, "val_loss": 384.70683097839355, "val_acc": 48.0}
{"epoch": 2, "training_loss": 1356.879409790039, "training_acc": 57.0, "val_loss": 618.3840274810791, "val_acc": 40.0}
{"epoch": 3, "training_loss": 1176.436050415039, "training_acc": 61.0, "val_loss": 704.9007892608643, "val_acc": 52.0}
{"epoch": 4, "training_loss": 1568.591953157793, "training_acc": 66.0, "val_loss": 799.9457359313965, "val_acc": 48.0}
{"epoch": 5, "training_loss": 1103.8367156982422, "training_acc": 69.0, "val_loss": 531.8673610687256, "val_acc": 48.0}
{"epoch": 6, "training_loss": 1832.76806640625, "training_acc": 52.0, "val_loss": 625.8349418640137, "val_acc": 48.0}
{"epoch": 7, "training_loss": 998.6913089752197, "training_acc": 59.0, "val_loss": 579.6509742736816, "val_acc": 48.0}
{"epoch": 8, "training_loss": 1038.0735244750977, "training_acc": 71.0, "val_loss": 513.7164115905762, "val_acc": 48.0}
{"epoch": 9, "training_loss": 762.6825790405273, "training_acc": 67.0, "val_loss": 561.9821548461914, "val_acc": 44.0}
{"epoch": 10, "training_loss": 852.8452606201172, "training_acc": 59.0, "val_loss": 443.20082664489746, "val_acc": 60.0}
{"epoch": 11, "training_loss": 626.4524154663086, "training_acc": 73.0, "val_loss": 436.43808364868164, "val_acc": 56.0}
{"epoch": 12, "training_loss": 396.97419357299805, "training_acc": 75.0, "val_loss": 376.5873908996582, "val_acc": 48.0}
{"epoch": 13, "training_loss": 488.547269821167, "training_acc": 71.0, "val_loss": 526.0805130004883, "val_acc": 44.0}
{"epoch": 14, "training_loss": 856.6502380371094, "training_acc": 65.0, "val_loss": 434.1460704803467, "val_acc": 40.0}
{"epoch": 15, "training_loss": 630.778564453125, "training_acc": 62.0, "val_loss": 472.2231864929199, "val_acc": 52.0}
{"epoch": 16, "training_loss": 458.96340875083115, "training_acc": 64.0, "val_loss": 475.7558345794678, "val_acc": 48.0}
{"epoch": 17, "training_loss": 442.71709060668945, "training_acc": 71.0, "val_loss": 455.3230285644531, "val_acc": 48.0}
{"epoch": 18, "training_loss": 640.2670755386353, "training_acc": 68.0, "val_loss": 477.83360481262207, "val_acc": 48.0}
{"epoch": 19, "training_loss": 449.71406650543213, "training_acc": 73.0, "val_loss": 487.8051280975342, "val_acc": 44.0}
{"epoch": 20, "training_loss": 464.94626235961914, "training_acc": 75.0, "val_loss": 532.2728157043457, "val_acc": 52.0}
{"epoch": 21, "training_loss": 391.37358689084067, "training_acc": 78.0, "val_loss": 464.7258758544922, "val_acc": 44.0}
{"epoch": 22, "training_loss": 248.67527631204575, "training_acc": 80.0, "val_loss": 442.49682426452637, "val_acc": 36.0}
{"epoch": 23, "training_loss": 258.6756204349804, "training_acc": 79.0, "val_loss": 432.16099739074707, "val_acc": 48.0}
{"epoch": 24, "training_loss": 158.14480805397034, "training_acc": 81.0, "val_loss": 427.37598419189453, "val_acc": 40.0}
{"epoch": 25, "training_loss": 249.81756234168188, "training_acc": 79.0, "val_loss": 414.29615020751953, "val_acc": 56.0}
{"epoch": 26, "training_loss": 188.00308215641417, "training_acc": 79.0, "val_loss": 421.23355865478516, "val_acc": 52.0}
{"epoch": 27, "training_loss": 271.9540910720825, "training_acc": 80.0, "val_loss": 462.4602794647217, "val_acc": 40.0}
{"epoch": 28, "training_loss": 348.6207540035248, "training_acc": 71.0, "val_loss": 404.270601272583, "val_acc": 48.0}
{"epoch": 29, "training_loss": 237.56456661224365, "training_acc": 78.0, "val_loss": 381.6303253173828, "val_acc": 40.0}
{"epoch": 30, "training_loss": 266.10256357491016, "training_acc": 76.0, "val_loss": 388.5549306869507, "val_acc": 48.0}
{"epoch": 31, "training_loss": 278.0305395126343, "training_acc": 77.0, "val_loss": 363.3410930633545, "val_acc": 52.0}
{"epoch": 32, "training_loss": 233.64402198791504, "training_acc": 81.0, "val_loss": 421.2655544281006, "val_acc": 40.0}
{"epoch": 33, "training_loss": 453.7938575744629, "training_acc": 74.0, "val_loss": 448.07472229003906, "val_acc": 44.0}
{"epoch": 34, "training_loss": 274.8621482849121, "training_acc": 81.0, "val_loss": 520.1287746429443, "val_acc": 40.0}
{"epoch": 35, "training_loss": 484.20921516418457, "training_acc": 72.0, "val_loss": 440.2101993560791, "val_acc": 48.0}
{"epoch": 36, "training_loss": 639.9081058502197, "training_acc": 70.0, "val_loss": 541.5405750274658, "val_acc": 56.0}
{"epoch": 37, "training_loss": 560.1011352539062, "training_acc": 77.0, "val_loss": 463.26212882995605, "val_acc": 52.0}
{"epoch": 38, "training_loss": 587.1369323730469, "training_acc": 71.0, "val_loss": 434.1239929199219, "val_acc": 48.0}
{"epoch": 39, "training_loss": 685.952823638916, "training_acc": 70.0, "val_loss": 414.6751403808594, "val_acc": 48.0}
{"epoch": 40, "training_loss": 513.0638217926025, "training_acc": 77.0, "val_loss": 361.8528366088867, "val_acc": 44.0}
{"epoch": 41, "training_loss": 683.1352081298828, "training_acc": 70.0, "val_loss": 369.7981834411621, "val_acc": 52.0}
{"epoch": 42, "training_loss": 532.2462387084961, "training_acc": 76.0, "val_loss": 443.52240562438965, "val_acc": 56.0}
{"epoch": 43, "training_loss": 593.5459728240967, "training_acc": 74.0, "val_loss": 490.8209800720215, "val_acc": 56.0}
{"epoch": 44, "training_loss": 668.3471374511719, "training_acc": 72.0, "val_loss": 501.2261390686035, "val_acc": 52.0}
{"epoch": 45, "training_loss": 835.1372237204141, "training_acc": 74.0, "val_loss": 678.3276081085205, "val_acc": 60.0}
{"epoch": 46, "training_loss": 943.0516662597656, "training_acc": 72.0, "val_loss": 510.85238456726074, "val_acc": 52.0}
{"epoch": 47, "training_loss": 725.9667625427246, "training_acc": 72.0, "val_loss": 493.66488456726074, "val_acc": 44.0}
{"epoch": 48, "training_loss": 746.2850036621094, "training_acc": 67.0, "val_loss": 697.9283809661865, "val_acc": 60.0}
{"epoch": 49, "training_loss": 1293.5606689453125, "training_acc": 67.0, "val_loss": 713.2097244262695, "val_acc": 52.0}
{"epoch": 50, "training_loss": 768.9334735870361, "training_acc": 73.0, "val_loss": 647.7859497070312, "val_acc": 52.0}
{"epoch": 51, "training_loss": 1116.5257625579834, "training_acc": 69.0, "val_loss": 547.6691722869873, "val_acc": 52.0}
{"epoch": 52, "training_loss": 550.0397644042969, "training_acc": 74.0, "val_loss": 728.0679225921631, "val_acc": 56.0}
{"epoch": 53, "training_loss": 1020.3733265548944, "training_acc": 72.0, "val_loss": 587.8959655761719, "val_acc": 56.0}
{"epoch": 54, "training_loss": 603.1270179748535, "training_acc": 77.0, "val_loss": 619.969367980957, "val_acc": 48.0}
{"epoch": 55, "training_loss": 680.3570938110352, "training_acc": 77.0, "val_loss": 599.7425556182861, "val_acc": 48.0}
{"epoch": 56, "training_loss": 477.40579986572266, "training_acc": 81.0, "val_loss": 619.5261001586914, "val_acc": 52.0}
{"epoch": 57, "training_loss": 472.1180485486984, "training_acc": 78.0, "val_loss": 646.814489364624, "val_acc": 44.0}
{"epoch": 58, "training_loss": 395.02069102905807, "training_acc": 73.0, "val_loss": 639.8896217346191, "val_acc": 40.0}
{"epoch": 59, "training_loss": 222.10959601399645, "training_acc": 85.0, "val_loss": 584.3218803405762, "val_acc": 36.0}
