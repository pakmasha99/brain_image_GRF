"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 262.96711349487305, "training_acc": 48.0, "val_loss": 63.823097944259644, "val_acc": 52.0}
{"epoch": 1, "training_loss": 146.11958026885986, "training_acc": 66.0, "val_loss": 100.76541900634766, "val_acc": 52.0}
{"epoch": 2, "training_loss": 252.0940055847168, "training_acc": 57.0, "val_loss": 50.41053295135498, "val_acc": 52.0}
{"epoch": 3, "training_loss": 125.64046096801758, "training_acc": 62.0, "val_loss": 61.83422803878784, "val_acc": 52.0}
{"epoch": 4, "training_loss": 113.61147499084473, "training_acc": 65.0, "val_loss": 66.62366390228271, "val_acc": 44.0}
{"epoch": 5, "training_loss": 127.37613677978516, "training_acc": 62.0, "val_loss": 53.68258357048035, "val_acc": 48.0}
{"epoch": 6, "training_loss": 112.0399580001831, "training_acc": 63.0, "val_loss": 54.63987588882446, "val_acc": 48.0}
{"epoch": 7, "training_loss": 136.73111820220947, "training_acc": 66.0, "val_loss": 70.303875207901, "val_acc": 44.0}
{"epoch": 8, "training_loss": 153.19901823997498, "training_acc": 66.0, "val_loss": 59.982651472091675, "val_acc": 48.0}
{"epoch": 9, "training_loss": 119.39234805107117, "training_acc": 70.0, "val_loss": 56.35820031166077, "val_acc": 52.0}
{"epoch": 10, "training_loss": 103.90624237060547, "training_acc": 70.0, "val_loss": 63.07783126831055, "val_acc": 56.0}
{"epoch": 11, "training_loss": 81.91360938549042, "training_acc": 71.0, "val_loss": 56.983184814453125, "val_acc": 56.0}
{"epoch": 12, "training_loss": 67.70391607284546, "training_acc": 74.0, "val_loss": 48.75057935714722, "val_acc": 48.0}
{"epoch": 13, "training_loss": 125.64816188812256, "training_acc": 64.0, "val_loss": 50.43019652366638, "val_acc": 56.0}
{"epoch": 14, "training_loss": 95.58123481273651, "training_acc": 69.0, "val_loss": 67.77874827384949, "val_acc": 52.0}
{"epoch": 15, "training_loss": 107.37318515777588, "training_acc": 72.0, "val_loss": 52.89115905761719, "val_acc": 52.0}
{"epoch": 16, "training_loss": 113.85175943374634, "training_acc": 70.0, "val_loss": 50.042080879211426, "val_acc": 48.0}
{"epoch": 17, "training_loss": 69.41785216331482, "training_acc": 78.0, "val_loss": 81.64152503013611, "val_acc": 48.0}
{"epoch": 18, "training_loss": 115.59320855140686, "training_acc": 68.0, "val_loss": 67.40090250968933, "val_acc": 44.0}
{"epoch": 19, "training_loss": 129.5040111541748, "training_acc": 64.0, "val_loss": 81.16573691368103, "val_acc": 40.0}
{"epoch": 20, "training_loss": 84.89700675010681, "training_acc": 74.0, "val_loss": 83.78912210464478, "val_acc": 52.0}
{"epoch": 21, "training_loss": 83.12566614151001, "training_acc": 75.0, "val_loss": 90.20543694496155, "val_acc": 56.0}
{"epoch": 22, "training_loss": 71.68930888175964, "training_acc": 77.0, "val_loss": 76.35206580162048, "val_acc": 52.0}
{"epoch": 23, "training_loss": 72.2341992855072, "training_acc": 79.0, "val_loss": 67.11074709892273, "val_acc": 52.0}
{"epoch": 24, "training_loss": 67.99456071853638, "training_acc": 74.0, "val_loss": 62.06292510032654, "val_acc": 44.0}
{"epoch": 25, "training_loss": 60.28493571281433, "training_acc": 78.0, "val_loss": 68.10975670814514, "val_acc": 52.0}
{"epoch": 26, "training_loss": 75.11415137350559, "training_acc": 81.0, "val_loss": 72.0149040222168, "val_acc": 52.0}
{"epoch": 27, "training_loss": 120.75100040435791, "training_acc": 69.0, "val_loss": 68.89910697937012, "val_acc": 52.0}
{"epoch": 28, "training_loss": 132.34288692474365, "training_acc": 75.0, "val_loss": 78.49413752555847, "val_acc": 44.0}
{"epoch": 29, "training_loss": 109.3340482711792, "training_acc": 72.0, "val_loss": 110.92913150787354, "val_acc": 40.0}
{"epoch": 30, "training_loss": 118.6929726600647, "training_acc": 67.0, "val_loss": 96.43832445144653, "val_acc": 48.0}
{"epoch": 31, "training_loss": 102.08580207824707, "training_acc": 76.0, "val_loss": 94.92658972740173, "val_acc": 48.0}
