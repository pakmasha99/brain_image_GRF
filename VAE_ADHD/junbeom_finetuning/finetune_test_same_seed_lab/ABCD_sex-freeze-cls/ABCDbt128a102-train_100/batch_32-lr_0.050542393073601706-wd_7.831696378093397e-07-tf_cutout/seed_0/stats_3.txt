"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 357.16007804870605, "training_acc": 40.0, "val_loss": 65.8567488193512, "val_acc": 52.0}
{"epoch": 1, "training_loss": 148.2175178527832, "training_acc": 66.0, "val_loss": 61.448776721954346, "val_acc": 52.0}
{"epoch": 2, "training_loss": 137.65765762329102, "training_acc": 62.0, "val_loss": 53.70005965232849, "val_acc": 32.0}
{"epoch": 3, "training_loss": 119.11330938339233, "training_acc": 68.0, "val_loss": 68.42881441116333, "val_acc": 44.0}
{"epoch": 4, "training_loss": 112.25992894172668, "training_acc": 60.0, "val_loss": 57.54942297935486, "val_acc": 36.0}
{"epoch": 5, "training_loss": 109.2412519454956, "training_acc": 69.0, "val_loss": 45.30792534351349, "val_acc": 40.0}
{"epoch": 6, "training_loss": 81.0177092552185, "training_acc": 67.0, "val_loss": 39.52326774597168, "val_acc": 52.0}
{"epoch": 7, "training_loss": 92.32002258300781, "training_acc": 63.0, "val_loss": 41.01708233356476, "val_acc": 44.0}
{"epoch": 8, "training_loss": 154.32008409500122, "training_acc": 62.0, "val_loss": 65.39580225944519, "val_acc": 40.0}
{"epoch": 9, "training_loss": 89.88525364547968, "training_acc": 74.0, "val_loss": 55.84613084793091, "val_acc": 52.0}
{"epoch": 10, "training_loss": 122.75999402999878, "training_acc": 66.0, "val_loss": 55.53942918777466, "val_acc": 44.0}
{"epoch": 11, "training_loss": 83.406503200531, "training_acc": 67.0, "val_loss": 57.64099955558777, "val_acc": 44.0}
{"epoch": 12, "training_loss": 80.98960399627686, "training_acc": 68.0, "val_loss": 64.79029655456543, "val_acc": 40.0}
{"epoch": 13, "training_loss": 84.5422830581665, "training_acc": 71.0, "val_loss": 80.9515655040741, "val_acc": 36.0}
{"epoch": 14, "training_loss": 90.28598260879517, "training_acc": 74.0, "val_loss": 85.60587763786316, "val_acc": 36.0}
{"epoch": 15, "training_loss": 76.15493083000183, "training_acc": 70.0, "val_loss": 75.66007971763611, "val_acc": 32.0}
{"epoch": 16, "training_loss": 93.19277763366699, "training_acc": 68.0, "val_loss": 84.87882614135742, "val_acc": 40.0}
{"epoch": 17, "training_loss": 87.80324172973633, "training_acc": 73.0, "val_loss": 98.88491034507751, "val_acc": 36.0}
{"epoch": 18, "training_loss": 97.76490879058838, "training_acc": 75.0, "val_loss": 106.6256046295166, "val_acc": 40.0}
{"epoch": 19, "training_loss": 138.76930403709412, "training_acc": 63.0, "val_loss": 92.51612424850464, "val_acc": 40.0}
{"epoch": 20, "training_loss": 94.94267702102661, "training_acc": 71.0, "val_loss": 66.14820957183838, "val_acc": 48.0}
{"epoch": 21, "training_loss": 56.76199007034302, "training_acc": 77.0, "val_loss": 81.71998262405396, "val_acc": 36.0}
{"epoch": 22, "training_loss": 106.59889888763428, "training_acc": 73.0, "val_loss": 73.08045029640198, "val_acc": 48.0}
{"epoch": 23, "training_loss": 71.57345485687256, "training_acc": 73.0, "val_loss": 74.00152683258057, "val_acc": 48.0}
{"epoch": 24, "training_loss": 67.13503074645996, "training_acc": 78.0, "val_loss": 75.92828869819641, "val_acc": 44.0}
{"epoch": 25, "training_loss": 59.1654109954834, "training_acc": 79.0, "val_loss": 79.20224070549011, "val_acc": 40.0}
