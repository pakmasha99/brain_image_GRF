"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 86952.92712783813, "training_acc": 49.0, "val_loss": 16287.338256835938, "val_acc": 48.0}
{"epoch": 1, "training_loss": 49341.841796875, "training_acc": 47.0, "val_loss": 15831.158447265625, "val_acc": 52.0}
{"epoch": 2, "training_loss": 51515.146484375, "training_acc": 41.0, "val_loss": 12124.243927001953, "val_acc": 48.0}
{"epoch": 3, "training_loss": 28190.21630859375, "training_acc": 55.0, "val_loss": 12271.107482910156, "val_acc": 52.0}
{"epoch": 4, "training_loss": 29391.6611328125, "training_acc": 51.0, "val_loss": 13142.67578125, "val_acc": 48.0}
{"epoch": 5, "training_loss": 42564.6164855957, "training_acc": 47.0, "val_loss": 6349.415969848633, "val_acc": 52.0}
{"epoch": 6, "training_loss": 19864.910705566406, "training_acc": 53.0, "val_loss": 6751.084136962891, "val_acc": 48.0}
{"epoch": 7, "training_loss": 14649.078643798828, "training_acc": 55.0, "val_loss": 979.8709869384766, "val_acc": 52.0}
{"epoch": 8, "training_loss": 10701.571426391602, "training_acc": 49.0, "val_loss": 707.302713394165, "val_acc": 52.0}
{"epoch": 9, "training_loss": 7889.176605224609, "training_acc": 49.0, "val_loss": 3221.71630859375, "val_acc": 52.0}
{"epoch": 10, "training_loss": 15915.796264648438, "training_acc": 47.0, "val_loss": 3252.2903442382812, "val_acc": 48.0}
{"epoch": 11, "training_loss": 27146.208740234375, "training_acc": 51.0, "val_loss": 8114.105987548828, "val_acc": 52.0}
{"epoch": 12, "training_loss": 15924.262756347656, "training_acc": 61.0, "val_loss": 12039.691162109375, "val_acc": 48.0}
{"epoch": 13, "training_loss": 35923.73107910156, "training_acc": 45.0, "val_loss": 8346.588897705078, "val_acc": 52.0}
{"epoch": 14, "training_loss": 24846.318237304688, "training_acc": 53.0, "val_loss": 7090.150451660156, "val_acc": 48.0}
{"epoch": 15, "training_loss": 19381.232681274414, "training_acc": 51.0, "val_loss": 8251.789093017578, "val_acc": 52.0}
{"epoch": 16, "training_loss": 21003.91455078125, "training_acc": 51.0, "val_loss": 10210.272979736328, "val_acc": 48.0}
{"epoch": 17, "training_loss": 24410.52264404297, "training_acc": 49.0, "val_loss": 14501.519775390625, "val_acc": 52.0}
{"epoch": 18, "training_loss": 58412.64453125, "training_acc": 53.0, "val_loss": 3102.767562866211, "val_acc": 52.0}
{"epoch": 19, "training_loss": 25180.136474609375, "training_acc": 55.0, "val_loss": 8397.785186767578, "val_acc": 48.0}
{"epoch": 20, "training_loss": 27604.235107421875, "training_acc": 47.0, "val_loss": 15440.446472167969, "val_acc": 52.0}
{"epoch": 21, "training_loss": 48544.896240234375, "training_acc": 53.0, "val_loss": 12038.307189941406, "val_acc": 48.0}
{"epoch": 22, "training_loss": 80690.67895507812, "training_acc": 47.0, "val_loss": 17025.619506835938, "val_acc": 48.0}
{"epoch": 23, "training_loss": 41707.969482421875, "training_acc": 47.0, "val_loss": 13875.881958007812, "val_acc": 52.0}
{"epoch": 24, "training_loss": 36014.15869140625, "training_acc": 57.0, "val_loss": 12335.77880859375, "val_acc": 48.0}
{"epoch": 25, "training_loss": 58479.99401855469, "training_acc": 47.0, "val_loss": 2020.328712463379, "val_acc": 52.0}
{"epoch": 26, "training_loss": 26238.857666015625, "training_acc": 53.0, "val_loss": 2097.0380783081055, "val_acc": 52.0}
{"epoch": 27, "training_loss": 38672.235595703125, "training_acc": 37.0, "val_loss": 8770.610046386719, "val_acc": 48.0}
