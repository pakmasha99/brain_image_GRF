"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 76238.63192749023, "training_acc": 48.0, "val_loss": 31790.33203125, "val_acc": 56.0}
{"epoch": 1, "training_loss": 114184.23620605469, "training_acc": 52.0, "val_loss": 19570.030212402344, "val_acc": 44.0}
{"epoch": 2, "training_loss": 111160.1865234375, "training_acc": 48.0, "val_loss": 15885.3515625, "val_acc": 44.0}
{"epoch": 3, "training_loss": 41448.03662109375, "training_acc": 50.0, "val_loss": 9672.593688964844, "val_acc": 56.0}
{"epoch": 4, "training_loss": 33900.073486328125, "training_acc": 50.0, "val_loss": 17286.471557617188, "val_acc": 44.0}
{"epoch": 5, "training_loss": 37214.53137207031, "training_acc": 44.0, "val_loss": 4363.087463378906, "val_acc": 56.0}
{"epoch": 6, "training_loss": 19139.35344696045, "training_acc": 50.0, "val_loss": 9720.460510253906, "val_acc": 44.0}
{"epoch": 7, "training_loss": 23158.291870117188, "training_acc": 52.0, "val_loss": 9920.714569091797, "val_acc": 56.0}
{"epoch": 8, "training_loss": 24943.291259765625, "training_acc": 52.0, "val_loss": 16472.01690673828, "val_acc": 44.0}
{"epoch": 9, "training_loss": 60743.5869140625, "training_acc": 48.0, "val_loss": 2703.45458984375, "val_acc": 56.0}
{"epoch": 10, "training_loss": 17501.498657226562, "training_acc": 50.0, "val_loss": 187.8904938697815, "val_acc": 44.0}
{"epoch": 11, "training_loss": 13273.778839111328, "training_acc": 54.0, "val_loss": 1490.7855987548828, "val_acc": 44.0}
{"epoch": 12, "training_loss": 5268.825790405273, "training_acc": 44.0, "val_loss": 2646.211814880371, "val_acc": 44.0}
{"epoch": 13, "training_loss": 10057.81591796875, "training_acc": 52.0, "val_loss": 4351.970672607422, "val_acc": 56.0}
{"epoch": 14, "training_loss": 19923.370727539062, "training_acc": 48.0, "val_loss": 11213.614654541016, "val_acc": 44.0}
{"epoch": 15, "training_loss": 23184.59375, "training_acc": 54.0, "val_loss": 10334.281158447266, "val_acc": 56.0}
{"epoch": 16, "training_loss": 36160.42321777344, "training_acc": 54.0, "val_loss": 7139.940643310547, "val_acc": 44.0}
{"epoch": 17, "training_loss": 21994.588012695312, "training_acc": 46.0, "val_loss": 6195.687484741211, "val_acc": 56.0}
{"epoch": 18, "training_loss": 17216.198486328125, "training_acc": 50.0, "val_loss": 2983.213233947754, "val_acc": 44.0}
{"epoch": 19, "training_loss": 10078.192977905273, "training_acc": 50.0, "val_loss": 1894.3571090698242, "val_acc": 56.0}
{"epoch": 20, "training_loss": 11189.266723632812, "training_acc": 48.0, "val_loss": 1555.5753707885742, "val_acc": 44.0}
{"epoch": 21, "training_loss": 20354.017333984375, "training_acc": 52.0, "val_loss": 2069.192123413086, "val_acc": 56.0}
{"epoch": 22, "training_loss": 20909.79132080078, "training_acc": 54.0, "val_loss": 4354.206466674805, "val_acc": 44.0}
{"epoch": 23, "training_loss": 20382.408203125, "training_acc": 48.0, "val_loss": 2200.2906799316406, "val_acc": 56.0}
{"epoch": 24, "training_loss": 9056.642097473145, "training_acc": 54.0, "val_loss": 2544.490623474121, "val_acc": 44.0}
{"epoch": 25, "training_loss": 4453.895462036133, "training_acc": 53.0, "val_loss": 1076.504898071289, "val_acc": 44.0}
{"epoch": 26, "training_loss": 21417.018920898438, "training_acc": 44.0, "val_loss": 2588.844108581543, "val_acc": 44.0}
{"epoch": 27, "training_loss": 10653.677978515625, "training_acc": 48.0, "val_loss": 6170.082855224609, "val_acc": 56.0}
{"epoch": 28, "training_loss": 18492.420288085938, "training_acc": 46.0, "val_loss": 1346.7238426208496, "val_acc": 56.0}
{"epoch": 29, "training_loss": 7058.8990478515625, "training_acc": 48.0, "val_loss": 6798.535919189453, "val_acc": 56.0}
