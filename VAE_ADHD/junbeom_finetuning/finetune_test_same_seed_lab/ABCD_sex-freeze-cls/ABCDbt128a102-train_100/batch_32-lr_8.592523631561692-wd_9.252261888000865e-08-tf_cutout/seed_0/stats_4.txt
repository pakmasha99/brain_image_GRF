"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 95398.06407928467, "training_acc": 40.0, "val_loss": 24282.374572753906, "val_acc": 52.0}
{"epoch": 1, "training_loss": 65611.23559570312, "training_acc": 51.0, "val_loss": 14900.4638671875, "val_acc": 48.0}
{"epoch": 2, "training_loss": 38462.768798828125, "training_acc": 47.0, "val_loss": 17265.792846679688, "val_acc": 52.0}
{"epoch": 3, "training_loss": 62842.25341796875, "training_acc": 53.0, "val_loss": 6706.658935546875, "val_acc": 48.0}
{"epoch": 4, "training_loss": 47720.558349609375, "training_acc": 47.0, "val_loss": 4363.045883178711, "val_acc": 48.0}
{"epoch": 5, "training_loss": 22469.417846679688, "training_acc": 59.0, "val_loss": 10503.778839111328, "val_acc": 52.0}
{"epoch": 6, "training_loss": 20690.970703125, "training_acc": 57.0, "val_loss": 1587.7548217773438, "val_acc": 48.0}
{"epoch": 7, "training_loss": 24163.42822265625, "training_acc": 45.0, "val_loss": 402.5219440460205, "val_acc": 52.0}
{"epoch": 8, "training_loss": 40952.81970214844, "training_acc": 49.0, "val_loss": 11307.112121582031, "val_acc": 48.0}
{"epoch": 9, "training_loss": 34116.032958984375, "training_acc": 49.0, "val_loss": 22915.51055908203, "val_acc": 52.0}
{"epoch": 10, "training_loss": 86817.03332519531, "training_acc": 53.0, "val_loss": 5235.6048583984375, "val_acc": 52.0}
{"epoch": 11, "training_loss": 30337.778076171875, "training_acc": 51.0, "val_loss": 10047.621154785156, "val_acc": 48.0}
{"epoch": 12, "training_loss": 26351.320434570312, "training_acc": 51.0, "val_loss": 15595.533752441406, "val_acc": 52.0}
{"epoch": 13, "training_loss": 50722.917625427246, "training_acc": 53.0, "val_loss": 9472.300720214844, "val_acc": 48.0}
{"epoch": 14, "training_loss": 47051.8779296875, "training_acc": 47.0, "val_loss": 2096.8212127685547, "val_acc": 48.0}
{"epoch": 15, "training_loss": 24524.291564941406, "training_acc": 53.0, "val_loss": 7017.719268798828, "val_acc": 52.0}
{"epoch": 16, "training_loss": 24662.559936523438, "training_acc": 51.0, "val_loss": 11786.01303100586, "val_acc": 48.0}
{"epoch": 17, "training_loss": 29990.59423828125, "training_acc": 49.0, "val_loss": 9083.462524414062, "val_acc": 52.0}
{"epoch": 18, "training_loss": 24146.448486328125, "training_acc": 49.0, "val_loss": 6453.855133056641, "val_acc": 48.0}
{"epoch": 19, "training_loss": 19677.566650390625, "training_acc": 51.0, "val_loss": 15572.894287109375, "val_acc": 52.0}
{"epoch": 20, "training_loss": 52734.493103027344, "training_acc": 53.0, "val_loss": 6926.895904541016, "val_acc": 48.0}
{"epoch": 21, "training_loss": 36665.548095703125, "training_acc": 47.0, "val_loss": 5851.335906982422, "val_acc": 52.0}
{"epoch": 22, "training_loss": 36183.280822753906, "training_acc": 53.0, "val_loss": 4187.466812133789, "val_acc": 52.0}
{"epoch": 23, "training_loss": 18561.404663085938, "training_acc": 53.0, "val_loss": 765.431022644043, "val_acc": 52.0}
{"epoch": 24, "training_loss": 4191.547382354736, "training_acc": 53.0, "val_loss": 301.2587785720825, "val_acc": 52.0}
{"epoch": 25, "training_loss": 10312.543151855469, "training_acc": 50.0, "val_loss": 2785.649299621582, "val_acc": 52.0}
{"epoch": 26, "training_loss": 10479.620727539062, "training_acc": 57.0, "val_loss": 4726.906967163086, "val_acc": 48.0}
{"epoch": 27, "training_loss": 22758.00439453125, "training_acc": 39.0, "val_loss": 3899.075698852539, "val_acc": 48.0}
{"epoch": 28, "training_loss": 19129.000457763672, "training_acc": 47.0, "val_loss": 3472.886276245117, "val_acc": 52.0}
{"epoch": 29, "training_loss": 9201.958984375, "training_acc": 59.0, "val_loss": 3647.00927734375, "val_acc": 48.0}
{"epoch": 30, "training_loss": 10743.771781921387, "training_acc": 47.0, "val_loss": 2247.158622741699, "val_acc": 52.0}
{"epoch": 31, "training_loss": 10310.02099609375, "training_acc": 49.0, "val_loss": 1414.391803741455, "val_acc": 48.0}
{"epoch": 32, "training_loss": 23192.369873046875, "training_acc": 47.0, "val_loss": 2482.105255126953, "val_acc": 48.0}
{"epoch": 33, "training_loss": 21923.82489013672, "training_acc": 47.0, "val_loss": 9024.591827392578, "val_acc": 52.0}
{"epoch": 34, "training_loss": 54755.2392578125, "training_acc": 53.0, "val_loss": 9103.857421875, "val_acc": 52.0}
{"epoch": 35, "training_loss": 20660.492614746094, "training_acc": 59.0, "val_loss": 4831.065368652344, "val_acc": 48.0}
{"epoch": 36, "training_loss": 13738.780029296875, "training_acc": 57.0, "val_loss": 1800.8625030517578, "val_acc": 52.0}
{"epoch": 37, "training_loss": 17062.073974609375, "training_acc": 49.0, "val_loss": 5377.675628662109, "val_acc": 52.0}
{"epoch": 38, "training_loss": 29260.27001953125, "training_acc": 53.0, "val_loss": 3626.5377044677734, "val_acc": 48.0}
{"epoch": 39, "training_loss": 23315.761596679688, "training_acc": 47.0, "val_loss": 6405.686950683594, "val_acc": 52.0}
{"epoch": 40, "training_loss": 38017.56475830078, "training_acc": 53.0, "val_loss": 371.9754934310913, "val_acc": 52.0}
{"epoch": 41, "training_loss": 22374.521545410156, "training_acc": 57.0, "val_loss": 1844.0279006958008, "val_acc": 48.0}
{"epoch": 42, "training_loss": 32826.61669921875, "training_acc": 51.0, "val_loss": 9541.544342041016, "val_acc": 52.0}
{"epoch": 43, "training_loss": 26194.89404296875, "training_acc": 53.0, "val_loss": 14084.907531738281, "val_acc": 48.0}
