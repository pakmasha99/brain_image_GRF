"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 80079.18981552124, "training_acc": 51.0, "val_loss": 21661.973571777344, "val_acc": 48.0}
{"epoch": 1, "training_loss": 56922.11413574219, "training_acc": 43.0, "val_loss": 8346.393585205078, "val_acc": 52.0}
{"epoch": 2, "training_loss": 28429.83349609375, "training_acc": 53.0, "val_loss": 17592.372131347656, "val_acc": 48.0}
{"epoch": 3, "training_loss": 46814.588623046875, "training_acc": 45.0, "val_loss": 16102.647399902344, "val_acc": 52.0}
{"epoch": 4, "training_loss": 54944.3076171875, "training_acc": 53.0, "val_loss": 8398.030853271484, "val_acc": 48.0}
{"epoch": 5, "training_loss": 56785.726806640625, "training_acc": 47.0, "val_loss": 1528.1657218933105, "val_acc": 48.0}
{"epoch": 6, "training_loss": 49614.9833984375, "training_acc": 45.0, "val_loss": 20798.150634765625, "val_acc": 52.0}
{"epoch": 7, "training_loss": 47976.841796875, "training_acc": 57.0, "val_loss": 17564.694213867188, "val_acc": 48.0}
{"epoch": 8, "training_loss": 94454.376953125, "training_acc": 47.0, "val_loss": 10790.978240966797, "val_acc": 48.0}
{"epoch": 9, "training_loss": 36640.84716796875, "training_acc": 51.0, "val_loss": 16479.8828125, "val_acc": 52.0}
{"epoch": 10, "training_loss": 44015.28454589844, "training_acc": 53.0, "val_loss": 10136.376953125, "val_acc": 48.0}
{"epoch": 11, "training_loss": 34664.11560058594, "training_acc": 45.0, "val_loss": 4224.200057983398, "val_acc": 52.0}
{"epoch": 12, "training_loss": 15994.190551757812, "training_acc": 49.0, "val_loss": 2286.894989013672, "val_acc": 48.0}
{"epoch": 13, "training_loss": 21495.28857421875, "training_acc": 53.0, "val_loss": 3358.5906982421875, "val_acc": 52.0}
{"epoch": 14, "training_loss": 24952.308837890625, "training_acc": 49.0, "val_loss": 4193.241119384766, "val_acc": 48.0}
{"epoch": 15, "training_loss": 17314.613739013672, "training_acc": 55.0, "val_loss": 3860.818099975586, "val_acc": 52.0}
{"epoch": 16, "training_loss": 16185.618961334229, "training_acc": 43.0, "val_loss": 4412.145233154297, "val_acc": 52.0}
{"epoch": 17, "training_loss": 16877.63104248047, "training_acc": 53.0, "val_loss": 4937.073516845703, "val_acc": 48.0}
{"epoch": 18, "training_loss": 17339.754150390625, "training_acc": 45.0, "val_loss": 6563.718414306641, "val_acc": 52.0}
{"epoch": 19, "training_loss": 17865.414672851562, "training_acc": 51.0, "val_loss": 4781.138229370117, "val_acc": 48.0}
{"epoch": 20, "training_loss": 9863.227333068848, "training_acc": 54.0, "val_loss": 4031.8164825439453, "val_acc": 48.0}
{"epoch": 21, "training_loss": 10272.158828735352, "training_acc": 53.0, "val_loss": 591.1895751953125, "val_acc": 48.0}
{"epoch": 22, "training_loss": 5564.3719482421875, "training_acc": 49.0, "val_loss": 4535.938262939453, "val_acc": 48.0}
{"epoch": 23, "training_loss": 11559.213256835938, "training_acc": 59.0, "val_loss": 10885.167694091797, "val_acc": 52.0}
{"epoch": 24, "training_loss": 31603.920532226562, "training_acc": 53.0, "val_loss": 9931.620788574219, "val_acc": 48.0}
{"epoch": 25, "training_loss": 28778.666015625, "training_acc": 47.0, "val_loss": 10748.604583740234, "val_acc": 52.0}
{"epoch": 26, "training_loss": 35076.44207763672, "training_acc": 53.0, "val_loss": 11965.880584716797, "val_acc": 48.0}
{"epoch": 27, "training_loss": 72826.53759765625, "training_acc": 47.0, "val_loss": 11876.367950439453, "val_acc": 48.0}
{"epoch": 28, "training_loss": 35328.55224609375, "training_acc": 47.0, "val_loss": 25365.51513671875, "val_acc": 52.0}
{"epoch": 29, "training_loss": 93784.0234375, "training_acc": 53.0, "val_loss": 6264.923858642578, "val_acc": 52.0}
{"epoch": 30, "training_loss": 39013.466796875, "training_acc": 49.0, "val_loss": 14105.953979492188, "val_acc": 48.0}
{"epoch": 31, "training_loss": 32538.9033203125, "training_acc": 51.0, "val_loss": 5584.812545776367, "val_acc": 52.0}
{"epoch": 32, "training_loss": 14142.454322814941, "training_acc": 45.0, "val_loss": 7976.396179199219, "val_acc": 52.0}
{"epoch": 33, "training_loss": 41206.34265136719, "training_acc": 53.0, "val_loss": 788.0365371704102, "val_acc": 52.0}
{"epoch": 34, "training_loss": 40494.417724609375, "training_acc": 47.0, "val_loss": 11762.308502197266, "val_acc": 48.0}
{"epoch": 35, "training_loss": 32427.765991210938, "training_acc": 53.0, "val_loss": 19972.20458984375, "val_acc": 52.0}
{"epoch": 36, "training_loss": 66109.65026855469, "training_acc": 53.0, "val_loss": 4329.761505126953, "val_acc": 48.0}
{"epoch": 37, "training_loss": 34166.40393066406, "training_acc": 47.0, "val_loss": 3157.2240829467773, "val_acc": 52.0}
{"epoch": 38, "training_loss": 22388.169372558594, "training_acc": 53.0, "val_loss": 6752.098846435547, "val_acc": 48.0}
{"epoch": 39, "training_loss": 40900.51593017578, "training_acc": 47.0, "val_loss": 410.46509742736816, "val_acc": 52.0}
{"epoch": 40, "training_loss": 9894.573181152344, "training_acc": 53.0, "val_loss": 1980.7701110839844, "val_acc": 52.0}
{"epoch": 41, "training_loss": 6726.575866699219, "training_acc": 53.0, "val_loss": 2661.3021850585938, "val_acc": 52.0}
{"epoch": 42, "training_loss": 7283.698394775391, "training_acc": 61.0, "val_loss": 4508.612060546875, "val_acc": 48.0}
{"epoch": 43, "training_loss": 12757.495624542236, "training_acc": 57.0, "val_loss": 689.5543575286865, "val_acc": 52.0}
{"epoch": 44, "training_loss": 26405.421997070312, "training_acc": 45.0, "val_loss": 396.45495414733887, "val_acc": 52.0}
{"epoch": 45, "training_loss": 25461.913024902344, "training_acc": 55.0, "val_loss": 8776.258850097656, "val_acc": 52.0}
{"epoch": 46, "training_loss": 23076.041015625, "training_acc": 51.0, "val_loss": 12054.179382324219, "val_acc": 48.0}
{"epoch": 47, "training_loss": 28199.938598632812, "training_acc": 57.0, "val_loss": 13379.037475585938, "val_acc": 52.0}
{"epoch": 48, "training_loss": 49495.326416015625, "training_acc": 53.0, "val_loss": 2644.1701889038086, "val_acc": 48.0}
{"epoch": 49, "training_loss": 16443.012969970703, "training_acc": 45.0, "val_loss": 1704.8599243164062, "val_acc": 52.0}
{"epoch": 50, "training_loss": 8210.418914794922, "training_acc": 49.0, "val_loss": 2975.3326416015625, "val_acc": 52.0}
{"epoch": 51, "training_loss": 4712.166961669922, "training_acc": 63.0, "val_loss": 3993.75, "val_acc": 52.0}
{"epoch": 52, "training_loss": 12000.363204956055, "training_acc": 49.0, "val_loss": 963.3792877197266, "val_acc": 48.0}
{"epoch": 53, "training_loss": 16676.461669921875, "training_acc": 51.0, "val_loss": 1532.6322555541992, "val_acc": 52.0}
{"epoch": 54, "training_loss": 14405.48388671875, "training_acc": 51.0, "val_loss": 7798.6785888671875, "val_acc": 52.0}
{"epoch": 55, "training_loss": 44195.884765625, "training_acc": 53.0, "val_loss": 5564.453125, "val_acc": 52.0}
{"epoch": 56, "training_loss": 17887.604370117188, "training_acc": 55.0, "val_loss": 518.5819149017334, "val_acc": 56.0}
{"epoch": 57, "training_loss": 30201.24853515625, "training_acc": 52.0, "val_loss": 11084.286499023438, "val_acc": 52.0}
{"epoch": 58, "training_loss": 25945.913696289062, "training_acc": 51.0, "val_loss": 7401.4892578125, "val_acc": 48.0}
{"epoch": 59, "training_loss": 18249.758728027344, "training_acc": 55.0, "val_loss": 10908.661651611328, "val_acc": 52.0}
{"epoch": 60, "training_loss": 28369.723876953125, "training_acc": 51.0, "val_loss": 9352.983856201172, "val_acc": 48.0}
{"epoch": 61, "training_loss": 25015.237823486328, "training_acc": 41.0, "val_loss": 503.09486389160156, "val_acc": 44.0}
{"epoch": 62, "training_loss": 2597.6661987304688, "training_acc": 62.0, "val_loss": 2914.888572692871, "val_acc": 52.0}
{"epoch": 63, "training_loss": 11129.440200805664, "training_acc": 45.0, "val_loss": 3755.6175231933594, "val_acc": 52.0}
