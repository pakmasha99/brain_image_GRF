"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.25441670417786, "training_acc": 53.0, "val_loss": 17.258329689502716, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.33006167411804, "training_acc": 53.0, "val_loss": 17.30230748653412, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.34090042114258, "training_acc": 53.0, "val_loss": 17.320938408374786, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.37056541442871, "training_acc": 53.0, "val_loss": 17.32289493083954, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.38586711883545, "training_acc": 53.0, "val_loss": 17.325183749198914, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.37701368331909, "training_acc": 53.0, "val_loss": 17.339730262756348, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.41517758369446, "training_acc": 53.0, "val_loss": 17.344149947166443, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.39550161361694, "training_acc": 53.0, "val_loss": 17.320115864276886, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.38436555862427, "training_acc": 53.0, "val_loss": 17.320500314235687, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.26860356330872, "training_acc": 53.0, "val_loss": 17.29028671979904, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.21817636489868, "training_acc": 53.0, "val_loss": 17.267367243766785, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.17560148239136, "training_acc": 53.0, "val_loss": 17.25059151649475, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.3954668045044, "training_acc": 53.0, "val_loss": 17.250417172908783, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.28108882904053, "training_acc": 53.0, "val_loss": 17.251163721084595, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.24600648880005, "training_acc": 53.0, "val_loss": 17.251963913440704, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.26382207870483, "training_acc": 53.0, "val_loss": 17.247942090034485, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.20281791687012, "training_acc": 53.0, "val_loss": 17.247413098812103, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.17119026184082, "training_acc": 53.0, "val_loss": 17.24834144115448, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.20100164413452, "training_acc": 53.0, "val_loss": 17.253489792346954, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.22453427314758, "training_acc": 53.0, "val_loss": 17.258629202842712, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.15876722335815, "training_acc": 53.0, "val_loss": 17.265349626541138, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.2059075832367, "training_acc": 53.0, "val_loss": 17.291954159736633, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.40946745872498, "training_acc": 53.0, "val_loss": 17.305374145507812, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.35196304321289, "training_acc": 53.0, "val_loss": 17.274780571460724, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.19578790664673, "training_acc": 53.0, "val_loss": 17.25260317325592, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.20922327041626, "training_acc": 53.0, "val_loss": 17.247430980205536, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.1852765083313, "training_acc": 53.0, "val_loss": 17.248769104480743, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.21496438980103, "training_acc": 53.0, "val_loss": 17.24746525287628, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.16628909111023, "training_acc": 53.0, "val_loss": 17.251533269882202, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.22648191452026, "training_acc": 53.0, "val_loss": 17.265847325325012, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.23134136199951, "training_acc": 53.0, "val_loss": 17.269176244735718, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.24079704284668, "training_acc": 53.0, "val_loss": 17.274127900600433, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.28303956985474, "training_acc": 53.0, "val_loss": 17.258460819721222, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.15458703041077, "training_acc": 53.0, "val_loss": 17.25001037120819, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.13445115089417, "training_acc": 53.0, "val_loss": 17.24981814622879, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.37000703811646, "training_acc": 51.0, "val_loss": 17.276008427143097, "val_acc": 52.0}
