"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.15929460525513, "training_acc": 53.0, "val_loss": 17.299556732177734, "val_acc": 52.0}
{"epoch": 1, "training_loss": 68.95091652870178, "training_acc": 53.0, "val_loss": 17.323923110961914, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.02270483970642, "training_acc": 53.0, "val_loss": 17.33759194612503, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.08738660812378, "training_acc": 53.0, "val_loss": 17.32235699892044, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.15107154846191, "training_acc": 53.0, "val_loss": 17.29811429977417, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.05779552459717, "training_acc": 53.0, "val_loss": 17.296314239501953, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.06385397911072, "training_acc": 53.0, "val_loss": 17.295853793621063, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.0706238746643, "training_acc": 53.0, "val_loss": 17.298713326454163, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.03679609298706, "training_acc": 53.0, "val_loss": 17.30455756187439, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.03735399246216, "training_acc": 53.0, "val_loss": 17.316165566444397, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.07008218765259, "training_acc": 53.0, "val_loss": 17.322352528572083, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.04919481277466, "training_acc": 53.0, "val_loss": 17.304368317127228, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.09366989135742, "training_acc": 53.0, "val_loss": 17.29613095521927, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.09561395645142, "training_acc": 53.0, "val_loss": 17.300190031528473, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.06527757644653, "training_acc": 53.0, "val_loss": 17.30458438396454, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.02631115913391, "training_acc": 53.0, "val_loss": 17.30598956346512, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.01705574989319, "training_acc": 53.0, "val_loss": 17.302852869033813, "val_acc": 52.0}
{"epoch": 17, "training_loss": 68.99543237686157, "training_acc": 53.0, "val_loss": 17.303678393363953, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.06151700019836, "training_acc": 53.0, "val_loss": 17.30874925851822, "val_acc": 52.0}
{"epoch": 19, "training_loss": 68.99864196777344, "training_acc": 53.0, "val_loss": 17.305223643779755, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.00534391403198, "training_acc": 53.0, "val_loss": 17.311859130859375, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.05995988845825, "training_acc": 53.0, "val_loss": 17.320387065410614, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.032155752182, "training_acc": 53.0, "val_loss": 17.31071174144745, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.02366924285889, "training_acc": 53.0, "val_loss": 17.309078574180603, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.00774669647217, "training_acc": 53.0, "val_loss": 17.322325706481934, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.0485634803772, "training_acc": 53.0, "val_loss": 17.32301414012909, "val_acc": 52.0}
