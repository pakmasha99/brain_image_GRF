"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.31243395805359, "training_acc": 52.0, "val_loss": 17.220023274421692, "val_acc": 56.0}
{"epoch": 1, "training_loss": 69.32593107223511, "training_acc": 52.0, "val_loss": 17.186976969242096, "val_acc": 56.0}
{"epoch": 2, "training_loss": 69.33794355392456, "training_acc": 52.0, "val_loss": 17.194488644599915, "val_acc": 56.0}
{"epoch": 3, "training_loss": 69.2662308216095, "training_acc": 52.0, "val_loss": 17.169612646102905, "val_acc": 56.0}
{"epoch": 4, "training_loss": 69.31134176254272, "training_acc": 52.0, "val_loss": 17.190364003181458, "val_acc": 56.0}
{"epoch": 5, "training_loss": 69.24424886703491, "training_acc": 52.0, "val_loss": 17.180465161800385, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.23975849151611, "training_acc": 52.0, "val_loss": 17.185452580451965, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.28071665763855, "training_acc": 52.0, "val_loss": 17.19885915517807, "val_acc": 56.0}
{"epoch": 8, "training_loss": 69.2404670715332, "training_acc": 52.0, "val_loss": 17.19956398010254, "val_acc": 56.0}
{"epoch": 9, "training_loss": 69.30801916122437, "training_acc": 52.0, "val_loss": 17.23557561635971, "val_acc": 56.0}
{"epoch": 10, "training_loss": 69.26538610458374, "training_acc": 50.0, "val_loss": 17.310091853141785, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.3183388710022, "training_acc": 53.0, "val_loss": 17.296908795833588, "val_acc": 56.0}
{"epoch": 12, "training_loss": 69.38225221633911, "training_acc": 48.0, "val_loss": 17.220203578472137, "val_acc": 56.0}
{"epoch": 13, "training_loss": 69.17067575454712, "training_acc": 52.0, "val_loss": 17.184525728225708, "val_acc": 56.0}
{"epoch": 14, "training_loss": 69.2647693157196, "training_acc": 52.0, "val_loss": 17.15444177389145, "val_acc": 56.0}
{"epoch": 15, "training_loss": 69.28046798706055, "training_acc": 52.0, "val_loss": 17.141270637512207, "val_acc": 56.0}
{"epoch": 16, "training_loss": 69.52357912063599, "training_acc": 52.0, "val_loss": 17.131909728050232, "val_acc": 56.0}
{"epoch": 17, "training_loss": 69.46874856948853, "training_acc": 52.0, "val_loss": 17.135436832904816, "val_acc": 56.0}
{"epoch": 18, "training_loss": 69.31906986236572, "training_acc": 52.0, "val_loss": 17.159374058246613, "val_acc": 56.0}
{"epoch": 19, "training_loss": 69.24539566040039, "training_acc": 52.0, "val_loss": 17.209826409816742, "val_acc": 56.0}
{"epoch": 20, "training_loss": 69.2558798789978, "training_acc": 52.0, "val_loss": 17.2372505068779, "val_acc": 56.0}
{"epoch": 21, "training_loss": 69.27256536483765, "training_acc": 52.0, "val_loss": 17.256996035575867, "val_acc": 56.0}
{"epoch": 22, "training_loss": 69.29443407058716, "training_acc": 52.0, "val_loss": 17.26645976305008, "val_acc": 56.0}
{"epoch": 23, "training_loss": 69.25554084777832, "training_acc": 51.0, "val_loss": 17.280158400535583, "val_acc": 56.0}
{"epoch": 24, "training_loss": 69.31903147697449, "training_acc": 54.0, "val_loss": 17.236793041229248, "val_acc": 56.0}
{"epoch": 25, "training_loss": 69.22153615951538, "training_acc": 52.0, "val_loss": 17.236922681331635, "val_acc": 56.0}
{"epoch": 26, "training_loss": 69.26475954055786, "training_acc": 52.0, "val_loss": 17.227348685264587, "val_acc": 56.0}
{"epoch": 27, "training_loss": 69.20739197731018, "training_acc": 52.0, "val_loss": 17.228667438030243, "val_acc": 56.0}
{"epoch": 28, "training_loss": 69.23660802841187, "training_acc": 52.0, "val_loss": 17.22474992275238, "val_acc": 56.0}
{"epoch": 29, "training_loss": 69.20962476730347, "training_acc": 52.0, "val_loss": 17.188239097595215, "val_acc": 56.0}
{"epoch": 30, "training_loss": 69.38812804222107, "training_acc": 52.0, "val_loss": 17.14329421520233, "val_acc": 56.0}
{"epoch": 31, "training_loss": 69.32834196090698, "training_acc": 52.0, "val_loss": 17.130832374095917, "val_acc": 56.0}
{"epoch": 32, "training_loss": 69.64475774765015, "training_acc": 52.0, "val_loss": 17.14284121990204, "val_acc": 56.0}
{"epoch": 33, "training_loss": 69.85586595535278, "training_acc": 52.0, "val_loss": 17.15465486049652, "val_acc": 56.0}
{"epoch": 34, "training_loss": 69.92151069641113, "training_acc": 52.0, "val_loss": 17.139622569084167, "val_acc": 56.0}
{"epoch": 35, "training_loss": 69.73394274711609, "training_acc": 52.0, "val_loss": 17.131321132183075, "val_acc": 56.0}
{"epoch": 36, "training_loss": 69.48529529571533, "training_acc": 52.0, "val_loss": 17.13913083076477, "val_acc": 56.0}
{"epoch": 37, "training_loss": 69.36612939834595, "training_acc": 52.0, "val_loss": 17.148536443710327, "val_acc": 56.0}
{"epoch": 38, "training_loss": 69.34544515609741, "training_acc": 52.0, "val_loss": 17.16955304145813, "val_acc": 56.0}
{"epoch": 39, "training_loss": 69.24056816101074, "training_acc": 52.0, "val_loss": 17.21169501543045, "val_acc": 56.0}
{"epoch": 40, "training_loss": 69.32791519165039, "training_acc": 52.0, "val_loss": 17.274269461631775, "val_acc": 56.0}
{"epoch": 41, "training_loss": 69.24052143096924, "training_acc": 53.0, "val_loss": 17.29585975408554, "val_acc": 56.0}
{"epoch": 42, "training_loss": 69.27172827720642, "training_acc": 53.0, "val_loss": 17.322704195976257, "val_acc": 56.0}
{"epoch": 43, "training_loss": 69.29437565803528, "training_acc": 52.0, "val_loss": 17.338721454143524, "val_acc": 56.0}
{"epoch": 44, "training_loss": 69.365318775177, "training_acc": 51.0, "val_loss": 17.379066348075867, "val_acc": 56.0}
{"epoch": 45, "training_loss": 69.41601967811584, "training_acc": 48.0, "val_loss": 17.375212907791138, "val_acc": 56.0}
{"epoch": 46, "training_loss": 69.38915753364563, "training_acc": 48.0, "val_loss": 17.34188199043274, "val_acc": 56.0}
{"epoch": 47, "training_loss": 69.28301572799683, "training_acc": 51.0, "val_loss": 17.279498279094696, "val_acc": 56.0}
{"epoch": 48, "training_loss": 69.27860641479492, "training_acc": 53.0, "val_loss": 17.242084443569183, "val_acc": 56.0}
{"epoch": 49, "training_loss": 69.21041774749756, "training_acc": 52.0, "val_loss": 17.2484889626503, "val_acc": 56.0}
{"epoch": 50, "training_loss": 69.2487564086914, "training_acc": 52.0, "val_loss": 17.29041039943695, "val_acc": 56.0}
