"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 70.52368879318237, "training_acc": 47.0, "val_loss": 17.446933686733246, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.72858762741089, "training_acc": 47.0, "val_loss": 17.33032763004303, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.24126124382019, "training_acc": 55.0, "val_loss": 17.30835735797882, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.33205890655518, "training_acc": 53.0, "val_loss": 17.343769967556, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.19920587539673, "training_acc": 53.0, "val_loss": 17.366766929626465, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.21571969985962, "training_acc": 53.0, "val_loss": 17.378421127796173, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.24556159973145, "training_acc": 53.0, "val_loss": 17.364250123500824, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.27042961120605, "training_acc": 53.0, "val_loss": 17.34921932220459, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.17851257324219, "training_acc": 53.0, "val_loss": 17.341306805610657, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.20684695243835, "training_acc": 53.0, "val_loss": 17.324572801589966, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.13600778579712, "training_acc": 53.0, "val_loss": 17.31989234685898, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.21209597587585, "training_acc": 53.0, "val_loss": 17.309927940368652, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.08312106132507, "training_acc": 53.0, "val_loss": 17.307396233081818, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.00597310066223, "training_acc": 53.0, "val_loss": 17.308208346366882, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.18836832046509, "training_acc": 53.0, "val_loss": 17.313048243522644, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.18142580986023, "training_acc": 53.0, "val_loss": 17.31354296207428, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.205650806427, "training_acc": 53.0, "val_loss": 17.3116534948349, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.14968347549438, "training_acc": 53.0, "val_loss": 17.307746410369873, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.07516241073608, "training_acc": 53.0, "val_loss": 17.30785071849823, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.04232978820801, "training_acc": 53.0, "val_loss": 17.317049205303192, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.05947494506836, "training_acc": 53.0, "val_loss": 17.34038144350052, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.17546844482422, "training_acc": 53.0, "val_loss": 17.351052165031433, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.13046455383301, "training_acc": 53.0, "val_loss": 17.33492761850357, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.12894678115845, "training_acc": 53.0, "val_loss": 17.32233613729477, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.08388996124268, "training_acc": 53.0, "val_loss": 17.315199971199036, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.08647465705872, "training_acc": 53.0, "val_loss": 17.31390953063965, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.08745503425598, "training_acc": 53.0, "val_loss": 17.309415340423584, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.13197994232178, "training_acc": 53.0, "val_loss": 17.30826199054718, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.07638168334961, "training_acc": 53.0, "val_loss": 17.310504615306854, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.11441993713379, "training_acc": 53.0, "val_loss": 17.312142252922058, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.05619621276855, "training_acc": 53.0, "val_loss": 17.308521270751953, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.10687732696533, "training_acc": 53.0, "val_loss": 17.30837970972061, "val_acc": 52.0}
