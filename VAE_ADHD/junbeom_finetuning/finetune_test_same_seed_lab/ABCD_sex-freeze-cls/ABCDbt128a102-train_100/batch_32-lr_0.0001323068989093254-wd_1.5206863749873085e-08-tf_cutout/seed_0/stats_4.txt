"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.55971479415894, "training_acc": 42.0, "val_loss": 17.265628278255463, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.3541305065155, "training_acc": 47.0, "val_loss": 17.26759672164917, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.3008041381836, "training_acc": 51.0, "val_loss": 17.25601702928543, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.23345851898193, "training_acc": 53.0, "val_loss": 17.255738377571106, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.13227486610413, "training_acc": 53.0, "val_loss": 17.269444465637207, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.24180126190186, "training_acc": 53.0, "val_loss": 17.279402911663055, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.24318861961365, "training_acc": 53.0, "val_loss": 17.2660231590271, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.35313749313354, "training_acc": 53.0, "val_loss": 17.25732833147049, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.24022459983826, "training_acc": 53.0, "val_loss": 17.26573556661606, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.24272966384888, "training_acc": 53.0, "val_loss": 17.276103794574738, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.2599287033081, "training_acc": 53.0, "val_loss": 17.267391085624695, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.23135662078857, "training_acc": 53.0, "val_loss": 17.275527119636536, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.24661684036255, "training_acc": 53.0, "val_loss": 17.295612394809723, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.30012512207031, "training_acc": 53.0, "val_loss": 17.320789396762848, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.38216400146484, "training_acc": 53.0, "val_loss": 17.324085533618927, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.3453598022461, "training_acc": 53.0, "val_loss": 17.336921393871307, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.4601697921753, "training_acc": 53.0, "val_loss": 17.34086126089096, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.33082723617554, "training_acc": 53.0, "val_loss": 17.29024350643158, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.26736402511597, "training_acc": 53.0, "val_loss": 17.276379466056824, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.17211508750916, "training_acc": 53.0, "val_loss": 17.271068692207336, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.20310521125793, "training_acc": 53.0, "val_loss": 17.2575905919075, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.196617603302, "training_acc": 53.0, "val_loss": 17.25468337535858, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.12470054626465, "training_acc": 53.0, "val_loss": 17.265009880065918, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.2382640838623, "training_acc": 53.0, "val_loss": 17.29295700788498, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.22690033912659, "training_acc": 53.0, "val_loss": 17.33095496892929, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.44422793388367, "training_acc": 53.0, "val_loss": 17.36989915370941, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.48943758010864, "training_acc": 53.0, "val_loss": 17.377382516860962, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.56154036521912, "training_acc": 53.0, "val_loss": 17.372645437717438, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.48194789886475, "training_acc": 53.0, "val_loss": 17.354804277420044, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.37596559524536, "training_acc": 53.0, "val_loss": 17.313824594020844, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.16920518875122, "training_acc": 53.0, "val_loss": 17.267730832099915, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.07912421226501, "training_acc": 53.0, "val_loss": 17.253166437149048, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.18443202972412, "training_acc": 53.0, "val_loss": 17.259547114372253, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.22674942016602, "training_acc": 53.0, "val_loss": 17.266058921813965, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.27368259429932, "training_acc": 57.0, "val_loss": 17.280250787734985, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.51433491706848, "training_acc": 46.0, "val_loss": 17.30963885784149, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.51252055168152, "training_acc": 47.0, "val_loss": 17.290182411670685, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.45173525810242, "training_acc": 45.0, "val_loss": 17.27616637945175, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.27685856819153, "training_acc": 54.0, "val_loss": 17.257873713970184, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.13339471817017, "training_acc": 53.0, "val_loss": 17.254135012626648, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.20957064628601, "training_acc": 53.0, "val_loss": 17.274175584316254, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.19654035568237, "training_acc": 53.0, "val_loss": 17.280247807502747, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.25456309318542, "training_acc": 53.0, "val_loss": 17.28421002626419, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.25541877746582, "training_acc": 53.0, "val_loss": 17.29259490966797, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.2519211769104, "training_acc": 53.0, "val_loss": 17.289744317531586, "val_acc": 52.0}
{"epoch": 45, "training_loss": 69.17863082885742, "training_acc": 53.0, "val_loss": 17.277005314826965, "val_acc": 52.0}
{"epoch": 46, "training_loss": 69.33145880699158, "training_acc": 53.0, "val_loss": 17.267237603664398, "val_acc": 52.0}
{"epoch": 47, "training_loss": 69.22452306747437, "training_acc": 53.0, "val_loss": 17.28881001472473, "val_acc": 52.0}
{"epoch": 48, "training_loss": 69.21278214454651, "training_acc": 53.0, "val_loss": 17.324239015579224, "val_acc": 52.0}
{"epoch": 49, "training_loss": 69.45952343940735, "training_acc": 53.0, "val_loss": 17.389944195747375, "val_acc": 52.0}
{"epoch": 50, "training_loss": 69.52869844436646, "training_acc": 53.0, "val_loss": 17.401260137557983, "val_acc": 52.0}
