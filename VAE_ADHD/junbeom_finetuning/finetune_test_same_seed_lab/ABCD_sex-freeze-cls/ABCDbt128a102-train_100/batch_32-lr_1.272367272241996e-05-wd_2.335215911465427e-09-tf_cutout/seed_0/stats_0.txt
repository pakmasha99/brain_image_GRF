"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.25769281387329, "training_acc": 52.0, "val_loss": 17.235592007637024, "val_acc": 56.0}
{"epoch": 1, "training_loss": 69.28516006469727, "training_acc": 52.0, "val_loss": 17.230457067489624, "val_acc": 56.0}
{"epoch": 2, "training_loss": 69.27428340911865, "training_acc": 52.0, "val_loss": 17.229458689689636, "val_acc": 56.0}
{"epoch": 3, "training_loss": 69.2209165096283, "training_acc": 52.0, "val_loss": 17.224320769309998, "val_acc": 56.0}
{"epoch": 4, "training_loss": 69.28267765045166, "training_acc": 52.0, "val_loss": 17.225012183189392, "val_acc": 56.0}
{"epoch": 5, "training_loss": 69.23018598556519, "training_acc": 52.0, "val_loss": 17.221705615520477, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.23223948478699, "training_acc": 52.0, "val_loss": 17.22014844417572, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.26421976089478, "training_acc": 52.0, "val_loss": 17.2195702791214, "val_acc": 56.0}
{"epoch": 8, "training_loss": 69.2220311164856, "training_acc": 52.0, "val_loss": 17.21777468919754, "val_acc": 56.0}
{"epoch": 9, "training_loss": 69.28410243988037, "training_acc": 52.0, "val_loss": 17.219872772693634, "val_acc": 56.0}
{"epoch": 10, "training_loss": 69.25641584396362, "training_acc": 52.0, "val_loss": 17.22523123025894, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.23195147514343, "training_acc": 52.0, "val_loss": 17.22494810819626, "val_acc": 56.0}
{"epoch": 12, "training_loss": 69.25477647781372, "training_acc": 52.0, "val_loss": 17.219653725624084, "val_acc": 56.0}
{"epoch": 13, "training_loss": 69.22430038452148, "training_acc": 52.0, "val_loss": 17.216554284095764, "val_acc": 56.0}
{"epoch": 14, "training_loss": 69.26332545280457, "training_acc": 52.0, "val_loss": 17.212173342704773, "val_acc": 56.0}
{"epoch": 15, "training_loss": 69.23284006118774, "training_acc": 52.0, "val_loss": 17.208412289619446, "val_acc": 56.0}
{"epoch": 16, "training_loss": 69.2953245639801, "training_acc": 52.0, "val_loss": 17.20263957977295, "val_acc": 56.0}
{"epoch": 17, "training_loss": 69.24139142036438, "training_acc": 52.0, "val_loss": 17.202085256576538, "val_acc": 56.0}
{"epoch": 18, "training_loss": 69.22284173965454, "training_acc": 52.0, "val_loss": 17.204444110393524, "val_acc": 56.0}
{"epoch": 19, "training_loss": 69.2230052947998, "training_acc": 52.0, "val_loss": 17.207784950733185, "val_acc": 56.0}
{"epoch": 20, "training_loss": 69.26205348968506, "training_acc": 52.0, "val_loss": 17.208456993103027, "val_acc": 56.0}
{"epoch": 21, "training_loss": 69.26444554328918, "training_acc": 52.0, "val_loss": 17.20920503139496, "val_acc": 56.0}
{"epoch": 22, "training_loss": 69.29000949859619, "training_acc": 52.0, "val_loss": 17.20990091562271, "val_acc": 56.0}
{"epoch": 23, "training_loss": 69.23966264724731, "training_acc": 52.0, "val_loss": 17.21154749393463, "val_acc": 56.0}
{"epoch": 24, "training_loss": 69.24363279342651, "training_acc": 52.0, "val_loss": 17.209266126155853, "val_acc": 56.0}
{"epoch": 25, "training_loss": 69.23098921775818, "training_acc": 52.0, "val_loss": 17.210322618484497, "val_acc": 56.0}
{"epoch": 26, "training_loss": 69.23664617538452, "training_acc": 52.0, "val_loss": 17.210491001605988, "val_acc": 56.0}
{"epoch": 27, "training_loss": 69.22887516021729, "training_acc": 52.0, "val_loss": 17.211486399173737, "val_acc": 56.0}
{"epoch": 28, "training_loss": 69.24782347679138, "training_acc": 52.0, "val_loss": 17.211955785751343, "val_acc": 56.0}
{"epoch": 29, "training_loss": 69.2378613948822, "training_acc": 52.0, "val_loss": 17.208972573280334, "val_acc": 56.0}
{"epoch": 30, "training_loss": 69.28106784820557, "training_acc": 52.0, "val_loss": 17.20232367515564, "val_acc": 56.0}
{"epoch": 31, "training_loss": 69.28146743774414, "training_acc": 52.0, "val_loss": 17.196404933929443, "val_acc": 56.0}
{"epoch": 32, "training_loss": 69.26874899864197, "training_acc": 52.0, "val_loss": 17.188583314418793, "val_acc": 56.0}
{"epoch": 33, "training_loss": 69.24496603012085, "training_acc": 52.0, "val_loss": 17.183460295200348, "val_acc": 56.0}
{"epoch": 34, "training_loss": 69.25754594802856, "training_acc": 52.0, "val_loss": 17.182159423828125, "val_acc": 56.0}
{"epoch": 35, "training_loss": 69.28458142280579, "training_acc": 52.0, "val_loss": 17.183203995227814, "val_acc": 56.0}
{"epoch": 36, "training_loss": 69.28693318367004, "training_acc": 52.0, "val_loss": 17.18227118253708, "val_acc": 56.0}
{"epoch": 37, "training_loss": 69.24030303955078, "training_acc": 52.0, "val_loss": 17.180868983268738, "val_acc": 56.0}
{"epoch": 38, "training_loss": 69.26300144195557, "training_acc": 52.0, "val_loss": 17.181073129177094, "val_acc": 56.0}
{"epoch": 39, "training_loss": 69.30842018127441, "training_acc": 52.0, "val_loss": 17.183099687099457, "val_acc": 56.0}
{"epoch": 40, "training_loss": 69.27782726287842, "training_acc": 52.0, "val_loss": 17.186477780342102, "val_acc": 56.0}
{"epoch": 41, "training_loss": 69.2295937538147, "training_acc": 52.0, "val_loss": 17.187978327274323, "val_acc": 56.0}
{"epoch": 42, "training_loss": 69.26022386550903, "training_acc": 52.0, "val_loss": 17.190609872341156, "val_acc": 56.0}
{"epoch": 43, "training_loss": 69.2211537361145, "training_acc": 52.0, "val_loss": 17.193393409252167, "val_acc": 56.0}
{"epoch": 44, "training_loss": 69.23253297805786, "training_acc": 52.0, "val_loss": 17.19808578491211, "val_acc": 56.0}
{"epoch": 45, "training_loss": 69.26063108444214, "training_acc": 52.0, "val_loss": 17.2012060880661, "val_acc": 56.0}
{"epoch": 46, "training_loss": 69.24960708618164, "training_acc": 52.0, "val_loss": 17.203162610530853, "val_acc": 56.0}
{"epoch": 47, "training_loss": 69.23366212844849, "training_acc": 52.0, "val_loss": 17.203088104724884, "val_acc": 56.0}
{"epoch": 48, "training_loss": 69.2553014755249, "training_acc": 52.0, "val_loss": 17.203661799430847, "val_acc": 56.0}
{"epoch": 49, "training_loss": 69.23008108139038, "training_acc": 52.0, "val_loss": 17.20680147409439, "val_acc": 56.0}
{"epoch": 50, "training_loss": 69.27134251594543, "training_acc": 52.0, "val_loss": 17.21215844154358, "val_acc": 56.0}
{"epoch": 51, "training_loss": 69.27983283996582, "training_acc": 52.0, "val_loss": 17.218802869319916, "val_acc": 56.0}
{"epoch": 52, "training_loss": 69.2622435092926, "training_acc": 52.0, "val_loss": 17.225749790668488, "val_acc": 56.0}
{"epoch": 53, "training_loss": 69.25702381134033, "training_acc": 52.0, "val_loss": 17.23044216632843, "val_acc": 56.0}
{"epoch": 54, "training_loss": 69.24308013916016, "training_acc": 52.0, "val_loss": 17.235541343688965, "val_acc": 56.0}
{"epoch": 55, "training_loss": 69.24323678016663, "training_acc": 52.0, "val_loss": 17.24432110786438, "val_acc": 56.0}
{"epoch": 56, "training_loss": 69.26863980293274, "training_acc": 52.0, "val_loss": 17.255809903144836, "val_acc": 56.0}
