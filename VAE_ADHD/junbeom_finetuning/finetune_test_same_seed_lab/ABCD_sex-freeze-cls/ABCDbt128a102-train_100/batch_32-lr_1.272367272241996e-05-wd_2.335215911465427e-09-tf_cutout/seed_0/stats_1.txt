"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 70.33471179008484, "training_acc": 47.0, "val_loss": 17.528800666332245, "val_acc": 52.0}
{"epoch": 1, "training_loss": 70.2145676612854, "training_acc": 47.0, "val_loss": 17.510297894477844, "val_acc": 52.0}
{"epoch": 2, "training_loss": 70.14431810379028, "training_acc": 47.0, "val_loss": 17.493976652622223, "val_acc": 52.0}
{"epoch": 3, "training_loss": 70.10421419143677, "training_acc": 47.0, "val_loss": 17.48296320438385, "val_acc": 52.0}
{"epoch": 4, "training_loss": 70.03825807571411, "training_acc": 47.0, "val_loss": 17.470763623714447, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.96308135986328, "training_acc": 47.0, "val_loss": 17.463146150112152, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.98824167251587, "training_acc": 47.0, "val_loss": 17.454715073108673, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.90289115905762, "training_acc": 47.0, "val_loss": 17.44883358478546, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.85774803161621, "training_acc": 47.0, "val_loss": 17.44289994239807, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.82452607154846, "training_acc": 47.0, "val_loss": 17.435336112976074, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.80785799026489, "training_acc": 47.0, "val_loss": 17.427638173103333, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.75875568389893, "training_acc": 47.0, "val_loss": 17.41887480020523, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.74194765090942, "training_acc": 47.0, "val_loss": 17.410220205783844, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.66470170021057, "training_acc": 47.0, "val_loss": 17.400580644607544, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.6087737083435, "training_acc": 47.0, "val_loss": 17.390647530555725, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.64555931091309, "training_acc": 47.0, "val_loss": 17.383281886577606, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.59292197227478, "training_acc": 47.0, "val_loss": 17.37901270389557, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.56102275848389, "training_acc": 47.0, "val_loss": 17.374815046787262, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.50008869171143, "training_acc": 47.0, "val_loss": 17.370595037937164, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.46135902404785, "training_acc": 47.0, "val_loss": 17.365922033786774, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.44101905822754, "training_acc": 47.0, "val_loss": 17.3630028963089, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.45206260681152, "training_acc": 47.0, "val_loss": 17.360040545463562, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.45379495620728, "training_acc": 45.0, "val_loss": 17.355641722679138, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.442706823349, "training_acc": 47.0, "val_loss": 17.352329194545746, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.40470743179321, "training_acc": 47.0, "val_loss": 17.351144552230835, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.38058233261108, "training_acc": 48.0, "val_loss": 17.349238693714142, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.35729384422302, "training_acc": 49.0, "val_loss": 17.346109449863434, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.34646821022034, "training_acc": 54.0, "val_loss": 17.34231561422348, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.30354499816895, "training_acc": 46.0, "val_loss": 17.3411026597023, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.338050365448, "training_acc": 50.0, "val_loss": 17.339250445365906, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.31240391731262, "training_acc": 48.0, "val_loss": 17.337605357170105, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.31219148635864, "training_acc": 52.0, "val_loss": 17.336253821849823, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.26187992095947, "training_acc": 51.0, "val_loss": 17.334668338298798, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.26544904708862, "training_acc": 53.0, "val_loss": 17.334352433681488, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.26927065849304, "training_acc": 53.0, "val_loss": 17.33376681804657, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.23965907096863, "training_acc": 53.0, "val_loss": 17.333179712295532, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.22853660583496, "training_acc": 53.0, "val_loss": 17.332693934440613, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.22547817230225, "training_acc": 53.0, "val_loss": 17.33223646879196, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.27113628387451, "training_acc": 53.0, "val_loss": 17.331463098526, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.234938621521, "training_acc": 53.0, "val_loss": 17.3311710357666, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.24888372421265, "training_acc": 53.0, "val_loss": 17.33054369688034, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.23755311965942, "training_acc": 53.0, "val_loss": 17.330750823020935, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.2369155883789, "training_acc": 53.0, "val_loss": 17.331477999687195, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.24092411994934, "training_acc": 53.0, "val_loss": 17.331719398498535, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.2506492137909, "training_acc": 53.0, "val_loss": 17.331072688102722, "val_acc": 52.0}
{"epoch": 45, "training_loss": 69.19339799880981, "training_acc": 53.0, "val_loss": 17.33122020959854, "val_acc": 52.0}
{"epoch": 46, "training_loss": 69.21304273605347, "training_acc": 53.0, "val_loss": 17.331913113594055, "val_acc": 52.0}
{"epoch": 47, "training_loss": 69.23341345787048, "training_acc": 53.0, "val_loss": 17.332284152507782, "val_acc": 52.0}
{"epoch": 48, "training_loss": 69.28725862503052, "training_acc": 53.0, "val_loss": 17.332610487937927, "val_acc": 52.0}
{"epoch": 49, "training_loss": 69.23562574386597, "training_acc": 53.0, "val_loss": 17.333051562309265, "val_acc": 52.0}
{"epoch": 50, "training_loss": 69.27538299560547, "training_acc": 53.0, "val_loss": 17.33327805995941, "val_acc": 52.0}
{"epoch": 51, "training_loss": 69.27934050559998, "training_acc": 53.0, "val_loss": 17.332549393177032, "val_acc": 52.0}
{"epoch": 52, "training_loss": 69.17836666107178, "training_acc": 53.0, "val_loss": 17.331480979919434, "val_acc": 52.0}
{"epoch": 53, "training_loss": 69.25357055664062, "training_acc": 53.0, "val_loss": 17.330504953861237, "val_acc": 52.0}
{"epoch": 54, "training_loss": 69.22782397270203, "training_acc": 53.0, "val_loss": 17.3298642039299, "val_acc": 52.0}
{"epoch": 55, "training_loss": 69.22246503829956, "training_acc": 53.0, "val_loss": 17.329470813274384, "val_acc": 52.0}
{"epoch": 56, "training_loss": 69.22033500671387, "training_acc": 53.0, "val_loss": 17.329290509223938, "val_acc": 52.0}
{"epoch": 57, "training_loss": 69.23648357391357, "training_acc": 53.0, "val_loss": 17.329178750514984, "val_acc": 52.0}
{"epoch": 58, "training_loss": 69.22796773910522, "training_acc": 53.0, "val_loss": 17.32921153306961, "val_acc": 52.0}
{"epoch": 59, "training_loss": 69.20748519897461, "training_acc": 53.0, "val_loss": 17.329174280166626, "val_acc": 52.0}
{"epoch": 60, "training_loss": 69.21049547195435, "training_acc": 53.0, "val_loss": 17.32901781797409, "val_acc": 52.0}
{"epoch": 61, "training_loss": 69.16077470779419, "training_acc": 53.0, "val_loss": 17.328917980194092, "val_acc": 52.0}
{"epoch": 62, "training_loss": 69.23247718811035, "training_acc": 53.0, "val_loss": 17.328864336013794, "val_acc": 52.0}
{"epoch": 63, "training_loss": 69.20695114135742, "training_acc": 53.0, "val_loss": 17.328844964504242, "val_acc": 52.0}
{"epoch": 64, "training_loss": 69.17215776443481, "training_acc": 53.0, "val_loss": 17.328885197639465, "val_acc": 52.0}
{"epoch": 65, "training_loss": 69.22693395614624, "training_acc": 53.0, "val_loss": 17.32892245054245, "val_acc": 52.0}
{"epoch": 66, "training_loss": 69.2025499343872, "training_acc": 53.0, "val_loss": 17.328956723213196, "val_acc": 52.0}
{"epoch": 67, "training_loss": 69.20494484901428, "training_acc": 53.0, "val_loss": 17.329061031341553, "val_acc": 52.0}
{"epoch": 68, "training_loss": 69.21200132369995, "training_acc": 53.0, "val_loss": 17.329108715057373, "val_acc": 52.0}
{"epoch": 69, "training_loss": 69.22886753082275, "training_acc": 53.0, "val_loss": 17.329025268554688, "val_acc": 52.0}
{"epoch": 70, "training_loss": 69.16681003570557, "training_acc": 53.0, "val_loss": 17.328937351703644, "val_acc": 52.0}
{"epoch": 71, "training_loss": 69.22634291648865, "training_acc": 53.0, "val_loss": 17.328861355781555, "val_acc": 52.0}
{"epoch": 72, "training_loss": 69.21965384483337, "training_acc": 53.0, "val_loss": 17.32887774705887, "val_acc": 52.0}
{"epoch": 73, "training_loss": 69.18343448638916, "training_acc": 53.0, "val_loss": 17.32891947031021, "val_acc": 52.0}
{"epoch": 74, "training_loss": 69.23103332519531, "training_acc": 53.0, "val_loss": 17.328929901123047, "val_acc": 52.0}
{"epoch": 75, "training_loss": 69.22323036193848, "training_acc": 53.0, "val_loss": 17.328903079032898, "val_acc": 52.0}
{"epoch": 76, "training_loss": 69.18081474304199, "training_acc": 53.0, "val_loss": 17.328962683677673, "val_acc": 52.0}
{"epoch": 77, "training_loss": 69.19962215423584, "training_acc": 53.0, "val_loss": 17.329078912734985, "val_acc": 52.0}
{"epoch": 78, "training_loss": 69.19645118713379, "training_acc": 53.0, "val_loss": 17.32947826385498, "val_acc": 52.0}
{"epoch": 79, "training_loss": 69.17515635490417, "training_acc": 53.0, "val_loss": 17.329692840576172, "val_acc": 52.0}
{"epoch": 80, "training_loss": 69.16748857498169, "training_acc": 53.0, "val_loss": 17.32974201440811, "val_acc": 52.0}
{"epoch": 81, "training_loss": 69.19065999984741, "training_acc": 53.0, "val_loss": 17.32962727546692, "val_acc": 52.0}
{"epoch": 82, "training_loss": 69.19321608543396, "training_acc": 53.0, "val_loss": 17.329703271389008, "val_acc": 52.0}
