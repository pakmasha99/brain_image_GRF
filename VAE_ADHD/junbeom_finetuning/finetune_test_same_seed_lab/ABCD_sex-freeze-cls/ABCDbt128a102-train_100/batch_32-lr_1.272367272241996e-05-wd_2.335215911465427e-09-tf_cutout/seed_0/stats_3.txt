"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.35005283355713, "training_acc": 48.0, "val_loss": 17.26633459329605, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.34563255310059, "training_acc": 48.0, "val_loss": 17.2616645693779, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.2828323841095, "training_acc": 54.0, "val_loss": 17.258721590042114, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.2495846748352, "training_acc": 54.0, "val_loss": 17.257501184940338, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.26645803451538, "training_acc": 54.0, "val_loss": 17.255672812461853, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.22336888313293, "training_acc": 54.0, "val_loss": 17.254266142845154, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.29120540618896, "training_acc": 54.0, "val_loss": 17.253223061561584, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.24895858764648, "training_acc": 53.0, "val_loss": 17.25175380706787, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.23981666564941, "training_acc": 53.0, "val_loss": 17.250190675258636, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.19882392883301, "training_acc": 53.0, "val_loss": 17.24928617477417, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.2152669429779, "training_acc": 53.0, "val_loss": 17.24882423877716, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.16433477401733, "training_acc": 53.0, "val_loss": 17.248569428920746, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.17484283447266, "training_acc": 53.0, "val_loss": 17.24839359521866, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.19747114181519, "training_acc": 53.0, "val_loss": 17.24822223186493, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.1758508682251, "training_acc": 53.0, "val_loss": 17.24821776151657, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.13625574111938, "training_acc": 53.0, "val_loss": 17.24855750799179, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.21417212486267, "training_acc": 53.0, "val_loss": 17.24950820207596, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.20209169387817, "training_acc": 53.0, "val_loss": 17.250144481658936, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.18014454841614, "training_acc": 53.0, "val_loss": 17.25042313337326, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.15030145645142, "training_acc": 53.0, "val_loss": 17.251138389110565, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.14538955688477, "training_acc": 53.0, "val_loss": 17.251770198345184, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.1486701965332, "training_acc": 53.0, "val_loss": 17.251308262348175, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.15374112129211, "training_acc": 53.0, "val_loss": 17.25090891122818, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.18030452728271, "training_acc": 53.0, "val_loss": 17.250731587409973, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.18486261367798, "training_acc": 53.0, "val_loss": 17.2510027885437, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.19372415542603, "training_acc": 53.0, "val_loss": 17.251083254814148, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.18245363235474, "training_acc": 53.0, "val_loss": 17.25132465362549, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.15874338150024, "training_acc": 53.0, "val_loss": 17.252567410469055, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.18935632705688, "training_acc": 53.0, "val_loss": 17.254558205604553, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.17081308364868, "training_acc": 53.0, "val_loss": 17.255915701389313, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.16806244850159, "training_acc": 53.0, "val_loss": 17.25628972053528, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.17734885215759, "training_acc": 53.0, "val_loss": 17.256520688533783, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.14979457855225, "training_acc": 53.0, "val_loss": 17.25667119026184, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.2085132598877, "training_acc": 53.0, "val_loss": 17.25637912750244, "val_acc": 52.0}
