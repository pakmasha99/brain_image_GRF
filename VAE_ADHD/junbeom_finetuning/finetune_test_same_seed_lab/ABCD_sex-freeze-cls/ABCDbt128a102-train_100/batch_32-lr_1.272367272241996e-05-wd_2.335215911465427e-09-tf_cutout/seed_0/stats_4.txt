"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.0912733078003, "training_acc": 53.0, "val_loss": 17.332792282104492, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.03232860565186, "training_acc": 53.0, "val_loss": 17.334972321987152, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.06873869895935, "training_acc": 53.0, "val_loss": 17.337369918823242, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.07221007347107, "training_acc": 53.0, "val_loss": 17.33632981777191, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.07278895378113, "training_acc": 53.0, "val_loss": 17.33521968126297, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.06435251235962, "training_acc": 53.0, "val_loss": 17.334483563899994, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.07879686355591, "training_acc": 53.0, "val_loss": 17.333899438381195, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.06321859359741, "training_acc": 53.0, "val_loss": 17.33422577381134, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.07414770126343, "training_acc": 53.0, "val_loss": 17.33498126268387, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.05914211273193, "training_acc": 53.0, "val_loss": 17.337553203105927, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.10827398300171, "training_acc": 53.0, "val_loss": 17.338134348392487, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.05662703514099, "training_acc": 53.0, "val_loss": 17.336803674697876, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.01866579055786, "training_acc": 53.0, "val_loss": 17.334742844104767, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.05041790008545, "training_acc": 53.0, "val_loss": 17.3318549990654, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.05742883682251, "training_acc": 53.0, "val_loss": 17.329570651054382, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.07451891899109, "training_acc": 53.0, "val_loss": 17.32899099588394, "val_acc": 52.0}
{"epoch": 16, "training_loss": 68.9951171875, "training_acc": 53.0, "val_loss": 17.329560220241547, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.06490325927734, "training_acc": 53.0, "val_loss": 17.33012944459915, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.0455572605133, "training_acc": 53.0, "val_loss": 17.32966899871826, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.02058982849121, "training_acc": 53.0, "val_loss": 17.329072952270508, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.08053207397461, "training_acc": 53.0, "val_loss": 17.32778549194336, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.05020141601562, "training_acc": 53.0, "val_loss": 17.32613295316696, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.0512170791626, "training_acc": 53.0, "val_loss": 17.324618995189667, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.05474662780762, "training_acc": 53.0, "val_loss": 17.32335090637207, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.05081605911255, "training_acc": 53.0, "val_loss": 17.323054373264313, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.04370641708374, "training_acc": 53.0, "val_loss": 17.322909832000732, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.07031750679016, "training_acc": 53.0, "val_loss": 17.322707176208496, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.09557056427002, "training_acc": 53.0, "val_loss": 17.322416603565216, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.07626366615295, "training_acc": 53.0, "val_loss": 17.321889102458954, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.06289672851562, "training_acc": 53.0, "val_loss": 17.32148677110672, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.06765365600586, "training_acc": 53.0, "val_loss": 17.32126772403717, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.12343502044678, "training_acc": 53.0, "val_loss": 17.321226000785828, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.09080696105957, "training_acc": 53.0, "val_loss": 17.32109785079956, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.07539296150208, "training_acc": 53.0, "val_loss": 17.321081459522247, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.05602312088013, "training_acc": 53.0, "val_loss": 17.321163415908813, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.04750466346741, "training_acc": 53.0, "val_loss": 17.321231961250305, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.05479288101196, "training_acc": 53.0, "val_loss": 17.32132136821747, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.07036519050598, "training_acc": 53.0, "val_loss": 17.32141822576523, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.04516553878784, "training_acc": 53.0, "val_loss": 17.321670055389404, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.08692169189453, "training_acc": 53.0, "val_loss": 17.321674525737762, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.05924129486084, "training_acc": 53.0, "val_loss": 17.32187569141388, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.04926371574402, "training_acc": 53.0, "val_loss": 17.32170432806015, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.09384393692017, "training_acc": 53.0, "val_loss": 17.32158064842224, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.03487348556519, "training_acc": 53.0, "val_loss": 17.321249842643738, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.11633729934692, "training_acc": 53.0, "val_loss": 17.321021854877472, "val_acc": 52.0}
{"epoch": 45, "training_loss": 69.10200834274292, "training_acc": 53.0, "val_loss": 17.321044206619263, "val_acc": 52.0}
{"epoch": 46, "training_loss": 69.10446119308472, "training_acc": 53.0, "val_loss": 17.321258783340454, "val_acc": 52.0}
{"epoch": 47, "training_loss": 69.03642392158508, "training_acc": 53.0, "val_loss": 17.32174903154373, "val_acc": 52.0}
{"epoch": 48, "training_loss": 69.07685470581055, "training_acc": 53.0, "val_loss": 17.322532832622528, "val_acc": 52.0}
{"epoch": 49, "training_loss": 69.06120347976685, "training_acc": 53.0, "val_loss": 17.3235222697258, "val_acc": 52.0}
{"epoch": 50, "training_loss": 69.01497721672058, "training_acc": 53.0, "val_loss": 17.324298620224, "val_acc": 52.0}
{"epoch": 51, "training_loss": 69.05028510093689, "training_acc": 53.0, "val_loss": 17.32500046491623, "val_acc": 52.0}
{"epoch": 52, "training_loss": 68.99619793891907, "training_acc": 53.0, "val_loss": 17.32618510723114, "val_acc": 52.0}
{"epoch": 53, "training_loss": 69.0582594871521, "training_acc": 53.0, "val_loss": 17.327560484409332, "val_acc": 52.0}
{"epoch": 54, "training_loss": 69.05541586875916, "training_acc": 53.0, "val_loss": 17.32814311981201, "val_acc": 52.0}
{"epoch": 55, "training_loss": 69.0284218788147, "training_acc": 53.0, "val_loss": 17.327235639095306, "val_acc": 52.0}
{"epoch": 56, "training_loss": 69.10004758834839, "training_acc": 53.0, "val_loss": 17.326220870018005, "val_acc": 52.0}
{"epoch": 57, "training_loss": 69.05095720291138, "training_acc": 53.0, "val_loss": 17.32553094625473, "val_acc": 52.0}
{"epoch": 58, "training_loss": 69.06756210327148, "training_acc": 53.0, "val_loss": 17.32492446899414, "val_acc": 52.0}
{"epoch": 59, "training_loss": 69.05229568481445, "training_acc": 53.0, "val_loss": 17.323875427246094, "val_acc": 52.0}
{"epoch": 60, "training_loss": 69.08511233329773, "training_acc": 53.0, "val_loss": 17.32335239648819, "val_acc": 52.0}
{"epoch": 61, "training_loss": 69.02148342132568, "training_acc": 53.0, "val_loss": 17.322859168052673, "val_acc": 52.0}
{"epoch": 62, "training_loss": 69.05438661575317, "training_acc": 53.0, "val_loss": 17.322291433811188, "val_acc": 52.0}
{"epoch": 63, "training_loss": 69.07235956192017, "training_acc": 53.0, "val_loss": 17.32196807861328, "val_acc": 52.0}
