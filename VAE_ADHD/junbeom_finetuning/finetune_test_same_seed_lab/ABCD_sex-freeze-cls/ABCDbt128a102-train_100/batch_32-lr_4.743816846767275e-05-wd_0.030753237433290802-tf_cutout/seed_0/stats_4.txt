"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.49291396141052, "training_acc": 58.0, "val_loss": 17.504940927028656, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.22181439399719, "training_acc": 58.0, "val_loss": 17.499421536922455, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.18109321594238, "training_acc": 57.0, "val_loss": 17.498351633548737, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.52209162712097, "training_acc": 54.0, "val_loss": 17.500896751880646, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.37832164764404, "training_acc": 57.0, "val_loss": 17.505590617656708, "val_acc": 52.0}
{"epoch": 5, "training_loss": 68.93294095993042, "training_acc": 58.0, "val_loss": 17.502978444099426, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.03972864151001, "training_acc": 59.0, "val_loss": 17.501601576805115, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.30002951622009, "training_acc": 61.0, "val_loss": 17.498452961444855, "val_acc": 52.0}
{"epoch": 8, "training_loss": 68.69883489608765, "training_acc": 58.0, "val_loss": 17.495356500148773, "val_acc": 52.0}
{"epoch": 9, "training_loss": 68.40764117240906, "training_acc": 57.0, "val_loss": 17.492030560970306, "val_acc": 52.0}
{"epoch": 10, "training_loss": 68.5673098564148, "training_acc": 59.0, "val_loss": 17.48543381690979, "val_acc": 52.0}
{"epoch": 11, "training_loss": 68.34691572189331, "training_acc": 61.0, "val_loss": 17.484787106513977, "val_acc": 52.0}
{"epoch": 12, "training_loss": 68.73293280601501, "training_acc": 56.0, "val_loss": 17.480553686618805, "val_acc": 52.0}
{"epoch": 13, "training_loss": 67.7491135597229, "training_acc": 59.0, "val_loss": 17.475424706935883, "val_acc": 52.0}
{"epoch": 14, "training_loss": 67.93595957756042, "training_acc": 61.0, "val_loss": 17.47329533100128, "val_acc": 52.0}
{"epoch": 15, "training_loss": 68.03344202041626, "training_acc": 58.0, "val_loss": 17.479312419891357, "val_acc": 52.0}
{"epoch": 16, "training_loss": 67.73399496078491, "training_acc": 58.0, "val_loss": 17.483322322368622, "val_acc": 52.0}
{"epoch": 17, "training_loss": 68.28930735588074, "training_acc": 57.0, "val_loss": 17.48352199792862, "val_acc": 52.0}
{"epoch": 18, "training_loss": 67.53213596343994, "training_acc": 57.0, "val_loss": 17.480002343654633, "val_acc": 52.0}
{"epoch": 19, "training_loss": 67.8078019618988, "training_acc": 60.0, "val_loss": 17.47366040945053, "val_acc": 52.0}
{"epoch": 20, "training_loss": 67.96732974052429, "training_acc": 57.0, "val_loss": 17.469191551208496, "val_acc": 52.0}
{"epoch": 21, "training_loss": 67.28072357177734, "training_acc": 58.0, "val_loss": 17.458459734916687, "val_acc": 52.0}
{"epoch": 22, "training_loss": 67.1664707660675, "training_acc": 59.0, "val_loss": 17.44937300682068, "val_acc": 52.0}
{"epoch": 23, "training_loss": 67.31639862060547, "training_acc": 61.0, "val_loss": 17.446057498455048, "val_acc": 52.0}
{"epoch": 24, "training_loss": 67.57540941238403, "training_acc": 59.0, "val_loss": 17.444340884685516, "val_acc": 52.0}
{"epoch": 25, "training_loss": 67.45187997817993, "training_acc": 60.0, "val_loss": 17.44130700826645, "val_acc": 52.0}
{"epoch": 26, "training_loss": 67.85107660293579, "training_acc": 55.0, "val_loss": 17.439033091068268, "val_acc": 52.0}
{"epoch": 27, "training_loss": 66.8480932712555, "training_acc": 59.0, "val_loss": 17.434684932231903, "val_acc": 52.0}
{"epoch": 28, "training_loss": 66.95048594474792, "training_acc": 62.0, "val_loss": 17.43180751800537, "val_acc": 52.0}
{"epoch": 29, "training_loss": 66.89584040641785, "training_acc": 61.0, "val_loss": 17.43113249540329, "val_acc": 52.0}
{"epoch": 30, "training_loss": 66.84149837493896, "training_acc": 62.0, "val_loss": 17.42594540119171, "val_acc": 52.0}
{"epoch": 31, "training_loss": 66.79879093170166, "training_acc": 60.0, "val_loss": 17.424961924552917, "val_acc": 52.0}
{"epoch": 32, "training_loss": 66.65448760986328, "training_acc": 63.0, "val_loss": 17.423515021800995, "val_acc": 52.0}
{"epoch": 33, "training_loss": 66.6130485534668, "training_acc": 65.0, "val_loss": 17.41832047700882, "val_acc": 52.0}
{"epoch": 34, "training_loss": 67.03423714637756, "training_acc": 63.0, "val_loss": 17.413227260112762, "val_acc": 52.0}
{"epoch": 35, "training_loss": 66.61839723587036, "training_acc": 60.0, "val_loss": 17.408248782157898, "val_acc": 52.0}
{"epoch": 36, "training_loss": 66.27031946182251, "training_acc": 64.0, "val_loss": 17.41003394126892, "val_acc": 52.0}
{"epoch": 37, "training_loss": 66.22754001617432, "training_acc": 64.0, "val_loss": 17.413653433322906, "val_acc": 52.0}
{"epoch": 38, "training_loss": 66.25260353088379, "training_acc": 61.0, "val_loss": 17.416927218437195, "val_acc": 52.0}
{"epoch": 39, "training_loss": 66.24306344985962, "training_acc": 64.0, "val_loss": 17.416882514953613, "val_acc": 52.0}
{"epoch": 40, "training_loss": 66.23437190055847, "training_acc": 64.0, "val_loss": 17.413701117038727, "val_acc": 52.0}
{"epoch": 41, "training_loss": 66.52317810058594, "training_acc": 62.0, "val_loss": 17.405913770198822, "val_acc": 52.0}
{"epoch": 42, "training_loss": 65.88675117492676, "training_acc": 61.0, "val_loss": 17.40042269229889, "val_acc": 52.0}
{"epoch": 43, "training_loss": 65.82170248031616, "training_acc": 62.0, "val_loss": 17.395851016044617, "val_acc": 52.0}
{"epoch": 44, "training_loss": 65.38204574584961, "training_acc": 60.0, "val_loss": 17.391566932201385, "val_acc": 52.0}
{"epoch": 45, "training_loss": 65.36931848526001, "training_acc": 61.0, "val_loss": 17.386049032211304, "val_acc": 52.0}
{"epoch": 46, "training_loss": 65.8741250038147, "training_acc": 62.0, "val_loss": 17.38203912973404, "val_acc": 52.0}
{"epoch": 47, "training_loss": 66.09898543357849, "training_acc": 59.0, "val_loss": 17.380788922309875, "val_acc": 52.0}
{"epoch": 48, "training_loss": 65.56923151016235, "training_acc": 64.0, "val_loss": 17.379073798656464, "val_acc": 52.0}
{"epoch": 49, "training_loss": 65.4664044380188, "training_acc": 60.0, "val_loss": 17.37925559282303, "val_acc": 52.0}
{"epoch": 50, "training_loss": 65.0674204826355, "training_acc": 62.0, "val_loss": 17.379389703273773, "val_acc": 52.0}
{"epoch": 51, "training_loss": 65.13243865966797, "training_acc": 62.0, "val_loss": 17.377978563308716, "val_acc": 52.0}
{"epoch": 52, "training_loss": 65.06328630447388, "training_acc": 61.0, "val_loss": 17.380094528198242, "val_acc": 52.0}
{"epoch": 53, "training_loss": 65.72009372711182, "training_acc": 60.0, "val_loss": 17.38184243440628, "val_acc": 52.0}
{"epoch": 54, "training_loss": 65.18113589286804, "training_acc": 59.0, "val_loss": 17.38564819097519, "val_acc": 52.0}
{"epoch": 55, "training_loss": 65.39787292480469, "training_acc": 57.0, "val_loss": 17.39145964384079, "val_acc": 52.0}
{"epoch": 56, "training_loss": 65.8082160949707, "training_acc": 57.0, "val_loss": 17.391426861286163, "val_acc": 52.0}
{"epoch": 57, "training_loss": 65.47723746299744, "training_acc": 59.0, "val_loss": 17.391501367092133, "val_acc": 52.0}
{"epoch": 58, "training_loss": 64.9070930480957, "training_acc": 60.0, "val_loss": 17.383891344070435, "val_acc": 52.0}
{"epoch": 59, "training_loss": 64.97007870674133, "training_acc": 61.0, "val_loss": 17.379020154476166, "val_acc": 52.0}
{"epoch": 60, "training_loss": 64.98618149757385, "training_acc": 60.0, "val_loss": 17.37689971923828, "val_acc": 52.0}
{"epoch": 61, "training_loss": 64.69107377529144, "training_acc": 61.0, "val_loss": 17.373965680599213, "val_acc": 52.0}
{"epoch": 62, "training_loss": 65.04460287094116, "training_acc": 59.0, "val_loss": 17.371293902397156, "val_acc": 52.0}
{"epoch": 63, "training_loss": 64.51175332069397, "training_acc": 60.0, "val_loss": 17.368777096271515, "val_acc": 52.0}
{"epoch": 64, "training_loss": 64.63360118865967, "training_acc": 58.0, "val_loss": 17.365089058876038, "val_acc": 52.0}
{"epoch": 65, "training_loss": 64.3467173576355, "training_acc": 63.0, "val_loss": 17.36787110567093, "val_acc": 52.0}
{"epoch": 66, "training_loss": 64.29950523376465, "training_acc": 62.0, "val_loss": 17.37198531627655, "val_acc": 52.0}
{"epoch": 67, "training_loss": 64.0538318157196, "training_acc": 62.0, "val_loss": 17.37813949584961, "val_acc": 52.0}
{"epoch": 68, "training_loss": 64.29487895965576, "training_acc": 61.0, "val_loss": 17.383234202861786, "val_acc": 52.0}
{"epoch": 69, "training_loss": 63.75771927833557, "training_acc": 61.0, "val_loss": 17.38838106393814, "val_acc": 52.0}
{"epoch": 70, "training_loss": 64.88831901550293, "training_acc": 60.0, "val_loss": 17.39383339881897, "val_acc": 48.0}
{"epoch": 71, "training_loss": 64.09414172172546, "training_acc": 59.0, "val_loss": 17.39749163389206, "val_acc": 48.0}
{"epoch": 72, "training_loss": 64.18184733390808, "training_acc": 61.0, "val_loss": 17.40325689315796, "val_acc": 48.0}
{"epoch": 73, "training_loss": 63.67775535583496, "training_acc": 60.0, "val_loss": 17.410066723823547, "val_acc": 48.0}
{"epoch": 74, "training_loss": 64.25024247169495, "training_acc": 59.0, "val_loss": 17.413492500782013, "val_acc": 48.0}
{"epoch": 75, "training_loss": 63.85252022743225, "training_acc": 61.0, "val_loss": 17.415009438991547, "val_acc": 48.0}
{"epoch": 76, "training_loss": 63.94064712524414, "training_acc": 60.0, "val_loss": 17.415954172611237, "val_acc": 48.0}
{"epoch": 77, "training_loss": 64.0123119354248, "training_acc": 61.0, "val_loss": 17.416657507419586, "val_acc": 48.0}
{"epoch": 78, "training_loss": 63.91612243652344, "training_acc": 63.0, "val_loss": 17.413613200187683, "val_acc": 48.0}
{"epoch": 79, "training_loss": 63.49593949317932, "training_acc": 61.0, "val_loss": 17.4130842089653, "val_acc": 48.0}
{"epoch": 80, "training_loss": 63.929585695266724, "training_acc": 62.0, "val_loss": 17.41056740283966, "val_acc": 48.0}
{"epoch": 81, "training_loss": 63.682437896728516, "training_acc": 61.0, "val_loss": 17.409931123256683, "val_acc": 48.0}
{"epoch": 82, "training_loss": 63.805758476257324, "training_acc": 61.0, "val_loss": 17.406661808490753, "val_acc": 48.0}
{"epoch": 83, "training_loss": 63.51318645477295, "training_acc": 62.0, "val_loss": 17.40296483039856, "val_acc": 48.0}
