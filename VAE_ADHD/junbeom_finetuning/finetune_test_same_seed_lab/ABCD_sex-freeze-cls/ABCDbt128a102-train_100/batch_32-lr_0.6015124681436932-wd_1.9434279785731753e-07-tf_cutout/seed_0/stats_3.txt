"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 4080.0722465515137, "training_acc": 42.0, "val_loss": 705.0155639648438, "val_acc": 52.0}
{"epoch": 1, "training_loss": 1588.7320556640625, "training_acc": 63.0, "val_loss": 755.9293270111084, "val_acc": 48.0}
{"epoch": 2, "training_loss": 1635.163589477539, "training_acc": 64.0, "val_loss": 649.2171287536621, "val_acc": 44.0}
{"epoch": 3, "training_loss": 1446.8492889404297, "training_acc": 64.0, "val_loss": 867.778205871582, "val_acc": 36.0}
{"epoch": 4, "training_loss": 1290.9221873283386, "training_acc": 61.0, "val_loss": 690.2461528778076, "val_acc": 36.0}
{"epoch": 5, "training_loss": 1473.6265487670898, "training_acc": 67.0, "val_loss": 612.2043132781982, "val_acc": 40.0}
{"epoch": 6, "training_loss": 1018.6728172302246, "training_acc": 67.0, "val_loss": 542.7351474761963, "val_acc": 56.0}
{"epoch": 7, "training_loss": 1158.1800537109375, "training_acc": 63.0, "val_loss": 451.367712020874, "val_acc": 48.0}
{"epoch": 8, "training_loss": 1914.2070007324219, "training_acc": 60.0, "val_loss": 829.4975280761719, "val_acc": 44.0}
{"epoch": 9, "training_loss": 1015.8236155509949, "training_acc": 72.0, "val_loss": 889.6798133850098, "val_acc": 44.0}
{"epoch": 10, "training_loss": 2133.4666902616154, "training_acc": 56.0, "val_loss": 676.190185546875, "val_acc": 36.0}
{"epoch": 11, "training_loss": 1388.7305526733398, "training_acc": 63.0, "val_loss": 959.0126037597656, "val_acc": 40.0}
{"epoch": 12, "training_loss": 1134.6833000183105, "training_acc": 71.0, "val_loss": 704.3148517608643, "val_acc": 40.0}
{"epoch": 13, "training_loss": 597.4791717529297, "training_acc": 79.0, "val_loss": 815.5309677124023, "val_acc": 40.0}
{"epoch": 14, "training_loss": 976.3975483826362, "training_acc": 66.0, "val_loss": 720.3143119812012, "val_acc": 48.0}
{"epoch": 15, "training_loss": 727.8119773864746, "training_acc": 71.0, "val_loss": 799.975061416626, "val_acc": 48.0}
{"epoch": 16, "training_loss": 465.9627685546875, "training_acc": 76.0, "val_loss": 828.4815788269043, "val_acc": 44.0}
{"epoch": 17, "training_loss": 925.4595565795898, "training_acc": 76.0, "val_loss": 939.2143249511719, "val_acc": 36.0}
{"epoch": 18, "training_loss": 1213.2220916748047, "training_acc": 77.0, "val_loss": 1168.7371253967285, "val_acc": 40.0}
{"epoch": 19, "training_loss": 1918.89817237854, "training_acc": 60.0, "val_loss": 958.6506843566895, "val_acc": 56.0}
{"epoch": 20, "training_loss": 814.0654258728027, "training_acc": 71.0, "val_loss": 638.8164043426514, "val_acc": 52.0}
{"epoch": 21, "training_loss": 618.9666042327881, "training_acc": 77.0, "val_loss": 800.9156227111816, "val_acc": 48.0}
{"epoch": 22, "training_loss": 1268.831771850586, "training_acc": 71.0, "val_loss": 772.1081256866455, "val_acc": 44.0}
{"epoch": 23, "training_loss": 896.7830009460449, "training_acc": 74.0, "val_loss": 841.1060333251953, "val_acc": 48.0}
{"epoch": 24, "training_loss": 939.1487579345703, "training_acc": 79.0, "val_loss": 1019.0438270568848, "val_acc": 52.0}
{"epoch": 25, "training_loss": 928.1434783935547, "training_acc": 73.0, "val_loss": 943.0569648742676, "val_acc": 44.0}
{"epoch": 26, "training_loss": 1814.3861198425293, "training_acc": 69.0, "val_loss": 1028.9630889892578, "val_acc": 40.0}
