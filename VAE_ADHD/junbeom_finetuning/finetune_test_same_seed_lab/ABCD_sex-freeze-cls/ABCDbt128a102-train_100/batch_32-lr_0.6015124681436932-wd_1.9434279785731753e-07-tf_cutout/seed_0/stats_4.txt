"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 2295.959461212158, "training_acc": 37.0, "val_loss": 1057.5491905212402, "val_acc": 56.0}
{"epoch": 1, "training_loss": 3316.3066024780273, "training_acc": 57.0, "val_loss": 764.7106647491455, "val_acc": 48.0}
{"epoch": 2, "training_loss": 2697.6419372558594, "training_acc": 57.0, "val_loss": 1227.0461082458496, "val_acc": 40.0}
{"epoch": 3, "training_loss": 2319.6961517333984, "training_acc": 61.0, "val_loss": 1405.303955078125, "val_acc": 52.0}
{"epoch": 4, "training_loss": 3166.2593383789062, "training_acc": 66.0, "val_loss": 1603.1145095825195, "val_acc": 48.0}
{"epoch": 5, "training_loss": 2256.4461212158203, "training_acc": 68.0, "val_loss": 1069.5112228393555, "val_acc": 48.0}
{"epoch": 6, "training_loss": 3639.368766784668, "training_acc": 52.0, "val_loss": 1290.3298377990723, "val_acc": 48.0}
{"epoch": 7, "training_loss": 2059.480155944824, "training_acc": 59.0, "val_loss": 1160.6010437011719, "val_acc": 40.0}
{"epoch": 8, "training_loss": 2014.5671691894531, "training_acc": 72.0, "val_loss": 1045.792293548584, "val_acc": 48.0}
{"epoch": 9, "training_loss": 1477.5615844726562, "training_acc": 66.0, "val_loss": 1136.661148071289, "val_acc": 44.0}
{"epoch": 10, "training_loss": 1669.457763671875, "training_acc": 59.0, "val_loss": 885.2142333984375, "val_acc": 60.0}
{"epoch": 11, "training_loss": 1149.8980560302734, "training_acc": 72.0, "val_loss": 832.7442169189453, "val_acc": 56.0}
{"epoch": 12, "training_loss": 788.4611473083496, "training_acc": 73.0, "val_loss": 731.2532901763916, "val_acc": 48.0}
{"epoch": 13, "training_loss": 953.6635131835938, "training_acc": 71.0, "val_loss": 1016.7909622192383, "val_acc": 44.0}
{"epoch": 14, "training_loss": 1641.9645080566406, "training_acc": 66.0, "val_loss": 819.0363883972168, "val_acc": 40.0}
{"epoch": 15, "training_loss": 1162.8790321350098, "training_acc": 65.0, "val_loss": 889.3746376037598, "val_acc": 52.0}
{"epoch": 16, "training_loss": 876.0180206298828, "training_acc": 67.0, "val_loss": 786.7526054382324, "val_acc": 52.0}
{"epoch": 17, "training_loss": 806.6593589782715, "training_acc": 74.0, "val_loss": 825.48828125, "val_acc": 48.0}
{"epoch": 18, "training_loss": 1414.0861072540283, "training_acc": 65.0, "val_loss": 848.2914924621582, "val_acc": 44.0}
{"epoch": 19, "training_loss": 1126.9646263122559, "training_acc": 71.0, "val_loss": 1077.0780563354492, "val_acc": 36.0}
{"epoch": 20, "training_loss": 1087.840747833252, "training_acc": 72.0, "val_loss": 998.9276885986328, "val_acc": 48.0}
{"epoch": 21, "training_loss": 799.6171618103981, "training_acc": 78.0, "val_loss": 872.9351043701172, "val_acc": 40.0}
{"epoch": 22, "training_loss": 439.62221598625183, "training_acc": 78.0, "val_loss": 866.3518905639648, "val_acc": 40.0}
{"epoch": 23, "training_loss": 483.6759853199328, "training_acc": 77.0, "val_loss": 755.4660320281982, "val_acc": 40.0}
{"epoch": 24, "training_loss": 390.7209380865097, "training_acc": 79.0, "val_loss": 767.6355838775635, "val_acc": 52.0}
{"epoch": 25, "training_loss": 589.0084907473356, "training_acc": 77.0, "val_loss": 757.8334808349609, "val_acc": 48.0}
{"epoch": 26, "training_loss": 738.9100208282471, "training_acc": 72.0, "val_loss": 733.1282615661621, "val_acc": 56.0}
{"epoch": 27, "training_loss": 909.2744674682617, "training_acc": 78.0, "val_loss": 885.7073783874512, "val_acc": 40.0}
{"epoch": 28, "training_loss": 814.5645484924316, "training_acc": 69.0, "val_loss": 1020.8009719848633, "val_acc": 48.0}
{"epoch": 29, "training_loss": 666.8548698425293, "training_acc": 66.0, "val_loss": 930.5290222167969, "val_acc": 44.0}
{"epoch": 30, "training_loss": 1587.4642181396484, "training_acc": 71.0, "val_loss": 810.6074333190918, "val_acc": 44.0}
{"epoch": 31, "training_loss": 945.0473670959473, "training_acc": 70.0, "val_loss": 1323.179054260254, "val_acc": 60.0}
