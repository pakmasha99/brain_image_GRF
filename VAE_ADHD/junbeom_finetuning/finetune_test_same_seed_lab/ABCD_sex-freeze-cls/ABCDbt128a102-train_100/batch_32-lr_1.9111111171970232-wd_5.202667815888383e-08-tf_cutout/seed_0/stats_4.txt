"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 8723.272972106934, "training_acc": 50.0, "val_loss": 2384.3231201171875, "val_acc": 48.0}
{"epoch": 1, "training_loss": 5215.348670959473, "training_acc": 62.0, "val_loss": 3693.5935974121094, "val_acc": 56.0}
{"epoch": 2, "training_loss": 9060.79995727539, "training_acc": 61.0, "val_loss": 1869.1587448120117, "val_acc": 44.0}
{"epoch": 3, "training_loss": 4568.350341796875, "training_acc": 60.0, "val_loss": 2267.815399169922, "val_acc": 48.0}
{"epoch": 4, "training_loss": 4073.5913848876953, "training_acc": 66.0, "val_loss": 2349.228286743164, "val_acc": 40.0}
{"epoch": 5, "training_loss": 4962.1781005859375, "training_acc": 62.0, "val_loss": 2178.498077392578, "val_acc": 48.0}
{"epoch": 6, "training_loss": 4417.864776611328, "training_acc": 65.0, "val_loss": 2091.8664932250977, "val_acc": 48.0}
{"epoch": 7, "training_loss": 4532.594863891602, "training_acc": 68.0, "val_loss": 2421.952247619629, "val_acc": 44.0}
{"epoch": 8, "training_loss": 5346.228050231934, "training_acc": 62.0, "val_loss": 2092.726516723633, "val_acc": 48.0}
{"epoch": 9, "training_loss": 4431.992809295654, "training_acc": 71.0, "val_loss": 1972.0096588134766, "val_acc": 52.0}
{"epoch": 10, "training_loss": 4863.551849365234, "training_acc": 67.0, "val_loss": 2228.658676147461, "val_acc": 68.0}
{"epoch": 11, "training_loss": 3511.6540098190308, "training_acc": 71.0, "val_loss": 2058.804702758789, "val_acc": 56.0}
{"epoch": 12, "training_loss": 2482.2543506622314, "training_acc": 77.0, "val_loss": 2311.768913269043, "val_acc": 44.0}
{"epoch": 13, "training_loss": 4789.32275390625, "training_acc": 64.0, "val_loss": 2167.7303314208984, "val_acc": 48.0}
{"epoch": 14, "training_loss": 2290.1163940429688, "training_acc": 74.0, "val_loss": 2462.6693725585938, "val_acc": 52.0}
{"epoch": 15, "training_loss": 4038.9285888671875, "training_acc": 71.0, "val_loss": 1682.284164428711, "val_acc": 56.0}
{"epoch": 16, "training_loss": 3094.0570678710938, "training_acc": 73.0, "val_loss": 1599.1369247436523, "val_acc": 48.0}
{"epoch": 17, "training_loss": 1925.8461608886719, "training_acc": 74.0, "val_loss": 2669.8421478271484, "val_acc": 52.0}
{"epoch": 18, "training_loss": 4668.124282836914, "training_acc": 62.0, "val_loss": 2308.1817626953125, "val_acc": 48.0}
{"epoch": 19, "training_loss": 4876.021141052246, "training_acc": 61.0, "val_loss": 3383.527374267578, "val_acc": 56.0}
{"epoch": 20, "training_loss": 3124.2654571533203, "training_acc": 68.0, "val_loss": 2826.01261138916, "val_acc": 52.0}
{"epoch": 21, "training_loss": 3265.1019560694695, "training_acc": 74.0, "val_loss": 2919.5722579956055, "val_acc": 52.0}
{"epoch": 22, "training_loss": 2237.86083984375, "training_acc": 73.0, "val_loss": 2551.5504837036133, "val_acc": 44.0}
{"epoch": 23, "training_loss": 1857.2978563308716, "training_acc": 75.0, "val_loss": 2627.837562561035, "val_acc": 36.0}
{"epoch": 24, "training_loss": 1811.1889419555664, "training_acc": 79.0, "val_loss": 2570.564651489258, "val_acc": 36.0}
{"epoch": 25, "training_loss": 1794.6073455810547, "training_acc": 76.0, "val_loss": 2796.857452392578, "val_acc": 40.0}
{"epoch": 26, "training_loss": 2169.685777004808, "training_acc": 77.0, "val_loss": 2861.910820007324, "val_acc": 48.0}
{"epoch": 27, "training_loss": 2672.1463775634766, "training_acc": 73.0, "val_loss": 2791.1380767822266, "val_acc": 52.0}
{"epoch": 28, "training_loss": 1390.3097839355469, "training_acc": 77.0, "val_loss": 2936.6273880004883, "val_acc": 44.0}
{"epoch": 29, "training_loss": 1742.725730895996, "training_acc": 74.0, "val_loss": 3048.858642578125, "val_acc": 44.0}
{"epoch": 30, "training_loss": 1811.7464294433594, "training_acc": 80.0, "val_loss": 2861.614227294922, "val_acc": 60.0}
{"epoch": 31, "training_loss": 3470.2114868164062, "training_acc": 72.0, "val_loss": 3041.948699951172, "val_acc": 60.0}
{"epoch": 32, "training_loss": 5390.616229057312, "training_acc": 70.0, "val_loss": 2677.207374572754, "val_acc": 56.0}
{"epoch": 33, "training_loss": 4622.535186767578, "training_acc": 73.0, "val_loss": 2617.518997192383, "val_acc": 52.0}
{"epoch": 34, "training_loss": 3848.5027465820312, "training_acc": 71.0, "val_loss": 3212.429428100586, "val_acc": 44.0}
{"epoch": 35, "training_loss": 2985.863250732422, "training_acc": 70.0, "val_loss": 3422.5807189941406, "val_acc": 48.0}
