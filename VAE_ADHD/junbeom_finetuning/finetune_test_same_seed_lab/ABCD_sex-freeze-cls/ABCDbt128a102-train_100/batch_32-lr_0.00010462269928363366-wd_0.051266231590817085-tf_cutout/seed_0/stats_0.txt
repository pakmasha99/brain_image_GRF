"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.29856657981873, "training_acc": 52.0, "val_loss": 17.226429283618927, "val_acc": 56.0}
{"epoch": 1, "training_loss": 69.31340146064758, "training_acc": 52.0, "val_loss": 17.194852232933044, "val_acc": 56.0}
{"epoch": 2, "training_loss": 69.31943607330322, "training_acc": 52.0, "val_loss": 17.19788759946823, "val_acc": 56.0}
{"epoch": 3, "training_loss": 69.25496697425842, "training_acc": 52.0, "val_loss": 17.175288498401642, "val_acc": 56.0}
{"epoch": 4, "training_loss": 69.30293464660645, "training_acc": 52.0, "val_loss": 17.1905055642128, "val_acc": 56.0}
{"epoch": 5, "training_loss": 69.24251127243042, "training_acc": 52.0, "val_loss": 17.18139946460724, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.23992371559143, "training_acc": 52.0, "val_loss": 17.184509336948395, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.27898097038269, "training_acc": 52.0, "val_loss": 17.194579541683197, "val_acc": 56.0}
{"epoch": 8, "training_loss": 69.2371392250061, "training_acc": 52.0, "val_loss": 17.195473611354828, "val_acc": 56.0}
{"epoch": 9, "training_loss": 69.30309915542603, "training_acc": 52.0, "val_loss": 17.223618924617767, "val_acc": 56.0}
{"epoch": 10, "training_loss": 69.25476932525635, "training_acc": 52.0, "val_loss": 17.280513048171997, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.28201532363892, "training_acc": 48.0, "val_loss": 17.275604605674744, "val_acc": 56.0}
{"epoch": 12, "training_loss": 69.34219598770142, "training_acc": 51.0, "val_loss": 17.221175134181976, "val_acc": 56.0}
{"epoch": 13, "training_loss": 69.1863796710968, "training_acc": 52.0, "val_loss": 17.194026708602905, "val_acc": 56.0}
{"epoch": 14, "training_loss": 69.25674843788147, "training_acc": 52.0, "val_loss": 17.16695725917816, "val_acc": 56.0}
{"epoch": 15, "training_loss": 69.24915552139282, "training_acc": 52.0, "val_loss": 17.152100801467896, "val_acc": 56.0}
{"epoch": 16, "training_loss": 69.44177103042603, "training_acc": 52.0, "val_loss": 17.13743507862091, "val_acc": 56.0}
{"epoch": 17, "training_loss": 69.38594365119934, "training_acc": 52.0, "val_loss": 17.14145690202713, "val_acc": 56.0}
{"epoch": 18, "training_loss": 69.29448342323303, "training_acc": 52.0, "val_loss": 17.161138355731964, "val_acc": 56.0}
{"epoch": 19, "training_loss": 69.24743747711182, "training_acc": 52.0, "val_loss": 17.196574807167053, "val_acc": 56.0}
{"epoch": 20, "training_loss": 69.25671911239624, "training_acc": 52.0, "val_loss": 17.21515655517578, "val_acc": 56.0}
{"epoch": 21, "training_loss": 69.26272869110107, "training_acc": 52.0, "val_loss": 17.23064035177231, "val_acc": 56.0}
{"epoch": 22, "training_loss": 69.27709078788757, "training_acc": 52.0, "val_loss": 17.24109798669815, "val_acc": 56.0}
{"epoch": 23, "training_loss": 69.23557782173157, "training_acc": 52.0, "val_loss": 17.25616455078125, "val_acc": 56.0}
{"epoch": 24, "training_loss": 69.28653240203857, "training_acc": 52.0, "val_loss": 17.229093611240387, "val_acc": 56.0}
{"epoch": 25, "training_loss": 69.22090458869934, "training_acc": 52.0, "val_loss": 17.233270406723022, "val_acc": 56.0}
{"epoch": 26, "training_loss": 69.25223159790039, "training_acc": 52.0, "val_loss": 17.22874343395233, "val_acc": 56.0}
{"epoch": 27, "training_loss": 69.21241497993469, "training_acc": 52.0, "val_loss": 17.231696844100952, "val_acc": 56.0}
{"epoch": 28, "training_loss": 69.24164319038391, "training_acc": 52.0, "val_loss": 17.22951829433441, "val_acc": 56.0}
{"epoch": 29, "training_loss": 69.21717238426208, "training_acc": 52.0, "val_loss": 17.19915419816971, "val_acc": 56.0}
{"epoch": 30, "training_loss": 69.35466623306274, "training_acc": 52.0, "val_loss": 17.15586483478546, "val_acc": 56.0}
{"epoch": 31, "training_loss": 69.27976322174072, "training_acc": 52.0, "val_loss": 17.136332392692566, "val_acc": 56.0}
{"epoch": 32, "training_loss": 69.50058007240295, "training_acc": 52.0, "val_loss": 17.132309079170227, "val_acc": 56.0}
