"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.89643883705139, "training_acc": 47.0, "val_loss": 17.445090413093567, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.80372452735901, "training_acc": 47.0, "val_loss": 17.438732087612152, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.76101756095886, "training_acc": 47.0, "val_loss": 17.435088753700256, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.77087688446045, "training_acc": 47.0, "val_loss": 17.429816722869873, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.69757795333862, "training_acc": 47.0, "val_loss": 17.41177886724472, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.6718258857727, "training_acc": 47.0, "val_loss": 17.395946383476257, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.53791999816895, "training_acc": 47.0, "val_loss": 17.382700741291046, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.49087381362915, "training_acc": 47.0, "val_loss": 17.370139062404633, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.40859508514404, "training_acc": 47.0, "val_loss": 17.35798269510269, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.37416815757751, "training_acc": 49.0, "val_loss": 17.346228659152985, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.2990493774414, "training_acc": 49.0, "val_loss": 17.337653040885925, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.22325849533081, "training_acc": 53.0, "val_loss": 17.330244183540344, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.22100591659546, "training_acc": 54.0, "val_loss": 17.326007783412933, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.17122483253479, "training_acc": 53.0, "val_loss": 17.32461154460907, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.15941429138184, "training_acc": 53.0, "val_loss": 17.323605716228485, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.17694520950317, "training_acc": 53.0, "val_loss": 17.321372032165527, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.18860149383545, "training_acc": 53.0, "val_loss": 17.320503294467926, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.15804862976074, "training_acc": 53.0, "val_loss": 17.320434749126434, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.12574481964111, "training_acc": 53.0, "val_loss": 17.32076406478882, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.11205577850342, "training_acc": 53.0, "val_loss": 17.32226312160492, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.11645865440369, "training_acc": 53.0, "val_loss": 17.32500195503235, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.11134195327759, "training_acc": 53.0, "val_loss": 17.32744425535202, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.12205672264099, "training_acc": 53.0, "val_loss": 17.331233620643616, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.10220527648926, "training_acc": 53.0, "val_loss": 17.334994673728943, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.10949110984802, "training_acc": 53.0, "val_loss": 17.333093285560608, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.08647608757019, "training_acc": 53.0, "val_loss": 17.33378916978836, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.10232663154602, "training_acc": 53.0, "val_loss": 17.33490377664566, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.09005355834961, "training_acc": 53.0, "val_loss": 17.332860827445984, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.1277813911438, "training_acc": 53.0, "val_loss": 17.332126200199127, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.07635283470154, "training_acc": 53.0, "val_loss": 17.33568161725998, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.12182927131653, "training_acc": 53.0, "val_loss": 17.340900003910065, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.09029388427734, "training_acc": 53.0, "val_loss": 17.346668243408203, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.1268663406372, "training_acc": 53.0, "val_loss": 17.352935671806335, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.19618916511536, "training_acc": 53.0, "val_loss": 17.356644570827484, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.16692328453064, "training_acc": 53.0, "val_loss": 17.359839379787445, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.16090416908264, "training_acc": 53.0, "val_loss": 17.361848056316376, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.16850352287292, "training_acc": 53.0, "val_loss": 17.359784245491028, "val_acc": 52.0}
