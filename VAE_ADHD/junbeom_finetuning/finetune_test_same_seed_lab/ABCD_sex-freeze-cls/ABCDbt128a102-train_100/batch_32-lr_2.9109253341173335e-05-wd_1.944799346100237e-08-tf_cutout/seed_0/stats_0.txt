"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.26441478729248, "training_acc": 52.0, "val_loss": 17.236295342445374, "val_acc": 56.0}
{"epoch": 1, "training_loss": 69.28950786590576, "training_acc": 52.0, "val_loss": 17.22443699836731, "val_acc": 56.0}
{"epoch": 2, "training_loss": 69.28028702735901, "training_acc": 52.0, "val_loss": 17.222556471824646, "val_acc": 56.0}
{"epoch": 3, "training_loss": 69.22448825836182, "training_acc": 52.0, "val_loss": 17.211948335170746, "val_acc": 56.0}
{"epoch": 4, "training_loss": 69.28121399879456, "training_acc": 52.0, "val_loss": 17.21421927213669, "val_acc": 56.0}
{"epoch": 5, "training_loss": 69.23000001907349, "training_acc": 52.0, "val_loss": 17.20810979604721, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.22971057891846, "training_acc": 52.0, "val_loss": 17.205996811389923, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.26322436332703, "training_acc": 52.0, "val_loss": 17.20615029335022, "val_acc": 56.0}
{"epoch": 8, "training_loss": 69.22635531425476, "training_acc": 52.0, "val_loss": 17.203842103481293, "val_acc": 56.0}
{"epoch": 9, "training_loss": 69.28688073158264, "training_acc": 52.0, "val_loss": 17.209720611572266, "val_acc": 56.0}
{"epoch": 10, "training_loss": 69.25308561325073, "training_acc": 52.0, "val_loss": 17.22261607646942, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.23205351829529, "training_acc": 52.0, "val_loss": 17.222800850868225, "val_acc": 56.0}
{"epoch": 12, "training_loss": 69.2636775970459, "training_acc": 52.0, "val_loss": 17.211604118347168, "val_acc": 56.0}
{"epoch": 13, "training_loss": 69.21844530105591, "training_acc": 52.0, "val_loss": 17.20549315214157, "val_acc": 56.0}
{"epoch": 14, "training_loss": 69.26097464561462, "training_acc": 52.0, "val_loss": 17.19699054956436, "val_acc": 56.0}
{"epoch": 15, "training_loss": 69.23158311843872, "training_acc": 52.0, "val_loss": 17.19019263982773, "val_acc": 56.0}
{"epoch": 16, "training_loss": 69.31177282333374, "training_acc": 52.0, "val_loss": 17.17993915081024, "val_acc": 56.0}
{"epoch": 17, "training_loss": 69.25322103500366, "training_acc": 52.0, "val_loss": 17.180046439170837, "val_acc": 56.0}
{"epoch": 18, "training_loss": 69.23081970214844, "training_acc": 52.0, "val_loss": 17.18595027923584, "val_acc": 56.0}
{"epoch": 19, "training_loss": 69.22930240631104, "training_acc": 52.0, "val_loss": 17.194125056266785, "val_acc": 56.0}
{"epoch": 20, "training_loss": 69.2639091014862, "training_acc": 52.0, "val_loss": 17.196904122829437, "val_acc": 56.0}
{"epoch": 21, "training_loss": 69.26568031311035, "training_acc": 52.0, "val_loss": 17.19975173473358, "val_acc": 56.0}
{"epoch": 22, "training_loss": 69.28782820701599, "training_acc": 52.0, "val_loss": 17.202380299568176, "val_acc": 56.0}
{"epoch": 23, "training_loss": 69.23638868331909, "training_acc": 52.0, "val_loss": 17.20694750547409, "val_acc": 56.0}
{"epoch": 24, "training_loss": 69.24680042266846, "training_acc": 52.0, "val_loss": 17.20266342163086, "val_acc": 56.0}
{"epoch": 25, "training_loss": 69.22843718528748, "training_acc": 52.0, "val_loss": 17.20564365386963, "val_acc": 56.0}
{"epoch": 26, "training_loss": 69.23511338233948, "training_acc": 52.0, "val_loss": 17.20658838748932, "val_acc": 56.0}
{"epoch": 27, "training_loss": 69.22453093528748, "training_acc": 52.0, "val_loss": 17.209312319755554, "val_acc": 56.0}
{"epoch": 28, "training_loss": 69.2461040019989, "training_acc": 52.0, "val_loss": 17.210757732391357, "val_acc": 56.0}
{"epoch": 29, "training_loss": 69.23410749435425, "training_acc": 52.0, "val_loss": 17.204348742961884, "val_acc": 56.0}
{"epoch": 30, "training_loss": 69.29232406616211, "training_acc": 52.0, "val_loss": 17.190341651439667, "val_acc": 56.0}
{"epoch": 31, "training_loss": 69.27289342880249, "training_acc": 52.0, "val_loss": 17.178863286972046, "val_acc": 56.0}
{"epoch": 32, "training_loss": 69.28735113143921, "training_acc": 52.0, "val_loss": 17.165037989616394, "val_acc": 56.0}
{"epoch": 33, "training_loss": 69.2715904712677, "training_acc": 52.0, "val_loss": 17.157340049743652, "val_acc": 56.0}
{"epoch": 34, "training_loss": 69.3061933517456, "training_acc": 52.0, "val_loss": 17.156371474266052, "val_acc": 56.0}
{"epoch": 35, "training_loss": 69.32805061340332, "training_acc": 52.0, "val_loss": 17.159403860569, "val_acc": 56.0}
{"epoch": 36, "training_loss": 69.32391691207886, "training_acc": 52.0, "val_loss": 17.15928465127945, "val_acc": 56.0}
{"epoch": 37, "training_loss": 69.27943325042725, "training_acc": 52.0, "val_loss": 17.158398032188416, "val_acc": 56.0}
{"epoch": 38, "training_loss": 69.30386281013489, "training_acc": 52.0, "val_loss": 17.160247266292572, "val_acc": 56.0}
{"epoch": 39, "training_loss": 69.33429145812988, "training_acc": 52.0, "val_loss": 17.1654149889946, "val_acc": 56.0}
{"epoch": 40, "training_loss": 69.30751466751099, "training_acc": 52.0, "val_loss": 17.173457145690918, "val_acc": 56.0}
{"epoch": 41, "training_loss": 69.23894214630127, "training_acc": 52.0, "val_loss": 17.17795580625534, "val_acc": 56.0}
{"epoch": 42, "training_loss": 69.25610518455505, "training_acc": 52.0, "val_loss": 17.18481034040451, "val_acc": 56.0}
{"epoch": 43, "training_loss": 69.21439266204834, "training_acc": 52.0, "val_loss": 17.192035913467407, "val_acc": 56.0}
{"epoch": 44, "training_loss": 69.23345565795898, "training_acc": 52.0, "val_loss": 17.20370054244995, "val_acc": 56.0}
{"epoch": 45, "training_loss": 69.2497410774231, "training_acc": 52.0, "val_loss": 17.2116219997406, "val_acc": 56.0}
{"epoch": 46, "training_loss": 69.24329924583435, "training_acc": 52.0, "val_loss": 17.216399312019348, "val_acc": 56.0}
{"epoch": 47, "training_loss": 69.22812032699585, "training_acc": 52.0, "val_loss": 17.215770483016968, "val_acc": 56.0}
{"epoch": 48, "training_loss": 69.25001668930054, "training_acc": 52.0, "val_loss": 17.2165185213089, "val_acc": 56.0}
{"epoch": 49, "training_loss": 69.22134494781494, "training_acc": 52.0, "val_loss": 17.2235369682312, "val_acc": 56.0}
{"epoch": 50, "training_loss": 69.26629590988159, "training_acc": 52.0, "val_loss": 17.236171662807465, "val_acc": 56.0}
{"epoch": 51, "training_loss": 69.279953956604, "training_acc": 52.0, "val_loss": 17.25222021341324, "val_acc": 56.0}
{"epoch": 52, "training_loss": 69.27701044082642, "training_acc": 52.0, "val_loss": 17.268988490104675, "val_acc": 56.0}
{"epoch": 53, "training_loss": 69.28180980682373, "training_acc": 52.0, "val_loss": 17.279095947742462, "val_acc": 56.0}
