"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 70.0096914768219, "training_acc": 47.0, "val_loss": 17.501582205295563, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.90258049964905, "training_acc": 47.0, "val_loss": 17.487044632434845, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.80901193618774, "training_acc": 47.0, "val_loss": 17.468208074569702, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.79766297340393, "training_acc": 47.0, "val_loss": 17.45433807373047, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.7052993774414, "training_acc": 47.0, "val_loss": 17.438551783561707, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.5758535861969, "training_acc": 47.0, "val_loss": 17.420867085456848, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.468186378479, "training_acc": 47.0, "val_loss": 17.41010993719101, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.45512056350708, "training_acc": 47.0, "val_loss": 17.396995425224304, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.3966817855835, "training_acc": 47.0, "val_loss": 17.390891909599304, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.36272072792053, "training_acc": 47.0, "val_loss": 17.3837810754776, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.33937168121338, "training_acc": 48.0, "val_loss": 17.38048642873764, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.27711391448975, "training_acc": 47.0, "val_loss": 17.37738400697708, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.33305764198303, "training_acc": 50.0, "val_loss": 17.372414469718933, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.22912883758545, "training_acc": 57.0, "val_loss": 17.367704212665558, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.20568799972534, "training_acc": 58.0, "val_loss": 17.362336814403534, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.13573956489563, "training_acc": 57.0, "val_loss": 17.357809841632843, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.13776087760925, "training_acc": 52.0, "val_loss": 17.353537678718567, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.0861439704895, "training_acc": 53.0, "val_loss": 17.35074371099472, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.09929037094116, "training_acc": 53.0, "val_loss": 17.350097000598907, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.12031292915344, "training_acc": 53.0, "val_loss": 17.350097000598907, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.08180618286133, "training_acc": 53.0, "val_loss": 17.35014319419861, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.06673526763916, "training_acc": 53.0, "val_loss": 17.35023856163025, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.0375189781189, "training_acc": 53.0, "val_loss": 17.3505499958992, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.03309488296509, "training_acc": 53.0, "val_loss": 17.35055446624756, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.07303309440613, "training_acc": 53.0, "val_loss": 17.35062152147293, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.07808017730713, "training_acc": 53.0, "val_loss": 17.35137701034546, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.10765051841736, "training_acc": 53.0, "val_loss": 17.351941764354706, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.02186489105225, "training_acc": 53.0, "val_loss": 17.351309955120087, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.0593991279602, "training_acc": 53.0, "val_loss": 17.351211607456207, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.07309460639954, "training_acc": 53.0, "val_loss": 17.352057993412018, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.01858282089233, "training_acc": 53.0, "val_loss": 17.35442876815796, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.03940010070801, "training_acc": 53.0, "val_loss": 17.3541858792305, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.06777834892273, "training_acc": 53.0, "val_loss": 17.355036735534668, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.04390096664429, "training_acc": 53.0, "val_loss": 17.355845868587494, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.04416131973267, "training_acc": 53.0, "val_loss": 17.356406152248383, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.09249758720398, "training_acc": 53.0, "val_loss": 17.357903718948364, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.03157424926758, "training_acc": 53.0, "val_loss": 17.356401681900024, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.05702686309814, "training_acc": 53.0, "val_loss": 17.355777323246002, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.01639127731323, "training_acc": 53.0, "val_loss": 17.355312407016754, "val_acc": 52.0}
