"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.32716846466064, "training_acc": 44.0, "val_loss": 17.35903173685074, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.25242853164673, "training_acc": 54.0, "val_loss": 17.35062003135681, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.2146828174591, "training_acc": 53.0, "val_loss": 17.348404228687286, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.22861909866333, "training_acc": 53.0, "val_loss": 17.345021665096283, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.15459179878235, "training_acc": 53.0, "val_loss": 17.34359562397003, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.17178440093994, "training_acc": 53.0, "val_loss": 17.34323799610138, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.14577293395996, "training_acc": 53.0, "val_loss": 17.3432856798172, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.15632677078247, "training_acc": 53.0, "val_loss": 17.343631386756897, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.10288643836975, "training_acc": 53.0, "val_loss": 17.344501614570618, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.13306951522827, "training_acc": 53.0, "val_loss": 17.344871163368225, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.12529754638672, "training_acc": 53.0, "val_loss": 17.34616458415985, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.11171293258667, "training_acc": 53.0, "val_loss": 17.347827553749084, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.115225315094, "training_acc": 53.0, "val_loss": 17.347778379917145, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.12600517272949, "training_acc": 53.0, "val_loss": 17.34842211008072, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.09042739868164, "training_acc": 53.0, "val_loss": 17.35086441040039, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.14571452140808, "training_acc": 53.0, "val_loss": 17.351889610290527, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.17438292503357, "training_acc": 53.0, "val_loss": 17.356692254543304, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.12175011634827, "training_acc": 53.0, "val_loss": 17.363610863685608, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.17226552963257, "training_acc": 53.0, "val_loss": 17.36978441476822, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.13236236572266, "training_acc": 53.0, "val_loss": 17.37622171640396, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.14335489273071, "training_acc": 53.0, "val_loss": 17.381982505321503, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.16144371032715, "training_acc": 53.0, "val_loss": 17.38491654396057, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.20028734207153, "training_acc": 53.0, "val_loss": 17.39068776369095, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.19679164886475, "training_acc": 53.0, "val_loss": 17.396654188632965, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.18583488464355, "training_acc": 53.0, "val_loss": 17.396289110183716, "val_acc": 52.0}
