"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 72.17502355575562, "training_acc": 46.0, "val_loss": 18.300874531269073, "val_acc": 56.0}
{"epoch": 1, "training_loss": 71.57872295379639, "training_acc": 48.0, "val_loss": 18.290264904499054, "val_acc": 56.0}
{"epoch": 2, "training_loss": 71.50627708435059, "training_acc": 47.0, "val_loss": 18.270868062973022, "val_acc": 56.0}
{"epoch": 3, "training_loss": 71.17387056350708, "training_acc": 52.0, "val_loss": 18.2519793510437, "val_acc": 56.0}
{"epoch": 4, "training_loss": 70.77381181716919, "training_acc": 52.0, "val_loss": 18.22720319032669, "val_acc": 52.0}
{"epoch": 5, "training_loss": 70.48343324661255, "training_acc": 53.0, "val_loss": 18.21099817752838, "val_acc": 52.0}
{"epoch": 6, "training_loss": 70.12778234481812, "training_acc": 54.0, "val_loss": 18.20485144853592, "val_acc": 52.0}
{"epoch": 7, "training_loss": 70.20547246932983, "training_acc": 55.0, "val_loss": 18.191474676132202, "val_acc": 52.0}
{"epoch": 8, "training_loss": 70.14433312416077, "training_acc": 52.0, "val_loss": 18.177136778831482, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.95125246047974, "training_acc": 54.0, "val_loss": 18.17687898874283, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.38095140457153, "training_acc": 55.0, "val_loss": 18.179316818714142, "val_acc": 52.0}
{"epoch": 11, "training_loss": 68.97579002380371, "training_acc": 56.0, "val_loss": 18.183207511901855, "val_acc": 52.0}
{"epoch": 12, "training_loss": 68.48650193214417, "training_acc": 56.0, "val_loss": 18.189068138599396, "val_acc": 52.0}
{"epoch": 13, "training_loss": 68.72300910949707, "training_acc": 54.0, "val_loss": 18.198104202747345, "val_acc": 52.0}
{"epoch": 14, "training_loss": 68.67520213127136, "training_acc": 54.0, "val_loss": 18.206851184368134, "val_acc": 52.0}
{"epoch": 15, "training_loss": 68.24331521987915, "training_acc": 57.0, "val_loss": 18.21770817041397, "val_acc": 52.0}
{"epoch": 16, "training_loss": 67.8717885017395, "training_acc": 58.0, "val_loss": 18.229572474956512, "val_acc": 52.0}
{"epoch": 17, "training_loss": 68.16656923294067, "training_acc": 58.0, "val_loss": 18.247953057289124, "val_acc": 52.0}
{"epoch": 18, "training_loss": 67.36435079574585, "training_acc": 64.0, "val_loss": 18.268831074237823, "val_acc": 52.0}
{"epoch": 19, "training_loss": 67.4989607334137, "training_acc": 65.0, "val_loss": 18.28157901763916, "val_acc": 52.0}
{"epoch": 20, "training_loss": 67.41318988800049, "training_acc": 66.0, "val_loss": 18.305249512195587, "val_acc": 52.0}
{"epoch": 21, "training_loss": 67.27429389953613, "training_acc": 69.0, "val_loss": 18.326336145401, "val_acc": 52.0}
{"epoch": 22, "training_loss": 66.90700817108154, "training_acc": 68.0, "val_loss": 18.343748152256012, "val_acc": 52.0}
{"epoch": 23, "training_loss": 66.55372405052185, "training_acc": 69.0, "val_loss": 18.363209068775177, "val_acc": 52.0}
{"epoch": 24, "training_loss": 66.85315895080566, "training_acc": 66.0, "val_loss": 18.375106155872345, "val_acc": 52.0}
{"epoch": 25, "training_loss": 66.6570155620575, "training_acc": 66.0, "val_loss": 18.373851478099823, "val_acc": 52.0}
{"epoch": 26, "training_loss": 66.47510051727295, "training_acc": 66.0, "val_loss": 18.372099101543427, "val_acc": 52.0}
{"epoch": 27, "training_loss": 66.19205284118652, "training_acc": 67.0, "val_loss": 18.376846611499786, "val_acc": 52.0}
{"epoch": 28, "training_loss": 65.74642515182495, "training_acc": 67.0, "val_loss": 18.37228685617447, "val_acc": 52.0}
