"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 71.49947047233582, "training_acc": 44.0, "val_loss": 17.711131274700165, "val_acc": 56.0}
{"epoch": 1, "training_loss": 70.70024871826172, "training_acc": 48.0, "val_loss": 17.670291662216187, "val_acc": 56.0}
{"epoch": 2, "training_loss": 70.50164651870728, "training_acc": 48.0, "val_loss": 17.644180357456207, "val_acc": 56.0}
{"epoch": 3, "training_loss": 70.46137642860413, "training_acc": 48.0, "val_loss": 17.625509202480316, "val_acc": 56.0}
{"epoch": 4, "training_loss": 69.97790622711182, "training_acc": 49.0, "val_loss": 17.61869341135025, "val_acc": 56.0}
{"epoch": 5, "training_loss": 69.67890739440918, "training_acc": 50.0, "val_loss": 17.611171305179596, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.90639591217041, "training_acc": 52.0, "val_loss": 17.605899274349213, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.16865658760071, "training_acc": 54.0, "val_loss": 17.595025897026062, "val_acc": 56.0}
{"epoch": 8, "training_loss": 69.04139232635498, "training_acc": 53.0, "val_loss": 17.58824735879898, "val_acc": 56.0}
{"epoch": 9, "training_loss": 68.98509883880615, "training_acc": 53.0, "val_loss": 17.584748566150665, "val_acc": 56.0}
{"epoch": 10, "training_loss": 68.48432970046997, "training_acc": 61.0, "val_loss": 17.570987343788147, "val_acc": 56.0}
{"epoch": 11, "training_loss": 68.59587574005127, "training_acc": 57.0, "val_loss": 17.562487721443176, "val_acc": 56.0}
{"epoch": 12, "training_loss": 68.3638505935669, "training_acc": 56.0, "val_loss": 17.55853295326233, "val_acc": 56.0}
{"epoch": 13, "training_loss": 67.98296356201172, "training_acc": 57.0, "val_loss": 17.557956278324127, "val_acc": 56.0}
{"epoch": 14, "training_loss": 67.78906083106995, "training_acc": 58.0, "val_loss": 17.555972933769226, "val_acc": 56.0}
{"epoch": 15, "training_loss": 67.47583436965942, "training_acc": 60.0, "val_loss": 17.54976660013199, "val_acc": 56.0}
{"epoch": 16, "training_loss": 67.53581261634827, "training_acc": 60.0, "val_loss": 17.540083825588226, "val_acc": 56.0}
{"epoch": 17, "training_loss": 67.03747415542603, "training_acc": 59.0, "val_loss": 17.53094792366028, "val_acc": 56.0}
{"epoch": 18, "training_loss": 66.73749685287476, "training_acc": 60.0, "val_loss": 17.530199885368347, "val_acc": 56.0}
{"epoch": 19, "training_loss": 66.91984987258911, "training_acc": 61.0, "val_loss": 17.53746122121811, "val_acc": 56.0}
{"epoch": 20, "training_loss": 66.41122961044312, "training_acc": 66.0, "val_loss": 17.55194067955017, "val_acc": 56.0}
{"epoch": 21, "training_loss": 66.26172733306885, "training_acc": 64.0, "val_loss": 17.569944262504578, "val_acc": 56.0}
{"epoch": 22, "training_loss": 66.06288194656372, "training_acc": 68.0, "val_loss": 17.58427321910858, "val_acc": 56.0}
{"epoch": 23, "training_loss": 66.00822925567627, "training_acc": 66.0, "val_loss": 17.594899237155914, "val_acc": 56.0}
{"epoch": 24, "training_loss": 65.95626974105835, "training_acc": 67.0, "val_loss": 17.60295182466507, "val_acc": 56.0}
{"epoch": 25, "training_loss": 65.6032063961029, "training_acc": 68.0, "val_loss": 17.602908611297607, "val_acc": 56.0}
{"epoch": 26, "training_loss": 65.1156735420227, "training_acc": 70.0, "val_loss": 17.607581615447998, "val_acc": 56.0}
{"epoch": 27, "training_loss": 64.94593000411987, "training_acc": 69.0, "val_loss": 17.61406511068344, "val_acc": 56.0}
{"epoch": 28, "training_loss": 65.01489758491516, "training_acc": 70.0, "val_loss": 17.617900669574738, "val_acc": 52.0}
{"epoch": 29, "training_loss": 65.03780555725098, "training_acc": 72.0, "val_loss": 17.628268897533417, "val_acc": 48.0}
{"epoch": 30, "training_loss": 64.85669040679932, "training_acc": 71.0, "val_loss": 17.6457479596138, "val_acc": 48.0}
{"epoch": 31, "training_loss": 64.482834815979, "training_acc": 70.0, "val_loss": 17.656882107257843, "val_acc": 48.0}
{"epoch": 32, "training_loss": 64.47512722015381, "training_acc": 71.0, "val_loss": 17.662647366523743, "val_acc": 48.0}
{"epoch": 33, "training_loss": 64.69992637634277, "training_acc": 72.0, "val_loss": 17.662903666496277, "val_acc": 48.0}
{"epoch": 34, "training_loss": 64.3690779209137, "training_acc": 69.0, "val_loss": 17.676034569740295, "val_acc": 48.0}
{"epoch": 35, "training_loss": 64.35686731338501, "training_acc": 73.0, "val_loss": 17.694325745105743, "val_acc": 48.0}
{"epoch": 36, "training_loss": 63.91871356964111, "training_acc": 73.0, "val_loss": 17.704202234745026, "val_acc": 48.0}
{"epoch": 37, "training_loss": 63.932777404785156, "training_acc": 71.0, "val_loss": 17.710287868976593, "val_acc": 48.0}
