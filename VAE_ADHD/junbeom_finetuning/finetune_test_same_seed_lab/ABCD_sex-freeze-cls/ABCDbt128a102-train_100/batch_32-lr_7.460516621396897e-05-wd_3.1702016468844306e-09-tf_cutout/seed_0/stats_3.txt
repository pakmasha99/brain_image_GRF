"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 71.54477071762085, "training_acc": 46.0, "val_loss": 18.91743689775467, "val_acc": 56.0}
{"epoch": 1, "training_loss": 71.5692195892334, "training_acc": 48.0, "val_loss": 18.85800063610077, "val_acc": 56.0}
{"epoch": 2, "training_loss": 70.67634391784668, "training_acc": 52.0, "val_loss": 18.824127316474915, "val_acc": 56.0}
{"epoch": 3, "training_loss": 70.65038585662842, "training_acc": 47.0, "val_loss": 18.795356154441833, "val_acc": 56.0}
{"epoch": 4, "training_loss": 70.33870697021484, "training_acc": 47.0, "val_loss": 18.790026009082794, "val_acc": 56.0}
{"epoch": 5, "training_loss": 70.49781703948975, "training_acc": 50.0, "val_loss": 18.78390610218048, "val_acc": 56.0}
{"epoch": 6, "training_loss": 70.04724550247192, "training_acc": 50.0, "val_loss": 18.776774406433105, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.37587881088257, "training_acc": 53.0, "val_loss": 18.750247359275818, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.68011856079102, "training_acc": 51.0, "val_loss": 18.71410310268402, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.28362107276917, "training_acc": 54.0, "val_loss": 18.686644732952118, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.25865578651428, "training_acc": 53.0, "val_loss": 18.663284182548523, "val_acc": 52.0}
{"epoch": 11, "training_loss": 68.98678159713745, "training_acc": 56.0, "val_loss": 18.637311458587646, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.04743194580078, "training_acc": 56.0, "val_loss": 18.618083000183105, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.0346188545227, "training_acc": 55.0, "val_loss": 18.607211112976074, "val_acc": 52.0}
{"epoch": 14, "training_loss": 68.6244580745697, "training_acc": 54.0, "val_loss": 18.591666221618652, "val_acc": 52.0}
{"epoch": 15, "training_loss": 68.53435754776001, "training_acc": 57.0, "val_loss": 18.579398095607758, "val_acc": 52.0}
{"epoch": 16, "training_loss": 68.69265484809875, "training_acc": 50.0, "val_loss": 18.562401831150055, "val_acc": 48.0}
{"epoch": 17, "training_loss": 68.38238716125488, "training_acc": 54.0, "val_loss": 18.547897040843964, "val_acc": 48.0}
{"epoch": 18, "training_loss": 68.37003374099731, "training_acc": 55.0, "val_loss": 18.532724678516388, "val_acc": 52.0}
{"epoch": 19, "training_loss": 68.16707420349121, "training_acc": 56.0, "val_loss": 18.520183861255646, "val_acc": 52.0}
{"epoch": 20, "training_loss": 67.94084358215332, "training_acc": 58.0, "val_loss": 18.50491315126419, "val_acc": 52.0}
{"epoch": 21, "training_loss": 67.72644901275635, "training_acc": 58.0, "val_loss": 18.4812992811203, "val_acc": 48.0}
{"epoch": 22, "training_loss": 67.8124327659607, "training_acc": 58.0, "val_loss": 18.45676898956299, "val_acc": 48.0}
{"epoch": 23, "training_loss": 67.94200325012207, "training_acc": 56.0, "val_loss": 18.425357341766357, "val_acc": 48.0}
{"epoch": 24, "training_loss": 67.36653518676758, "training_acc": 59.0, "val_loss": 18.40057522058487, "val_acc": 48.0}
{"epoch": 25, "training_loss": 67.28801226615906, "training_acc": 58.0, "val_loss": 18.386496603488922, "val_acc": 48.0}
{"epoch": 26, "training_loss": 67.40500211715698, "training_acc": 59.0, "val_loss": 18.370772898197174, "val_acc": 48.0}
{"epoch": 27, "training_loss": 67.3786518573761, "training_acc": 57.0, "val_loss": 18.367549777030945, "val_acc": 48.0}
{"epoch": 28, "training_loss": 67.04462718963623, "training_acc": 61.0, "val_loss": 18.36441308259964, "val_acc": 48.0}
{"epoch": 29, "training_loss": 67.44321775436401, "training_acc": 58.0, "val_loss": 18.370869755744934, "val_acc": 48.0}
{"epoch": 30, "training_loss": 66.48968696594238, "training_acc": 58.0, "val_loss": 18.378204107284546, "val_acc": 48.0}
{"epoch": 31, "training_loss": 66.59639120101929, "training_acc": 59.0, "val_loss": 18.38516592979431, "val_acc": 48.0}
{"epoch": 32, "training_loss": 66.32341575622559, "training_acc": 60.0, "val_loss": 18.379703164100647, "val_acc": 48.0}
{"epoch": 33, "training_loss": 66.8152539730072, "training_acc": 60.0, "val_loss": 18.363472819328308, "val_acc": 48.0}
{"epoch": 34, "training_loss": 66.3458240032196, "training_acc": 61.0, "val_loss": 18.34496706724167, "val_acc": 48.0}
{"epoch": 35, "training_loss": 66.24970269203186, "training_acc": 59.0, "val_loss": 18.32032948732376, "val_acc": 48.0}
{"epoch": 36, "training_loss": 66.67344427108765, "training_acc": 62.0, "val_loss": 18.30134093761444, "val_acc": 48.0}
{"epoch": 37, "training_loss": 65.54860186576843, "training_acc": 61.0, "val_loss": 18.284083902835846, "val_acc": 48.0}
{"epoch": 38, "training_loss": 66.08101010322571, "training_acc": 63.0, "val_loss": 18.27337294816971, "val_acc": 48.0}
{"epoch": 39, "training_loss": 66.02524757385254, "training_acc": 62.0, "val_loss": 18.25583279132843, "val_acc": 48.0}
{"epoch": 40, "training_loss": 65.85148191452026, "training_acc": 63.0, "val_loss": 18.22759360074997, "val_acc": 48.0}
{"epoch": 41, "training_loss": 65.96902251243591, "training_acc": 64.0, "val_loss": 18.21354478597641, "val_acc": 48.0}
{"epoch": 42, "training_loss": 65.66347622871399, "training_acc": 63.0, "val_loss": 18.19899082183838, "val_acc": 48.0}
{"epoch": 43, "training_loss": 66.00111079216003, "training_acc": 59.0, "val_loss": 18.19368451833725, "val_acc": 52.0}
{"epoch": 44, "training_loss": 65.59530305862427, "training_acc": 63.0, "val_loss": 18.179933726787567, "val_acc": 52.0}
{"epoch": 45, "training_loss": 65.29159498214722, "training_acc": 64.0, "val_loss": 18.171638250350952, "val_acc": 52.0}
{"epoch": 46, "training_loss": 65.23489570617676, "training_acc": 65.0, "val_loss": 18.17258447408676, "val_acc": 52.0}
{"epoch": 47, "training_loss": 65.16626524925232, "training_acc": 63.0, "val_loss": 18.17580759525299, "val_acc": 52.0}
{"epoch": 48, "training_loss": 65.93197846412659, "training_acc": 62.0, "val_loss": 18.181373178958893, "val_acc": 52.0}
{"epoch": 49, "training_loss": 65.16452550888062, "training_acc": 64.0, "val_loss": 18.18430721759796, "val_acc": 56.0}
{"epoch": 50, "training_loss": 64.77689146995544, "training_acc": 64.0, "val_loss": 18.18532645702362, "val_acc": 56.0}
{"epoch": 51, "training_loss": 65.45503497123718, "training_acc": 62.0, "val_loss": 18.19142997264862, "val_acc": 56.0}
{"epoch": 52, "training_loss": 65.06280899047852, "training_acc": 64.0, "val_loss": 18.197688460350037, "val_acc": 56.0}
{"epoch": 53, "training_loss": 64.85552549362183, "training_acc": 64.0, "val_loss": 18.19707900285721, "val_acc": 56.0}
{"epoch": 54, "training_loss": 64.72726202011108, "training_acc": 65.0, "val_loss": 18.193770945072174, "val_acc": 56.0}
{"epoch": 55, "training_loss": 65.01517581939697, "training_acc": 65.0, "val_loss": 18.19090098142624, "val_acc": 56.0}
{"epoch": 56, "training_loss": 64.61659264564514, "training_acc": 63.0, "val_loss": 18.18087100982666, "val_acc": 56.0}
{"epoch": 57, "training_loss": 64.67883110046387, "training_acc": 65.0, "val_loss": 18.171675503253937, "val_acc": 56.0}
{"epoch": 58, "training_loss": 64.37912893295288, "training_acc": 64.0, "val_loss": 18.174627423286438, "val_acc": 52.0}
{"epoch": 59, "training_loss": 64.631760597229, "training_acc": 65.0, "val_loss": 18.186725676059723, "val_acc": 52.0}
{"epoch": 60, "training_loss": 64.82023239135742, "training_acc": 64.0, "val_loss": 18.19031536579132, "val_acc": 52.0}
{"epoch": 61, "training_loss": 64.58458852767944, "training_acc": 64.0, "val_loss": 18.19234937429428, "val_acc": 52.0}
{"epoch": 62, "training_loss": 64.78006100654602, "training_acc": 62.0, "val_loss": 18.179187178611755, "val_acc": 52.0}
{"epoch": 63, "training_loss": 64.41162705421448, "training_acc": 63.0, "val_loss": 18.170085549354553, "val_acc": 52.0}
{"epoch": 64, "training_loss": 64.41689825057983, "training_acc": 64.0, "val_loss": 18.16498339176178, "val_acc": 52.0}
{"epoch": 65, "training_loss": 64.26535415649414, "training_acc": 63.0, "val_loss": 18.164581060409546, "val_acc": 52.0}
{"epoch": 66, "training_loss": 63.94978213310242, "training_acc": 63.0, "val_loss": 18.16149652004242, "val_acc": 52.0}
{"epoch": 67, "training_loss": 63.46920728683472, "training_acc": 63.0, "val_loss": 18.15256029367447, "val_acc": 52.0}
{"epoch": 68, "training_loss": 63.72434449195862, "training_acc": 66.0, "val_loss": 18.143101036548615, "val_acc": 52.0}
{"epoch": 69, "training_loss": 63.16691493988037, "training_acc": 63.0, "val_loss": 18.13805401325226, "val_acc": 52.0}
{"epoch": 70, "training_loss": 63.940324544906616, "training_acc": 64.0, "val_loss": 18.137048184871674, "val_acc": 52.0}
{"epoch": 71, "training_loss": 63.78233289718628, "training_acc": 63.0, "val_loss": 18.13773363828659, "val_acc": 52.0}
{"epoch": 72, "training_loss": 63.44248819351196, "training_acc": 64.0, "val_loss": 18.138091266155243, "val_acc": 48.0}
{"epoch": 73, "training_loss": 63.77360677719116, "training_acc": 64.0, "val_loss": 18.130530416965485, "val_acc": 48.0}
{"epoch": 74, "training_loss": 63.35841107368469, "training_acc": 65.0, "val_loss": 18.131613731384277, "val_acc": 48.0}
{"epoch": 75, "training_loss": 63.463552474975586, "training_acc": 65.0, "val_loss": 18.1355819106102, "val_acc": 48.0}
{"epoch": 76, "training_loss": 63.24595785140991, "training_acc": 66.0, "val_loss": 18.131469190120697, "val_acc": 48.0}
{"epoch": 77, "training_loss": 63.66711640357971, "training_acc": 63.0, "val_loss": 18.116603791713715, "val_acc": 48.0}
{"epoch": 78, "training_loss": 63.45155096054077, "training_acc": 62.0, "val_loss": 18.0994912981987, "val_acc": 48.0}
{"epoch": 79, "training_loss": 62.88781213760376, "training_acc": 64.0, "val_loss": 18.08849275112152, "val_acc": 48.0}
{"epoch": 80, "training_loss": 63.42741537094116, "training_acc": 65.0, "val_loss": 18.087176978588104, "val_acc": 48.0}
{"epoch": 81, "training_loss": 63.086031436920166, "training_acc": 63.0, "val_loss": 18.07945966720581, "val_acc": 48.0}
{"epoch": 82, "training_loss": 62.94687056541443, "training_acc": 63.0, "val_loss": 18.078677356243134, "val_acc": 48.0}
{"epoch": 83, "training_loss": 63.2062406539917, "training_acc": 65.0, "val_loss": 18.077945709228516, "val_acc": 48.0}
{"epoch": 84, "training_loss": 62.70671820640564, "training_acc": 63.0, "val_loss": 18.08498352766037, "val_acc": 48.0}
{"epoch": 85, "training_loss": 62.88982820510864, "training_acc": 64.0, "val_loss": 18.091365694999695, "val_acc": 48.0}
{"epoch": 86, "training_loss": 63.29230356216431, "training_acc": 62.0, "val_loss": 18.101027607917786, "val_acc": 48.0}
{"epoch": 87, "training_loss": 62.779179096221924, "training_acc": 65.0, "val_loss": 18.11363250017166, "val_acc": 48.0}
{"epoch": 88, "training_loss": 62.77780818939209, "training_acc": 65.0, "val_loss": 18.11669170856476, "val_acc": 48.0}
{"epoch": 89, "training_loss": 62.782727003097534, "training_acc": 65.0, "val_loss": 18.11092346906662, "val_acc": 48.0}
{"epoch": 90, "training_loss": 62.79922389984131, "training_acc": 65.0, "val_loss": 18.10190975666046, "val_acc": 48.0}
{"epoch": 91, "training_loss": 63.02690291404724, "training_acc": 65.0, "val_loss": 18.094603717327118, "val_acc": 48.0}
{"epoch": 92, "training_loss": 62.62275266647339, "training_acc": 66.0, "val_loss": 18.09406578540802, "val_acc": 52.0}
{"epoch": 93, "training_loss": 62.94194483757019, "training_acc": 69.0, "val_loss": 18.106994032859802, "val_acc": 52.0}
{"epoch": 94, "training_loss": 62.2749445438385, "training_acc": 70.0, "val_loss": 18.106168508529663, "val_acc": 52.0}
{"epoch": 95, "training_loss": 62.00457501411438, "training_acc": 72.0, "val_loss": 18.10869127511978, "val_acc": 52.0}
{"epoch": 96, "training_loss": 62.69630217552185, "training_acc": 68.0, "val_loss": 18.108659982681274, "val_acc": 52.0}
{"epoch": 97, "training_loss": 62.09190845489502, "training_acc": 67.0, "val_loss": 18.10242235660553, "val_acc": 52.0}
{"epoch": 98, "training_loss": 62.84498167037964, "training_acc": 67.0, "val_loss": 18.10108870267868, "val_acc": 52.0}
{"epoch": 99, "training_loss": 62.278258323669434, "training_acc": 69.0, "val_loss": 18.10118407011032, "val_acc": 52.0}
{"epoch": 100, "training_loss": 62.32019758224487, "training_acc": 71.0, "val_loss": 18.10072660446167, "val_acc": 52.0}
{"epoch": 101, "training_loss": 61.827513217926025, "training_acc": 70.0, "val_loss": 18.100182712078094, "val_acc": 52.0}
{"epoch": 102, "training_loss": 62.60708427429199, "training_acc": 69.0, "val_loss": 18.09834986925125, "val_acc": 52.0}
