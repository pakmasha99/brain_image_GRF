"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 71.39831757545471, "training_acc": 52.0, "val_loss": 18.315035104751587, "val_acc": 56.0}
{"epoch": 1, "training_loss": 71.17429971694946, "training_acc": 51.0, "val_loss": 18.247100710868835, "val_acc": 56.0}
{"epoch": 2, "training_loss": 71.14892864227295, "training_acc": 51.0, "val_loss": 18.21819394826889, "val_acc": 56.0}
{"epoch": 3, "training_loss": 70.98967862129211, "training_acc": 52.0, "val_loss": 18.19075495004654, "val_acc": 56.0}
{"epoch": 4, "training_loss": 70.56656074523926, "training_acc": 49.0, "val_loss": 18.163394927978516, "val_acc": 56.0}
{"epoch": 5, "training_loss": 70.40100455284119, "training_acc": 51.0, "val_loss": 18.13676804304123, "val_acc": 56.0}
{"epoch": 6, "training_loss": 70.51161432266235, "training_acc": 50.0, "val_loss": 18.113261461257935, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.56302118301392, "training_acc": 54.0, "val_loss": 18.082305788993835, "val_acc": 56.0}
{"epoch": 8, "training_loss": 69.85804224014282, "training_acc": 51.0, "val_loss": 18.047714233398438, "val_acc": 56.0}
{"epoch": 9, "training_loss": 70.11482191085815, "training_acc": 50.0, "val_loss": 18.03419589996338, "val_acc": 56.0}
{"epoch": 10, "training_loss": 69.70100450515747, "training_acc": 50.0, "val_loss": 18.02222430706024, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.1274983882904, "training_acc": 50.0, "val_loss": 18.008525669574738, "val_acc": 56.0}
{"epoch": 12, "training_loss": 68.34162831306458, "training_acc": 56.0, "val_loss": 17.997901141643524, "val_acc": 56.0}
{"epoch": 13, "training_loss": 69.07235050201416, "training_acc": 55.0, "val_loss": 17.99459159374237, "val_acc": 56.0}
{"epoch": 14, "training_loss": 68.97996377944946, "training_acc": 56.0, "val_loss": 17.98819750547409, "val_acc": 56.0}
{"epoch": 15, "training_loss": 68.50650906562805, "training_acc": 55.0, "val_loss": 17.982321977615356, "val_acc": 56.0}
{"epoch": 16, "training_loss": 68.2993950843811, "training_acc": 57.0, "val_loss": 17.961667478084564, "val_acc": 56.0}
{"epoch": 17, "training_loss": 68.26413774490356, "training_acc": 56.0, "val_loss": 17.942412197589874, "val_acc": 56.0}
{"epoch": 18, "training_loss": 68.18060159683228, "training_acc": 55.0, "val_loss": 17.92493909597397, "val_acc": 56.0}
{"epoch": 19, "training_loss": 67.96708488464355, "training_acc": 56.0, "val_loss": 17.90454238653183, "val_acc": 56.0}
{"epoch": 20, "training_loss": 67.99981379508972, "training_acc": 56.0, "val_loss": 17.885929346084595, "val_acc": 56.0}
{"epoch": 21, "training_loss": 67.79297256469727, "training_acc": 52.0, "val_loss": 17.870795726776123, "val_acc": 56.0}
{"epoch": 22, "training_loss": 68.17484951019287, "training_acc": 55.0, "val_loss": 17.856545746326447, "val_acc": 56.0}
{"epoch": 23, "training_loss": 67.46144485473633, "training_acc": 57.0, "val_loss": 17.84808188676834, "val_acc": 56.0}
{"epoch": 24, "training_loss": 67.6072154045105, "training_acc": 55.0, "val_loss": 17.835058271884918, "val_acc": 56.0}
{"epoch": 25, "training_loss": 67.03429961204529, "training_acc": 59.0, "val_loss": 17.81429648399353, "val_acc": 56.0}
{"epoch": 26, "training_loss": 66.98477053642273, "training_acc": 55.0, "val_loss": 17.803657054901123, "val_acc": 56.0}
{"epoch": 27, "training_loss": 66.8852481842041, "training_acc": 55.0, "val_loss": 17.794807255268097, "val_acc": 56.0}
{"epoch": 28, "training_loss": 66.42757034301758, "training_acc": 56.0, "val_loss": 17.77823567390442, "val_acc": 56.0}
{"epoch": 29, "training_loss": 66.48086476325989, "training_acc": 57.0, "val_loss": 17.771606147289276, "val_acc": 56.0}
{"epoch": 30, "training_loss": 66.98839044570923, "training_acc": 54.0, "val_loss": 17.75204986333847, "val_acc": 56.0}
{"epoch": 31, "training_loss": 66.06092953681946, "training_acc": 60.0, "val_loss": 17.743661999702454, "val_acc": 56.0}
{"epoch": 32, "training_loss": 66.24378204345703, "training_acc": 58.0, "val_loss": 17.75326281785965, "val_acc": 56.0}
{"epoch": 33, "training_loss": 66.71709728240967, "training_acc": 55.0, "val_loss": 17.756305634975433, "val_acc": 56.0}
{"epoch": 34, "training_loss": 65.6025025844574, "training_acc": 60.0, "val_loss": 17.75994449853897, "val_acc": 56.0}
{"epoch": 35, "training_loss": 65.51872825622559, "training_acc": 58.0, "val_loss": 17.77370721101761, "val_acc": 56.0}
{"epoch": 36, "training_loss": 65.51713752746582, "training_acc": 60.0, "val_loss": 17.785385251045227, "val_acc": 56.0}
{"epoch": 37, "training_loss": 65.82321119308472, "training_acc": 61.0, "val_loss": 17.793941497802734, "val_acc": 56.0}
{"epoch": 38, "training_loss": 65.6329574584961, "training_acc": 63.0, "val_loss": 17.804181575775146, "val_acc": 56.0}
{"epoch": 39, "training_loss": 65.34201526641846, "training_acc": 64.0, "val_loss": 17.804792523384094, "val_acc": 52.0}
{"epoch": 40, "training_loss": 64.91518640518188, "training_acc": 60.0, "val_loss": 17.797306180000305, "val_acc": 52.0}
{"epoch": 41, "training_loss": 65.01495170593262, "training_acc": 61.0, "val_loss": 17.792540788650513, "val_acc": 52.0}
{"epoch": 42, "training_loss": 65.25753831863403, "training_acc": 61.0, "val_loss": 17.79504120349884, "val_acc": 52.0}
{"epoch": 43, "training_loss": 64.98181486129761, "training_acc": 60.0, "val_loss": 17.793691158294678, "val_acc": 52.0}
{"epoch": 44, "training_loss": 65.36074733734131, "training_acc": 65.0, "val_loss": 17.793963849544525, "val_acc": 52.0}
{"epoch": 45, "training_loss": 64.81464195251465, "training_acc": 64.0, "val_loss": 17.787133157253265, "val_acc": 52.0}
{"epoch": 46, "training_loss": 64.29421067237854, "training_acc": 68.0, "val_loss": 17.785903811454773, "val_acc": 52.0}
{"epoch": 47, "training_loss": 65.0565094947815, "training_acc": 64.0, "val_loss": 17.793281376361847, "val_acc": 52.0}
{"epoch": 48, "training_loss": 64.45015740394592, "training_acc": 63.0, "val_loss": 17.80383437871933, "val_acc": 52.0}
{"epoch": 49, "training_loss": 64.55118441581726, "training_acc": 67.0, "val_loss": 17.810645699501038, "val_acc": 52.0}
{"epoch": 50, "training_loss": 64.206223487854, "training_acc": 66.0, "val_loss": 17.81199723482132, "val_acc": 52.0}
