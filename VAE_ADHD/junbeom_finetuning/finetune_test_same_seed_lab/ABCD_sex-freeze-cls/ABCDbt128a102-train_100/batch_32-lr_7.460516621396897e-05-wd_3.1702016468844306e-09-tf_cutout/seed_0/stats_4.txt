"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 72.67126441001892, "training_acc": 49.0, "val_loss": 17.919889092445374, "val_acc": 48.0}
{"epoch": 1, "training_loss": 72.79532074928284, "training_acc": 47.0, "val_loss": 17.902493476867676, "val_acc": 52.0}
{"epoch": 2, "training_loss": 72.27325201034546, "training_acc": 49.0, "val_loss": 17.89599061012268, "val_acc": 52.0}
{"epoch": 3, "training_loss": 71.70286798477173, "training_acc": 51.0, "val_loss": 17.89351999759674, "val_acc": 52.0}
{"epoch": 4, "training_loss": 71.67741346359253, "training_acc": 51.0, "val_loss": 17.891915142536163, "val_acc": 52.0}
{"epoch": 5, "training_loss": 71.11395859718323, "training_acc": 54.0, "val_loss": 17.88661479949951, "val_acc": 52.0}
{"epoch": 6, "training_loss": 71.46549105644226, "training_acc": 51.0, "val_loss": 17.88284480571747, "val_acc": 56.0}
{"epoch": 7, "training_loss": 71.06818795204163, "training_acc": 49.0, "val_loss": 17.88245588541031, "val_acc": 56.0}
{"epoch": 8, "training_loss": 70.78990507125854, "training_acc": 49.0, "val_loss": 17.889396846294403, "val_acc": 56.0}
{"epoch": 9, "training_loss": 70.75471258163452, "training_acc": 53.0, "val_loss": 17.89700984954834, "val_acc": 56.0}
{"epoch": 10, "training_loss": 70.91985511779785, "training_acc": 51.0, "val_loss": 17.90175437927246, "val_acc": 56.0}
{"epoch": 11, "training_loss": 70.27413892745972, "training_acc": 52.0, "val_loss": 17.908023297786713, "val_acc": 56.0}
{"epoch": 12, "training_loss": 70.4119143486023, "training_acc": 53.0, "val_loss": 17.91956275701523, "val_acc": 56.0}
{"epoch": 13, "training_loss": 69.57476210594177, "training_acc": 53.0, "val_loss": 17.932772636413574, "val_acc": 52.0}
{"epoch": 14, "training_loss": 70.01597690582275, "training_acc": 50.0, "val_loss": 17.945489287376404, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.66796159744263, "training_acc": 52.0, "val_loss": 17.9660826921463, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.68715214729309, "training_acc": 51.0, "val_loss": 17.983900010585785, "val_acc": 52.0}
{"epoch": 17, "training_loss": 68.78437995910645, "training_acc": 53.0, "val_loss": 17.99393743276596, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.30698370933533, "training_acc": 51.0, "val_loss": 18.00501197576523, "val_acc": 52.0}
{"epoch": 19, "training_loss": 68.83808159828186, "training_acc": 52.0, "val_loss": 18.01758110523224, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.5702211856842, "training_acc": 50.0, "val_loss": 18.03077459335327, "val_acc": 52.0}
{"epoch": 21, "training_loss": 68.69845747947693, "training_acc": 54.0, "val_loss": 18.05020421743393, "val_acc": 52.0}
{"epoch": 22, "training_loss": 68.75635027885437, "training_acc": 52.0, "val_loss": 18.069472908973694, "val_acc": 52.0}
{"epoch": 23, "training_loss": 68.62256050109863, "training_acc": 55.0, "val_loss": 18.082714080810547, "val_acc": 52.0}
{"epoch": 24, "training_loss": 68.97083950042725, "training_acc": 54.0, "val_loss": 18.087033927440643, "val_acc": 48.0}
{"epoch": 25, "training_loss": 68.65801858901978, "training_acc": 54.0, "val_loss": 18.091578781604767, "val_acc": 48.0}
{"epoch": 26, "training_loss": 68.95559167861938, "training_acc": 55.0, "val_loss": 18.09917390346527, "val_acc": 48.0}
