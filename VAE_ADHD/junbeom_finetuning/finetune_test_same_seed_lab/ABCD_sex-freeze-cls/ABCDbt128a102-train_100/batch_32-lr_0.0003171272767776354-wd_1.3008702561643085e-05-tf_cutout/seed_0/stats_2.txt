"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.55732107162476, "training_acc": 53.0, "val_loss": 17.407764494419098, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.45478868484497, "training_acc": 53.0, "val_loss": 17.462310194969177, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.29930400848389, "training_acc": 53.0, "val_loss": 17.367498576641083, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.4928719997406, "training_acc": 54.0, "val_loss": 17.392587661743164, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.79397869110107, "training_acc": 47.0, "val_loss": 17.519433796405792, "val_acc": 52.0}
{"epoch": 5, "training_loss": 70.28008365631104, "training_acc": 47.0, "val_loss": 17.48950332403183, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.84058809280396, "training_acc": 46.0, "val_loss": 17.363204061985016, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.38159656524658, "training_acc": 54.0, "val_loss": 17.35260784626007, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.4838547706604, "training_acc": 48.0, "val_loss": 17.369069159030914, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.22367024421692, "training_acc": 50.0, "val_loss": 17.350634932518005, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.09443068504333, "training_acc": 53.0, "val_loss": 17.363548278808594, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.23459124565125, "training_acc": 53.0, "val_loss": 17.376191914081573, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.21661615371704, "training_acc": 53.0, "val_loss": 17.3553466796875, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.14403986930847, "training_acc": 53.0, "val_loss": 17.353256046772003, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.1187813282013, "training_acc": 53.0, "val_loss": 17.350459098815918, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.27125692367554, "training_acc": 53.0, "val_loss": 17.353440821170807, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.13775563240051, "training_acc": 53.0, "val_loss": 17.34980344772339, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.16372108459473, "training_acc": 53.0, "val_loss": 17.34999418258667, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.12597942352295, "training_acc": 53.0, "val_loss": 17.349886894226074, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.1918363571167, "training_acc": 53.0, "val_loss": 17.364850640296936, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.10394716262817, "training_acc": 53.0, "val_loss": 17.3736572265625, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.12419605255127, "training_acc": 53.0, "val_loss": 17.375965416431427, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.14910578727722, "training_acc": 53.0, "val_loss": 17.400789260864258, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.30321764945984, "training_acc": 53.0, "val_loss": 17.40424484014511, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.13431739807129, "training_acc": 53.0, "val_loss": 17.464716732501984, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.41940116882324, "training_acc": 53.0, "val_loss": 17.473118007183075, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.35287189483643, "training_acc": 53.0, "val_loss": 17.410175502300262, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.20424556732178, "training_acc": 53.0, "val_loss": 17.34936833381653, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.13219666481018, "training_acc": 53.0, "val_loss": 17.36057996749878, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.25001573562622, "training_acc": 50.0, "val_loss": 17.354556918144226, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.12663698196411, "training_acc": 53.0, "val_loss": 17.349286377429962, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.17864108085632, "training_acc": 53.0, "val_loss": 17.349351942539215, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.3751106262207, "training_acc": 51.0, "val_loss": 17.373207211494446, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.26509094238281, "training_acc": 50.0, "val_loss": 17.35578626394272, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.11540126800537, "training_acc": 53.0, "val_loss": 17.349033057689667, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.11760759353638, "training_acc": 53.0, "val_loss": 17.376722395420074, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.1461763381958, "training_acc": 53.0, "val_loss": 17.415478825569153, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.1976432800293, "training_acc": 53.0, "val_loss": 17.447181046009064, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.3036527633667, "training_acc": 53.0, "val_loss": 17.419330775737762, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.14497900009155, "training_acc": 53.0, "val_loss": 17.34869033098221, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.22565245628357, "training_acc": 47.0, "val_loss": 17.372913658618927, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.25380802154541, "training_acc": 50.0, "val_loss": 17.349223792552948, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.1992392539978, "training_acc": 53.0, "val_loss": 17.35270470380783, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.1192512512207, "training_acc": 53.0, "val_loss": 17.361722886562347, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.11737060546875, "training_acc": 53.0, "val_loss": 17.357023060321808, "val_acc": 52.0}
{"epoch": 45, "training_loss": 69.0203685760498, "training_acc": 53.0, "val_loss": 17.36043095588684, "val_acc": 52.0}
{"epoch": 46, "training_loss": 69.0760064125061, "training_acc": 53.0, "val_loss": 17.372700572013855, "val_acc": 52.0}
{"epoch": 47, "training_loss": 69.08794355392456, "training_acc": 53.0, "val_loss": 17.357657849788666, "val_acc": 52.0}
{"epoch": 48, "training_loss": 69.07481288909912, "training_acc": 53.0, "val_loss": 17.368176579475403, "val_acc": 52.0}
{"epoch": 49, "training_loss": 69.08569145202637, "training_acc": 53.0, "val_loss": 17.38283932209015, "val_acc": 52.0}
{"epoch": 50, "training_loss": 69.0704870223999, "training_acc": 53.0, "val_loss": 17.355217039585114, "val_acc": 52.0}
{"epoch": 51, "training_loss": 69.01656985282898, "training_acc": 53.0, "val_loss": 17.352069914340973, "val_acc": 52.0}
{"epoch": 52, "training_loss": 69.05625057220459, "training_acc": 53.0, "val_loss": 17.379210889339447, "val_acc": 52.0}
{"epoch": 53, "training_loss": 69.1349835395813, "training_acc": 53.0, "val_loss": 17.38070249557495, "val_acc": 52.0}
{"epoch": 54, "training_loss": 69.19183564186096, "training_acc": 53.0, "val_loss": 17.48594045639038, "val_acc": 52.0}
{"epoch": 55, "training_loss": 69.50919508934021, "training_acc": 53.0, "val_loss": 17.65422821044922, "val_acc": 52.0}
{"epoch": 56, "training_loss": 70.14872789382935, "training_acc": 53.0, "val_loss": 17.682886123657227, "val_acc": 52.0}
{"epoch": 57, "training_loss": 69.99444532394409, "training_acc": 53.0, "val_loss": 17.63036698102951, "val_acc": 52.0}
{"epoch": 58, "training_loss": 69.7707748413086, "training_acc": 53.0, "val_loss": 17.53023862838745, "val_acc": 52.0}
