"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.42658948898315, "training_acc": 48.0, "val_loss": 17.17405468225479, "val_acc": 56.0}
{"epoch": 1, "training_loss": 69.46029853820801, "training_acc": 52.0, "val_loss": 17.170535027980804, "val_acc": 56.0}
{"epoch": 2, "training_loss": 69.46899151802063, "training_acc": 52.0, "val_loss": 17.22147762775421, "val_acc": 56.0}
{"epoch": 3, "training_loss": 69.31535053253174, "training_acc": 52.0, "val_loss": 17.163345217704773, "val_acc": 56.0}
{"epoch": 4, "training_loss": 69.32855415344238, "training_acc": 52.0, "val_loss": 17.216089367866516, "val_acc": 56.0}
{"epoch": 5, "training_loss": 69.25877332687378, "training_acc": 52.0, "val_loss": 17.181997001171112, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.22891068458557, "training_acc": 52.0, "val_loss": 17.186400294303894, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.2885205745697, "training_acc": 52.0, "val_loss": 17.21092164516449, "val_acc": 56.0}
{"epoch": 8, "training_loss": 69.29297089576721, "training_acc": 52.0, "val_loss": 17.203018069267273, "val_acc": 56.0}
{"epoch": 9, "training_loss": 69.34844017028809, "training_acc": 52.0, "val_loss": 17.288489639759064, "val_acc": 56.0}
{"epoch": 10, "training_loss": 69.37638759613037, "training_acc": 50.0, "val_loss": 17.48405396938324, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.58652591705322, "training_acc": 48.0, "val_loss": 17.3600971698761, "val_acc": 56.0}
{"epoch": 12, "training_loss": 69.61254501342773, "training_acc": 43.0, "val_loss": 17.156480252742767, "val_acc": 56.0}
{"epoch": 13, "training_loss": 69.17688989639282, "training_acc": 52.0, "val_loss": 17.12970733642578, "val_acc": 56.0}
{"epoch": 14, "training_loss": 69.63985776901245, "training_acc": 52.0, "val_loss": 17.142406105995178, "val_acc": 56.0}
{"epoch": 15, "training_loss": 69.83817911148071, "training_acc": 52.0, "val_loss": 17.142431437969208, "val_acc": 56.0}
{"epoch": 16, "training_loss": 70.07526016235352, "training_acc": 52.0, "val_loss": 17.15182065963745, "val_acc": 56.0}
{"epoch": 17, "training_loss": 69.74076533317566, "training_acc": 52.0, "val_loss": 17.136815190315247, "val_acc": 56.0}
{"epoch": 18, "training_loss": 69.12120819091797, "training_acc": 52.0, "val_loss": 17.29746162891388, "val_acc": 56.0}
{"epoch": 19, "training_loss": 69.42458534240723, "training_acc": 52.0, "val_loss": 17.560985684394836, "val_acc": 56.0}
{"epoch": 20, "training_loss": 69.80722236633301, "training_acc": 48.0, "val_loss": 17.54191368818283, "val_acc": 56.0}
{"epoch": 21, "training_loss": 69.6547646522522, "training_acc": 48.0, "val_loss": 17.415788769721985, "val_acc": 56.0}
{"epoch": 22, "training_loss": 69.57788348197937, "training_acc": 46.0, "val_loss": 17.287147045135498, "val_acc": 56.0}
{"epoch": 23, "training_loss": 69.2577383518219, "training_acc": 51.0, "val_loss": 17.231708765029907, "val_acc": 56.0}
{"epoch": 24, "training_loss": 69.44012117385864, "training_acc": 52.0, "val_loss": 17.145051062107086, "val_acc": 56.0}
{"epoch": 25, "training_loss": 69.29030275344849, "training_acc": 52.0, "val_loss": 17.151030898094177, "val_acc": 56.0}
{"epoch": 26, "training_loss": 69.28628253936768, "training_acc": 52.0, "val_loss": 17.16165542602539, "val_acc": 56.0}
{"epoch": 27, "training_loss": 69.17230892181396, "training_acc": 52.0, "val_loss": 17.201152443885803, "val_acc": 56.0}
{"epoch": 28, "training_loss": 69.23173093795776, "training_acc": 52.0, "val_loss": 17.23257601261139, "val_acc": 56.0}
{"epoch": 29, "training_loss": 69.18097996711731, "training_acc": 52.0, "val_loss": 17.171861231327057, "val_acc": 56.0}
{"epoch": 30, "training_loss": 69.56405305862427, "training_acc": 52.0, "val_loss": 17.130865156650543, "val_acc": 56.0}
{"epoch": 31, "training_loss": 69.65532112121582, "training_acc": 52.0, "val_loss": 17.17340797185898, "val_acc": 56.0}
{"epoch": 32, "training_loss": 70.5073778629303, "training_acc": 52.0, "val_loss": 17.280001938343048, "val_acc": 56.0}
