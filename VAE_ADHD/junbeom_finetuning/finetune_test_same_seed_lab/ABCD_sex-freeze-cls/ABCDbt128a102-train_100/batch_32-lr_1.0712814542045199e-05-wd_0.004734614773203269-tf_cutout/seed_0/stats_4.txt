"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.08881783485413, "training_acc": 53.0, "val_loss": 17.332586646080017, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.03257584571838, "training_acc": 53.0, "val_loss": 17.334401607513428, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.06636619567871, "training_acc": 53.0, "val_loss": 17.33638346195221, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.07021164894104, "training_acc": 53.0, "val_loss": 17.335551977157593, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.0710916519165, "training_acc": 53.0, "val_loss": 17.33466535806656, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.06366395950317, "training_acc": 53.0, "val_loss": 17.33407974243164, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.0782151222229, "training_acc": 53.0, "val_loss": 17.333613336086273, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.06293821334839, "training_acc": 53.0, "val_loss": 17.333903908729553, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.07373762130737, "training_acc": 53.0, "val_loss": 17.334549129009247, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.05563068389893, "training_acc": 53.0, "val_loss": 17.336682975292206, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.10663151741028, "training_acc": 53.0, "val_loss": 17.33718067407608, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.05531454086304, "training_acc": 53.0, "val_loss": 17.336110770702362, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.01948046684265, "training_acc": 53.0, "val_loss": 17.334426939487457, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.05349063873291, "training_acc": 53.0, "val_loss": 17.332017421722412, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.05418038368225, "training_acc": 53.0, "val_loss": 17.330069839954376, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.07427024841309, "training_acc": 53.0, "val_loss": 17.329564690589905, "val_acc": 52.0}
{"epoch": 16, "training_loss": 68.99542474746704, "training_acc": 53.0, "val_loss": 17.330054938793182, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.06546354293823, "training_acc": 53.0, "val_loss": 17.330537736415863, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.04605913162231, "training_acc": 53.0, "val_loss": 17.33013093471527, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.02122497558594, "training_acc": 53.0, "val_loss": 17.329604923725128, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.08148455619812, "training_acc": 53.0, "val_loss": 17.328467965126038, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.05068588256836, "training_acc": 53.0, "val_loss": 17.326976358890533, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.0523591041565, "training_acc": 53.0, "val_loss": 17.325565218925476, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.05217456817627, "training_acc": 53.0, "val_loss": 17.32432097196579, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.04959893226624, "training_acc": 53.0, "val_loss": 17.32400506734848, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.04191303253174, "training_acc": 53.0, "val_loss": 17.32383668422699, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.06885242462158, "training_acc": 53.0, "val_loss": 17.323605716228485, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.09351062774658, "training_acc": 53.0, "val_loss": 17.3232764005661, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.07371544837952, "training_acc": 53.0, "val_loss": 17.322683334350586, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.06053733825684, "training_acc": 53.0, "val_loss": 17.32219308614731, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.06293225288391, "training_acc": 53.0, "val_loss": 17.321892082691193, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.11883163452148, "training_acc": 53.0, "val_loss": 17.321819067001343, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.08593273162842, "training_acc": 53.0, "val_loss": 17.321619391441345, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.07055759429932, "training_acc": 53.0, "val_loss": 17.32158213853836, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.05187773704529, "training_acc": 53.0, "val_loss": 17.32168197631836, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.04391980171204, "training_acc": 53.0, "val_loss": 17.32175648212433, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.05107593536377, "training_acc": 53.0, "val_loss": 17.32185184955597, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.06728339195251, "training_acc": 53.0, "val_loss": 17.32194423675537, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.04225492477417, "training_acc": 53.0, "val_loss": 17.322202026844025, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.08502244949341, "training_acc": 53.0, "val_loss": 17.32218563556671, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.05690693855286, "training_acc": 53.0, "val_loss": 17.322373390197754, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.04742932319641, "training_acc": 53.0, "val_loss": 17.322181165218353, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.09208631515503, "training_acc": 53.0, "val_loss": 17.322036623954773, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.03297567367554, "training_acc": 53.0, "val_loss": 17.321661114692688, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.11365079879761, "training_acc": 53.0, "val_loss": 17.32136905193329, "val_acc": 52.0}
{"epoch": 45, "training_loss": 69.09875059127808, "training_acc": 53.0, "val_loss": 17.32138991355896, "val_acc": 52.0}
{"epoch": 46, "training_loss": 69.10164713859558, "training_acc": 53.0, "val_loss": 17.32162833213806, "val_acc": 52.0}
{"epoch": 47, "training_loss": 69.03392624855042, "training_acc": 53.0, "val_loss": 17.322121560573578, "val_acc": 52.0}
{"epoch": 48, "training_loss": 69.07660293579102, "training_acc": 53.0, "val_loss": 17.322836816310883, "val_acc": 52.0}
{"epoch": 49, "training_loss": 69.0621485710144, "training_acc": 53.0, "val_loss": 17.32369214296341, "val_acc": 52.0}
{"epoch": 50, "training_loss": 69.01595830917358, "training_acc": 53.0, "val_loss": 17.324337363243103, "val_acc": 52.0}
{"epoch": 51, "training_loss": 69.05135440826416, "training_acc": 53.0, "val_loss": 17.324912548065186, "val_acc": 52.0}
{"epoch": 52, "training_loss": 68.99742102622986, "training_acc": 53.0, "val_loss": 17.325879633426666, "val_acc": 52.0}
{"epoch": 53, "training_loss": 69.05944061279297, "training_acc": 53.0, "val_loss": 17.326991260051727, "val_acc": 52.0}
