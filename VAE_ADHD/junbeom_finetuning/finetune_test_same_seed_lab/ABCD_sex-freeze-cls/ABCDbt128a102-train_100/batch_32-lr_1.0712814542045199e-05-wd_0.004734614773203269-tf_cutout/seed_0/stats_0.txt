"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.25688672065735, "training_acc": 52.0, "val_loss": 17.235395312309265, "val_acc": 56.0}
{"epoch": 1, "training_loss": 69.28461384773254, "training_acc": 52.0, "val_loss": 17.2310933470726, "val_acc": 56.0}
{"epoch": 2, "training_loss": 69.27354431152344, "training_acc": 52.0, "val_loss": 17.230239510536194, "val_acc": 56.0}
{"epoch": 3, "training_loss": 69.2205581665039, "training_acc": 52.0, "val_loss": 17.2258660197258, "val_acc": 56.0}
{"epoch": 4, "training_loss": 69.2830901145935, "training_acc": 52.0, "val_loss": 17.226417362689972, "val_acc": 56.0}
{"epoch": 5, "training_loss": 69.23041439056396, "training_acc": 52.0, "val_loss": 17.22356379032135, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.23294186592102, "training_acc": 52.0, "val_loss": 17.22217947244644, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.2648286819458, "training_acc": 52.0, "val_loss": 17.221616208553314, "val_acc": 56.0}
{"epoch": 8, "training_loss": 69.2216968536377, "training_acc": 52.0, "val_loss": 17.22000539302826, "val_acc": 56.0}
{"epoch": 9, "training_loss": 69.28417563438416, "training_acc": 52.0, "val_loss": 17.221710085868835, "val_acc": 56.0}
{"epoch": 10, "training_loss": 69.25707983970642, "training_acc": 52.0, "val_loss": 17.226190865039825, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.23222351074219, "training_acc": 52.0, "val_loss": 17.225898802280426, "val_acc": 56.0}
{"epoch": 12, "training_loss": 69.25387525558472, "training_acc": 52.0, "val_loss": 17.221364378929138, "val_acc": 56.0}
{"epoch": 13, "training_loss": 69.22533464431763, "training_acc": 52.0, "val_loss": 17.218680679798126, "val_acc": 56.0}
{"epoch": 14, "training_loss": 69.26400303840637, "training_acc": 52.0, "val_loss": 17.214885354042053, "val_acc": 56.0}
{"epoch": 15, "training_loss": 69.23358511924744, "training_acc": 52.0, "val_loss": 17.211593687534332, "val_acc": 56.0}
{"epoch": 16, "training_loss": 69.29405903816223, "training_acc": 52.0, "val_loss": 17.206546664237976, "val_acc": 56.0}
{"epoch": 17, "training_loss": 69.24103617668152, "training_acc": 52.0, "val_loss": 17.20597743988037, "val_acc": 56.0}
{"epoch": 18, "training_loss": 69.2226939201355, "training_acc": 52.0, "val_loss": 17.20791757106781, "val_acc": 56.0}
{"epoch": 19, "training_loss": 69.22275447845459, "training_acc": 52.0, "val_loss": 17.210689187049866, "val_acc": 56.0}
{"epoch": 20, "training_loss": 69.26216149330139, "training_acc": 52.0, "val_loss": 17.21116602420807, "val_acc": 56.0}
{"epoch": 21, "training_loss": 69.2645812034607, "training_acc": 52.0, "val_loss": 17.211711406707764, "val_acc": 56.0}
{"epoch": 22, "training_loss": 69.29031300544739, "training_acc": 52.0, "val_loss": 17.212215065956116, "val_acc": 56.0}
{"epoch": 23, "training_loss": 69.24014329910278, "training_acc": 52.0, "val_loss": 17.21353530883789, "val_acc": 56.0}
{"epoch": 24, "training_loss": 69.24332571029663, "training_acc": 52.0, "val_loss": 17.211514711380005, "val_acc": 56.0}
{"epoch": 25, "training_loss": 69.23137831687927, "training_acc": 52.0, "val_loss": 17.21234619617462, "val_acc": 56.0}
{"epoch": 26, "training_loss": 69.2368848323822, "training_acc": 52.0, "val_loss": 17.212417721748352, "val_acc": 56.0}
{"epoch": 27, "training_loss": 69.22941493988037, "training_acc": 52.0, "val_loss": 17.21319854259491, "val_acc": 56.0}
{"epoch": 28, "training_loss": 69.24811172485352, "training_acc": 52.0, "val_loss": 17.21353530883789, "val_acc": 56.0}
{"epoch": 29, "training_loss": 69.23836660385132, "training_acc": 52.0, "val_loss": 17.21094101667404, "val_acc": 56.0}
{"epoch": 30, "training_loss": 69.2796893119812, "training_acc": 52.0, "val_loss": 17.205192148685455, "val_acc": 56.0}
{"epoch": 31, "training_loss": 69.28263330459595, "training_acc": 52.0, "val_loss": 17.200027406215668, "val_acc": 56.0}
{"epoch": 32, "training_loss": 69.2669448852539, "training_acc": 52.0, "val_loss": 17.19314604997635, "val_acc": 56.0}
{"epoch": 33, "training_loss": 69.24292325973511, "training_acc": 52.0, "val_loss": 17.188550531864166, "val_acc": 56.0}
{"epoch": 34, "training_loss": 69.25300550460815, "training_acc": 52.0, "val_loss": 17.187289893627167, "val_acc": 56.0}
{"epoch": 35, "training_loss": 69.28058505058289, "training_acc": 52.0, "val_loss": 17.188087105751038, "val_acc": 56.0}
{"epoch": 36, "training_loss": 69.28325486183167, "training_acc": 52.0, "val_loss": 17.18713343143463, "val_acc": 56.0}
{"epoch": 37, "training_loss": 69.23617887496948, "training_acc": 52.0, "val_loss": 17.18575954437256, "val_acc": 56.0}
{"epoch": 38, "training_loss": 69.25847053527832, "training_acc": 52.0, "val_loss": 17.185798287391663, "val_acc": 56.0}
{"epoch": 39, "training_loss": 69.30487489700317, "training_acc": 52.0, "val_loss": 17.187440395355225, "val_acc": 56.0}
{"epoch": 40, "training_loss": 69.27358555793762, "training_acc": 52.0, "val_loss": 17.190253734588623, "val_acc": 56.0}
{"epoch": 41, "training_loss": 69.22748756408691, "training_acc": 52.0, "val_loss": 17.191429436206818, "val_acc": 56.0}
{"epoch": 42, "training_loss": 69.25947952270508, "training_acc": 52.0, "val_loss": 17.19358265399933, "val_acc": 56.0}
{"epoch": 43, "training_loss": 69.22069597244263, "training_acc": 52.0, "val_loss": 17.195865511894226, "val_acc": 56.0}
{"epoch": 44, "training_loss": 69.23141431808472, "training_acc": 52.0, "val_loss": 17.19977855682373, "val_acc": 56.0}
{"epoch": 45, "training_loss": 69.26135063171387, "training_acc": 52.0, "val_loss": 17.20234602689743, "val_acc": 56.0}
{"epoch": 46, "training_loss": 69.25002765655518, "training_acc": 52.0, "val_loss": 17.203937470912933, "val_acc": 56.0}
{"epoch": 47, "training_loss": 69.23409605026245, "training_acc": 52.0, "val_loss": 17.203818261623383, "val_acc": 56.0}
{"epoch": 48, "training_loss": 69.25586533546448, "training_acc": 52.0, "val_loss": 17.20426082611084, "val_acc": 56.0}
{"epoch": 49, "training_loss": 69.2312240600586, "training_acc": 52.0, "val_loss": 17.20687597990036, "val_acc": 56.0}
{"epoch": 50, "training_loss": 69.27241134643555, "training_acc": 52.0, "val_loss": 17.211352288722992, "val_acc": 56.0}
{"epoch": 51, "training_loss": 69.28099608421326, "training_acc": 52.0, "val_loss": 17.216895520687103, "val_acc": 56.0}
{"epoch": 52, "training_loss": 69.26246619224548, "training_acc": 52.0, "val_loss": 17.222678661346436, "val_acc": 56.0}
{"epoch": 53, "training_loss": 69.25690841674805, "training_acc": 52.0, "val_loss": 17.226603627204895, "val_acc": 56.0}
{"epoch": 54, "training_loss": 69.2418475151062, "training_acc": 52.0, "val_loss": 17.23087728023529, "val_acc": 56.0}
{"epoch": 55, "training_loss": 69.24312448501587, "training_acc": 52.0, "val_loss": 17.238177359104156, "val_acc": 56.0}
{"epoch": 56, "training_loss": 69.26678276062012, "training_acc": 52.0, "val_loss": 17.247679829597473, "val_acc": 56.0}
