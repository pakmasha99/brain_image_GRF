"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.2078275680542, "training_acc": 53.0, "val_loss": 17.345543205738068, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.18146800994873, "training_acc": 53.0, "val_loss": 17.343097925186157, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.15903568267822, "training_acc": 53.0, "val_loss": 17.343299090862274, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.19039821624756, "training_acc": 53.0, "val_loss": 17.341403663158417, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.08327603340149, "training_acc": 53.0, "val_loss": 17.339062690734863, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.1278567314148, "training_acc": 53.0, "val_loss": 17.336927354335785, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.14766931533813, "training_acc": 53.0, "val_loss": 17.335571348667145, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.19060134887695, "training_acc": 53.0, "val_loss": 17.335639894008636, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.16064167022705, "training_acc": 53.0, "val_loss": 17.335782945156097, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.11608242988586, "training_acc": 53.0, "val_loss": 17.333613336086273, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.13308525085449, "training_acc": 53.0, "val_loss": 17.33160763978958, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.1694757938385, "training_acc": 53.0, "val_loss": 17.329975962638855, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.14811992645264, "training_acc": 53.0, "val_loss": 17.32831597328186, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.13981294631958, "training_acc": 53.0, "val_loss": 17.32725501060486, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.15122938156128, "training_acc": 53.0, "val_loss": 17.327113449573517, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.16640281677246, "training_acc": 53.0, "val_loss": 17.32751876115799, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.13911604881287, "training_acc": 53.0, "val_loss": 17.32833683490753, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.14042663574219, "training_acc": 53.0, "val_loss": 17.32795685529709, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.1813006401062, "training_acc": 53.0, "val_loss": 17.326851189136505, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.1273455619812, "training_acc": 53.0, "val_loss": 17.325420677661896, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.12158298492432, "training_acc": 53.0, "val_loss": 17.32395589351654, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.13854646682739, "training_acc": 53.0, "val_loss": 17.32308566570282, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.12150454521179, "training_acc": 53.0, "val_loss": 17.32204407453537, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.12570881843567, "training_acc": 53.0, "val_loss": 17.32141524553299, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.19190263748169, "training_acc": 53.0, "val_loss": 17.321541905403137, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.12465476989746, "training_acc": 53.0, "val_loss": 17.3223078250885, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.09867095947266, "training_acc": 53.0, "val_loss": 17.322860658168793, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.16372299194336, "training_acc": 53.0, "val_loss": 17.323413491249084, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.139901638031, "training_acc": 53.0, "val_loss": 17.32417643070221, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.14130783081055, "training_acc": 53.0, "val_loss": 17.325720191001892, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.11134600639343, "training_acc": 53.0, "val_loss": 17.32780486345291, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.13809394836426, "training_acc": 53.0, "val_loss": 17.32936054468155, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.12173962593079, "training_acc": 53.0, "val_loss": 17.330878973007202, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.1181571483612, "training_acc": 53.0, "val_loss": 17.33061373233795, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.12988305091858, "training_acc": 53.0, "val_loss": 17.32891947031021, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.15755462646484, "training_acc": 53.0, "val_loss": 17.328020930290222, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.1264898777008, "training_acc": 53.0, "val_loss": 17.327991127967834, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.14189004898071, "training_acc": 53.0, "val_loss": 17.328649759292603, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.13411569595337, "training_acc": 53.0, "val_loss": 17.3293337225914, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.12613296508789, "training_acc": 53.0, "val_loss": 17.328985035419464, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.14942502975464, "training_acc": 53.0, "val_loss": 17.329052090644836, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.12323045730591, "training_acc": 53.0, "val_loss": 17.329443991184235, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.16239142417908, "training_acc": 53.0, "val_loss": 17.329736053943634, "val_acc": 52.0}
