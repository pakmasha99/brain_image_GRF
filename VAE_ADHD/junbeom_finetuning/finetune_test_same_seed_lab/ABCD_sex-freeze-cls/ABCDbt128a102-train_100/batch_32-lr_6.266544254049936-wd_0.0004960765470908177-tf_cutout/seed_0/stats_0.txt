"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 55495.966888427734, "training_acc": 48.0, "val_loss": 23240.22674560547, "val_acc": 56.0}
{"epoch": 1, "training_loss": 83174.47454833984, "training_acc": 52.0, "val_loss": 14450.120544433594, "val_acc": 44.0}
{"epoch": 2, "training_loss": 81499.2060546875, "training_acc": 48.0, "val_loss": 11494.931030273438, "val_acc": 44.0}
{"epoch": 3, "training_loss": 30400.88623046875, "training_acc": 50.0, "val_loss": 7091.646575927734, "val_acc": 56.0}
{"epoch": 4, "training_loss": 24774.19482421875, "training_acc": 50.0, "val_loss": 12522.914123535156, "val_acc": 44.0}
{"epoch": 5, "training_loss": 27081.089416503906, "training_acc": 44.0, "val_loss": 3284.9021911621094, "val_acc": 56.0}
{"epoch": 6, "training_loss": 14150.255004882812, "training_acc": 50.0, "val_loss": 7065.289306640625, "val_acc": 44.0}
{"epoch": 7, "training_loss": 16829.545654296875, "training_acc": 52.0, "val_loss": 7134.981536865234, "val_acc": 56.0}
{"epoch": 8, "training_loss": 17623.877685546875, "training_acc": 52.0, "val_loss": 12247.624969482422, "val_acc": 44.0}
{"epoch": 9, "training_loss": 45084.2421875, "training_acc": 48.0, "val_loss": 1837.131690979004, "val_acc": 56.0}
{"epoch": 10, "training_loss": 12175.34701538086, "training_acc": 50.0, "val_loss": 372.07396030426025, "val_acc": 44.0}
{"epoch": 11, "training_loss": 9456.281555175781, "training_acc": 54.0, "val_loss": 1292.181396484375, "val_acc": 44.0}
{"epoch": 12, "training_loss": 4092.096013069153, "training_acc": 43.0, "val_loss": 85.73211431503296, "val_acc": 52.0}
{"epoch": 13, "training_loss": 4159.404762268066, "training_acc": 55.0, "val_loss": 629.0746212005615, "val_acc": 44.0}
{"epoch": 14, "training_loss": 12067.162780761719, "training_acc": 50.0, "val_loss": 1852.7128219604492, "val_acc": 44.0}
{"epoch": 15, "training_loss": 6827.8515625, "training_acc": 52.0, "val_loss": 7192.484283447266, "val_acc": 56.0}
{"epoch": 16, "training_loss": 31294.85858154297, "training_acc": 52.0, "val_loss": 6414.344787597656, "val_acc": 44.0}
{"epoch": 17, "training_loss": 38130.75750732422, "training_acc": 48.0, "val_loss": 5866.8060302734375, "val_acc": 44.0}
{"epoch": 18, "training_loss": 16292.42041015625, "training_acc": 46.0, "val_loss": 277.91717052459717, "val_acc": 44.0}
{"epoch": 19, "training_loss": 3509.177104949951, "training_acc": 47.0, "val_loss": 1089.510154724121, "val_acc": 56.0}
{"epoch": 20, "training_loss": 3162.2557678222656, "training_acc": 52.0, "val_loss": 1071.22163772583, "val_acc": 56.0}
{"epoch": 21, "training_loss": 2863.677502155304, "training_acc": 56.0, "val_loss": 495.09434700012207, "val_acc": 56.0}
{"epoch": 22, "training_loss": 2597.0685958862305, "training_acc": 54.0, "val_loss": 964.5391464233398, "val_acc": 44.0}
{"epoch": 23, "training_loss": 9834.606567382812, "training_acc": 48.0, "val_loss": 2917.621421813965, "val_acc": 44.0}
{"epoch": 24, "training_loss": 9741.765930175781, "training_acc": 46.0, "val_loss": 984.1598510742188, "val_acc": 44.0}
{"epoch": 25, "training_loss": 3922.5341186523438, "training_acc": 52.0, "val_loss": 2832.398796081543, "val_acc": 44.0}
{"epoch": 26, "training_loss": 10943.61929321289, "training_acc": 38.0, "val_loss": 1162.24365234375, "val_acc": 44.0}
{"epoch": 27, "training_loss": 6259.5858154296875, "training_acc": 48.0, "val_loss": 184.62448120117188, "val_acc": 56.0}
{"epoch": 28, "training_loss": 17324.21337890625, "training_acc": 52.0, "val_loss": 187.32088804244995, "val_acc": 48.0}
{"epoch": 29, "training_loss": 27788.37857055664, "training_acc": 48.0, "val_loss": 12089.933013916016, "val_acc": 56.0}
{"epoch": 30, "training_loss": 38801.99607849121, "training_acc": 50.0, "val_loss": 3050.79288482666, "val_acc": 44.0}
{"epoch": 31, "training_loss": 6602.3876953125, "training_acc": 58.0, "val_loss": 1997.2917556762695, "val_acc": 56.0}
