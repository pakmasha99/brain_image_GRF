"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 58032.148109436035, "training_acc": 51.0, "val_loss": 16950.254821777344, "val_acc": 48.0}
{"epoch": 1, "training_loss": 39045.626220703125, "training_acc": 49.0, "val_loss": 3478.1028747558594, "val_acc": 52.0}
{"epoch": 2, "training_loss": 15543.096374511719, "training_acc": 51.0, "val_loss": 4590.725326538086, "val_acc": 52.0}
{"epoch": 3, "training_loss": 18735.126174926758, "training_acc": 53.0, "val_loss": 1346.9179153442383, "val_acc": 48.0}
{"epoch": 4, "training_loss": 3955.219955444336, "training_acc": 57.0, "val_loss": 4599.846267700195, "val_acc": 48.0}
{"epoch": 5, "training_loss": 12894.046905517578, "training_acc": 53.0, "val_loss": 5371.225357055664, "val_acc": 52.0}
{"epoch": 6, "training_loss": 15643.710327148438, "training_acc": 53.0, "val_loss": 5704.596710205078, "val_acc": 48.0}
{"epoch": 7, "training_loss": 16802.6279296875, "training_acc": 49.0, "val_loss": 9885.87875366211, "val_acc": 52.0}
{"epoch": 8, "training_loss": 30176.464324951172, "training_acc": 51.0, "val_loss": 2738.148307800293, "val_acc": 48.0}
{"epoch": 9, "training_loss": 8140.302703857422, "training_acc": 57.0, "val_loss": 5380.157089233398, "val_acc": 52.0}
{"epoch": 10, "training_loss": 21837.43115234375, "training_acc": 39.0, "val_loss": 2456.730079650879, "val_acc": 48.0}
{"epoch": 11, "training_loss": 18129.8271484375, "training_acc": 55.0, "val_loss": 5788.196182250977, "val_acc": 52.0}
{"epoch": 12, "training_loss": 20467.340698242188, "training_acc": 47.0, "val_loss": 11027.017211914062, "val_acc": 48.0}
{"epoch": 13, "training_loss": 27376.231201171875, "training_acc": 47.0, "val_loss": 11410.617065429688, "val_acc": 52.0}
{"epoch": 14, "training_loss": 45175.880859375, "training_acc": 53.0, "val_loss": 710.5924129486084, "val_acc": 48.0}
{"epoch": 15, "training_loss": 14770.504089355469, "training_acc": 47.0, "val_loss": 2856.5109252929688, "val_acc": 52.0}
{"epoch": 16, "training_loss": 13234.960139274597, "training_acc": 53.0, "val_loss": 2400.1474380493164, "val_acc": 48.0}
{"epoch": 17, "training_loss": 10510.75634765625, "training_acc": 45.0, "val_loss": 1375.8551597595215, "val_acc": 52.0}
{"epoch": 18, "training_loss": 19833.406127929688, "training_acc": 45.0, "val_loss": 591.2848949432373, "val_acc": 52.0}
{"epoch": 19, "training_loss": 13142.487548828125, "training_acc": 53.0, "val_loss": 3684.65576171875, "val_acc": 48.0}
{"epoch": 20, "training_loss": 19031.441833496094, "training_acc": 45.0, "val_loss": 354.66673374176025, "val_acc": 52.0}
{"epoch": 21, "training_loss": 5236.3375244140625, "training_acc": 49.0, "val_loss": 2119.3355560302734, "val_acc": 52.0}
{"epoch": 22, "training_loss": 6534.630157470703, "training_acc": 51.0, "val_loss": 1072.8005409240723, "val_acc": 52.0}
{"epoch": 23, "training_loss": 6102.953125, "training_acc": 47.0, "val_loss": 1172.098445892334, "val_acc": 52.0}
{"epoch": 24, "training_loss": 4365.886535644531, "training_acc": 59.0, "val_loss": 1072.231674194336, "val_acc": 52.0}
{"epoch": 25, "training_loss": 7109.519790649414, "training_acc": 49.0, "val_loss": 2543.216323852539, "val_acc": 52.0}
{"epoch": 26, "training_loss": 7457.7052001953125, "training_acc": 57.0, "val_loss": 826.9838333129883, "val_acc": 48.0}
{"epoch": 27, "training_loss": 15890.546752929688, "training_acc": 51.0, "val_loss": 1864.8839950561523, "val_acc": 52.0}
{"epoch": 28, "training_loss": 14949.763916015625, "training_acc": 53.0, "val_loss": 278.41551303863525, "val_acc": 48.0}
{"epoch": 29, "training_loss": 18601.102325439453, "training_acc": 62.0, "val_loss": 8721.633911132812, "val_acc": 52.0}
{"epoch": 30, "training_loss": 20677.481811523438, "training_acc": 49.0, "val_loss": 2026.5405654907227, "val_acc": 48.0}
{"epoch": 31, "training_loss": 12230.37353515625, "training_acc": 51.0, "val_loss": 1550.8578300476074, "val_acc": 48.0}
{"epoch": 32, "training_loss": 6454.294738769531, "training_acc": 51.0, "val_loss": 7143.186187744141, "val_acc": 52.0}
{"epoch": 33, "training_loss": 25629.77032470703, "training_acc": 53.0, "val_loss": 7591.973876953125, "val_acc": 48.0}
{"epoch": 34, "training_loss": 43146.734130859375, "training_acc": 47.0, "val_loss": 5269.629669189453, "val_acc": 48.0}
{"epoch": 35, "training_loss": 21919.198974609375, "training_acc": 47.0, "val_loss": 5666.005325317383, "val_acc": 52.0}
{"epoch": 36, "training_loss": 16471.1474609375, "training_acc": 53.0, "val_loss": 11770.188903808594, "val_acc": 48.0}
{"epoch": 37, "training_loss": 39128.933502197266, "training_acc": 47.0, "val_loss": 9843.687438964844, "val_acc": 52.0}
{"epoch": 38, "training_loss": 60277.00537109375, "training_acc": 53.0, "val_loss": 18270.510864257812, "val_acc": 52.0}
{"epoch": 39, "training_loss": 45691.821533203125, "training_acc": 51.0, "val_loss": 9818.702697753906, "val_acc": 48.0}
{"epoch": 40, "training_loss": 37405.52734375, "training_acc": 47.0, "val_loss": 5134.177017211914, "val_acc": 52.0}
{"epoch": 41, "training_loss": 32511.6875, "training_acc": 53.0, "val_loss": 3363.900375366211, "val_acc": 52.0}
{"epoch": 42, "training_loss": 19187.7529296875, "training_acc": 53.0, "val_loss": 2979.8675537109375, "val_acc": 48.0}
{"epoch": 43, "training_loss": 18287.402099609375, "training_acc": 51.0, "val_loss": 5192.762756347656, "val_acc": 52.0}
{"epoch": 44, "training_loss": 17116.68341064453, "training_acc": 51.0, "val_loss": 9919.944763183594, "val_acc": 48.0}
{"epoch": 45, "training_loss": 20469.45669555664, "training_acc": 57.0, "val_loss": 12988.58642578125, "val_acc": 52.0}
{"epoch": 46, "training_loss": 58300.51708984375, "training_acc": 53.0, "val_loss": 7980.267333984375, "val_acc": 52.0}
{"epoch": 47, "training_loss": 16056.34423828125, "training_acc": 61.0, "val_loss": 17417.112731933594, "val_acc": 48.0}
