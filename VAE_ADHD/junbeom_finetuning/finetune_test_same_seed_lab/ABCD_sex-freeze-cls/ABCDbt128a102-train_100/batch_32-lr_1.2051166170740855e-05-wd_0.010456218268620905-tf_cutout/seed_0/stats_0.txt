"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.25742530822754, "training_acc": 52.0, "val_loss": 17.23552644252777, "val_acc": 56.0}
{"epoch": 1, "training_loss": 69.28497743606567, "training_acc": 52.0, "val_loss": 17.230674624443054, "val_acc": 56.0}
{"epoch": 2, "training_loss": 69.2740364074707, "training_acc": 52.0, "val_loss": 17.229722440242767, "val_acc": 56.0}
{"epoch": 3, "training_loss": 69.22079467773438, "training_acc": 52.0, "val_loss": 17.224837839603424, "val_acc": 56.0}
{"epoch": 4, "training_loss": 69.28281044960022, "training_acc": 52.0, "val_loss": 17.22548007965088, "val_acc": 56.0}
{"epoch": 5, "training_loss": 69.23025798797607, "training_acc": 52.0, "val_loss": 17.222324013710022, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.23246312141418, "training_acc": 52.0, "val_loss": 17.220820486545563, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.2644100189209, "training_acc": 52.0, "val_loss": 17.22024530172348, "val_acc": 56.0}
{"epoch": 8, "training_loss": 69.22191381454468, "training_acc": 52.0, "val_loss": 17.218509316444397, "val_acc": 56.0}
{"epoch": 9, "training_loss": 69.28411436080933, "training_acc": 52.0, "val_loss": 17.220471799373627, "val_acc": 56.0}
{"epoch": 10, "training_loss": 69.2566294670105, "training_acc": 52.0, "val_loss": 17.225536704063416, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.23203039169312, "training_acc": 52.0, "val_loss": 17.225247621536255, "val_acc": 56.0}
{"epoch": 12, "training_loss": 69.2544686794281, "training_acc": 52.0, "val_loss": 17.220206558704376, "val_acc": 56.0}
{"epoch": 13, "training_loss": 69.22463154792786, "training_acc": 52.0, "val_loss": 17.21724271774292, "val_acc": 56.0}
{"epoch": 14, "training_loss": 69.26353359222412, "training_acc": 52.0, "val_loss": 17.21305400133133, "val_acc": 56.0}
{"epoch": 15, "training_loss": 69.23306727409363, "training_acc": 52.0, "val_loss": 17.209449410438538, "val_acc": 56.0}
{"epoch": 16, "training_loss": 69.29487133026123, "training_acc": 52.0, "val_loss": 17.203913629055023, "val_acc": 56.0}
{"epoch": 17, "training_loss": 69.24123024940491, "training_acc": 52.0, "val_loss": 17.203350365161896, "val_acc": 56.0}
{"epoch": 18, "training_loss": 69.22275352478027, "training_acc": 52.0, "val_loss": 17.20556765794754, "val_acc": 56.0}
{"epoch": 19, "training_loss": 69.2228946685791, "training_acc": 52.0, "val_loss": 17.2087162733078, "val_acc": 56.0}
{"epoch": 20, "training_loss": 69.26206636428833, "training_acc": 52.0, "val_loss": 17.20932126045227, "val_acc": 56.0}
{"epoch": 21, "training_loss": 69.26447200775146, "training_acc": 52.0, "val_loss": 17.20999926328659, "val_acc": 56.0}
{"epoch": 22, "training_loss": 69.29009866714478, "training_acc": 52.0, "val_loss": 17.21062809228897, "val_acc": 56.0}
{"epoch": 23, "training_loss": 69.23980760574341, "training_acc": 52.0, "val_loss": 17.212162911891937, "val_acc": 56.0}
{"epoch": 24, "training_loss": 69.24352025985718, "training_acc": 52.0, "val_loss": 17.209967970848083, "val_acc": 56.0}
{"epoch": 25, "training_loss": 69.23110723495483, "training_acc": 52.0, "val_loss": 17.21094846725464, "val_acc": 56.0}
{"epoch": 26, "training_loss": 69.23671770095825, "training_acc": 52.0, "val_loss": 17.211082577705383, "val_acc": 56.0}
{"epoch": 27, "training_loss": 69.22904682159424, "training_acc": 52.0, "val_loss": 17.212004959583282, "val_acc": 56.0}
{"epoch": 28, "training_loss": 69.24790954589844, "training_acc": 52.0, "val_loss": 17.212428152561188, "val_acc": 56.0}
{"epoch": 29, "training_loss": 69.23802137374878, "training_acc": 52.0, "val_loss": 17.209577560424805, "val_acc": 56.0}
{"epoch": 30, "training_loss": 69.28059554100037, "training_acc": 52.0, "val_loss": 17.203228175640106, "val_acc": 56.0}
{"epoch": 31, "training_loss": 69.28183627128601, "training_acc": 52.0, "val_loss": 17.197556793689728, "val_acc": 56.0}
{"epoch": 32, "training_loss": 69.26811456680298, "training_acc": 52.0, "val_loss": 17.19004213809967, "val_acc": 56.0}
{"epoch": 33, "training_loss": 69.24422860145569, "training_acc": 52.0, "val_loss": 17.185088992118835, "val_acc": 56.0}
{"epoch": 34, "training_loss": 69.25596714019775, "training_acc": 52.0, "val_loss": 17.18379557132721, "val_acc": 56.0}
{"epoch": 35, "training_loss": 69.28318476676941, "training_acc": 52.0, "val_loss": 17.18476116657257, "val_acc": 56.0}
{"epoch": 36, "training_loss": 69.28564786911011, "training_acc": 52.0, "val_loss": 17.18381494283676, "val_acc": 56.0}
{"epoch": 37, "training_loss": 69.2388710975647, "training_acc": 52.0, "val_loss": 17.182418704032898, "val_acc": 56.0}
{"epoch": 38, "training_loss": 69.26143741607666, "training_acc": 52.0, "val_loss": 17.182567715644836, "val_acc": 56.0}
{"epoch": 39, "training_loss": 69.3072144985199, "training_acc": 52.0, "val_loss": 17.184463143348694, "val_acc": 56.0}
{"epoch": 40, "training_loss": 69.27639746665955, "training_acc": 52.0, "val_loss": 17.187651991844177, "val_acc": 56.0}
{"epoch": 41, "training_loss": 69.22889375686646, "training_acc": 52.0, "val_loss": 17.189045250415802, "val_acc": 56.0}
{"epoch": 42, "training_loss": 69.25998854637146, "training_acc": 52.0, "val_loss": 17.191512882709503, "val_acc": 56.0}
{"epoch": 43, "training_loss": 69.22102451324463, "training_acc": 52.0, "val_loss": 17.194129526615143, "val_acc": 56.0}
{"epoch": 44, "training_loss": 69.232182264328, "training_acc": 52.0, "val_loss": 17.198558151721954, "val_acc": 56.0}
{"epoch": 45, "training_loss": 69.26089215278625, "training_acc": 52.0, "val_loss": 17.201492190361023, "val_acc": 56.0}
{"epoch": 46, "training_loss": 69.24976658821106, "training_acc": 52.0, "val_loss": 17.203328013420105, "val_acc": 56.0}
{"epoch": 47, "training_loss": 69.23382496833801, "training_acc": 52.0, "val_loss": 17.203238606452942, "val_acc": 56.0}
{"epoch": 48, "training_loss": 69.25550699234009, "training_acc": 52.0, "val_loss": 17.203769087791443, "val_acc": 56.0}
{"epoch": 49, "training_loss": 69.23047542572021, "training_acc": 52.0, "val_loss": 17.206737399101257, "val_acc": 56.0}
{"epoch": 50, "training_loss": 69.27170395851135, "training_acc": 52.0, "val_loss": 17.21179485321045, "val_acc": 56.0}
{"epoch": 51, "training_loss": 69.28021383285522, "training_acc": 52.0, "val_loss": 17.218071222305298, "val_acc": 56.0}
{"epoch": 52, "training_loss": 69.26228666305542, "training_acc": 52.0, "val_loss": 17.224624752998352, "val_acc": 56.0}
{"epoch": 53, "training_loss": 69.25692653656006, "training_acc": 52.0, "val_loss": 17.22906082868576, "val_acc": 56.0}
{"epoch": 54, "training_loss": 69.24259686470032, "training_acc": 52.0, "val_loss": 17.23388433456421, "val_acc": 56.0}
{"epoch": 55, "training_loss": 69.24309062957764, "training_acc": 52.0, "val_loss": 17.24216789007187, "val_acc": 56.0}
{"epoch": 56, "training_loss": 69.26786422729492, "training_acc": 52.0, "val_loss": 17.252983152866364, "val_acc": 56.0}
