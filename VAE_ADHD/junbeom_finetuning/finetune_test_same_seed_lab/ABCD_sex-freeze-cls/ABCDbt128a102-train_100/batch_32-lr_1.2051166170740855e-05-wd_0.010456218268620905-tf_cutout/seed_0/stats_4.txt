"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.14921188354492, "training_acc": 53.0, "val_loss": 17.379973828792572, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.16808366775513, "training_acc": 53.0, "val_loss": 17.38259792327881, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.14190340042114, "training_acc": 53.0, "val_loss": 17.38322377204895, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.175936460495, "training_acc": 53.0, "val_loss": 17.38496720790863, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.18286347389221, "training_acc": 53.0, "val_loss": 17.38283336162567, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.13519549369812, "training_acc": 53.0, "val_loss": 17.38080233335495, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.13159132003784, "training_acc": 53.0, "val_loss": 17.3793762922287, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.17465400695801, "training_acc": 53.0, "val_loss": 17.378215491771698, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.11587810516357, "training_acc": 53.0, "val_loss": 17.378230392932892, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.18527841567993, "training_acc": 53.0, "val_loss": 17.37881302833557, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.15189361572266, "training_acc": 53.0, "val_loss": 17.38154888153076, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.14279580116272, "training_acc": 53.0, "val_loss": 17.381982505321503, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.09735631942749, "training_acc": 53.0, "val_loss": 17.38027185201645, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.12386274337769, "training_acc": 53.0, "val_loss": 17.37777441740036, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.14248728752136, "training_acc": 53.0, "val_loss": 17.374354600906372, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.12971305847168, "training_acc": 53.0, "val_loss": 17.371615767478943, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.16000437736511, "training_acc": 53.0, "val_loss": 17.370840907096863, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.11626482009888, "training_acc": 53.0, "val_loss": 17.37140417098999, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.16306686401367, "training_acc": 53.0, "val_loss": 17.37196147441864, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.08154082298279, "training_acc": 53.0, "val_loss": 17.371344566345215, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.06848621368408, "training_acc": 53.0, "val_loss": 17.37057864665985, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.07489585876465, "training_acc": 53.0, "val_loss": 17.369025945663452, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.12780117988586, "training_acc": 53.0, "val_loss": 17.36704260110855, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.1109869480133, "training_acc": 53.0, "val_loss": 17.365191876888275, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.13010954856873, "training_acc": 53.0, "val_loss": 17.36358255147934, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.14907121658325, "training_acc": 53.0, "val_loss": 17.363150417804718, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.15043926239014, "training_acc": 53.0, "val_loss": 17.362916469573975, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.17253637313843, "training_acc": 53.0, "val_loss": 17.36261397600174, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.10885238647461, "training_acc": 53.0, "val_loss": 17.36219674348831, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.14382410049438, "training_acc": 53.0, "val_loss": 17.361465096473694, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.0977931022644, "training_acc": 53.0, "val_loss": 17.360864579677582, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.12638854980469, "training_acc": 53.0, "val_loss": 17.360496520996094, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.11636328697205, "training_acc": 53.0, "val_loss": 17.360401153564453, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.14472532272339, "training_acc": 53.0, "val_loss": 17.36016571521759, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.11493563652039, "training_acc": 53.0, "val_loss": 17.360129952430725, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.14376401901245, "training_acc": 53.0, "val_loss": 17.360252141952515, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.13040447235107, "training_acc": 53.0, "val_loss": 17.360348999500275, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.10602974891663, "training_acc": 53.0, "val_loss": 17.360472679138184, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.10297322273254, "training_acc": 53.0, "val_loss": 17.360594868659973, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.10090827941895, "training_acc": 53.0, "val_loss": 17.360907793045044, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.14938306808472, "training_acc": 53.0, "val_loss": 17.36089438199997, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.09678220748901, "training_acc": 53.0, "val_loss": 17.361129820346832, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.12041401863098, "training_acc": 53.0, "val_loss": 17.360900342464447, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.12670755386353, "training_acc": 53.0, "val_loss": 17.360731959342957, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.11413860321045, "training_acc": 53.0, "val_loss": 17.360281944274902, "val_acc": 52.0}
{"epoch": 45, "training_loss": 69.13873362541199, "training_acc": 53.0, "val_loss": 17.359939217567444, "val_acc": 52.0}
{"epoch": 46, "training_loss": 69.11091566085815, "training_acc": 53.0, "val_loss": 17.359966039657593, "val_acc": 52.0}
{"epoch": 47, "training_loss": 69.12465023994446, "training_acc": 53.0, "val_loss": 17.36025959253311, "val_acc": 52.0}
{"epoch": 48, "training_loss": 69.11251854896545, "training_acc": 53.0, "val_loss": 17.36087054014206, "val_acc": 52.0}
{"epoch": 49, "training_loss": 69.08936071395874, "training_acc": 53.0, "val_loss": 17.36176311969757, "val_acc": 52.0}
{"epoch": 50, "training_loss": 69.10758638381958, "training_acc": 53.0, "val_loss": 17.362843453884125, "val_acc": 52.0}
{"epoch": 51, "training_loss": 69.1414704322815, "training_acc": 53.0, "val_loss": 17.363664507865906, "val_acc": 52.0}
{"epoch": 52, "training_loss": 69.12401628494263, "training_acc": 53.0, "val_loss": 17.364394664764404, "val_acc": 52.0}
{"epoch": 53, "training_loss": 69.13237357139587, "training_acc": 53.0, "val_loss": 17.365609109401703, "val_acc": 52.0}
{"epoch": 54, "training_loss": 69.14546346664429, "training_acc": 53.0, "val_loss": 17.36699491739273, "val_acc": 52.0}
{"epoch": 55, "training_loss": 69.13462352752686, "training_acc": 53.0, "val_loss": 17.367568612098694, "val_acc": 52.0}
{"epoch": 56, "training_loss": 69.11872816085815, "training_acc": 53.0, "val_loss": 17.366641759872437, "val_acc": 52.0}
{"epoch": 57, "training_loss": 69.09577322006226, "training_acc": 53.0, "val_loss": 17.365604639053345, "val_acc": 52.0}
{"epoch": 58, "training_loss": 69.11329317092896, "training_acc": 53.0, "val_loss": 17.36489087343216, "val_acc": 52.0}
{"epoch": 59, "training_loss": 69.08843517303467, "training_acc": 53.0, "val_loss": 17.3642560839653, "val_acc": 52.0}
{"epoch": 60, "training_loss": 69.08090305328369, "training_acc": 53.0, "val_loss": 17.363153398036957, "val_acc": 52.0}
{"epoch": 61, "training_loss": 69.09130311012268, "training_acc": 53.0, "val_loss": 17.362596094608307, "val_acc": 52.0}
{"epoch": 62, "training_loss": 69.13465881347656, "training_acc": 53.0, "val_loss": 17.36205965280533, "val_acc": 52.0}
{"epoch": 63, "training_loss": 69.12090468406677, "training_acc": 53.0, "val_loss": 17.361436784267426, "val_acc": 52.0}
{"epoch": 64, "training_loss": 69.12250137329102, "training_acc": 53.0, "val_loss": 17.36106425523758, "val_acc": 52.0}
