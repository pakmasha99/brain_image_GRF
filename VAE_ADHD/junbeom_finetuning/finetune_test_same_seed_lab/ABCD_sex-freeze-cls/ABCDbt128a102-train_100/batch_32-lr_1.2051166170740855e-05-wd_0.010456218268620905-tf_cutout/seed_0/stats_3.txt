"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.1820878982544, "training_acc": 53.0, "val_loss": 17.297038435935974, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.16505122184753, "training_acc": 53.0, "val_loss": 17.297948896884918, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.14506578445435, "training_acc": 53.0, "val_loss": 17.296823859214783, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.1271231174469, "training_acc": 53.0, "val_loss": 17.295771837234497, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.14293479919434, "training_acc": 53.0, "val_loss": 17.29491353034973, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.11708378791809, "training_acc": 53.0, "val_loss": 17.29346066713333, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.11024713516235, "training_acc": 53.0, "val_loss": 17.291924357414246, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.12116622924805, "training_acc": 53.0, "val_loss": 17.291054129600525, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.12367010116577, "training_acc": 53.0, "val_loss": 17.290619015693665, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.0592098236084, "training_acc": 53.0, "val_loss": 17.29038953781128, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.09020590782166, "training_acc": 53.0, "val_loss": 17.29024350643158, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.08795499801636, "training_acc": 53.0, "val_loss": 17.290136218070984, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.08899974822998, "training_acc": 53.0, "val_loss": 17.290231585502625, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.08261346817017, "training_acc": 53.0, "val_loss": 17.290741205215454, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.08795619010925, "training_acc": 53.0, "val_loss": 17.291900515556335, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.06077003479004, "training_acc": 53.0, "val_loss": 17.29263961315155, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.0428957939148, "training_acc": 53.0, "val_loss": 17.29295700788498, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.01442956924438, "training_acc": 53.0, "val_loss": 17.293761670589447, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.08730173110962, "training_acc": 53.0, "val_loss": 17.294463515281677, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.08291506767273, "training_acc": 53.0, "val_loss": 17.293962836265564, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.06025004386902, "training_acc": 53.0, "val_loss": 17.293529212474823, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.03078150749207, "training_acc": 53.0, "val_loss": 17.29333996772766, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.04705548286438, "training_acc": 53.0, "val_loss": 17.293646931648254, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.07225751876831, "training_acc": 53.0, "val_loss": 17.293749749660492, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.06310081481934, "training_acc": 53.0, "val_loss": 17.294028401374817, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.04587697982788, "training_acc": 53.0, "val_loss": 17.29537546634674, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.12474727630615, "training_acc": 53.0, "val_loss": 17.29748696088791, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.09352397918701, "training_acc": 53.0, "val_loss": 17.29891151189804, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.06934428215027, "training_acc": 53.0, "val_loss": 17.299310863018036, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.09533190727234, "training_acc": 53.0, "val_loss": 17.299552261829376, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.10309314727783, "training_acc": 53.0, "val_loss": 17.29971617460251, "val_acc": 52.0}
