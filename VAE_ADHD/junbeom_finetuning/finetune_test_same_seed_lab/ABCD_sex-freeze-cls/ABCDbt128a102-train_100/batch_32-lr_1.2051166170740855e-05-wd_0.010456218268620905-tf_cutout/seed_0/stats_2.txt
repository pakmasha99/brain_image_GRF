"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.21285390853882, "training_acc": 53.0, "val_loss": 17.35021024942398, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.2249903678894, "training_acc": 53.0, "val_loss": 17.35207587480545, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.19067478179932, "training_acc": 53.0, "val_loss": 17.350737750530243, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.16811180114746, "training_acc": 53.0, "val_loss": 17.34912395477295, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.20308446884155, "training_acc": 53.0, "val_loss": 17.34759509563446, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.16428518295288, "training_acc": 53.0, "val_loss": 17.346270382404327, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.18848085403442, "training_acc": 53.0, "val_loss": 17.344573140144348, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.15668106079102, "training_acc": 53.0, "val_loss": 17.34381467103958, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.1859359741211, "training_acc": 53.0, "val_loss": 17.342928051948547, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.16891717910767, "training_acc": 53.0, "val_loss": 17.342109978199005, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.1861720085144, "training_acc": 53.0, "val_loss": 17.341499030590057, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.15470814704895, "training_acc": 53.0, "val_loss": 17.341193556785583, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.20403623580933, "training_acc": 53.0, "val_loss": 17.341342568397522, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.1816930770874, "training_acc": 53.0, "val_loss": 17.34151840209961, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.19036722183228, "training_acc": 53.0, "val_loss": 17.340901494026184, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.19560766220093, "training_acc": 53.0, "val_loss": 17.340362071990967, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.1917839050293, "training_acc": 53.0, "val_loss": 17.339976131916046, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.20289134979248, "training_acc": 53.0, "val_loss": 17.3396036028862, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.17003846168518, "training_acc": 53.0, "val_loss": 17.33941286802292, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.17692375183105, "training_acc": 53.0, "val_loss": 17.339469492435455, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.1597511768341, "training_acc": 53.0, "val_loss": 17.339687049388885, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.18647980690002, "training_acc": 53.0, "val_loss": 17.340083420276642, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.2085132598877, "training_acc": 53.0, "val_loss": 17.340070009231567, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.15490818023682, "training_acc": 53.0, "val_loss": 17.339785397052765, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.14809012413025, "training_acc": 53.0, "val_loss": 17.339399456977844, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.18760561943054, "training_acc": 53.0, "val_loss": 17.339052259922028, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.1872010231018, "training_acc": 53.0, "val_loss": 17.33890473842621, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.1826753616333, "training_acc": 53.0, "val_loss": 17.338767647743225, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.2172303199768, "training_acc": 53.0, "val_loss": 17.33873188495636, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.21446204185486, "training_acc": 53.0, "val_loss": 17.33875274658203, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.23236918449402, "training_acc": 53.0, "val_loss": 17.33890473842621, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.19274187088013, "training_acc": 53.0, "val_loss": 17.339102923870087, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.18643474578857, "training_acc": 53.0, "val_loss": 17.339356243610382, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.18174600601196, "training_acc": 53.0, "val_loss": 17.3397496342659, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.22750616073608, "training_acc": 53.0, "val_loss": 17.340615391731262, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.17800951004028, "training_acc": 53.0, "val_loss": 17.34195053577423, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.19371032714844, "training_acc": 53.0, "val_loss": 17.343086004257202, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.21654415130615, "training_acc": 53.0, "val_loss": 17.344270646572113, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.17990803718567, "training_acc": 53.0, "val_loss": 17.34423190355301, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.20286774635315, "training_acc": 53.0, "val_loss": 17.34318435192108, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.16072726249695, "training_acc": 53.0, "val_loss": 17.342692613601685, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.18264174461365, "training_acc": 53.0, "val_loss": 17.342792451381683, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.15666246414185, "training_acc": 53.0, "val_loss": 17.343387007713318, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.21888184547424, "training_acc": 53.0, "val_loss": 17.34401285648346, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.15509128570557, "training_acc": 53.0, "val_loss": 17.343875765800476, "val_acc": 52.0}
{"epoch": 45, "training_loss": 69.15733098983765, "training_acc": 53.0, "val_loss": 17.34403520822525, "val_acc": 52.0}
{"epoch": 46, "training_loss": 69.1751618385315, "training_acc": 53.0, "val_loss": 17.344458401203156, "val_acc": 52.0}
{"epoch": 47, "training_loss": 69.173410654068, "training_acc": 53.0, "val_loss": 17.344796657562256, "val_acc": 52.0}
