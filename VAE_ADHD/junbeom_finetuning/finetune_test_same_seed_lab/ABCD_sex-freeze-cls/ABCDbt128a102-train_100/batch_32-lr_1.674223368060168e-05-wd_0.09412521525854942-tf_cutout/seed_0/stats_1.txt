"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 70.35326528549194, "training_acc": 47.0, "val_loss": 17.52931922674179, "val_acc": 52.0}
{"epoch": 1, "training_loss": 70.20574045181274, "training_acc": 47.0, "val_loss": 17.505116760730743, "val_acc": 52.0}
{"epoch": 2, "training_loss": 70.11169624328613, "training_acc": 47.0, "val_loss": 17.484180629253387, "val_acc": 52.0}
{"epoch": 3, "training_loss": 70.05069971084595, "training_acc": 47.0, "val_loss": 17.470373213291168, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.97386074066162, "training_acc": 47.0, "val_loss": 17.455342411994934, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.88521099090576, "training_acc": 47.0, "val_loss": 17.44626611471176, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.90764856338501, "training_acc": 47.0, "val_loss": 17.43636727333069, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.81355667114258, "training_acc": 47.0, "val_loss": 17.429743707180023, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.7623381614685, "training_acc": 47.0, "val_loss": 17.423193156719208, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.72888946533203, "training_acc": 47.0, "val_loss": 17.414814233779907, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.70531749725342, "training_acc": 47.0, "val_loss": 17.406432330608368, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.64922666549683, "training_acc": 47.0, "val_loss": 17.396968603134155, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.62892484664917, "training_acc": 47.0, "val_loss": 17.38785356283188, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.54516363143921, "training_acc": 47.0, "val_loss": 17.37787425518036, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.49048519134521, "training_acc": 47.0, "val_loss": 17.36791580915451, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.52571105957031, "training_acc": 47.0, "val_loss": 17.36101508140564, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.47294616699219, "training_acc": 45.0, "val_loss": 17.357461154460907, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.44319009780884, "training_acc": 46.0, "val_loss": 17.354072630405426, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.38490605354309, "training_acc": 49.0, "val_loss": 17.350737750530243, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.34832286834717, "training_acc": 48.0, "val_loss": 17.347033321857452, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.33253598213196, "training_acc": 50.0, "val_loss": 17.345039546489716, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.3476333618164, "training_acc": 52.0, "val_loss": 17.343032360076904, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.35312461853027, "training_acc": 49.0, "val_loss": 17.339801788330078, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.34780144691467, "training_acc": 51.0, "val_loss": 17.337603867053986, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.31221103668213, "training_acc": 51.0, "val_loss": 17.33718514442444, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.29181575775146, "training_acc": 52.0, "val_loss": 17.33616590499878, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.27205324172974, "training_acc": 52.0, "val_loss": 17.33420193195343, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.26619482040405, "training_acc": 53.0, "val_loss": 17.33189970254898, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.22698450088501, "training_acc": 53.0, "val_loss": 17.331428825855255, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.26526689529419, "training_acc": 53.0, "val_loss": 17.33059138059616, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.24431538581848, "training_acc": 53.0, "val_loss": 17.32993721961975, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.24759101867676, "training_acc": 53.0, "val_loss": 17.329488694667816, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.2014753818512, "training_acc": 53.0, "val_loss": 17.32902228832245, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.21022891998291, "training_acc": 53.0, "val_loss": 17.329032719135284, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.21304702758789, "training_acc": 53.0, "val_loss": 17.328952252864838, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.18698143959045, "training_acc": 53.0, "val_loss": 17.32887625694275, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.17837142944336, "training_acc": 53.0, "val_loss": 17.328830063343048, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.18042421340942, "training_acc": 53.0, "val_loss": 17.328789830207825, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.22654438018799, "training_acc": 53.0, "val_loss": 17.328724265098572, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.19411659240723, "training_acc": 53.0, "val_loss": 17.32873171567917, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.21125912666321, "training_acc": 53.0, "val_loss": 17.328771948814392, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.20087170600891, "training_acc": 53.0, "val_loss": 17.328758537769318, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.19903802871704, "training_acc": 53.0, "val_loss": 17.328914999961853, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.20883679389954, "training_acc": 53.0, "val_loss": 17.329075932502747, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.21705961227417, "training_acc": 53.0, "val_loss": 17.328912019729614, "val_acc": 52.0}
{"epoch": 45, "training_loss": 69.16153979301453, "training_acc": 53.0, "val_loss": 17.32902079820633, "val_acc": 52.0}
{"epoch": 46, "training_loss": 69.18157982826233, "training_acc": 53.0, "val_loss": 17.329441010951996, "val_acc": 52.0}
{"epoch": 47, "training_loss": 69.203453540802, "training_acc": 53.0, "val_loss": 17.329774796962738, "val_acc": 52.0}
{"epoch": 48, "training_loss": 69.25873613357544, "training_acc": 53.0, "val_loss": 17.330121994018555, "val_acc": 52.0}
{"epoch": 49, "training_loss": 69.20726919174194, "training_acc": 53.0, "val_loss": 17.33059287071228, "val_acc": 52.0}
{"epoch": 50, "training_loss": 69.25407981872559, "training_acc": 53.0, "val_loss": 17.330913245677948, "val_acc": 52.0}
{"epoch": 51, "training_loss": 69.25488591194153, "training_acc": 53.0, "val_loss": 17.330388724803925, "val_acc": 52.0}
{"epoch": 52, "training_loss": 69.15370392799377, "training_acc": 53.0, "val_loss": 17.329642176628113, "val_acc": 52.0}
{"epoch": 53, "training_loss": 69.22794580459595, "training_acc": 53.0, "val_loss": 17.329107224941254, "val_acc": 52.0}
{"epoch": 54, "training_loss": 69.20434808731079, "training_acc": 53.0, "val_loss": 17.328909039497375, "val_acc": 52.0}
{"epoch": 55, "training_loss": 69.20022106170654, "training_acc": 53.0, "val_loss": 17.32889711856842, "val_acc": 52.0}
{"epoch": 56, "training_loss": 69.19972324371338, "training_acc": 53.0, "val_loss": 17.328937351703644, "val_acc": 52.0}
{"epoch": 57, "training_loss": 69.21598625183105, "training_acc": 53.0, "val_loss": 17.32897460460663, "val_acc": 52.0}
