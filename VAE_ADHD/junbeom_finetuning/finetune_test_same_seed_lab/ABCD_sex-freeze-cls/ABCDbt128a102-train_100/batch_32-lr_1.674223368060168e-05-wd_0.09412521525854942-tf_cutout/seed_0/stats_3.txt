"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.07011604309082, "training_acc": 53.0, "val_loss": 17.30949729681015, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.03011345863342, "training_acc": 53.0, "val_loss": 17.3094242811203, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.06238865852356, "training_acc": 53.0, "val_loss": 17.308883368968964, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.03864336013794, "training_acc": 53.0, "val_loss": 17.308765649795532, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.06613898277283, "training_acc": 53.0, "val_loss": 17.30867326259613, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.06155157089233, "training_acc": 53.0, "val_loss": 17.308734357357025, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.09779572486877, "training_acc": 53.0, "val_loss": 17.30913072824478, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.05302047729492, "training_acc": 53.0, "val_loss": 17.309686541557312, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.03874564170837, "training_acc": 53.0, "val_loss": 17.31003224849701, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.04534673690796, "training_acc": 53.0, "val_loss": 17.309953272342682, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.07931780815125, "training_acc": 53.0, "val_loss": 17.309869825839996, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.08975672721863, "training_acc": 53.0, "val_loss": 17.310184240341187, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.01593685150146, "training_acc": 53.0, "val_loss": 17.310088872909546, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.07347631454468, "training_acc": 53.0, "val_loss": 17.309509217739105, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.05309438705444, "training_acc": 53.0, "val_loss": 17.308905720710754, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.00963997840881, "training_acc": 53.0, "val_loss": 17.30876713991165, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.05693078041077, "training_acc": 53.0, "val_loss": 17.3087015748024, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.06885957717896, "training_acc": 53.0, "val_loss": 17.308740317821503, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.04212713241577, "training_acc": 53.0, "val_loss": 17.30884462594986, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.06365752220154, "training_acc": 53.0, "val_loss": 17.309094965457916, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.04718661308289, "training_acc": 53.0, "val_loss": 17.309147119522095, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.060218334198, "training_acc": 53.0, "val_loss": 17.309558391571045, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.04429292678833, "training_acc": 53.0, "val_loss": 17.310190200805664, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.04046630859375, "training_acc": 53.0, "val_loss": 17.31007695198059, "val_acc": 52.0}
