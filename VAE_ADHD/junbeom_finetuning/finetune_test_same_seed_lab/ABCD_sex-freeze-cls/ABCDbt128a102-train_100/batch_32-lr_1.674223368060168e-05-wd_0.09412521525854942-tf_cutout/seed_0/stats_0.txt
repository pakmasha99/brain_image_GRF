"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.2593162059784, "training_acc": 52.0, "val_loss": 17.235906422138214, "val_acc": 56.0}
{"epoch": 1, "training_loss": 69.28624296188354, "training_acc": 52.0, "val_loss": 17.22910702228546, "val_acc": 56.0}
{"epoch": 2, "training_loss": 69.27574515342712, "training_acc": 52.0, "val_loss": 17.227837443351746, "val_acc": 56.0}
{"epoch": 3, "training_loss": 69.22167348861694, "training_acc": 52.0, "val_loss": 17.221233248710632, "val_acc": 56.0}
{"epoch": 4, "training_loss": 69.28199195861816, "training_acc": 52.0, "val_loss": 17.222242057323456, "val_acc": 56.0}
{"epoch": 5, "training_loss": 69.22987079620361, "training_acc": 52.0, "val_loss": 17.218105494976044, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.23111701011658, "training_acc": 52.0, "val_loss": 17.21627712249756, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.26336741447449, "training_acc": 52.0, "val_loss": 17.215751111507416, "val_acc": 56.0}
{"epoch": 8, "training_loss": 69.22287607192993, "training_acc": 52.0, "val_loss": 17.21367985010147, "val_acc": 56.0}
{"epoch": 9, "training_loss": 69.28430819511414, "training_acc": 52.0, "val_loss": 17.21661686897278, "val_acc": 56.0}
{"epoch": 10, "training_loss": 69.25532245635986, "training_acc": 52.0, "val_loss": 17.22375899553299, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.23163270950317, "training_acc": 52.0, "val_loss": 17.22354143857956, "val_acc": 56.0}
{"epoch": 12, "training_loss": 69.2567400932312, "training_acc": 52.0, "val_loss": 17.216771841049194, "val_acc": 56.0}
{"epoch": 13, "training_loss": 69.22254347801208, "training_acc": 52.0, "val_loss": 17.212891578674316, "val_acc": 56.0}
{"epoch": 14, "training_loss": 69.26235580444336, "training_acc": 52.0, "val_loss": 17.207421362400055, "val_acc": 56.0}
{"epoch": 15, "training_loss": 69.23192834854126, "training_acc": 52.0, "val_loss": 17.20280647277832, "val_acc": 56.0}
{"epoch": 16, "training_loss": 69.29854679107666, "training_acc": 52.0, "val_loss": 17.19573438167572, "val_acc": 56.0}
{"epoch": 17, "training_loss": 69.2430968284607, "training_acc": 52.0, "val_loss": 17.195266485214233, "val_acc": 56.0}
{"epoch": 18, "training_loss": 69.22395396232605, "training_acc": 52.0, "val_loss": 17.198486626148224, "val_acc": 56.0}
{"epoch": 19, "training_loss": 69.22410106658936, "training_acc": 52.0, "val_loss": 17.202983796596527, "val_acc": 56.0}
{"epoch": 20, "training_loss": 69.26228857040405, "training_acc": 52.0, "val_loss": 17.204104363918304, "val_acc": 56.0}
{"epoch": 21, "training_loss": 69.26458644866943, "training_acc": 52.0, "val_loss": 17.205309867858887, "val_acc": 56.0}
{"epoch": 22, "training_loss": 69.28958320617676, "training_acc": 52.0, "val_loss": 17.20643639564514, "val_acc": 56.0}
{"epoch": 23, "training_loss": 69.23890733718872, "training_acc": 52.0, "val_loss": 17.20876693725586, "val_acc": 56.0}
{"epoch": 24, "training_loss": 69.24443316459656, "training_acc": 52.0, "val_loss": 17.20600426197052, "val_acc": 56.0}
{"epoch": 25, "training_loss": 69.2304253578186, "training_acc": 52.0, "val_loss": 17.207534611225128, "val_acc": 56.0}
{"epoch": 26, "training_loss": 69.2362904548645, "training_acc": 52.0, "val_loss": 17.207904160022736, "val_acc": 56.0}
{"epoch": 27, "training_loss": 69.22789549827576, "training_acc": 52.0, "val_loss": 17.20934510231018, "val_acc": 56.0}
{"epoch": 28, "training_loss": 69.24740552902222, "training_acc": 52.0, "val_loss": 17.210087180137634, "val_acc": 56.0}
{"epoch": 29, "training_loss": 69.23698019981384, "training_acc": 52.0, "val_loss": 17.206336557865143, "val_acc": 56.0}
{"epoch": 30, "training_loss": 69.28392124176025, "training_acc": 52.0, "val_loss": 17.19791740179062, "val_acc": 56.0}
{"epoch": 31, "training_loss": 69.27941274642944, "training_acc": 52.0, "val_loss": 17.190571129322052, "val_acc": 56.0}
{"epoch": 32, "training_loss": 69.27289915084839, "training_acc": 52.0, "val_loss": 17.18103587627411, "val_acc": 56.0}
{"epoch": 33, "training_loss": 69.25018167495728, "training_acc": 52.0, "val_loss": 17.17502921819687, "val_acc": 56.0}
{"epoch": 34, "training_loss": 69.26791572570801, "training_acc": 52.0, "val_loss": 17.17372238636017, "val_acc": 56.0}
{"epoch": 35, "training_loss": 69.29384756088257, "training_acc": 52.0, "val_loss": 17.175281047821045, "val_acc": 56.0}
{"epoch": 36, "training_loss": 69.29525136947632, "training_acc": 52.0, "val_loss": 17.17446744441986, "val_acc": 56.0}
{"epoch": 37, "training_loss": 69.24943852424622, "training_acc": 52.0, "val_loss": 17.173095047473907, "val_acc": 56.0}
{"epoch": 38, "training_loss": 69.27286148071289, "training_acc": 52.0, "val_loss": 17.17366874217987, "val_acc": 56.0}
{"epoch": 39, "training_loss": 69.31564950942993, "training_acc": 52.0, "val_loss": 17.176471650600433, "val_acc": 56.0}
{"epoch": 40, "training_loss": 69.28627324104309, "training_acc": 52.0, "val_loss": 17.180976271629333, "val_acc": 56.0}
{"epoch": 41, "training_loss": 69.2334451675415, "training_acc": 52.0, "val_loss": 17.18316674232483, "val_acc": 56.0}
{"epoch": 42, "training_loss": 69.26100254058838, "training_acc": 52.0, "val_loss": 17.1867772936821, "val_acc": 56.0}
{"epoch": 43, "training_loss": 69.22121953964233, "training_acc": 52.0, "val_loss": 17.190589010715485, "val_acc": 56.0}
{"epoch": 44, "training_loss": 69.23404812812805, "training_acc": 52.0, "val_loss": 17.196880280971527, "val_acc": 56.0}
{"epoch": 45, "training_loss": 69.25854635238647, "training_acc": 52.0, "val_loss": 17.20113307237625, "val_acc": 56.0}
{"epoch": 46, "training_loss": 69.24822878837585, "training_acc": 52.0, "val_loss": 17.203830182552338, "val_acc": 56.0}
{"epoch": 47, "training_loss": 69.23232364654541, "training_acc": 52.0, "val_loss": 17.203795909881592, "val_acc": 56.0}
{"epoch": 48, "training_loss": 69.25379204750061, "training_acc": 52.0, "val_loss": 17.204585671424866, "val_acc": 56.0}
{"epoch": 49, "training_loss": 69.22749257087708, "training_acc": 52.0, "val_loss": 17.2087624669075, "val_acc": 56.0}
{"epoch": 50, "training_loss": 69.26915884017944, "training_acc": 52.0, "val_loss": 17.215906083583832, "val_acc": 56.0}
{"epoch": 51, "training_loss": 69.27795910835266, "training_acc": 52.0, "val_loss": 17.224811017513275, "val_acc": 56.0}
{"epoch": 52, "training_loss": 69.2628710269928, "training_acc": 52.0, "val_loss": 17.234142124652863, "val_acc": 56.0}
{"epoch": 53, "training_loss": 69.25901937484741, "training_acc": 52.0, "val_loss": 17.240341007709503, "val_acc": 56.0}
{"epoch": 54, "training_loss": 69.24771618843079, "training_acc": 52.0, "val_loss": 17.247051000595093, "val_acc": 56.0}
{"epoch": 55, "training_loss": 69.24655532836914, "training_acc": 52.0, "val_loss": 17.25885421037674, "val_acc": 56.0}
{"epoch": 56, "training_loss": 69.27652072906494, "training_acc": 52.0, "val_loss": 17.274461686611176, "val_acc": 56.0}
