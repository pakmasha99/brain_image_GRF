"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.1836371421814, "training_acc": 53.0, "val_loss": 17.261068522930145, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.20434856414795, "training_acc": 53.0, "val_loss": 17.2639399766922, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.13742399215698, "training_acc": 53.0, "val_loss": 17.264331877231598, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.17580699920654, "training_acc": 53.0, "val_loss": 17.264480888843536, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.18683815002441, "training_acc": 53.0, "val_loss": 17.266330122947693, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.15765452384949, "training_acc": 53.0, "val_loss": 17.266826331615448, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.16573715209961, "training_acc": 53.0, "val_loss": 17.26522594690323, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.17111158370972, "training_acc": 53.0, "val_loss": 17.263713479042053, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.16787385940552, "training_acc": 53.0, "val_loss": 17.263908684253693, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.18427467346191, "training_acc": 53.0, "val_loss": 17.26338565349579, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.13769006729126, "training_acc": 53.0, "val_loss": 17.262043058872223, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.16133451461792, "training_acc": 53.0, "val_loss": 17.26147085428238, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.14634323120117, "training_acc": 53.0, "val_loss": 17.259737849235535, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.18934535980225, "training_acc": 53.0, "val_loss": 17.258521914482117, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.18454790115356, "training_acc": 53.0, "val_loss": 17.257434129714966, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.1852958202362, "training_acc": 53.0, "val_loss": 17.25614368915558, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.18130111694336, "training_acc": 53.0, "val_loss": 17.255376279354095, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.177659034729, "training_acc": 53.0, "val_loss": 17.255572974681854, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.17942810058594, "training_acc": 53.0, "val_loss": 17.255689203739166, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.16569471359253, "training_acc": 53.0, "val_loss": 17.256000638008118, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.16005635261536, "training_acc": 53.0, "val_loss": 17.25667119026184, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.11394095420837, "training_acc": 53.0, "val_loss": 17.256733775138855, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.15847396850586, "training_acc": 53.0, "val_loss": 17.25587248802185, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.19380521774292, "training_acc": 53.0, "val_loss": 17.255334556102753, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.12793636322021, "training_acc": 53.0, "val_loss": 17.254921793937683, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.14055156707764, "training_acc": 53.0, "val_loss": 17.254462838172913, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.15984106063843, "training_acc": 53.0, "val_loss": 17.254123091697693, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.16810727119446, "training_acc": 53.0, "val_loss": 17.254047095775604, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.2268533706665, "training_acc": 53.0, "val_loss": 17.254188656806946, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.21287965774536, "training_acc": 53.0, "val_loss": 17.254382371902466, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.21865439414978, "training_acc": 53.0, "val_loss": 17.25464016199112, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.25046920776367, "training_acc": 53.0, "val_loss": 17.254294455051422, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.2123191356659, "training_acc": 53.0, "val_loss": 17.25410670042038, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.19165086746216, "training_acc": 53.0, "val_loss": 17.254067957401276, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.16725206375122, "training_acc": 53.0, "val_loss": 17.254088819026947, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.16090393066406, "training_acc": 53.0, "val_loss": 17.25425273180008, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.2298231124878, "training_acc": 53.0, "val_loss": 17.254114151000977, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.19218492507935, "training_acc": 53.0, "val_loss": 17.254048585891724, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.18532609939575, "training_acc": 53.0, "val_loss": 17.25415140390396, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.20568799972534, "training_acc": 53.0, "val_loss": 17.25439429283142, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.19530296325684, "training_acc": 53.0, "val_loss": 17.255134880542755, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.22893071174622, "training_acc": 53.0, "val_loss": 17.25545972585678, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.20471239089966, "training_acc": 53.0, "val_loss": 17.25601702928543, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.2226631641388, "training_acc": 53.0, "val_loss": 17.25667119026184, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.22484970092773, "training_acc": 53.0, "val_loss": 17.257145047187805, "val_acc": 52.0}
{"epoch": 45, "training_loss": 69.22289419174194, "training_acc": 53.0, "val_loss": 17.257140576839447, "val_acc": 52.0}
{"epoch": 46, "training_loss": 69.21792650222778, "training_acc": 53.0, "val_loss": 17.256392538547516, "val_acc": 52.0}
