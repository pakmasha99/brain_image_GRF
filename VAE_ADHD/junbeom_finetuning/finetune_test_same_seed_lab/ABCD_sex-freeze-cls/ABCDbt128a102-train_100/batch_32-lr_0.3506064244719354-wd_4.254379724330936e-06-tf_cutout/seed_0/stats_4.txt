"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 2979.2650051116943, "training_acc": 51.0, "val_loss": 1118.559169769287, "val_acc": 48.0}
{"epoch": 1, "training_loss": 2930.5580406188965, "training_acc": 45.0, "val_loss": 677.0800113677979, "val_acc": 52.0}
{"epoch": 2, "training_loss": 1525.1634407043457, "training_acc": 52.0, "val_loss": 350.04239082336426, "val_acc": 48.0}
{"epoch": 3, "training_loss": 1141.8944969177246, "training_acc": 49.0, "val_loss": 603.9485931396484, "val_acc": 52.0}
{"epoch": 4, "training_loss": 1291.6106986999512, "training_acc": 55.0, "val_loss": 864.9970054626465, "val_acc": 48.0}
{"epoch": 5, "training_loss": 3481.7451705932617, "training_acc": 47.0, "val_loss": 74.45229291915894, "val_acc": 48.0}
{"epoch": 6, "training_loss": 1897.075325012207, "training_acc": 45.0, "val_loss": 869.2104339599609, "val_acc": 52.0}
{"epoch": 7, "training_loss": 2617.464241027832, "training_acc": 53.0, "val_loss": 534.7397327423096, "val_acc": 48.0}
{"epoch": 8, "training_loss": 3239.242935180664, "training_acc": 47.0, "val_loss": 548.8292217254639, "val_acc": 48.0}
{"epoch": 9, "training_loss": 1284.93896484375, "training_acc": 57.0, "val_loss": 953.3971786499023, "val_acc": 52.0}
{"epoch": 10, "training_loss": 3312.576835632324, "training_acc": 53.0, "val_loss": 81.1360776424408, "val_acc": 48.0}
{"epoch": 11, "training_loss": 979.894229888916, "training_acc": 47.0, "val_loss": 200.16722679138184, "val_acc": 52.0}
{"epoch": 12, "training_loss": 1240.134147644043, "training_acc": 53.0, "val_loss": 70.43579816818237, "val_acc": 48.0}
{"epoch": 13, "training_loss": 505.8911762237549, "training_acc": 49.0, "val_loss": 301.6400098800659, "val_acc": 52.0}
{"epoch": 14, "training_loss": 845.1357803344727, "training_acc": 53.0, "val_loss": 418.3215618133545, "val_acc": 48.0}
{"epoch": 15, "training_loss": 1183.763500213623, "training_acc": 45.0, "val_loss": 461.84496879577637, "val_acc": 52.0}
{"epoch": 16, "training_loss": 1370.9776802062988, "training_acc": 53.0, "val_loss": 316.93246364593506, "val_acc": 48.0}
{"epoch": 17, "training_loss": 919.4597473144531, "training_acc": 49.0, "val_loss": 507.716703414917, "val_acc": 52.0}
{"epoch": 18, "training_loss": 2183.816665649414, "training_acc": 53.0, "val_loss": 33.500778675079346, "val_acc": 48.0}
{"epoch": 19, "training_loss": 375.9043426513672, "training_acc": 45.0, "val_loss": 96.03880047798157, "val_acc": 52.0}
{"epoch": 20, "training_loss": 451.49733805656433, "training_acc": 51.0, "val_loss": 123.50168228149414, "val_acc": 52.0}
{"epoch": 21, "training_loss": 481.8789596557617, "training_acc": 51.0, "val_loss": 25.51427185535431, "val_acc": 52.0}
{"epoch": 22, "training_loss": 311.54290771484375, "training_acc": 51.0, "val_loss": 149.248468875885, "val_acc": 48.0}
{"epoch": 23, "training_loss": 496.8366689682007, "training_acc": 50.0, "val_loss": 207.35275745391846, "val_acc": 48.0}
{"epoch": 24, "training_loss": 643.4531383514404, "training_acc": 49.0, "val_loss": 461.1018180847168, "val_acc": 52.0}
{"epoch": 25, "training_loss": 1638.5608983039856, "training_acc": 53.0, "val_loss": 311.1041307449341, "val_acc": 48.0}
{"epoch": 26, "training_loss": 1338.2254054546356, "training_acc": 46.0, "val_loss": 107.79762268066406, "val_acc": 52.0}
{"epoch": 27, "training_loss": 382.90111923217773, "training_acc": 53.0, "val_loss": 102.26682424545288, "val_acc": 52.0}
{"epoch": 28, "training_loss": 428.41063690185547, "training_acc": 51.0, "val_loss": 181.6945195198059, "val_acc": 48.0}
{"epoch": 29, "training_loss": 378.34436988830566, "training_acc": 57.0, "val_loss": 264.7660732269287, "val_acc": 48.0}
{"epoch": 30, "training_loss": 1152.632574558258, "training_acc": 48.0, "val_loss": 202.71146297454834, "val_acc": 52.0}
{"epoch": 31, "training_loss": 665.4156951904297, "training_acc": 49.0, "val_loss": 195.02358436584473, "val_acc": 48.0}
{"epoch": 32, "training_loss": 441.9594347476959, "training_acc": 50.0, "val_loss": 20.653679966926575, "val_acc": 56.0}
{"epoch": 33, "training_loss": 131.5043430328369, "training_acc": 54.0, "val_loss": 51.964348554611206, "val_acc": 52.0}
{"epoch": 34, "training_loss": 154.3603596687317, "training_acc": 56.0, "val_loss": 86.22655868530273, "val_acc": 52.0}
{"epoch": 35, "training_loss": 190.8879566192627, "training_acc": 56.0, "val_loss": 94.0537691116333, "val_acc": 52.0}
{"epoch": 36, "training_loss": 318.15136432647705, "training_acc": 51.0, "val_loss": 187.78226375579834, "val_acc": 52.0}
{"epoch": 37, "training_loss": 724.8375701904297, "training_acc": 43.0, "val_loss": 73.6614465713501, "val_acc": 48.0}
{"epoch": 38, "training_loss": 1207.1056747436523, "training_acc": 41.0, "val_loss": 150.4223346710205, "val_acc": 52.0}
{"epoch": 39, "training_loss": 619.5107116699219, "training_acc": 61.0, "val_loss": 90.77955484390259, "val_acc": 48.0}
{"epoch": 40, "training_loss": 906.2347831726074, "training_acc": 49.0, "val_loss": 59.026867151260376, "val_acc": 52.0}
{"epoch": 41, "training_loss": 789.6820640563965, "training_acc": 59.0, "val_loss": 180.9043526649475, "val_acc": 48.0}
{"epoch": 42, "training_loss": 731.4921245574951, "training_acc": 49.0, "val_loss": 22.924289107322693, "val_acc": 52.0}
{"epoch": 43, "training_loss": 623.2569806575775, "training_acc": 51.0, "val_loss": 164.83646631240845, "val_acc": 52.0}
{"epoch": 44, "training_loss": 510.84348368641804, "training_acc": 57.0, "val_loss": 457.355260848999, "val_acc": 48.0}
{"epoch": 45, "training_loss": 1709.3933715820312, "training_acc": 47.0, "val_loss": 526.1218070983887, "val_acc": 52.0}
{"epoch": 46, "training_loss": 3218.1214599609375, "training_acc": 53.0, "val_loss": 821.7397689819336, "val_acc": 52.0}
{"epoch": 47, "training_loss": 1619.7935581207275, "training_acc": 53.0, "val_loss": 481.3448429107666, "val_acc": 48.0}
{"epoch": 48, "training_loss": 1196.0038013458252, "training_acc": 49.0, "val_loss": 604.5342922210693, "val_acc": 52.0}
{"epoch": 49, "training_loss": 2537.1765174865723, "training_acc": 53.0, "val_loss": 147.07038402557373, "val_acc": 52.0}
{"epoch": 50, "training_loss": 1280.6766090393066, "training_acc": 45.0, "val_loss": 340.6670570373535, "val_acc": 48.0}
{"epoch": 51, "training_loss": 1110.1965789794922, "training_acc": 47.0, "val_loss": 544.1752910614014, "val_acc": 52.0}
