"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 3130.2977142333984, "training_acc": 48.0, "val_loss": 1297.7506637573242, "val_acc": 56.0}
{"epoch": 1, "training_loss": 4661.86697769165, "training_acc": 52.0, "val_loss": 797.7404117584229, "val_acc": 44.0}
{"epoch": 2, "training_loss": 4532.836700439453, "training_acc": 48.0, "val_loss": 647.3884105682373, "val_acc": 44.0}
{"epoch": 3, "training_loss": 1692.3079986572266, "training_acc": 50.0, "val_loss": 395.5958843231201, "val_acc": 56.0}
{"epoch": 4, "training_loss": 1383.2992324829102, "training_acc": 50.0, "val_loss": 702.2555351257324, "val_acc": 44.0}
{"epoch": 5, "training_loss": 1520.4750518798828, "training_acc": 44.0, "val_loss": 213.3272409439087, "val_acc": 56.0}
{"epoch": 6, "training_loss": 793.3205261230469, "training_acc": 50.0, "val_loss": 296.14083766937256, "val_acc": 44.0}
{"epoch": 7, "training_loss": 818.6524276733398, "training_acc": 52.0, "val_loss": 471.9226360321045, "val_acc": 56.0}
{"epoch": 8, "training_loss": 1253.08158493042, "training_acc": 52.0, "val_loss": 623.9330768585205, "val_acc": 44.0}
{"epoch": 9, "training_loss": 2321.4588335751105, "training_acc": 48.0, "val_loss": 136.71005964279175, "val_acc": 56.0}
{"epoch": 10, "training_loss": 811.0731515884399, "training_acc": 50.0, "val_loss": 21.30204290151596, "val_acc": 44.0}
{"epoch": 11, "training_loss": 187.39981651306152, "training_acc": 50.0, "val_loss": 19.777362048625946, "val_acc": 56.0}
{"epoch": 12, "training_loss": 301.1936378479004, "training_acc": 52.0, "val_loss": 283.3298683166504, "val_acc": 44.0}
{"epoch": 13, "training_loss": 657.4995021820068, "training_acc": 52.0, "val_loss": 184.85066890716553, "val_acc": 56.0}
{"epoch": 14, "training_loss": 752.8078460693359, "training_acc": 48.0, "val_loss": 308.6336612701416, "val_acc": 44.0}
{"epoch": 15, "training_loss": 769.9774551391602, "training_acc": 54.0, "val_loss": 545.9833145141602, "val_acc": 56.0}
{"epoch": 16, "training_loss": 1978.626184463501, "training_acc": 52.0, "val_loss": 533.9016437530518, "val_acc": 44.0}
{"epoch": 17, "training_loss": 3047.0894165039062, "training_acc": 48.0, "val_loss": 743.2847023010254, "val_acc": 44.0}
{"epoch": 18, "training_loss": 1618.4057693481445, "training_acc": 50.0, "val_loss": 358.64410400390625, "val_acc": 56.0}
{"epoch": 19, "training_loss": 917.2337493896484, "training_acc": 52.0, "val_loss": 75.26728510856628, "val_acc": 44.0}
{"epoch": 20, "training_loss": 779.5367431640625, "training_acc": 52.0, "val_loss": 39.300280809402466, "val_acc": 56.0}
{"epoch": 21, "training_loss": 1225.2613677978516, "training_acc": 48.0, "val_loss": 234.74926948547363, "val_acc": 44.0}
{"epoch": 22, "training_loss": 1147.7643280029297, "training_acc": 46.0, "val_loss": 114.96864557266235, "val_acc": 56.0}
{"epoch": 23, "training_loss": 1023.2397613525391, "training_acc": 52.0, "val_loss": 99.03993010520935, "val_acc": 44.0}
{"epoch": 24, "training_loss": 1748.1330108642578, "training_acc": 44.0, "val_loss": 519.0974235534668, "val_acc": 56.0}
{"epoch": 25, "training_loss": 1346.7421340942383, "training_acc": 48.0, "val_loss": 479.8097610473633, "val_acc": 44.0}
{"epoch": 26, "training_loss": 1244.4269924163818, "training_acc": 38.0, "val_loss": 102.57313251495361, "val_acc": 56.0}
{"epoch": 27, "training_loss": 397.08667397499084, "training_acc": 52.0, "val_loss": 35.08341908454895, "val_acc": 56.0}
{"epoch": 28, "training_loss": 184.18062019348145, "training_acc": 54.0, "val_loss": 228.2900333404541, "val_acc": 56.0}
{"epoch": 29, "training_loss": 643.6501579284668, "training_acc": 48.0, "val_loss": 138.7947916984558, "val_acc": 56.0}
{"epoch": 30, "training_loss": 766.1464824676514, "training_acc": 50.0, "val_loss": 35.99151968955994, "val_acc": 44.0}
