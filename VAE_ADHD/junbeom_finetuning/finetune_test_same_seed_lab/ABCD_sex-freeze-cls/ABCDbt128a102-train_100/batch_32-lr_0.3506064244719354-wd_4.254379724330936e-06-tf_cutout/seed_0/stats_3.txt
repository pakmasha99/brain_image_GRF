"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 2660.263006210327, "training_acc": 53.0, "val_loss": 1387.479019165039, "val_acc": 48.0}
{"epoch": 1, "training_loss": 4036.3663635253906, "training_acc": 45.0, "val_loss": 673.4373569488525, "val_acc": 52.0}
{"epoch": 2, "training_loss": 1903.560115814209, "training_acc": 55.0, "val_loss": 536.7465496063232, "val_acc": 48.0}
{"epoch": 3, "training_loss": 1817.8345737457275, "training_acc": 47.0, "val_loss": 307.8871965408325, "val_acc": 52.0}
{"epoch": 4, "training_loss": 754.341100692749, "training_acc": 49.0, "val_loss": 55.34057021141052, "val_acc": 48.0}
{"epoch": 5, "training_loss": 569.2489967346191, "training_acc": 55.0, "val_loss": 255.57219982147217, "val_acc": 48.0}
{"epoch": 6, "training_loss": 1239.2167768478394, "training_acc": 47.0, "val_loss": 409.3860626220703, "val_acc": 52.0}
{"epoch": 7, "training_loss": 2259.5425872802734, "training_acc": 53.0, "val_loss": 310.79704761505127, "val_acc": 52.0}
{"epoch": 8, "training_loss": 1177.1931762695312, "training_acc": 49.0, "val_loss": 144.54870223999023, "val_acc": 48.0}
{"epoch": 9, "training_loss": 1290.5355415344238, "training_acc": 53.0, "val_loss": 529.9103736877441, "val_acc": 52.0}
{"epoch": 10, "training_loss": 1370.0444564819336, "training_acc": 43.0, "val_loss": 50.8206307888031, "val_acc": 48.0}
{"epoch": 11, "training_loss": 510.9757709503169, "training_acc": 61.0, "val_loss": 117.12648868560791, "val_acc": 52.0}
{"epoch": 12, "training_loss": 527.9646224975586, "training_acc": 45.0, "val_loss": 46.516090631484985, "val_acc": 52.0}
{"epoch": 13, "training_loss": 223.05536460876465, "training_acc": 51.0, "val_loss": 199.43227767944336, "val_acc": 52.0}
{"epoch": 14, "training_loss": 709.9858055114746, "training_acc": 49.0, "val_loss": 184.44002866744995, "val_acc": 48.0}
{"epoch": 15, "training_loss": 583.0446643829346, "training_acc": 51.0, "val_loss": 191.43656492233276, "val_acc": 48.0}
{"epoch": 16, "training_loss": 883.5228271484375, "training_acc": 47.0, "val_loss": 120.22100687026978, "val_acc": 52.0}
{"epoch": 17, "training_loss": 421.2707462310791, "training_acc": 47.0, "val_loss": 127.82270908355713, "val_acc": 52.0}
{"epoch": 18, "training_loss": 414.0599889755249, "training_acc": 55.0, "val_loss": 353.740930557251, "val_acc": 48.0}
{"epoch": 19, "training_loss": 977.0991973876953, "training_acc": 51.0, "val_loss": 567.7611351013184, "val_acc": 52.0}
{"epoch": 20, "training_loss": 2632.4606170654297, "training_acc": 53.0, "val_loss": 162.5712752342224, "val_acc": 52.0}
{"epoch": 21, "training_loss": 1740.699649810791, "training_acc": 45.0, "val_loss": 588.9029502868652, "val_acc": 48.0}
{"epoch": 22, "training_loss": 1562.5016784667969, "training_acc": 41.0, "val_loss": 246.5031862258911, "val_acc": 52.0}
{"epoch": 23, "training_loss": 909.1415138244629, "training_acc": 47.0, "val_loss": 427.29406356811523, "val_acc": 48.0}
{"epoch": 24, "training_loss": 1114.4819030761719, "training_acc": 49.0, "val_loss": 293.15428733825684, "val_acc": 52.0}
{"epoch": 25, "training_loss": 630.9441566467285, "training_acc": 61.0, "val_loss": 341.78926944732666, "val_acc": 48.0}
{"epoch": 26, "training_loss": 964.7403202056885, "training_acc": 47.0, "val_loss": 426.0894298553467, "val_acc": 52.0}
{"epoch": 27, "training_loss": 1045.0567092895508, "training_acc": 53.0, "val_loss": 485.8226776123047, "val_acc": 48.0}
{"epoch": 28, "training_loss": 1610.4596366882324, "training_acc": 47.0, "val_loss": 253.2233715057373, "val_acc": 52.0}
{"epoch": 29, "training_loss": 765.9113311767578, "training_acc": 51.0, "val_loss": 290.67797660827637, "val_acc": 48.0}
{"epoch": 30, "training_loss": 717.2459411621094, "training_acc": 51.0, "val_loss": 215.97230434417725, "val_acc": 52.0}
{"epoch": 31, "training_loss": 758.4164695739746, "training_acc": 49.0, "val_loss": 158.93932580947876, "val_acc": 48.0}
{"epoch": 32, "training_loss": 820.6275939941406, "training_acc": 51.0, "val_loss": 35.98213791847229, "val_acc": 52.0}
{"epoch": 33, "training_loss": 1075.4804763793945, "training_acc": 53.0, "val_loss": 99.94648694992065, "val_acc": 48.0}
{"epoch": 34, "training_loss": 1597.2315826416016, "training_acc": 49.0, "val_loss": 614.2749309539795, "val_acc": 52.0}
{"epoch": 35, "training_loss": 1266.2230186462402, "training_acc": 51.0, "val_loss": 296.2376594543457, "val_acc": 48.0}
{"epoch": 36, "training_loss": 888.4444160461426, "training_acc": 47.0, "val_loss": 282.234787940979, "val_acc": 52.0}
{"epoch": 37, "training_loss": 782.7234039306641, "training_acc": 53.0, "val_loss": 462.37378120422363, "val_acc": 48.0}
{"epoch": 38, "training_loss": 1249.607666015625, "training_acc": 51.0, "val_loss": 549.5154857635498, "val_acc": 52.0}
{"epoch": 39, "training_loss": 2405.9530334472656, "training_acc": 53.0, "val_loss": 138.57853412628174, "val_acc": 52.0}
{"epoch": 40, "training_loss": 1527.3094787597656, "training_acc": 51.0, "val_loss": 542.9101467132568, "val_acc": 48.0}
{"epoch": 41, "training_loss": 1719.9434432983398, "training_acc": 39.0, "val_loss": 675.9252548217773, "val_acc": 52.0}
{"epoch": 42, "training_loss": 2042.9927959442139, "training_acc": 53.0, "val_loss": 472.8066921234131, "val_acc": 48.0}
{"epoch": 43, "training_loss": 2713.477554321289, "training_acc": 47.0, "val_loss": 344.14684772491455, "val_acc": 48.0}
{"epoch": 44, "training_loss": 1172.3867797851562, "training_acc": 51.0, "val_loss": 435.14418601989746, "val_acc": 52.0}
{"epoch": 45, "training_loss": 1018.2500915527344, "training_acc": 51.0, "val_loss": 245.48394680023193, "val_acc": 48.0}
{"epoch": 46, "training_loss": 651.6677894592285, "training_acc": 54.0, "val_loss": 404.6180725097656, "val_acc": 52.0}
{"epoch": 47, "training_loss": 1029.9317779541016, "training_acc": 53.0, "val_loss": 433.4713935852051, "val_acc": 48.0}
{"epoch": 48, "training_loss": 1287.8829193115234, "training_acc": 45.0, "val_loss": 344.63517665863037, "val_acc": 52.0}
{"epoch": 49, "training_loss": 775.6711196899414, "training_acc": 53.0, "val_loss": 138.22283744812012, "val_acc": 48.0}
{"epoch": 50, "training_loss": 302.80817794799805, "training_acc": 49.0, "val_loss": 47.26232886314392, "val_acc": 48.0}
{"epoch": 51, "training_loss": 370.96208000183105, "training_acc": 57.0, "val_loss": 245.99363803863525, "val_acc": 48.0}
