"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 3133.720754623413, "training_acc": 51.0, "val_loss": 1079.079246520996, "val_acc": 48.0}
{"epoch": 1, "training_loss": 2369.678342819214, "training_acc": 49.0, "val_loss": 366.5468692779541, "val_acc": 52.0}
{"epoch": 2, "training_loss": 866.7594833374023, "training_acc": 59.0, "val_loss": 176.35549306869507, "val_acc": 48.0}
{"epoch": 3, "training_loss": 839.980052947998, "training_acc": 53.0, "val_loss": 123.89004230499268, "val_acc": 48.0}
{"epoch": 4, "training_loss": 360.09335248335265, "training_acc": 53.0, "val_loss": 146.32813930511475, "val_acc": 52.0}
{"epoch": 5, "training_loss": 347.0902729034424, "training_acc": 55.0, "val_loss": 24.8365581035614, "val_acc": 48.0}
{"epoch": 6, "training_loss": 201.9637050628662, "training_acc": 47.0, "val_loss": 51.27190351486206, "val_acc": 48.0}
{"epoch": 7, "training_loss": 366.7978820800781, "training_acc": 33.0, "val_loss": 29.44055199623108, "val_acc": 52.0}
{"epoch": 8, "training_loss": 294.6249370574951, "training_acc": 55.0, "val_loss": 154.40194606781006, "val_acc": 52.0}
{"epoch": 9, "training_loss": 415.6522674560547, "training_acc": 59.0, "val_loss": 23.62232208251953, "val_acc": 52.0}
{"epoch": 10, "training_loss": 410.18041229248047, "training_acc": 51.0, "val_loss": 121.82531356811523, "val_acc": 48.0}
{"epoch": 11, "training_loss": 296.63451290130615, "training_acc": 59.0, "val_loss": 18.53809505701065, "val_acc": 52.0}
{"epoch": 12, "training_loss": 213.38266563415527, "training_acc": 55.0, "val_loss": 231.27686977386475, "val_acc": 52.0}
{"epoch": 13, "training_loss": 894.854602098465, "training_acc": 55.0, "val_loss": 296.8585729598999, "val_acc": 48.0}
{"epoch": 14, "training_loss": 1291.666390657425, "training_acc": 49.0, "val_loss": 244.289231300354, "val_acc": 52.0}
{"epoch": 15, "training_loss": 720.2843132019043, "training_acc": 53.0, "val_loss": 358.60371589660645, "val_acc": 48.0}
{"epoch": 16, "training_loss": 957.0245742797852, "training_acc": 47.0, "val_loss": 49.27098751068115, "val_acc": 52.0}
{"epoch": 17, "training_loss": 1203.5420608520508, "training_acc": 39.0, "val_loss": 52.46073603630066, "val_acc": 52.0}
{"epoch": 18, "training_loss": 382.13317489624023, "training_acc": 53.0, "val_loss": 60.93379259109497, "val_acc": 48.0}
{"epoch": 19, "training_loss": 466.8749113082886, "training_acc": 47.0, "val_loss": 402.79459953308105, "val_acc": 48.0}
{"epoch": 20, "training_loss": 2244.4923973083496, "training_acc": 47.0, "val_loss": 113.86082172393799, "val_acc": 48.0}
{"epoch": 21, "training_loss": 1372.252700805664, "training_acc": 49.0, "val_loss": 589.691162109375, "val_acc": 52.0}
{"epoch": 22, "training_loss": 1520.2904586791992, "training_acc": 51.0, "val_loss": 410.03575325012207, "val_acc": 48.0}
{"epoch": 23, "training_loss": 1176.6796016693115, "training_acc": 51.0, "val_loss": 243.11656951904297, "val_acc": 52.0}
{"epoch": 24, "training_loss": 717.2566833496094, "training_acc": 53.0, "val_loss": 191.6908860206604, "val_acc": 48.0}
{"epoch": 25, "training_loss": 477.25891494750977, "training_acc": 63.0, "val_loss": 102.40887403488159, "val_acc": 48.0}
{"epoch": 26, "training_loss": 436.3689651489258, "training_acc": 47.0, "val_loss": 289.40086364746094, "val_acc": 52.0}
{"epoch": 27, "training_loss": 710.8563117980957, "training_acc": 51.0, "val_loss": 93.12900900840759, "val_acc": 52.0}
{"epoch": 28, "training_loss": 277.71453309059143, "training_acc": 51.0, "val_loss": 31.939470767974854, "val_acc": 44.0}
{"epoch": 29, "training_loss": 74.2050598859787, "training_acc": 67.0, "val_loss": 65.14425277709961, "val_acc": 52.0}
{"epoch": 30, "training_loss": 145.9936079978943, "training_acc": 56.0, "val_loss": 184.7314715385437, "val_acc": 48.0}
