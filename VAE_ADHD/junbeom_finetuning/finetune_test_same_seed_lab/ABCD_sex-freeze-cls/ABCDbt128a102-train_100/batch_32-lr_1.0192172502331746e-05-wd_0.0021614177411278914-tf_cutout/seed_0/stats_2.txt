"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.19721174240112, "training_acc": 53.0, "val_loss": 17.305931448936462, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.20396780967712, "training_acc": 53.0, "val_loss": 17.3064187169075, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.23144555091858, "training_acc": 53.0, "val_loss": 17.304107546806335, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.17535519599915, "training_acc": 53.0, "val_loss": 17.301395535469055, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.1669864654541, "training_acc": 53.0, "val_loss": 17.298884689807892, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.17749500274658, "training_acc": 53.0, "val_loss": 17.297226190567017, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.17015814781189, "training_acc": 53.0, "val_loss": 17.297126352787018, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.19506239891052, "training_acc": 53.0, "val_loss": 17.297104001045227, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.1530213356018, "training_acc": 53.0, "val_loss": 17.294688522815704, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.16347408294678, "training_acc": 53.0, "val_loss": 17.292433977127075, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.20094680786133, "training_acc": 53.0, "val_loss": 17.290565371513367, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.15683507919312, "training_acc": 53.0, "val_loss": 17.288659512996674, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.17242503166199, "training_acc": 53.0, "val_loss": 17.287394404411316, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.17554712295532, "training_acc": 53.0, "val_loss": 17.28711426258087, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.11788129806519, "training_acc": 53.0, "val_loss": 17.287425696849823, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.11893272399902, "training_acc": 53.0, "val_loss": 17.288166284561157, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.10295581817627, "training_acc": 53.0, "val_loss": 17.287641763687134, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.158034324646, "training_acc": 53.0, "val_loss": 17.286357283592224, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.14372253417969, "training_acc": 53.0, "val_loss": 17.28471964597702, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.13262939453125, "training_acc": 53.0, "val_loss": 17.283031344413757, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.18968057632446, "training_acc": 53.0, "val_loss": 17.281976342201233, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.1665780544281, "training_acc": 53.0, "val_loss": 17.280715703964233, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.15158319473267, "training_acc": 53.0, "val_loss": 17.279911041259766, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.16737842559814, "training_acc": 53.0, "val_loss": 17.27999448776245, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.15815448760986, "training_acc": 53.0, "val_loss": 17.280791699886322, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.14375686645508, "training_acc": 53.0, "val_loss": 17.28132665157318, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.11630868911743, "training_acc": 53.0, "val_loss": 17.281848192214966, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.14352750778198, "training_acc": 53.0, "val_loss": 17.282584309577942, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.14794445037842, "training_acc": 53.0, "val_loss": 17.284126579761505, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.14286351203918, "training_acc": 53.0, "val_loss": 17.2861710190773, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.1843671798706, "training_acc": 53.0, "val_loss": 17.287643253803253, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.19407844543457, "training_acc": 53.0, "val_loss": 17.28905886411667, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.14501237869263, "training_acc": 53.0, "val_loss": 17.288722097873688, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.1318633556366, "training_acc": 53.0, "val_loss": 17.287008464336395, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.14942002296448, "training_acc": 53.0, "val_loss": 17.286063730716705, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.12536311149597, "training_acc": 53.0, "val_loss": 17.28597581386566, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.1449522972107, "training_acc": 53.0, "val_loss": 17.286567389965057, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.12862825393677, "training_acc": 53.0, "val_loss": 17.287182807922363, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.17152833938599, "training_acc": 53.0, "val_loss": 17.286792397499084, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.19638442993164, "training_acc": 53.0, "val_loss": 17.28680729866028, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.18655776977539, "training_acc": 53.0, "val_loss": 17.287148535251617, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.1452283859253, "training_acc": 53.0, "val_loss": 17.28738248348236, "val_acc": 52.0}
