"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.25667905807495, "training_acc": 52.0, "val_loss": 17.235340178012848, "val_acc": 56.0}
{"epoch": 1, "training_loss": 69.28447127342224, "training_acc": 52.0, "val_loss": 17.231252789497375, "val_acc": 56.0}
{"epoch": 2, "training_loss": 69.27335500717163, "training_acc": 52.0, "val_loss": 17.23043918609619, "val_acc": 56.0}
{"epoch": 3, "training_loss": 69.2204692363739, "training_acc": 52.0, "val_loss": 17.226263880729675, "val_acc": 56.0}
{"epoch": 4, "training_loss": 69.28320550918579, "training_acc": 52.0, "val_loss": 17.226780951023102, "val_acc": 56.0}
{"epoch": 5, "training_loss": 69.23047947883606, "training_acc": 52.0, "val_loss": 17.22404956817627, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.23314094543457, "training_acc": 52.0, "val_loss": 17.2227144241333, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.26500225067139, "training_acc": 52.0, "val_loss": 17.22215861082077, "val_acc": 56.0}
{"epoch": 8, "training_loss": 69.22161841392517, "training_acc": 52.0, "val_loss": 17.220599949359894, "val_acc": 56.0}
{"epoch": 9, "training_loss": 69.28421783447266, "training_acc": 52.0, "val_loss": 17.22220778465271, "val_acc": 56.0}
{"epoch": 10, "training_loss": 69.2572648525238, "training_acc": 52.0, "val_loss": 17.226463556289673, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.23230957984924, "training_acc": 52.0, "val_loss": 17.226171493530273, "val_acc": 56.0}
{"epoch": 12, "training_loss": 69.25365138053894, "training_acc": 52.0, "val_loss": 17.221839725971222, "val_acc": 56.0}
{"epoch": 13, "training_loss": 69.22562599182129, "training_acc": 52.0, "val_loss": 17.219264805316925, "val_acc": 56.0}
{"epoch": 14, "training_loss": 69.2642068862915, "training_acc": 52.0, "val_loss": 17.215625941753387, "val_acc": 56.0}
{"epoch": 15, "training_loss": 69.2338171005249, "training_acc": 52.0, "val_loss": 17.212462425231934, "val_acc": 56.0}
{"epoch": 16, "training_loss": 69.29378032684326, "training_acc": 52.0, "val_loss": 17.207612097263336, "val_acc": 56.0}
{"epoch": 17, "training_loss": 69.24100971221924, "training_acc": 52.0, "val_loss": 17.20704287290573, "val_acc": 56.0}
{"epoch": 18, "training_loss": 69.22271537780762, "training_acc": 52.0, "val_loss": 17.208877205848694, "val_acc": 56.0}
{"epoch": 19, "training_loss": 69.22273850440979, "training_acc": 52.0, "val_loss": 17.21150130033493, "val_acc": 56.0}
{"epoch": 20, "training_loss": 69.26222515106201, "training_acc": 52.0, "val_loss": 17.211933434009552, "val_acc": 56.0}
{"epoch": 21, "training_loss": 69.26465129852295, "training_acc": 52.0, "val_loss": 17.212428152561188, "val_acc": 56.0}
{"epoch": 22, "training_loss": 69.29040956497192, "training_acc": 52.0, "val_loss": 17.21288710832596, "val_acc": 56.0}
{"epoch": 23, "training_loss": 69.2402868270874, "training_acc": 52.0, "val_loss": 17.214122414588928, "val_acc": 56.0}
{"epoch": 24, "training_loss": 69.24326276779175, "training_acc": 52.0, "val_loss": 17.212173342704773, "val_acc": 56.0}
{"epoch": 25, "training_loss": 69.23149871826172, "training_acc": 52.0, "val_loss": 17.212948203086853, "val_acc": 56.0}
{"epoch": 26, "training_loss": 69.23696064949036, "training_acc": 52.0, "val_loss": 17.212997376918793, "val_acc": 56.0}
{"epoch": 27, "training_loss": 69.22956943511963, "training_acc": 52.0, "val_loss": 17.213726043701172, "val_acc": 56.0}
{"epoch": 28, "training_loss": 69.24820709228516, "training_acc": 52.0, "val_loss": 17.214027047157288, "val_acc": 56.0}
{"epoch": 29, "training_loss": 69.23851585388184, "training_acc": 52.0, "val_loss": 17.211534082889557, "val_acc": 56.0}
{"epoch": 30, "training_loss": 69.27934741973877, "training_acc": 52.0, "val_loss": 17.206023633480072, "val_acc": 56.0}
{"epoch": 31, "training_loss": 69.2829737663269, "training_acc": 52.0, "val_loss": 17.20106154680252, "val_acc": 56.0}
{"epoch": 32, "training_loss": 69.26652717590332, "training_acc": 52.0, "val_loss": 17.19443053007126, "val_acc": 56.0}
{"epoch": 33, "training_loss": 69.24248433113098, "training_acc": 52.0, "val_loss": 17.189985513687134, "val_acc": 56.0}
{"epoch": 34, "training_loss": 69.25192904472351, "training_acc": 52.0, "val_loss": 17.188739776611328, "val_acc": 56.0}
{"epoch": 35, "training_loss": 69.27965140342712, "training_acc": 52.0, "val_loss": 17.189475893974304, "val_acc": 56.0}
{"epoch": 36, "training_loss": 69.28238844871521, "training_acc": 52.0, "val_loss": 17.188522219657898, "val_acc": 56.0}
{"epoch": 37, "training_loss": 69.23519587516785, "training_acc": 52.0, "val_loss": 17.18716025352478, "val_acc": 56.0}
{"epoch": 38, "training_loss": 69.25738167762756, "training_acc": 52.0, "val_loss": 17.1871617436409, "val_acc": 56.0}
{"epoch": 39, "training_loss": 69.30400323867798, "training_acc": 52.0, "val_loss": 17.188705503940582, "val_acc": 56.0}
{"epoch": 40, "training_loss": 69.27252054214478, "training_acc": 52.0, "val_loss": 17.19137132167816, "val_acc": 56.0}
{"epoch": 41, "training_loss": 69.22694778442383, "training_acc": 52.0, "val_loss": 17.19246506690979, "val_acc": 56.0}
{"epoch": 42, "training_loss": 69.25927567481995, "training_acc": 52.0, "val_loss": 17.19449609518051, "val_acc": 56.0}
{"epoch": 43, "training_loss": 69.22054481506348, "training_acc": 52.0, "val_loss": 17.196650803089142, "val_acc": 56.0}
{"epoch": 44, "training_loss": 69.23110103607178, "training_acc": 52.0, "val_loss": 17.20036417245865, "val_acc": 56.0}
{"epoch": 45, "training_loss": 69.26150727272034, "training_acc": 52.0, "val_loss": 17.202793061733246, "val_acc": 56.0}
{"epoch": 46, "training_loss": 69.25011253356934, "training_acc": 52.0, "val_loss": 17.20428764820099, "val_acc": 56.0}
{"epoch": 47, "training_loss": 69.2341833114624, "training_acc": 52.0, "val_loss": 17.204156517982483, "val_acc": 56.0}
{"epoch": 48, "training_loss": 69.2559928894043, "training_acc": 52.0, "val_loss": 17.204563319683075, "val_acc": 56.0}
{"epoch": 49, "training_loss": 69.2314920425415, "training_acc": 52.0, "val_loss": 17.20704287290573, "val_acc": 56.0}
{"epoch": 50, "training_loss": 69.27267742156982, "training_acc": 52.0, "val_loss": 17.211294174194336, "val_acc": 56.0}
{"epoch": 51, "training_loss": 69.28131222724915, "training_acc": 52.0, "val_loss": 17.216554284095764, "val_acc": 56.0}
{"epoch": 52, "training_loss": 69.26256775856018, "training_acc": 52.0, "val_loss": 17.22204238176346, "val_acc": 56.0}
{"epoch": 53, "training_loss": 69.25695610046387, "training_acc": 52.0, "val_loss": 17.22576469182968, "val_acc": 56.0}
{"epoch": 54, "training_loss": 69.24163913726807, "training_acc": 52.0, "val_loss": 17.229825258255005, "val_acc": 56.0}
{"epoch": 55, "training_loss": 69.24325895309448, "training_acc": 52.0, "val_loss": 17.236746847629547, "val_acc": 56.0}
{"epoch": 56, "training_loss": 69.26652812957764, "training_acc": 52.0, "val_loss": 17.245742678642273, "val_acc": 56.0}
