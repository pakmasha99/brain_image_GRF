"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.17832231521606, "training_acc": 53.0, "val_loss": 17.296943068504333, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.16139626502991, "training_acc": 53.0, "val_loss": 17.297710478305817, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.14366507530212, "training_acc": 53.0, "val_loss": 17.296771705150604, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.12659072875977, "training_acc": 53.0, "val_loss": 17.29588210582733, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.14434719085693, "training_acc": 53.0, "val_loss": 17.295144498348236, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.12050294876099, "training_acc": 53.0, "val_loss": 17.293864488601685, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.11436581611633, "training_acc": 53.0, "val_loss": 17.292441427707672, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.1252703666687, "training_acc": 53.0, "val_loss": 17.291560769081116, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.12976121902466, "training_acc": 53.0, "val_loss": 17.291072010993958, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.06583976745605, "training_acc": 53.0, "val_loss": 17.29077249765396, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.09720516204834, "training_acc": 53.0, "val_loss": 17.290550470352173, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.0952525138855, "training_acc": 53.0, "val_loss": 17.290279269218445, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.09718823432922, "training_acc": 53.0, "val_loss": 17.29014217853546, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.08961582183838, "training_acc": 53.0, "val_loss": 17.290237545967102, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.0910096168518, "training_acc": 53.0, "val_loss": 17.290771007537842, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.06599855422974, "training_acc": 53.0, "val_loss": 17.29118674993515, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.04655456542969, "training_acc": 53.0, "val_loss": 17.291390895843506, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.01767134666443, "training_acc": 53.0, "val_loss": 17.29189306497574, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.08919143676758, "training_acc": 53.0, "val_loss": 17.29235351085663, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.08547139167786, "training_acc": 53.0, "val_loss": 17.292094230651855, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.06278324127197, "training_acc": 53.0, "val_loss": 17.291875183582306, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.03328847885132, "training_acc": 53.0, "val_loss": 17.291800677776337, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.04992651939392, "training_acc": 53.0, "val_loss": 17.292030155658722, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.07455039024353, "training_acc": 53.0, "val_loss": 17.292137444019318, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.06498885154724, "training_acc": 53.0, "val_loss": 17.292359471321106, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.04789924621582, "training_acc": 53.0, "val_loss": 17.293281853199005, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.12360906600952, "training_acc": 53.0, "val_loss": 17.294734716415405, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.09158253669739, "training_acc": 53.0, "val_loss": 17.29576140642166, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.06660318374634, "training_acc": 53.0, "val_loss": 17.29610562324524, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.09239864349365, "training_acc": 53.0, "val_loss": 17.29635000228882, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.10080099105835, "training_acc": 53.0, "val_loss": 17.296546697616577, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.0536618232727, "training_acc": 53.0, "val_loss": 17.296430468559265, "val_acc": 52.0}
