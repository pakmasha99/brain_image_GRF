"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 75.81764936447144, "training_acc": 50.0, "val_loss": 17.23657250404358, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.10342144966125, "training_acc": 53.0, "val_loss": 17.315421998500824, "val_acc": 52.0}
{"epoch": 2, "training_loss": 70.7780556678772, "training_acc": 53.0, "val_loss": 17.476773262023926, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.04782462120056, "training_acc": 53.0, "val_loss": 17.297808825969696, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.96690249443054, "training_acc": 47.0, "val_loss": 17.318572103977203, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.0376033782959, "training_acc": 51.0, "val_loss": 17.495988309383392, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.9703426361084, "training_acc": 53.0, "val_loss": 17.36665517091751, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.55069351196289, "training_acc": 49.0, "val_loss": 17.235562205314636, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.56397247314453, "training_acc": 53.0, "val_loss": 17.608752846717834, "val_acc": 52.0}
{"epoch": 9, "training_loss": 70.96989941596985, "training_acc": 53.0, "val_loss": 17.262263596057892, "val_acc": 52.0}
{"epoch": 10, "training_loss": 70.67058515548706, "training_acc": 43.0, "val_loss": 18.32048147916794, "val_acc": 60.0}
{"epoch": 11, "training_loss": 74.3198835849762, "training_acc": 47.0, "val_loss": 17.88499355316162, "val_acc": 52.0}
{"epoch": 12, "training_loss": 70.66289806365967, "training_acc": 47.0, "val_loss": 17.364777624607086, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.41216444969177, "training_acc": 53.0, "val_loss": 17.92636662721634, "val_acc": 52.0}
{"epoch": 14, "training_loss": 71.61018466949463, "training_acc": 53.0, "val_loss": 17.680947482585907, "val_acc": 52.0}
{"epoch": 15, "training_loss": 70.2067494392395, "training_acc": 53.0, "val_loss": 17.28055328130722, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.10508632659912, "training_acc": 53.0, "val_loss": 17.237496376037598, "val_acc": 52.0}
{"epoch": 17, "training_loss": 68.87963914871216, "training_acc": 60.0, "val_loss": 17.38346368074417, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.98971676826477, "training_acc": 52.0, "val_loss": 17.241452634334564, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.32648706436157, "training_acc": 49.0, "val_loss": 17.278651893138885, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.20578813552856, "training_acc": 51.0, "val_loss": 17.26309359073639, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.5388708114624, "training_acc": 56.0, "val_loss": 17.555026710033417, "val_acc": 52.0}
{"epoch": 22, "training_loss": 72.68926858901978, "training_acc": 47.0, "val_loss": 17.703737318515778, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.97838068008423, "training_acc": 49.0, "val_loss": 17.542293667793274, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.84235310554504, "training_acc": 53.0, "val_loss": 17.47545599937439, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.31413173675537, "training_acc": 54.0, "val_loss": 17.367833852767944, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.63038444519043, "training_acc": 47.0, "val_loss": 17.47937500476837, "val_acc": 52.0}
