"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 71.69090461730957, "training_acc": 49.0, "val_loss": 18.86262595653534, "val_acc": 52.0}
{"epoch": 1, "training_loss": 78.22583603858948, "training_acc": 53.0, "val_loss": 17.98207461833954, "val_acc": 52.0}
{"epoch": 2, "training_loss": 70.37933731079102, "training_acc": 51.0, "val_loss": 18.249157071113586, "val_acc": 52.0}
{"epoch": 3, "training_loss": 73.67564082145691, "training_acc": 47.0, "val_loss": 17.465755343437195, "val_acc": 52.0}
{"epoch": 4, "training_loss": 70.04781579971313, "training_acc": 49.0, "val_loss": 18.39062124490738, "val_acc": 52.0}
{"epoch": 5, "training_loss": 72.81006598472595, "training_acc": 53.0, "val_loss": 17.936129868030548, "val_acc": 52.0}
{"epoch": 6, "training_loss": 71.23727416992188, "training_acc": 53.0, "val_loss": 17.54092127084732, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.42300224304199, "training_acc": 53.0, "val_loss": 17.357295751571655, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.34846448898315, "training_acc": 52.0, "val_loss": 17.510128021240234, "val_acc": 52.0}
{"epoch": 9, "training_loss": 70.65768718719482, "training_acc": 47.0, "val_loss": 17.565688490867615, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.67314624786377, "training_acc": 53.0, "val_loss": 17.460797727108, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.39860391616821, "training_acc": 53.0, "val_loss": 17.955882847309113, "val_acc": 52.0}
{"epoch": 12, "training_loss": 70.68455100059509, "training_acc": 53.0, "val_loss": 17.36667901277542, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.31536436080933, "training_acc": 49.0, "val_loss": 17.84067153930664, "val_acc": 52.0}
{"epoch": 14, "training_loss": 74.48676609992981, "training_acc": 47.0, "val_loss": 18.223601579666138, "val_acc": 52.0}
{"epoch": 15, "training_loss": 71.61928153038025, "training_acc": 48.0, "val_loss": 17.635469138622284, "val_acc": 52.0}
{"epoch": 16, "training_loss": 70.54024195671082, "training_acc": 53.0, "val_loss": 18.262627720832825, "val_acc": 52.0}
{"epoch": 17, "training_loss": 72.09881472587585, "training_acc": 53.0, "val_loss": 17.55906641483307, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.3094687461853, "training_acc": 51.0, "val_loss": 17.408660054206848, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.49690818786621, "training_acc": 47.0, "val_loss": 17.34030991792679, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.40714883804321, "training_acc": 53.0, "val_loss": 17.367269098758698, "val_acc": 52.0}
{"epoch": 21, "training_loss": 68.99633836746216, "training_acc": 58.0, "val_loss": 17.52755641937256, "val_acc": 52.0}
{"epoch": 22, "training_loss": 70.6058030128479, "training_acc": 47.0, "val_loss": 17.430028319358826, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.85874223709106, "training_acc": 49.0, "val_loss": 18.17397028207779, "val_acc": 52.0}
{"epoch": 24, "training_loss": 72.20063424110413, "training_acc": 53.0, "val_loss": 17.763450741767883, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.1514527797699, "training_acc": 54.0, "val_loss": 17.449279129505157, "val_acc": 52.0}
{"epoch": 26, "training_loss": 71.03345108032227, "training_acc": 47.0, "val_loss": 17.694905400276184, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.92719173431396, "training_acc": 50.0, "val_loss": 17.410513758659363, "val_acc": 52.0}
{"epoch": 28, "training_loss": 70.29161739349365, "training_acc": 53.0, "val_loss": 17.92403757572174, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.75681686401367, "training_acc": 53.0, "val_loss": 17.33601689338684, "val_acc": 52.0}
{"epoch": 30, "training_loss": 68.92774987220764, "training_acc": 64.0, "val_loss": 17.337070405483246, "val_acc": 52.0}
{"epoch": 31, "training_loss": 68.92909669876099, "training_acc": 53.0, "val_loss": 17.388585209846497, "val_acc": 52.0}
{"epoch": 32, "training_loss": 68.75336122512817, "training_acc": 53.0, "val_loss": 17.362526059150696, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.09545540809631, "training_acc": 55.0, "val_loss": 17.349272966384888, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.62079572677612, "training_acc": 53.0, "val_loss": 17.861337959766388, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.88151121139526, "training_acc": 53.0, "val_loss": 17.384672164916992, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.08476734161377, "training_acc": 53.0, "val_loss": 17.742793262004852, "val_acc": 52.0}
{"epoch": 37, "training_loss": 70.677649974823, "training_acc": 53.0, "val_loss": 18.05005669593811, "val_acc": 52.0}
{"epoch": 38, "training_loss": 70.15963077545166, "training_acc": 53.0, "val_loss": 17.397943139076233, "val_acc": 52.0}
{"epoch": 39, "training_loss": 68.74457120895386, "training_acc": 53.0, "val_loss": 17.339862883090973, "val_acc": 52.0}
{"epoch": 40, "training_loss": 68.68686151504517, "training_acc": 53.0, "val_loss": 17.371341586112976, "val_acc": 52.0}
{"epoch": 41, "training_loss": 68.87557482719421, "training_acc": 53.0, "val_loss": 17.396265268325806, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.46718835830688, "training_acc": 53.0, "val_loss": 17.710421979427338, "val_acc": 52.0}
{"epoch": 43, "training_loss": 70.75770688056946, "training_acc": 53.0, "val_loss": 17.588138580322266, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.52388858795166, "training_acc": 54.0, "val_loss": 17.441383004188538, "val_acc": 52.0}
{"epoch": 45, "training_loss": 70.29431319236755, "training_acc": 47.0, "val_loss": 17.406684160232544, "val_acc": 52.0}
{"epoch": 46, "training_loss": 69.77293157577515, "training_acc": 51.0, "val_loss": 17.776812613010406, "val_acc": 52.0}
{"epoch": 47, "training_loss": 70.57196521759033, "training_acc": 53.0, "val_loss": 17.453479766845703, "val_acc": 52.0}
{"epoch": 48, "training_loss": 68.37141633033752, "training_acc": 51.0, "val_loss": 17.805014550685883, "val_acc": 52.0}
