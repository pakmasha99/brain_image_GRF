"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 71.60779285430908, "training_acc": 48.0, "val_loss": 17.409099638462067, "val_acc": 56.0}
{"epoch": 1, "training_loss": 71.49565052986145, "training_acc": 52.0, "val_loss": 17.470477521419525, "val_acc": 56.0}
{"epoch": 2, "training_loss": 71.70548748970032, "training_acc": 48.0, "val_loss": 17.42611825466156, "val_acc": 56.0}
{"epoch": 3, "training_loss": 70.45430564880371, "training_acc": 50.0, "val_loss": 17.271587252616882, "val_acc": 56.0}
{"epoch": 4, "training_loss": 70.32858800888062, "training_acc": 50.0, "val_loss": 17.31474995613098, "val_acc": 56.0}
{"epoch": 5, "training_loss": 69.3992428779602, "training_acc": 48.0, "val_loss": 17.231982946395874, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.2070894241333, "training_acc": 52.0, "val_loss": 17.203745245933533, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.30332803726196, "training_acc": 52.0, "val_loss": 17.210213840007782, "val_acc": 56.0}
{"epoch": 8, "training_loss": 69.78314423561096, "training_acc": 52.0, "val_loss": 17.164140939712524, "val_acc": 56.0}
{"epoch": 9, "training_loss": 69.67079496383667, "training_acc": 53.0, "val_loss": 17.744620144367218, "val_acc": 56.0}
{"epoch": 10, "training_loss": 71.40653038024902, "training_acc": 48.0, "val_loss": 18.290702998638153, "val_acc": 56.0}
{"epoch": 11, "training_loss": 70.00819253921509, "training_acc": 52.0, "val_loss": 17.240819334983826, "val_acc": 56.0}
{"epoch": 12, "training_loss": 76.08944702148438, "training_acc": 52.0, "val_loss": 18.58845055103302, "val_acc": 56.0}
{"epoch": 13, "training_loss": 76.06577444076538, "training_acc": 52.0, "val_loss": 17.1310156583786, "val_acc": 56.0}
{"epoch": 14, "training_loss": 69.48958611488342, "training_acc": 54.0, "val_loss": 17.692095041275024, "val_acc": 56.0}
{"epoch": 15, "training_loss": 70.54721164703369, "training_acc": 48.0, "val_loss": 17.417117953300476, "val_acc": 56.0}
{"epoch": 16, "training_loss": 70.53748273849487, "training_acc": 46.0, "val_loss": 17.323558032512665, "val_acc": 56.0}
{"epoch": 17, "training_loss": 70.53805327415466, "training_acc": 52.0, "val_loss": 17.201124131679535, "val_acc": 56.0}
{"epoch": 18, "training_loss": 69.1790201663971, "training_acc": 54.0, "val_loss": 18.668648600578308, "val_acc": 56.0}
{"epoch": 19, "training_loss": 73.67381858825684, "training_acc": 48.0, "val_loss": 18.23229193687439, "val_acc": 56.0}
{"epoch": 20, "training_loss": 69.75239086151123, "training_acc": 53.0, "val_loss": 17.170217633247375, "val_acc": 56.0}
{"epoch": 21, "training_loss": 70.65760850906372, "training_acc": 52.0, "val_loss": 17.333535850048065, "val_acc": 56.0}
{"epoch": 22, "training_loss": 70.4307861328125, "training_acc": 52.0, "val_loss": 17.214927077293396, "val_acc": 56.0}
{"epoch": 23, "training_loss": 69.55972003936768, "training_acc": 52.0, "val_loss": 18.028008937835693, "val_acc": 56.0}
{"epoch": 24, "training_loss": 70.75414371490479, "training_acc": 48.0, "val_loss": 17.135056853294373, "val_acc": 56.0}
{"epoch": 25, "training_loss": 69.23981952667236, "training_acc": 52.0, "val_loss": 17.152415215969086, "val_acc": 56.0}
{"epoch": 26, "training_loss": 69.27954959869385, "training_acc": 52.0, "val_loss": 17.204615473747253, "val_acc": 56.0}
{"epoch": 27, "training_loss": 68.94949102401733, "training_acc": 55.0, "val_loss": 17.38799810409546, "val_acc": 56.0}
{"epoch": 28, "training_loss": 69.32349920272827, "training_acc": 48.0, "val_loss": 17.23766326904297, "val_acc": 56.0}
{"epoch": 29, "training_loss": 69.12132501602173, "training_acc": 52.0, "val_loss": 17.283307015895844, "val_acc": 56.0}
{"epoch": 30, "training_loss": 73.72077107429504, "training_acc": 52.0, "val_loss": 17.965564131736755, "val_acc": 56.0}
{"epoch": 31, "training_loss": 74.38780784606934, "training_acc": 52.0, "val_loss": 17.221207916736603, "val_acc": 56.0}
{"epoch": 32, "training_loss": 70.36445188522339, "training_acc": 52.0, "val_loss": 17.125989496707916, "val_acc": 56.0}
{"epoch": 33, "training_loss": 69.46974968910217, "training_acc": 52.0, "val_loss": 17.26667732000351, "val_acc": 56.0}
{"epoch": 34, "training_loss": 68.81683683395386, "training_acc": 56.0, "val_loss": 18.081168830394745, "val_acc": 56.0}
{"epoch": 35, "training_loss": 71.75369071960449, "training_acc": 48.0, "val_loss": 17.900551855564117, "val_acc": 56.0}
{"epoch": 36, "training_loss": 69.29419231414795, "training_acc": 52.0, "val_loss": 17.212386429309845, "val_acc": 56.0}
{"epoch": 37, "training_loss": 71.2643232345581, "training_acc": 52.0, "val_loss": 17.56402999162674, "val_acc": 56.0}
{"epoch": 38, "training_loss": 72.012686252594, "training_acc": 52.0, "val_loss": 17.22559630870819, "val_acc": 56.0}
{"epoch": 39, "training_loss": 69.18734550476074, "training_acc": 50.0, "val_loss": 18.782104551792145, "val_acc": 60.0}
{"epoch": 40, "training_loss": 74.70116567611694, "training_acc": 48.0, "val_loss": 18.277062475681305, "val_acc": 56.0}
{"epoch": 41, "training_loss": 69.8115804195404, "training_acc": 50.0, "val_loss": 17.15092808008194, "val_acc": 56.0}
{"epoch": 42, "training_loss": 70.44768762588501, "training_acc": 52.0, "val_loss": 17.1514093875885, "val_acc": 56.0}
{"epoch": 43, "training_loss": 69.00583839416504, "training_acc": 52.0, "val_loss": 17.413023114204407, "val_acc": 56.0}
{"epoch": 44, "training_loss": 70.81464004516602, "training_acc": 48.0, "val_loss": 18.536949157714844, "val_acc": 56.0}
{"epoch": 45, "training_loss": 71.57751798629761, "training_acc": 48.0, "val_loss": 17.395980656147003, "val_acc": 56.0}
{"epoch": 46, "training_loss": 69.06189775466919, "training_acc": 50.0, "val_loss": 17.186294496059418, "val_acc": 56.0}
{"epoch": 47, "training_loss": 70.7172532081604, "training_acc": 52.0, "val_loss": 17.28016287088394, "val_acc": 56.0}
{"epoch": 48, "training_loss": 69.82224631309509, "training_acc": 52.0, "val_loss": 17.304448783397675, "val_acc": 56.0}
{"epoch": 49, "training_loss": 69.11839127540588, "training_acc": 55.0, "val_loss": 18.83625239133835, "val_acc": 64.0}
{"epoch": 50, "training_loss": 73.8555679321289, "training_acc": 48.0, "val_loss": 18.62904280424118, "val_acc": 56.0}
{"epoch": 51, "training_loss": 71.63493180274963, "training_acc": 48.0, "val_loss": 17.43079125881195, "val_acc": 56.0}
