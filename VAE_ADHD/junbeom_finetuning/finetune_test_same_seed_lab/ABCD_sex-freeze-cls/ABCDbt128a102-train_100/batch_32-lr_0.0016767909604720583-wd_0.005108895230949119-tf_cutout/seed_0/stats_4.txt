"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 74.90873289108276, "training_acc": 45.0, "val_loss": 17.47427135705948, "val_acc": 52.0}
{"epoch": 1, "training_loss": 70.58511900901794, "training_acc": 53.0, "val_loss": 17.424003779888153, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.08428049087524, "training_acc": 51.0, "val_loss": 17.956365644931793, "val_acc": 52.0}
{"epoch": 3, "training_loss": 73.057302236557, "training_acc": 47.0, "val_loss": 17.392778396606445, "val_acc": 52.0}
{"epoch": 4, "training_loss": 71.70743870735168, "training_acc": 43.0, "val_loss": 18.72202903032303, "val_acc": 52.0}
{"epoch": 5, "training_loss": 75.59150719642639, "training_acc": 53.0, "val_loss": 18.0839940905571, "val_acc": 52.0}
{"epoch": 6, "training_loss": 71.41600465774536, "training_acc": 51.0, "val_loss": 17.629098892211914, "val_acc": 52.0}
{"epoch": 7, "training_loss": 70.72753810882568, "training_acc": 47.0, "val_loss": 17.430900037288666, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.50315237045288, "training_acc": 47.0, "val_loss": 17.70091950893402, "val_acc": 52.0}
{"epoch": 9, "training_loss": 71.72706270217896, "training_acc": 53.0, "val_loss": 18.140017986297607, "val_acc": 52.0}
{"epoch": 10, "training_loss": 71.06910228729248, "training_acc": 53.0, "val_loss": 17.334505915641785, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.30040240287781, "training_acc": 52.0, "val_loss": 17.30995923280716, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.49182605743408, "training_acc": 53.0, "val_loss": 17.308799922466278, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.50902438163757, "training_acc": 47.0, "val_loss": 17.53586083650589, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.64427351951599, "training_acc": 49.0, "val_loss": 17.33529269695282, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.52239561080933, "training_acc": 53.0, "val_loss": 17.706359922885895, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.73371887207031, "training_acc": 53.0, "val_loss": 17.30949729681015, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.590651512146, "training_acc": 48.0, "val_loss": 17.343078553676605, "val_acc": 52.0}
{"epoch": 18, "training_loss": 68.86175107955933, "training_acc": 51.0, "val_loss": 17.85360276699066, "val_acc": 52.0}
{"epoch": 19, "training_loss": 72.01748085021973, "training_acc": 53.0, "val_loss": 18.390651047229767, "val_acc": 52.0}
{"epoch": 20, "training_loss": 72.12450456619263, "training_acc": 53.0, "val_loss": 17.558342218399048, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.40106296539307, "training_acc": 53.0, "val_loss": 17.301955819129944, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.15720987319946, "training_acc": 52.0, "val_loss": 17.347683012485504, "val_acc": 52.0}
{"epoch": 23, "training_loss": 68.86305928230286, "training_acc": 54.0, "val_loss": 17.352883517742157, "val_acc": 52.0}
{"epoch": 24, "training_loss": 70.1108946800232, "training_acc": 53.0, "val_loss": 17.475499212741852, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.13600635528564, "training_acc": 53.0, "val_loss": 17.318440973758698, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.46968126296997, "training_acc": 54.0, "val_loss": 17.881587147712708, "val_acc": 52.0}
{"epoch": 27, "training_loss": 71.58433055877686, "training_acc": 47.0, "val_loss": 17.410844564437866, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.33710050582886, "training_acc": 47.0, "val_loss": 17.29619801044464, "val_acc": 52.0}
{"epoch": 29, "training_loss": 68.89580535888672, "training_acc": 53.0, "val_loss": 17.483869194984436, "val_acc": 52.0}
{"epoch": 30, "training_loss": 68.87071323394775, "training_acc": 53.0, "val_loss": 17.374055087566376, "val_acc": 52.0}
{"epoch": 31, "training_loss": 71.88710451126099, "training_acc": 47.0, "val_loss": 18.176285922527313, "val_acc": 48.0}
{"epoch": 32, "training_loss": 70.75515174865723, "training_acc": 45.0, "val_loss": 17.351683974266052, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.53142976760864, "training_acc": 53.0, "val_loss": 17.686696350574493, "val_acc": 52.0}
{"epoch": 34, "training_loss": 70.14335346221924, "training_acc": 53.0, "val_loss": 17.562541365623474, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.67829155921936, "training_acc": 53.0, "val_loss": 17.456328868865967, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.22104597091675, "training_acc": 53.0, "val_loss": 17.306987941265106, "val_acc": 52.0}
{"epoch": 37, "training_loss": 68.57153058052063, "training_acc": 59.0, "val_loss": 17.528578639030457, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.93281626701355, "training_acc": 47.0, "val_loss": 17.318084836006165, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.65958452224731, "training_acc": 53.0, "val_loss": 17.588400840759277, "val_acc": 52.0}
{"epoch": 40, "training_loss": 70.08633279800415, "training_acc": 53.0, "val_loss": 17.44113117456436, "val_acc": 52.0}
{"epoch": 41, "training_loss": 68.7631163597107, "training_acc": 54.0, "val_loss": 17.343032360076904, "val_acc": 52.0}
{"epoch": 42, "training_loss": 70.66042041778564, "training_acc": 47.0, "val_loss": 17.323973774909973, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.33850979804993, "training_acc": 54.0, "val_loss": 18.375548720359802, "val_acc": 52.0}
{"epoch": 44, "training_loss": 73.44737839698792, "training_acc": 53.0, "val_loss": 18.786931037902832, "val_acc": 52.0}
{"epoch": 45, "training_loss": 74.18385124206543, "training_acc": 53.0, "val_loss": 17.858773469924927, "val_acc": 52.0}
{"epoch": 46, "training_loss": 69.62807416915894, "training_acc": 53.0, "val_loss": 17.522670328617096, "val_acc": 52.0}
{"epoch": 47, "training_loss": 70.8227858543396, "training_acc": 47.0, "val_loss": 17.795707285404205, "val_acc": 52.0}
