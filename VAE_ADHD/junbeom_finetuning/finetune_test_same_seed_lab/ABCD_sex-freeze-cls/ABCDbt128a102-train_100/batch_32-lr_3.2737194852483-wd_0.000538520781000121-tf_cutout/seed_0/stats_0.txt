"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 29026.478118896484, "training_acc": 48.0, "val_loss": 12128.833770751953, "val_acc": 56.0}
{"epoch": 1, "training_loss": 43475.959045410156, "training_acc": 52.0, "val_loss": 7508.287811279297, "val_acc": 44.0}
{"epoch": 2, "training_loss": 42477.57177734375, "training_acc": 48.0, "val_loss": 6025.20866394043, "val_acc": 44.0}
{"epoch": 3, "training_loss": 15843.17529296875, "training_acc": 50.0, "val_loss": 3696.6537475585938, "val_acc": 56.0}
{"epoch": 4, "training_loss": 12931.233337402344, "training_acc": 50.0, "val_loss": 6560.638427734375, "val_acc": 44.0}
{"epoch": 5, "training_loss": 14160.430297851562, "training_acc": 44.0, "val_loss": 1693.3391571044922, "val_acc": 56.0}
{"epoch": 6, "training_loss": 7342.088317871094, "training_acc": 50.0, "val_loss": 3681.656265258789, "val_acc": 44.0}
{"epoch": 7, "training_loss": 8792.772399902344, "training_acc": 52.0, "val_loss": 3772.318649291992, "val_acc": 56.0}
{"epoch": 8, "training_loss": 9431.699890136719, "training_acc": 52.0, "val_loss": 6309.7503662109375, "val_acc": 44.0}
{"epoch": 9, "training_loss": 23237.5908203125, "training_acc": 48.0, "val_loss": 1022.3938941955566, "val_acc": 56.0}
{"epoch": 10, "training_loss": 6625.019271850586, "training_acc": 50.0, "val_loss": 97.49082922935486, "val_acc": 44.0}
{"epoch": 11, "training_loss": 5012.185256958008, "training_acc": 54.0, "val_loss": 615.199613571167, "val_acc": 44.0}
{"epoch": 12, "training_loss": 2067.922296524048, "training_acc": 44.0, "val_loss": 871.3411331176758, "val_acc": 44.0}
{"epoch": 13, "training_loss": 3835.0985260009766, "training_acc": 52.0, "val_loss": 2232.43350982666, "val_acc": 56.0}
{"epoch": 14, "training_loss": 8048.765441894531, "training_acc": 48.0, "val_loss": 3159.219169616699, "val_acc": 44.0}
{"epoch": 15, "training_loss": 7645.582763671875, "training_acc": 54.0, "val_loss": 5023.852920532227, "val_acc": 56.0}
{"epoch": 16, "training_loss": 18519.97036743164, "training_acc": 52.0, "val_loss": 4768.440246582031, "val_acc": 44.0}
{"epoch": 17, "training_loss": 27401.275268554688, "training_acc": 48.0, "val_loss": 6460.941314697266, "val_acc": 44.0}
{"epoch": 18, "training_loss": 14644.40380859375, "training_acc": 50.0, "val_loss": 3835.2493286132812, "val_acc": 56.0}
{"epoch": 19, "training_loss": 9078.878234863281, "training_acc": 52.0, "val_loss": 37.18940317630768, "val_acc": 60.0}
{"epoch": 20, "training_loss": 2544.6536979675293, "training_acc": 60.0, "val_loss": 1280.593490600586, "val_acc": 44.0}
{"epoch": 21, "training_loss": 2253.866241455078, "training_acc": 52.0, "val_loss": 1279.925537109375, "val_acc": 44.0}
{"epoch": 22, "training_loss": 4680.787477493286, "training_acc": 46.0, "val_loss": 3496.6079711914062, "val_acc": 44.0}
{"epoch": 23, "training_loss": 14217.775146484375, "training_acc": 48.0, "val_loss": 2908.113670349121, "val_acc": 56.0}
{"epoch": 24, "training_loss": 23500.5673828125, "training_acc": 52.0, "val_loss": 3903.3615112304688, "val_acc": 56.0}
{"epoch": 25, "training_loss": 13316.236633300781, "training_acc": 48.0, "val_loss": 9627.47802734375, "val_acc": 44.0}
{"epoch": 26, "training_loss": 29739.145385742188, "training_acc": 48.0, "val_loss": 2981.851577758789, "val_acc": 56.0}
{"epoch": 27, "training_loss": 22169.936157226562, "training_acc": 52.0, "val_loss": 4013.385009765625, "val_acc": 56.0}
{"epoch": 28, "training_loss": 13425.150146484375, "training_acc": 46.0, "val_loss": 6751.396179199219, "val_acc": 44.0}
{"epoch": 29, "training_loss": 14959.043579101562, "training_acc": 52.0, "val_loss": 5963.764190673828, "val_acc": 56.0}
{"epoch": 30, "training_loss": 32537.918334960938, "training_acc": 52.0, "val_loss": 5252.914810180664, "val_acc": 56.0}
{"epoch": 31, "training_loss": 16958.55712890625, "training_acc": 42.0, "val_loss": 4799.285888671875, "val_acc": 44.0}
{"epoch": 32, "training_loss": 11334.747924804688, "training_acc": 48.0, "val_loss": 3982.8842163085938, "val_acc": 56.0}
{"epoch": 33, "training_loss": 11707.643600463867, "training_acc": 54.0, "val_loss": 4825.99983215332, "val_acc": 44.0}
{"epoch": 34, "training_loss": 15736.551169395447, "training_acc": 48.0, "val_loss": 2842.69962310791, "val_acc": 56.0}
{"epoch": 35, "training_loss": 15482.271377563477, "training_acc": 52.0, "val_loss": 331.52008056640625, "val_acc": 56.0}
{"epoch": 36, "training_loss": 12038.248352050781, "training_acc": 50.0, "val_loss": 2699.822425842285, "val_acc": 44.0}
{"epoch": 37, "training_loss": 7992.557952880859, "training_acc": 52.0, "val_loss": 1603.042221069336, "val_acc": 56.0}
{"epoch": 38, "training_loss": 7893.452430725098, "training_acc": 44.0, "val_loss": 514.6719932556152, "val_acc": 56.0}
