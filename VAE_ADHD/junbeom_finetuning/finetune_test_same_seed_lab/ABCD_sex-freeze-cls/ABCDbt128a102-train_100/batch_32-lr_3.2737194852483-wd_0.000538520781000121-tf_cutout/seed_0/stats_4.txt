"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 30474.071102142334, "training_acc": 49.0, "val_loss": 8548.845672607422, "val_acc": 48.0}
{"epoch": 1, "training_loss": 18804.091690063477, "training_acc": 51.0, "val_loss": 1680.4834365844727, "val_acc": 52.0}
{"epoch": 2, "training_loss": 4674.850799560547, "training_acc": 51.0, "val_loss": 2514.888572692871, "val_acc": 52.0}
{"epoch": 3, "training_loss": 9803.561889648438, "training_acc": 49.0, "val_loss": 3568.0774688720703, "val_acc": 48.0}
{"epoch": 4, "training_loss": 6954.052728176117, "training_acc": 45.0, "val_loss": 2510.725212097168, "val_acc": 48.0}
{"epoch": 5, "training_loss": 8342.692626953125, "training_acc": 49.0, "val_loss": 4514.561462402344, "val_acc": 52.0}
{"epoch": 6, "training_loss": 14159.372597694397, "training_acc": 55.0, "val_loss": 2231.7583084106445, "val_acc": 48.0}
{"epoch": 7, "training_loss": 6177.861419677734, "training_acc": 47.0, "val_loss": 4700.206756591797, "val_acc": 52.0}
{"epoch": 8, "training_loss": 14952.078550577164, "training_acc": 55.0, "val_loss": 3837.7548217773438, "val_acc": 48.0}
{"epoch": 9, "training_loss": 18791.4736328125, "training_acc": 47.0, "val_loss": 1984.8333358764648, "val_acc": 52.0}
{"epoch": 10, "training_loss": 14646.096557617188, "training_acc": 53.0, "val_loss": 1424.6482849121094, "val_acc": 52.0}
{"epoch": 11, "training_loss": 10226.919342041016, "training_acc": 51.0, "val_loss": 2536.555290222168, "val_acc": 48.0}
{"epoch": 12, "training_loss": 7879.687072753906, "training_acc": 43.0, "val_loss": 1937.179946899414, "val_acc": 48.0}
{"epoch": 13, "training_loss": 7945.088134765625, "training_acc": 51.0, "val_loss": 2413.838005065918, "val_acc": 52.0}
{"epoch": 14, "training_loss": 8297.07876586914, "training_acc": 53.0, "val_loss": 2102.870559692383, "val_acc": 48.0}
{"epoch": 15, "training_loss": 5823.543426513672, "training_acc": 53.0, "val_loss": 1515.676212310791, "val_acc": 52.0}
{"epoch": 16, "training_loss": 6015.659915924072, "training_acc": 45.0, "val_loss": 2688.315200805664, "val_acc": 52.0}
{"epoch": 17, "training_loss": 11877.691833496094, "training_acc": 53.0, "val_loss": 2378.695297241211, "val_acc": 48.0}
{"epoch": 18, "training_loss": 11788.988494873047, "training_acc": 47.0, "val_loss": 3311.290740966797, "val_acc": 52.0}
{"epoch": 19, "training_loss": 19432.49462890625, "training_acc": 53.0, "val_loss": 2739.222526550293, "val_acc": 52.0}
{"epoch": 20, "training_loss": 9874.829772949219, "training_acc": 45.0, "val_loss": 230.1121950149536, "val_acc": 48.0}
{"epoch": 21, "training_loss": 11596.536071777344, "training_acc": 53.0, "val_loss": 4418.806457519531, "val_acc": 52.0}
{"epoch": 22, "training_loss": 10037.936096191406, "training_acc": 55.0, "val_loss": 4469.477844238281, "val_acc": 48.0}
{"epoch": 23, "training_loss": 11186.823791503906, "training_acc": 47.0, "val_loss": 5539.012908935547, "val_acc": 52.0}
{"epoch": 24, "training_loss": 19593.63232421875, "training_acc": 53.0, "val_loss": 1887.186622619629, "val_acc": 48.0}
{"epoch": 25, "training_loss": 7901.95911026001, "training_acc": 49.0, "val_loss": 1709.8649978637695, "val_acc": 52.0}
{"epoch": 26, "training_loss": 4450.473781585693, "training_acc": 51.0, "val_loss": 454.6818256378174, "val_acc": 52.0}
{"epoch": 27, "training_loss": 1148.7886199951172, "training_acc": 57.0, "val_loss": 1453.3449172973633, "val_acc": 52.0}
{"epoch": 28, "training_loss": 5076.932342529297, "training_acc": 51.0, "val_loss": 2811.2565994262695, "val_acc": 48.0}
{"epoch": 29, "training_loss": 8071.243377685547, "training_acc": 49.0, "val_loss": 3419.8436737060547, "val_acc": 52.0}
{"epoch": 30, "training_loss": 6868.136459350586, "training_acc": 55.0, "val_loss": 327.71522998809814, "val_acc": 48.0}
{"epoch": 31, "training_loss": 5954.660926818848, "training_acc": 53.0, "val_loss": 188.15127611160278, "val_acc": 52.0}
{"epoch": 32, "training_loss": 1354.5988330841064, "training_acc": 57.0, "val_loss": 867.5827026367188, "val_acc": 48.0}
{"epoch": 33, "training_loss": 2493.304183959961, "training_acc": 41.0, "val_loss": 1748.8842010498047, "val_acc": 48.0}
{"epoch": 34, "training_loss": 5921.517288208008, "training_acc": 49.0, "val_loss": 4120.399475097656, "val_acc": 52.0}
{"epoch": 35, "training_loss": 13918.530358314514, "training_acc": 52.0, "val_loss": 2464.388084411621, "val_acc": 48.0}
{"epoch": 36, "training_loss": 9547.866455078125, "training_acc": 45.0, "val_loss": 1749.1697311401367, "val_acc": 52.0}
{"epoch": 37, "training_loss": 6300.2196044921875, "training_acc": 49.0, "val_loss": 1901.180648803711, "val_acc": 48.0}
{"epoch": 38, "training_loss": 4904.58251953125, "training_acc": 53.0, "val_loss": 317.90599822998047, "val_acc": 48.0}
{"epoch": 39, "training_loss": 1282.9687685966492, "training_acc": 52.0, "val_loss": 435.28246879577637, "val_acc": 48.0}
{"epoch": 40, "training_loss": 2442.1231384277344, "training_acc": 49.0, "val_loss": 856.6177368164062, "val_acc": 48.0}
{"epoch": 41, "training_loss": 4339.657855987549, "training_acc": 55.0, "val_loss": 1263.310718536377, "val_acc": 48.0}
{"epoch": 42, "training_loss": 3676.3564453125, "training_acc": 51.0, "val_loss": 677.2675514221191, "val_acc": 48.0}
{"epoch": 43, "training_loss": 3406.730010986328, "training_acc": 49.0, "val_loss": 1124.5546340942383, "val_acc": 52.0}
{"epoch": 44, "training_loss": 5185.745057106018, "training_acc": 47.0, "val_loss": 2835.355567932129, "val_acc": 52.0}
{"epoch": 45, "training_loss": 15140.173889160156, "training_acc": 53.0, "val_loss": 1210.9344482421875, "val_acc": 48.0}
{"epoch": 46, "training_loss": 5897.532806396484, "training_acc": 45.0, "val_loss": 332.75628089904785, "val_acc": 52.0}
{"epoch": 47, "training_loss": 3898.6378479003906, "training_acc": 51.0, "val_loss": 221.34950160980225, "val_acc": 52.0}
{"epoch": 48, "training_loss": 1459.6032485961914, "training_acc": 61.0, "val_loss": 731.9916725158691, "val_acc": 48.0}
{"epoch": 49, "training_loss": 3235.921661376953, "training_acc": 51.0, "val_loss": 1073.1219291687012, "val_acc": 52.0}
{"epoch": 50, "training_loss": 4826.690696716309, "training_acc": 45.0, "val_loss": 582.1744441986084, "val_acc": 48.0}
