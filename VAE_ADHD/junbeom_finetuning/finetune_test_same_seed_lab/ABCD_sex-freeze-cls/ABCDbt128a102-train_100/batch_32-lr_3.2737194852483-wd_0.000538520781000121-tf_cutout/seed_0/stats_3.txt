"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 30883.772798538208, "training_acc": 49.0, "val_loss": 6303.373336791992, "val_acc": 48.0}
{"epoch": 1, "training_loss": 15860.888671875, "training_acc": 51.0, "val_loss": 8754.058074951172, "val_acc": 52.0}
{"epoch": 2, "training_loss": 30835.947814941406, "training_acc": 53.0, "val_loss": 3739.3386840820312, "val_acc": 48.0}
{"epoch": 3, "training_loss": 26329.649658203125, "training_acc": 47.0, "val_loss": 3166.1306381225586, "val_acc": 48.0}
{"epoch": 4, "training_loss": 12521.897583007812, "training_acc": 49.0, "val_loss": 1727.3195266723633, "val_acc": 52.0}
{"epoch": 5, "training_loss": 10183.000427246094, "training_acc": 51.0, "val_loss": 1209.469985961914, "val_acc": 48.0}
{"epoch": 6, "training_loss": 10110.303466796875, "training_acc": 51.0, "val_loss": 1687.3811721801758, "val_acc": 52.0}
{"epoch": 7, "training_loss": 6187.877014160156, "training_acc": 59.0, "val_loss": 623.5116004943848, "val_acc": 48.0}
{"epoch": 8, "training_loss": 9417.58837890625, "training_acc": 53.0, "val_loss": 2664.2181396484375, "val_acc": 52.0}
{"epoch": 9, "training_loss": 7980.9285888671875, "training_acc": 57.0, "val_loss": 4989.006805419922, "val_acc": 48.0}
{"epoch": 10, "training_loss": 12102.924896240234, "training_acc": 45.0, "val_loss": 1728.8387298583984, "val_acc": 52.0}
{"epoch": 11, "training_loss": 5698.35139465332, "training_acc": 57.0, "val_loss": 2602.655029296875, "val_acc": 48.0}
{"epoch": 12, "training_loss": 6561.277641296387, "training_acc": 60.0, "val_loss": 4291.632080078125, "val_acc": 52.0}
{"epoch": 13, "training_loss": 10501.622467041016, "training_acc": 51.0, "val_loss": 5213.4521484375, "val_acc": 48.0}
{"epoch": 14, "training_loss": 15999.166809082031, "training_acc": 45.0, "val_loss": 2720.1765060424805, "val_acc": 52.0}
{"epoch": 15, "training_loss": 7912.896240234375, "training_acc": 49.0, "val_loss": 2769.2323684692383, "val_acc": 48.0}
{"epoch": 16, "training_loss": 8397.438232421875, "training_acc": 49.0, "val_loss": 2741.56494140625, "val_acc": 52.0}
{"epoch": 17, "training_loss": 8456.736328125, "training_acc": 51.0, "val_loss": 4423.453521728516, "val_acc": 48.0}
{"epoch": 18, "training_loss": 10996.029174804688, "training_acc": 47.0, "val_loss": 2117.7995681762695, "val_acc": 52.0}
{"epoch": 19, "training_loss": 6586.322998046875, "training_acc": 53.0, "val_loss": 4135.959243774414, "val_acc": 48.0}
{"epoch": 20, "training_loss": 10465.891235351562, "training_acc": 51.0, "val_loss": 5755.926132202148, "val_acc": 52.0}
{"epoch": 21, "training_loss": 25303.423095703125, "training_acc": 53.0, "val_loss": 2164.6242141723633, "val_acc": 52.0}
{"epoch": 22, "training_loss": 12609.833129882812, "training_acc": 51.0, "val_loss": 3930.248260498047, "val_acc": 48.0}
{"epoch": 23, "training_loss": 14732.154357910156, "training_acc": 39.0, "val_loss": 7434.671783447266, "val_acc": 52.0}
{"epoch": 24, "training_loss": 23661.42123413086, "training_acc": 53.0, "val_loss": 3228.237533569336, "val_acc": 48.0}
{"epoch": 25, "training_loss": 20887.189575195312, "training_acc": 47.0, "val_loss": 2300.8935928344727, "val_acc": 48.0}
{"epoch": 26, "training_loss": 11829.784790039062, "training_acc": 51.0, "val_loss": 4683.457946777344, "val_acc": 52.0}
