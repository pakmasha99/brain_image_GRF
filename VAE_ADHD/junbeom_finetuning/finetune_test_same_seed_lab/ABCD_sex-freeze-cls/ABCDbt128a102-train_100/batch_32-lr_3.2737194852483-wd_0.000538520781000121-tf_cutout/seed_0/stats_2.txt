"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 35303.28745651245, "training_acc": 45.0, "val_loss": 13606.849670410156, "val_acc": 52.0}
{"epoch": 1, "training_loss": 48188.9638671875, "training_acc": 53.0, "val_loss": 1955.5540084838867, "val_acc": 48.0}
{"epoch": 2, "training_loss": 13819.346649169922, "training_acc": 47.0, "val_loss": 2501.1083602905273, "val_acc": 52.0}
{"epoch": 3, "training_loss": 9572.699523925781, "training_acc": 51.0, "val_loss": 993.0645942687988, "val_acc": 48.0}
{"epoch": 4, "training_loss": 4399.937942504883, "training_acc": 53.0, "val_loss": 3934.304428100586, "val_acc": 48.0}
{"epoch": 5, "training_loss": 19378.09014892578, "training_acc": 47.0, "val_loss": 1275.1005172729492, "val_acc": 52.0}
{"epoch": 6, "training_loss": 8156.010391235352, "training_acc": 53.0, "val_loss": 1553.6774635314941, "val_acc": 48.0}
{"epoch": 7, "training_loss": 6723.693054199219, "training_acc": 45.0, "val_loss": 2732.036018371582, "val_acc": 52.0}
{"epoch": 8, "training_loss": 6288.487838745117, "training_acc": 53.0, "val_loss": 95.91059684753418, "val_acc": 48.0}
{"epoch": 9, "training_loss": 8309.820999145508, "training_acc": 51.0, "val_loss": 1215.8246040344238, "val_acc": 52.0}
{"epoch": 10, "training_loss": 6656.889221191406, "training_acc": 53.0, "val_loss": 858.7370872497559, "val_acc": 52.0}
{"epoch": 11, "training_loss": 3560.55419921875, "training_acc": 53.0, "val_loss": 2430.77392578125, "val_acc": 48.0}
{"epoch": 12, "training_loss": 6395.834976196289, "training_acc": 53.0, "val_loss": 2587.3491287231445, "val_acc": 52.0}
{"epoch": 13, "training_loss": 7115.371673583984, "training_acc": 51.0, "val_loss": 1498.5074996948242, "val_acc": 48.0}
{"epoch": 14, "training_loss": 3717.718671288807, "training_acc": 57.0, "val_loss": 781.1309814453125, "val_acc": 48.0}
{"epoch": 15, "training_loss": 2566.5335998535156, "training_acc": 63.0, "val_loss": 1790.5324935913086, "val_acc": 52.0}
{"epoch": 16, "training_loss": 4592.076995849609, "training_acc": 47.0, "val_loss": 217.66939163208008, "val_acc": 52.0}
{"epoch": 17, "training_loss": 4504.666320800781, "training_acc": 47.0, "val_loss": 57.99236297607422, "val_acc": 48.0}
{"epoch": 18, "training_loss": 2531.865520477295, "training_acc": 49.0, "val_loss": 1561.3592147827148, "val_acc": 52.0}
{"epoch": 19, "training_loss": 5753.817810058594, "training_acc": 51.0, "val_loss": 1613.0422592163086, "val_acc": 48.0}
{"epoch": 20, "training_loss": 5006.205307006836, "training_acc": 56.0, "val_loss": 3339.2654418945312, "val_acc": 52.0}
{"epoch": 21, "training_loss": 9714.475830078125, "training_acc": 47.0, "val_loss": 2815.8615112304688, "val_acc": 48.0}
{"epoch": 22, "training_loss": 8771.87890625, "training_acc": 51.0, "val_loss": 3557.966995239258, "val_acc": 52.0}
{"epoch": 23, "training_loss": 12498.956359863281, "training_acc": 45.0, "val_loss": 4972.429275512695, "val_acc": 48.0}
{"epoch": 24, "training_loss": 12870.23486328125, "training_acc": 45.0, "val_loss": 3286.895751953125, "val_acc": 52.0}
{"epoch": 25, "training_loss": 8296.266845703125, "training_acc": 49.0, "val_loss": 345.6027030944824, "val_acc": 48.0}
{"epoch": 26, "training_loss": 9648.436401367188, "training_acc": 47.0, "val_loss": 1794.1251754760742, "val_acc": 52.0}
{"epoch": 27, "training_loss": 7466.197113037109, "training_acc": 49.0, "val_loss": 924.3434906005859, "val_acc": 52.0}
{"epoch": 28, "training_loss": 4337.895477294922, "training_acc": 53.0, "val_loss": 1005.5487632751465, "val_acc": 48.0}
{"epoch": 29, "training_loss": 3669.4026489257812, "training_acc": 47.0, "val_loss": 1143.069076538086, "val_acc": 48.0}
{"epoch": 30, "training_loss": 3753.4549255371094, "training_acc": 57.0, "val_loss": 328.08499336242676, "val_acc": 48.0}
{"epoch": 31, "training_loss": 1950.3657531738281, "training_acc": 51.0, "val_loss": 80.80448508262634, "val_acc": 44.0}
{"epoch": 32, "training_loss": 1583.4672927856445, "training_acc": 55.0, "val_loss": 1420.8218574523926, "val_acc": 52.0}
{"epoch": 33, "training_loss": 5255.3897705078125, "training_acc": 53.0, "val_loss": 2458.8871002197266, "val_acc": 48.0}
{"epoch": 34, "training_loss": 7198.168487548828, "training_acc": 47.0, "val_loss": 1413.6881828308105, "val_acc": 52.0}
{"epoch": 35, "training_loss": 5919.270722746849, "training_acc": 44.0, "val_loss": 1769.472885131836, "val_acc": 52.0}
{"epoch": 36, "training_loss": 5744.477111816406, "training_acc": 51.0, "val_loss": 2008.5920333862305, "val_acc": 48.0}
