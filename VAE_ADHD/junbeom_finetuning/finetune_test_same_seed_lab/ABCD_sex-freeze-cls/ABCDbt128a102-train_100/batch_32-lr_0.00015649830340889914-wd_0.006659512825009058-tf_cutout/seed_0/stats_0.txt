"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.32518267631531, "training_acc": 51.0, "val_loss": 17.213954031467438, "val_acc": 56.0}
{"epoch": 1, "training_loss": 69.33885526657104, "training_acc": 52.0, "val_loss": 17.18164086341858, "val_acc": 56.0}
{"epoch": 2, "training_loss": 69.35470509529114, "training_acc": 52.0, "val_loss": 17.193707823753357, "val_acc": 56.0}
{"epoch": 3, "training_loss": 69.27445197105408, "training_acc": 52.0, "val_loss": 17.166678607463837, "val_acc": 56.0}
{"epoch": 4, "training_loss": 69.3165922164917, "training_acc": 52.0, "val_loss": 17.19233989715576, "val_acc": 56.0}
{"epoch": 5, "training_loss": 69.24472618103027, "training_acc": 52.0, "val_loss": 17.181119322776794, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.23810005187988, "training_acc": 52.0, "val_loss": 17.18718707561493, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.28128361701965, "training_acc": 52.0, "val_loss": 17.202867567539215, "val_acc": 56.0}
{"epoch": 8, "training_loss": 69.24573922157288, "training_acc": 52.0, "val_loss": 17.20254421234131, "val_acc": 56.0}
{"epoch": 9, "training_loss": 69.3129301071167, "training_acc": 52.0, "val_loss": 17.244860529899597, "val_acc": 56.0}
{"epoch": 10, "training_loss": 69.27693772315979, "training_acc": 47.0, "val_loss": 17.334559559822083, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.3516993522644, "training_acc": 51.0, "val_loss": 17.311806976795197, "val_acc": 56.0}
{"epoch": 12, "training_loss": 69.41604542732239, "training_acc": 45.0, "val_loss": 17.215314507484436, "val_acc": 56.0}
{"epoch": 13, "training_loss": 69.15737819671631, "training_acc": 52.0, "val_loss": 17.174075543880463, "val_acc": 56.0}
{"epoch": 14, "training_loss": 69.28107380867004, "training_acc": 52.0, "val_loss": 17.14460700750351, "val_acc": 56.0}
{"epoch": 15, "training_loss": 69.32436060905457, "training_acc": 52.0, "val_loss": 17.1348437666893, "val_acc": 56.0}
{"epoch": 16, "training_loss": 69.60737681388855, "training_acc": 52.0, "val_loss": 17.13149845600128, "val_acc": 56.0}
{"epoch": 17, "training_loss": 69.54541611671448, "training_acc": 52.0, "val_loss": 17.13278293609619, "val_acc": 56.0}
{"epoch": 18, "training_loss": 69.32954502105713, "training_acc": 52.0, "val_loss": 17.16153472661972, "val_acc": 56.0}
{"epoch": 19, "training_loss": 69.23847341537476, "training_acc": 52.0, "val_loss": 17.229409515857697, "val_acc": 56.0}
{"epoch": 20, "training_loss": 69.26421976089478, "training_acc": 52.0, "val_loss": 17.26515293121338, "val_acc": 56.0}
{"epoch": 21, "training_loss": 69.29488348960876, "training_acc": 49.0, "val_loss": 17.286017537117004, "val_acc": 56.0}
{"epoch": 22, "training_loss": 69.32632446289062, "training_acc": 50.0, "val_loss": 17.290009558200836, "val_acc": 56.0}
{"epoch": 23, "training_loss": 69.28047966957092, "training_acc": 55.0, "val_loss": 17.297984659671783, "val_acc": 56.0}
{"epoch": 24, "training_loss": 69.34977340698242, "training_acc": 51.0, "val_loss": 17.237600684165955, "val_acc": 56.0}
{"epoch": 25, "training_loss": 69.2195098400116, "training_acc": 52.0, "val_loss": 17.233166098594666, "val_acc": 56.0}
{"epoch": 26, "training_loss": 69.27280330657959, "training_acc": 52.0, "val_loss": 17.2198623418808, "val_acc": 56.0}
{"epoch": 27, "training_loss": 69.20107698440552, "training_acc": 52.0, "val_loss": 17.2209233045578, "val_acc": 56.0}
{"epoch": 28, "training_loss": 69.23152256011963, "training_acc": 52.0, "val_loss": 17.21729040145874, "val_acc": 56.0}
{"epoch": 29, "training_loss": 69.20453834533691, "training_acc": 52.0, "val_loss": 17.17829406261444, "val_acc": 56.0}
{"epoch": 30, "training_loss": 69.42202186584473, "training_acc": 52.0, "val_loss": 17.135782539844513, "val_acc": 56.0}
{"epoch": 31, "training_loss": 69.38735580444336, "training_acc": 52.0, "val_loss": 17.13157594203949, "val_acc": 56.0}
{"epoch": 32, "training_loss": 69.78136777877808, "training_acc": 52.0, "val_loss": 17.158327996730804, "val_acc": 56.0}
{"epoch": 33, "training_loss": 70.04896593093872, "training_acc": 52.0, "val_loss": 17.171835899353027, "val_acc": 56.0}
{"epoch": 34, "training_loss": 70.05210161209106, "training_acc": 52.0, "val_loss": 17.14370548725128, "val_acc": 56.0}
{"epoch": 35, "training_loss": 69.76903629302979, "training_acc": 52.0, "val_loss": 17.131896317005157, "val_acc": 56.0}
