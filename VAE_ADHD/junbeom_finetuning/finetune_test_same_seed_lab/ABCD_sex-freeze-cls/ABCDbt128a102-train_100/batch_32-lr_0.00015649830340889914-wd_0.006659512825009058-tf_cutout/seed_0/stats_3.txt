"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.68489360809326, "training_acc": 40.0, "val_loss": 17.336630821228027, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.08157062530518, "training_acc": 54.0, "val_loss": 17.329366505146027, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.10005927085876, "training_acc": 53.0, "val_loss": 17.3816978931427, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.33043670654297, "training_acc": 53.0, "val_loss": 17.428690195083618, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.43112134933472, "training_acc": 53.0, "val_loss": 17.430061101913452, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.43646454811096, "training_acc": 53.0, "val_loss": 17.394794523715973, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.30100989341736, "training_acc": 53.0, "val_loss": 17.345455288887024, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.22879266738892, "training_acc": 53.0, "val_loss": 17.327171564102173, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.173415184021, "training_acc": 53.0, "val_loss": 17.32892394065857, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.37788367271423, "training_acc": 51.0, "val_loss": 17.34033226966858, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.34478998184204, "training_acc": 50.0, "val_loss": 17.32816994190216, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.2266173362732, "training_acc": 53.0, "val_loss": 17.32652634382248, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.22133159637451, "training_acc": 53.0, "val_loss": 17.326943576335907, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.17754483222961, "training_acc": 53.0, "val_loss": 17.33291745185852, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.24304127693176, "training_acc": 54.0, "val_loss": 17.34015643596649, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.28544735908508, "training_acc": 51.0, "val_loss": 17.348231375217438, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.34614729881287, "training_acc": 49.0, "val_loss": 17.34801083803177, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.33036398887634, "training_acc": 47.0, "val_loss": 17.331501841545105, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.2804811000824, "training_acc": 53.0, "val_loss": 17.326384782791138, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.19453620910645, "training_acc": 53.0, "val_loss": 17.32962727546692, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.08567428588867, "training_acc": 53.0, "val_loss": 17.34541207551956, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.17723393440247, "training_acc": 53.0, "val_loss": 17.382927238941193, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.30813503265381, "training_acc": 53.0, "val_loss": 17.418228089809418, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.36081457138062, "training_acc": 53.0, "val_loss": 17.413990199565887, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.42245864868164, "training_acc": 53.0, "val_loss": 17.427007853984833, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.37781286239624, "training_acc": 53.0, "val_loss": 17.386195063591003, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.20033407211304, "training_acc": 53.0, "val_loss": 17.352917790412903, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.136709690094, "training_acc": 53.0, "val_loss": 17.32933074235916, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.4873571395874, "training_acc": 53.0, "val_loss": 17.335279285907745, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.2650203704834, "training_acc": 54.0, "val_loss": 17.336153984069824, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.27979302406311, "training_acc": 54.0, "val_loss": 17.336300015449524, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.300790309906, "training_acc": 54.0, "val_loss": 17.327800393104553, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.237468957901, "training_acc": 53.0, "val_loss": 17.326894402503967, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.14142656326294, "training_acc": 53.0, "val_loss": 17.329108715057373, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.14742136001587, "training_acc": 53.0, "val_loss": 17.33836829662323, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.14929413795471, "training_acc": 53.0, "val_loss": 17.34578162431717, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.1581928730011, "training_acc": 53.0, "val_loss": 17.354394495487213, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.24266839027405, "training_acc": 53.0, "val_loss": 17.391158640384674, "val_acc": 52.0}
