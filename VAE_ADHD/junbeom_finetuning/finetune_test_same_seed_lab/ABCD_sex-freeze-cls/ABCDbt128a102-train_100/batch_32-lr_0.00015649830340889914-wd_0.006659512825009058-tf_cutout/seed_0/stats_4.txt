"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 70.0359275341034, "training_acc": 47.0, "val_loss": 17.305614054203033, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.34618330001831, "training_acc": 50.0, "val_loss": 17.296671867370605, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.10631656646729, "training_acc": 53.0, "val_loss": 17.331649363040924, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.21202850341797, "training_acc": 53.0, "val_loss": 17.353977262973785, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.26185870170593, "training_acc": 53.0, "val_loss": 17.329004406929016, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.20141220092773, "training_acc": 53.0, "val_loss": 17.30564832687378, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.15557384490967, "training_acc": 53.0, "val_loss": 17.28854328393936, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.12926530838013, "training_acc": 53.0, "val_loss": 17.288196086883545, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.13128614425659, "training_acc": 53.0, "val_loss": 17.288929224014282, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.09878134727478, "training_acc": 53.0, "val_loss": 17.296983301639557, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.23035526275635, "training_acc": 48.0, "val_loss": 17.30898767709732, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.24322366714478, "training_acc": 52.0, "val_loss": 17.29242354631424, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.11410450935364, "training_acc": 53.0, "val_loss": 17.28782206773758, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.12020444869995, "training_acc": 53.0, "val_loss": 17.288661003112793, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.10154223442078, "training_acc": 53.0, "val_loss": 17.291736602783203, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.1191771030426, "training_acc": 53.0, "val_loss": 17.29799211025238, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.08118486404419, "training_acc": 53.0, "val_loss": 17.312198877334595, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.10360836982727, "training_acc": 53.0, "val_loss": 17.320097982883453, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.20559215545654, "training_acc": 53.0, "val_loss": 17.33367294073105, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.18880558013916, "training_acc": 53.0, "val_loss": 17.326301336288452, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.14899969100952, "training_acc": 53.0, "val_loss": 17.297789454460144, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.2693121433258, "training_acc": 53.0, "val_loss": 17.287220060825348, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.17903327941895, "training_acc": 53.0, "val_loss": 17.291368544101715, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.1634635925293, "training_acc": 53.0, "val_loss": 17.299386858940125, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.1290717124939, "training_acc": 53.0, "val_loss": 17.292693257331848, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.10929155349731, "training_acc": 53.0, "val_loss": 17.301583290100098, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.17393922805786, "training_acc": 53.0, "val_loss": 17.32618361711502, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.19687795639038, "training_acc": 53.0, "val_loss": 17.358888685703278, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.28796815872192, "training_acc": 53.0, "val_loss": 17.36384779214859, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.27042293548584, "training_acc": 53.0, "val_loss": 17.378181219100952, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.39404487609863, "training_acc": 53.0, "val_loss": 17.37823784351349, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.24591875076294, "training_acc": 53.0, "val_loss": 17.315511405467987, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.14451313018799, "training_acc": 53.0, "val_loss": 17.30063408613205, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.05000400543213, "training_acc": 53.0, "val_loss": 17.295977473258972, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.10275554656982, "training_acc": 53.0, "val_loss": 17.286285758018494, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.11221551895142, "training_acc": 53.0, "val_loss": 17.28551685810089, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.03232216835022, "training_acc": 53.0, "val_loss": 17.295745015144348, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.13031315803528, "training_acc": 53.0, "val_loss": 17.333857715129852, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.15234661102295, "training_acc": 53.0, "val_loss": 17.386555671691895, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.42008447647095, "training_acc": 53.0, "val_loss": 17.43619292974472, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.48731803894043, "training_acc": 53.0, "val_loss": 17.437243461608887, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.50757455825806, "training_acc": 53.0, "val_loss": 17.42013245820999, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.40085482597351, "training_acc": 53.0, "val_loss": 17.388585209846497, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.26567435264587, "training_acc": 53.0, "val_loss": 17.335858941078186, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.05345940589905, "training_acc": 53.0, "val_loss": 17.290115356445312, "val_acc": 52.0}
{"epoch": 45, "training_loss": 68.98435831069946, "training_acc": 53.0, "val_loss": 17.2867089509964, "val_acc": 52.0}
{"epoch": 46, "training_loss": 69.14853882789612, "training_acc": 54.0, "val_loss": 17.309853434562683, "val_acc": 52.0}
{"epoch": 47, "training_loss": 69.24606895446777, "training_acc": 56.0, "val_loss": 17.316876351833344, "val_acc": 52.0}
{"epoch": 48, "training_loss": 69.27826118469238, "training_acc": 51.0, "val_loss": 17.33253002166748, "val_acc": 52.0}
{"epoch": 49, "training_loss": 69.52107787132263, "training_acc": 47.0, "val_loss": 17.365893721580505, "val_acc": 52.0}
{"epoch": 50, "training_loss": 69.49375128746033, "training_acc": 47.0, "val_loss": 17.32863187789917, "val_acc": 52.0}
{"epoch": 51, "training_loss": 69.39923048019409, "training_acc": 47.0, "val_loss": 17.305633425712585, "val_acc": 52.0}
{"epoch": 52, "training_loss": 69.10555934906006, "training_acc": 54.0, "val_loss": 17.285974323749542, "val_acc": 52.0}
{"epoch": 53, "training_loss": 69.0260066986084, "training_acc": 53.0, "val_loss": 17.295023798942566, "val_acc": 52.0}
{"epoch": 54, "training_loss": 69.0913953781128, "training_acc": 53.0, "val_loss": 17.33223646879196, "val_acc": 52.0}
