"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.27691721916199, "training_acc": 53.0, "val_loss": 17.307022213935852, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.3139579296112, "training_acc": 53.0, "val_loss": 17.33681857585907, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.38955783843994, "training_acc": 53.0, "val_loss": 17.32335388660431, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.18354892730713, "training_acc": 53.0, "val_loss": 17.33703911304474, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.23628377914429, "training_acc": 53.0, "val_loss": 17.339278757572174, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.24040150642395, "training_acc": 53.0, "val_loss": 17.335234582424164, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.27644896507263, "training_acc": 53.0, "val_loss": 17.31761395931244, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.17194366455078, "training_acc": 53.0, "val_loss": 17.312467098236084, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.10153031349182, "training_acc": 53.0, "val_loss": 17.305557429790497, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.18115043640137, "training_acc": 53.0, "val_loss": 17.30429232120514, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.20089817047119, "training_acc": 53.0, "val_loss": 17.304354906082153, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.2143042087555, "training_acc": 53.0, "val_loss": 17.30438619852066, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.19283270835876, "training_acc": 53.0, "val_loss": 17.304258048534393, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.17639303207397, "training_acc": 53.0, "val_loss": 17.306095361709595, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.14846968650818, "training_acc": 53.0, "val_loss": 17.30901747941971, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.18386769294739, "training_acc": 53.0, "val_loss": 17.31288880109787, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.18151068687439, "training_acc": 53.0, "val_loss": 17.326495051383972, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.20163059234619, "training_acc": 53.0, "val_loss": 17.336879670619965, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.14193344116211, "training_acc": 53.0, "val_loss": 17.370226979255676, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.29132771492004, "training_acc": 53.0, "val_loss": 17.38942414522171, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.33756422996521, "training_acc": 53.0, "val_loss": 17.377296090126038, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.34494638442993, "training_acc": 53.0, "val_loss": 17.333318293094635, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.2192177772522, "training_acc": 53.0, "val_loss": 17.31512099504471, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.17321419715881, "training_acc": 53.0, "val_loss": 17.311011254787445, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.17133498191833, "training_acc": 53.0, "val_loss": 17.308692634105682, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.16238594055176, "training_acc": 53.0, "val_loss": 17.304599285125732, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.32225179672241, "training_acc": 53.0, "val_loss": 17.31005758047104, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.22956013679504, "training_acc": 53.0, "val_loss": 17.30928122997284, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.24452376365662, "training_acc": 53.0, "val_loss": 17.306247353553772, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.21579122543335, "training_acc": 53.0, "val_loss": 17.30433851480484, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.12646222114563, "training_acc": 53.0, "val_loss": 17.31196939945221, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.11081027984619, "training_acc": 53.0, "val_loss": 17.329081892967224, "val_acc": 52.0}
