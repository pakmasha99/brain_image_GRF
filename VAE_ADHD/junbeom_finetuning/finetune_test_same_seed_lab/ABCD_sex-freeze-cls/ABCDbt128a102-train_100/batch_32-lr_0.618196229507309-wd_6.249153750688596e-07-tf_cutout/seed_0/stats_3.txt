"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 4192.943248748779, "training_acc": 42.0, "val_loss": 724.6937274932861, "val_acc": 52.0}
{"epoch": 1, "training_loss": 1633.2860870361328, "training_acc": 63.0, "val_loss": 777.5410175323486, "val_acc": 48.0}
{"epoch": 2, "training_loss": 1680.5372619628906, "training_acc": 64.0, "val_loss": 666.9211387634277, "val_acc": 44.0}
{"epoch": 3, "training_loss": 1486.1818923950195, "training_acc": 64.0, "val_loss": 891.5280342102051, "val_acc": 36.0}
{"epoch": 4, "training_loss": 1326.0652742385864, "training_acc": 60.0, "val_loss": 706.2526226043701, "val_acc": 28.0}
{"epoch": 5, "training_loss": 1502.1274719238281, "training_acc": 67.0, "val_loss": 634.4676494598389, "val_acc": 40.0}
{"epoch": 6, "training_loss": 1053.8477325439453, "training_acc": 67.0, "val_loss": 552.0645618438721, "val_acc": 56.0}
{"epoch": 7, "training_loss": 1174.8445129394531, "training_acc": 64.0, "val_loss": 468.1654453277588, "val_acc": 44.0}
{"epoch": 8, "training_loss": 1983.2463684082031, "training_acc": 61.0, "val_loss": 894.8159217834473, "val_acc": 44.0}
{"epoch": 9, "training_loss": 1117.6088256835938, "training_acc": 72.0, "val_loss": 897.6372718811035, "val_acc": 40.0}
{"epoch": 10, "training_loss": 2118.7538009880554, "training_acc": 58.0, "val_loss": 738.4834289550781, "val_acc": 36.0}
{"epoch": 11, "training_loss": 1418.5005722045898, "training_acc": 61.0, "val_loss": 1107.5509071350098, "val_acc": 44.0}
{"epoch": 12, "training_loss": 1356.3069553375244, "training_acc": 66.0, "val_loss": 797.6064205169678, "val_acc": 36.0}
{"epoch": 13, "training_loss": 701.8296051025391, "training_acc": 77.0, "val_loss": 903.4581184387207, "val_acc": 48.0}
{"epoch": 14, "training_loss": 944.2733527808159, "training_acc": 68.0, "val_loss": 785.3631973266602, "val_acc": 48.0}
{"epoch": 15, "training_loss": 819.0335521697998, "training_acc": 69.0, "val_loss": 878.1387329101562, "val_acc": 44.0}
{"epoch": 16, "training_loss": 436.4513397216797, "training_acc": 76.0, "val_loss": 1005.7361602783203, "val_acc": 36.0}
{"epoch": 17, "training_loss": 818.5453681945801, "training_acc": 79.0, "val_loss": 1068.5598373413086, "val_acc": 32.0}
{"epoch": 18, "training_loss": 905.51318359375, "training_acc": 78.0, "val_loss": 1223.9538192749023, "val_acc": 40.0}
{"epoch": 19, "training_loss": 1405.8174686431885, "training_acc": 63.0, "val_loss": 967.4527168273926, "val_acc": 56.0}
{"epoch": 20, "training_loss": 732.2463912963867, "training_acc": 68.0, "val_loss": 739.7073745727539, "val_acc": 48.0}
{"epoch": 21, "training_loss": 670.0130596160889, "training_acc": 74.0, "val_loss": 859.4366073608398, "val_acc": 40.0}
{"epoch": 22, "training_loss": 996.4931793212891, "training_acc": 66.0, "val_loss": 1021.0566520690918, "val_acc": 32.0}
{"epoch": 23, "training_loss": 718.0800247192383, "training_acc": 76.0, "val_loss": 929.5453071594238, "val_acc": 36.0}
{"epoch": 24, "training_loss": 613.1588754653931, "training_acc": 75.0, "val_loss": 1001.9369125366211, "val_acc": 32.0}
{"epoch": 25, "training_loss": 740.2817687988281, "training_acc": 70.0, "val_loss": 1046.493911743164, "val_acc": 44.0}
{"epoch": 26, "training_loss": 1926.0680494308472, "training_acc": 63.0, "val_loss": 1130.14497756958, "val_acc": 44.0}
