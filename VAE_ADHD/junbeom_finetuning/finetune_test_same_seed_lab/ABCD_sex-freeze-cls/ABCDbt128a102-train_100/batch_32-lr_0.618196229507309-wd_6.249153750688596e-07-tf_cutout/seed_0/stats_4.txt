"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 2359.2503395080566, "training_acc": 37.0, "val_loss": 1086.6110801696777, "val_acc": 56.0}
{"epoch": 1, "training_loss": 3407.337692260742, "training_acc": 57.0, "val_loss": 786.021089553833, "val_acc": 48.0}
{"epoch": 2, "training_loss": 2772.760284423828, "training_acc": 57.0, "val_loss": 1260.9115600585938, "val_acc": 40.0}
{"epoch": 3, "training_loss": 2382.7782440185547, "training_acc": 61.0, "val_loss": 1445.3214645385742, "val_acc": 52.0}
{"epoch": 4, "training_loss": 3258.7540893554688, "training_acc": 66.0, "val_loss": 1649.6158599853516, "val_acc": 48.0}
{"epoch": 5, "training_loss": 2324.533203125, "training_acc": 68.0, "val_loss": 1099.538516998291, "val_acc": 48.0}
{"epoch": 6, "training_loss": 3742.180030822754, "training_acc": 52.0, "val_loss": 1327.8518676757812, "val_acc": 48.0}
{"epoch": 7, "training_loss": 2120.426856994629, "training_acc": 60.0, "val_loss": 1194.8923110961914, "val_acc": 40.0}
{"epoch": 8, "training_loss": 2073.668670654297, "training_acc": 73.0, "val_loss": 1074.2531776428223, "val_acc": 48.0}
{"epoch": 9, "training_loss": 1520.841697692871, "training_acc": 67.0, "val_loss": 1161.118507385254, "val_acc": 44.0}
{"epoch": 10, "training_loss": 1685.37109375, "training_acc": 59.0, "val_loss": 911.8579864501953, "val_acc": 60.0}
{"epoch": 11, "training_loss": 1195.5339736938477, "training_acc": 72.0, "val_loss": 854.2584419250488, "val_acc": 56.0}
{"epoch": 12, "training_loss": 816.1351776123047, "training_acc": 72.0, "val_loss": 746.3645935058594, "val_acc": 48.0}
{"epoch": 13, "training_loss": 983.1678085327148, "training_acc": 71.0, "val_loss": 1054.0392875671387, "val_acc": 44.0}
{"epoch": 14, "training_loss": 1743.7020111083984, "training_acc": 64.0, "val_loss": 841.7003631591797, "val_acc": 40.0}
{"epoch": 15, "training_loss": 1246.3072052001953, "training_acc": 65.0, "val_loss": 944.3659782409668, "val_acc": 52.0}
{"epoch": 16, "training_loss": 884.9212036132812, "training_acc": 65.0, "val_loss": 862.3687744140625, "val_acc": 48.0}
{"epoch": 17, "training_loss": 833.9455642700195, "training_acc": 71.0, "val_loss": 854.0040969848633, "val_acc": 48.0}
{"epoch": 18, "training_loss": 1104.5688736326993, "training_acc": 71.0, "val_loss": 918.2222366333008, "val_acc": 44.0}
{"epoch": 19, "training_loss": 805.4599876403809, "training_acc": 72.0, "val_loss": 1102.5177955627441, "val_acc": 40.0}
{"epoch": 20, "training_loss": 1128.1721076965332, "training_acc": 73.0, "val_loss": 1099.583625793457, "val_acc": 48.0}
{"epoch": 21, "training_loss": 883.0858268737793, "training_acc": 71.0, "val_loss": 1092.605209350586, "val_acc": 44.0}
{"epoch": 22, "training_loss": 953.1510620117188, "training_acc": 72.0, "val_loss": 1122.2963333129883, "val_acc": 36.0}
{"epoch": 23, "training_loss": 1083.1429443359375, "training_acc": 65.0, "val_loss": 897.8594779968262, "val_acc": 40.0}
{"epoch": 24, "training_loss": 1018.0486869812012, "training_acc": 74.0, "val_loss": 1170.1699256896973, "val_acc": 48.0}
{"epoch": 25, "training_loss": 1474.9161205291748, "training_acc": 68.0, "val_loss": 845.5264091491699, "val_acc": 48.0}
{"epoch": 26, "training_loss": 1944.6917266845703, "training_acc": 60.0, "val_loss": 823.0073928833008, "val_acc": 40.0}
{"epoch": 27, "training_loss": 1391.0236358642578, "training_acc": 72.0, "val_loss": 1170.9806442260742, "val_acc": 44.0}
{"epoch": 28, "training_loss": 1450.502784729004, "training_acc": 71.0, "val_loss": 1278.271484375, "val_acc": 44.0}
{"epoch": 29, "training_loss": 1346.9326553344727, "training_acc": 68.0, "val_loss": 1333.5844039916992, "val_acc": 52.0}
{"epoch": 30, "training_loss": 965.503398835659, "training_acc": 79.0, "val_loss": 1244.6879386901855, "val_acc": 40.0}
{"epoch": 31, "training_loss": 1258.231201171875, "training_acc": 74.0, "val_loss": 1194.3111419677734, "val_acc": 52.0}
