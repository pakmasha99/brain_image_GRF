"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.47115802764893, "training_acc": 47.0, "val_loss": 17.162220180034637, "val_acc": 56.0}
{"epoch": 1, "training_loss": 69.51335525512695, "training_acc": 52.0, "val_loss": 17.17270463705063, "val_acc": 56.0}
{"epoch": 2, "training_loss": 69.51625442504883, "training_acc": 52.0, "val_loss": 17.239485681056976, "val_acc": 56.0}
{"epoch": 3, "training_loss": 69.33722448348999, "training_acc": 52.0, "val_loss": 17.16178059577942, "val_acc": 56.0}
{"epoch": 4, "training_loss": 69.33294105529785, "training_acc": 52.0, "val_loss": 17.219944298267365, "val_acc": 56.0}
{"epoch": 5, "training_loss": 69.26788640022278, "training_acc": 52.0, "val_loss": 17.176879942417145, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.23134326934814, "training_acc": 52.0, "val_loss": 17.18185842037201, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.29632902145386, "training_acc": 52.0, "val_loss": 17.212070524692535, "val_acc": 56.0}
{"epoch": 8, "training_loss": 69.30397272109985, "training_acc": 52.0, "val_loss": 17.204181849956512, "val_acc": 56.0}
{"epoch": 9, "training_loss": 69.36381912231445, "training_acc": 52.0, "val_loss": 17.308448255062103, "val_acc": 56.0}
{"epoch": 10, "training_loss": 69.43382406234741, "training_acc": 46.0, "val_loss": 17.54331737756729, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.68152213096619, "training_acc": 48.0, "val_loss": 17.363296449184418, "val_acc": 56.0}
{"epoch": 12, "training_loss": 69.68379735946655, "training_acc": 43.0, "val_loss": 17.138943076133728, "val_acc": 56.0}
{"epoch": 13, "training_loss": 69.26287984848022, "training_acc": 52.0, "val_loss": 17.135104537010193, "val_acc": 56.0}
{"epoch": 14, "training_loss": 69.8672022819519, "training_acc": 52.0, "val_loss": 17.160210013389587, "val_acc": 56.0}
{"epoch": 15, "training_loss": 70.0359718799591, "training_acc": 52.0, "val_loss": 17.146478593349457, "val_acc": 56.0}
{"epoch": 16, "training_loss": 70.1106424331665, "training_acc": 52.0, "val_loss": 17.148330807685852, "val_acc": 56.0}
{"epoch": 17, "training_loss": 69.65405440330505, "training_acc": 52.0, "val_loss": 17.153248190879822, "val_acc": 56.0}
{"epoch": 18, "training_loss": 69.0521993637085, "training_acc": 52.0, "val_loss": 17.40562468767166, "val_acc": 56.0}
{"epoch": 19, "training_loss": 69.69382762908936, "training_acc": 48.0, "val_loss": 17.72109419107437, "val_acc": 56.0}
{"epoch": 20, "training_loss": 70.11397433280945, "training_acc": 48.0, "val_loss": 17.596589028835297, "val_acc": 56.0}
{"epoch": 21, "training_loss": 69.67580032348633, "training_acc": 48.0, "val_loss": 17.37731248140335, "val_acc": 56.0}
{"epoch": 22, "training_loss": 69.55784511566162, "training_acc": 45.0, "val_loss": 17.225250601768494, "val_acc": 56.0}
{"epoch": 23, "training_loss": 69.20954895019531, "training_acc": 52.0, "val_loss": 17.18282252550125, "val_acc": 56.0}
{"epoch": 24, "training_loss": 69.50616455078125, "training_acc": 52.0, "val_loss": 17.130422592163086, "val_acc": 56.0}
{"epoch": 25, "training_loss": 69.38277101516724, "training_acc": 52.0, "val_loss": 17.143236100673676, "val_acc": 56.0}
{"epoch": 26, "training_loss": 69.23729228973389, "training_acc": 52.0, "val_loss": 17.16996729373932, "val_acc": 56.0}
{"epoch": 27, "training_loss": 69.13318133354187, "training_acc": 52.0, "val_loss": 17.236360907554626, "val_acc": 56.0}
{"epoch": 28, "training_loss": 69.25035667419434, "training_acc": 50.0, "val_loss": 17.276327311992645, "val_acc": 56.0}
{"epoch": 29, "training_loss": 69.18545484542847, "training_acc": 51.0, "val_loss": 17.178848385810852, "val_acc": 56.0}
{"epoch": 30, "training_loss": 69.6282696723938, "training_acc": 52.0, "val_loss": 17.13729500770569, "val_acc": 56.0}
{"epoch": 31, "training_loss": 69.8016996383667, "training_acc": 52.0, "val_loss": 17.209872603416443, "val_acc": 56.0}
{"epoch": 32, "training_loss": 70.8914725780487, "training_acc": 52.0, "val_loss": 17.347927391529083, "val_acc": 56.0}
{"epoch": 33, "training_loss": 71.40448188781738, "training_acc": 52.0, "val_loss": 17.272086441516876, "val_acc": 56.0}
{"epoch": 34, "training_loss": 70.28261804580688, "training_acc": 52.0, "val_loss": 17.127616703510284, "val_acc": 56.0}
{"epoch": 35, "training_loss": 69.45331859588623, "training_acc": 52.0, "val_loss": 17.306621372699738, "val_acc": 56.0}
{"epoch": 36, "training_loss": 69.39187145233154, "training_acc": 49.0, "val_loss": 17.424678802490234, "val_acc": 56.0}
{"epoch": 37, "training_loss": 69.39106035232544, "training_acc": 48.0, "val_loss": 17.349018156528473, "val_acc": 56.0}
{"epoch": 38, "training_loss": 69.25373816490173, "training_acc": 52.0, "val_loss": 17.29346364736557, "val_acc": 56.0}
{"epoch": 39, "training_loss": 69.28792762756348, "training_acc": 49.0, "val_loss": 17.305094003677368, "val_acc": 56.0}
{"epoch": 40, "training_loss": 69.41924047470093, "training_acc": 50.0, "val_loss": 17.360635101795197, "val_acc": 56.0}
{"epoch": 41, "training_loss": 69.24828100204468, "training_acc": 51.0, "val_loss": 17.284664511680603, "val_acc": 56.0}
{"epoch": 42, "training_loss": 69.22757816314697, "training_acc": 52.0, "val_loss": 17.27239191532135, "val_acc": 56.0}
{"epoch": 43, "training_loss": 69.22661542892456, "training_acc": 52.0, "val_loss": 17.274047434329987, "val_acc": 56.0}
{"epoch": 44, "training_loss": 69.27833819389343, "training_acc": 51.0, "val_loss": 17.367400228977203, "val_acc": 56.0}
{"epoch": 45, "training_loss": 69.3323245048523, "training_acc": 48.0, "val_loss": 17.35657900571823, "val_acc": 56.0}
{"epoch": 46, "training_loss": 69.28848075866699, "training_acc": 51.0, "val_loss": 17.278708517551422, "val_acc": 56.0}
{"epoch": 47, "training_loss": 69.1144437789917, "training_acc": 54.0, "val_loss": 17.17480570077896, "val_acc": 56.0}
{"epoch": 48, "training_loss": 69.24405193328857, "training_acc": 52.0, "val_loss": 17.151446640491486, "val_acc": 56.0}
{"epoch": 49, "training_loss": 69.10408759117126, "training_acc": 52.0, "val_loss": 17.213638126850128, "val_acc": 56.0}
{"epoch": 50, "training_loss": 69.14691233634949, "training_acc": 54.0, "val_loss": 17.41633117198944, "val_acc": 56.0}
{"epoch": 51, "training_loss": 69.53404903411865, "training_acc": 48.0, "val_loss": 17.694588005542755, "val_acc": 56.0}
{"epoch": 52, "training_loss": 70.25659108161926, "training_acc": 48.0, "val_loss": 17.842067778110504, "val_acc": 56.0}
{"epoch": 53, "training_loss": 70.32578325271606, "training_acc": 48.0, "val_loss": 17.64148473739624, "val_acc": 56.0}
