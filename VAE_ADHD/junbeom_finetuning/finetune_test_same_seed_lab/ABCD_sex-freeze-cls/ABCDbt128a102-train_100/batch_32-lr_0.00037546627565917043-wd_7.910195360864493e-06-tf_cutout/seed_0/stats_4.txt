"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 71.15483665466309, "training_acc": 47.0, "val_loss": 17.422787845134735, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.69271993637085, "training_acc": 47.0, "val_loss": 17.361266911029816, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.40676808357239, "training_acc": 46.0, "val_loss": 17.363286018371582, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.45704531669617, "training_acc": 47.0, "val_loss": 17.360757291316986, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.25045204162598, "training_acc": 50.0, "val_loss": 17.3238605260849, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.1199426651001, "training_acc": 53.0, "val_loss": 17.36777275800705, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.20058870315552, "training_acc": 53.0, "val_loss": 17.378467321395874, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.2084448337555, "training_acc": 53.0, "val_loss": 17.36070215702057, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.1177909374237, "training_acc": 53.0, "val_loss": 17.34556555747986, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.15790581703186, "training_acc": 53.0, "val_loss": 17.35178530216217, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.16349744796753, "training_acc": 53.0, "val_loss": 17.3435777425766, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.09754276275635, "training_acc": 53.0, "val_loss": 17.356640100479126, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.1504909992218, "training_acc": 53.0, "val_loss": 17.337560653686523, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.01004672050476, "training_acc": 53.0, "val_loss": 17.32138991355896, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.54452657699585, "training_acc": 42.0, "val_loss": 17.345546185970306, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.41830682754517, "training_acc": 48.0, "val_loss": 17.321106791496277, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.3221743106842, "training_acc": 53.0, "val_loss": 17.375655472278595, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.29329299926758, "training_acc": 53.0, "val_loss": 17.34727770090103, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.15891218185425, "training_acc": 53.0, "val_loss": 17.376765608787537, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.28436470031738, "training_acc": 53.0, "val_loss": 17.441178858280182, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.52397584915161, "training_acc": 53.0, "val_loss": 17.48582273721695, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.50697064399719, "training_acc": 53.0, "val_loss": 17.41624027490616, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.35344243049622, "training_acc": 53.0, "val_loss": 17.39657372236252, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.3241605758667, "training_acc": 53.0, "val_loss": 17.371295392513275, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.12992143630981, "training_acc": 53.0, "val_loss": 17.319074273109436, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.13355207443237, "training_acc": 53.0, "val_loss": 17.323099076747894, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.10507082939148, "training_acc": 53.0, "val_loss": 17.314910888671875, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.04980230331421, "training_acc": 53.0, "val_loss": 17.316585779190063, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.11752700805664, "training_acc": 53.0, "val_loss": 17.314891517162323, "val_acc": 52.0}
{"epoch": 29, "training_loss": 68.95280003547668, "training_acc": 53.0, "val_loss": 17.40773320198059, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.49670100212097, "training_acc": 53.0, "val_loss": 17.611728608608246, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.97557926177979, "training_acc": 53.0, "val_loss": 17.719557881355286, "val_acc": 52.0}
{"epoch": 32, "training_loss": 70.39110040664673, "training_acc": 53.0, "val_loss": 17.67410635948181, "val_acc": 52.0}
{"epoch": 33, "training_loss": 70.0137505531311, "training_acc": 53.0, "val_loss": 17.474956810474396, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.50868129730225, "training_acc": 53.0, "val_loss": 17.359739542007446, "val_acc": 52.0}
{"epoch": 35, "training_loss": 68.96435403823853, "training_acc": 53.0, "val_loss": 17.31918305158615, "val_acc": 52.0}
{"epoch": 36, "training_loss": 68.91914701461792, "training_acc": 53.0, "val_loss": 17.319011688232422, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.03746128082275, "training_acc": 59.0, "val_loss": 17.38642007112503, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.48965859413147, "training_acc": 47.0, "val_loss": 17.40601658821106, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.62521600723267, "training_acc": 47.0, "val_loss": 17.413721978664398, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.40027904510498, "training_acc": 48.0, "val_loss": 17.33219176530838, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.38638639450073, "training_acc": 54.0, "val_loss": 17.32226312160492, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.33089518547058, "training_acc": 49.0, "val_loss": 17.35420972108841, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.17695808410645, "training_acc": 53.0, "val_loss": 17.313353717327118, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.02802777290344, "training_acc": 53.0, "val_loss": 17.314723134040833, "val_acc": 52.0}
{"epoch": 45, "training_loss": 68.91199684143066, "training_acc": 53.0, "val_loss": 17.34680086374283, "val_acc": 52.0}
{"epoch": 46, "training_loss": 69.0161144733429, "training_acc": 53.0, "val_loss": 17.451074719429016, "val_acc": 52.0}
{"epoch": 47, "training_loss": 69.54422879219055, "training_acc": 53.0, "val_loss": 17.51641482114792, "val_acc": 52.0}
{"epoch": 48, "training_loss": 69.37820386886597, "training_acc": 53.0, "val_loss": 17.38303452730179, "val_acc": 52.0}
{"epoch": 49, "training_loss": 68.95993518829346, "training_acc": 53.0, "val_loss": 17.32233017683029, "val_acc": 52.0}
{"epoch": 50, "training_loss": 68.99460029602051, "training_acc": 53.0, "val_loss": 17.314891517162323, "val_acc": 52.0}
{"epoch": 51, "training_loss": 69.02047204971313, "training_acc": 53.0, "val_loss": 17.311103641986847, "val_acc": 52.0}
{"epoch": 52, "training_loss": 68.99012327194214, "training_acc": 53.0, "val_loss": 17.310987412929535, "val_acc": 52.0}
{"epoch": 53, "training_loss": 69.24373292922974, "training_acc": 53.0, "val_loss": 17.310357093811035, "val_acc": 52.0}
{"epoch": 54, "training_loss": 69.02507185935974, "training_acc": 53.0, "val_loss": 17.391464114189148, "val_acc": 52.0}
{"epoch": 55, "training_loss": 69.20575332641602, "training_acc": 53.0, "val_loss": 17.60466694831848, "val_acc": 52.0}
{"epoch": 56, "training_loss": 70.5415906906128, "training_acc": 53.0, "val_loss": 17.888647317886353, "val_acc": 52.0}
{"epoch": 57, "training_loss": 70.81209349632263, "training_acc": 53.0, "val_loss": 17.709586024284363, "val_acc": 52.0}
{"epoch": 58, "training_loss": 69.92760396003723, "training_acc": 53.0, "val_loss": 17.42570251226425, "val_acc": 52.0}
{"epoch": 59, "training_loss": 69.06350994110107, "training_acc": 53.0, "val_loss": 17.31591522693634, "val_acc": 52.0}
{"epoch": 60, "training_loss": 68.97884917259216, "training_acc": 53.0, "val_loss": 17.31138378381729, "val_acc": 52.0}
{"epoch": 61, "training_loss": 68.96145391464233, "training_acc": 53.0, "val_loss": 17.324185371398926, "val_acc": 52.0}
{"epoch": 62, "training_loss": 68.92656254768372, "training_acc": 53.0, "val_loss": 17.36578494310379, "val_acc": 52.0}
{"epoch": 63, "training_loss": 69.16337728500366, "training_acc": 53.0, "val_loss": 17.447035014629364, "val_acc": 52.0}
{"epoch": 64, "training_loss": 69.36911725997925, "training_acc": 53.0, "val_loss": 17.44549125432968, "val_acc": 52.0}
{"epoch": 65, "training_loss": 69.4720048904419, "training_acc": 53.0, "val_loss": 17.50008761882782, "val_acc": 52.0}
{"epoch": 66, "training_loss": 69.56969785690308, "training_acc": 53.0, "val_loss": 17.528152465820312, "val_acc": 52.0}
{"epoch": 67, "training_loss": 69.46252012252808, "training_acc": 53.0, "val_loss": 17.346268892288208, "val_acc": 52.0}
{"epoch": 68, "training_loss": 69.03191328048706, "training_acc": 53.0, "val_loss": 17.33751893043518, "val_acc": 52.0}
{"epoch": 69, "training_loss": 69.02267551422119, "training_acc": 51.0, "val_loss": 17.440280318260193, "val_acc": 52.0}
{"epoch": 70, "training_loss": 69.6927261352539, "training_acc": 47.0, "val_loss": 17.44348704814911, "val_acc": 52.0}
{"epoch": 71, "training_loss": 69.93270444869995, "training_acc": 47.0, "val_loss": 17.497344315052032, "val_acc": 52.0}
{"epoch": 72, "training_loss": 69.94546508789062, "training_acc": 47.0, "val_loss": 17.42628365755081, "val_acc": 52.0}
