"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.32771635055542, "training_acc": 51.0, "val_loss": 17.212751507759094, "val_acc": 56.0}
{"epoch": 1, "training_loss": 69.3415596485138, "training_acc": 52.0, "val_loss": 17.180761694908142, "val_acc": 56.0}
{"epoch": 2, "training_loss": 69.35796308517456, "training_acc": 52.0, "val_loss": 17.193762958049774, "val_acc": 56.0}
{"epoch": 3, "training_loss": 69.27586960792542, "training_acc": 52.0, "val_loss": 17.166274785995483, "val_acc": 56.0}
{"epoch": 4, "training_loss": 69.31739377975464, "training_acc": 52.0, "val_loss": 17.192882299423218, "val_acc": 56.0}
{"epoch": 5, "training_loss": 69.24478530883789, "training_acc": 52.0, "val_loss": 17.18132495880127, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.23769307136536, "training_acc": 52.0, "val_loss": 17.187535762786865, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.28135633468628, "training_acc": 52.0, "val_loss": 17.20358431339264, "val_acc": 56.0}
{"epoch": 8, "training_loss": 69.24697661399841, "training_acc": 52.0, "val_loss": 17.202994227409363, "val_acc": 56.0}
{"epoch": 9, "training_loss": 69.3139169216156, "training_acc": 52.0, "val_loss": 17.24647879600525, "val_acc": 56.0}
{"epoch": 10, "training_loss": 69.27926683425903, "training_acc": 48.0, "val_loss": 17.339088022708893, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.35813570022583, "training_acc": 51.0, "val_loss": 17.31429398059845, "val_acc": 56.0}
{"epoch": 12, "training_loss": 69.42232608795166, "training_acc": 44.0, "val_loss": 17.214038968086243, "val_acc": 56.0}
{"epoch": 13, "training_loss": 69.15503311157227, "training_acc": 52.0, "val_loss": 17.17197746038437, "val_acc": 56.0}
{"epoch": 14, "training_loss": 69.28536462783813, "training_acc": 52.0, "val_loss": 17.142952978610992, "val_acc": 56.0}
{"epoch": 15, "training_loss": 69.33451008796692, "training_acc": 52.0, "val_loss": 17.133967578411102, "val_acc": 56.0}
{"epoch": 16, "training_loss": 69.62432336807251, "training_acc": 52.0, "val_loss": 17.131800949573517, "val_acc": 56.0}
{"epoch": 17, "training_loss": 69.55983233451843, "training_acc": 52.0, "val_loss": 17.132465541362762, "val_acc": 56.0}
{"epoch": 18, "training_loss": 69.32992911338806, "training_acc": 52.0, "val_loss": 17.16238558292389, "val_acc": 56.0}
{"epoch": 19, "training_loss": 69.2368597984314, "training_acc": 52.0, "val_loss": 17.234157025814056, "val_acc": 56.0}
{"epoch": 20, "training_loss": 69.26743030548096, "training_acc": 52.0, "val_loss": 17.271453142166138, "val_acc": 56.0}
{"epoch": 21, "training_loss": 69.30099773406982, "training_acc": 49.0, "val_loss": 17.292071878910065, "val_acc": 56.0}
{"epoch": 22, "training_loss": 69.33421015739441, "training_acc": 48.0, "val_loss": 17.29438006877899, "val_acc": 56.0}
{"epoch": 23, "training_loss": 69.28554272651672, "training_acc": 56.0, "val_loss": 17.300738394260406, "val_acc": 56.0}
{"epoch": 24, "training_loss": 69.35539078712463, "training_acc": 49.0, "val_loss": 17.23695397377014, "val_acc": 56.0}
{"epoch": 25, "training_loss": 69.21872806549072, "training_acc": 52.0, "val_loss": 17.231668531894684, "val_acc": 56.0}
{"epoch": 26, "training_loss": 69.27393388748169, "training_acc": 52.0, "val_loss": 17.217856645584106, "val_acc": 56.0}
{"epoch": 27, "training_loss": 69.1998143196106, "training_acc": 52.0, "val_loss": 17.21908301115036, "val_acc": 56.0}
{"epoch": 28, "training_loss": 69.23069405555725, "training_acc": 52.0, "val_loss": 17.215748131275177, "val_acc": 56.0}
{"epoch": 29, "training_loss": 69.20383071899414, "training_acc": 52.0, "val_loss": 17.176534235477448, "val_acc": 56.0}
{"epoch": 30, "training_loss": 69.42877674102783, "training_acc": 52.0, "val_loss": 17.134742438793182, "val_acc": 56.0}
{"epoch": 31, "training_loss": 69.39956450462341, "training_acc": 52.0, "val_loss": 17.132171988487244, "val_acc": 56.0}
{"epoch": 32, "training_loss": 69.80742883682251, "training_acc": 52.0, "val_loss": 17.161650955677032, "val_acc": 56.0}
{"epoch": 33, "training_loss": 70.08451819419861, "training_acc": 52.0, "val_loss": 17.17507094144821, "val_acc": 56.0}
{"epoch": 34, "training_loss": 70.07287406921387, "training_acc": 52.0, "val_loss": 17.1441912651062, "val_acc": 56.0}
{"epoch": 35, "training_loss": 69.77122068405151, "training_acc": 52.0, "val_loss": 17.132197320461273, "val_acc": 56.0}
