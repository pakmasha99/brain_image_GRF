"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.69651460647583, "training_acc": 41.0, "val_loss": 17.33662039041519, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.0737361907959, "training_acc": 54.0, "val_loss": 17.329952120780945, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.10016393661499, "training_acc": 53.0, "val_loss": 17.38632470369339, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.34743881225586, "training_acc": 53.0, "val_loss": 17.435213923454285, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.4517891407013, "training_acc": 53.0, "val_loss": 17.43466407060623, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.44821453094482, "training_acc": 53.0, "val_loss": 17.395932972431183, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.30266046524048, "training_acc": 53.0, "val_loss": 17.344456911087036, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.2300169467926, "training_acc": 53.0, "val_loss": 17.326776683330536, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.17712354660034, "training_acc": 53.0, "val_loss": 17.32995957136154, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.39068078994751, "training_acc": 50.0, "val_loss": 17.34261065721512, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.35748243331909, "training_acc": 51.0, "val_loss": 17.32858568429947, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.22720718383789, "training_acc": 53.0, "val_loss": 17.326566576957703, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.22362780570984, "training_acc": 53.0, "val_loss": 17.326942086219788, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.17748379707336, "training_acc": 53.0, "val_loss": 17.332983016967773, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.2431755065918, "training_acc": 54.0, "val_loss": 17.340299487113953, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.28613615036011, "training_acc": 51.0, "val_loss": 17.348487675189972, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.34739065170288, "training_acc": 49.0, "val_loss": 17.348094284534454, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.3307511806488, "training_acc": 46.0, "val_loss": 17.331179976463318, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.27811765670776, "training_acc": 53.0, "val_loss": 17.326320707798004, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.19319748878479, "training_acc": 53.0, "val_loss": 17.330244183540344, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.08516931533813, "training_acc": 53.0, "val_loss": 17.347154021263123, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.18146085739136, "training_acc": 53.0, "val_loss": 17.386403679847717, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.31959295272827, "training_acc": 53.0, "val_loss": 17.422331869602203, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.37058305740356, "training_acc": 53.0, "val_loss": 17.416326701641083, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.42894792556763, "training_acc": 53.0, "val_loss": 17.428454756736755, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.37834882736206, "training_acc": 53.0, "val_loss": 17.385393381118774, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.19477796554565, "training_acc": 53.0, "val_loss": 17.351320385932922, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.13183355331421, "training_acc": 53.0, "val_loss": 17.328517138957977, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.5002429485321, "training_acc": 53.0, "val_loss": 17.337486147880554, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.27680826187134, "training_acc": 54.0, "val_loss": 17.337943613529205, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.29037261009216, "training_acc": 52.0, "val_loss": 17.337481677532196, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.31079006195068, "training_acc": 54.0, "val_loss": 17.327888309955597, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.23891186714172, "training_acc": 53.0, "val_loss": 17.32696145772934, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.1396632194519, "training_acc": 53.0, "val_loss": 17.329539358615875, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.1462287902832, "training_acc": 53.0, "val_loss": 17.339687049388885, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.15087366104126, "training_acc": 53.0, "val_loss": 17.34745353460312, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.16147899627686, "training_acc": 53.0, "val_loss": 17.356181144714355, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.24816131591797, "training_acc": 53.0, "val_loss": 17.394275963306427, "val_acc": 52.0}
