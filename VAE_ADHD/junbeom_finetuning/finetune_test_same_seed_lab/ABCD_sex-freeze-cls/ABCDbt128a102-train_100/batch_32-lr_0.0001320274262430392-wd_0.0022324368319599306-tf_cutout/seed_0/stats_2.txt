"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.15925884246826, "training_acc": 53.0, "val_loss": 17.299525439739227, "val_acc": 52.0}
{"epoch": 1, "training_loss": 68.95099830627441, "training_acc": 53.0, "val_loss": 17.32379049062729, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.02238178253174, "training_acc": 53.0, "val_loss": 17.33746826648712, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.08715558052063, "training_acc": 53.0, "val_loss": 17.322342097759247, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.15077757835388, "training_acc": 53.0, "val_loss": 17.29814112186432, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.05774688720703, "training_acc": 53.0, "val_loss": 17.296329140663147, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.06368494033813, "training_acc": 53.0, "val_loss": 17.2958642244339, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.07050514221191, "training_acc": 53.0, "val_loss": 17.298731207847595, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.03681182861328, "training_acc": 53.0, "val_loss": 17.30455756187439, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.03747987747192, "training_acc": 53.0, "val_loss": 17.316126823425293, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.06990337371826, "training_acc": 53.0, "val_loss": 17.32228994369507, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.04914093017578, "training_acc": 53.0, "val_loss": 17.30436533689499, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.09362816810608, "training_acc": 53.0, "val_loss": 17.296138405799866, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.09558391571045, "training_acc": 53.0, "val_loss": 17.30019301176071, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.06525993347168, "training_acc": 53.0, "val_loss": 17.30457991361618, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.0263397693634, "training_acc": 53.0, "val_loss": 17.30598211288452, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.01705312728882, "training_acc": 53.0, "val_loss": 17.302852869033813, "val_acc": 52.0}
{"epoch": 17, "training_loss": 68.99546408653259, "training_acc": 53.0, "val_loss": 17.303679883480072, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.06152153015137, "training_acc": 53.0, "val_loss": 17.308738827705383, "val_acc": 52.0}
{"epoch": 19, "training_loss": 68.99866771697998, "training_acc": 53.0, "val_loss": 17.305223643779755, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.00538849830627, "training_acc": 53.0, "val_loss": 17.3118457198143, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.05994892120361, "training_acc": 53.0, "val_loss": 17.320354282855988, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.03218245506287, "training_acc": 53.0, "val_loss": 17.31071025133133, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.02371191978455, "training_acc": 53.0, "val_loss": 17.309081554412842, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.0077645778656, "training_acc": 53.0, "val_loss": 17.322303354740143, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.048574924469, "training_acc": 53.0, "val_loss": 17.322997748851776, "val_acc": 52.0}
