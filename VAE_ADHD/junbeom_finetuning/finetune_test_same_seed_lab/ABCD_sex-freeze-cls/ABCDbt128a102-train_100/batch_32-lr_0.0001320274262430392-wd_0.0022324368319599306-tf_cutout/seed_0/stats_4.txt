"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.55954241752625, "training_acc": 42.0, "val_loss": 17.265668511390686, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.35438442230225, "training_acc": 47.0, "val_loss": 17.26762354373932, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.30094337463379, "training_acc": 51.0, "val_loss": 17.256034910678864, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.23362898826599, "training_acc": 53.0, "val_loss": 17.255721986293793, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.13233542442322, "training_acc": 53.0, "val_loss": 17.269355058670044, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.24157881736755, "training_acc": 53.0, "val_loss": 17.279304563999176, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.24308633804321, "training_acc": 53.0, "val_loss": 17.266003787517548, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.35272026062012, "training_acc": 53.0, "val_loss": 17.257341742515564, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.24016404151917, "training_acc": 53.0, "val_loss": 17.265748977661133, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.24266719818115, "training_acc": 53.0, "val_loss": 17.276103794574738, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.25985145568848, "training_acc": 53.0, "val_loss": 17.267410457134247, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.23138117790222, "training_acc": 53.0, "val_loss": 17.275531589984894, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.24658155441284, "training_acc": 53.0, "val_loss": 17.29557067155838, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.29989290237427, "training_acc": 53.0, "val_loss": 17.32068657875061, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.38187885284424, "training_acc": 53.0, "val_loss": 17.323993146419525, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.34506559371948, "training_acc": 53.0, "val_loss": 17.336824536323547, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.4597737789154, "training_acc": 53.0, "val_loss": 17.34078973531723, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.3307454586029, "training_acc": 53.0, "val_loss": 17.290298640727997, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.26750946044922, "training_acc": 53.0, "val_loss": 17.27645695209503, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.17230367660522, "training_acc": 53.0, "val_loss": 17.271138727664948, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.20323657989502, "training_acc": 53.0, "val_loss": 17.25763976573944, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.19658708572388, "training_acc": 53.0, "val_loss": 17.25471466779709, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.12477946281433, "training_acc": 53.0, "val_loss": 17.265035212039948, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.23824977874756, "training_acc": 53.0, "val_loss": 17.292895913124084, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.22677230834961, "training_acc": 53.0, "val_loss": 17.330758273601532, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.4434642791748, "training_acc": 53.0, "val_loss": 17.36959218978882, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.4884831905365, "training_acc": 53.0, "val_loss": 17.377106845378876, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.56065034866333, "training_acc": 53.0, "val_loss": 17.372459173202515, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.48155236244202, "training_acc": 53.0, "val_loss": 17.354735732078552, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.37608003616333, "training_acc": 53.0, "val_loss": 17.313890159130096, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.16963338851929, "training_acc": 53.0, "val_loss": 17.26783961057663, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.0795168876648, "training_acc": 53.0, "val_loss": 17.253200709819794, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.18407464027405, "training_acc": 53.0, "val_loss": 17.25943684577942, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.22593879699707, "training_acc": 53.0, "val_loss": 17.26592183113098, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.27261233329773, "training_acc": 57.0, "val_loss": 17.280077934265137, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.51318597793579, "training_acc": 46.0, "val_loss": 17.30940341949463, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.5116229057312, "training_acc": 47.0, "val_loss": 17.29010045528412, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.45138359069824, "training_acc": 45.0, "val_loss": 17.27617383003235, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.27729988098145, "training_acc": 54.0, "val_loss": 17.257918417453766, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.13388967514038, "training_acc": 53.0, "val_loss": 17.254094779491425, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.20953559875488, "training_acc": 53.0, "val_loss": 17.273972928524017, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.1961362361908, "training_acc": 53.0, "val_loss": 17.28004664182663, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.2542052268982, "training_acc": 53.0, "val_loss": 17.28403866291046, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.25509905815125, "training_acc": 53.0, "val_loss": 17.292451858520508, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.25172567367554, "training_acc": 53.0, "val_loss": 17.289675772190094, "val_acc": 52.0}
{"epoch": 45, "training_loss": 69.17861700057983, "training_acc": 53.0, "val_loss": 17.2770157456398, "val_acc": 52.0}
{"epoch": 46, "training_loss": 69.33114719390869, "training_acc": 53.0, "val_loss": 17.26728528738022, "val_acc": 52.0}
{"epoch": 47, "training_loss": 69.22465014457703, "training_acc": 53.0, "val_loss": 17.28884130716324, "val_acc": 52.0}
{"epoch": 48, "training_loss": 69.21294164657593, "training_acc": 53.0, "val_loss": 17.324198782444, "val_acc": 52.0}
{"epoch": 49, "training_loss": 69.45909595489502, "training_acc": 53.0, "val_loss": 17.38974303007126, "val_acc": 52.0}
{"epoch": 50, "training_loss": 69.52805233001709, "training_acc": 53.0, "val_loss": 17.401064932346344, "val_acc": 52.0}
