"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.25429487228394, "training_acc": 53.0, "val_loss": 17.258264124393463, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.32964181900024, "training_acc": 53.0, "val_loss": 17.30206608772278, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.34013175964355, "training_acc": 53.0, "val_loss": 17.32068806886673, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.3699254989624, "training_acc": 53.0, "val_loss": 17.32272058725357, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.38529443740845, "training_acc": 53.0, "val_loss": 17.325080931186676, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.37669563293457, "training_acc": 53.0, "val_loss": 17.33965426683426, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.41495323181152, "training_acc": 53.0, "val_loss": 17.344120144844055, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.39555597305298, "training_acc": 53.0, "val_loss": 17.320166528224945, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.3845329284668, "training_acc": 53.0, "val_loss": 17.32056736946106, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.26890087127686, "training_acc": 53.0, "val_loss": 17.290394008159637, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.21853375434875, "training_acc": 53.0, "val_loss": 17.26747155189514, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.17582368850708, "training_acc": 53.0, "val_loss": 17.250646650791168, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.39486265182495, "training_acc": 53.0, "val_loss": 17.250347137451172, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.28057909011841, "training_acc": 53.0, "val_loss": 17.251098155975342, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.24550366401672, "training_acc": 53.0, "val_loss": 17.251913249492645, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.26327180862427, "training_acc": 53.0, "val_loss": 17.247934639453888, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.20266222953796, "training_acc": 53.0, "val_loss": 17.247413098812103, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.17123413085938, "training_acc": 53.0, "val_loss": 17.248332500457764, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.20108795166016, "training_acc": 53.0, "val_loss": 17.25344955921173, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.2245101928711, "training_acc": 53.0, "val_loss": 17.2585591673851, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.15866184234619, "training_acc": 53.0, "val_loss": 17.265261709690094, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.20561599731445, "training_acc": 53.0, "val_loss": 17.291776835918427, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.40877175331116, "training_acc": 53.0, "val_loss": 17.30518639087677, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.35148334503174, "training_acc": 53.0, "val_loss": 17.27473735809326, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.19584822654724, "training_acc": 53.0, "val_loss": 17.25262552499771, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.2091736793518, "training_acc": 53.0, "val_loss": 17.247432470321655, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.18500232696533, "training_acc": 53.0, "val_loss": 17.248740792274475, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.21475172042847, "training_acc": 53.0, "val_loss": 17.247462272644043, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.1663658618927, "training_acc": 53.0, "val_loss": 17.2515407204628, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.22634601593018, "training_acc": 53.0, "val_loss": 17.265810072422028, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.23124504089355, "training_acc": 53.0, "val_loss": 17.26912260055542, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.24059772491455, "training_acc": 53.0, "val_loss": 17.27406233549118, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.28286695480347, "training_acc": 53.0, "val_loss": 17.258448898792267, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.15465831756592, "training_acc": 53.0, "val_loss": 17.250020802021027, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.13443613052368, "training_acc": 53.0, "val_loss": 17.249789834022522, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.36927556991577, "training_acc": 51.0, "val_loss": 17.27583408355713, "val_acc": 52.0}
