"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.47529077529907, "training_acc": 47.0, "val_loss": 17.16126799583435, "val_acc": 56.0}
{"epoch": 1, "training_loss": 69.51814937591553, "training_acc": 52.0, "val_loss": 17.172998189926147, "val_acc": 56.0}
{"epoch": 2, "training_loss": 69.52069544792175, "training_acc": 52.0, "val_loss": 17.241165041923523, "val_acc": 56.0}
{"epoch": 3, "training_loss": 69.33944869041443, "training_acc": 52.0, "val_loss": 17.16156303882599, "val_acc": 56.0}
{"epoch": 4, "training_loss": 69.33341789245605, "training_acc": 52.0, "val_loss": 17.22010225057602, "val_acc": 56.0}
{"epoch": 5, "training_loss": 69.2686858177185, "training_acc": 52.0, "val_loss": 17.17635691165924, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.23172855377197, "training_acc": 52.0, "val_loss": 17.181484401226044, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.29714035987854, "training_acc": 52.0, "val_loss": 17.212291061878204, "val_acc": 56.0}
{"epoch": 8, "training_loss": 69.30482816696167, "training_acc": 52.0, "val_loss": 17.20440983772278, "val_acc": 56.0}
{"epoch": 9, "training_loss": 69.36526203155518, "training_acc": 52.0, "val_loss": 17.310340702533722, "val_acc": 56.0}
{"epoch": 10, "training_loss": 69.43941879272461, "training_acc": 46.0, "val_loss": 17.548510432243347, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.68961691856384, "training_acc": 48.0, "val_loss": 17.36309975385666, "val_acc": 56.0}
{"epoch": 12, "training_loss": 69.69016146659851, "training_acc": 43.0, "val_loss": 17.137718200683594, "val_acc": 56.0}
{"epoch": 13, "training_loss": 69.2733063697815, "training_acc": 52.0, "val_loss": 17.136189341545105, "val_acc": 56.0}
{"epoch": 14, "training_loss": 69.88905954360962, "training_acc": 52.0, "val_loss": 17.161910235881805, "val_acc": 56.0}
{"epoch": 15, "training_loss": 70.05175828933716, "training_acc": 52.0, "val_loss": 17.146559059619904, "val_acc": 56.0}
{"epoch": 16, "training_loss": 70.10942506790161, "training_acc": 52.0, "val_loss": 17.147712409496307, "val_acc": 56.0}
{"epoch": 17, "training_loss": 69.64377975463867, "training_acc": 52.0, "val_loss": 17.155398428440094, "val_acc": 56.0}
{"epoch": 18, "training_loss": 69.04906868934631, "training_acc": 51.0, "val_loss": 17.41624027490616, "val_acc": 56.0}
{"epoch": 19, "training_loss": 69.72163081169128, "training_acc": 48.0, "val_loss": 17.734427750110626, "val_acc": 56.0}
{"epoch": 20, "training_loss": 70.13846778869629, "training_acc": 48.0, "val_loss": 17.59878545999527, "val_acc": 56.0}
{"epoch": 21, "training_loss": 69.67181015014648, "training_acc": 48.0, "val_loss": 17.372065782546997, "val_acc": 56.0}
{"epoch": 22, "training_loss": 69.55461311340332, "training_acc": 43.0, "val_loss": 17.21985638141632, "val_acc": 56.0}
{"epoch": 23, "training_loss": 69.20790195465088, "training_acc": 52.0, "val_loss": 17.179381847381592, "val_acc": 56.0}
{"epoch": 24, "training_loss": 69.51329064369202, "training_acc": 52.0, "val_loss": 17.1299010515213, "val_acc": 56.0}
{"epoch": 25, "training_loss": 69.38949489593506, "training_acc": 52.0, "val_loss": 17.143134772777557, "val_acc": 56.0}
{"epoch": 26, "training_loss": 69.2305154800415, "training_acc": 52.0, "val_loss": 17.17156618833542, "val_acc": 56.0}
{"epoch": 27, "training_loss": 69.1298360824585, "training_acc": 52.0, "val_loss": 17.24051684141159, "val_acc": 56.0}
{"epoch": 28, "training_loss": 69.25368404388428, "training_acc": 48.0, "val_loss": 17.280401289463043, "val_acc": 56.0}
{"epoch": 29, "training_loss": 69.18644642829895, "training_acc": 52.0, "val_loss": 17.178936302661896, "val_acc": 56.0}
{"epoch": 30, "training_loss": 69.63623189926147, "training_acc": 52.0, "val_loss": 17.138393223285675, "val_acc": 56.0}
{"epoch": 31, "training_loss": 69.82120513916016, "training_acc": 52.0, "val_loss": 17.214198410511017, "val_acc": 56.0}
{"epoch": 32, "training_loss": 70.93143844604492, "training_acc": 52.0, "val_loss": 17.354530096054077, "val_acc": 56.0}
{"epoch": 33, "training_loss": 71.44110345840454, "training_acc": 52.0, "val_loss": 17.273476719856262, "val_acc": 56.0}
{"epoch": 34, "training_loss": 70.27558970451355, "training_acc": 52.0, "val_loss": 17.127473652362823, "val_acc": 56.0}
{"epoch": 35, "training_loss": 69.44605541229248, "training_acc": 52.0, "val_loss": 17.316654324531555, "val_acc": 56.0}
{"epoch": 36, "training_loss": 69.40910267829895, "training_acc": 49.0, "val_loss": 17.435073852539062, "val_acc": 56.0}
{"epoch": 37, "training_loss": 69.40230131149292, "training_acc": 48.0, "val_loss": 17.351730167865753, "val_acc": 56.0}
{"epoch": 38, "training_loss": 69.24987030029297, "training_acc": 52.0, "val_loss": 17.29132831096649, "val_acc": 56.0}
{"epoch": 39, "training_loss": 69.2853147983551, "training_acc": 49.0, "val_loss": 17.301177978515625, "val_acc": 56.0}
{"epoch": 40, "training_loss": 69.4132399559021, "training_acc": 53.0, "val_loss": 17.356909811496735, "val_acc": 56.0}
{"epoch": 41, "training_loss": 69.24163055419922, "training_acc": 52.0, "val_loss": 17.281770706176758, "val_acc": 56.0}
{"epoch": 42, "training_loss": 69.22243022918701, "training_acc": 53.0, "val_loss": 17.271031439304352, "val_acc": 56.0}
{"epoch": 43, "training_loss": 69.22362422943115, "training_acc": 52.0, "val_loss": 17.274218797683716, "val_acc": 56.0}
{"epoch": 44, "training_loss": 69.28014373779297, "training_acc": 51.0, "val_loss": 17.370116710662842, "val_acc": 56.0}
{"epoch": 45, "training_loss": 69.33575701713562, "training_acc": 48.0, "val_loss": 17.358946800231934, "val_acc": 56.0}
{"epoch": 46, "training_loss": 69.29043960571289, "training_acc": 50.0, "val_loss": 17.27897673845291, "val_acc": 56.0}
{"epoch": 47, "training_loss": 69.11246752738953, "training_acc": 54.0, "val_loss": 17.173664271831512, "val_acc": 56.0}
{"epoch": 48, "training_loss": 69.24578881263733, "training_acc": 52.0, "val_loss": 17.150497436523438, "val_acc": 56.0}
{"epoch": 49, "training_loss": 69.1030056476593, "training_acc": 52.0, "val_loss": 17.213639616966248, "val_acc": 56.0}
{"epoch": 50, "training_loss": 69.14578986167908, "training_acc": 55.0, "val_loss": 17.420558631420135, "val_acc": 56.0}
{"epoch": 51, "training_loss": 69.54443907737732, "training_acc": 48.0, "val_loss": 17.703357338905334, "val_acc": 56.0}
{"epoch": 52, "training_loss": 70.28004717826843, "training_acc": 48.0, "val_loss": 17.84948706626892, "val_acc": 56.0}
{"epoch": 53, "training_loss": 70.33918190002441, "training_acc": 48.0, "val_loss": 17.640838027000427, "val_acc": 56.0}
