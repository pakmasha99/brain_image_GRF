"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.47677302360535, "training_acc": 53.0, "val_loss": 17.34086126089096, "val_acc": 52.0}
{"epoch": 1, "training_loss": 68.95574522018433, "training_acc": 53.0, "val_loss": 17.444652318954468, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.24182415008545, "training_acc": 53.0, "val_loss": 17.633140087127686, "val_acc": 52.0}
{"epoch": 3, "training_loss": 70.01340556144714, "training_acc": 53.0, "val_loss": 17.613485455513, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.70598316192627, "training_acc": 53.0, "val_loss": 17.431937158107758, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.40481281280518, "training_acc": 53.0, "val_loss": 17.332029342651367, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.29317426681519, "training_acc": 59.0, "val_loss": 17.372523248195648, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.44088888168335, "training_acc": 47.0, "val_loss": 17.35095977783203, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.19556641578674, "training_acc": 57.0, "val_loss": 17.330463230609894, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.12934613227844, "training_acc": 53.0, "val_loss": 17.330825328826904, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.36866354942322, "training_acc": 52.0, "val_loss": 17.34597682952881, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.2309775352478, "training_acc": 56.0, "val_loss": 17.331725358963013, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.05498576164246, "training_acc": 53.0, "val_loss": 17.332957684993744, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.08128595352173, "training_acc": 53.0, "val_loss": 17.37830340862274, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.13740348815918, "training_acc": 53.0, "val_loss": 17.42037534713745, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.27042579650879, "training_acc": 53.0, "val_loss": 17.444756627082825, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.32552766799927, "training_acc": 53.0, "val_loss": 17.3967644572258, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.09732675552368, "training_acc": 53.0, "val_loss": 17.33022779226303, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.308833360672, "training_acc": 50.0, "val_loss": 17.38322377204895, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.39556527137756, "training_acc": 55.0, "val_loss": 17.33155846595764, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.17832708358765, "training_acc": 53.0, "val_loss": 17.339809238910675, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.0526602268219, "training_acc": 53.0, "val_loss": 17.360907793045044, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.0798773765564, "training_acc": 53.0, "val_loss": 17.350320518016815, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.05230188369751, "training_acc": 53.0, "val_loss": 17.34960824251175, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.08873796463013, "training_acc": 53.0, "val_loss": 17.359551787376404, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.07318592071533, "training_acc": 53.0, "val_loss": 17.336899042129517, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.06034398078918, "training_acc": 53.0, "val_loss": 17.348836362361908, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.07353115081787, "training_acc": 53.0, "val_loss": 17.36772507429123, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.07276630401611, "training_acc": 53.0, "val_loss": 17.335091531276703, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.06405591964722, "training_acc": 53.0, "val_loss": 17.332665622234344, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.07168412208557, "training_acc": 53.0, "val_loss": 17.370745539665222, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.06109118461609, "training_acc": 53.0, "val_loss": 17.373326420783997, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.21432209014893, "training_acc": 53.0, "val_loss": 17.510996758937836, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.66219997406006, "training_acc": 53.0, "val_loss": 17.712220549583435, "val_acc": 52.0}
{"epoch": 34, "training_loss": 70.29551076889038, "training_acc": 53.0, "val_loss": 17.70922988653183, "val_acc": 52.0}
{"epoch": 35, "training_loss": 70.06593894958496, "training_acc": 53.0, "val_loss": 17.609259486198425, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.66301393508911, "training_acc": 53.0, "val_loss": 17.481108009815216, "val_acc": 52.0}
