"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 70.04701232910156, "training_acc": 54.0, "val_loss": 17.434599995613098, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.57775497436523, "training_acc": 53.0, "val_loss": 17.378772795200348, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.17645049095154, "training_acc": 53.0, "val_loss": 17.299148440361023, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.21264290809631, "training_acc": 53.0, "val_loss": 17.308439314365387, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.0863676071167, "training_acc": 53.0, "val_loss": 17.29576885700226, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.15669131278992, "training_acc": 53.0, "val_loss": 17.297326028347015, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.17559432983398, "training_acc": 53.0, "val_loss": 17.31237769126892, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.93754720687866, "training_acc": 39.0, "val_loss": 17.386317253112793, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.46357107162476, "training_acc": 46.0, "val_loss": 17.321041226387024, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.30979013442993, "training_acc": 52.0, "val_loss": 17.297907173633575, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.42345762252808, "training_acc": 53.0, "val_loss": 17.320413887500763, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.2615385055542, "training_acc": 53.0, "val_loss": 17.351573705673218, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.27711868286133, "training_acc": 53.0, "val_loss": 17.35149174928665, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.26826596260071, "training_acc": 53.0, "val_loss": 17.360806465148926, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.25355434417725, "training_acc": 53.0, "val_loss": 17.336612939834595, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.20417308807373, "training_acc": 53.0, "val_loss": 17.323140799999237, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.2669448852539, "training_acc": 53.0, "val_loss": 17.3817440867424, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.64586567878723, "training_acc": 53.0, "val_loss": 17.38162934780121, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.35112619400024, "training_acc": 53.0, "val_loss": 17.29707270860672, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.12681770324707, "training_acc": 54.0, "val_loss": 17.350900173187256, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.63970518112183, "training_acc": 47.0, "val_loss": 17.401258647441864, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.76476430892944, "training_acc": 47.0, "val_loss": 17.358313500881195, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.27538180351257, "training_acc": 56.0, "val_loss": 17.297621071338654, "val_acc": 52.0}
{"epoch": 23, "training_loss": 68.90784645080566, "training_acc": 53.0, "val_loss": 17.40492731332779, "val_acc": 52.0}
