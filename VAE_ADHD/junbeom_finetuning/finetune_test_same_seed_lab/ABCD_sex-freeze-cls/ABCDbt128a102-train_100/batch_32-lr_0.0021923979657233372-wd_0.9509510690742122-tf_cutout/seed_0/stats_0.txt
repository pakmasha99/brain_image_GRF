"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 73.03991508483887, "training_acc": 48.0, "val_loss": 17.755800485610962, "val_acc": 56.0}
{"epoch": 1, "training_loss": 72.7921838760376, "training_acc": 52.0, "val_loss": 17.672790586948395, "val_acc": 56.0}
{"epoch": 2, "training_loss": 72.89625215530396, "training_acc": 48.0, "val_loss": 17.40906983613968, "val_acc": 56.0}
{"epoch": 3, "training_loss": 71.12455296516418, "training_acc": 50.0, "val_loss": 17.43011176586151, "val_acc": 56.0}
{"epoch": 4, "training_loss": 70.8293092250824, "training_acc": 50.0, "val_loss": 17.50444769859314, "val_acc": 56.0}
{"epoch": 5, "training_loss": 69.75046372413635, "training_acc": 48.0, "val_loss": 17.254531383514404, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.15217638015747, "training_acc": 52.0, "val_loss": 17.153052985668182, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.34602212905884, "training_acc": 52.0, "val_loss": 17.20006912946701, "val_acc": 56.0}
{"epoch": 8, "training_loss": 69.70195484161377, "training_acc": 52.0, "val_loss": 17.20680445432663, "val_acc": 56.0}
{"epoch": 9, "training_loss": 69.91116142272949, "training_acc": 41.0, "val_loss": 17.96073466539383, "val_acc": 56.0}
{"epoch": 10, "training_loss": 72.096999168396, "training_acc": 48.0, "val_loss": 18.138007819652557, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.5030198097229, "training_acc": 48.0, "val_loss": 17.694106698036194, "val_acc": 56.0}
{"epoch": 12, "training_loss": 80.22561812400818, "training_acc": 52.0, "val_loss": 18.74592900276184, "val_acc": 56.0}
{"epoch": 13, "training_loss": 75.79496479034424, "training_acc": 52.0, "val_loss": 17.379607260227203, "val_acc": 56.0}
{"epoch": 14, "training_loss": 70.08065462112427, "training_acc": 48.0, "val_loss": 18.00183206796646, "val_acc": 56.0}
{"epoch": 15, "training_loss": 70.01431655883789, "training_acc": 48.0, "val_loss": 17.127740383148193, "val_acc": 56.0}
{"epoch": 16, "training_loss": 73.08034801483154, "training_acc": 52.0, "val_loss": 17.863593995571136, "val_acc": 56.0}
{"epoch": 17, "training_loss": 71.65274715423584, "training_acc": 52.0, "val_loss": 17.71048903465271, "val_acc": 56.0}
{"epoch": 18, "training_loss": 71.86523675918579, "training_acc": 48.0, "val_loss": 20.211467146873474, "val_acc": 44.0}
{"epoch": 19, "training_loss": 76.83247375488281, "training_acc": 48.0, "val_loss": 17.67352670431137, "val_acc": 56.0}
{"epoch": 20, "training_loss": 69.31266021728516, "training_acc": 52.0, "val_loss": 17.858847975730896, "val_acc": 56.0}
{"epoch": 21, "training_loss": 74.28089165687561, "training_acc": 52.0, "val_loss": 17.24497526884079, "val_acc": 56.0}
{"epoch": 22, "training_loss": 68.80492115020752, "training_acc": 54.0, "val_loss": 18.04508864879608, "val_acc": 56.0}
{"epoch": 23, "training_loss": 72.48195552825928, "training_acc": 48.0, "val_loss": 18.284572660923004, "val_acc": 56.0}
{"epoch": 24, "training_loss": 71.85535335540771, "training_acc": 46.0, "val_loss": 17.336246371269226, "val_acc": 56.0}
{"epoch": 25, "training_loss": 71.16906762123108, "training_acc": 52.0, "val_loss": 17.122629284858704, "val_acc": 56.0}
{"epoch": 26, "training_loss": 68.55508875846863, "training_acc": 52.0, "val_loss": 17.557059228420258, "val_acc": 56.0}
{"epoch": 27, "training_loss": 69.73952412605286, "training_acc": 48.0, "val_loss": 17.691490054130554, "val_acc": 56.0}
{"epoch": 28, "training_loss": 69.48581099510193, "training_acc": 47.0, "val_loss": 17.13116765022278, "val_acc": 56.0}
{"epoch": 29, "training_loss": 69.96451425552368, "training_acc": 52.0, "val_loss": 17.676158249378204, "val_acc": 56.0}
{"epoch": 30, "training_loss": 75.88949751853943, "training_acc": 52.0, "val_loss": 17.89657175540924, "val_acc": 56.0}
{"epoch": 31, "training_loss": 73.7637689113617, "training_acc": 52.0, "val_loss": 17.126010358333588, "val_acc": 56.0}
{"epoch": 32, "training_loss": 69.28335523605347, "training_acc": 52.0, "val_loss": 17.121106386184692, "val_acc": 56.0}
{"epoch": 33, "training_loss": 69.3428246974945, "training_acc": 52.0, "val_loss": 17.162834107875824, "val_acc": 56.0}
{"epoch": 34, "training_loss": 68.53144073486328, "training_acc": 54.0, "val_loss": 18.02682876586914, "val_acc": 56.0}
{"epoch": 35, "training_loss": 71.94172167778015, "training_acc": 48.0, "val_loss": 17.9123193025589, "val_acc": 56.0}
{"epoch": 36, "training_loss": 69.21503162384033, "training_acc": 52.0, "val_loss": 17.378242313861847, "val_acc": 56.0}
{"epoch": 37, "training_loss": 72.59216785430908, "training_acc": 52.0, "val_loss": 17.547035217285156, "val_acc": 56.0}
{"epoch": 38, "training_loss": 71.87562847137451, "training_acc": 52.0, "val_loss": 17.617805302143097, "val_acc": 56.0}
{"epoch": 39, "training_loss": 70.79157376289368, "training_acc": 48.0, "val_loss": 19.195233285427094, "val_acc": 44.0}
{"epoch": 40, "training_loss": 74.62585687637329, "training_acc": 48.0, "val_loss": 17.5908625125885, "val_acc": 56.0}
{"epoch": 41, "training_loss": 69.2680299282074, "training_acc": 54.0, "val_loss": 17.490437626838684, "val_acc": 56.0}
{"epoch": 42, "training_loss": 71.28105854988098, "training_acc": 52.0, "val_loss": 17.138175666332245, "val_acc": 56.0}
{"epoch": 43, "training_loss": 68.50121736526489, "training_acc": 62.0, "val_loss": 18.253158032894135, "val_acc": 56.0}
{"epoch": 44, "training_loss": 73.18874621391296, "training_acc": 48.0, "val_loss": 18.45375746488571, "val_acc": 56.0}
{"epoch": 45, "training_loss": 70.89047074317932, "training_acc": 48.0, "val_loss": 17.124728858470917, "val_acc": 56.0}
{"epoch": 46, "training_loss": 69.9775881767273, "training_acc": 52.0, "val_loss": 17.32190102338791, "val_acc": 56.0}
{"epoch": 47, "training_loss": 70.9453010559082, "training_acc": 52.0, "val_loss": 17.1236515045166, "val_acc": 56.0}
{"epoch": 48, "training_loss": 68.66116857528687, "training_acc": 55.0, "val_loss": 17.857390642166138, "val_acc": 56.0}
{"epoch": 49, "training_loss": 70.7730131149292, "training_acc": 48.0, "val_loss": 18.597491085529327, "val_acc": 56.0}
{"epoch": 50, "training_loss": 72.2412793636322, "training_acc": 48.0, "val_loss": 17.744925618171692, "val_acc": 56.0}
{"epoch": 51, "training_loss": 69.65044498443604, "training_acc": 46.0, "val_loss": 17.2830730676651, "val_acc": 56.0}
