"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 74.25817584991455, "training_acc": 45.0, "val_loss": 19.33789849281311, "val_acc": 52.0}
{"epoch": 1, "training_loss": 80.64831829071045, "training_acc": 53.0, "val_loss": 18.396711349487305, "val_acc": 52.0}
{"epoch": 2, "training_loss": 70.93182706832886, "training_acc": 53.0, "val_loss": 17.925044894218445, "val_acc": 52.0}
{"epoch": 3, "training_loss": 71.94857501983643, "training_acc": 47.0, "val_loss": 17.31947809457779, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.14248561859131, "training_acc": 52.0, "val_loss": 17.744596302509308, "val_acc": 52.0}
{"epoch": 5, "training_loss": 70.1614236831665, "training_acc": 53.0, "val_loss": 17.317719757556915, "val_acc": 52.0}
{"epoch": 6, "training_loss": 72.99058318138123, "training_acc": 45.0, "val_loss": 18.419115245342255, "val_acc": 52.0}
{"epoch": 7, "training_loss": 72.50995635986328, "training_acc": 47.0, "val_loss": 17.395681142807007, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.69216465950012, "training_acc": 53.0, "val_loss": 18.143585324287415, "val_acc": 52.0}
{"epoch": 9, "training_loss": 72.28874206542969, "training_acc": 53.0, "val_loss": 17.570123076438904, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.46685457229614, "training_acc": 51.0, "val_loss": 17.37731546163559, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.59154796600342, "training_acc": 47.0, "val_loss": 17.30007529258728, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.63873767852783, "training_acc": 53.0, "val_loss": 17.339476943016052, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.27903270721436, "training_acc": 51.0, "val_loss": 17.653805017471313, "val_acc": 52.0}
{"epoch": 14, "training_loss": 71.48296785354614, "training_acc": 47.0, "val_loss": 17.371316254138947, "val_acc": 52.0}
{"epoch": 15, "training_loss": 70.46914529800415, "training_acc": 49.0, "val_loss": 18.686187267303467, "val_acc": 52.0}
{"epoch": 16, "training_loss": 73.73386120796204, "training_acc": 53.0, "val_loss": 17.602841556072235, "val_acc": 52.0}
{"epoch": 17, "training_loss": 68.95531630516052, "training_acc": 57.0, "val_loss": 17.735102772712708, "val_acc": 52.0}
{"epoch": 18, "training_loss": 72.47678542137146, "training_acc": 47.0, "val_loss": 17.58185774087906, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.53402304649353, "training_acc": 47.0, "val_loss": 17.742212116718292, "val_acc": 52.0}
{"epoch": 20, "training_loss": 72.07967901229858, "training_acc": 53.0, "val_loss": 17.906753718852997, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.2032458782196, "training_acc": 51.0, "val_loss": 17.506110668182373, "val_acc": 52.0}
{"epoch": 22, "training_loss": 70.06030488014221, "training_acc": 47.0, "val_loss": 17.300163209438324, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.30343294143677, "training_acc": 53.0, "val_loss": 17.59953200817108, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.1551444530487, "training_acc": 53.0, "val_loss": 17.337188124656677, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.39154767990112, "training_acc": 48.0, "val_loss": 17.30128824710846, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.73090124130249, "training_acc": 53.0, "val_loss": 17.89509505033493, "val_acc": 52.0}
{"epoch": 27, "training_loss": 70.02797627449036, "training_acc": 53.0, "val_loss": 17.334099113941193, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.2380633354187, "training_acc": 53.0, "val_loss": 17.807166278362274, "val_acc": 52.0}
{"epoch": 29, "training_loss": 71.31912207603455, "training_acc": 53.0, "val_loss": 18.06224137544632, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.97824287414551, "training_acc": 53.0, "val_loss": 17.31049120426178, "val_acc": 52.0}
