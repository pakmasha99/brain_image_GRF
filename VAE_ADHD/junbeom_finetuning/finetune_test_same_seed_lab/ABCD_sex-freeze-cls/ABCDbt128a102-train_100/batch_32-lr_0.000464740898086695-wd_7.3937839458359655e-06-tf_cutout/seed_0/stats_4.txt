"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 70.94656705856323, "training_acc": 45.0, "val_loss": 17.367732524871826, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.16177320480347, "training_acc": 53.0, "val_loss": 17.369821667671204, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.30522155761719, "training_acc": 53.0, "val_loss": 17.37222671508789, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.51030778884888, "training_acc": 53.0, "val_loss": 17.370162904262543, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.14483547210693, "training_acc": 53.0, "val_loss": 17.37423539161682, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.39716339111328, "training_acc": 53.0, "val_loss": 17.483992874622345, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.71576452255249, "training_acc": 53.0, "val_loss": 17.544277012348175, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.66483974456787, "training_acc": 53.0, "val_loss": 17.383749783039093, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.19427299499512, "training_acc": 53.0, "val_loss": 17.367255687713623, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.19893097877502, "training_acc": 53.0, "val_loss": 17.36527532339096, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.77976417541504, "training_acc": 44.0, "val_loss": 17.41230934858322, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.38045120239258, "training_acc": 49.0, "val_loss": 17.375652492046356, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.35039854049683, "training_acc": 46.0, "val_loss": 17.370258271694183, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.04706287384033, "training_acc": 53.0, "val_loss": 17.369429767131805, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.12398195266724, "training_acc": 53.0, "val_loss": 17.363882064819336, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.2789695262909, "training_acc": 52.0, "val_loss": 17.394202947616577, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.3483567237854, "training_acc": 47.0, "val_loss": 17.379021644592285, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.15674352645874, "training_acc": 56.0, "val_loss": 17.364726960659027, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.10439157485962, "training_acc": 53.0, "val_loss": 17.365218698978424, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.0102014541626, "training_acc": 53.0, "val_loss": 17.42968112230301, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.30070662498474, "training_acc": 53.0, "val_loss": 17.719054222106934, "val_acc": 52.0}
{"epoch": 21, "training_loss": 70.52699375152588, "training_acc": 53.0, "val_loss": 17.86361038684845, "val_acc": 52.0}
{"epoch": 22, "training_loss": 70.50552082061768, "training_acc": 53.0, "val_loss": 17.580947279930115, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.40557360649109, "training_acc": 53.0, "val_loss": 17.379485070705414, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.15151596069336, "training_acc": 53.0, "val_loss": 17.37411469221115, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.39015769958496, "training_acc": 43.0, "val_loss": 17.38460659980774, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.28175139427185, "training_acc": 52.0, "val_loss": 17.39777773618698, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.44511938095093, "training_acc": 40.0, "val_loss": 17.37835854291916, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.30767464637756, "training_acc": 52.0, "val_loss": 17.38424450159073, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.1755735874176, "training_acc": 55.0, "val_loss": 17.362506687641144, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.1469578742981, "training_acc": 53.0, "val_loss": 17.37954616546631, "val_acc": 52.0}
{"epoch": 31, "training_loss": 68.97977066040039, "training_acc": 53.0, "val_loss": 17.360007762908936, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.1257734298706, "training_acc": 52.0, "val_loss": 17.387862503528595, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.15252351760864, "training_acc": 53.0, "val_loss": 17.363081872463226, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.0033860206604, "training_acc": 53.0, "val_loss": 17.39439368247986, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.12514400482178, "training_acc": 53.0, "val_loss": 17.39114075899124, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.03585243225098, "training_acc": 53.0, "val_loss": 17.376668751239777, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.05123090744019, "training_acc": 53.0, "val_loss": 17.372499406337738, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.0742814540863, "training_acc": 53.0, "val_loss": 17.393046617507935, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.08965516090393, "training_acc": 53.0, "val_loss": 17.3933744430542, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.0530698299408, "training_acc": 53.0, "val_loss": 17.416106164455414, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.12723517417908, "training_acc": 53.0, "val_loss": 17.38092452287674, "val_acc": 52.0}
{"epoch": 42, "training_loss": 68.89927434921265, "training_acc": 53.0, "val_loss": 17.367082834243774, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.5956163406372, "training_acc": 44.0, "val_loss": 17.404761910438538, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.48785257339478, "training_acc": 48.0, "val_loss": 17.364035546779633, "val_acc": 52.0}
{"epoch": 45, "training_loss": 69.31086683273315, "training_acc": 53.0, "val_loss": 17.457623779773712, "val_acc": 52.0}
{"epoch": 46, "training_loss": 69.2925477027893, "training_acc": 53.0, "val_loss": 17.39937514066696, "val_acc": 52.0}
{"epoch": 47, "training_loss": 69.13065791130066, "training_acc": 53.0, "val_loss": 17.42468774318695, "val_acc": 52.0}
{"epoch": 48, "training_loss": 69.25435400009155, "training_acc": 53.0, "val_loss": 17.48901754617691, "val_acc": 52.0}
{"epoch": 49, "training_loss": 69.4655179977417, "training_acc": 53.0, "val_loss": 17.522956430912018, "val_acc": 52.0}
{"epoch": 50, "training_loss": 69.39073395729065, "training_acc": 53.0, "val_loss": 17.432889342308044, "val_acc": 52.0}
