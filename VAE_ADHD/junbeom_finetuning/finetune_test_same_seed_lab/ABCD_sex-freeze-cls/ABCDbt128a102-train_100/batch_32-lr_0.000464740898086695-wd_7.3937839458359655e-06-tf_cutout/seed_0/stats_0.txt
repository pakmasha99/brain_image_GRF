"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.54773283004761, "training_acc": 48.0, "val_loss": 17.147916555404663, "val_acc": 56.0}
{"epoch": 1, "training_loss": 69.59880328178406, "training_acc": 52.0, "val_loss": 17.179736495018005, "val_acc": 56.0}
{"epoch": 2, "training_loss": 69.60080766677856, "training_acc": 47.0, "val_loss": 17.26951152086258, "val_acc": 56.0}
{"epoch": 3, "training_loss": 69.38198661804199, "training_acc": 52.0, "val_loss": 17.156249284744263, "val_acc": 56.0}
{"epoch": 4, "training_loss": 69.34409475326538, "training_acc": 52.0, "val_loss": 17.21929907798767, "val_acc": 56.0}
{"epoch": 5, "training_loss": 69.28122091293335, "training_acc": 52.0, "val_loss": 17.167624831199646, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.24089574813843, "training_acc": 52.0, "val_loss": 17.17708706855774, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.31183099746704, "training_acc": 52.0, "val_loss": 17.219369113445282, "val_acc": 56.0}
{"epoch": 8, "training_loss": 69.31976699829102, "training_acc": 52.0, "val_loss": 17.21019446849823, "val_acc": 56.0}
{"epoch": 9, "training_loss": 69.39026737213135, "training_acc": 52.0, "val_loss": 17.341533303260803, "val_acc": 56.0}
{"epoch": 10, "training_loss": 69.53622436523438, "training_acc": 47.0, "val_loss": 17.628106474876404, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.80519533157349, "training_acc": 48.0, "val_loss": 17.34869033098221, "val_acc": 56.0}
{"epoch": 12, "training_loss": 69.80039095878601, "training_acc": 43.0, "val_loss": 17.128777503967285, "val_acc": 56.0}
{"epoch": 13, "training_loss": 69.51581573486328, "training_acc": 52.0, "val_loss": 17.16621369123459, "val_acc": 56.0}
{"epoch": 14, "training_loss": 70.26429891586304, "training_acc": 52.0, "val_loss": 17.18679815530777, "val_acc": 56.0}
{"epoch": 15, "training_loss": 70.24597454071045, "training_acc": 52.0, "val_loss": 17.14143007993698, "val_acc": 56.0}
{"epoch": 16, "training_loss": 70.00193285942078, "training_acc": 52.0, "val_loss": 17.13516265153885, "val_acc": 56.0}
{"epoch": 17, "training_loss": 69.45202612876892, "training_acc": 52.0, "val_loss": 17.207393050193787, "val_acc": 56.0}
{"epoch": 18, "training_loss": 69.07609987258911, "training_acc": 51.0, "val_loss": 17.597320675849915, "val_acc": 56.0}
{"epoch": 19, "training_loss": 70.20489931106567, "training_acc": 48.0, "val_loss": 17.910368740558624, "val_acc": 56.0}
{"epoch": 20, "training_loss": 70.41536712646484, "training_acc": 48.0, "val_loss": 17.576023936271667, "val_acc": 56.0}
{"epoch": 21, "training_loss": 69.50005340576172, "training_acc": 48.0, "val_loss": 17.268960177898407, "val_acc": 56.0}
{"epoch": 22, "training_loss": 69.52683854103088, "training_acc": 52.0, "val_loss": 17.151841521263123, "val_acc": 56.0}
{"epoch": 23, "training_loss": 69.25147485733032, "training_acc": 52.0, "val_loss": 17.14472472667694, "val_acc": 56.0}
{"epoch": 24, "training_loss": 69.61085987091064, "training_acc": 52.0, "val_loss": 17.12787002325058, "val_acc": 56.0}
{"epoch": 25, "training_loss": 69.42051124572754, "training_acc": 52.0, "val_loss": 17.154011130332947, "val_acc": 56.0}
{"epoch": 26, "training_loss": 69.1025903224945, "training_acc": 52.0, "val_loss": 17.217805981636047, "val_acc": 56.0}
{"epoch": 27, "training_loss": 69.11618041992188, "training_acc": 53.0, "val_loss": 17.319194972515106, "val_acc": 56.0}
{"epoch": 28, "training_loss": 69.33425998687744, "training_acc": 50.0, "val_loss": 17.330265045166016, "val_acc": 56.0}
{"epoch": 29, "training_loss": 69.18977928161621, "training_acc": 52.0, "val_loss": 17.164036631584167, "val_acc": 56.0}
{"epoch": 30, "training_loss": 69.83227062225342, "training_acc": 52.0, "val_loss": 17.17865765094757, "val_acc": 56.0}
{"epoch": 31, "training_loss": 70.31916546821594, "training_acc": 52.0, "val_loss": 17.306677997112274, "val_acc": 56.0}
{"epoch": 32, "training_loss": 71.64427852630615, "training_acc": 52.0, "val_loss": 17.454105615615845, "val_acc": 56.0}
{"epoch": 33, "training_loss": 71.92413878440857, "training_acc": 52.0, "val_loss": 17.26854294538498, "val_acc": 56.0}
{"epoch": 34, "training_loss": 69.99002146720886, "training_acc": 52.0, "val_loss": 17.143943905830383, "val_acc": 56.0}
{"epoch": 35, "training_loss": 69.40084648132324, "training_acc": 52.0, "val_loss": 17.538931965827942, "val_acc": 56.0}
{"epoch": 36, "training_loss": 69.8530683517456, "training_acc": 48.0, "val_loss": 17.604179680347443, "val_acc": 56.0}
{"epoch": 37, "training_loss": 69.55992698669434, "training_acc": 48.0, "val_loss": 17.3544704914093, "val_acc": 56.0}
{"epoch": 38, "training_loss": 69.11861753463745, "training_acc": 62.0, "val_loss": 17.226696014404297, "val_acc": 56.0}
{"epoch": 39, "training_loss": 69.23229360580444, "training_acc": 52.0, "val_loss": 17.225700616836548, "val_acc": 56.0}
{"epoch": 40, "training_loss": 69.34146404266357, "training_acc": 49.0, "val_loss": 17.301805317401886, "val_acc": 56.0}
{"epoch": 41, "training_loss": 69.16129732131958, "training_acc": 56.0, "val_loss": 17.25534349679947, "val_acc": 56.0}
{"epoch": 42, "training_loss": 69.15573501586914, "training_acc": 52.0, "val_loss": 17.27469265460968, "val_acc": 56.0}
{"epoch": 43, "training_loss": 69.20298027992249, "training_acc": 54.0, "val_loss": 17.300015687942505, "val_acc": 56.0}
