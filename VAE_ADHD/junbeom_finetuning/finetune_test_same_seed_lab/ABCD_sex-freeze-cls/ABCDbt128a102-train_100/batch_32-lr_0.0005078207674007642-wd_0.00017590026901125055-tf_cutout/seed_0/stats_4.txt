"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 70.92553353309631, "training_acc": 41.0, "val_loss": 17.369699478149414, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.12993264198303, "training_acc": 53.0, "val_loss": 17.370589077472687, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.34578204154968, "training_acc": 52.0, "val_loss": 17.377670109272003, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.55898261070251, "training_acc": 54.0, "val_loss": 17.368656396865845, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.14199161529541, "training_acc": 53.0, "val_loss": 17.374716699123383, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.43604564666748, "training_acc": 53.0, "val_loss": 17.505565285682678, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.81327676773071, "training_acc": 53.0, "val_loss": 17.564047873020172, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.72834205627441, "training_acc": 53.0, "val_loss": 17.378802597522736, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.19579243659973, "training_acc": 53.0, "val_loss": 17.365485429763794, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.22650623321533, "training_acc": 53.0, "val_loss": 17.365962266921997, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.83718490600586, "training_acc": 38.0, "val_loss": 17.420202493667603, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.39569807052612, "training_acc": 49.0, "val_loss": 17.37290471792221, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.32179117202759, "training_acc": 51.0, "val_loss": 17.366977035999298, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.01751399040222, "training_acc": 53.0, "val_loss": 17.374366521835327, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.1239104270935, "training_acc": 53.0, "val_loss": 17.36360937356949, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.30286312103271, "training_acc": 51.0, "val_loss": 17.40235835313797, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.39186382293701, "training_acc": 47.0, "val_loss": 17.383265495300293, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.16692352294922, "training_acc": 56.0, "val_loss": 17.36483871936798, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.09723210334778, "training_acc": 53.0, "val_loss": 17.365916073322296, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.00245523452759, "training_acc": 53.0, "val_loss": 17.443077266216278, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.37292289733887, "training_acc": 53.0, "val_loss": 17.76837557554245, "val_acc": 52.0}
{"epoch": 21, "training_loss": 70.71034097671509, "training_acc": 53.0, "val_loss": 17.8952157497406, "val_acc": 52.0}
{"epoch": 22, "training_loss": 70.56126022338867, "training_acc": 53.0, "val_loss": 17.55715161561966, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.29941487312317, "training_acc": 53.0, "val_loss": 17.366458475589752, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.18038034439087, "training_acc": 51.0, "val_loss": 17.38917827606201, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.44953870773315, "training_acc": 46.0, "val_loss": 17.38884150981903, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.2856125831604, "training_acc": 48.0, "val_loss": 17.39055961370468, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.43083477020264, "training_acc": 46.0, "val_loss": 17.369236052036285, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.24524021148682, "training_acc": 47.0, "val_loss": 17.376068234443665, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.13343930244446, "training_acc": 52.0, "val_loss": 17.36484318971634, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.13885426521301, "training_acc": 53.0, "val_loss": 17.380812764167786, "val_acc": 52.0}
{"epoch": 31, "training_loss": 68.93871974945068, "training_acc": 53.0, "val_loss": 17.36103743314743, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.16141486167908, "training_acc": 53.0, "val_loss": 17.402148246765137, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.19290161132812, "training_acc": 57.0, "val_loss": 17.362140119075775, "val_acc": 52.0}
{"epoch": 34, "training_loss": 68.99112844467163, "training_acc": 53.0, "val_loss": 17.402338981628418, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.13864326477051, "training_acc": 53.0, "val_loss": 17.397914826869965, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.04088258743286, "training_acc": 53.0, "val_loss": 17.377454042434692, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.03776741027832, "training_acc": 53.0, "val_loss": 17.370493710041046, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.06370115280151, "training_acc": 53.0, "val_loss": 17.390933632850647, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.07680463790894, "training_acc": 53.0, "val_loss": 17.391152679920197, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.03780698776245, "training_acc": 53.0, "val_loss": 17.41647720336914, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.11889481544495, "training_acc": 53.0, "val_loss": 17.378486692905426, "val_acc": 52.0}
{"epoch": 42, "training_loss": 68.88213777542114, "training_acc": 53.0, "val_loss": 17.371708154678345, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.65989661216736, "training_acc": 47.0, "val_loss": 17.412589490413666, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.53685474395752, "training_acc": 49.0, "val_loss": 17.368245124816895, "val_acc": 52.0}
{"epoch": 45, "training_loss": 69.36606502532959, "training_acc": 53.0, "val_loss": 17.485924065113068, "val_acc": 52.0}
{"epoch": 46, "training_loss": 69.37045431137085, "training_acc": 53.0, "val_loss": 17.40381568670273, "val_acc": 52.0}
{"epoch": 47, "training_loss": 69.12599635124207, "training_acc": 53.0, "val_loss": 17.42188185453415, "val_acc": 52.0}
{"epoch": 48, "training_loss": 69.23268699645996, "training_acc": 53.0, "val_loss": 17.48294085264206, "val_acc": 52.0}
{"epoch": 49, "training_loss": 69.43880081176758, "training_acc": 53.0, "val_loss": 17.515233159065247, "val_acc": 52.0}
{"epoch": 50, "training_loss": 69.3551778793335, "training_acc": 53.0, "val_loss": 17.422164976596832, "val_acc": 52.0}
