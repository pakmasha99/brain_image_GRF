"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.58834981918335, "training_acc": 48.0, "val_loss": 17.142726480960846, "val_acc": 56.0}
{"epoch": 1, "training_loss": 69.6415343284607, "training_acc": 52.0, "val_loss": 17.18442291021347, "val_acc": 56.0}
{"epoch": 2, "training_loss": 69.64744281768799, "training_acc": 46.0, "val_loss": 17.284011840820312, "val_acc": 56.0}
{"epoch": 3, "training_loss": 69.40746307373047, "training_acc": 51.0, "val_loss": 17.152515053749084, "val_acc": 56.0}
{"epoch": 4, "training_loss": 69.35223770141602, "training_acc": 52.0, "val_loss": 17.217135429382324, "val_acc": 56.0}
{"epoch": 5, "training_loss": 69.28711938858032, "training_acc": 52.0, "val_loss": 17.163684964179993, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.24663829803467, "training_acc": 52.0, "val_loss": 17.17645972967148, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.31953287124634, "training_acc": 52.0, "val_loss": 17.225347459316254, "val_acc": 56.0}
{"epoch": 8, "training_loss": 69.33027029037476, "training_acc": 52.0, "val_loss": 17.213697731494904, "val_acc": 56.0}
{"epoch": 9, "training_loss": 69.4031982421875, "training_acc": 52.0, "val_loss": 17.355845868587494, "val_acc": 56.0}
{"epoch": 10, "training_loss": 69.58455324172974, "training_acc": 47.0, "val_loss": 17.662906646728516, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.84845161437988, "training_acc": 48.0, "val_loss": 17.33456701040268, "val_acc": 56.0}
{"epoch": 12, "training_loss": 69.86485695838928, "training_acc": 43.0, "val_loss": 17.13407039642334, "val_acc": 56.0}
{"epoch": 13, "training_loss": 69.6898787021637, "training_acc": 52.0, "val_loss": 17.18798726797104, "val_acc": 56.0}
{"epoch": 14, "training_loss": 70.45005536079407, "training_acc": 52.0, "val_loss": 17.194297909736633, "val_acc": 56.0}
{"epoch": 15, "training_loss": 70.28902244567871, "training_acc": 52.0, "val_loss": 17.135906219482422, "val_acc": 56.0}
{"epoch": 16, "training_loss": 69.90134620666504, "training_acc": 52.0, "val_loss": 17.130228877067566, "val_acc": 56.0}
{"epoch": 17, "training_loss": 69.35892105102539, "training_acc": 52.0, "val_loss": 17.24327951669693, "val_acc": 56.0}
{"epoch": 18, "training_loss": 69.13787364959717, "training_acc": 56.0, "val_loss": 17.68254041671753, "val_acc": 56.0}
{"epoch": 19, "training_loss": 70.42851495742798, "training_acc": 48.0, "val_loss": 17.960983514785767, "val_acc": 56.0}
{"epoch": 20, "training_loss": 70.45062971115112, "training_acc": 48.0, "val_loss": 17.531241476535797, "val_acc": 56.0}
{"epoch": 21, "training_loss": 69.37209892272949, "training_acc": 49.0, "val_loss": 17.219039797782898, "val_acc": 56.0}
{"epoch": 22, "training_loss": 69.55419945716858, "training_acc": 52.0, "val_loss": 17.135177552700043, "val_acc": 56.0}
{"epoch": 23, "training_loss": 69.30582523345947, "training_acc": 52.0, "val_loss": 17.13889241218567, "val_acc": 56.0}
{"epoch": 24, "training_loss": 69.61996507644653, "training_acc": 52.0, "val_loss": 17.127399146556854, "val_acc": 56.0}
{"epoch": 25, "training_loss": 69.37905097007751, "training_acc": 52.0, "val_loss": 17.169979214668274, "val_acc": 56.0}
{"epoch": 26, "training_loss": 69.0594892501831, "training_acc": 52.0, "val_loss": 17.2520712018013, "val_acc": 56.0}
{"epoch": 27, "training_loss": 69.14481210708618, "training_acc": 56.0, "val_loss": 17.354421317577362, "val_acc": 56.0}
{"epoch": 28, "training_loss": 69.3696677684784, "training_acc": 49.0, "val_loss": 17.334403097629547, "val_acc": 56.0}
{"epoch": 29, "training_loss": 69.17240810394287, "training_acc": 51.0, "val_loss": 17.14913100004196, "val_acc": 56.0}
{"epoch": 30, "training_loss": 69.98576736450195, "training_acc": 52.0, "val_loss": 17.218017578125, "val_acc": 56.0}
{"epoch": 31, "training_loss": 70.67823481559753, "training_acc": 52.0, "val_loss": 17.35915094614029, "val_acc": 56.0}
{"epoch": 32, "training_loss": 71.96895933151245, "training_acc": 52.0, "val_loss": 17.48347580432892, "val_acc": 56.0}
{"epoch": 33, "training_loss": 72.01699209213257, "training_acc": 52.0, "val_loss": 17.24536269903183, "val_acc": 56.0}
{"epoch": 34, "training_loss": 69.74208307266235, "training_acc": 52.0, "val_loss": 17.17536151409149, "val_acc": 56.0}
{"epoch": 35, "training_loss": 69.47014331817627, "training_acc": 48.0, "val_loss": 17.684412002563477, "val_acc": 56.0}
{"epoch": 36, "training_loss": 70.16064548492432, "training_acc": 48.0, "val_loss": 17.66730397939682, "val_acc": 56.0}
{"epoch": 37, "training_loss": 69.57162427902222, "training_acc": 48.0, "val_loss": 17.32175648212433, "val_acc": 56.0}
{"epoch": 38, "training_loss": 69.02264642715454, "training_acc": 57.0, "val_loss": 17.184866964817047, "val_acc": 56.0}
{"epoch": 39, "training_loss": 69.23090314865112, "training_acc": 52.0, "val_loss": 17.19166934490204, "val_acc": 56.0}
{"epoch": 40, "training_loss": 69.34784817695618, "training_acc": 51.0, "val_loss": 17.28883683681488, "val_acc": 56.0}
{"epoch": 41, "training_loss": 69.14777708053589, "training_acc": 54.0, "val_loss": 17.26185977458954, "val_acc": 56.0}
{"epoch": 42, "training_loss": 69.1412889957428, "training_acc": 52.0, "val_loss": 17.296132445335388, "val_acc": 56.0}
{"epoch": 43, "training_loss": 69.22394609451294, "training_acc": 47.0, "val_loss": 17.324502766132355, "val_acc": 56.0}
