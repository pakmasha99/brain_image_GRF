"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.25693154335022, "training_acc": 52.0, "val_loss": 17.23540723323822, "val_acc": 56.0}
{"epoch": 1, "training_loss": 69.28464365005493, "training_acc": 52.0, "val_loss": 17.231059074401855, "val_acc": 56.0}
{"epoch": 2, "training_loss": 69.27358675003052, "training_acc": 52.0, "val_loss": 17.23019778728485, "val_acc": 56.0}
{"epoch": 3, "training_loss": 69.22057700157166, "training_acc": 52.0, "val_loss": 17.225779592990875, "val_acc": 56.0}
{"epoch": 4, "training_loss": 69.28307008743286, "training_acc": 52.0, "val_loss": 17.226338386535645, "val_acc": 56.0}
{"epoch": 5, "training_loss": 69.23039937019348, "training_acc": 52.0, "val_loss": 17.223462462425232, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.23290276527405, "training_acc": 52.0, "val_loss": 17.22206622362137, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.26479196548462, "training_acc": 52.0, "val_loss": 17.22150146961212, "val_acc": 56.0}
{"epoch": 8, "training_loss": 69.22171211242676, "training_acc": 52.0, "val_loss": 17.21987873315811, "val_acc": 56.0}
{"epoch": 9, "training_loss": 69.28416585922241, "training_acc": 52.0, "val_loss": 17.22160428762436, "val_acc": 56.0}
{"epoch": 10, "training_loss": 69.25704145431519, "training_acc": 52.0, "val_loss": 17.22613424062729, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.23220610618591, "training_acc": 52.0, "val_loss": 17.22584366798401, "val_acc": 56.0}
{"epoch": 12, "training_loss": 69.25392127037048, "training_acc": 52.0, "val_loss": 17.22126603126526, "val_acc": 56.0}
{"epoch": 13, "training_loss": 69.22527313232422, "training_acc": 52.0, "val_loss": 17.218558490276337, "val_acc": 56.0}
{"epoch": 14, "training_loss": 69.26396107673645, "training_acc": 52.0, "val_loss": 17.214728891849518, "val_acc": 56.0}
{"epoch": 15, "training_loss": 69.23353910446167, "training_acc": 52.0, "val_loss": 17.211413383483887, "val_acc": 56.0}
{"epoch": 16, "training_loss": 69.29412031173706, "training_acc": 52.0, "val_loss": 17.206324636936188, "val_acc": 56.0}
{"epoch": 17, "training_loss": 69.24104595184326, "training_acc": 52.0, "val_loss": 17.205756902694702, "val_acc": 56.0}
{"epoch": 18, "training_loss": 69.22269439697266, "training_acc": 52.0, "val_loss": 17.207717895507812, "val_acc": 56.0}
{"epoch": 19, "training_loss": 69.22276520729065, "training_acc": 52.0, "val_loss": 17.210520803928375, "val_acc": 56.0}
{"epoch": 20, "training_loss": 69.26215124130249, "training_acc": 52.0, "val_loss": 17.211009562015533, "val_acc": 56.0}
{"epoch": 21, "training_loss": 69.26456928253174, "training_acc": 52.0, "val_loss": 17.211562395095825, "val_acc": 56.0}
{"epoch": 22, "training_loss": 69.29029083251953, "training_acc": 52.0, "val_loss": 17.212077975273132, "val_acc": 56.0}
{"epoch": 23, "training_loss": 69.24011039733887, "training_acc": 52.0, "val_loss": 17.21341460943222, "val_acc": 56.0}
{"epoch": 24, "training_loss": 69.24333930015564, "training_acc": 52.0, "val_loss": 17.21137911081314, "val_acc": 56.0}
{"epoch": 25, "training_loss": 69.23135089874268, "training_acc": 52.0, "val_loss": 17.212222516536713, "val_acc": 56.0}
{"epoch": 26, "training_loss": 69.23686838150024, "training_acc": 52.0, "val_loss": 17.2122985124588, "val_acc": 56.0}
{"epoch": 27, "training_loss": 69.2293815612793, "training_acc": 52.0, "val_loss": 17.213092744350433, "val_acc": 56.0}
{"epoch": 28, "training_loss": 69.2480936050415, "training_acc": 52.0, "val_loss": 17.213435471057892, "val_acc": 56.0}
{"epoch": 29, "training_loss": 69.23833918571472, "training_acc": 52.0, "val_loss": 17.210818827152252, "val_acc": 56.0}
{"epoch": 30, "training_loss": 69.27975797653198, "training_acc": 52.0, "val_loss": 17.205020785331726, "val_acc": 56.0}
{"epoch": 31, "training_loss": 69.28256797790527, "training_acc": 52.0, "val_loss": 17.199815809726715, "val_acc": 56.0}
{"epoch": 32, "training_loss": 69.26703572273254, "training_acc": 52.0, "val_loss": 17.19287782907486, "val_acc": 56.0}
{"epoch": 33, "training_loss": 69.24302387237549, "training_acc": 52.0, "val_loss": 17.188255488872528, "val_acc": 56.0}
{"epoch": 34, "training_loss": 69.25323748588562, "training_acc": 52.0, "val_loss": 17.18698889017105, "val_acc": 56.0}
{"epoch": 35, "training_loss": 69.28078937530518, "training_acc": 52.0, "val_loss": 17.187802493572235, "val_acc": 56.0}
{"epoch": 36, "training_loss": 69.2834415435791, "training_acc": 52.0, "val_loss": 17.18684732913971, "val_acc": 56.0}
{"epoch": 37, "training_loss": 69.2363908290863, "training_acc": 52.0, "val_loss": 17.185470461845398, "val_acc": 56.0}
{"epoch": 38, "training_loss": 69.25870275497437, "training_acc": 52.0, "val_loss": 17.185519635677338, "val_acc": 56.0}
{"epoch": 39, "training_loss": 69.30506610870361, "training_acc": 52.0, "val_loss": 17.187179625034332, "val_acc": 56.0}
{"epoch": 40, "training_loss": 69.27381229400635, "training_acc": 52.0, "val_loss": 17.190027236938477, "val_acc": 56.0}
{"epoch": 41, "training_loss": 69.22760057449341, "training_acc": 52.0, "val_loss": 17.191219329833984, "val_acc": 56.0}
{"epoch": 42, "training_loss": 69.2595272064209, "training_acc": 52.0, "val_loss": 17.193397879600525, "val_acc": 56.0}
{"epoch": 43, "training_loss": 69.2207236289978, "training_acc": 52.0, "val_loss": 17.19570755958557, "val_acc": 56.0}
{"epoch": 44, "training_loss": 69.23147964477539, "training_acc": 52.0, "val_loss": 17.199665307998657, "val_acc": 56.0}
{"epoch": 45, "training_loss": 69.26131629943848, "training_acc": 52.0, "val_loss": 17.202261090278625, "val_acc": 56.0}
{"epoch": 46, "training_loss": 69.25001096725464, "training_acc": 52.0, "val_loss": 17.20387041568756, "val_acc": 56.0}
{"epoch": 47, "training_loss": 69.23407292366028, "training_acc": 52.0, "val_loss": 17.20375418663025, "val_acc": 56.0}
{"epoch": 48, "training_loss": 69.25584125518799, "training_acc": 52.0, "val_loss": 17.204204201698303, "val_acc": 56.0}
{"epoch": 49, "training_loss": 69.23115921020508, "training_acc": 52.0, "val_loss": 17.206847667694092, "val_acc": 56.0}
{"epoch": 50, "training_loss": 69.2723536491394, "training_acc": 52.0, "val_loss": 17.211371660232544, "val_acc": 56.0}
{"epoch": 51, "training_loss": 69.28093385696411, "training_acc": 52.0, "val_loss": 17.21697598695755, "val_acc": 56.0}
{"epoch": 52, "training_loss": 69.26244831085205, "training_acc": 52.0, "val_loss": 17.222823202610016, "val_acc": 56.0}
{"epoch": 53, "training_loss": 69.25690197944641, "training_acc": 52.0, "val_loss": 17.22678691148758, "val_acc": 56.0}
{"epoch": 54, "training_loss": 69.24190068244934, "training_acc": 52.0, "val_loss": 17.231106758117676, "val_acc": 56.0}
{"epoch": 55, "training_loss": 69.2431092262268, "training_acc": 52.0, "val_loss": 17.23848730325699, "val_acc": 56.0}
{"epoch": 56, "training_loss": 69.26684927940369, "training_acc": 52.0, "val_loss": 17.248094081878662, "val_acc": 56.0}
