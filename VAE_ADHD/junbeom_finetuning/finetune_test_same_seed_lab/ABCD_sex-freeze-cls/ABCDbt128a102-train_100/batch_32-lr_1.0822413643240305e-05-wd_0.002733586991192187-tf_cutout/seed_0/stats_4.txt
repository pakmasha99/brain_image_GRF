"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.08894896507263, "training_acc": 53.0, "val_loss": 17.332598567008972, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.03256225585938, "training_acc": 53.0, "val_loss": 17.334434390068054, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.06649112701416, "training_acc": 53.0, "val_loss": 17.336437106132507, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.07031607627869, "training_acc": 53.0, "val_loss": 17.335593700408936, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.07118225097656, "training_acc": 53.0, "val_loss": 17.334695160388947, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.06370306015015, "training_acc": 53.0, "val_loss": 17.33410209417343, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.07824778556824, "training_acc": 53.0, "val_loss": 17.333631217479706, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.0629563331604, "training_acc": 53.0, "val_loss": 17.333923280239105, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.07376503944397, "training_acc": 53.0, "val_loss": 17.334575951099396, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.05581998825073, "training_acc": 53.0, "val_loss": 17.336730659008026, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.1067225933075, "training_acc": 53.0, "val_loss": 17.337234318256378, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.05538702011108, "training_acc": 53.0, "val_loss": 17.336151003837585, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.01943826675415, "training_acc": 53.0, "val_loss": 17.33444780111313, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.05332517623901, "training_acc": 53.0, "val_loss": 17.332009971141815, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.05435752868652, "training_acc": 53.0, "val_loss": 17.330045998096466, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.07428359985352, "training_acc": 53.0, "val_loss": 17.329534888267517, "val_acc": 52.0}
{"epoch": 16, "training_loss": 68.99540567398071, "training_acc": 53.0, "val_loss": 17.330028116703033, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.06543588638306, "training_acc": 53.0, "val_loss": 17.33051687479019, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.04603266716003, "training_acc": 53.0, "val_loss": 17.33010858297348, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.02118968963623, "training_acc": 53.0, "val_loss": 17.32957661151886, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.0814299583435, "training_acc": 53.0, "val_loss": 17.328429222106934, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.05065488815308, "training_acc": 53.0, "val_loss": 17.326928675174713, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.0522871017456, "training_acc": 53.0, "val_loss": 17.325511574745178, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.05230045318604, "training_acc": 53.0, "val_loss": 17.324264347553253, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.04964709281921, "training_acc": 53.0, "val_loss": 17.323949933052063, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.04199266433716, "training_acc": 53.0, "val_loss": 17.323781549930573, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.06891441345215, "training_acc": 53.0, "val_loss": 17.323552072048187, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.0936074256897, "training_acc": 53.0, "val_loss": 17.323224246501923, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.07383871078491, "training_acc": 53.0, "val_loss": 17.322632670402527, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.0606439113617, "training_acc": 53.0, "val_loss": 17.322146892547607, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.06316566467285, "training_acc": 53.0, "val_loss": 17.32184886932373, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.11905813217163, "training_acc": 53.0, "val_loss": 17.32177883386612, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.086177110672, "training_acc": 53.0, "val_loss": 17.32158064842224, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.07080411911011, "training_acc": 53.0, "val_loss": 17.321546375751495, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.0520851612091, "training_acc": 53.0, "val_loss": 17.321647703647614, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.04410600662231, "training_acc": 53.0, "val_loss": 17.321719229221344, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.05126762390137, "training_acc": 53.0, "val_loss": 17.321817576885223, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.06745219230652, "training_acc": 53.0, "val_loss": 17.321909964084625, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.04241228103638, "training_acc": 53.0, "val_loss": 17.32216775417328, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.0851263999939, "training_acc": 53.0, "val_loss": 17.322152853012085, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.05703353881836, "training_acc": 53.0, "val_loss": 17.322342097759247, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.04753255844116, "training_acc": 53.0, "val_loss": 17.322151362895966, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.09218955039978, "training_acc": 53.0, "val_loss": 17.322008311748505, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.0330810546875, "training_acc": 53.0, "val_loss": 17.3216313123703, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.11380672454834, "training_acc": 53.0, "val_loss": 17.32134521007538, "val_acc": 52.0}
{"epoch": 45, "training_loss": 69.09893035888672, "training_acc": 53.0, "val_loss": 17.32136458158493, "val_acc": 52.0}
{"epoch": 46, "training_loss": 69.10180759429932, "training_acc": 53.0, "val_loss": 17.32160598039627, "val_acc": 52.0}
{"epoch": 47, "training_loss": 69.03406834602356, "training_acc": 53.0, "val_loss": 17.322097718715668, "val_acc": 52.0}
{"epoch": 48, "training_loss": 69.07662391662598, "training_acc": 53.0, "val_loss": 17.32281893491745, "val_acc": 52.0}
{"epoch": 49, "training_loss": 69.06210565567017, "training_acc": 53.0, "val_loss": 17.323678731918335, "val_acc": 52.0}
{"epoch": 50, "training_loss": 69.01590991020203, "training_acc": 53.0, "val_loss": 17.324331402778625, "val_acc": 52.0}
{"epoch": 51, "training_loss": 69.05129909515381, "training_acc": 53.0, "val_loss": 17.324912548065186, "val_acc": 52.0}
{"epoch": 52, "training_loss": 68.99735403060913, "training_acc": 53.0, "val_loss": 17.325888574123383, "val_acc": 52.0}
{"epoch": 53, "training_loss": 69.05937194824219, "training_acc": 53.0, "val_loss": 17.327015101909637, "val_acc": 52.0}
{"epoch": 54, "training_loss": 69.05499124526978, "training_acc": 53.0, "val_loss": 17.32749491930008, "val_acc": 52.0}
