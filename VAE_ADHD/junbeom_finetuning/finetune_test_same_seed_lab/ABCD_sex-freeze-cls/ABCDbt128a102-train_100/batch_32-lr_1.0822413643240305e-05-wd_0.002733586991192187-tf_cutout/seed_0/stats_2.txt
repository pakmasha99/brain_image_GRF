"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.20795893669128, "training_acc": 53.0, "val_loss": 17.345546185970306, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.18166470527649, "training_acc": 53.0, "val_loss": 17.343074083328247, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.15900135040283, "training_acc": 53.0, "val_loss": 17.343279719352722, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.19033527374268, "training_acc": 53.0, "val_loss": 17.34136790037155, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.08312892913818, "training_acc": 53.0, "val_loss": 17.339007556438446, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.12773418426514, "training_acc": 53.0, "val_loss": 17.336852848529816, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.14754486083984, "training_acc": 53.0, "val_loss": 17.33548641204834, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.19046688079834, "training_acc": 53.0, "val_loss": 17.33555942773819, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.16052961349487, "training_acc": 53.0, "val_loss": 17.33570396900177, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.11603736877441, "training_acc": 53.0, "val_loss": 17.33352243900299, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.13295221328735, "training_acc": 53.0, "val_loss": 17.331506311893463, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.16932916641235, "training_acc": 53.0, "val_loss": 17.32986867427826, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.14796018600464, "training_acc": 53.0, "val_loss": 17.328201234340668, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.13971376419067, "training_acc": 53.0, "val_loss": 17.327140271663666, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.15116763114929, "training_acc": 53.0, "val_loss": 17.326998710632324, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.16631197929382, "training_acc": 53.0, "val_loss": 17.327411472797394, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.13906526565552, "training_acc": 53.0, "val_loss": 17.328236997127533, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.14034032821655, "training_acc": 53.0, "val_loss": 17.32785701751709, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.18121576309204, "training_acc": 53.0, "val_loss": 17.326751351356506, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.1272644996643, "training_acc": 53.0, "val_loss": 17.32531636953354, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.12160396575928, "training_acc": 53.0, "val_loss": 17.32385605573654, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.13856625556946, "training_acc": 53.0, "val_loss": 17.32298880815506, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.12161135673523, "training_acc": 53.0, "val_loss": 17.321953177452087, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.12598180770874, "training_acc": 53.0, "val_loss": 17.321328818798065, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.1920976638794, "training_acc": 53.0, "val_loss": 17.32145994901657, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.1247673034668, "training_acc": 53.0, "val_loss": 17.322222888469696, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.0987458229065, "training_acc": 53.0, "val_loss": 17.322778701782227, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.16380596160889, "training_acc": 53.0, "val_loss": 17.323333024978638, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.13986444473267, "training_acc": 53.0, "val_loss": 17.32410192489624, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.14139223098755, "training_acc": 53.0, "val_loss": 17.32565462589264, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.11142253875732, "training_acc": 53.0, "val_loss": 17.32775717973709, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.13805723190308, "training_acc": 53.0, "val_loss": 17.329326272010803, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.1218011379242, "training_acc": 53.0, "val_loss": 17.33085960149765, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.11813449859619, "training_acc": 53.0, "val_loss": 17.33059287071228, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.12982130050659, "training_acc": 53.0, "val_loss": 17.328888177871704, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.15748167037964, "training_acc": 53.0, "val_loss": 17.327983677387238, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.12645483016968, "training_acc": 53.0, "val_loss": 17.32795238494873, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.141845703125, "training_acc": 53.0, "val_loss": 17.328616976737976, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.13407945632935, "training_acc": 53.0, "val_loss": 17.32931137084961, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.12607955932617, "training_acc": 53.0, "val_loss": 17.328959703445435, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.14939880371094, "training_acc": 53.0, "val_loss": 17.329025268554688, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.1232123374939, "training_acc": 53.0, "val_loss": 17.329423129558563, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.16235446929932, "training_acc": 53.0, "val_loss": 17.32971966266632, "val_acc": 52.0}
