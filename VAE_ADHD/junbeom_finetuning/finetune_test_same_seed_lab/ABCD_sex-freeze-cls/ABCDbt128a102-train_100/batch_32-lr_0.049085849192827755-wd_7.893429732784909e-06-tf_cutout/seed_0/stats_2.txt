"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 468.3252000808716, "training_acc": 49.0, "val_loss": 242.05245971679688, "val_acc": 52.0}
{"epoch": 1, "training_loss": 939.3779697418213, "training_acc": 53.0, "val_loss": 26.095497608184814, "val_acc": 52.0}
{"epoch": 2, "training_loss": 467.35948181152344, "training_acc": 47.0, "val_loss": 167.5846815109253, "val_acc": 48.0}
{"epoch": 3, "training_loss": 378.89652252197266, "training_acc": 49.0, "val_loss": 148.0946660041809, "val_acc": 52.0}
{"epoch": 4, "training_loss": 669.4166088104248, "training_acc": 53.0, "val_loss": 100.01726150512695, "val_acc": 52.0}
{"epoch": 5, "training_loss": 342.76649475097656, "training_acc": 41.0, "val_loss": 115.52461385726929, "val_acc": 48.0}
{"epoch": 6, "training_loss": 268.2564344406128, "training_acc": 47.0, "val_loss": 94.28575038909912, "val_acc": 52.0}
{"epoch": 7, "training_loss": 389.5916328430176, "training_acc": 53.0, "val_loss": 23.619727790355682, "val_acc": 52.0}
{"epoch": 8, "training_loss": 193.61865282058716, "training_acc": 49.0, "val_loss": 61.17826700210571, "val_acc": 48.0}
{"epoch": 9, "training_loss": 147.29938507080078, "training_acc": 55.0, "val_loss": 56.80883526802063, "val_acc": 52.0}
{"epoch": 10, "training_loss": 144.25857734680176, "training_acc": 53.0, "val_loss": 33.69162678718567, "val_acc": 48.0}
{"epoch": 11, "training_loss": 93.71334457397461, "training_acc": 55.0, "val_loss": 29.616349935531616, "val_acc": 52.0}
{"epoch": 12, "training_loss": 108.5973482131958, "training_acc": 52.0, "val_loss": 17.70317107439041, "val_acc": 52.0}
{"epoch": 13, "training_loss": 86.5820255279541, "training_acc": 53.0, "val_loss": 35.80660820007324, "val_acc": 48.0}
{"epoch": 14, "training_loss": 197.656635761261, "training_acc": 47.0, "val_loss": 41.45597815513611, "val_acc": 52.0}
{"epoch": 15, "training_loss": 246.94798469543457, "training_acc": 53.0, "val_loss": 19.413764774799347, "val_acc": 52.0}
{"epoch": 16, "training_loss": 179.45090675354004, "training_acc": 47.0, "val_loss": 18.733154237270355, "val_acc": 52.0}
{"epoch": 17, "training_loss": 175.25975513458252, "training_acc": 47.0, "val_loss": 34.6773624420166, "val_acc": 52.0}
{"epoch": 18, "training_loss": 133.6272430419922, "training_acc": 49.0, "val_loss": 18.318255245685577, "val_acc": 44.0}
{"epoch": 19, "training_loss": 106.68799781799316, "training_acc": 57.0, "val_loss": 18.021708726882935, "val_acc": 52.0}
{"epoch": 20, "training_loss": 93.7843062877655, "training_acc": 55.0, "val_loss": 17.27782040834427, "val_acc": 52.0}
{"epoch": 21, "training_loss": 71.27797079086304, "training_acc": 61.0, "val_loss": 26.181533932685852, "val_acc": 48.0}
{"epoch": 22, "training_loss": 91.28898721933365, "training_acc": 51.0, "val_loss": 46.729543805122375, "val_acc": 52.0}
{"epoch": 23, "training_loss": 163.91903448104858, "training_acc": 53.0, "val_loss": 38.669195771217346, "val_acc": 48.0}
{"epoch": 24, "training_loss": 126.67492723464966, "training_acc": 50.0, "val_loss": 30.553916096687317, "val_acc": 52.0}
{"epoch": 25, "training_loss": 88.96606636047363, "training_acc": 53.0, "val_loss": 19.76749151945114, "val_acc": 48.0}
{"epoch": 26, "training_loss": 73.82945942878723, "training_acc": 57.0, "val_loss": 17.53421425819397, "val_acc": 52.0}
{"epoch": 27, "training_loss": 80.11155939102173, "training_acc": 53.0, "val_loss": 30.141964554786682, "val_acc": 52.0}
{"epoch": 28, "training_loss": 93.43647122383118, "training_acc": 58.0, "val_loss": 36.26815676689148, "val_acc": 48.0}
{"epoch": 29, "training_loss": 109.73728346824646, "training_acc": 49.0, "val_loss": 26.963233947753906, "val_acc": 52.0}
{"epoch": 30, "training_loss": 83.39215087890625, "training_acc": 53.0, "val_loss": 19.492870569229126, "val_acc": 52.0}
{"epoch": 31, "training_loss": 90.3545413017273, "training_acc": 55.0, "val_loss": 33.60814452171326, "val_acc": 48.0}
{"epoch": 32, "training_loss": 112.69483137130737, "training_acc": 49.0, "val_loss": 28.099718689918518, "val_acc": 52.0}
{"epoch": 33, "training_loss": 90.21482849121094, "training_acc": 51.0, "val_loss": 25.359809398651123, "val_acc": 52.0}
{"epoch": 34, "training_loss": 137.3418836593628, "training_acc": 53.0, "val_loss": 43.13027858734131, "val_acc": 48.0}
{"epoch": 35, "training_loss": 193.25733041763306, "training_acc": 47.0, "val_loss": 63.66352438926697, "val_acc": 52.0}
{"epoch": 36, "training_loss": 350.1307524316944, "training_acc": 53.0, "val_loss": 55.84943890571594, "val_acc": 52.0}
{"epoch": 37, "training_loss": 159.4433193206787, "training_acc": 51.0, "val_loss": 43.71967613697052, "val_acc": 48.0}
{"epoch": 38, "training_loss": 165.1229372024536, "training_acc": 45.0, "val_loss": 35.69833040237427, "val_acc": 52.0}
{"epoch": 39, "training_loss": 102.77851915359497, "training_acc": 59.0, "val_loss": 17.79949516057968, "val_acc": 52.0}
