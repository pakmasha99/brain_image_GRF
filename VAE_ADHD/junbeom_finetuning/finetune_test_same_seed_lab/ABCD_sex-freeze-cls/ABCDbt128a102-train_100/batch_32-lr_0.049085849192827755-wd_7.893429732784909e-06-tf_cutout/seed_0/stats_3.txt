"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 575.3181400299072, "training_acc": 52.0, "val_loss": 95.34662961959839, "val_acc": 48.0}
{"epoch": 1, "training_loss": 240.756938457489, "training_acc": 51.0, "val_loss": 91.10631346702576, "val_acc": 52.0}
{"epoch": 2, "training_loss": 243.9970245361328, "training_acc": 55.0, "val_loss": 97.53289818763733, "val_acc": 48.0}
{"epoch": 3, "training_loss": 300.17806845903397, "training_acc": 51.0, "val_loss": 89.95978832244873, "val_acc": 52.0}
{"epoch": 4, "training_loss": 408.98977851867676, "training_acc": 53.0, "val_loss": 17.305488884449005, "val_acc": 52.0}
{"epoch": 5, "training_loss": 240.2786750793457, "training_acc": 53.0, "val_loss": 55.50991892814636, "val_acc": 48.0}
{"epoch": 6, "training_loss": 190.6380558013916, "training_acc": 47.0, "val_loss": 50.82504749298096, "val_acc": 52.0}
{"epoch": 7, "training_loss": 139.00835417211056, "training_acc": 53.0, "val_loss": 62.71269917488098, "val_acc": 48.0}
{"epoch": 8, "training_loss": 153.6559441536665, "training_acc": 51.0, "val_loss": 88.14829587936401, "val_acc": 52.0}
{"epoch": 9, "training_loss": 353.11465644836426, "training_acc": 53.0, "val_loss": 22.52344787120819, "val_acc": 48.0}
{"epoch": 10, "training_loss": 213.3812198638916, "training_acc": 47.0, "val_loss": 17.650198936462402, "val_acc": 52.0}
{"epoch": 11, "training_loss": 225.3626585006714, "training_acc": 47.0, "val_loss": 46.46507799625397, "val_acc": 52.0}
{"epoch": 12, "training_loss": 134.94207763671875, "training_acc": 53.0, "val_loss": 44.61316466331482, "val_acc": 48.0}
{"epoch": 13, "training_loss": 127.33100295066833, "training_acc": 47.0, "val_loss": 38.131847977638245, "val_acc": 52.0}
{"epoch": 14, "training_loss": 122.41468572616577, "training_acc": 47.0, "val_loss": 22.628773748874664, "val_acc": 48.0}
{"epoch": 15, "training_loss": 101.94663763046265, "training_acc": 51.0, "val_loss": 18.15246492624283, "val_acc": 48.0}
{"epoch": 16, "training_loss": 88.73902130126953, "training_acc": 51.0, "val_loss": 47.19998836517334, "val_acc": 52.0}
{"epoch": 17, "training_loss": 208.5027642250061, "training_acc": 53.0, "val_loss": 34.728339314460754, "val_acc": 48.0}
{"epoch": 18, "training_loss": 144.212970495224, "training_acc": 45.0, "val_loss": 20.233440399169922, "val_acc": 52.0}
{"epoch": 19, "training_loss": 75.04451560974121, "training_acc": 48.0, "val_loss": 19.817858934402466, "val_acc": 48.0}
{"epoch": 20, "training_loss": 73.02380704879761, "training_acc": 49.0, "val_loss": 17.39751249551773, "val_acc": 52.0}
{"epoch": 21, "training_loss": 76.32108354568481, "training_acc": 49.0, "val_loss": 17.70990341901779, "val_acc": 52.0}
{"epoch": 22, "training_loss": 72.31231141090393, "training_acc": 45.0, "val_loss": 25.746124982833862, "val_acc": 52.0}
{"epoch": 23, "training_loss": 92.85357284545898, "training_acc": 47.0, "val_loss": 17.714567482471466, "val_acc": 52.0}
