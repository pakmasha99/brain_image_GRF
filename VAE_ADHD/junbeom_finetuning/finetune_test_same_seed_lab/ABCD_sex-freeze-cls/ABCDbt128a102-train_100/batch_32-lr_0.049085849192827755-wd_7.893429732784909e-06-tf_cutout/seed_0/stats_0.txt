"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 455.9178104400635, "training_acc": 48.0, "val_loss": 181.55324459075928, "val_acc": 56.0}
{"epoch": 1, "training_loss": 649.4939117431641, "training_acc": 52.0, "val_loss": 107.98462629318237, "val_acc": 44.0}
{"epoch": 2, "training_loss": 592.8494682312012, "training_acc": 48.0, "val_loss": 60.726284980773926, "val_acc": 44.0}
{"epoch": 3, "training_loss": 293.85136795043945, "training_acc": 50.0, "val_loss": 93.76104474067688, "val_acc": 56.0}
{"epoch": 4, "training_loss": 245.58023834228516, "training_acc": 50.0, "val_loss": 73.06870818138123, "val_acc": 44.0}
{"epoch": 5, "training_loss": 183.77545881271362, "training_acc": 44.0, "val_loss": 43.53964924812317, "val_acc": 56.0}
{"epoch": 6, "training_loss": 131.03844738006592, "training_acc": 50.0, "val_loss": 35.74041426181793, "val_acc": 44.0}
{"epoch": 7, "training_loss": 97.37479543685913, "training_acc": 52.0, "val_loss": 24.056771397590637, "val_acc": 56.0}
{"epoch": 8, "training_loss": 78.1625862121582, "training_acc": 58.0, "val_loss": 20.940524339675903, "val_acc": 44.0}
{"epoch": 9, "training_loss": 83.9400577545166, "training_acc": 58.0, "val_loss": 31.586244702339172, "val_acc": 44.0}
{"epoch": 10, "training_loss": 151.040105342865, "training_acc": 48.0, "val_loss": 36.63865625858307, "val_acc": 56.0}
{"epoch": 11, "training_loss": 197.89998316764832, "training_acc": 52.0, "val_loss": 17.335398495197296, "val_acc": 56.0}
{"epoch": 12, "training_loss": 73.42199492454529, "training_acc": 53.0, "val_loss": 18.03450733423233, "val_acc": 56.0}
{"epoch": 13, "training_loss": 72.80888175964355, "training_acc": 54.0, "val_loss": 18.458576500415802, "val_acc": 56.0}
{"epoch": 14, "training_loss": 76.69870376586914, "training_acc": 53.0, "val_loss": 19.348274171352386, "val_acc": 44.0}
{"epoch": 15, "training_loss": 69.93684887886047, "training_acc": 52.0, "val_loss": 25.554654002189636, "val_acc": 56.0}
{"epoch": 16, "training_loss": 93.83910965919495, "training_acc": 54.0, "val_loss": 42.713820934295654, "val_acc": 44.0}
{"epoch": 17, "training_loss": 116.45878601074219, "training_acc": 50.0, "val_loss": 18.334239721298218, "val_acc": 56.0}
{"epoch": 18, "training_loss": 93.52778816223145, "training_acc": 54.0, "val_loss": 17.069518566131592, "val_acc": 56.0}
{"epoch": 19, "training_loss": 72.09280967712402, "training_acc": 50.0, "val_loss": 17.06165075302124, "val_acc": 56.0}
{"epoch": 20, "training_loss": 68.76386857032776, "training_acc": 53.0, "val_loss": 17.658378183841705, "val_acc": 56.0}
{"epoch": 21, "training_loss": 67.55353116989136, "training_acc": 65.0, "val_loss": 17.082789540290833, "val_acc": 56.0}
{"epoch": 22, "training_loss": 66.5198826789856, "training_acc": 58.0, "val_loss": 26.365837454795837, "val_acc": 44.0}
{"epoch": 23, "training_loss": 88.82032871246338, "training_acc": 46.0, "val_loss": 25.86040496826172, "val_acc": 56.0}
{"epoch": 24, "training_loss": 88.42892336845398, "training_acc": 54.0, "val_loss": 36.552080512046814, "val_acc": 44.0}
{"epoch": 25, "training_loss": 101.80683755874634, "training_acc": 53.0, "val_loss": 17.27110594511032, "val_acc": 56.0}
{"epoch": 26, "training_loss": 73.03531694412231, "training_acc": 58.0, "val_loss": 17.178353667259216, "val_acc": 56.0}
{"epoch": 27, "training_loss": 67.39302086830139, "training_acc": 58.0, "val_loss": 17.88758486509323, "val_acc": 56.0}
{"epoch": 28, "training_loss": 69.58015298843384, "training_acc": 49.0, "val_loss": 20.749203860759735, "val_acc": 56.0}
{"epoch": 29, "training_loss": 86.59908390045166, "training_acc": 48.0, "val_loss": 27.233028411865234, "val_acc": 56.0}
{"epoch": 30, "training_loss": 151.1930389404297, "training_acc": 50.0, "val_loss": 29.344379901885986, "val_acc": 44.0}
{"epoch": 31, "training_loss": 75.87531733512878, "training_acc": 58.0, "val_loss": 39.776745438575745, "val_acc": 56.0}
{"epoch": 32, "training_loss": 125.98593807220459, "training_acc": 52.0, "val_loss": 23.829764127731323, "val_acc": 44.0}
{"epoch": 33, "training_loss": 88.54972410202026, "training_acc": 56.0, "val_loss": 35.50363779067993, "val_acc": 44.0}
{"epoch": 34, "training_loss": 126.89891004562378, "training_acc": 46.0, "val_loss": 19.822150468826294, "val_acc": 56.0}
{"epoch": 35, "training_loss": 85.34167218208313, "training_acc": 48.0, "val_loss": 31.398984789848328, "val_acc": 56.0}
{"epoch": 36, "training_loss": 126.66535758972168, "training_acc": 50.0, "val_loss": 24.746431410312653, "val_acc": 44.0}
{"epoch": 37, "training_loss": 80.26213073730469, "training_acc": 52.0, "val_loss": 19.72183585166931, "val_acc": 52.0}
{"epoch": 38, "training_loss": 84.27307748794556, "training_acc": 46.0, "val_loss": 18.67363303899765, "val_acc": 56.0}
