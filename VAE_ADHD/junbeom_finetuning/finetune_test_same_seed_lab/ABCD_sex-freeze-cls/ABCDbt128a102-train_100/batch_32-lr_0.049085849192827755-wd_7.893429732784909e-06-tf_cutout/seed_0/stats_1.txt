"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 508.1065158843994, "training_acc": 48.0, "val_loss": 206.91754817962646, "val_acc": 52.0}
{"epoch": 1, "training_loss": 837.4288654327393, "training_acc": 53.0, "val_loss": 36.32799685001373, "val_acc": 52.0}
{"epoch": 2, "training_loss": 320.9002113342285, "training_acc": 55.0, "val_loss": 94.80822682380676, "val_acc": 48.0}
{"epoch": 3, "training_loss": 224.88430976867676, "training_acc": 53.0, "val_loss": 132.9086422920227, "val_acc": 52.0}
{"epoch": 4, "training_loss": 422.1082441806793, "training_acc": 53.0, "val_loss": 52.54544019699097, "val_acc": 48.0}
{"epoch": 5, "training_loss": 297.5170912742615, "training_acc": 47.0, "val_loss": 20.044293999671936, "val_acc": 52.0}
{"epoch": 6, "training_loss": 111.69623446464539, "training_acc": 53.0, "val_loss": 18.48667711019516, "val_acc": 48.0}
{"epoch": 7, "training_loss": 90.12177658081055, "training_acc": 47.0, "val_loss": 27.48618721961975, "val_acc": 52.0}
{"epoch": 8, "training_loss": 86.55554413795471, "training_acc": 53.0, "val_loss": 31.4113050699234, "val_acc": 48.0}
{"epoch": 9, "training_loss": 107.37567090988159, "training_acc": 47.0, "val_loss": 34.57477390766144, "val_acc": 52.0}
{"epoch": 10, "training_loss": 136.4753713607788, "training_acc": 45.0, "val_loss": 18.24193000793457, "val_acc": 52.0}
{"epoch": 11, "training_loss": 96.7019100189209, "training_acc": 55.0, "val_loss": 52.078479528427124, "val_acc": 48.0}
{"epoch": 12, "training_loss": 234.16302919387817, "training_acc": 46.0, "val_loss": 31.630343198776245, "val_acc": 52.0}
{"epoch": 13, "training_loss": 122.14741039276123, "training_acc": 49.0, "val_loss": 18.960927426815033, "val_acc": 40.0}
{"epoch": 14, "training_loss": 89.35074996948242, "training_acc": 53.0, "val_loss": 17.61951446533203, "val_acc": 52.0}
{"epoch": 15, "training_loss": 73.77323126792908, "training_acc": 40.0, "val_loss": 25.62432885169983, "val_acc": 48.0}
{"epoch": 16, "training_loss": 88.43137526512146, "training_acc": 47.0, "val_loss": 26.998531818389893, "val_acc": 52.0}
{"epoch": 17, "training_loss": 95.55241370201111, "training_acc": 44.0, "val_loss": 19.061279296875, "val_acc": 40.0}
{"epoch": 18, "training_loss": 70.57375001907349, "training_acc": 53.0, "val_loss": 17.76241660118103, "val_acc": 52.0}
{"epoch": 19, "training_loss": 76.7637882232666, "training_acc": 51.0, "val_loss": 27.97164022922516, "val_acc": 52.0}
{"epoch": 20, "training_loss": 90.73784756660461, "training_acc": 49.0, "val_loss": 21.121156215667725, "val_acc": 48.0}
{"epoch": 21, "training_loss": 72.92222094535828, "training_acc": 53.0, "val_loss": 23.33541363477707, "val_acc": 52.0}
{"epoch": 22, "training_loss": 85.29659581184387, "training_acc": 54.0, "val_loss": 18.21257770061493, "val_acc": 56.0}
{"epoch": 23, "training_loss": 72.97097969055176, "training_acc": 53.0, "val_loss": 18.271656334400177, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.61155939102173, "training_acc": 53.0, "val_loss": 21.696875989437103, "val_acc": 48.0}
{"epoch": 25, "training_loss": 80.50199794769287, "training_acc": 45.0, "val_loss": 19.29170936346054, "val_acc": 40.0}
{"epoch": 26, "training_loss": 76.01265048980713, "training_acc": 49.0, "val_loss": 23.47310781478882, "val_acc": 52.0}
{"epoch": 27, "training_loss": 70.4725546836853, "training_acc": 61.0, "val_loss": 17.689000070095062, "val_acc": 52.0}
{"epoch": 28, "training_loss": 79.48711490631104, "training_acc": 56.0, "val_loss": 17.738761007785797, "val_acc": 52.0}
{"epoch": 29, "training_loss": 71.45216941833496, "training_acc": 54.0, "val_loss": 17.73522198200226, "val_acc": 52.0}
{"epoch": 30, "training_loss": 65.16219234466553, "training_acc": 67.0, "val_loss": 22.01762944459915, "val_acc": 52.0}
{"epoch": 31, "training_loss": 81.2417345046997, "training_acc": 52.0, "val_loss": 29.642996191978455, "val_acc": 52.0}
{"epoch": 32, "training_loss": 94.59003162384033, "training_acc": 53.0, "val_loss": 22.373169660568237, "val_acc": 48.0}
{"epoch": 33, "training_loss": 90.62941408157349, "training_acc": 47.0, "val_loss": 39.23673927783966, "val_acc": 48.0}
