"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.88910388946533, "training_acc": 54.0, "val_loss": 17.402999103069305, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.49178981781006, "training_acc": 53.0, "val_loss": 17.382758855819702, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.21845865249634, "training_acc": 53.0, "val_loss": 17.308229207992554, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.2231023311615, "training_acc": 53.0, "val_loss": 17.317859828472137, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.09839034080505, "training_acc": 53.0, "val_loss": 17.297881841659546, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.14088249206543, "training_acc": 53.0, "val_loss": 17.29590743780136, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.15276837348938, "training_acc": 53.0, "val_loss": 17.309197783470154, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.84514927864075, "training_acc": 44.0, "val_loss": 17.373545467853546, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.45046949386597, "training_acc": 46.0, "val_loss": 17.32509732246399, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.33844041824341, "training_acc": 53.0, "val_loss": 17.301951348781586, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.41030263900757, "training_acc": 53.0, "val_loss": 17.305421829223633, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.21912455558777, "training_acc": 53.0, "val_loss": 17.3299178481102, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.2152349948883, "training_acc": 53.0, "val_loss": 17.340098321437836, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.2396183013916, "training_acc": 53.0, "val_loss": 17.358721792697906, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.26335525512695, "training_acc": 53.0, "val_loss": 17.34529435634613, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.22385430335999, "training_acc": 53.0, "val_loss": 17.33425110578537, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.29100322723389, "training_acc": 53.0, "val_loss": 17.38651394844055, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.60770606994629, "training_acc": 53.0, "val_loss": 17.383448779582977, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.35336923599243, "training_acc": 53.0, "val_loss": 17.299678921699524, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.09800410270691, "training_acc": 53.0, "val_loss": 17.326191067695618, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.48721051216125, "training_acc": 47.0, "val_loss": 17.372699081897736, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.67091035842896, "training_acc": 47.0, "val_loss": 17.355258762836456, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.31145763397217, "training_acc": 53.0, "val_loss": 17.29712188243866, "val_acc": 52.0}
{"epoch": 23, "training_loss": 68.9372968673706, "training_acc": 53.0, "val_loss": 17.350459098815918, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.55296778678894, "training_acc": 53.0, "val_loss": 17.47085005044937, "val_acc": 52.0}
