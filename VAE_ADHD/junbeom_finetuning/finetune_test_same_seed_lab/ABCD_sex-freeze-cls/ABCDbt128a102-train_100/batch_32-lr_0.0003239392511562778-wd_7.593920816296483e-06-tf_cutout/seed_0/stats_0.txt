"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.43157291412354, "training_acc": 47.0, "val_loss": 17.17257648706436, "val_acc": 56.0}
{"epoch": 1, "training_loss": 69.46634578704834, "training_acc": 52.0, "val_loss": 17.17066913843155, "val_acc": 56.0}
{"epoch": 2, "training_loss": 69.47423791885376, "training_acc": 52.0, "val_loss": 17.223455011844635, "val_acc": 56.0}
{"epoch": 3, "training_loss": 69.31758427619934, "training_acc": 52.0, "val_loss": 17.163242399692535, "val_acc": 56.0}
{"epoch": 4, "training_loss": 69.32898092269897, "training_acc": 52.0, "val_loss": 17.216740548610687, "val_acc": 56.0}
{"epoch": 5, "training_loss": 69.25982236862183, "training_acc": 52.0, "val_loss": 17.181506752967834, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.22900629043579, "training_acc": 52.0, "val_loss": 17.18587577342987, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.28927707672119, "training_acc": 52.0, "val_loss": 17.210952937602997, "val_acc": 56.0}
{"epoch": 8, "training_loss": 69.29442811012268, "training_acc": 52.0, "val_loss": 17.203012108802795, "val_acc": 56.0}
{"epoch": 9, "training_loss": 69.35013508796692, "training_acc": 52.0, "val_loss": 17.290636897087097, "val_acc": 56.0}
{"epoch": 10, "training_loss": 69.3824303150177, "training_acc": 49.0, "val_loss": 17.49090552330017, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.59768009185791, "training_acc": 48.0, "val_loss": 17.360949516296387, "val_acc": 56.0}
{"epoch": 12, "training_loss": 69.6208004951477, "training_acc": 43.0, "val_loss": 17.154158651828766, "val_acc": 56.0}
{"epoch": 13, "training_loss": 69.18407583236694, "training_acc": 52.0, "val_loss": 17.129676043987274, "val_acc": 56.0}
{"epoch": 14, "training_loss": 69.66410684585571, "training_acc": 52.0, "val_loss": 17.144231498241425, "val_acc": 56.0}
{"epoch": 15, "training_loss": 69.86247253417969, "training_acc": 52.0, "val_loss": 17.14315414428711, "val_acc": 56.0}
{"epoch": 16, "training_loss": 70.08402752876282, "training_acc": 52.0, "val_loss": 17.1518012881279, "val_acc": 56.0}
{"epoch": 17, "training_loss": 69.73407673835754, "training_acc": 52.0, "val_loss": 17.138047516345978, "val_acc": 56.0}
{"epoch": 18, "training_loss": 69.11050271987915, "training_acc": 52.0, "val_loss": 17.30867326259613, "val_acc": 56.0}
{"epoch": 19, "training_loss": 69.45070195198059, "training_acc": 51.0, "val_loss": 17.579971253871918, "val_acc": 56.0}
{"epoch": 20, "training_loss": 69.8441743850708, "training_acc": 48.0, "val_loss": 17.55092591047287, "val_acc": 56.0}
{"epoch": 21, "training_loss": 69.66338920593262, "training_acc": 48.0, "val_loss": 17.41366684436798, "val_acc": 56.0}
{"epoch": 22, "training_loss": 69.57819080352783, "training_acc": 46.0, "val_loss": 17.28035658597946, "val_acc": 56.0}
{"epoch": 23, "training_loss": 69.25014877319336, "training_acc": 53.0, "val_loss": 17.225292325019836, "val_acc": 56.0}
{"epoch": 24, "training_loss": 69.44582223892212, "training_acc": 52.0, "val_loss": 17.142337560653687, "val_acc": 56.0}
{"epoch": 25, "training_loss": 69.30133295059204, "training_acc": 52.0, "val_loss": 17.149406671524048, "val_acc": 56.0}
{"epoch": 26, "training_loss": 69.28338575363159, "training_acc": 52.0, "val_loss": 17.161718010902405, "val_acc": 56.0}
{"epoch": 27, "training_loss": 69.16841053962708, "training_acc": 52.0, "val_loss": 17.20401793718338, "val_acc": 56.0}
{"epoch": 28, "training_loss": 69.2323865890503, "training_acc": 52.0, "val_loss": 17.23707616329193, "val_acc": 56.0}
{"epoch": 29, "training_loss": 69.18057632446289, "training_acc": 52.0, "val_loss": 17.173032462596893, "val_acc": 56.0}
{"epoch": 30, "training_loss": 69.56937289237976, "training_acc": 52.0, "val_loss": 17.1312153339386, "val_acc": 56.0}
{"epoch": 31, "training_loss": 69.66651010513306, "training_acc": 52.0, "val_loss": 17.176568508148193, "val_acc": 56.0}
{"epoch": 32, "training_loss": 70.54521822929382, "training_acc": 52.0, "val_loss": 17.28702187538147, "val_acc": 56.0}
