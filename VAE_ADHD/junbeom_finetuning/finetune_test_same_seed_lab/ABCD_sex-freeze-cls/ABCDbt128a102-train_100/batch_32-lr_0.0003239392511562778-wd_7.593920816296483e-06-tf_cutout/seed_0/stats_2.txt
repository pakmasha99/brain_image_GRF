"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.56776857376099, "training_acc": 53.0, "val_loss": 17.408692836761475, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.4605438709259, "training_acc": 53.0, "val_loss": 17.463018000125885, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.29713129997253, "training_acc": 53.0, "val_loss": 17.366179823875427, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.50532007217407, "training_acc": 54.0, "val_loss": 17.39722639322281, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.82891368865967, "training_acc": 47.0, "val_loss": 17.527936398983, "val_acc": 52.0}
{"epoch": 5, "training_loss": 70.30870819091797, "training_acc": 47.0, "val_loss": 17.491276562213898, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.8450140953064, "training_acc": 47.0, "val_loss": 17.361551523208618, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.37470054626465, "training_acc": 53.0, "val_loss": 17.351819574832916, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.48080015182495, "training_acc": 50.0, "val_loss": 17.367541790008545, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.21562814712524, "training_acc": 55.0, "val_loss": 17.350925505161285, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.09393525123596, "training_acc": 53.0, "val_loss": 17.36462414264679, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.23520994186401, "training_acc": 53.0, "val_loss": 17.376798391342163, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.22046494483948, "training_acc": 53.0, "val_loss": 17.354948818683624, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.14365291595459, "training_acc": 53.0, "val_loss": 17.3527792096138, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.11940789222717, "training_acc": 53.0, "val_loss": 17.3507958650589, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.27646160125732, "training_acc": 53.0, "val_loss": 17.353984713554382, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.13846731185913, "training_acc": 53.0, "val_loss": 17.349815368652344, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.16401147842407, "training_acc": 53.0, "val_loss": 17.349961400032043, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.1247386932373, "training_acc": 53.0, "val_loss": 17.349973320961, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.191739320755, "training_acc": 53.0, "val_loss": 17.36599951982498, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.10528755187988, "training_acc": 53.0, "val_loss": 17.374832928180695, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.12654209136963, "training_acc": 53.0, "val_loss": 17.37655997276306, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.14979553222656, "training_acc": 53.0, "val_loss": 17.401309311389923, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.30712294578552, "training_acc": 53.0, "val_loss": 17.404133081436157, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.13236880302429, "training_acc": 53.0, "val_loss": 17.46542453765869, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.4204375743866, "training_acc": 53.0, "val_loss": 17.473427951335907, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.3507022857666, "training_acc": 53.0, "val_loss": 17.40899831056595, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.20270919799805, "training_acc": 53.0, "val_loss": 17.349331080913544, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.13816809654236, "training_acc": 54.0, "val_loss": 17.362071573734283, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.25456523895264, "training_acc": 54.0, "val_loss": 17.354901134967804, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.12532997131348, "training_acc": 53.0, "val_loss": 17.349258065223694, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.17598581314087, "training_acc": 53.0, "val_loss": 17.349286377429962, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.37490177154541, "training_acc": 51.0, "val_loss": 17.373017966747284, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.26275277137756, "training_acc": 51.0, "val_loss": 17.355483770370483, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.11154460906982, "training_acc": 53.0, "val_loss": 17.34907478094101, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.11657905578613, "training_acc": 53.0, "val_loss": 17.37830638885498, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.14903783798218, "training_acc": 53.0, "val_loss": 17.417657375335693, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.20328664779663, "training_acc": 53.0, "val_loss": 17.44874119758606, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.30413913726807, "training_acc": 53.0, "val_loss": 17.418698966503143, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.14074921607971, "training_acc": 53.0, "val_loss": 17.348361015319824, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.2339506149292, "training_acc": 46.0, "val_loss": 17.37601011991501, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.26692056655884, "training_acc": 52.0, "val_loss": 17.3494353890419, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.20250368118286, "training_acc": 53.0, "val_loss": 17.353007197380066, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.11883974075317, "training_acc": 53.0, "val_loss": 17.362883687019348, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.11710262298584, "training_acc": 53.0, "val_loss": 17.357787489891052, "val_acc": 52.0}
{"epoch": 45, "training_loss": 69.0190200805664, "training_acc": 53.0, "val_loss": 17.361050844192505, "val_acc": 52.0}
{"epoch": 46, "training_loss": 69.07553100585938, "training_acc": 53.0, "val_loss": 17.37324446439743, "val_acc": 52.0}
{"epoch": 47, "training_loss": 69.08504605293274, "training_acc": 53.0, "val_loss": 17.35737919807434, "val_acc": 52.0}
{"epoch": 48, "training_loss": 69.07200050354004, "training_acc": 53.0, "val_loss": 17.367812991142273, "val_acc": 52.0}
{"epoch": 49, "training_loss": 69.08330011367798, "training_acc": 53.0, "val_loss": 17.382632195949554, "val_acc": 52.0}
{"epoch": 50, "training_loss": 69.06706094741821, "training_acc": 53.0, "val_loss": 17.354770004749298, "val_acc": 52.0}
{"epoch": 51, "training_loss": 69.01456594467163, "training_acc": 53.0, "val_loss": 17.351780831813812, "val_acc": 52.0}
{"epoch": 52, "training_loss": 69.05454587936401, "training_acc": 53.0, "val_loss": 17.379668354988098, "val_acc": 52.0}
{"epoch": 53, "training_loss": 69.1335802078247, "training_acc": 53.0, "val_loss": 17.381341755390167, "val_acc": 52.0}
{"epoch": 54, "training_loss": 69.19358086585999, "training_acc": 53.0, "val_loss": 17.489947378635406, "val_acc": 52.0}
{"epoch": 55, "training_loss": 69.52460527420044, "training_acc": 53.0, "val_loss": 17.662303149700165, "val_acc": 52.0}
{"epoch": 56, "training_loss": 70.1726427078247, "training_acc": 53.0, "val_loss": 17.688079178333282, "val_acc": 52.0}
{"epoch": 57, "training_loss": 70.00444054603577, "training_acc": 53.0, "val_loss": 17.630423605442047, "val_acc": 52.0}
{"epoch": 58, "training_loss": 69.76440572738647, "training_acc": 53.0, "val_loss": 17.526541650295258, "val_acc": 52.0}
