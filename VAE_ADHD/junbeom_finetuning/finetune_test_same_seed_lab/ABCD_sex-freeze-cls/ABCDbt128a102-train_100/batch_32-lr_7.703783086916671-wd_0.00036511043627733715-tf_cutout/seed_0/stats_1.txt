"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 66904.5055847168, "training_acc": 51.0, "val_loss": 42629.66613769531, "val_acc": 52.0}
{"epoch": 1, "training_loss": 174136.65185546875, "training_acc": 53.0, "val_loss": 22977.340698242188, "val_acc": 52.0}
{"epoch": 2, "training_loss": 63965.99951171875, "training_acc": 49.0, "val_loss": 32055.145263671875, "val_acc": 48.0}
{"epoch": 3, "training_loss": 119352.884765625, "training_acc": 47.0, "val_loss": 2802.2865295410156, "val_acc": 52.0}
{"epoch": 4, "training_loss": 42723.030517578125, "training_acc": 53.0, "val_loss": 10725.887298583984, "val_acc": 52.0}
{"epoch": 5, "training_loss": 27346.732360839844, "training_acc": 35.0, "val_loss": 1315.670108795166, "val_acc": 52.0}
{"epoch": 6, "training_loss": 4790.908355712891, "training_acc": 55.0, "val_loss": 10434.349060058594, "val_acc": 48.0}
{"epoch": 7, "training_loss": 32457.30340576172, "training_acc": 47.0, "val_loss": 8030.489349365234, "val_acc": 52.0}
{"epoch": 8, "training_loss": 24307.1357421875, "training_acc": 53.0, "val_loss": 6956.407165527344, "val_acc": 48.0}
{"epoch": 9, "training_loss": 20574.588012695312, "training_acc": 41.0, "val_loss": 1092.2411918640137, "val_acc": 52.0}
{"epoch": 10, "training_loss": 14256.072631835938, "training_acc": 53.0, "val_loss": 4649.716567993164, "val_acc": 52.0}
{"epoch": 11, "training_loss": 24636.184692382812, "training_acc": 53.0, "val_loss": 3752.1656036376953, "val_acc": 48.0}
{"epoch": 12, "training_loss": 19027.733924865723, "training_acc": 49.0, "val_loss": 3758.303451538086, "val_acc": 52.0}
{"epoch": 13, "training_loss": 12499.589050292969, "training_acc": 45.0, "val_loss": 3306.5406799316406, "val_acc": 52.0}
{"epoch": 14, "training_loss": 13951.800964355469, "training_acc": 53.0, "val_loss": 3532.7884674072266, "val_acc": 48.0}
{"epoch": 15, "training_loss": 7725.467239379883, "training_acc": 52.0, "val_loss": 6155.680465698242, "val_acc": 48.0}
{"epoch": 16, "training_loss": 26842.046585083008, "training_acc": 47.0, "val_loss": 8178.630065917969, "val_acc": 52.0}
{"epoch": 17, "training_loss": 43757.95837402344, "training_acc": 53.0, "val_loss": 4167.819595336914, "val_acc": 52.0}
{"epoch": 18, "training_loss": 29607.0166015625, "training_acc": 45.0, "val_loss": 4874.611282348633, "val_acc": 48.0}
{"epoch": 19, "training_loss": 19276.298583984375, "training_acc": 53.0, "val_loss": 3523.5111236572266, "val_acc": 52.0}
{"epoch": 20, "training_loss": 18890.509033203125, "training_acc": 51.0, "val_loss": 919.8516845703125, "val_acc": 48.0}
{"epoch": 21, "training_loss": 20952.518920898438, "training_acc": 55.0, "val_loss": 7798.081970214844, "val_acc": 52.0}
{"epoch": 22, "training_loss": 19653.781982421875, "training_acc": 55.0, "val_loss": 8162.062072753906, "val_acc": 48.0}
{"epoch": 23, "training_loss": 27558.06591796875, "training_acc": 39.0, "val_loss": 7301.527404785156, "val_acc": 52.0}
{"epoch": 24, "training_loss": 23219.917724609375, "training_acc": 49.0, "val_loss": 10073.695373535156, "val_acc": 48.0}
{"epoch": 25, "training_loss": 24015.61279296875, "training_acc": 47.0, "val_loss": 6778.150177001953, "val_acc": 52.0}
{"epoch": 26, "training_loss": 21346.149169921875, "training_acc": 39.0, "val_loss": 1915.8437728881836, "val_acc": 52.0}
{"epoch": 27, "training_loss": 9317.740478515625, "training_acc": 57.0, "val_loss": 6224.474334716797, "val_acc": 48.0}
{"epoch": 28, "training_loss": 17978.668426513672, "training_acc": 49.0, "val_loss": 10091.860961914062, "val_acc": 52.0}
{"epoch": 29, "training_loss": 37520.109954833984, "training_acc": 53.0, "val_loss": 6522.6654052734375, "val_acc": 48.0}
{"epoch": 30, "training_loss": 34441.878967285156, "training_acc": 47.0, "val_loss": 2351.028251647949, "val_acc": 52.0}
{"epoch": 31, "training_loss": 14748.39574623108, "training_acc": 53.0, "val_loss": 5462.709426879883, "val_acc": 48.0}
{"epoch": 32, "training_loss": 19643.959594726562, "training_acc": 45.0, "val_loss": 4534.212875366211, "val_acc": 52.0}
{"epoch": 33, "training_loss": 19162.33917236328, "training_acc": 43.0, "val_loss": 5642.787933349609, "val_acc": 48.0}
{"epoch": 34, "training_loss": 13083.969482421875, "training_acc": 45.0, "val_loss": 4403.353500366211, "val_acc": 48.0}
{"epoch": 35, "training_loss": 13697.856506347656, "training_acc": 47.0, "val_loss": 7679.082489013672, "val_acc": 52.0}
{"epoch": 36, "training_loss": 20948.378662109375, "training_acc": 53.0, "val_loss": 8904.43115234375, "val_acc": 48.0}
{"epoch": 37, "training_loss": 26230.363708496094, "training_acc": 49.0, "val_loss": 9380.773162841797, "val_acc": 52.0}
{"epoch": 38, "training_loss": 35833.884033203125, "training_acc": 53.0, "val_loss": 6563.578796386719, "val_acc": 48.0}
{"epoch": 39, "training_loss": 42370.16357421875, "training_acc": 47.0, "val_loss": 1333.1453323364258, "val_acc": 48.0}
