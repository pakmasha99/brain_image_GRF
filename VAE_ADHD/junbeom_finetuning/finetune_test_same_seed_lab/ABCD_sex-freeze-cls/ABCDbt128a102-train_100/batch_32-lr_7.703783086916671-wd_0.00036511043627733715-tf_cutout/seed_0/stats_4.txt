"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 74405.20792770386, "training_acc": 47.0, "val_loss": 28925.32958984375, "val_acc": 52.0}
{"epoch": 1, "training_loss": 97192.55981445312, "training_acc": 53.0, "val_loss": 12956.25, "val_acc": 48.0}
{"epoch": 2, "training_loss": 74541.64794921875, "training_acc": 47.0, "val_loss": 5138.180923461914, "val_acc": 48.0}
{"epoch": 3, "training_loss": 55112.673828125, "training_acc": 45.0, "val_loss": 24446.32568359375, "val_acc": 52.0}
{"epoch": 4, "training_loss": 78921.53979492188, "training_acc": 53.0, "val_loss": 6867.209625244141, "val_acc": 48.0}
{"epoch": 5, "training_loss": 57992.302734375, "training_acc": 47.0, "val_loss": 13302.08740234375, "val_acc": 48.0}
{"epoch": 6, "training_loss": 30894.15234375, "training_acc": 53.0, "val_loss": 11028.938293457031, "val_acc": 52.0}
{"epoch": 7, "training_loss": 26346.62841796875, "training_acc": 51.0, "val_loss": 11070.976257324219, "val_acc": 48.0}
{"epoch": 8, "training_loss": 35882.8408203125, "training_acc": 43.0, "val_loss": 4060.4354858398438, "val_acc": 52.0}
{"epoch": 9, "training_loss": 14056.186401367188, "training_acc": 53.0, "val_loss": 4524.831390380859, "val_acc": 48.0}
{"epoch": 10, "training_loss": 6557.020818710327, "training_acc": 59.0, "val_loss": 4859.792709350586, "val_acc": 48.0}
{"epoch": 11, "training_loss": 16527.623504638672, "training_acc": 49.0, "val_loss": 8648.265075683594, "val_acc": 52.0}
{"epoch": 12, "training_loss": 29026.412616729736, "training_acc": 52.0, "val_loss": 4647.120666503906, "val_acc": 48.0}
{"epoch": 13, "training_loss": 12933.319519042969, "training_acc": 49.0, "val_loss": 10823.194885253906, "val_acc": 52.0}
{"epoch": 14, "training_loss": 38256.75357055664, "training_acc": 53.0, "val_loss": 6947.370147705078, "val_acc": 48.0}
{"epoch": 15, "training_loss": 41943.192626953125, "training_acc": 47.0, "val_loss": 2352.214241027832, "val_acc": 52.0}
{"epoch": 16, "training_loss": 31781.101806640625, "training_acc": 53.0, "val_loss": 4335.836410522461, "val_acc": 52.0}
{"epoch": 17, "training_loss": 21390.540100097656, "training_acc": 47.0, "val_loss": 3228.504180908203, "val_acc": 48.0}
{"epoch": 18, "training_loss": 12880.546691894531, "training_acc": 55.0, "val_loss": 1223.4655380249023, "val_acc": 48.0}
{"epoch": 19, "training_loss": 7971.264678955078, "training_acc": 43.0, "val_loss": 1510.0919723510742, "val_acc": 48.0}
{"epoch": 20, "training_loss": 5818.096130371094, "training_acc": 55.0, "val_loss": 554.3146133422852, "val_acc": 48.0}
{"epoch": 21, "training_loss": 1209.5075378417969, "training_acc": 56.0, "val_loss": 143.41397285461426, "val_acc": 60.0}
{"epoch": 22, "training_loss": 11722.954070091248, "training_acc": 52.0, "val_loss": 1888.8236999511719, "val_acc": 52.0}
{"epoch": 23, "training_loss": 4991.651285029948, "training_acc": 50.0, "val_loss": 1429.1927337646484, "val_acc": 48.0}
{"epoch": 24, "training_loss": 4000.7907104492188, "training_acc": 49.0, "val_loss": 118.36230754852295, "val_acc": 56.0}
{"epoch": 25, "training_loss": 3159.89208984375, "training_acc": 59.0, "val_loss": 992.6308631896973, "val_acc": 52.0}
{"epoch": 26, "training_loss": 11597.088989257812, "training_acc": 41.0, "val_loss": 362.92245388031006, "val_acc": 52.0}
{"epoch": 27, "training_loss": 7805.382879257202, "training_acc": 59.0, "val_loss": 1365.6669616699219, "val_acc": 52.0}
{"epoch": 28, "training_loss": 3535.4283142089844, "training_acc": 54.0, "val_loss": 967.327880859375, "val_acc": 48.0}
{"epoch": 29, "training_loss": 12624.048736572266, "training_acc": 49.0, "val_loss": 4407.228088378906, "val_acc": 48.0}
{"epoch": 30, "training_loss": 19016.892974853516, "training_acc": 49.0, "val_loss": 5449.691390991211, "val_acc": 52.0}
{"epoch": 31, "training_loss": 15538.963195800781, "training_acc": 55.0, "val_loss": 9232.210540771484, "val_acc": 48.0}
{"epoch": 32, "training_loss": 31956.81866455078, "training_acc": 47.0, "val_loss": 5534.302520751953, "val_acc": 52.0}
{"epoch": 33, "training_loss": 14217.222686767578, "training_acc": 49.0, "val_loss": 1038.418960571289, "val_acc": 48.0}
{"epoch": 34, "training_loss": 16321.41650390625, "training_acc": 45.0, "val_loss": 2428.83243560791, "val_acc": 48.0}
{"epoch": 35, "training_loss": 10085.974487304688, "training_acc": 49.0, "val_loss": 8793.433380126953, "val_acc": 52.0}
{"epoch": 36, "training_loss": 28002.529998779297, "training_acc": 53.0, "val_loss": 4872.147750854492, "val_acc": 48.0}
{"epoch": 37, "training_loss": 14598.29248046875, "training_acc": 47.0, "val_loss": 5312.762451171875, "val_acc": 52.0}
{"epoch": 38, "training_loss": 10246.155578613281, "training_acc": 61.0, "val_loss": 178.90859842300415, "val_acc": 52.0}
{"epoch": 39, "training_loss": 12744.192901611328, "training_acc": 51.0, "val_loss": 6352.715301513672, "val_acc": 48.0}
{"epoch": 40, "training_loss": 33175.553161621094, "training_acc": 47.0, "val_loss": 3374.246597290039, "val_acc": 52.0}
{"epoch": 41, "training_loss": 22021.584228515625, "training_acc": 53.0, "val_loss": 9611.807250976562, "val_acc": 48.0}
{"epoch": 42, "training_loss": 51370.07849121094, "training_acc": 47.0, "val_loss": 7313.671112060547, "val_acc": 48.0}
{"epoch": 43, "training_loss": 12660.18212890625, "training_acc": 59.0, "val_loss": 3700.1129150390625, "val_acc": 52.0}
