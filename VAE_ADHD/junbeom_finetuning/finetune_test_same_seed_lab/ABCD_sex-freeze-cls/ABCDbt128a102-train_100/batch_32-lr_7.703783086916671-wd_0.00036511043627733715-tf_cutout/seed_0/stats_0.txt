"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 68232.30917358398, "training_acc": 48.0, "val_loss": 28563.8427734375, "val_acc": 56.0}
{"epoch": 1, "training_loss": 102262.12634277344, "training_acc": 52.0, "val_loss": 17743.6279296875, "val_acc": 44.0}
{"epoch": 2, "training_loss": 100141.60009765625, "training_acc": 48.0, "val_loss": 14142.138671875, "val_acc": 44.0}
{"epoch": 3, "training_loss": 37352.994140625, "training_acc": 50.0, "val_loss": 8713.652038574219, "val_acc": 56.0}
{"epoch": 4, "training_loss": 30450.120361328125, "training_acc": 50.0, "val_loss": 15404.991149902344, "val_acc": 44.0}
{"epoch": 5, "training_loss": 33299.308837890625, "training_acc": 44.0, "val_loss": 4026.3221740722656, "val_acc": 56.0}
{"epoch": 6, "training_loss": 17381.91680908203, "training_acc": 50.0, "val_loss": 8704.698944091797, "val_acc": 44.0}
{"epoch": 7, "training_loss": 20710.171630859375, "training_acc": 52.0, "val_loss": 8757.823944091797, "val_acc": 56.0}
{"epoch": 8, "training_loss": 21622.151489257812, "training_acc": 52.0, "val_loss": 15070.32470703125, "val_acc": 44.0}
{"epoch": 9, "training_loss": 55492.94921875, "training_acc": 48.0, "val_loss": 2236.397171020508, "val_acc": 56.0}
{"epoch": 10, "training_loss": 14882.389221191406, "training_acc": 50.0, "val_loss": 481.7978858947754, "val_acc": 44.0}
{"epoch": 11, "training_loss": 11598.467468261719, "training_acc": 54.0, "val_loss": 1609.3456268310547, "val_acc": 44.0}
{"epoch": 12, "training_loss": 5054.387987136841, "training_acc": 42.0, "val_loss": 805.366039276123, "val_acc": 56.0}
{"epoch": 13, "training_loss": 7615.368560791016, "training_acc": 44.0, "val_loss": 5369.118881225586, "val_acc": 56.0}
{"epoch": 14, "training_loss": 14591.158260345459, "training_acc": 58.0, "val_loss": 3868.7232971191406, "val_acc": 44.0}
{"epoch": 15, "training_loss": 6550.232269287109, "training_acc": 48.0, "val_loss": 331.7565441131592, "val_acc": 56.0}
{"epoch": 16, "training_loss": 4970.228965759277, "training_acc": 52.0, "val_loss": 843.8247680664062, "val_acc": 56.0}
{"epoch": 17, "training_loss": 3865.3157653808594, "training_acc": 53.0, "val_loss": 6258.482360839844, "val_acc": 44.0}
{"epoch": 18, "training_loss": 24039.331115722656, "training_acc": 46.0, "val_loss": 234.9698543548584, "val_acc": 56.0}
{"epoch": 19, "training_loss": 9023.56755065918, "training_acc": 50.0, "val_loss": 2315.493583679199, "val_acc": 56.0}
{"epoch": 20, "training_loss": 12041.329711914062, "training_acc": 48.0, "val_loss": 3466.5374755859375, "val_acc": 44.0}
{"epoch": 21, "training_loss": 13707.8115234375, "training_acc": 52.0, "val_loss": 1698.057746887207, "val_acc": 44.0}
{"epoch": 22, "training_loss": 7739.7518310546875, "training_acc": 44.0, "val_loss": 4727.396392822266, "val_acc": 44.0}
{"epoch": 23, "training_loss": 16076.0595703125, "training_acc": 52.0, "val_loss": 6782.837677001953, "val_acc": 56.0}
{"epoch": 24, "training_loss": 29055.180786132812, "training_acc": 52.0, "val_loss": 10818.1640625, "val_acc": 44.0}
{"epoch": 25, "training_loss": 56447.25537109375, "training_acc": 48.0, "val_loss": 9146.544647216797, "val_acc": 44.0}
{"epoch": 26, "training_loss": 28928.47998046875, "training_acc": 44.0, "val_loss": 4464.356231689453, "val_acc": 56.0}
{"epoch": 27, "training_loss": 10355.304336547852, "training_acc": 52.0, "val_loss": 4240.72151184082, "val_acc": 56.0}
{"epoch": 28, "training_loss": 19754.736328125, "training_acc": 50.0, "val_loss": 1846.4973449707031, "val_acc": 44.0}
{"epoch": 29, "training_loss": 9754.33740234375, "training_acc": 44.0, "val_loss": 2329.3588638305664, "val_acc": 56.0}
{"epoch": 30, "training_loss": 7937.427001953125, "training_acc": 50.0, "val_loss": 8079.109954833984, "val_acc": 44.0}
{"epoch": 31, "training_loss": 14472.648193359375, "training_acc": 58.0, "val_loss": 4347.145080566406, "val_acc": 56.0}
{"epoch": 32, "training_loss": 12084.44482421875, "training_acc": 52.0, "val_loss": 1665.4060363769531, "val_acc": 56.0}
{"epoch": 33, "training_loss": 6600.616249084473, "training_acc": 47.0, "val_loss": 2450.6141662597656, "val_acc": 44.0}
{"epoch": 34, "training_loss": 4541.54069519043, "training_acc": 50.0, "val_loss": 893.8226699829102, "val_acc": 44.0}
{"epoch": 35, "training_loss": 1353.121223449707, "training_acc": 59.0, "val_loss": 2903.685760498047, "val_acc": 56.0}
{"epoch": 36, "training_loss": 14111.331359863281, "training_acc": 48.0, "val_loss": 4431.565475463867, "val_acc": 44.0}
{"epoch": 37, "training_loss": 14546.424438476562, "training_acc": 52.0, "val_loss": 124.22342300415039, "val_acc": 48.0}
{"epoch": 38, "training_loss": 16753.258644104004, "training_acc": 54.0, "val_loss": 2899.9462127685547, "val_acc": 56.0}
{"epoch": 39, "training_loss": 12865.849655151367, "training_acc": 54.0, "val_loss": 5992.62809753418, "val_acc": 44.0}
{"epoch": 40, "training_loss": 14340.902954101562, "training_acc": 52.0, "val_loss": 10793.343353271484, "val_acc": 56.0}
{"epoch": 41, "training_loss": 42850.36047363281, "training_acc": 52.0, "val_loss": 6992.557525634766, "val_acc": 44.0}
{"epoch": 42, "training_loss": 39685.204833984375, "training_acc": 48.0, "val_loss": 3966.0545349121094, "val_acc": 44.0}
{"epoch": 43, "training_loss": 29950.565185546875, "training_acc": 50.0, "val_loss": 6697.882080078125, "val_acc": 56.0}
{"epoch": 44, "training_loss": 16999.39288330078, "training_acc": 46.0, "val_loss": 1402.4954795837402, "val_acc": 56.0}
{"epoch": 45, "training_loss": 4777.3628005981445, "training_acc": 58.0, "val_loss": 2263.120460510254, "val_acc": 56.0}
{"epoch": 46, "training_loss": 7979.966339111328, "training_acc": 52.0, "val_loss": 3010.877799987793, "val_acc": 56.0}
{"epoch": 47, "training_loss": 13422.51513671875, "training_acc": 52.0, "val_loss": 5319.00520324707, "val_acc": 44.0}
{"epoch": 48, "training_loss": 15978.958984375, "training_acc": 48.0, "val_loss": 2993.1108474731445, "val_acc": 56.0}
{"epoch": 49, "training_loss": 13130.38623046875, "training_acc": 52.0, "val_loss": 1138.4047508239746, "val_acc": 56.0}
{"epoch": 50, "training_loss": 5345.77409362793, "training_acc": 54.0, "val_loss": 710.3050231933594, "val_acc": 56.0}
{"epoch": 51, "training_loss": 3672.004867553711, "training_acc": 52.0, "val_loss": 6705.3436279296875, "val_acc": 44.0}
{"epoch": 52, "training_loss": 27938.584014892578, "training_acc": 48.0, "val_loss": 7067.9718017578125, "val_acc": 56.0}
{"epoch": 53, "training_loss": 41723.88659667969, "training_acc": 52.0, "val_loss": 2479.897689819336, "val_acc": 56.0}
{"epoch": 54, "training_loss": 32346.6572265625, "training_acc": 46.0, "val_loss": 13035.68115234375, "val_acc": 44.0}
{"epoch": 55, "training_loss": 29443.899047851562, "training_acc": 44.0, "val_loss": 190.98650217056274, "val_acc": 52.0}
{"epoch": 56, "training_loss": 14908.829772949219, "training_acc": 60.0, "val_loss": 461.3327980041504, "val_acc": 56.0}
