"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 330.3365306854248, "training_acc": 53.0, "val_loss": 119.44092512130737, "val_acc": 48.0}
{"epoch": 1, "training_loss": 321.492347240448, "training_acc": 49.0, "val_loss": 98.15829992294312, "val_acc": 52.0}
{"epoch": 2, "training_loss": 415.5928535461426, "training_acc": 53.0, "val_loss": 21.76404893398285, "val_acc": 52.0}
{"epoch": 3, "training_loss": 229.3153133392334, "training_acc": 47.0, "val_loss": 55.85541129112244, "val_acc": 48.0}
{"epoch": 4, "training_loss": 159.28379154205322, "training_acc": 53.0, "val_loss": 64.27274942398071, "val_acc": 52.0}
{"epoch": 5, "training_loss": 172.65074825286865, "training_acc": 51.0, "val_loss": 40.615829825401306, "val_acc": 48.0}
{"epoch": 6, "training_loss": 120.93596506118774, "training_acc": 47.0, "val_loss": 38.39750587940216, "val_acc": 52.0}
{"epoch": 7, "training_loss": 125.29009985923767, "training_acc": 55.0, "val_loss": 41.22767448425293, "val_acc": 48.0}
{"epoch": 8, "training_loss": 174.656503200531, "training_acc": 47.0, "val_loss": 22.5127175450325, "val_acc": 52.0}
{"epoch": 9, "training_loss": 100.63019013404846, "training_acc": 49.0, "val_loss": 17.219485342502594, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.3945620059967, "training_acc": 54.0, "val_loss": 17.44559109210968, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.79577088356018, "training_acc": 46.0, "val_loss": 17.21249371767044, "val_acc": 52.0}
{"epoch": 12, "training_loss": 68.39280366897583, "training_acc": 62.0, "val_loss": 17.21273362636566, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.70460319519043, "training_acc": 51.0, "val_loss": 17.991559207439423, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.56858038902283, "training_acc": 53.0, "val_loss": 18.610861897468567, "val_acc": 52.0}
{"epoch": 15, "training_loss": 74.27534484863281, "training_acc": 55.0, "val_loss": 26.708325743675232, "val_acc": 48.0}
{"epoch": 16, "training_loss": 89.39167737960815, "training_acc": 57.0, "val_loss": 27.064159512519836, "val_acc": 52.0}
{"epoch": 17, "training_loss": 87.31596565246582, "training_acc": 57.0, "val_loss": 18.311916291713715, "val_acc": 56.0}
{"epoch": 18, "training_loss": 77.48387575149536, "training_acc": 49.0, "val_loss": 21.785174310207367, "val_acc": 48.0}
{"epoch": 19, "training_loss": 93.42885231971741, "training_acc": 47.0, "val_loss": 24.253852665424347, "val_acc": 52.0}
{"epoch": 20, "training_loss": 84.49098873138428, "training_acc": 51.0, "val_loss": 18.055152893066406, "val_acc": 52.0}
{"epoch": 21, "training_loss": 93.43680691719055, "training_acc": 53.0, "val_loss": 29.773050546646118, "val_acc": 48.0}
{"epoch": 22, "training_loss": 121.93594670295715, "training_acc": 51.0, "val_loss": 48.02992045879364, "val_acc": 52.0}
{"epoch": 23, "training_loss": 222.6940353512764, "training_acc": 53.0, "val_loss": 17.546166479587555, "val_acc": 52.0}
{"epoch": 24, "training_loss": 99.72313928604126, "training_acc": 47.0, "val_loss": 20.96954882144928, "val_acc": 52.0}
{"epoch": 25, "training_loss": 103.60303688049316, "training_acc": 51.0, "val_loss": 21.12901210784912, "val_acc": 48.0}
{"epoch": 26, "training_loss": 79.4610435962677, "training_acc": 45.0, "val_loss": 23.007795214653015, "val_acc": 52.0}
{"epoch": 27, "training_loss": 76.72719717025757, "training_acc": 51.0, "val_loss": 18.354852497577667, "val_acc": 52.0}
{"epoch": 28, "training_loss": 67.93678998947144, "training_acc": 53.0, "val_loss": 20.615150034427643, "val_acc": 52.0}
{"epoch": 29, "training_loss": 73.10014820098877, "training_acc": 54.0, "val_loss": 17.318090796470642, "val_acc": 52.0}
{"epoch": 30, "training_loss": 92.64197015762329, "training_acc": 52.0, "val_loss": 22.930343449115753, "val_acc": 48.0}
