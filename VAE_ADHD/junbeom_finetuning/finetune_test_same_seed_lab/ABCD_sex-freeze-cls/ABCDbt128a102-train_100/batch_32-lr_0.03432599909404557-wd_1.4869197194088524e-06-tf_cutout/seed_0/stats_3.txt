"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 305.66375255584717, "training_acc": 53.0, "val_loss": 120.91535329818726, "val_acc": 48.0}
{"epoch": 1, "training_loss": 327.890118598938, "training_acc": 47.0, "val_loss": 82.79027938842773, "val_acc": 52.0}
{"epoch": 2, "training_loss": 291.4338812828064, "training_acc": 53.0, "val_loss": 43.996551632881165, "val_acc": 48.0}
{"epoch": 3, "training_loss": 240.41906797885895, "training_acc": 47.0, "val_loss": 18.55769455432892, "val_acc": 48.0}
{"epoch": 4, "training_loss": 78.63859820365906, "training_acc": 59.0, "val_loss": 26.030626893043518, "val_acc": 52.0}
{"epoch": 5, "training_loss": 93.61742162704468, "training_acc": 45.0, "val_loss": 21.70931249856949, "val_acc": 48.0}
{"epoch": 6, "training_loss": 81.68896317481995, "training_acc": 49.0, "val_loss": 19.924859702587128, "val_acc": 52.0}
{"epoch": 7, "training_loss": 68.83233451843262, "training_acc": 60.0, "val_loss": 19.844956696033478, "val_acc": 48.0}
{"epoch": 8, "training_loss": 81.52223896980286, "training_acc": 47.0, "val_loss": 17.982354760169983, "val_acc": 52.0}
{"epoch": 9, "training_loss": 73.68932271003723, "training_acc": 49.0, "val_loss": 26.611334085464478, "val_acc": 52.0}
{"epoch": 10, "training_loss": 95.7763569355011, "training_acc": 53.0, "val_loss": 23.588046431541443, "val_acc": 48.0}
{"epoch": 11, "training_loss": 82.2190523147583, "training_acc": 49.0, "val_loss": 17.687927186489105, "val_acc": 52.0}
{"epoch": 12, "training_loss": 76.93656897544861, "training_acc": 51.0, "val_loss": 35.5184942483902, "val_acc": 52.0}
{"epoch": 13, "training_loss": 160.7452096939087, "training_acc": 53.0, "val_loss": 31.67639672756195, "val_acc": 48.0}
{"epoch": 14, "training_loss": 122.88819742202759, "training_acc": 45.0, "val_loss": 18.592998385429382, "val_acc": 52.0}
{"epoch": 15, "training_loss": 74.89232349395752, "training_acc": 41.0, "val_loss": 18.09879243373871, "val_acc": 48.0}
{"epoch": 16, "training_loss": 69.67535924911499, "training_acc": 49.0, "val_loss": 17.475225031375885, "val_acc": 52.0}
{"epoch": 17, "training_loss": 72.23345732688904, "training_acc": 42.0, "val_loss": 17.358604073524475, "val_acc": 52.0}
{"epoch": 18, "training_loss": 70.00319933891296, "training_acc": 46.0, "val_loss": 23.460189998149872, "val_acc": 52.0}
{"epoch": 19, "training_loss": 86.44047784805298, "training_acc": 47.0, "val_loss": 17.379257082939148, "val_acc": 52.0}
{"epoch": 20, "training_loss": 73.32903385162354, "training_acc": 54.0, "val_loss": 17.473766207695007, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.23705339431763, "training_acc": 58.0, "val_loss": 23.523688316345215, "val_acc": 48.0}
{"epoch": 22, "training_loss": 82.84983777999878, "training_acc": 47.0, "val_loss": 20.358875393867493, "val_acc": 52.0}
{"epoch": 23, "training_loss": 70.90797090530396, "training_acc": 55.0, "val_loss": 26.646006107330322, "val_acc": 48.0}
{"epoch": 24, "training_loss": 80.30178833007812, "training_acc": 55.0, "val_loss": 21.862582862377167, "val_acc": 52.0}
{"epoch": 25, "training_loss": 85.16496682167053, "training_acc": 47.0, "val_loss": 17.887170612812042, "val_acc": 48.0}
{"epoch": 26, "training_loss": 69.91531753540039, "training_acc": 54.0, "val_loss": 28.544923663139343, "val_acc": 48.0}
{"epoch": 27, "training_loss": 132.86769771575928, "training_acc": 49.0, "val_loss": 46.37822210788727, "val_acc": 52.0}
{"epoch": 28, "training_loss": 171.23738145828247, "training_acc": 53.0, "val_loss": 39.23379182815552, "val_acc": 48.0}
{"epoch": 29, "training_loss": 162.95261335372925, "training_acc": 45.0, "val_loss": 35.06841957569122, "val_acc": 52.0}
{"epoch": 30, "training_loss": 100.91821646690369, "training_acc": 51.0, "val_loss": 22.670195996761322, "val_acc": 48.0}
{"epoch": 31, "training_loss": 104.64868402481079, "training_acc": 45.0, "val_loss": 25.4836767911911, "val_acc": 48.0}
{"epoch": 32, "training_loss": 114.53061389923096, "training_acc": 47.0, "val_loss": 28.140753507614136, "val_acc": 52.0}
{"epoch": 33, "training_loss": 98.55970239639282, "training_acc": 53.0, "val_loss": 18.424464762210846, "val_acc": 48.0}
{"epoch": 34, "training_loss": 104.78740978240967, "training_acc": 41.0, "val_loss": 32.3587566614151, "val_acc": 48.0}
{"epoch": 35, "training_loss": 144.00644612312317, "training_acc": 47.0, "val_loss": 31.354528665542603, "val_acc": 52.0}
{"epoch": 36, "training_loss": 110.92093181610107, "training_acc": 51.0, "val_loss": 23.717986047267914, "val_acc": 48.0}
