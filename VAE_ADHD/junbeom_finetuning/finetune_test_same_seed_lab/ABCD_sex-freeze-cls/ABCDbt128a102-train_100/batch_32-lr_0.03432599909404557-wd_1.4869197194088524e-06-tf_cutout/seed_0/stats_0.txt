"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 325.7067012786865, "training_acc": 48.0, "val_loss": 125.48056840896606, "val_acc": 56.0}
{"epoch": 1, "training_loss": 441.0742483139038, "training_acc": 52.0, "val_loss": 74.89622831344604, "val_acc": 44.0}
{"epoch": 2, "training_loss": 378.36729431152344, "training_acc": 48.0, "val_loss": 20.26982456445694, "val_acc": 44.0}
{"epoch": 3, "training_loss": 224.50653266906738, "training_acc": 50.0, "val_loss": 65.23465514183044, "val_acc": 56.0}
{"epoch": 4, "training_loss": 181.6842164993286, "training_acc": 50.0, "val_loss": 62.72592544555664, "val_acc": 44.0}
{"epoch": 5, "training_loss": 160.2845630645752, "training_acc": 47.0, "val_loss": 39.76864814758301, "val_acc": 56.0}
{"epoch": 6, "training_loss": 132.90086245536804, "training_acc": 52.0, "val_loss": 35.67964732646942, "val_acc": 44.0}
{"epoch": 7, "training_loss": 115.92486810684204, "training_acc": 48.0, "val_loss": 27.834346890449524, "val_acc": 56.0}
{"epoch": 8, "training_loss": 111.884765625, "training_acc": 52.0, "val_loss": 33.45341384410858, "val_acc": 44.0}
{"epoch": 9, "training_loss": 125.52985382080078, "training_acc": 44.0, "val_loss": 18.307769298553467, "val_acc": 56.0}
{"epoch": 10, "training_loss": 73.13858604431152, "training_acc": 50.0, "val_loss": 17.104335129261017, "val_acc": 56.0}
{"epoch": 11, "training_loss": 77.9032883644104, "training_acc": 52.0, "val_loss": 18.426956236362457, "val_acc": 56.0}
{"epoch": 12, "training_loss": 75.78369331359863, "training_acc": 52.0, "val_loss": 23.17131906747818, "val_acc": 44.0}
{"epoch": 13, "training_loss": 76.16778993606567, "training_acc": 52.0, "val_loss": 25.900477170944214, "val_acc": 56.0}
{"epoch": 14, "training_loss": 93.34764313697815, "training_acc": 50.0, "val_loss": 25.865375995635986, "val_acc": 44.0}
{"epoch": 15, "training_loss": 76.37491357326508, "training_acc": 54.0, "val_loss": 30.45678734779358, "val_acc": 56.0}
{"epoch": 16, "training_loss": 108.74474740028381, "training_acc": 54.0, "val_loss": 44.656407833099365, "val_acc": 44.0}
{"epoch": 17, "training_loss": 143.1555061340332, "training_acc": 46.0, "val_loss": 24.673810601234436, "val_acc": 56.0}
{"epoch": 18, "training_loss": 87.24466276168823, "training_acc": 50.0, "val_loss": 34.11664962768555, "val_acc": 44.0}
{"epoch": 19, "training_loss": 98.76781558990479, "training_acc": 48.0, "val_loss": 30.204519629478455, "val_acc": 56.0}
{"epoch": 20, "training_loss": 102.67524361610413, "training_acc": 48.0, "val_loss": 27.740535140037537, "val_acc": 44.0}
{"epoch": 21, "training_loss": 80.2865834236145, "training_acc": 54.0, "val_loss": 19.8965385556221, "val_acc": 56.0}
{"epoch": 22, "training_loss": 71.44929504394531, "training_acc": 57.0, "val_loss": 24.32592660188675, "val_acc": 44.0}
{"epoch": 23, "training_loss": 83.97549557685852, "training_acc": 46.0, "val_loss": 24.17825609445572, "val_acc": 56.0}
{"epoch": 24, "training_loss": 85.25203967094421, "training_acc": 54.0, "val_loss": 38.26543688774109, "val_acc": 44.0}
{"epoch": 25, "training_loss": 103.84596920013428, "training_acc": 57.0, "val_loss": 25.696316361427307, "val_acc": 56.0}
{"epoch": 26, "training_loss": 77.14774942398071, "training_acc": 62.0, "val_loss": 31.554746627807617, "val_acc": 44.0}
{"epoch": 27, "training_loss": 95.66764736175537, "training_acc": 48.0, "val_loss": 18.520957231521606, "val_acc": 56.0}
{"epoch": 28, "training_loss": 84.78452062606812, "training_acc": 52.0, "val_loss": 20.12450397014618, "val_acc": 56.0}
{"epoch": 29, "training_loss": 112.18468713760376, "training_acc": 50.0, "val_loss": 17.237378656864166, "val_acc": 56.0}
{"epoch": 30, "training_loss": 72.98725938796997, "training_acc": 54.0, "val_loss": 17.07542985677719, "val_acc": 56.0}
{"epoch": 31, "training_loss": 69.13178324699402, "training_acc": 44.0, "val_loss": 26.462772488594055, "val_acc": 56.0}
{"epoch": 32, "training_loss": 116.26816606521606, "training_acc": 50.0, "val_loss": 24.625274538993835, "val_acc": 44.0}
{"epoch": 33, "training_loss": 79.16432571411133, "training_acc": 52.0, "val_loss": 17.23732352256775, "val_acc": 56.0}
{"epoch": 34, "training_loss": 71.88414359092712, "training_acc": 56.0, "val_loss": 18.791790306568146, "val_acc": 56.0}
{"epoch": 35, "training_loss": 68.79630470275879, "training_acc": 56.0, "val_loss": 18.868139386177063, "val_acc": 56.0}
{"epoch": 36, "training_loss": 74.45091414451599, "training_acc": 48.0, "val_loss": 17.442867159843445, "val_acc": 56.0}
{"epoch": 37, "training_loss": 77.47921895980835, "training_acc": 52.0, "val_loss": 19.32089775800705, "val_acc": 48.0}
{"epoch": 38, "training_loss": 71.23904609680176, "training_acc": 47.0, "val_loss": 17.107120156288147, "val_acc": 56.0}
{"epoch": 39, "training_loss": 71.30156373977661, "training_acc": 51.0, "val_loss": 18.877021968364716, "val_acc": 56.0}
{"epoch": 40, "training_loss": 71.89614868164062, "training_acc": 52.0, "val_loss": 22.901691496372223, "val_acc": 56.0}
{"epoch": 41, "training_loss": 81.15039944648743, "training_acc": 50.0, "val_loss": 26.01144015789032, "val_acc": 44.0}
{"epoch": 42, "training_loss": 88.02183151245117, "training_acc": 44.0, "val_loss": 17.15790182352066, "val_acc": 56.0}
{"epoch": 43, "training_loss": 73.08307695388794, "training_acc": 53.0, "val_loss": 19.650745391845703, "val_acc": 48.0}
{"epoch": 44, "training_loss": 72.25558614730835, "training_acc": 56.0, "val_loss": 18.486076593399048, "val_acc": 56.0}
{"epoch": 45, "training_loss": 68.72769355773926, "training_acc": 56.0, "val_loss": 17.394958436489105, "val_acc": 56.0}
{"epoch": 46, "training_loss": 71.75201606750488, "training_acc": 54.0, "val_loss": 17.112483084201813, "val_acc": 56.0}
{"epoch": 47, "training_loss": 67.85115623474121, "training_acc": 50.0, "val_loss": 17.142850160598755, "val_acc": 56.0}
{"epoch": 48, "training_loss": 66.44614171981812, "training_acc": 59.0, "val_loss": 25.403058528900146, "val_acc": 44.0}
{"epoch": 49, "training_loss": 86.70751762390137, "training_acc": 44.0, "val_loss": 18.246570229530334, "val_acc": 56.0}
