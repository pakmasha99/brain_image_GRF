"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 277.77778148651123, "training_acc": 53.0, "val_loss": 126.11396312713623, "val_acc": 48.0}
{"epoch": 1, "training_loss": 346.8110294342041, "training_acc": 45.0, "val_loss": 74.18171763420105, "val_acc": 52.0}
{"epoch": 2, "training_loss": 240.5188684463501, "training_acc": 49.0, "val_loss": 25.918620824813843, "val_acc": 48.0}
{"epoch": 3, "training_loss": 94.3858323097229, "training_acc": 43.0, "val_loss": 27.186289429664612, "val_acc": 52.0}
{"epoch": 4, "training_loss": 79.59673881530762, "training_acc": 55.0, "val_loss": 38.05244863033295, "val_acc": 48.0}
{"epoch": 5, "training_loss": 118.92685127258301, "training_acc": 49.0, "val_loss": 38.644057512283325, "val_acc": 52.0}
{"epoch": 6, "training_loss": 116.02704167366028, "training_acc": 55.0, "val_loss": 36.43862307071686, "val_acc": 48.0}
{"epoch": 7, "training_loss": 122.55748200416565, "training_acc": 47.0, "val_loss": 32.05639123916626, "val_acc": 52.0}
{"epoch": 8, "training_loss": 100.12021541595459, "training_acc": 51.0, "val_loss": 19.336996972560883, "val_acc": 48.0}
{"epoch": 9, "training_loss": 73.64888381958008, "training_acc": 55.0, "val_loss": 18.242095410823822, "val_acc": 52.0}
{"epoch": 10, "training_loss": 106.49330067634583, "training_acc": 48.0, "val_loss": 31.862017512321472, "val_acc": 52.0}
{"epoch": 11, "training_loss": 132.22691106796265, "training_acc": 53.0, "val_loss": 35.62438488006592, "val_acc": 48.0}
{"epoch": 12, "training_loss": 205.96924495697021, "training_acc": 47.0, "val_loss": 26.25935971736908, "val_acc": 52.0}
{"epoch": 13, "training_loss": 167.1854248046875, "training_acc": 53.0, "val_loss": 19.716346263885498, "val_acc": 48.0}
{"epoch": 14, "training_loss": 145.0977268218994, "training_acc": 47.0, "val_loss": 17.57378727197647, "val_acc": 52.0}
{"epoch": 15, "training_loss": 97.3937635421753, "training_acc": 57.0, "val_loss": 17.862071096897125, "val_acc": 52.0}
{"epoch": 16, "training_loss": 81.05244064331055, "training_acc": 53.0, "val_loss": 17.554911971092224, "val_acc": 52.0}
{"epoch": 17, "training_loss": 83.85494709014893, "training_acc": 53.0, "val_loss": 17.600546777248383, "val_acc": 52.0}
{"epoch": 18, "training_loss": 70.39333581924438, "training_acc": 51.0, "val_loss": 18.815402686595917, "val_acc": 52.0}
{"epoch": 19, "training_loss": 71.59173345565796, "training_acc": 47.0, "val_loss": 18.230387568473816, "val_acc": 52.0}
{"epoch": 20, "training_loss": 71.0313549041748, "training_acc": 54.0, "val_loss": 18.26971024274826, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.42297220230103, "training_acc": 53.0, "val_loss": 18.769171833992004, "val_acc": 48.0}
{"epoch": 22, "training_loss": 70.84032821655273, "training_acc": 56.0, "val_loss": 19.67807710170746, "val_acc": 48.0}
{"epoch": 23, "training_loss": 77.43401050567627, "training_acc": 47.0, "val_loss": 21.618857979774475, "val_acc": 52.0}
{"epoch": 24, "training_loss": 74.43592929840088, "training_acc": 51.0, "val_loss": 18.76818835735321, "val_acc": 44.0}
{"epoch": 25, "training_loss": 67.63873720169067, "training_acc": 59.0, "val_loss": 20.762212574481964, "val_acc": 52.0}
{"epoch": 26, "training_loss": 82.031005859375, "training_acc": 43.0, "val_loss": 18.720778822898865, "val_acc": 44.0}
{"epoch": 27, "training_loss": 69.6473970413208, "training_acc": 51.0, "val_loss": 17.6324725151062, "val_acc": 52.0}
{"epoch": 28, "training_loss": 70.38018488883972, "training_acc": 55.0, "val_loss": 27.787622809410095, "val_acc": 52.0}
{"epoch": 29, "training_loss": 89.47969770431519, "training_acc": 53.0, "val_loss": 28.19303572177887, "val_acc": 48.0}
{"epoch": 30, "training_loss": 108.32238864898682, "training_acc": 39.0, "val_loss": 18.29274147748947, "val_acc": 44.0}
{"epoch": 31, "training_loss": 109.45752596855164, "training_acc": 48.0, "val_loss": 25.289487838745117, "val_acc": 52.0}
{"epoch": 32, "training_loss": 88.66439628601074, "training_acc": 51.0, "val_loss": 19.14992928504944, "val_acc": 36.0}
{"epoch": 33, "training_loss": 65.34170246124268, "training_acc": 59.0, "val_loss": 31.678375601768494, "val_acc": 52.0}
{"epoch": 34, "training_loss": 98.07243168354034, "training_acc": 57.0, "val_loss": 50.679224729537964, "val_acc": 48.0}
{"epoch": 35, "training_loss": 175.99958062171936, "training_acc": 49.0, "val_loss": 50.51925182342529, "val_acc": 52.0}
