"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 335.8664050102234, "training_acc": 47.0, "val_loss": 161.47541999816895, "val_acc": 52.0}
{"epoch": 1, "training_loss": 613.4494333267212, "training_acc": 53.0, "val_loss": 18.3109849691391, "val_acc": 52.0}
{"epoch": 2, "training_loss": 218.07805252075195, "training_acc": 53.0, "val_loss": 32.954978942871094, "val_acc": 48.0}
{"epoch": 3, "training_loss": 190.92530727386475, "training_acc": 47.0, "val_loss": 65.47980308532715, "val_acc": 52.0}
{"epoch": 4, "training_loss": 153.7285919189453, "training_acc": 53.0, "val_loss": 56.09650015830994, "val_acc": 48.0}
{"epoch": 5, "training_loss": 165.69398093223572, "training_acc": 49.0, "val_loss": 51.780056953430176, "val_acc": 52.0}
{"epoch": 6, "training_loss": 205.86880683898926, "training_acc": 53.0, "val_loss": 20.661666989326477, "val_acc": 48.0}
{"epoch": 7, "training_loss": 99.28159832954407, "training_acc": 45.0, "val_loss": 17.571663856506348, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.98072910308838, "training_acc": 47.0, "val_loss": 20.183883607387543, "val_acc": 52.0}
{"epoch": 9, "training_loss": 74.72068381309509, "training_acc": 51.0, "val_loss": 17.26383864879608, "val_acc": 52.0}
{"epoch": 10, "training_loss": 70.318932056427, "training_acc": 53.0, "val_loss": 21.764929592609406, "val_acc": 48.0}
{"epoch": 11, "training_loss": 87.14037489891052, "training_acc": 47.0, "val_loss": 23.68982285261154, "val_acc": 52.0}
{"epoch": 12, "training_loss": 83.4837236404419, "training_acc": 49.0, "val_loss": 18.94005388021469, "val_acc": 52.0}
{"epoch": 13, "training_loss": 93.73088550567627, "training_acc": 53.0, "val_loss": 17.902150750160217, "val_acc": 52.0}
{"epoch": 14, "training_loss": 71.79458141326904, "training_acc": 45.0, "val_loss": 20.638999342918396, "val_acc": 52.0}
{"epoch": 15, "training_loss": 75.79167079925537, "training_acc": 52.0, "val_loss": 17.763151228427887, "val_acc": 52.0}
{"epoch": 16, "training_loss": 78.28753018379211, "training_acc": 53.0, "val_loss": 20.93145251274109, "val_acc": 48.0}
{"epoch": 17, "training_loss": 75.34737253189087, "training_acc": 49.0, "val_loss": 23.389193415641785, "val_acc": 52.0}
{"epoch": 18, "training_loss": 89.30863642692566, "training_acc": 43.0, "val_loss": 17.22971498966217, "val_acc": 52.0}
{"epoch": 19, "training_loss": 91.47395920753479, "training_acc": 51.0, "val_loss": 24.01861399412155, "val_acc": 48.0}
{"epoch": 20, "training_loss": 87.08513069152832, "training_acc": 45.0, "val_loss": 18.779903650283813, "val_acc": 52.0}
{"epoch": 21, "training_loss": 77.53552293777466, "training_acc": 47.0, "val_loss": 18.576055765151978, "val_acc": 52.0}
{"epoch": 22, "training_loss": 67.54545545578003, "training_acc": 58.0, "val_loss": 29.598119854927063, "val_acc": 48.0}
{"epoch": 23, "training_loss": 90.40370750427246, "training_acc": 51.0, "val_loss": 29.62186634540558, "val_acc": 52.0}
{"epoch": 24, "training_loss": 93.54184055328369, "training_acc": 49.0, "val_loss": 24.829067289829254, "val_acc": 48.0}
{"epoch": 25, "training_loss": 107.45382118225098, "training_acc": 48.0, "val_loss": 28.07898223400116, "val_acc": 48.0}
{"epoch": 26, "training_loss": 158.61697721481323, "training_acc": 47.0, "val_loss": 47.275832295417786, "val_acc": 52.0}
{"epoch": 27, "training_loss": 266.7248764038086, "training_acc": 53.0, "val_loss": 23.343880474567413, "val_acc": 52.0}
{"epoch": 28, "training_loss": 169.47553825378418, "training_acc": 53.0, "val_loss": 39.32147026062012, "val_acc": 48.0}
{"epoch": 29, "training_loss": 110.45459413528442, "training_acc": 57.0, "val_loss": 55.608195066452026, "val_acc": 52.0}
{"epoch": 30, "training_loss": 154.64139366149902, "training_acc": 45.0, "val_loss": 31.693804264068604, "val_acc": 48.0}
{"epoch": 31, "training_loss": 107.48923206329346, "training_acc": 45.0, "val_loss": 20.45803964138031, "val_acc": 52.0}
{"epoch": 32, "training_loss": 94.67300796508789, "training_acc": 51.0, "val_loss": 17.173585295677185, "val_acc": 52.0}
{"epoch": 33, "training_loss": 106.55825519561768, "training_acc": 57.0, "val_loss": 17.892232537269592, "val_acc": 40.0}
{"epoch": 34, "training_loss": 74.2960774898529, "training_acc": 50.0, "val_loss": 17.44941622018814, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.8726499080658, "training_acc": 52.0, "val_loss": 18.864504992961884, "val_acc": 52.0}
{"epoch": 36, "training_loss": 71.35651683807373, "training_acc": 47.0, "val_loss": 17.52299815416336, "val_acc": 52.0}
{"epoch": 37, "training_loss": 79.67828464508057, "training_acc": 43.0, "val_loss": 36.94654703140259, "val_acc": 52.0}
{"epoch": 38, "training_loss": 150.66776657104492, "training_acc": 53.0, "val_loss": 23.5981285572052, "val_acc": 48.0}
{"epoch": 39, "training_loss": 90.48291182518005, "training_acc": 51.0, "val_loss": 45.061370730400085, "val_acc": 52.0}
{"epoch": 40, "training_loss": 196.08813381195068, "training_acc": 53.0, "val_loss": 30.236536264419556, "val_acc": 48.0}
{"epoch": 41, "training_loss": 164.8332371711731, "training_acc": 47.0, "val_loss": 25.764620304107666, "val_acc": 52.0}
{"epoch": 42, "training_loss": 132.53148412704468, "training_acc": 53.0, "val_loss": 24.39955025911331, "val_acc": 48.0}
{"epoch": 43, "training_loss": 104.39958357810974, "training_acc": 49.0, "val_loss": 33.00746977329254, "val_acc": 52.0}
{"epoch": 44, "training_loss": 151.16092777252197, "training_acc": 53.0, "val_loss": 26.069548726081848, "val_acc": 48.0}
{"epoch": 45, "training_loss": 95.62038230895996, "training_acc": 49.0, "val_loss": 31.456199288368225, "val_acc": 52.0}
{"epoch": 46, "training_loss": 102.61657571792603, "training_acc": 54.0, "val_loss": 17.763085663318634, "val_acc": 40.0}
{"epoch": 47, "training_loss": 79.07467460632324, "training_acc": 50.0, "val_loss": 18.38850975036621, "val_acc": 60.0}
{"epoch": 48, "training_loss": 70.93711400032043, "training_acc": 49.0, "val_loss": 23.395922780036926, "val_acc": 52.0}
{"epoch": 49, "training_loss": 88.80639553070068, "training_acc": 49.0, "val_loss": 17.63533502817154, "val_acc": 44.0}
{"epoch": 50, "training_loss": 72.86398983001709, "training_acc": 54.0, "val_loss": 22.614820301532745, "val_acc": 48.0}
{"epoch": 51, "training_loss": 100.54953622817993, "training_acc": 45.0, "val_loss": 20.542947947978973, "val_acc": 52.0}
