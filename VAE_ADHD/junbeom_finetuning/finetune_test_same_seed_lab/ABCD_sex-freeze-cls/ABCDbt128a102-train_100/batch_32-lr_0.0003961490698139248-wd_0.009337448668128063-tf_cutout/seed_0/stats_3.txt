"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 70.33028817176819, "training_acc": 41.0, "val_loss": 17.32334792613983, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.22691750526428, "training_acc": 53.0, "val_loss": 17.441926896572113, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.56860589981079, "training_acc": 53.0, "val_loss": 17.445552349090576, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.19216012954712, "training_acc": 53.0, "val_loss": 17.32495427131653, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.24629402160645, "training_acc": 53.0, "val_loss": 17.328891158103943, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.31191802024841, "training_acc": 56.0, "val_loss": 17.317838966846466, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.1993145942688, "training_acc": 53.0, "val_loss": 17.314952611923218, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.17905974388123, "training_acc": 53.0, "val_loss": 17.32959896326065, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.07759046554565, "training_acc": 53.0, "val_loss": 17.386150360107422, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.27227592468262, "training_acc": 53.0, "val_loss": 17.506438493728638, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.77520275115967, "training_acc": 53.0, "val_loss": 17.531882226467133, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.54888248443604, "training_acc": 53.0, "val_loss": 17.397543787956238, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.35877895355225, "training_acc": 53.0, "val_loss": 17.37525165081024, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.16476249694824, "training_acc": 53.0, "val_loss": 17.315654456615448, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.09470319747925, "training_acc": 53.0, "val_loss": 17.329591512680054, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.30016040802002, "training_acc": 51.0, "val_loss": 17.37593412399292, "val_acc": 52.0}
{"epoch": 16, "training_loss": 70.30507850646973, "training_acc": 47.0, "val_loss": 17.48839169740677, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.6654920578003, "training_acc": 47.0, "val_loss": 17.35185533761978, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.30519723892212, "training_acc": 48.0, "val_loss": 17.315736413002014, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.45734548568726, "training_acc": 53.0, "val_loss": 17.373637855052948, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.34361267089844, "training_acc": 53.0, "val_loss": 17.412500083446503, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.39921140670776, "training_acc": 53.0, "val_loss": 17.388354241847992, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.2692642211914, "training_acc": 53.0, "val_loss": 17.378516495227814, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.22377467155457, "training_acc": 53.0, "val_loss": 17.342986166477203, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.15975189208984, "training_acc": 53.0, "val_loss": 17.33047515153885, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.16645956039429, "training_acc": 53.0, "val_loss": 17.391882836818695, "val_acc": 52.0}
