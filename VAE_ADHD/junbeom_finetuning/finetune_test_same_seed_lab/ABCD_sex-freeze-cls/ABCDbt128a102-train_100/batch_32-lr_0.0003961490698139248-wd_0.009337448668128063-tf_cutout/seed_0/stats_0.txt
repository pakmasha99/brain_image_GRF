"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.48799180984497, "training_acc": 48.0, "val_loss": 17.15848445892334, "val_acc": 56.0}
{"epoch": 1, "training_loss": 69.53274631500244, "training_acc": 52.0, "val_loss": 17.173974215984344, "val_acc": 56.0}
{"epoch": 2, "training_loss": 69.53443455696106, "training_acc": 52.0, "val_loss": 17.246301472187042, "val_acc": 56.0}
{"epoch": 3, "training_loss": 69.34646105766296, "training_acc": 52.0, "val_loss": 17.16082990169525, "val_acc": 56.0}
{"epoch": 4, "training_loss": 69.33496689796448, "training_acc": 52.0, "val_loss": 17.22041517496109, "val_acc": 56.0}
{"epoch": 5, "training_loss": 69.27109241485596, "training_acc": 52.0, "val_loss": 17.174744606018066, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.2330379486084, "training_acc": 52.0, "val_loss": 17.18040555715561, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.29968690872192, "training_acc": 52.0, "val_loss": 17.213112115859985, "val_acc": 56.0}
{"epoch": 8, "training_loss": 69.30739498138428, "training_acc": 52.0, "val_loss": 17.205223441123962, "val_acc": 56.0}
{"epoch": 9, "training_loss": 69.36970233917236, "training_acc": 52.0, "val_loss": 17.31613278388977, "val_acc": 56.0}
{"epoch": 10, "training_loss": 69.45672798156738, "training_acc": 49.0, "val_loss": 17.56407767534256, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.71355319023132, "training_acc": 48.0, "val_loss": 17.36200302839279, "val_acc": 56.0}
{"epoch": 12, "training_loss": 69.70957851409912, "training_acc": 43.0, "val_loss": 17.134417593479156, "val_acc": 56.0}
{"epoch": 13, "training_loss": 69.3078978061676, "training_acc": 52.0, "val_loss": 17.140060663223267, "val_acc": 56.0}
{"epoch": 14, "training_loss": 69.9564642906189, "training_acc": 52.0, "val_loss": 17.167045176029205, "val_acc": 56.0}
{"epoch": 15, "training_loss": 70.09718608856201, "training_acc": 52.0, "val_loss": 17.146506905555725, "val_acc": 56.0}
{"epoch": 16, "training_loss": 70.10152268409729, "training_acc": 52.0, "val_loss": 17.145639657974243, "val_acc": 56.0}
{"epoch": 17, "training_loss": 69.61090135574341, "training_acc": 52.0, "val_loss": 17.16267019510269, "val_acc": 56.0}
{"epoch": 18, "training_loss": 69.04296398162842, "training_acc": 52.0, "val_loss": 17.44907647371292, "val_acc": 56.0}
{"epoch": 19, "training_loss": 69.8083848953247, "training_acc": 48.0, "val_loss": 17.77336150407791, "val_acc": 56.0}
{"epoch": 20, "training_loss": 70.20821166038513, "training_acc": 48.0, "val_loss": 17.602737247943878, "val_acc": 56.0}
{"epoch": 21, "training_loss": 69.65422248840332, "training_acc": 48.0, "val_loss": 17.354924976825714, "val_acc": 56.0}
{"epoch": 22, "training_loss": 69.54481649398804, "training_acc": 43.0, "val_loss": 17.204144597053528, "val_acc": 56.0}
{"epoch": 23, "training_loss": 69.2061378955841, "training_acc": 52.0, "val_loss": 17.16996878385544, "val_acc": 56.0}
{"epoch": 24, "training_loss": 69.53510522842407, "training_acc": 52.0, "val_loss": 17.128822207450867, "val_acc": 56.0}
{"epoch": 25, "training_loss": 69.40700387954712, "training_acc": 52.0, "val_loss": 17.143358290195465, "val_acc": 56.0}
{"epoch": 26, "training_loss": 69.20821380615234, "training_acc": 52.0, "val_loss": 17.177315056324005, "val_acc": 56.0}
{"epoch": 27, "training_loss": 69.12087869644165, "training_acc": 52.0, "val_loss": 17.25391149520874, "val_acc": 56.0}
{"epoch": 28, "training_loss": 69.2654333114624, "training_acc": 47.0, "val_loss": 17.292432487010956, "val_acc": 56.0}
{"epoch": 29, "training_loss": 69.18945550918579, "training_acc": 50.0, "val_loss": 17.17849373817444, "val_acc": 56.0}
{"epoch": 30, "training_loss": 69.66290283203125, "training_acc": 52.0, "val_loss": 17.142505943775177, "val_acc": 56.0}
{"epoch": 31, "training_loss": 69.88787603378296, "training_acc": 52.0, "val_loss": 17.228414118289948, "val_acc": 56.0}
{"epoch": 32, "training_loss": 71.05706691741943, "training_acc": 52.0, "val_loss": 17.374661564826965, "val_acc": 56.0}
{"epoch": 33, "training_loss": 71.54961109161377, "training_acc": 52.0, "val_loss": 17.276690900325775, "val_acc": 56.0}
{"epoch": 34, "training_loss": 70.24721765518188, "training_acc": 52.0, "val_loss": 17.12755858898163, "val_acc": 56.0}
{"epoch": 35, "training_loss": 69.4258975982666, "training_acc": 52.0, "val_loss": 17.349570989608765, "val_acc": 56.0}
{"epoch": 36, "training_loss": 69.46851229667664, "training_acc": 49.0, "val_loss": 17.467112839221954, "val_acc": 56.0}
{"epoch": 37, "training_loss": 69.43673896789551, "training_acc": 48.0, "val_loss": 17.35846996307373, "val_acc": 56.0}
{"epoch": 38, "training_loss": 69.23525214195251, "training_acc": 52.0, "val_loss": 17.28336364030838, "val_acc": 56.0}
{"epoch": 39, "training_loss": 69.27610373497009, "training_acc": 53.0, "val_loss": 17.28845238685608, "val_acc": 56.0}
{"epoch": 40, "training_loss": 69.39519691467285, "training_acc": 46.0, "val_loss": 17.34551191329956, "val_acc": 56.0}
{"epoch": 41, "training_loss": 69.2223608493805, "training_acc": 53.0, "val_loss": 17.27372705936432, "val_acc": 56.0}
{"epoch": 42, "training_loss": 69.20750761032104, "training_acc": 51.0, "val_loss": 17.26796180009842, "val_acc": 56.0}
{"epoch": 43, "training_loss": 69.21556091308594, "training_acc": 52.0, "val_loss": 17.275802791118622, "val_acc": 56.0}
{"epoch": 44, "training_loss": 69.2871241569519, "training_acc": 52.0, "val_loss": 17.379337549209595, "val_acc": 56.0}
{"epoch": 45, "training_loss": 69.34810614585876, "training_acc": 48.0, "val_loss": 17.36631691455841, "val_acc": 56.0}
{"epoch": 46, "training_loss": 69.29655170440674, "training_acc": 49.0, "val_loss": 17.279373109340668, "val_acc": 56.0}
{"epoch": 47, "training_loss": 69.10624361038208, "training_acc": 53.0, "val_loss": 17.1699658036232, "val_acc": 56.0}
{"epoch": 48, "training_loss": 69.25210452079773, "training_acc": 52.0, "val_loss": 17.14758425951004, "val_acc": 56.0}
{"epoch": 49, "training_loss": 69.10009551048279, "training_acc": 52.0, "val_loss": 17.21363067626953, "val_acc": 56.0}
{"epoch": 50, "training_loss": 69.14252209663391, "training_acc": 54.0, "val_loss": 17.43384450674057, "val_acc": 56.0}
{"epoch": 51, "training_loss": 69.57773184776306, "training_acc": 48.0, "val_loss": 17.730559408664703, "val_acc": 56.0}
{"epoch": 52, "training_loss": 70.35297703742981, "training_acc": 48.0, "val_loss": 17.871740460395813, "val_acc": 56.0}
{"epoch": 53, "training_loss": 70.37874627113342, "training_acc": 48.0, "val_loss": 17.63797700405121, "val_acc": 56.0}
