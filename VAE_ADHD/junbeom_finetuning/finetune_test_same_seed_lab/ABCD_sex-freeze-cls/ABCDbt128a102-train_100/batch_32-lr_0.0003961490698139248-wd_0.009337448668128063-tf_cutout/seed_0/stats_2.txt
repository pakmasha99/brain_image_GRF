"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.49291944503784, "training_acc": 53.0, "val_loss": 17.340975999832153, "val_acc": 52.0}
{"epoch": 1, "training_loss": 68.95172929763794, "training_acc": 53.0, "val_loss": 17.451034486293793, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.26416993141174, "training_acc": 53.0, "val_loss": 17.64844059944153, "val_acc": 52.0}
{"epoch": 3, "training_loss": 70.06179165840149, "training_acc": 53.0, "val_loss": 17.619231343269348, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.70579409599304, "training_acc": 53.0, "val_loss": 17.42570400238037, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.39980292320251, "training_acc": 53.0, "val_loss": 17.334569990634918, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.33457279205322, "training_acc": 53.0, "val_loss": 17.381858825683594, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.47180581092834, "training_acc": 47.0, "val_loss": 17.352451384067535, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.1921284198761, "training_acc": 53.0, "val_loss": 17.33010858297348, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.12200164794922, "training_acc": 53.0, "val_loss": 17.331714928150177, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.36516547203064, "training_acc": 53.0, "val_loss": 17.344102263450623, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.21906471252441, "training_acc": 57.0, "val_loss": 17.331166565418243, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.05049204826355, "training_acc": 53.0, "val_loss": 17.333263158798218, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.08205127716064, "training_acc": 53.0, "val_loss": 17.380449175834656, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.14144396781921, "training_acc": 53.0, "val_loss": 17.42205023765564, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.27488017082214, "training_acc": 53.0, "val_loss": 17.444607615470886, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.31961941719055, "training_acc": 53.0, "val_loss": 17.39349067211151, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.08960747718811, "training_acc": 53.0, "val_loss": 17.331573367118835, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.34038829803467, "training_acc": 51.0, "val_loss": 17.39163100719452, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.4286756515503, "training_acc": 52.0, "val_loss": 17.331431806087494, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.18374466896057, "training_acc": 53.0, "val_loss": 17.342419922351837, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.05850839614868, "training_acc": 53.0, "val_loss": 17.3664391040802, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.0909116268158, "training_acc": 53.0, "val_loss": 17.352698743343353, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.05343914031982, "training_acc": 53.0, "val_loss": 17.349858582019806, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.0858097076416, "training_acc": 53.0, "val_loss": 17.35846996307373, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.06510329246521, "training_acc": 53.0, "val_loss": 17.335303127765656, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.05647039413452, "training_acc": 53.0, "val_loss": 17.34735071659088, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.0694055557251, "training_acc": 53.0, "val_loss": 17.36759841442108, "val_acc": 52.0}
