"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 271.14934062957764, "training_acc": 55.0, "val_loss": 107.3866605758667, "val_acc": 48.0}
{"epoch": 1, "training_loss": 286.09042406082153, "training_acc": 49.0, "val_loss": 88.09771537780762, "val_acc": 52.0}
{"epoch": 2, "training_loss": 334.26258277893066, "training_acc": 53.0, "val_loss": 18.10983270406723, "val_acc": 52.0}
{"epoch": 3, "training_loss": 149.1358778476715, "training_acc": 47.0, "val_loss": 25.24566352367401, "val_acc": 48.0}
{"epoch": 4, "training_loss": 103.52003860473633, "training_acc": 53.0, "val_loss": 20.34854143857956, "val_acc": 52.0}
{"epoch": 5, "training_loss": 103.78510856628418, "training_acc": 51.0, "val_loss": 17.505712807178497, "val_acc": 52.0}
{"epoch": 6, "training_loss": 132.88551807403564, "training_acc": 53.0, "val_loss": 23.489241302013397, "val_acc": 52.0}
{"epoch": 7, "training_loss": 91.70531511306763, "training_acc": 53.0, "val_loss": 24.00745004415512, "val_acc": 48.0}
{"epoch": 8, "training_loss": 95.86277675628662, "training_acc": 49.0, "val_loss": 21.838974952697754, "val_acc": 52.0}
{"epoch": 9, "training_loss": 96.10034084320068, "training_acc": 43.0, "val_loss": 17.76052415370941, "val_acc": 52.0}
{"epoch": 10, "training_loss": 80.50720858573914, "training_acc": 53.0, "val_loss": 18.823643028736115, "val_acc": 52.0}
{"epoch": 11, "training_loss": 72.32184505462646, "training_acc": 56.0, "val_loss": 19.906902313232422, "val_acc": 48.0}
{"epoch": 12, "training_loss": 78.08335638046265, "training_acc": 41.0, "val_loss": 19.25334930419922, "val_acc": 52.0}
{"epoch": 13, "training_loss": 71.83585572242737, "training_acc": 51.0, "val_loss": 17.268693447113037, "val_acc": 52.0}
{"epoch": 14, "training_loss": 71.60714149475098, "training_acc": 53.0, "val_loss": 20.38973569869995, "val_acc": 48.0}
{"epoch": 15, "training_loss": 82.59392809867859, "training_acc": 47.0, "val_loss": 20.88399827480316, "val_acc": 52.0}
{"epoch": 16, "training_loss": 77.4643702507019, "training_acc": 53.0, "val_loss": 17.974646389484406, "val_acc": 52.0}
{"epoch": 17, "training_loss": 85.67390942573547, "training_acc": 53.0, "val_loss": 25.170373916625977, "val_acc": 48.0}
{"epoch": 18, "training_loss": 106.0977201461792, "training_acc": 51.0, "val_loss": 39.47979211807251, "val_acc": 52.0}
{"epoch": 19, "training_loss": 188.5685070157051, "training_acc": 53.0, "val_loss": 17.963440716266632, "val_acc": 52.0}
{"epoch": 20, "training_loss": 90.82895421981812, "training_acc": 47.0, "val_loss": 18.15047562122345, "val_acc": 52.0}
{"epoch": 21, "training_loss": 96.92864871025085, "training_acc": 53.0, "val_loss": 18.324105441570282, "val_acc": 48.0}
{"epoch": 22, "training_loss": 74.13422775268555, "training_acc": 50.0, "val_loss": 22.448594868183136, "val_acc": 52.0}
{"epoch": 23, "training_loss": 78.65540599822998, "training_acc": 53.0, "val_loss": 21.00069373846054, "val_acc": 48.0}
{"epoch": 24, "training_loss": 71.62651801109314, "training_acc": 55.0, "val_loss": 26.678481698036194, "val_acc": 52.0}
{"epoch": 25, "training_loss": 88.78246307373047, "training_acc": 51.0, "val_loss": 20.83214521408081, "val_acc": 48.0}
{"epoch": 26, "training_loss": 94.65739440917969, "training_acc": 43.0, "val_loss": 17.55538582801819, "val_acc": 52.0}
{"epoch": 27, "training_loss": 83.0702862739563, "training_acc": 47.0, "val_loss": 21.541275084018707, "val_acc": 52.0}
{"epoch": 28, "training_loss": 80.29181456565857, "training_acc": 51.0, "val_loss": 17.32899397611618, "val_acc": 52.0}
{"epoch": 29, "training_loss": 73.37278699874878, "training_acc": 53.0, "val_loss": 18.988700211048126, "val_acc": 56.0}
{"epoch": 30, "training_loss": 73.3099799156189, "training_acc": 49.0, "val_loss": 18.111129105091095, "val_acc": 44.0}
{"epoch": 31, "training_loss": 70.46609020233154, "training_acc": 47.0, "val_loss": 18.001994490623474, "val_acc": 52.0}
{"epoch": 32, "training_loss": 76.88696765899658, "training_acc": 41.0, "val_loss": 22.67546057701111, "val_acc": 52.0}
