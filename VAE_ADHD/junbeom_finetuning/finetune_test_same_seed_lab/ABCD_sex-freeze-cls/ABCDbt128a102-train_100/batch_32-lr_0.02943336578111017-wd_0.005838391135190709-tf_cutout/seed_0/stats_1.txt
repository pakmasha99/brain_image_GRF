"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 272.9736108779907, "training_acc": 49.0, "val_loss": 86.37188673019409, "val_acc": 48.0}
{"epoch": 1, "training_loss": 235.92144775390625, "training_acc": 49.0, "val_loss": 71.95059657096863, "val_acc": 52.0}
{"epoch": 2, "training_loss": 247.65495204925537, "training_acc": 53.0, "val_loss": 46.872279047966, "val_acc": 48.0}
{"epoch": 3, "training_loss": 272.0252170562744, "training_acc": 47.0, "val_loss": 25.786834955215454, "val_acc": 48.0}
{"epoch": 4, "training_loss": 126.11967658996582, "training_acc": 63.0, "val_loss": 55.879372358322144, "val_acc": 52.0}
{"epoch": 5, "training_loss": 139.05905055999756, "training_acc": 51.0, "val_loss": 48.63104522228241, "val_acc": 48.0}
{"epoch": 6, "training_loss": 150.43483877182007, "training_acc": 49.0, "val_loss": 42.91900992393494, "val_acc": 52.0}
{"epoch": 7, "training_loss": 175.06247997283936, "training_acc": 53.0, "val_loss": 20.02090960741043, "val_acc": 48.0}
{"epoch": 8, "training_loss": 174.93561935424805, "training_acc": 47.0, "val_loss": 19.913379848003387, "val_acc": 48.0}
{"epoch": 9, "training_loss": 121.67226219177246, "training_acc": 51.0, "val_loss": 33.56720209121704, "val_acc": 52.0}
{"epoch": 10, "training_loss": 133.332377910614, "training_acc": 35.0, "val_loss": 30.560410022735596, "val_acc": 48.0}
{"epoch": 11, "training_loss": 110.68203496932983, "training_acc": 41.0, "val_loss": 23.633189499378204, "val_acc": 52.0}
{"epoch": 12, "training_loss": 79.72775053977966, "training_acc": 59.0, "val_loss": 23.18596988916397, "val_acc": 48.0}
{"epoch": 13, "training_loss": 74.55073928833008, "training_acc": 55.0, "val_loss": 23.19977581501007, "val_acc": 52.0}
{"epoch": 14, "training_loss": 72.43042230606079, "training_acc": 59.0, "val_loss": 23.19447100162506, "val_acc": 48.0}
{"epoch": 15, "training_loss": 78.48920440673828, "training_acc": 51.0, "val_loss": 23.96659106016159, "val_acc": 52.0}
{"epoch": 16, "training_loss": 86.11395168304443, "training_acc": 43.0, "val_loss": 20.079797506332397, "val_acc": 48.0}
{"epoch": 17, "training_loss": 70.57974147796631, "training_acc": 53.0, "val_loss": 23.149001598358154, "val_acc": 52.0}
{"epoch": 18, "training_loss": 83.25667929649353, "training_acc": 45.0, "val_loss": 17.42738038301468, "val_acc": 52.0}
{"epoch": 19, "training_loss": 72.36266756057739, "training_acc": 58.0, "val_loss": 17.68108308315277, "val_acc": 52.0}
{"epoch": 20, "training_loss": 70.93242049217224, "training_acc": 47.0, "val_loss": 17.736132442951202, "val_acc": 52.0}
{"epoch": 21, "training_loss": 68.38979387283325, "training_acc": 54.0, "val_loss": 17.864544689655304, "val_acc": 52.0}
{"epoch": 22, "training_loss": 67.9317626953125, "training_acc": 56.0, "val_loss": 17.69888401031494, "val_acc": 52.0}
{"epoch": 23, "training_loss": 67.20126438140869, "training_acc": 57.0, "val_loss": 21.37603908777237, "val_acc": 52.0}
{"epoch": 24, "training_loss": 79.98756241798401, "training_acc": 49.0, "val_loss": 22.32258766889572, "val_acc": 48.0}
{"epoch": 25, "training_loss": 77.57678747177124, "training_acc": 49.0, "val_loss": 19.864928722381592, "val_acc": 52.0}
{"epoch": 26, "training_loss": 77.76645016670227, "training_acc": 50.0, "val_loss": 19.62588131427765, "val_acc": 52.0}
{"epoch": 27, "training_loss": 85.6874589920044, "training_acc": 53.0, "val_loss": 22.486476600170135, "val_acc": 48.0}
{"epoch": 28, "training_loss": 90.28524255752563, "training_acc": 47.0, "val_loss": 17.513874173164368, "val_acc": 52.0}
{"epoch": 29, "training_loss": 75.52227139472961, "training_acc": 47.0, "val_loss": 18.82760226726532, "val_acc": 52.0}
{"epoch": 30, "training_loss": 70.38744068145752, "training_acc": 51.0, "val_loss": 17.580243945121765, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.53886318206787, "training_acc": 46.0, "val_loss": 26.881012320518494, "val_acc": 52.0}
{"epoch": 32, "training_loss": 99.57492637634277, "training_acc": 57.0, "val_loss": 43.16428303718567, "val_acc": 48.0}
{"epoch": 33, "training_loss": 193.57556533813477, "training_acc": 47.0, "val_loss": 33.8582307100296, "val_acc": 52.0}
{"epoch": 34, "training_loss": 199.06630420684814, "training_acc": 53.0, "val_loss": 20.39318084716797, "val_acc": 52.0}
{"epoch": 35, "training_loss": 114.81462383270264, "training_acc": 51.0, "val_loss": 21.671009063720703, "val_acc": 48.0}
{"epoch": 36, "training_loss": 98.46704077720642, "training_acc": 53.0, "val_loss": 24.871011078357697, "val_acc": 52.0}
{"epoch": 37, "training_loss": 75.84006333351135, "training_acc": 59.0, "val_loss": 26.14889144897461, "val_acc": 48.0}
