"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 271.3500680923462, "training_acc": 54.0, "val_loss": 108.93818140029907, "val_acc": 48.0}
{"epoch": 1, "training_loss": 310.9296040534973, "training_acc": 47.0, "val_loss": 67.25459098815918, "val_acc": 52.0}
{"epoch": 2, "training_loss": 218.54013967514038, "training_acc": 53.0, "val_loss": 42.234405875205994, "val_acc": 48.0}
{"epoch": 3, "training_loss": 175.71273517608643, "training_acc": 47.0, "val_loss": 32.887816429138184, "val_acc": 52.0}
{"epoch": 4, "training_loss": 183.28164052963257, "training_acc": 53.0, "val_loss": 19.722014665603638, "val_acc": 52.0}
{"epoch": 5, "training_loss": 119.87077331542969, "training_acc": 49.0, "val_loss": 23.20864051580429, "val_acc": 48.0}
{"epoch": 6, "training_loss": 97.49948394298553, "training_acc": 49.0, "val_loss": 33.55631232261658, "val_acc": 52.0}
{"epoch": 7, "training_loss": 95.80218458175659, "training_acc": 53.0, "val_loss": 29.611709713935852, "val_acc": 48.0}
{"epoch": 8, "training_loss": 108.96966934204102, "training_acc": 39.0, "val_loss": 17.570464313030243, "val_acc": 52.0}
{"epoch": 9, "training_loss": 89.01045799255371, "training_acc": 43.0, "val_loss": 17.42146462202072, "val_acc": 52.0}
{"epoch": 10, "training_loss": 71.77904558181763, "training_acc": 50.0, "val_loss": 17.535987496376038, "val_acc": 52.0}
{"epoch": 11, "training_loss": 75.5665864944458, "training_acc": 47.0, "val_loss": 18.23612004518509, "val_acc": 52.0}
{"epoch": 12, "training_loss": 71.18232083320618, "training_acc": 49.0, "val_loss": 18.604016304016113, "val_acc": 52.0}
{"epoch": 13, "training_loss": 76.65128326416016, "training_acc": 51.0, "val_loss": 17.356784641742706, "val_acc": 52.0}
{"epoch": 14, "training_loss": 67.81401014328003, "training_acc": 57.0, "val_loss": 19.453154504299164, "val_acc": 52.0}
{"epoch": 15, "training_loss": 74.3734712600708, "training_acc": 55.0, "val_loss": 26.26996338367462, "val_acc": 48.0}
{"epoch": 16, "training_loss": 88.40882110595703, "training_acc": 47.0, "val_loss": 25.420868396759033, "val_acc": 52.0}
{"epoch": 17, "training_loss": 83.50176429748535, "training_acc": 55.0, "val_loss": 34.944745898246765, "val_acc": 48.0}
{"epoch": 18, "training_loss": 110.36502408981323, "training_acc": 47.0, "val_loss": 34.96111333370209, "val_acc": 52.0}
{"epoch": 19, "training_loss": 100.11289930343628, "training_acc": 55.0, "val_loss": 38.016387820243835, "val_acc": 48.0}
{"epoch": 20, "training_loss": 129.02890491485596, "training_acc": 43.0, "val_loss": 22.30485826730728, "val_acc": 52.0}
{"epoch": 21, "training_loss": 95.21738481521606, "training_acc": 43.0, "val_loss": 19.244980812072754, "val_acc": 52.0}
{"epoch": 22, "training_loss": 100.04729294776917, "training_acc": 53.0, "val_loss": 21.49692177772522, "val_acc": 48.0}
{"epoch": 23, "training_loss": 118.53536987304688, "training_acc": 45.0, "val_loss": 22.089049220085144, "val_acc": 52.0}
{"epoch": 24, "training_loss": 76.72066831588745, "training_acc": 53.0, "val_loss": 19.902770221233368, "val_acc": 48.0}
{"epoch": 25, "training_loss": 82.44256830215454, "training_acc": 38.0, "val_loss": 19.691896438598633, "val_acc": 48.0}
{"epoch": 26, "training_loss": 83.08463525772095, "training_acc": 47.0, "val_loss": 20.989425480365753, "val_acc": 52.0}
{"epoch": 27, "training_loss": 79.97829961776733, "training_acc": 52.0, "val_loss": 17.37341284751892, "val_acc": 52.0}
{"epoch": 28, "training_loss": 88.51186513900757, "training_acc": 49.0, "val_loss": 27.81306505203247, "val_acc": 48.0}
{"epoch": 29, "training_loss": 119.55682516098022, "training_acc": 47.0, "val_loss": 26.591789722442627, "val_acc": 52.0}
{"epoch": 30, "training_loss": 98.94459438323975, "training_acc": 51.0, "val_loss": 21.044042706489563, "val_acc": 48.0}
{"epoch": 31, "training_loss": 78.64518594741821, "training_acc": 45.0, "val_loss": 19.79360282421112, "val_acc": 52.0}
{"epoch": 32, "training_loss": 78.71666026115417, "training_acc": 45.0, "val_loss": 17.391128838062286, "val_acc": 52.0}
