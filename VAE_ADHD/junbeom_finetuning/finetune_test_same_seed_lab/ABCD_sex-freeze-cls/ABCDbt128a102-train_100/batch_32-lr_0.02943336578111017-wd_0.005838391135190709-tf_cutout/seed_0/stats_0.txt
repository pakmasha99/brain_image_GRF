"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 282.8717613220215, "training_acc": 48.0, "val_loss": 106.5025806427002, "val_acc": 56.0}
{"epoch": 1, "training_loss": 369.03560304641724, "training_acc": 52.0, "val_loss": 63.95652890205383, "val_acc": 44.0}
{"epoch": 2, "training_loss": 303.0039806365967, "training_acc": 48.0, "val_loss": 17.52418875694275, "val_acc": 56.0}
{"epoch": 3, "training_loss": 190.0336685180664, "training_acc": 52.0, "val_loss": 37.88694441318512, "val_acc": 56.0}
{"epoch": 4, "training_loss": 139.29517650604248, "training_acc": 55.0, "val_loss": 48.85282814502716, "val_acc": 44.0}
{"epoch": 5, "training_loss": 131.29747772216797, "training_acc": 44.0, "val_loss": 38.34208548069, "val_acc": 56.0}
{"epoch": 6, "training_loss": 120.09615516662598, "training_acc": 52.0, "val_loss": 35.670146346092224, "val_acc": 44.0}
{"epoch": 7, "training_loss": 109.73897361755371, "training_acc": 48.0, "val_loss": 26.685550808906555, "val_acc": 56.0}
{"epoch": 8, "training_loss": 109.72601556777954, "training_acc": 52.0, "val_loss": 29.579755663871765, "val_acc": 44.0}
{"epoch": 9, "training_loss": 121.93901252746582, "training_acc": 44.0, "val_loss": 17.32991933822632, "val_acc": 56.0}
{"epoch": 10, "training_loss": 70.55739617347717, "training_acc": 52.0, "val_loss": 17.11875945329666, "val_acc": 56.0}
{"epoch": 11, "training_loss": 71.37302255630493, "training_acc": 52.0, "val_loss": 19.06794160604477, "val_acc": 56.0}
{"epoch": 12, "training_loss": 84.04324579238892, "training_acc": 52.0, "val_loss": 24.082253873348236, "val_acc": 44.0}
{"epoch": 13, "training_loss": 85.3222279548645, "training_acc": 50.0, "val_loss": 27.27278769016266, "val_acc": 56.0}
{"epoch": 14, "training_loss": 108.27157020568848, "training_acc": 52.0, "val_loss": 27.90166735649109, "val_acc": 44.0}
{"epoch": 15, "training_loss": 90.0771963596344, "training_acc": 52.0, "val_loss": 34.704458713531494, "val_acc": 56.0}
{"epoch": 16, "training_loss": 166.6909999847412, "training_acc": 52.0, "val_loss": 30.772876739501953, "val_acc": 44.0}
{"epoch": 17, "training_loss": 175.9208745956421, "training_acc": 48.0, "val_loss": 20.68551331758499, "val_acc": 44.0}
{"epoch": 18, "training_loss": 109.86244010925293, "training_acc": 46.0, "val_loss": 17.20033586025238, "val_acc": 56.0}
{"epoch": 19, "training_loss": 107.00129222869873, "training_acc": 48.0, "val_loss": 17.11551994085312, "val_acc": 56.0}
{"epoch": 20, "training_loss": 104.33175230026245, "training_acc": 52.0, "val_loss": 17.2073096036911, "val_acc": 56.0}
{"epoch": 21, "training_loss": 100.29845905303955, "training_acc": 48.0, "val_loss": 17.168818414211273, "val_acc": 56.0}
{"epoch": 22, "training_loss": 100.49974822998047, "training_acc": 54.0, "val_loss": 21.322055160999298, "val_acc": 44.0}
{"epoch": 23, "training_loss": 106.38027477264404, "training_acc": 48.0, "val_loss": 24.503466486930847, "val_acc": 56.0}
{"epoch": 24, "training_loss": 176.8457546234131, "training_acc": 52.0, "val_loss": 17.119506001472473, "val_acc": 56.0}
{"epoch": 25, "training_loss": 131.92175483703613, "training_acc": 54.0, "val_loss": 31.637918949127197, "val_acc": 44.0}
{"epoch": 26, "training_loss": 140.31149101257324, "training_acc": 44.0, "val_loss": 28.22122871875763, "val_acc": 56.0}
{"epoch": 27, "training_loss": 95.10306882858276, "training_acc": 52.0, "val_loss": 32.32914209365845, "val_acc": 44.0}
{"epoch": 28, "training_loss": 89.32255554199219, "training_acc": 54.0, "val_loss": 28.171497583389282, "val_acc": 56.0}
{"epoch": 29, "training_loss": 99.05712413787842, "training_acc": 49.0, "val_loss": 19.33734118938446, "val_acc": 52.0}
{"epoch": 30, "training_loss": 95.01340699195862, "training_acc": 48.0, "val_loss": 18.04608553647995, "val_acc": 56.0}
{"epoch": 31, "training_loss": 86.55906295776367, "training_acc": 46.0, "val_loss": 20.35991996526718, "val_acc": 56.0}
{"epoch": 32, "training_loss": 128.13817071914673, "training_acc": 52.0, "val_loss": 17.097650468349457, "val_acc": 56.0}
{"epoch": 33, "training_loss": 96.88497686386108, "training_acc": 44.0, "val_loss": 17.676961421966553, "val_acc": 56.0}
{"epoch": 34, "training_loss": 80.23979711532593, "training_acc": 44.0, "val_loss": 22.15059995651245, "val_acc": 44.0}
{"epoch": 35, "training_loss": 95.90017557144165, "training_acc": 48.0, "val_loss": 24.48248267173767, "val_acc": 56.0}
{"epoch": 36, "training_loss": 120.44626569747925, "training_acc": 52.0, "val_loss": 19.061286747455597, "val_acc": 48.0}
{"epoch": 37, "training_loss": 82.5981457233429, "training_acc": 49.0, "val_loss": 19.236913323402405, "val_acc": 56.0}
{"epoch": 38, "training_loss": 78.54436159133911, "training_acc": 55.0, "val_loss": 23.444758355617523, "val_acc": 44.0}
{"epoch": 39, "training_loss": 85.3276469707489, "training_acc": 46.0, "val_loss": 17.198625206947327, "val_acc": 56.0}
{"epoch": 40, "training_loss": 73.81324505805969, "training_acc": 42.0, "val_loss": 18.917499482631683, "val_acc": 56.0}
{"epoch": 41, "training_loss": 77.51487612724304, "training_acc": 53.0, "val_loss": 26.424065232276917, "val_acc": 44.0}
{"epoch": 42, "training_loss": 92.86310124397278, "training_acc": 48.0, "val_loss": 20.305582880973816, "val_acc": 56.0}
{"epoch": 43, "training_loss": 75.08736324310303, "training_acc": 54.0, "val_loss": 31.361326575279236, "val_acc": 44.0}
{"epoch": 44, "training_loss": 106.4547553062439, "training_acc": 48.0, "val_loss": 25.34020245075226, "val_acc": 56.0}
{"epoch": 45, "training_loss": 93.4436993598938, "training_acc": 52.0, "val_loss": 28.384289145469666, "val_acc": 44.0}
{"epoch": 46, "training_loss": 86.9296202659607, "training_acc": 55.0, "val_loss": 26.198023557662964, "val_acc": 56.0}
{"epoch": 47, "training_loss": 95.71574401855469, "training_acc": 52.0, "val_loss": 25.81634819507599, "val_acc": 44.0}
{"epoch": 48, "training_loss": 82.70460557937622, "training_acc": 48.0, "val_loss": 17.13019460439682, "val_acc": 56.0}
{"epoch": 49, "training_loss": 72.09225225448608, "training_acc": 52.0, "val_loss": 18.73694807291031, "val_acc": 52.0}
{"epoch": 50, "training_loss": 70.37193250656128, "training_acc": 55.0, "val_loss": 18.62819790840149, "val_acc": 44.0}
{"epoch": 51, "training_loss": 76.71205544471741, "training_acc": 46.0, "val_loss": 17.16534048318863, "val_acc": 56.0}
