"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.37281847000122, "training_acc": 48.0, "val_loss": 17.192843556404114, "val_acc": 56.0}
{"epoch": 1, "training_loss": 69.3942334651947, "training_acc": 52.0, "val_loss": 17.171920835971832, "val_acc": 56.0}
{"epoch": 2, "training_loss": 69.41151213645935, "training_acc": 52.0, "val_loss": 17.202255129814148, "val_acc": 56.0}
{"epoch": 3, "training_loss": 69.29448366165161, "training_acc": 52.0, "val_loss": 17.163746058940887, "val_acc": 56.0}
{"epoch": 4, "training_loss": 69.32448983192444, "training_acc": 52.0, "val_loss": 17.20508188009262, "val_acc": 56.0}
{"epoch": 5, "training_loss": 69.24843883514404, "training_acc": 52.0, "val_loss": 17.18440353870392, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.23120641708374, "training_acc": 52.0, "val_loss": 17.190390825271606, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.28295302391052, "training_acc": 52.0, "val_loss": 17.210403084754944, "val_acc": 56.0}
{"epoch": 8, "training_loss": 69.2715265750885, "training_acc": 52.0, "val_loss": 17.20474064350128, "val_acc": 56.0}
{"epoch": 9, "training_loss": 69.33028817176819, "training_acc": 52.0, "val_loss": 17.267246544361115, "val_acc": 56.0}
{"epoch": 10, "training_loss": 69.31973505020142, "training_acc": 55.0, "val_loss": 17.40870624780655, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.46410989761353, "training_acc": 48.0, "val_loss": 17.34326481819153, "val_acc": 56.0}
{"epoch": 12, "training_loss": 69.517569065094, "training_acc": 42.0, "val_loss": 17.186206579208374, "val_acc": 56.0}
{"epoch": 13, "training_loss": 69.13692283630371, "training_acc": 52.0, "val_loss": 17.14215874671936, "val_acc": 56.0}
{"epoch": 14, "training_loss": 69.41023516654968, "training_acc": 52.0, "val_loss": 17.13051199913025, "val_acc": 56.0}
{"epoch": 15, "training_loss": 69.55708575248718, "training_acc": 52.0, "val_loss": 17.132891714572906, "val_acc": 56.0}
{"epoch": 16, "training_loss": 69.89390468597412, "training_acc": 52.0, "val_loss": 17.144188284873962, "val_acc": 56.0}
{"epoch": 17, "training_loss": 69.73157835006714, "training_acc": 52.0, "val_loss": 17.13101863861084, "val_acc": 56.0}
{"epoch": 18, "training_loss": 69.26076221466064, "training_acc": 52.0, "val_loss": 17.200498282909393, "val_acc": 56.0}
{"epoch": 19, "training_loss": 69.24327754974365, "training_acc": 50.0, "val_loss": 17.359232902526855, "val_acc": 56.0}
{"epoch": 20, "training_loss": 69.43198919296265, "training_acc": 48.0, "val_loss": 17.40472763776779, "val_acc": 56.0}
{"epoch": 21, "training_loss": 69.4734787940979, "training_acc": 48.0, "val_loss": 17.38705039024353, "val_acc": 56.0}
{"epoch": 22, "training_loss": 69.49514126777649, "training_acc": 49.0, "val_loss": 17.332954704761505, "val_acc": 56.0}
{"epoch": 23, "training_loss": 69.32746171951294, "training_acc": 50.0, "val_loss": 17.299456894397736, "val_acc": 56.0}
{"epoch": 24, "training_loss": 69.40821599960327, "training_acc": 50.0, "val_loss": 17.195342481136322, "val_acc": 56.0}
{"epoch": 25, "training_loss": 69.21021699905396, "training_acc": 52.0, "val_loss": 17.187045514583588, "val_acc": 56.0}
{"epoch": 26, "training_loss": 69.28535485267639, "training_acc": 52.0, "val_loss": 17.17873513698578, "val_acc": 56.0}
{"epoch": 27, "training_loss": 69.1903429031372, "training_acc": 52.0, "val_loss": 17.192642390727997, "val_acc": 56.0}
{"epoch": 28, "training_loss": 69.22997450828552, "training_acc": 52.0, "val_loss": 17.202937602996826, "val_acc": 56.0}
{"epoch": 29, "training_loss": 69.19612336158752, "training_acc": 52.0, "val_loss": 17.162486910820007, "val_acc": 56.0}
{"epoch": 30, "training_loss": 69.51684665679932, "training_acc": 52.0, "val_loss": 17.129451036453247, "val_acc": 56.0}
{"epoch": 31, "training_loss": 69.56217312812805, "training_acc": 52.0, "val_loss": 17.148984968662262, "val_acc": 56.0}
{"epoch": 32, "training_loss": 70.16322493553162, "training_acc": 52.0, "val_loss": 17.215803265571594, "val_acc": 56.0}
{"epoch": 33, "training_loss": 70.55295729637146, "training_acc": 52.0, "val_loss": 17.21559166908264, "val_acc": 56.0}
{"epoch": 34, "training_loss": 70.24465346336365, "training_acc": 52.0, "val_loss": 17.140880227088928, "val_acc": 56.0}
{"epoch": 35, "training_loss": 69.6827826499939, "training_acc": 52.0, "val_loss": 17.154161632061005, "val_acc": 56.0}
{"epoch": 36, "training_loss": 69.28844165802002, "training_acc": 52.0, "val_loss": 17.209617793560028, "val_acc": 56.0}
{"epoch": 37, "training_loss": 69.22720623016357, "training_acc": 52.0, "val_loss": 17.232437431812286, "val_acc": 56.0}
{"epoch": 38, "training_loss": 69.26272416114807, "training_acc": 52.0, "val_loss": 17.265526950359344, "val_acc": 56.0}
{"epoch": 39, "training_loss": 69.25662398338318, "training_acc": 53.0, "val_loss": 17.327086627483368, "val_acc": 56.0}
{"epoch": 40, "training_loss": 69.47525906562805, "training_acc": 48.0, "val_loss": 17.404265701770782, "val_acc": 56.0}
{"epoch": 41, "training_loss": 69.38093852996826, "training_acc": 48.0, "val_loss": 17.36411303281784, "val_acc": 56.0}
{"epoch": 42, "training_loss": 69.3679928779602, "training_acc": 48.0, "val_loss": 17.346294224262238, "val_acc": 56.0}
{"epoch": 43, "training_loss": 69.3421995639801, "training_acc": 49.0, "val_loss": 17.325115203857422, "val_acc": 56.0}
{"epoch": 44, "training_loss": 69.32396411895752, "training_acc": 53.0, "val_loss": 17.363078892230988, "val_acc": 56.0}
{"epoch": 45, "training_loss": 69.35978984832764, "training_acc": 49.0, "val_loss": 17.3390731215477, "val_acc": 56.0}
{"epoch": 46, "training_loss": 69.2973484992981, "training_acc": 54.0, "val_loss": 17.282982170581818, "val_acc": 56.0}
{"epoch": 47, "training_loss": 69.16903448104858, "training_acc": 52.0, "val_loss": 17.204511165618896, "val_acc": 56.0}
{"epoch": 48, "training_loss": 69.23307991027832, "training_acc": 52.0, "val_loss": 17.176009714603424, "val_acc": 56.0}
{"epoch": 49, "training_loss": 69.15192413330078, "training_acc": 52.0, "val_loss": 17.20854938030243, "val_acc": 56.0}
