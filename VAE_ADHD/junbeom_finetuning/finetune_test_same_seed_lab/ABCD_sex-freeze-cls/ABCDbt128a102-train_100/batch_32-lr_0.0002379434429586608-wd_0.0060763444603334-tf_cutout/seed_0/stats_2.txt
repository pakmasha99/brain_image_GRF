"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.6789481639862, "training_acc": 47.0, "val_loss": 17.282401025295258, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.21966457366943, "training_acc": 53.0, "val_loss": 17.29646325111389, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.26481533050537, "training_acc": 53.0, "val_loss": 17.37026423215866, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.29533243179321, "training_acc": 53.0, "val_loss": 17.355741560459137, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.29495024681091, "training_acc": 53.0, "val_loss": 17.315810918807983, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.14320659637451, "training_acc": 53.0, "val_loss": 17.323176562786102, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.32659816741943, "training_acc": 53.0, "val_loss": 17.30906218290329, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.06171560287476, "training_acc": 53.0, "val_loss": 17.353053390979767, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.26528263092041, "training_acc": 53.0, "val_loss": 17.37191677093506, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.2551794052124, "training_acc": 53.0, "val_loss": 17.33785569667816, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.2388002872467, "training_acc": 53.0, "val_loss": 17.282573878765106, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.10722970962524, "training_acc": 53.0, "val_loss": 17.281487584114075, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.14317512512207, "training_acc": 53.0, "val_loss": 17.280390858650208, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.0770845413208, "training_acc": 53.0, "val_loss": 17.280039191246033, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.06253933906555, "training_acc": 53.0, "val_loss": 17.279796302318573, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.27373600006104, "training_acc": 49.0, "val_loss": 17.29705184698105, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.23505020141602, "training_acc": 51.0, "val_loss": 17.28636920452118, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.12567663192749, "training_acc": 53.0, "val_loss": 17.2796830534935, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.1132640838623, "training_acc": 53.0, "val_loss": 17.294776439666748, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.12236547470093, "training_acc": 53.0, "val_loss": 17.32647716999054, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.06286525726318, "training_acc": 53.0, "val_loss": 17.36183762550354, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.27308225631714, "training_acc": 53.0, "val_loss": 17.353039979934692, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.12905311584473, "training_acc": 53.0, "val_loss": 17.286212742328644, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.0917398929596, "training_acc": 53.0, "val_loss": 17.28375554084778, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.13916969299316, "training_acc": 53.0, "val_loss": 17.278774082660675, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.12194585800171, "training_acc": 53.0, "val_loss": 17.283108830451965, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.06300139427185, "training_acc": 53.0, "val_loss": 17.28762835264206, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.0572280883789, "training_acc": 53.0, "val_loss": 17.284686863422394, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.02616500854492, "training_acc": 53.0, "val_loss": 17.28775054216385, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.05654883384705, "training_acc": 53.0, "val_loss": 17.298831045627594, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.05978798866272, "training_acc": 53.0, "val_loss": 17.290054261684418, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.05433464050293, "training_acc": 53.0, "val_loss": 17.30143576860428, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.12741827964783, "training_acc": 53.0, "val_loss": 17.31579899787903, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.05748867988586, "training_acc": 53.0, "val_loss": 17.291803658008575, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.03112864494324, "training_acc": 53.0, "val_loss": 17.287561297416687, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.07662630081177, "training_acc": 53.0, "val_loss": 17.31008142232895, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.11839604377747, "training_acc": 53.0, "val_loss": 17.309962213039398, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.1225380897522, "training_acc": 53.0, "val_loss": 17.38494485616684, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.34439253807068, "training_acc": 53.0, "val_loss": 17.508551478385925, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.78628659248352, "training_acc": 53.0, "val_loss": 17.553234100341797, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.80067253112793, "training_acc": 53.0, "val_loss": 17.545543611049652, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.76070475578308, "training_acc": 53.0, "val_loss": 17.488868534564972, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.59836077690125, "training_acc": 53.0, "val_loss": 17.39184409379959, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.21128463745117, "training_acc": 53.0, "val_loss": 17.36752688884735, "val_acc": 52.0}
