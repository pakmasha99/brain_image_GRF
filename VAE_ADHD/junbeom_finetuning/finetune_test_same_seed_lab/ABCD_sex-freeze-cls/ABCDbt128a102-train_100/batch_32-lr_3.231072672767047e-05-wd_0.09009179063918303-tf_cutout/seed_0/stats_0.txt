"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.26575684547424, "training_acc": 52.0, "val_loss": 17.236268520355225, "val_acc": 56.0}
{"epoch": 1, "training_loss": 69.2903482913971, "training_acc": 52.0, "val_loss": 17.223142087459564, "val_acc": 56.0}
{"epoch": 2, "training_loss": 69.28150773048401, "training_acc": 52.0, "val_loss": 17.22116768360138, "val_acc": 56.0}
{"epoch": 3, "training_loss": 69.22535371780396, "training_acc": 52.0, "val_loss": 17.209653556346893, "val_acc": 56.0}
{"epoch": 4, "training_loss": 69.28133392333984, "training_acc": 52.0, "val_loss": 17.212314903736115, "val_acc": 56.0}
{"epoch": 5, "training_loss": 69.23028135299683, "training_acc": 52.0, "val_loss": 17.205825448036194, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.22976303100586, "training_acc": 52.0, "val_loss": 17.20374971628189, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.26364946365356, "training_acc": 52.0, "val_loss": 17.204177379608154, "val_acc": 56.0}
{"epoch": 8, "training_loss": 69.2273199558258, "training_acc": 52.0, "val_loss": 17.201924324035645, "val_acc": 56.0}
{"epoch": 9, "training_loss": 69.28779220581055, "training_acc": 52.0, "val_loss": 17.208629846572876, "val_acc": 56.0}
{"epoch": 10, "training_loss": 69.25264430046082, "training_acc": 52.0, "val_loss": 17.223091423511505, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.23244452476501, "training_acc": 52.0, "val_loss": 17.22339540719986, "val_acc": 56.0}
{"epoch": 12, "training_loss": 69.26564192771912, "training_acc": 52.0, "val_loss": 17.21101850271225, "val_acc": 56.0}
{"epoch": 13, "training_loss": 69.21747660636902, "training_acc": 52.0, "val_loss": 17.204321920871735, "val_acc": 56.0}
{"epoch": 14, "training_loss": 69.26072883605957, "training_acc": 52.0, "val_loss": 17.19506084918976, "val_acc": 56.0}
{"epoch": 15, "training_loss": 69.23172497749329, "training_acc": 52.0, "val_loss": 17.187754809856415, "val_acc": 56.0}
{"epoch": 16, "training_loss": 69.31558179855347, "training_acc": 52.0, "val_loss": 17.176811397075653, "val_acc": 56.0}
{"epoch": 17, "training_loss": 69.25648212432861, "training_acc": 52.0, "val_loss": 17.177104949951172, "val_acc": 56.0}
{"epoch": 18, "training_loss": 69.23291611671448, "training_acc": 52.0, "val_loss": 17.18369871377945, "val_acc": 56.0}
{"epoch": 19, "training_loss": 69.23064470291138, "training_acc": 52.0, "val_loss": 17.192846536636353, "val_acc": 56.0}
{"epoch": 20, "training_loss": 69.26417708396912, "training_acc": 52.0, "val_loss": 17.19609946012497, "val_acc": 56.0}
{"epoch": 21, "training_loss": 69.26578569412231, "training_acc": 52.0, "val_loss": 17.199403047561646, "val_acc": 56.0}
{"epoch": 22, "training_loss": 69.2870876789093, "training_acc": 52.0, "val_loss": 17.202438414096832, "val_acc": 56.0}
{"epoch": 23, "training_loss": 69.23556900024414, "training_acc": 52.0, "val_loss": 17.207591235637665, "val_acc": 56.0}
{"epoch": 24, "training_loss": 69.24734020233154, "training_acc": 52.0, "val_loss": 17.20285713672638, "val_acc": 56.0}
{"epoch": 25, "training_loss": 69.22776913642883, "training_acc": 52.0, "val_loss": 17.206187546253204, "val_acc": 56.0}
{"epoch": 26, "training_loss": 69.23481965065002, "training_acc": 52.0, "val_loss": 17.207229137420654, "val_acc": 56.0}
{"epoch": 27, "training_loss": 69.22358965873718, "training_acc": 52.0, "val_loss": 17.21023917198181, "val_acc": 56.0}
{"epoch": 28, "training_loss": 69.24574422836304, "training_acc": 52.0, "val_loss": 17.211803793907166, "val_acc": 56.0}
{"epoch": 29, "training_loss": 69.2333242893219, "training_acc": 52.0, "val_loss": 17.20460057258606, "val_acc": 56.0}
