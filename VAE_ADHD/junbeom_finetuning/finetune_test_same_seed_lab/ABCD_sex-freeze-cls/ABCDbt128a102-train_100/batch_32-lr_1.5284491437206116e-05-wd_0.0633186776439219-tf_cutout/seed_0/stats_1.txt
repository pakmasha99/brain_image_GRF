"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 70.34655284881592, "training_acc": 47.0, "val_loss": 17.529135942459106, "val_acc": 52.0}
{"epoch": 1, "training_loss": 70.20896577835083, "training_acc": 47.0, "val_loss": 17.506994307041168, "val_acc": 52.0}
{"epoch": 2, "training_loss": 70.12348890304565, "training_acc": 47.0, "val_loss": 17.487700283527374, "val_acc": 52.0}
{"epoch": 3, "training_loss": 70.06993389129639, "training_acc": 47.0, "val_loss": 17.474868893623352, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.9968729019165, "training_acc": 47.0, "val_loss": 17.460811138153076, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.91286897659302, "training_acc": 47.0, "val_loss": 17.452216148376465, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.93613147735596, "training_acc": 47.0, "val_loss": 17.442794144153595, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.84495735168457, "training_acc": 47.0, "val_loss": 17.4363911151886, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.79566359519958, "training_acc": 47.0, "val_loss": 17.430008947849274, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.76211357116699, "training_acc": 47.0, "val_loss": 17.421860992908478, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.74070000648499, "training_acc": 47.0, "val_loss": 17.413656413555145, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.68676733970642, "training_acc": 47.0, "val_loss": 17.40436553955078, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.66738080978394, "training_acc": 47.0, "val_loss": 17.39533245563507, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.58554697036743, "training_acc": 47.0, "val_loss": 17.385375499725342, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.53003406524658, "training_acc": 47.0, "val_loss": 17.375314235687256, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.56535935401917, "training_acc": 47.0, "val_loss": 17.36815571784973, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.5122594833374, "training_acc": 47.0, "val_loss": 17.364294826984406, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.48149800300598, "training_acc": 46.0, "val_loss": 17.360571026802063, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.42196083068848, "training_acc": 49.0, "val_loss": 17.356878519058228, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.38435888290405, "training_acc": 47.0, "val_loss": 17.35278069972992, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.36682319641113, "training_acc": 47.0, "val_loss": 17.350436747074127, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.38023471832275, "training_acc": 48.0, "val_loss": 17.348074913024902, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.38434553146362, "training_acc": 45.0, "val_loss": 17.344389855861664, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.3768539428711, "training_acc": 47.0, "val_loss": 17.34178066253662, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.34019088745117, "training_acc": 45.0, "val_loss": 17.341110110282898, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.31850743293762, "training_acc": 51.0, "val_loss": 17.339789867401123, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.2973461151123, "training_acc": 51.0, "val_loss": 17.33740121126175, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.28975343704224, "training_acc": 53.0, "val_loss": 17.334546148777008, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.24911952018738, "training_acc": 53.0, "val_loss": 17.33383685350418, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.28611040115356, "training_acc": 53.0, "val_loss": 17.33265519142151, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.26359224319458, "training_acc": 53.0, "val_loss": 17.331668734550476, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.26561498641968, "training_acc": 53.0, "val_loss": 17.330925166606903, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.21809935569763, "training_acc": 53.0, "val_loss": 17.33008176088333, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.22494196891785, "training_acc": 53.0, "val_loss": 17.330028116703033, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.22833800315857, "training_acc": 53.0, "val_loss": 17.32981503009796, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.20122075080872, "training_acc": 53.0, "val_loss": 17.32959896326065, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.19181871414185, "training_acc": 53.0, "val_loss": 17.329443991184235, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.19214749336243, "training_acc": 53.0, "val_loss": 17.329299449920654, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.2382824420929, "training_acc": 53.0, "val_loss": 17.32902079820633, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.20465159416199, "training_acc": 53.0, "val_loss": 17.328965663909912, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.22087001800537, "training_acc": 53.0, "val_loss": 17.328806221485138, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.21022963523865, "training_acc": 53.0, "val_loss": 17.328917980194092, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.20894479751587, "training_acc": 53.0, "val_loss": 17.329320311546326, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.21682834625244, "training_acc": 53.0, "val_loss": 17.329539358615875, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.22573709487915, "training_acc": 53.0, "val_loss": 17.32923835515976, "val_acc": 52.0}
{"epoch": 45, "training_loss": 69.16969347000122, "training_acc": 53.0, "val_loss": 17.32938289642334, "val_acc": 52.0}
{"epoch": 46, "training_loss": 69.18967819213867, "training_acc": 53.0, "val_loss": 17.32991635799408, "val_acc": 52.0}
{"epoch": 47, "training_loss": 69.21111297607422, "training_acc": 53.0, "val_loss": 17.33027547597885, "val_acc": 52.0}
{"epoch": 48, "training_loss": 69.26586389541626, "training_acc": 53.0, "val_loss": 17.330627143383026, "val_acc": 52.0}
{"epoch": 49, "training_loss": 69.21439242362976, "training_acc": 53.0, "val_loss": 17.331092059612274, "val_acc": 52.0}
{"epoch": 50, "training_loss": 69.25877785682678, "training_acc": 53.0, "val_loss": 17.33138859272003, "val_acc": 52.0}
{"epoch": 51, "training_loss": 69.26074361801147, "training_acc": 53.0, "val_loss": 17.330819368362427, "val_acc": 52.0}
{"epoch": 52, "training_loss": 69.15983533859253, "training_acc": 53.0, "val_loss": 17.329992353916168, "val_acc": 52.0}
{"epoch": 53, "training_loss": 69.23457264900208, "training_acc": 53.0, "val_loss": 17.32933223247528, "val_acc": 52.0}
{"epoch": 54, "training_loss": 69.21033573150635, "training_acc": 53.0, "val_loss": 17.329001426696777, "val_acc": 52.0}
{"epoch": 55, "training_loss": 69.20589828491211, "training_acc": 53.0, "val_loss": 17.32887625694275, "val_acc": 52.0}
{"epoch": 56, "training_loss": 69.20497989654541, "training_acc": 53.0, "val_loss": 17.328855395317078, "val_acc": 52.0}
{"epoch": 57, "training_loss": 69.22124314308167, "training_acc": 53.0, "val_loss": 17.328856885433197, "val_acc": 52.0}
{"epoch": 58, "training_loss": 69.2133412361145, "training_acc": 53.0, "val_loss": 17.328867316246033, "val_acc": 52.0}
{"epoch": 59, "training_loss": 69.19385147094727, "training_acc": 53.0, "val_loss": 17.328868806362152, "val_acc": 52.0}
