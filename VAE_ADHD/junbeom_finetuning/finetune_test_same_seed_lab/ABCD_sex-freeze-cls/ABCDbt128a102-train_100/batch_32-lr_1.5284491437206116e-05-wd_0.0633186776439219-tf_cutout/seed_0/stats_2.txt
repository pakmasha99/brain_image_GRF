"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.07642889022827, "training_acc": 53.0, "val_loss": 17.35352724790573, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.0612051486969, "training_acc": 53.0, "val_loss": 17.355448007583618, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.04355764389038, "training_acc": 53.0, "val_loss": 17.35840141773224, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.09116435050964, "training_acc": 53.0, "val_loss": 17.359736561775208, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.01087260246277, "training_acc": 53.0, "val_loss": 17.35880821943283, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.03316497802734, "training_acc": 53.0, "val_loss": 17.357775568962097, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.0409197807312, "training_acc": 53.0, "val_loss": 17.35820472240448, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.04496121406555, "training_acc": 53.0, "val_loss": 17.357949912548065, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.12449645996094, "training_acc": 53.0, "val_loss": 17.356882989406586, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.09446716308594, "training_acc": 53.0, "val_loss": 17.356479167938232, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.04584431648254, "training_acc": 53.0, "val_loss": 17.354941368103027, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.02649736404419, "training_acc": 53.0, "val_loss": 17.35382378101349, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.05034065246582, "training_acc": 53.0, "val_loss": 17.352791130542755, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.05288028717041, "training_acc": 53.0, "val_loss": 17.351481318473816, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.07303738594055, "training_acc": 53.0, "val_loss": 17.35064536333084, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.049973487854, "training_acc": 53.0, "val_loss": 17.350859940052032, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.0937077999115, "training_acc": 53.0, "val_loss": 17.350982129573822, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.03473424911499, "training_acc": 53.0, "val_loss": 17.351306974887848, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.0721983909607, "training_acc": 53.0, "val_loss": 17.351992428302765, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.07998895645142, "training_acc": 53.0, "val_loss": 17.35205054283142, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.06246972084045, "training_acc": 53.0, "val_loss": 17.351146042346954, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.11444139480591, "training_acc": 53.0, "val_loss": 17.35055446624756, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.03869581222534, "training_acc": 53.0, "val_loss": 17.350077629089355, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.04516458511353, "training_acc": 53.0, "val_loss": 17.349490523338318, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.11444401741028, "training_acc": 53.0, "val_loss": 17.34895408153534, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.06987404823303, "training_acc": 53.0, "val_loss": 17.348697781562805, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.09333324432373, "training_acc": 53.0, "val_loss": 17.348575592041016, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.0737817287445, "training_acc": 53.0, "val_loss": 17.348603904247284, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.10663771629333, "training_acc": 53.0, "val_loss": 17.348699271678925, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.08054232597351, "training_acc": 53.0, "val_loss": 17.34858602285385, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.04607343673706, "training_acc": 53.0, "val_loss": 17.348578572273254, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.08284568786621, "training_acc": 53.0, "val_loss": 17.348605394363403, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.11755990982056, "training_acc": 53.0, "val_loss": 17.348793148994446, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.0941276550293, "training_acc": 53.0, "val_loss": 17.349053919315338, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.06256937980652, "training_acc": 53.0, "val_loss": 17.34883040189743, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.08008289337158, "training_acc": 53.0, "val_loss": 17.348632216453552, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.09998512268066, "training_acc": 53.0, "val_loss": 17.348575592041016, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.09280180931091, "training_acc": 53.0, "val_loss": 17.348647117614746, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.05008125305176, "training_acc": 53.0, "val_loss": 17.34905242919922, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.03655338287354, "training_acc": 53.0, "val_loss": 17.34927147626877, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.0961799621582, "training_acc": 53.0, "val_loss": 17.34965294599533, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.10633659362793, "training_acc": 53.0, "val_loss": 17.35011637210846, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.0999641418457, "training_acc": 53.0, "val_loss": 17.350471019744873, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.11810398101807, "training_acc": 53.0, "val_loss": 17.350496351718903, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.1448667049408, "training_acc": 53.0, "val_loss": 17.34999269247055, "val_acc": 52.0}
{"epoch": 45, "training_loss": 69.09871506690979, "training_acc": 53.0, "val_loss": 17.34955608844757, "val_acc": 52.0}
{"epoch": 46, "training_loss": 69.09375476837158, "training_acc": 53.0, "val_loss": 17.34992116689682, "val_acc": 52.0}
{"epoch": 47, "training_loss": 69.14655947685242, "training_acc": 53.0, "val_loss": 17.350348830223083, "val_acc": 52.0}
{"epoch": 48, "training_loss": 69.09885120391846, "training_acc": 53.0, "val_loss": 17.35066920518875, "val_acc": 52.0}
{"epoch": 49, "training_loss": 69.10469436645508, "training_acc": 53.0, "val_loss": 17.351119220256805, "val_acc": 52.0}
{"epoch": 50, "training_loss": 69.11193418502808, "training_acc": 53.0, "val_loss": 17.35122501850128, "val_acc": 52.0}
{"epoch": 51, "training_loss": 69.10788130760193, "training_acc": 53.0, "val_loss": 17.350715398788452, "val_acc": 52.0}
{"epoch": 52, "training_loss": 69.13364839553833, "training_acc": 53.0, "val_loss": 17.349958419799805, "val_acc": 52.0}
{"epoch": 53, "training_loss": 69.17498517036438, "training_acc": 53.0, "val_loss": 17.349235713481903, "val_acc": 52.0}
{"epoch": 54, "training_loss": 69.08438682556152, "training_acc": 53.0, "val_loss": 17.34909415245056, "val_acc": 52.0}
{"epoch": 55, "training_loss": 69.07722759246826, "training_acc": 53.0, "val_loss": 17.349226772785187, "val_acc": 52.0}
