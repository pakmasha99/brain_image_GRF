"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.2587251663208, "training_acc": 52.0, "val_loss": 17.235803604125977, "val_acc": 56.0}
{"epoch": 1, "training_loss": 69.2858533859253, "training_acc": 52.0, "val_loss": 17.229610681533813, "val_acc": 56.0}
{"epoch": 2, "training_loss": 69.27521514892578, "training_acc": 52.0, "val_loss": 17.2284334897995, "val_acc": 56.0}
{"epoch": 3, "training_loss": 69.22139048576355, "training_acc": 52.0, "val_loss": 17.22235083580017, "val_acc": 56.0}
{"epoch": 4, "training_loss": 69.28221321105957, "training_acc": 52.0, "val_loss": 17.223238945007324, "val_acc": 56.0}
{"epoch": 5, "training_loss": 69.2299656867981, "training_acc": 52.0, "val_loss": 17.21939444541931, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.23148345947266, "training_acc": 52.0, "val_loss": 17.21765249967575, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.26362657546997, "training_acc": 52.0, "val_loss": 17.21709370613098, "val_acc": 56.0}
{"epoch": 8, "training_loss": 69.22254943847656, "training_acc": 52.0, "val_loss": 17.21511036157608, "val_acc": 56.0}
{"epoch": 9, "training_loss": 69.28418397903442, "training_acc": 52.0, "val_loss": 17.217737436294556, "val_acc": 56.0}
{"epoch": 10, "training_loss": 69.2556881904602, "training_acc": 52.0, "val_loss": 17.224223911762238, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.23171615600586, "training_acc": 52.0, "val_loss": 17.22397953271866, "val_acc": 56.0}
{"epoch": 12, "training_loss": 69.25600624084473, "training_acc": 52.0, "val_loss": 17.217738926410675, "val_acc": 56.0}
{"epoch": 13, "training_loss": 69.22313737869263, "training_acc": 52.0, "val_loss": 17.214137315750122, "val_acc": 56.0}
{"epoch": 14, "training_loss": 69.26265525817871, "training_acc": 52.0, "val_loss": 17.209050059318542, "val_acc": 56.0}
{"epoch": 15, "training_loss": 69.2321834564209, "training_acc": 52.0, "val_loss": 17.204734683036804, "val_acc": 56.0}
{"epoch": 16, "training_loss": 69.29728555679321, "training_acc": 52.0, "val_loss": 17.19811260700226, "val_acc": 56.0}
{"epoch": 17, "training_loss": 69.24234580993652, "training_acc": 52.0, "val_loss": 17.197604477405548, "val_acc": 56.0}
{"epoch": 18, "training_loss": 69.2234423160553, "training_acc": 52.0, "val_loss": 17.200511693954468, "val_acc": 56.0}
{"epoch": 19, "training_loss": 69.22363042831421, "training_acc": 52.0, "val_loss": 17.204585671424866, "val_acc": 56.0}
{"epoch": 20, "training_loss": 69.26214861869812, "training_acc": 52.0, "val_loss": 17.205534875392914, "val_acc": 56.0}
{"epoch": 21, "training_loss": 69.26449012756348, "training_acc": 52.0, "val_loss": 17.206567525863647, "val_acc": 56.0}
{"epoch": 22, "training_loss": 69.2897219657898, "training_acc": 52.0, "val_loss": 17.207534611225128, "val_acc": 56.0}
{"epoch": 23, "training_loss": 69.23916578292847, "training_acc": 52.0, "val_loss": 17.20961183309555, "val_acc": 56.0}
{"epoch": 24, "training_loss": 69.24412631988525, "training_acc": 52.0, "val_loss": 17.20701903104782, "val_acc": 56.0}
{"epoch": 25, "training_loss": 69.2306158542633, "training_acc": 52.0, "val_loss": 17.20837503671646, "val_acc": 56.0}
{"epoch": 26, "training_loss": 69.23640871047974, "training_acc": 52.0, "val_loss": 17.20867156982422, "val_acc": 56.0}
{"epoch": 27, "training_loss": 69.22824716567993, "training_acc": 52.0, "val_loss": 17.20994859933853, "val_acc": 56.0}
{"epoch": 28, "training_loss": 69.24754619598389, "training_acc": 52.0, "val_loss": 17.210589349269867, "val_acc": 56.0}
{"epoch": 29, "training_loss": 69.23729181289673, "training_acc": 52.0, "val_loss": 17.20712184906006, "val_acc": 56.0}
{"epoch": 30, "training_loss": 69.28288125991821, "training_acc": 52.0, "val_loss": 17.19934493303299, "val_acc": 56.0}
{"epoch": 31, "training_loss": 69.28013467788696, "training_acc": 52.0, "val_loss": 17.192503809928894, "val_acc": 56.0}
{"epoch": 32, "training_loss": 69.27133917808533, "training_acc": 52.0, "val_loss": 17.183570563793182, "val_acc": 56.0}
{"epoch": 33, "training_loss": 69.24815392494202, "training_acc": 52.0, "val_loss": 17.17786192893982, "val_acc": 56.0}
{"epoch": 34, "training_loss": 69.26399183273315, "training_acc": 52.0, "val_loss": 17.176547646522522, "val_acc": 56.0}
{"epoch": 35, "training_loss": 69.29033136367798, "training_acc": 52.0, "val_loss": 17.177917063236237, "val_acc": 56.0}
{"epoch": 36, "training_loss": 69.29212808609009, "training_acc": 52.0, "val_loss": 17.177051305770874, "val_acc": 56.0}
{"epoch": 37, "training_loss": 69.24603295326233, "training_acc": 52.0, "val_loss": 17.17565953731537, "val_acc": 56.0}
{"epoch": 38, "training_loss": 69.269207239151, "training_acc": 52.0, "val_loss": 17.17609316110611, "val_acc": 56.0}
{"epoch": 39, "training_loss": 69.31304597854614, "training_acc": 52.0, "val_loss": 17.17861145734787, "val_acc": 56.0}
{"epoch": 40, "training_loss": 69.28325653076172, "training_acc": 52.0, "val_loss": 17.18270778656006, "val_acc": 56.0}
{"epoch": 41, "training_loss": 69.23214030265808, "training_acc": 52.0, "val_loss": 17.18464493751526, "val_acc": 56.0}
{"epoch": 42, "training_loss": 69.26085162162781, "training_acc": 52.0, "val_loss": 17.187896370887756, "val_acc": 56.0}
{"epoch": 43, "training_loss": 69.22134304046631, "training_acc": 52.0, "val_loss": 17.19133108854294, "val_acc": 56.0}
{"epoch": 44, "training_loss": 69.23362040519714, "training_acc": 52.0, "val_loss": 17.197033762931824, "val_acc": 56.0}
{"epoch": 45, "training_loss": 69.25939726829529, "training_acc": 52.0, "val_loss": 17.20087230205536, "val_acc": 56.0}
{"epoch": 46, "training_loss": 69.24879908561707, "training_acc": 52.0, "val_loss": 17.203304171562195, "val_acc": 56.0}
{"epoch": 47, "training_loss": 69.23287057876587, "training_acc": 52.0, "val_loss": 17.203262448310852, "val_acc": 56.0}
{"epoch": 48, "training_loss": 69.2543842792511, "training_acc": 52.0, "val_loss": 17.203983664512634, "val_acc": 56.0}
{"epoch": 49, "training_loss": 69.2284586429596, "training_acc": 52.0, "val_loss": 17.207786440849304, "val_acc": 56.0}
{"epoch": 50, "training_loss": 69.26993465423584, "training_acc": 52.0, "val_loss": 17.214278876781464, "val_acc": 56.0}
{"epoch": 51, "training_loss": 69.27853965759277, "training_acc": 52.0, "val_loss": 17.22235381603241, "val_acc": 56.0}
{"epoch": 52, "training_loss": 69.26245188713074, "training_acc": 52.0, "val_loss": 17.230813205242157, "val_acc": 56.0}
{"epoch": 53, "training_loss": 69.25799560546875, "training_acc": 52.0, "val_loss": 17.23647266626358, "val_acc": 56.0}
{"epoch": 54, "training_loss": 69.24568128585815, "training_acc": 52.0, "val_loss": 17.242608964443207, "val_acc": 56.0}
{"epoch": 55, "training_loss": 69.24485945701599, "training_acc": 52.0, "val_loss": 17.25330650806427, "val_acc": 56.0}
{"epoch": 56, "training_loss": 69.2730221748352, "training_acc": 52.0, "val_loss": 17.26740300655365, "val_acc": 56.0}
