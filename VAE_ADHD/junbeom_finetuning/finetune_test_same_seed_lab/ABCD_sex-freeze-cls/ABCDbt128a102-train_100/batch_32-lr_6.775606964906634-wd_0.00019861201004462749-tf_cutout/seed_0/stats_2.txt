"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 63104.65172958374, "training_acc": 51.0, "val_loss": 17123.904418945312, "val_acc": 48.0}
{"epoch": 1, "training_loss": 44960.36865234375, "training_acc": 43.0, "val_loss": 6558.8470458984375, "val_acc": 52.0}
{"epoch": 2, "training_loss": 22400.586364746094, "training_acc": 53.0, "val_loss": 13876.368713378906, "val_acc": 48.0}
{"epoch": 3, "training_loss": 36859.74755859375, "training_acc": 45.0, "val_loss": 12716.659545898438, "val_acc": 52.0}
{"epoch": 4, "training_loss": 43334.527404785156, "training_acc": 53.0, "val_loss": 6655.2276611328125, "val_acc": 48.0}
{"epoch": 5, "training_loss": 44852.70263671875, "training_acc": 47.0, "val_loss": 1182.0117950439453, "val_acc": 48.0}
{"epoch": 6, "training_loss": 39133.2275390625, "training_acc": 45.0, "val_loss": 16363.267517089844, "val_acc": 52.0}
{"epoch": 7, "training_loss": 37630.9716796875, "training_acc": 57.0, "val_loss": 13930.604553222656, "val_acc": 48.0}
{"epoch": 8, "training_loss": 74697.68408203125, "training_acc": 47.0, "val_loss": 8493.384552001953, "val_acc": 48.0}
{"epoch": 9, "training_loss": 28934.078857421875, "training_acc": 51.0, "val_loss": 12985.493469238281, "val_acc": 52.0}
{"epoch": 10, "training_loss": 34631.4013671875, "training_acc": 53.0, "val_loss": 8035.979461669922, "val_acc": 48.0}
{"epoch": 11, "training_loss": 27466.28271484375, "training_acc": 45.0, "val_loss": 3311.642837524414, "val_acc": 52.0}
{"epoch": 12, "training_loss": 12602.7392578125, "training_acc": 49.0, "val_loss": 1816.084861755371, "val_acc": 48.0}
{"epoch": 13, "training_loss": 16938.47607421875, "training_acc": 53.0, "val_loss": 2613.298225402832, "val_acc": 52.0}
{"epoch": 14, "training_loss": 19742.7119140625, "training_acc": 49.0, "val_loss": 3322.0699310302734, "val_acc": 48.0}
{"epoch": 15, "training_loss": 13648.695068359375, "training_acc": 55.0, "val_loss": 3016.2538528442383, "val_acc": 52.0}
{"epoch": 16, "training_loss": 12827.140943527222, "training_acc": 43.0, "val_loss": 3454.679489135742, "val_acc": 52.0}
{"epoch": 17, "training_loss": 13219.8466796875, "training_acc": 53.0, "val_loss": 3930.875015258789, "val_acc": 48.0}
{"epoch": 18, "training_loss": 13733.372985839844, "training_acc": 45.0, "val_loss": 5134.989929199219, "val_acc": 52.0}
{"epoch": 19, "training_loss": 14069.196166992188, "training_acc": 51.0, "val_loss": 3813.018798828125, "val_acc": 48.0}
{"epoch": 20, "training_loss": 7757.195592880249, "training_acc": 54.0, "val_loss": 3209.075927734375, "val_acc": 48.0}
{"epoch": 21, "training_loss": 8167.119846343994, "training_acc": 53.0, "val_loss": 451.83701515197754, "val_acc": 48.0}
{"epoch": 22, "training_loss": 4438.786712646484, "training_acc": 49.0, "val_loss": 3536.588668823242, "val_acc": 48.0}
{"epoch": 23, "training_loss": 9071.645446777344, "training_acc": 59.0, "val_loss": 8626.50146484375, "val_acc": 52.0}
{"epoch": 24, "training_loss": 25088.24285888672, "training_acc": 53.0, "val_loss": 7791.490173339844, "val_acc": 48.0}
{"epoch": 25, "training_loss": 22503.614318847656, "training_acc": 47.0, "val_loss": 8533.20083618164, "val_acc": 52.0}
{"epoch": 26, "training_loss": 27879.513076782227, "training_acc": 53.0, "val_loss": 9403.52783203125, "val_acc": 48.0}
{"epoch": 27, "training_loss": 57224.474365234375, "training_acc": 47.0, "val_loss": 9254.120635986328, "val_acc": 48.0}
{"epoch": 28, "training_loss": 27733.66796875, "training_acc": 47.0, "val_loss": 20084.136962890625, "val_acc": 52.0}
{"epoch": 29, "training_loss": 74224.8994140625, "training_acc": 53.0, "val_loss": 4929.100036621094, "val_acc": 52.0}
{"epoch": 30, "training_loss": 30809.208374023438, "training_acc": 49.0, "val_loss": 11101.034545898438, "val_acc": 48.0}
{"epoch": 31, "training_loss": 25655.17626953125, "training_acc": 51.0, "val_loss": 4433.941268920898, "val_acc": 52.0}
{"epoch": 32, "training_loss": 11139.41136932373, "training_acc": 45.0, "val_loss": 6242.252731323242, "val_acc": 52.0}
{"epoch": 33, "training_loss": 32022.16229248047, "training_acc": 53.0, "val_loss": 287.546443939209, "val_acc": 52.0}
{"epoch": 34, "training_loss": 26627.004760742188, "training_acc": 52.0, "val_loss": 4950.713729858398, "val_acc": 48.0}
{"epoch": 35, "training_loss": 21982.01708984375, "training_acc": 47.0, "val_loss": 5838.959884643555, "val_acc": 52.0}
{"epoch": 36, "training_loss": 19224.54083251953, "training_acc": 51.0, "val_loss": 9406.455993652344, "val_acc": 48.0}
{"epoch": 37, "training_loss": 19613.60205078125, "training_acc": 55.0, "val_loss": 6406.5521240234375, "val_acc": 52.0}
{"epoch": 38, "training_loss": 13279.038681030273, "training_acc": 53.0, "val_loss": 1620.0237274169922, "val_acc": 48.0}
{"epoch": 39, "training_loss": 6636.013885498047, "training_acc": 54.0, "val_loss": 2941.240692138672, "val_acc": 48.0}
{"epoch": 40, "training_loss": 8775.5205078125, "training_acc": 51.0, "val_loss": 2934.077262878418, "val_acc": 52.0}
{"epoch": 41, "training_loss": 10641.151489257812, "training_acc": 53.0, "val_loss": 4600.403213500977, "val_acc": 48.0}
{"epoch": 42, "training_loss": 11804.355377197266, "training_acc": 47.0, "val_loss": 3541.0892486572266, "val_acc": 48.0}
{"epoch": 43, "training_loss": 12519.067138671875, "training_acc": 47.0, "val_loss": 6459.100341796875, "val_acc": 52.0}
{"epoch": 44, "training_loss": 15375.9931640625, "training_acc": 56.0, "val_loss": 5877.345657348633, "val_acc": 48.0}
{"epoch": 45, "training_loss": 15718.12777709961, "training_acc": 47.0, "val_loss": 4178.299713134766, "val_acc": 52.0}
{"epoch": 46, "training_loss": 12751.298797607422, "training_acc": 51.0, "val_loss": 5874.426651000977, "val_acc": 48.0}
{"epoch": 47, "training_loss": 18430.366943359375, "training_acc": 49.0, "val_loss": 8950.531005859375, "val_acc": 52.0}
{"epoch": 48, "training_loss": 22106.189819335938, "training_acc": 51.0, "val_loss": 8581.465911865234, "val_acc": 48.0}
{"epoch": 49, "training_loss": 24135.748413085938, "training_acc": 45.0, "val_loss": 7899.660491943359, "val_acc": 52.0}
{"epoch": 50, "training_loss": 22137.95147705078, "training_acc": 53.0, "val_loss": 5993.599319458008, "val_acc": 48.0}
{"epoch": 51, "training_loss": 14006.987365722656, "training_acc": 53.0, "val_loss": 9215.464782714844, "val_acc": 52.0}
{"epoch": 52, "training_loss": 33857.46374511719, "training_acc": 53.0, "val_loss": 6065.047073364258, "val_acc": 48.0}
