"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 66757.10431671143, "training_acc": 47.0, "val_loss": 23534.75799560547, "val_acc": 48.0}
{"epoch": 1, "training_loss": 73102.49658203125, "training_acc": 49.0, "val_loss": 13867.448425292969, "val_acc": 52.0}
{"epoch": 2, "training_loss": 51545.46765136719, "training_acc": 53.0, "val_loss": 8812.806701660156, "val_acc": 48.0}
{"epoch": 3, "training_loss": 52548.571533203125, "training_acc": 47.0, "val_loss": 1261.67573928833, "val_acc": 52.0}
{"epoch": 4, "training_loss": 17502.725463867188, "training_acc": 53.0, "val_loss": 2716.3497924804688, "val_acc": 48.0}
{"epoch": 5, "training_loss": 12771.907592773438, "training_acc": 45.0, "val_loss": 5191.525650024414, "val_acc": 52.0}
{"epoch": 6, "training_loss": 14935.552124023438, "training_acc": 51.0, "val_loss": 3628.4255981445312, "val_acc": 48.0}
{"epoch": 7, "training_loss": 6443.458255767822, "training_acc": 57.0, "val_loss": 3158.684539794922, "val_acc": 52.0}
{"epoch": 8, "training_loss": 11373.24072265625, "training_acc": 51.0, "val_loss": 5976.008605957031, "val_acc": 48.0}
{"epoch": 9, "training_loss": 17453.6142578125, "training_acc": 49.0, "val_loss": 3575.3662109375, "val_acc": 52.0}
{"epoch": 10, "training_loss": 12933.228454589844, "training_acc": 49.0, "val_loss": 4603.643035888672, "val_acc": 52.0}
{"epoch": 11, "training_loss": 26304.391845703125, "training_acc": 53.0, "val_loss": 661.1364364624023, "val_acc": 48.0}
{"epoch": 12, "training_loss": 4082.130931854248, "training_acc": 49.0, "val_loss": 2047.4311828613281, "val_acc": 48.0}
{"epoch": 13, "training_loss": 10319.309509277344, "training_acc": 45.0, "val_loss": 2936.846160888672, "val_acc": 52.0}
{"epoch": 14, "training_loss": 12180.780517578125, "training_acc": 47.0, "val_loss": 5119.496154785156, "val_acc": 52.0}
{"epoch": 15, "training_loss": 26568.937103271484, "training_acc": 53.0, "val_loss": 505.7743549346924, "val_acc": 48.0}
{"epoch": 16, "training_loss": 4562.192138671875, "training_acc": 47.0, "val_loss": 4670.968246459961, "val_acc": 48.0}
{"epoch": 17, "training_loss": 23933.43585205078, "training_acc": 47.0, "val_loss": 6605.371856689453, "val_acc": 52.0}
{"epoch": 18, "training_loss": 38500.46862792969, "training_acc": 53.0, "val_loss": 6966.685485839844, "val_acc": 52.0}
{"epoch": 19, "training_loss": 21840.174926757812, "training_acc": 51.0, "val_loss": 13904.536437988281, "val_acc": 48.0}
{"epoch": 20, "training_loss": 43403.81530761719, "training_acc": 47.0, "val_loss": 4335.160446166992, "val_acc": 52.0}
{"epoch": 21, "training_loss": 14922.914428710938, "training_acc": 49.0, "val_loss": 2655.7167053222656, "val_acc": 48.0}
{"epoch": 22, "training_loss": 8988.907299041748, "training_acc": 51.0, "val_loss": 3207.8929901123047, "val_acc": 48.0}
{"epoch": 23, "training_loss": 9938.7855758667, "training_acc": 45.0, "val_loss": 803.5840034484863, "val_acc": 48.0}
{"epoch": 24, "training_loss": 2556.948341369629, "training_acc": 53.0, "val_loss": 1528.7846565246582, "val_acc": 52.0}
{"epoch": 25, "training_loss": 2703.849266052246, "training_acc": 59.0, "val_loss": 81.91243410110474, "val_acc": 44.0}
{"epoch": 26, "training_loss": 2764.597702026367, "training_acc": 57.0, "val_loss": 2506.787109375, "val_acc": 52.0}
{"epoch": 27, "training_loss": 8532.509029388428, "training_acc": 43.0, "val_loss": 649.8217582702637, "val_acc": 52.0}
{"epoch": 28, "training_loss": 2772.710952758789, "training_acc": 55.0, "val_loss": 210.98096370697021, "val_acc": 56.0}
{"epoch": 29, "training_loss": 2989.9577865600586, "training_acc": 63.0, "val_loss": 1677.6220321655273, "val_acc": 52.0}
{"epoch": 30, "training_loss": 5721.694763183594, "training_acc": 55.0, "val_loss": 1584.605884552002, "val_acc": 52.0}
{"epoch": 31, "training_loss": 5314.6956787109375, "training_acc": 58.0, "val_loss": 8401.929473876953, "val_acc": 48.0}
{"epoch": 32, "training_loss": 24687.698669433594, "training_acc": 49.0, "val_loss": 8197.45101928711, "val_acc": 52.0}
{"epoch": 33, "training_loss": 29933.31561279297, "training_acc": 53.0, "val_loss": 7148.458099365234, "val_acc": 48.0}
{"epoch": 34, "training_loss": 40463.6015625, "training_acc": 47.0, "val_loss": 5165.4022216796875, "val_acc": 48.0}
{"epoch": 35, "training_loss": 10898.233947753906, "training_acc": 53.0, "val_loss": 299.85783100128174, "val_acc": 52.0}
{"epoch": 36, "training_loss": 9174.756530761719, "training_acc": 51.0, "val_loss": 298.58386516571045, "val_acc": 52.0}
{"epoch": 37, "training_loss": 5008.059875488281, "training_acc": 45.0, "val_loss": 2754.3081283569336, "val_acc": 48.0}
{"epoch": 38, "training_loss": 5079.8486328125, "training_acc": 55.0, "val_loss": 999.6547698974609, "val_acc": 52.0}
{"epoch": 39, "training_loss": 7432.695861816406, "training_acc": 47.0, "val_loss": 2071.824836730957, "val_acc": 52.0}
{"epoch": 40, "training_loss": 8647.398681640625, "training_acc": 55.0, "val_loss": 4418.7591552734375, "val_acc": 48.0}
{"epoch": 41, "training_loss": 13621.80419921875, "training_acc": 45.0, "val_loss": 1428.8323402404785, "val_acc": 52.0}
{"epoch": 42, "training_loss": 8642.484558105469, "training_acc": 57.0, "val_loss": 5374.682998657227, "val_acc": 52.0}
{"epoch": 43, "training_loss": 29833.73406982422, "training_acc": 53.0, "val_loss": 338.473105430603, "val_acc": 52.0}
{"epoch": 44, "training_loss": 8145.335510253906, "training_acc": 51.0, "val_loss": 2604.0517807006836, "val_acc": 52.0}
