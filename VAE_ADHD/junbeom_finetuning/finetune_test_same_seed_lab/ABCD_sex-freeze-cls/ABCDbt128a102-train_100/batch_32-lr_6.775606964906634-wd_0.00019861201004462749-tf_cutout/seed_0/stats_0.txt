"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 60070.32406616211, "training_acc": 48.0, "val_loss": 25094.32830810547, "val_acc": 56.0}
{"epoch": 1, "training_loss": 89993.70306396484, "training_acc": 52.0, "val_loss": 15514.92919921875, "val_acc": 44.0}
{"epoch": 2, "training_loss": 87856.37890625, "training_acc": 48.0, "val_loss": 12484.298706054688, "val_acc": 44.0}
{"epoch": 3, "training_loss": 32764.64794921875, "training_acc": 50.0, "val_loss": 7644.905853271484, "val_acc": 56.0}
{"epoch": 4, "training_loss": 26755.659057617188, "training_acc": 50.0, "val_loss": 13591.572570800781, "val_acc": 44.0}
{"epoch": 5, "training_loss": 29317.42657470703, "training_acc": 44.0, "val_loss": 3489.153289794922, "val_acc": 56.0}
{"epoch": 6, "training_loss": 15225.413848876953, "training_acc": 50.0, "val_loss": 7733.561706542969, "val_acc": 44.0}
{"epoch": 7, "training_loss": 18301.005126953125, "training_acc": 52.0, "val_loss": 7651.088714599609, "val_acc": 56.0}
{"epoch": 8, "training_loss": 18859.180419921875, "training_acc": 52.0, "val_loss": 13302.096557617188, "val_acc": 44.0}
{"epoch": 9, "training_loss": 49058.6552734375, "training_acc": 48.0, "val_loss": 1881.5299987792969, "val_acc": 56.0}
{"epoch": 10, "training_loss": 12760.927215576172, "training_acc": 50.0, "val_loss": 515.7753944396973, "val_acc": 44.0}
{"epoch": 11, "training_loss": 10101.535949707031, "training_acc": 54.0, "val_loss": 1491.7219161987305, "val_acc": 44.0}
{"epoch": 12, "training_loss": 4541.187677383423, "training_acc": 44.0, "val_loss": 3726.9065856933594, "val_acc": 56.0}
{"epoch": 13, "training_loss": 13516.912963867188, "training_acc": 50.0, "val_loss": 4324.032211303711, "val_acc": 44.0}
{"epoch": 14, "training_loss": 13382.062927246094, "training_acc": 52.0, "val_loss": 6674.821472167969, "val_acc": 56.0}
{"epoch": 15, "training_loss": 21032.3388671875, "training_acc": 46.0, "val_loss": 2735.64453125, "val_acc": 44.0}
{"epoch": 16, "training_loss": 21204.237670898438, "training_acc": 46.0, "val_loss": 2358.560562133789, "val_acc": 56.0}
{"epoch": 17, "training_loss": 21556.36492919922, "training_acc": 50.0, "val_loss": 6481.0699462890625, "val_acc": 44.0}
{"epoch": 18, "training_loss": 20032.88592529297, "training_acc": 50.0, "val_loss": 9435.939025878906, "val_acc": 56.0}
{"epoch": 19, "training_loss": 23508.729858398438, "training_acc": 50.0, "val_loss": 13675.901794433594, "val_acc": 44.0}
{"epoch": 20, "training_loss": 44201.02813720703, "training_acc": 48.0, "val_loss": 5685.587310791016, "val_acc": 56.0}
{"epoch": 21, "training_loss": 38161.988525390625, "training_acc": 52.0, "val_loss": 5476.148986816406, "val_acc": 56.0}
{"epoch": 22, "training_loss": 11003.373123168945, "training_acc": 54.0, "val_loss": 574.6213436126709, "val_acc": 44.0}
{"epoch": 23, "training_loss": 16957.912963867188, "training_acc": 48.0, "val_loss": 2640.926742553711, "val_acc": 56.0}
{"epoch": 24, "training_loss": 6232.523452758789, "training_acc": 54.0, "val_loss": 1688.6865615844727, "val_acc": 44.0}
{"epoch": 25, "training_loss": 3508.390335083008, "training_acc": 49.0, "val_loss": 1460.1229667663574, "val_acc": 56.0}
{"epoch": 26, "training_loss": 3516.963357925415, "training_acc": 57.0, "val_loss": 1038.2343292236328, "val_acc": 44.0}
{"epoch": 27, "training_loss": 3280.6761627197266, "training_acc": 48.0, "val_loss": 954.5368194580078, "val_acc": 44.0}
{"epoch": 28, "training_loss": 3818.7240142822266, "training_acc": 44.0, "val_loss": 1192.1698570251465, "val_acc": 56.0}
{"epoch": 29, "training_loss": 3933.789794921875, "training_acc": 56.0, "val_loss": 5147.587203979492, "val_acc": 56.0}
