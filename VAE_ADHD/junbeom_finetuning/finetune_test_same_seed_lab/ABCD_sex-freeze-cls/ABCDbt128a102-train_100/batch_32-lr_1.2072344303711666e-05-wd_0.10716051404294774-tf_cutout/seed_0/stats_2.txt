"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.21286416053772, "training_acc": 53.0, "val_loss": 17.35021024942398, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.2250018119812, "training_acc": 53.0, "val_loss": 17.35207885503769, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.190682888031, "training_acc": 53.0, "val_loss": 17.350737750530243, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.16811037063599, "training_acc": 53.0, "val_loss": 17.34912097454071, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.20307326316833, "training_acc": 53.0, "val_loss": 17.347587645053864, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.16427230834961, "training_acc": 53.0, "val_loss": 17.34626293182373, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.18851089477539, "training_acc": 53.0, "val_loss": 17.344562709331512, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.15668654441833, "training_acc": 53.0, "val_loss": 17.343802750110626, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.18592977523804, "training_acc": 53.0, "val_loss": 17.342914640903473, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.1689100265503, "training_acc": 53.0, "val_loss": 17.34209656715393, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.18617939949036, "training_acc": 53.0, "val_loss": 17.341487109661102, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.15472507476807, "training_acc": 53.0, "val_loss": 17.34118163585663, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.20405530929565, "training_acc": 53.0, "val_loss": 17.341330647468567, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.18170690536499, "training_acc": 53.0, "val_loss": 17.341506481170654, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.19039225578308, "training_acc": 53.0, "val_loss": 17.34089106321335, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.19562721252441, "training_acc": 53.0, "val_loss": 17.34035015106201, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.19180560112, "training_acc": 53.0, "val_loss": 17.33996570110321, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.20292234420776, "training_acc": 53.0, "val_loss": 17.339591681957245, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.17007756233215, "training_acc": 53.0, "val_loss": 17.339403927326202, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.17696714401245, "training_acc": 53.0, "val_loss": 17.3394575715065, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.1597831249237, "training_acc": 53.0, "val_loss": 17.33967959880829, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.18651676177979, "training_acc": 53.0, "val_loss": 17.340071499347687, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.2085371017456, "training_acc": 53.0, "val_loss": 17.34005957841873, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.15493440628052, "training_acc": 53.0, "val_loss": 17.33977496623993, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.14812111854553, "training_acc": 53.0, "val_loss": 17.33938902616501, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.1876471042633, "training_acc": 53.0, "val_loss": 17.33904480934143, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.18724751472473, "training_acc": 53.0, "val_loss": 17.33889728784561, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.18273758888245, "training_acc": 53.0, "val_loss": 17.338761687278748, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.21731448173523, "training_acc": 53.0, "val_loss": 17.338727414608, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.21453881263733, "training_acc": 53.0, "val_loss": 17.338746786117554, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.23241519927979, "training_acc": 53.0, "val_loss": 17.33889728784561, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.19278383255005, "training_acc": 53.0, "val_loss": 17.33909547328949, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.18646764755249, "training_acc": 53.0, "val_loss": 17.339347302913666, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.18175625801086, "training_acc": 53.0, "val_loss": 17.339740693569183, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.227530002594, "training_acc": 53.0, "val_loss": 17.340606451034546, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.1780309677124, "training_acc": 53.0, "val_loss": 17.341943085193634, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.19370317459106, "training_acc": 53.0, "val_loss": 17.343078553676605, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.216543674469, "training_acc": 53.0, "val_loss": 17.344264686107635, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.17989706993103, "training_acc": 53.0, "val_loss": 17.34422594308853, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.20284676551819, "training_acc": 53.0, "val_loss": 17.343176901340485, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.1607129573822, "training_acc": 53.0, "val_loss": 17.34268367290497, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.18263053894043, "training_acc": 53.0, "val_loss": 17.342785000801086, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.156653881073, "training_acc": 53.0, "val_loss": 17.3433780670166, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.21885895729065, "training_acc": 53.0, "val_loss": 17.344006896018982, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.1550772190094, "training_acc": 53.0, "val_loss": 17.34386831521988, "val_acc": 52.0}
{"epoch": 45, "training_loss": 69.1573212146759, "training_acc": 53.0, "val_loss": 17.344027757644653, "val_acc": 52.0}
{"epoch": 46, "training_loss": 69.17514967918396, "training_acc": 53.0, "val_loss": 17.34445095062256, "val_acc": 52.0}
{"epoch": 47, "training_loss": 69.17339491844177, "training_acc": 53.0, "val_loss": 17.34479069709778, "val_acc": 52.0}
