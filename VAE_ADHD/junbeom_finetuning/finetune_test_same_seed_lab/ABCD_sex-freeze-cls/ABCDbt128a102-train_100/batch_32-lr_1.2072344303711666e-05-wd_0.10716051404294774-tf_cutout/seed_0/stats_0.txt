"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.2574348449707, "training_acc": 52.0, "val_loss": 17.23553091287613, "val_acc": 56.0}
{"epoch": 1, "training_loss": 69.28498125076294, "training_acc": 52.0, "val_loss": 17.230668663978577, "val_acc": 56.0}
{"epoch": 2, "training_loss": 69.27404451370239, "training_acc": 52.0, "val_loss": 17.22971647977829, "val_acc": 56.0}
{"epoch": 3, "training_loss": 69.22079801559448, "training_acc": 52.0, "val_loss": 17.22482293844223, "val_acc": 56.0}
{"epoch": 4, "training_loss": 69.2828016281128, "training_acc": 52.0, "val_loss": 17.225466668605804, "val_acc": 56.0}
{"epoch": 5, "training_loss": 69.23025274276733, "training_acc": 52.0, "val_loss": 17.22230613231659, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.2324571609497, "training_acc": 52.0, "val_loss": 17.22080260515213, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.26440238952637, "training_acc": 52.0, "val_loss": 17.220225930213928, "val_acc": 56.0}
{"epoch": 8, "training_loss": 69.22191596031189, "training_acc": 52.0, "val_loss": 17.218486964702606, "val_acc": 56.0}
{"epoch": 9, "training_loss": 69.28411149978638, "training_acc": 52.0, "val_loss": 17.220456898212433, "val_acc": 56.0}
{"epoch": 10, "training_loss": 69.25662183761597, "training_acc": 52.0, "val_loss": 17.2255277633667, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.23203182220459, "training_acc": 52.0, "val_loss": 17.225241661071777, "val_acc": 56.0}
{"epoch": 12, "training_loss": 69.25447630882263, "training_acc": 52.0, "val_loss": 17.220191657543182, "val_acc": 56.0}
{"epoch": 13, "training_loss": 69.22462320327759, "training_acc": 52.0, "val_loss": 17.217226326465607, "val_acc": 56.0}
{"epoch": 14, "training_loss": 69.26352453231812, "training_acc": 52.0, "val_loss": 17.21303164958954, "val_acc": 56.0}
{"epoch": 15, "training_loss": 69.23305749893188, "training_acc": 52.0, "val_loss": 17.20942109823227, "val_acc": 56.0}
{"epoch": 16, "training_loss": 69.29487800598145, "training_acc": 52.0, "val_loss": 17.203877866268158, "val_acc": 56.0}
{"epoch": 17, "training_loss": 69.2412314414978, "training_acc": 52.0, "val_loss": 17.20331311225891, "val_acc": 56.0}
{"epoch": 18, "training_loss": 69.22275733947754, "training_acc": 52.0, "val_loss": 17.205539345741272, "val_acc": 56.0}
{"epoch": 19, "training_loss": 69.2228946685791, "training_acc": 52.0, "val_loss": 17.20869094133377, "val_acc": 56.0}
{"epoch": 20, "training_loss": 69.26206064224243, "training_acc": 52.0, "val_loss": 17.2093003988266, "val_acc": 56.0}
{"epoch": 21, "training_loss": 69.2644681930542, "training_acc": 52.0, "val_loss": 17.209981381893158, "val_acc": 56.0}
{"epoch": 22, "training_loss": 69.29008913040161, "training_acc": 52.0, "val_loss": 17.210613191127777, "val_acc": 56.0}
{"epoch": 23, "training_loss": 69.23980355262756, "training_acc": 52.0, "val_loss": 17.2121524810791, "val_acc": 56.0}
{"epoch": 24, "training_loss": 69.2435212135315, "training_acc": 52.0, "val_loss": 17.20995306968689, "val_acc": 56.0}
{"epoch": 25, "training_loss": 69.23110437393188, "training_acc": 52.0, "val_loss": 17.210936546325684, "val_acc": 56.0}
{"epoch": 26, "training_loss": 69.23671317100525, "training_acc": 52.0, "val_loss": 17.211070656776428, "val_acc": 56.0}
{"epoch": 27, "training_loss": 69.22904133796692, "training_acc": 52.0, "val_loss": 17.211997509002686, "val_acc": 56.0}
{"epoch": 28, "training_loss": 69.24790573120117, "training_acc": 52.0, "val_loss": 17.21242517232895, "val_acc": 56.0}
{"epoch": 29, "training_loss": 69.23801374435425, "training_acc": 52.0, "val_loss": 17.20956563949585, "val_acc": 56.0}
{"epoch": 30, "training_loss": 69.2806077003479, "training_acc": 52.0, "val_loss": 17.203207314014435, "val_acc": 56.0}
{"epoch": 31, "training_loss": 69.28181743621826, "training_acc": 52.0, "val_loss": 17.19752848148346, "val_acc": 56.0}
{"epoch": 32, "training_loss": 69.26812982559204, "training_acc": 52.0, "val_loss": 17.190003395080566, "val_acc": 56.0}
{"epoch": 33, "training_loss": 69.2442479133606, "training_acc": 52.0, "val_loss": 17.185047268867493, "val_acc": 56.0}
{"epoch": 34, "training_loss": 69.25600910186768, "training_acc": 52.0, "val_loss": 17.183753848075867, "val_acc": 56.0}
{"epoch": 35, "training_loss": 69.28320932388306, "training_acc": 52.0, "val_loss": 17.184720933437347, "val_acc": 56.0}
{"epoch": 36, "training_loss": 69.28567457199097, "training_acc": 52.0, "val_loss": 17.183776199817657, "val_acc": 56.0}
{"epoch": 37, "training_loss": 69.23891162872314, "training_acc": 52.0, "val_loss": 17.182379961013794, "val_acc": 56.0}
{"epoch": 38, "training_loss": 69.26147556304932, "training_acc": 52.0, "val_loss": 17.182528972625732, "val_acc": 56.0}
{"epoch": 39, "training_loss": 69.30723142623901, "training_acc": 52.0, "val_loss": 17.184428870677948, "val_acc": 56.0}
{"epoch": 40, "training_loss": 69.27642917633057, "training_acc": 52.0, "val_loss": 17.18762516975403, "val_acc": 56.0}
{"epoch": 41, "training_loss": 69.22890615463257, "training_acc": 52.0, "val_loss": 17.189019918441772, "val_acc": 56.0}
{"epoch": 42, "training_loss": 69.25998520851135, "training_acc": 52.0, "val_loss": 17.19149798154831, "val_acc": 56.0}
{"epoch": 43, "training_loss": 69.22102665901184, "training_acc": 52.0, "val_loss": 17.194117605686188, "val_acc": 56.0}
{"epoch": 44, "training_loss": 69.23218655586243, "training_acc": 52.0, "val_loss": 17.198556661605835, "val_acc": 56.0}
{"epoch": 45, "training_loss": 69.26087355613708, "training_acc": 52.0, "val_loss": 17.20149517059326, "val_acc": 56.0}
{"epoch": 46, "training_loss": 69.24975395202637, "training_acc": 52.0, "val_loss": 17.203333973884583, "val_acc": 56.0}
{"epoch": 47, "training_loss": 69.23381543159485, "training_acc": 52.0, "val_loss": 17.20324605703354, "val_acc": 56.0}
{"epoch": 48, "training_loss": 69.2554931640625, "training_acc": 52.0, "val_loss": 17.20377951860428, "val_acc": 56.0}
{"epoch": 49, "training_loss": 69.23045754432678, "training_acc": 52.0, "val_loss": 17.20675230026245, "val_acc": 56.0}
{"epoch": 50, "training_loss": 69.2716851234436, "training_acc": 52.0, "val_loss": 17.21182018518448, "val_acc": 56.0}
{"epoch": 51, "training_loss": 69.2801866531372, "training_acc": 52.0, "val_loss": 17.218105494976044, "val_acc": 56.0}
{"epoch": 52, "training_loss": 69.26228284835815, "training_acc": 52.0, "val_loss": 17.22467392683029, "val_acc": 56.0}
{"epoch": 53, "training_loss": 69.25692248344421, "training_acc": 52.0, "val_loss": 17.229117453098297, "val_acc": 56.0}
{"epoch": 54, "training_loss": 69.24261474609375, "training_acc": 52.0, "val_loss": 17.23395138978958, "val_acc": 56.0}
{"epoch": 55, "training_loss": 69.24309778213501, "training_acc": 52.0, "val_loss": 17.242249846458435, "val_acc": 56.0}
{"epoch": 56, "training_loss": 69.2678861618042, "training_acc": 52.0, "val_loss": 17.2530859708786, "val_acc": 56.0}
