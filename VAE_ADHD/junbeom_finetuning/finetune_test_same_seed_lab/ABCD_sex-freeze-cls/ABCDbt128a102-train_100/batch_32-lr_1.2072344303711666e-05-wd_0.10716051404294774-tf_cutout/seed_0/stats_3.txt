"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.18212914466858, "training_acc": 53.0, "val_loss": 17.297038435935974, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.16509532928467, "training_acc": 53.0, "val_loss": 17.297950387001038, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.14508295059204, "training_acc": 53.0, "val_loss": 17.296825349330902, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.12713122367859, "training_acc": 53.0, "val_loss": 17.295770347118378, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.14292240142822, "training_acc": 53.0, "val_loss": 17.29491353034973, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.11704683303833, "training_acc": 53.0, "val_loss": 17.293456196784973, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.11020278930664, "training_acc": 53.0, "val_loss": 17.291921377182007, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.12112665176392, "training_acc": 53.0, "val_loss": 17.291048169136047, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.12360763549805, "training_acc": 53.0, "val_loss": 17.290616035461426, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.05914115905762, "training_acc": 53.0, "val_loss": 17.29038655757904, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.09013772010803, "training_acc": 53.0, "val_loss": 17.29024052619934, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.08788537979126, "training_acc": 53.0, "val_loss": 17.290136218070984, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.08892393112183, "training_acc": 53.0, "val_loss": 17.290234565734863, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.08254432678223, "training_acc": 53.0, "val_loss": 17.290745675563812, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.08793997764587, "training_acc": 53.0, "val_loss": 17.29191541671753, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.06073212623596, "training_acc": 53.0, "val_loss": 17.29266196489334, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.04287958145142, "training_acc": 53.0, "val_loss": 17.29297786951065, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.01441669464111, "training_acc": 53.0, "val_loss": 17.293787002563477, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.08729887008667, "training_acc": 53.0, "val_loss": 17.294490337371826, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.0829086303711, "training_acc": 53.0, "val_loss": 17.293985188007355, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.06024551391602, "training_acc": 53.0, "val_loss": 17.293550074100494, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.03077745437622, "training_acc": 53.0, "val_loss": 17.293360829353333, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.04704642295837, "training_acc": 53.0, "val_loss": 17.293667793273926, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.07225251197815, "training_acc": 53.0, "val_loss": 17.293769121170044, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.06310081481934, "training_acc": 53.0, "val_loss": 17.29404777288437, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.04588031768799, "training_acc": 53.0, "val_loss": 17.29540228843689, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.12477540969849, "training_acc": 53.0, "val_loss": 17.297519743442535, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.09356451034546, "training_acc": 53.0, "val_loss": 17.298950254917145, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.06939792633057, "training_acc": 53.0, "val_loss": 17.2993466258049, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.09538125991821, "training_acc": 53.0, "val_loss": 17.29958951473236, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.10313272476196, "training_acc": 53.0, "val_loss": 17.299754917621613, "val_acc": 52.0}
