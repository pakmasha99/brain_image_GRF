"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.192209482193, "training_acc": 53.0, "val_loss": 17.285968363285065, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.16044425964355, "training_acc": 53.0, "val_loss": 17.285341024398804, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.10657501220703, "training_acc": 53.0, "val_loss": 17.28403866291046, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.1859610080719, "training_acc": 53.0, "val_loss": 17.282746732234955, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.14402174949646, "training_acc": 53.0, "val_loss": 17.28198528289795, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.16162109375, "training_acc": 53.0, "val_loss": 17.28135794401169, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.1715579032898, "training_acc": 53.0, "val_loss": 17.281116545200348, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.15196371078491, "training_acc": 53.0, "val_loss": 17.280997335910797, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.21386098861694, "training_acc": 53.0, "val_loss": 17.28109121322632, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.19146156311035, "training_acc": 53.0, "val_loss": 17.28125363588333, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.15934801101685, "training_acc": 53.0, "val_loss": 17.28132665157318, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.18426251411438, "training_acc": 53.0, "val_loss": 17.281684279441833, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.16604471206665, "training_acc": 53.0, "val_loss": 17.28205233812332, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.15022706985474, "training_acc": 53.0, "val_loss": 17.281703650951385, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.16465878486633, "training_acc": 53.0, "val_loss": 17.281334102153778, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.16257619857788, "training_acc": 53.0, "val_loss": 17.281092703342438, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.17716455459595, "training_acc": 53.0, "val_loss": 17.28098839521408, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.18070030212402, "training_acc": 53.0, "val_loss": 17.281056940555573, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.21355843544006, "training_acc": 53.0, "val_loss": 17.281153798103333, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.14848256111145, "training_acc": 53.0, "val_loss": 17.281360924243927, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.2154529094696, "training_acc": 53.0, "val_loss": 17.28164851665497, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.22783637046814, "training_acc": 53.0, "val_loss": 17.281898856163025, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.20313453674316, "training_acc": 53.0, "val_loss": 17.28195548057556, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.20656442642212, "training_acc": 53.0, "val_loss": 17.28166937828064, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.16492176055908, "training_acc": 53.0, "val_loss": 17.28142499923706, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.20750665664673, "training_acc": 53.0, "val_loss": 17.281682789325714, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.18698811531067, "training_acc": 53.0, "val_loss": 17.281995713710785, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.20062303543091, "training_acc": 53.0, "val_loss": 17.282256484031677, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.22154641151428, "training_acc": 53.0, "val_loss": 17.28261709213257, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.20157432556152, "training_acc": 53.0, "val_loss": 17.28273779153824, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.20132303237915, "training_acc": 53.0, "val_loss": 17.28239804506302, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.18704104423523, "training_acc": 53.0, "val_loss": 17.281875014305115, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.19212579727173, "training_acc": 53.0, "val_loss": 17.28137582540512, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.19441795349121, "training_acc": 53.0, "val_loss": 17.281295359134674, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.16184306144714, "training_acc": 53.0, "val_loss": 17.281408607959747, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.14436459541321, "training_acc": 53.0, "val_loss": 17.28169173002243, "val_acc": 52.0}
