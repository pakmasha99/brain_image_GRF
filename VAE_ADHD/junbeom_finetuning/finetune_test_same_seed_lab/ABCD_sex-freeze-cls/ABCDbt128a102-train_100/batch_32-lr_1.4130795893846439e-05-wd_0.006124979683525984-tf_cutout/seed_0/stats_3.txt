"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.50397825241089, "training_acc": 47.0, "val_loss": 17.3679381608963, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.43381237983704, "training_acc": 49.0, "val_loss": 17.35713928937912, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.4020004272461, "training_acc": 47.0, "val_loss": 17.350302636623383, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.38021087646484, "training_acc": 44.0, "val_loss": 17.34684556722641, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.3113214969635, "training_acc": 47.0, "val_loss": 17.342562973499298, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.28504920005798, "training_acc": 54.0, "val_loss": 17.339767515659332, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.26201391220093, "training_acc": 53.0, "val_loss": 17.336812615394592, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.29589414596558, "training_acc": 52.0, "val_loss": 17.333674430847168, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.2022635936737, "training_acc": 53.0, "val_loss": 17.33221411705017, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.24228525161743, "training_acc": 53.0, "val_loss": 17.329633235931396, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.24771189689636, "training_acc": 52.0, "val_loss": 17.32722818851471, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.2239363193512, "training_acc": 53.0, "val_loss": 17.326614260673523, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.18847370147705, "training_acc": 53.0, "val_loss": 17.3267662525177, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.22584104537964, "training_acc": 54.0, "val_loss": 17.327575385570526, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.20519304275513, "training_acc": 53.0, "val_loss": 17.327335476875305, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.23161554336548, "training_acc": 53.0, "val_loss": 17.326314747333527, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.207266330719, "training_acc": 53.0, "val_loss": 17.32560247182846, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.19286108016968, "training_acc": 53.0, "val_loss": 17.324988543987274, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.18797206878662, "training_acc": 53.0, "val_loss": 17.324188351631165, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.22712230682373, "training_acc": 53.0, "val_loss": 17.322532832622528, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.1386365890503, "training_acc": 53.0, "val_loss": 17.32175052165985, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.15745067596436, "training_acc": 53.0, "val_loss": 17.32170879840851, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.14849758148193, "training_acc": 53.0, "val_loss": 17.321766912937164, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.14243936538696, "training_acc": 53.0, "val_loss": 17.321637272834778, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.23209142684937, "training_acc": 53.0, "val_loss": 17.321771383285522, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.18435001373291, "training_acc": 53.0, "val_loss": 17.321357131004333, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.1474678516388, "training_acc": 53.0, "val_loss": 17.321033775806427, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.20586776733398, "training_acc": 53.0, "val_loss": 17.320798337459564, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.15536546707153, "training_acc": 53.0, "val_loss": 17.320334911346436, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.14717388153076, "training_acc": 53.0, "val_loss": 17.319998145103455, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.15195155143738, "training_acc": 53.0, "val_loss": 17.32006072998047, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.1050615310669, "training_acc": 53.0, "val_loss": 17.320232093334198, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.12413787841797, "training_acc": 53.0, "val_loss": 17.320384085178375, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.14555215835571, "training_acc": 53.0, "val_loss": 17.32054054737091, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.08911848068237, "training_acc": 53.0, "val_loss": 17.320987582206726, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.14132022857666, "training_acc": 53.0, "val_loss": 17.321689426898956, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.10569381713867, "training_acc": 53.0, "val_loss": 17.3230841755867, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.16029500961304, "training_acc": 53.0, "val_loss": 17.325472831726074, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.1846113204956, "training_acc": 53.0, "val_loss": 17.326627671718597, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.09911394119263, "training_acc": 53.0, "val_loss": 17.326900362968445, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.14278149604797, "training_acc": 53.0, "val_loss": 17.32800304889679, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.13057589530945, "training_acc": 53.0, "val_loss": 17.328861355781555, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.1229305267334, "training_acc": 53.0, "val_loss": 17.327693104743958, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.20778036117554, "training_acc": 53.0, "val_loss": 17.326678335666656, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.13171696662903, "training_acc": 53.0, "val_loss": 17.32611060142517, "val_acc": 52.0}
{"epoch": 45, "training_loss": 69.12828350067139, "training_acc": 53.0, "val_loss": 17.326362431049347, "val_acc": 52.0}
{"epoch": 46, "training_loss": 69.1578483581543, "training_acc": 53.0, "val_loss": 17.326299846172333, "val_acc": 52.0}
{"epoch": 47, "training_loss": 69.09779763221741, "training_acc": 53.0, "val_loss": 17.326506972312927, "val_acc": 52.0}
{"epoch": 48, "training_loss": 69.14607882499695, "training_acc": 53.0, "val_loss": 17.32834428548813, "val_acc": 52.0}
