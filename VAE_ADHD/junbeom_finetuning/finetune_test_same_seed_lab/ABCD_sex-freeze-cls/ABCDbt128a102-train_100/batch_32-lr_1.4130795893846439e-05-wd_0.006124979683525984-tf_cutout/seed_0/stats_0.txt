"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.25826048851013, "training_acc": 52.0, "val_loss": 17.235714197158813, "val_acc": 56.0}
{"epoch": 1, "training_loss": 69.28554248809814, "training_acc": 52.0, "val_loss": 17.229996621608734, "val_acc": 56.0}
{"epoch": 2, "training_loss": 69.27479577064514, "training_acc": 52.0, "val_loss": 17.228898406028748, "val_acc": 56.0}
{"epoch": 3, "training_loss": 69.22117161750793, "training_acc": 52.0, "val_loss": 17.223237454891205, "val_acc": 56.0}
{"epoch": 4, "training_loss": 69.28241229057312, "training_acc": 52.0, "val_loss": 17.224034667015076, "val_acc": 56.0}
{"epoch": 5, "training_loss": 69.23005270957947, "training_acc": 52.0, "val_loss": 17.220428586006165, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.23180413246155, "training_acc": 52.0, "val_loss": 17.218762636184692, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.26387071609497, "training_acc": 52.0, "val_loss": 17.21819043159485, "val_acc": 56.0}
{"epoch": 8, "training_loss": 69.22230339050293, "training_acc": 52.0, "val_loss": 17.216286063194275, "val_acc": 56.0}
{"epoch": 9, "training_loss": 69.28412675857544, "training_acc": 52.0, "val_loss": 17.21867024898529, "val_acc": 56.0}
{"epoch": 10, "training_loss": 69.25600004196167, "training_acc": 52.0, "val_loss": 17.224647104740143, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.23180532455444, "training_acc": 52.0, "val_loss": 17.224380373954773, "val_acc": 56.0}
{"epoch": 12, "training_loss": 69.25544214248657, "training_acc": 52.0, "val_loss": 17.218564450740814, "val_acc": 56.0}
{"epoch": 13, "training_loss": 69.2236397266388, "training_acc": 52.0, "val_loss": 17.21518635749817, "val_acc": 56.0}
{"epoch": 14, "training_loss": 69.26293325424194, "training_acc": 52.0, "val_loss": 17.2104150056839, "val_acc": 56.0}
{"epoch": 15, "training_loss": 69.23244261741638, "training_acc": 52.0, "val_loss": 17.2063410282135, "val_acc": 56.0}
{"epoch": 16, "training_loss": 69.29635953903198, "training_acc": 52.0, "val_loss": 17.20009446144104, "val_acc": 56.0}
{"epoch": 17, "training_loss": 69.2418532371521, "training_acc": 52.0, "val_loss": 17.19955801963806, "val_acc": 56.0}
{"epoch": 18, "training_loss": 69.22311973571777, "training_acc": 52.0, "val_loss": 17.202216386795044, "val_acc": 56.0}
{"epoch": 19, "training_loss": 69.22331142425537, "training_acc": 52.0, "val_loss": 17.20595955848694, "val_acc": 56.0}
{"epoch": 20, "training_loss": 69.26208424568176, "training_acc": 52.0, "val_loss": 17.20678359270096, "val_acc": 56.0}
{"epoch": 21, "training_loss": 69.26445055007935, "training_acc": 52.0, "val_loss": 17.207683622837067, "val_acc": 56.0}
{"epoch": 22, "training_loss": 69.2898440361023, "training_acc": 52.0, "val_loss": 17.20852255821228, "val_acc": 56.0}
{"epoch": 23, "training_loss": 69.23937845230103, "training_acc": 52.0, "val_loss": 17.210407555103302, "val_acc": 56.0}
{"epoch": 24, "training_loss": 69.24389624595642, "training_acc": 52.0, "val_loss": 17.207950353622437, "val_acc": 56.0}
{"epoch": 25, "training_loss": 69.23077535629272, "training_acc": 52.0, "val_loss": 17.209170758724213, "val_acc": 56.0}
{"epoch": 26, "training_loss": 69.23651027679443, "training_acc": 52.0, "val_loss": 17.209407687187195, "val_acc": 56.0}
{"epoch": 27, "training_loss": 69.22852277755737, "training_acc": 52.0, "val_loss": 17.21055805683136, "val_acc": 56.0}
{"epoch": 28, "training_loss": 69.24766421318054, "training_acc": 52.0, "val_loss": 17.211122810840607, "val_acc": 56.0}
{"epoch": 29, "training_loss": 69.23754262924194, "training_acc": 52.0, "val_loss": 17.20787137746811, "val_acc": 56.0}
{"epoch": 30, "training_loss": 69.2820634841919, "training_acc": 52.0, "val_loss": 17.20059961080551, "val_acc": 56.0}
{"epoch": 31, "training_loss": 69.2807264328003, "training_acc": 52.0, "val_loss": 17.194172739982605, "val_acc": 56.0}
{"epoch": 32, "training_loss": 69.27014255523682, "training_acc": 52.0, "val_loss": 17.18572825193405, "val_acc": 56.0}
{"epoch": 33, "training_loss": 69.246652841568, "training_acc": 52.0, "val_loss": 17.180272936820984, "val_acc": 56.0}
{"epoch": 34, "training_loss": 69.26101350784302, "training_acc": 52.0, "val_loss": 17.178958654403687, "val_acc": 56.0}
{"epoch": 35, "training_loss": 69.28766584396362, "training_acc": 52.0, "val_loss": 17.18018352985382, "val_acc": 56.0}
{"epoch": 36, "training_loss": 69.28973340988159, "training_acc": 52.0, "val_loss": 17.179283499717712, "val_acc": 56.0}
{"epoch": 37, "training_loss": 69.24340438842773, "training_acc": 52.0, "val_loss": 17.177878320217133, "val_acc": 56.0}
{"epoch": 38, "training_loss": 69.26637411117554, "training_acc": 52.0, "val_loss": 17.178207635879517, "val_acc": 56.0}
{"epoch": 39, "training_loss": 69.31097173690796, "training_acc": 52.0, "val_loss": 17.18050390481949, "val_acc": 56.0}
{"epoch": 40, "training_loss": 69.28082513809204, "training_acc": 52.0, "val_loss": 17.18427836894989, "val_acc": 56.0}
{"epoch": 41, "training_loss": 69.23102450370789, "training_acc": 52.0, "val_loss": 17.186015844345093, "val_acc": 56.0}
{"epoch": 42, "training_loss": 69.26062679290771, "training_acc": 52.0, "val_loss": 17.188985645771027, "val_acc": 56.0}
{"epoch": 43, "training_loss": 69.22132158279419, "training_acc": 52.0, "val_loss": 17.19212532043457, "val_acc": 56.0}
{"epoch": 44, "training_loss": 69.23318767547607, "training_acc": 52.0, "val_loss": 17.197370529174805, "val_acc": 56.0}
{"epoch": 45, "training_loss": 69.26000022888184, "training_acc": 52.0, "val_loss": 17.200881242752075, "val_acc": 56.0}
{"epoch": 46, "training_loss": 69.24920129776001, "training_acc": 52.0, "val_loss": 17.20310002565384, "val_acc": 56.0}
{"epoch": 47, "training_loss": 69.23325514793396, "training_acc": 52.0, "val_loss": 17.20304638147354, "val_acc": 56.0}
{"epoch": 48, "training_loss": 69.2548246383667, "training_acc": 52.0, "val_loss": 17.20370501279831, "val_acc": 56.0}
{"epoch": 49, "training_loss": 69.22920346260071, "training_acc": 52.0, "val_loss": 17.207209765911102, "val_acc": 56.0}
{"epoch": 50, "training_loss": 69.27057123184204, "training_acc": 52.0, "val_loss": 17.213188111782074, "val_acc": 56.0}
{"epoch": 51, "training_loss": 69.27909183502197, "training_acc": 52.0, "val_loss": 17.220616340637207, "val_acc": 56.0}
{"epoch": 52, "training_loss": 69.26228284835815, "training_acc": 52.0, "val_loss": 17.228388786315918, "val_acc": 56.0}
{"epoch": 53, "training_loss": 69.25743460655212, "training_acc": 52.0, "val_loss": 17.23361611366272, "val_acc": 56.0}
{"epoch": 54, "training_loss": 69.2443585395813, "training_acc": 52.0, "val_loss": 17.239287495613098, "val_acc": 56.0}
{"epoch": 55, "training_loss": 69.24391722679138, "training_acc": 52.0, "val_loss": 17.24911779165268, "val_acc": 56.0}
{"epoch": 56, "training_loss": 69.27076458930969, "training_acc": 52.0, "val_loss": 17.26202666759491, "val_acc": 56.0}
