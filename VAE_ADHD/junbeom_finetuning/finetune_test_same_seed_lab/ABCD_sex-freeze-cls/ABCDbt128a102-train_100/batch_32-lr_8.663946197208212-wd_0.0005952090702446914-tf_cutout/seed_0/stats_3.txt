"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 84925.49262619019, "training_acc": 55.0, "val_loss": 29516.558837890625, "val_acc": 48.0}
{"epoch": 1, "training_loss": 91017.06201171875, "training_acc": 47.0, "val_loss": 13925.790405273438, "val_acc": 52.0}
{"epoch": 2, "training_loss": 46120.8369140625, "training_acc": 51.0, "val_loss": 8908.814239501953, "val_acc": 48.0}
{"epoch": 3, "training_loss": 28727.928833007812, "training_acc": 45.0, "val_loss": 7865.3594970703125, "val_acc": 52.0}
{"epoch": 4, "training_loss": 22172.889038085938, "training_acc": 55.0, "val_loss": 13702.664184570312, "val_acc": 48.0}
{"epoch": 5, "training_loss": 34236.931579589844, "training_acc": 45.0, "val_loss": 16138.748168945312, "val_acc": 52.0}
{"epoch": 6, "training_loss": 56011.77355957031, "training_acc": 53.0, "val_loss": 7671.991729736328, "val_acc": 48.0}
{"epoch": 7, "training_loss": 50894.82531738281, "training_acc": 47.0, "val_loss": 1978.4292221069336, "val_acc": 48.0}
{"epoch": 8, "training_loss": 32164.720703125, "training_acc": 55.0, "val_loss": 12686.241912841797, "val_acc": 52.0}
{"epoch": 9, "training_loss": 34361.94287109375, "training_acc": 47.0, "val_loss": 10942.851257324219, "val_acc": 48.0}
{"epoch": 10, "training_loss": 28074.65185546875, "training_acc": 47.0, "val_loss": 6587.953948974609, "val_acc": 52.0}
{"epoch": 11, "training_loss": 22522.949462890625, "training_acc": 51.0, "val_loss": 7550.437927246094, "val_acc": 48.0}
{"epoch": 12, "training_loss": 17174.60968017578, "training_acc": 53.0, "val_loss": 1277.3124694824219, "val_acc": 52.0}
{"epoch": 13, "training_loss": 25482.603759765625, "training_acc": 39.0, "val_loss": 3351.321792602539, "val_acc": 52.0}
{"epoch": 14, "training_loss": 17848.35400390625, "training_acc": 53.0, "val_loss": 2877.263069152832, "val_acc": 48.0}
{"epoch": 15, "training_loss": 9368.852355957031, "training_acc": 53.0, "val_loss": 315.93785285949707, "val_acc": 52.0}
{"epoch": 16, "training_loss": 20230.103240966797, "training_acc": 51.0, "val_loss": 1387.8689765930176, "val_acc": 52.0}
{"epoch": 17, "training_loss": 7025.970275878906, "training_acc": 53.0, "val_loss": 5448.396301269531, "val_acc": 48.0}
{"epoch": 18, "training_loss": 17031.97998046875, "training_acc": 51.0, "val_loss": 5508.958053588867, "val_acc": 52.0}
{"epoch": 19, "training_loss": 11567.88219833374, "training_acc": 52.0, "val_loss": 522.1321105957031, "val_acc": 52.0}
{"epoch": 20, "training_loss": 9224.293487548828, "training_acc": 45.0, "val_loss": 4109.282684326172, "val_acc": 52.0}
{"epoch": 21, "training_loss": 12969.90005493164, "training_acc": 55.0, "val_loss": 5316.049575805664, "val_acc": 48.0}
{"epoch": 22, "training_loss": 17407.33428955078, "training_acc": 51.0, "val_loss": 15158.477783203125, "val_acc": 52.0}
{"epoch": 23, "training_loss": 52147.555267333984, "training_acc": 53.0, "val_loss": 8960.025024414062, "val_acc": 48.0}
{"epoch": 24, "training_loss": 58565.979736328125, "training_acc": 47.0, "val_loss": 7101.811218261719, "val_acc": 48.0}
{"epoch": 25, "training_loss": 27568.0947265625, "training_acc": 49.0, "val_loss": 3337.8978729248047, "val_acc": 52.0}
{"epoch": 26, "training_loss": 25808.55712890625, "training_acc": 51.0, "val_loss": 3651.4816284179688, "val_acc": 48.0}
{"epoch": 27, "training_loss": 24237.62060546875, "training_acc": 51.0, "val_loss": 3363.5940551757812, "val_acc": 52.0}
{"epoch": 28, "training_loss": 17400.066650390625, "training_acc": 59.0, "val_loss": 2592.0427322387695, "val_acc": 48.0}
{"epoch": 29, "training_loss": 22947.95782470703, "training_acc": 53.0, "val_loss": 5869.687271118164, "val_acc": 52.0}
{"epoch": 30, "training_loss": 8999.09603881836, "training_acc": 49.0, "val_loss": 752.2699356079102, "val_acc": 52.0}
{"epoch": 31, "training_loss": 12227.364912986755, "training_acc": 50.0, "val_loss": 1685.7629776000977, "val_acc": 52.0}
{"epoch": 32, "training_loss": 3756.7181243896484, "training_acc": 55.0, "val_loss": 432.7970504760742, "val_acc": 48.0}
{"epoch": 33, "training_loss": 14650.847595214844, "training_acc": 59.0, "val_loss": 1308.6959838867188, "val_acc": 52.0}
{"epoch": 34, "training_loss": 25372.830200195312, "training_acc": 45.0, "val_loss": 1298.0701446533203, "val_acc": 52.0}
