"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 91646.14727401733, "training_acc": 43.0, "val_loss": 34950.1708984375, "val_acc": 48.0}
{"epoch": 1, "training_loss": 194083.166015625, "training_acc": 47.0, "val_loss": 34929.25720214844, "val_acc": 48.0}
{"epoch": 2, "training_loss": 72408.2197265625, "training_acc": 53.0, "val_loss": 26269.577026367188, "val_acc": 52.0}
{"epoch": 3, "training_loss": 101221.4912109375, "training_acc": 53.0, "val_loss": 2902.665138244629, "val_acc": 52.0}
{"epoch": 4, "training_loss": 77085.65478515625, "training_acc": 39.0, "val_loss": 28808.450317382812, "val_acc": 48.0}
{"epoch": 5, "training_loss": 80431.90002441406, "training_acc": 45.0, "val_loss": 10579.377746582031, "val_acc": 52.0}
{"epoch": 6, "training_loss": 40082.276763916016, "training_acc": 53.0, "val_loss": 3603.213882446289, "val_acc": 48.0}
{"epoch": 7, "training_loss": 21021.8125, "training_acc": 45.0, "val_loss": 1989.0571594238281, "val_acc": 52.0}
{"epoch": 8, "training_loss": 7611.9185791015625, "training_acc": 45.0, "val_loss": 2933.1605911254883, "val_acc": 52.0}
{"epoch": 9, "training_loss": 11020.39517211914, "training_acc": 45.0, "val_loss": 1350.8687019348145, "val_acc": 52.0}
{"epoch": 10, "training_loss": 5992.9061279296875, "training_acc": 53.0, "val_loss": 5242.293930053711, "val_acc": 52.0}
{"epoch": 11, "training_loss": 13616.344940185547, "training_acc": 51.0, "val_loss": 503.0723571777344, "val_acc": 48.0}
{"epoch": 12, "training_loss": 13171.07534790039, "training_acc": 55.0, "val_loss": 1127.4227142333984, "val_acc": 48.0}
{"epoch": 13, "training_loss": 6296.5797119140625, "training_acc": 53.0, "val_loss": 2806.2129974365234, "val_acc": 52.0}
{"epoch": 14, "training_loss": 5416.717803955078, "training_acc": 51.0, "val_loss": 6548.632049560547, "val_acc": 52.0}
{"epoch": 15, "training_loss": 14910.842666625977, "training_acc": 53.0, "val_loss": 511.15894317626953, "val_acc": 48.0}
{"epoch": 16, "training_loss": 15399.195068359375, "training_acc": 53.0, "val_loss": 1807.9841613769531, "val_acc": 48.0}
{"epoch": 17, "training_loss": 6276.590519054487, "training_acc": 51.0, "val_loss": 775.6646633148193, "val_acc": 48.0}
{"epoch": 18, "training_loss": 6787.1346435546875, "training_acc": 51.0, "val_loss": 837.8496170043945, "val_acc": 48.0}
{"epoch": 19, "training_loss": 9995.056518554688, "training_acc": 63.0, "val_loss": 4926.074600219727, "val_acc": 48.0}
{"epoch": 20, "training_loss": 23453.696258544922, "training_acc": 47.0, "val_loss": 1832.673454284668, "val_acc": 52.0}
{"epoch": 21, "training_loss": 3184.1008911132812, "training_acc": 55.0, "val_loss": 1575.8601188659668, "val_acc": 48.0}
{"epoch": 22, "training_loss": 9457.405487060547, "training_acc": 49.0, "val_loss": 4121.868133544922, "val_acc": 52.0}
{"epoch": 23, "training_loss": 8028.874816894531, "training_acc": 49.0, "val_loss": 2730.6766510009766, "val_acc": 52.0}
{"epoch": 24, "training_loss": 4746.002178192139, "training_acc": 57.0, "val_loss": 4350.382614135742, "val_acc": 48.0}
{"epoch": 25, "training_loss": 15091.835388183594, "training_acc": 43.0, "val_loss": 1935.6788635253906, "val_acc": 48.0}
{"epoch": 26, "training_loss": 7270.68310546875, "training_acc": 49.0, "val_loss": 95.46430706977844, "val_acc": 56.0}
{"epoch": 27, "training_loss": 3917.4065551757812, "training_acc": 57.0, "val_loss": 4926.198959350586, "val_acc": 52.0}
{"epoch": 28, "training_loss": 18482.542419433594, "training_acc": 53.0, "val_loss": 4935.060119628906, "val_acc": 48.0}
{"epoch": 29, "training_loss": 12906.912353515625, "training_acc": 57.0, "val_loss": 4891.375732421875, "val_acc": 52.0}
{"epoch": 30, "training_loss": 9270.325317382812, "training_acc": 47.0, "val_loss": 2215.0739669799805, "val_acc": 52.0}
{"epoch": 31, "training_loss": 6467.3427734375, "training_acc": 53.0, "val_loss": 5266.767501831055, "val_acc": 52.0}
{"epoch": 32, "training_loss": 13013.496337890625, "training_acc": 55.0, "val_loss": 107.21169710159302, "val_acc": 52.0}
{"epoch": 33, "training_loss": 4510.278472900391, "training_acc": 60.0, "val_loss": 3588.1996154785156, "val_acc": 48.0}
{"epoch": 34, "training_loss": 9735.231925964355, "training_acc": 54.0, "val_loss": 589.3666744232178, "val_acc": 48.0}
{"epoch": 35, "training_loss": 8972.048721313477, "training_acc": 53.0, "val_loss": 1860.0261688232422, "val_acc": 48.0}
{"epoch": 36, "training_loss": 6482.788635253906, "training_acc": 47.0, "val_loss": 4885.401916503906, "val_acc": 48.0}
{"epoch": 37, "training_loss": 12324.513549804688, "training_acc": 59.0, "val_loss": 9795.948791503906, "val_acc": 52.0}
{"epoch": 38, "training_loss": 23984.655029296875, "training_acc": 51.0, "val_loss": 12395.712280273438, "val_acc": 48.0}
{"epoch": 39, "training_loss": 35495.451904296875, "training_acc": 45.0, "val_loss": 9456.305694580078, "val_acc": 52.0}
{"epoch": 40, "training_loss": 27700.60546875, "training_acc": 51.0, "val_loss": 9071.910858154297, "val_acc": 48.0}
{"epoch": 41, "training_loss": 22005.28091430664, "training_acc": 49.0, "val_loss": 3855.9558868408203, "val_acc": 52.0}
{"epoch": 42, "training_loss": 15993.141235351562, "training_acc": 52.0, "val_loss": 5751.400375366211, "val_acc": 48.0}
{"epoch": 43, "training_loss": 21052.08447265625, "training_acc": 49.0, "val_loss": 437.91537284851074, "val_acc": 52.0}
{"epoch": 44, "training_loss": 30344.546447753906, "training_acc": 47.0, "val_loss": 2954.6493530273438, "val_acc": 48.0}
{"epoch": 45, "training_loss": 31150.720703125, "training_acc": 51.0, "val_loss": 13607.347106933594, "val_acc": 52.0}
