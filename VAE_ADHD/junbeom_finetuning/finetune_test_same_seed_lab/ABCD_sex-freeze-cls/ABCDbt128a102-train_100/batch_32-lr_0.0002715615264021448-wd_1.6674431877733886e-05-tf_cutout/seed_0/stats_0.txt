"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.39471769332886, "training_acc": 47.0, "val_loss": 17.184536159038544, "val_acc": 56.0}
{"epoch": 1, "training_loss": 69.42119193077087, "training_acc": 52.0, "val_loss": 17.170599102973938, "val_acc": 56.0}
{"epoch": 2, "training_loss": 69.43530082702637, "training_acc": 52.0, "val_loss": 17.20944046974182, "val_acc": 56.0}
{"epoch": 3, "training_loss": 69.30238366127014, "training_acc": 52.0, "val_loss": 17.163659632205963, "val_acc": 56.0}
{"epoch": 4, "training_loss": 69.32612228393555, "training_acc": 52.0, "val_loss": 17.210425436496735, "val_acc": 56.0}
{"epoch": 5, "training_loss": 69.25224781036377, "training_acc": 52.0, "val_loss": 17.184196412563324, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.22950839996338, "training_acc": 52.0, "val_loss": 17.189347743988037, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.2846269607544, "training_acc": 52.0, "val_loss": 17.210906744003296, "val_acc": 56.0}
{"epoch": 8, "training_loss": 69.28164649009705, "training_acc": 52.0, "val_loss": 17.20380336046219, "val_acc": 56.0}
{"epoch": 9, "training_loss": 69.33768033981323, "training_acc": 52.0, "val_loss": 17.275534570217133, "val_acc": 56.0}
{"epoch": 10, "training_loss": 69.34093308448792, "training_acc": 53.0, "val_loss": 17.439627647399902, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.5140027999878, "training_acc": 48.0, "val_loss": 17.35178679227829, "val_acc": 56.0}
{"epoch": 12, "training_loss": 69.55769681930542, "training_acc": 42.0, "val_loss": 17.173147201538086, "val_acc": 56.0}
{"epoch": 13, "training_loss": 69.1450617313385, "training_acc": 52.0, "val_loss": 17.134244740009308, "val_acc": 56.0}
{"epoch": 14, "training_loss": 69.49566078186035, "training_acc": 52.0, "val_loss": 17.133115231990814, "val_acc": 56.0}
{"epoch": 15, "training_loss": 69.67421960830688, "training_acc": 52.0, "val_loss": 17.136788368225098, "val_acc": 56.0}
{"epoch": 16, "training_loss": 69.98741912841797, "training_acc": 52.0, "val_loss": 17.14906394481659, "val_acc": 56.0}
{"epoch": 17, "training_loss": 69.75626373291016, "training_acc": 52.0, "val_loss": 17.132005095481873, "val_acc": 56.0}
{"epoch": 18, "training_loss": 69.20177149772644, "training_acc": 52.0, "val_loss": 17.23400503396988, "val_acc": 56.0}
{"epoch": 19, "training_loss": 69.29276251792908, "training_acc": 47.0, "val_loss": 17.43871420621872, "val_acc": 56.0}
{"epoch": 20, "training_loss": 69.57306909561157, "training_acc": 48.0, "val_loss": 17.46818870306015, "val_acc": 56.0}
{"epoch": 21, "training_loss": 69.5635187625885, "training_acc": 48.0, "val_loss": 17.41112321615219, "val_acc": 56.0}
{"epoch": 22, "training_loss": 69.54844045639038, "training_acc": 48.0, "val_loss": 17.32286661863327, "val_acc": 56.0}
{"epoch": 23, "training_loss": 69.307137966156, "training_acc": 53.0, "val_loss": 17.27451980113983, "val_acc": 56.0}
{"epoch": 24, "training_loss": 69.41658544540405, "training_acc": 53.0, "val_loss": 17.170622944831848, "val_acc": 56.0}
{"epoch": 25, "training_loss": 69.23049640655518, "training_acc": 52.0, "val_loss": 17.167922854423523, "val_acc": 56.0}
{"epoch": 26, "training_loss": 69.2906494140625, "training_acc": 52.0, "val_loss": 17.16739684343338, "val_acc": 56.0}
{"epoch": 27, "training_loss": 69.18818616867065, "training_acc": 52.0, "val_loss": 17.190787196159363, "val_acc": 56.0}
{"epoch": 28, "training_loss": 69.23112940788269, "training_acc": 52.0, "val_loss": 17.209939658641815, "val_acc": 56.0}
{"epoch": 29, "training_loss": 69.1890115737915, "training_acc": 52.0, "val_loss": 17.16465950012207, "val_acc": 56.0}
{"epoch": 30, "training_loss": 69.53693151473999, "training_acc": 52.0, "val_loss": 17.12968796491623, "val_acc": 56.0}
{"epoch": 31, "training_loss": 69.60077834129333, "training_acc": 52.0, "val_loss": 17.15746968984604, "val_acc": 56.0}
{"epoch": 32, "training_loss": 70.29463195800781, "training_acc": 52.0, "val_loss": 17.239858210086823, "val_acc": 56.0}
{"epoch": 33, "training_loss": 70.7280945777893, "training_acc": 52.0, "val_loss": 17.22940057516098, "val_acc": 56.0}
{"epoch": 34, "training_loss": 70.2723560333252, "training_acc": 52.0, "val_loss": 17.137038707733154, "val_acc": 56.0}
{"epoch": 35, "training_loss": 69.62243986129761, "training_acc": 52.0, "val_loss": 17.17578023672104, "val_acc": 56.0}
{"epoch": 36, "training_loss": 69.26027154922485, "training_acc": 52.0, "val_loss": 17.250625789165497, "val_acc": 56.0}
{"epoch": 37, "training_loss": 69.23522663116455, "training_acc": 52.0, "val_loss": 17.265094816684723, "val_acc": 56.0}
{"epoch": 38, "training_loss": 69.26653790473938, "training_acc": 52.0, "val_loss": 17.286276817321777, "val_acc": 56.0}
{"epoch": 39, "training_loss": 69.2807605266571, "training_acc": 52.0, "val_loss": 17.338936030864716, "val_acc": 56.0}
{"epoch": 40, "training_loss": 69.489670753479, "training_acc": 48.0, "val_loss": 17.40889549255371, "val_acc": 56.0}
{"epoch": 41, "training_loss": 69.36889553070068, "training_acc": 48.0, "val_loss": 17.350003123283386, "val_acc": 56.0}
{"epoch": 42, "training_loss": 69.34089994430542, "training_acc": 52.0, "val_loss": 17.325565218925476, "val_acc": 56.0}
{"epoch": 43, "training_loss": 69.31115055084229, "training_acc": 48.0, "val_loss": 17.30453222990036, "val_acc": 56.0}
{"epoch": 44, "training_loss": 69.29568719863892, "training_acc": 50.0, "val_loss": 17.352202534675598, "val_acc": 56.0}
{"epoch": 45, "training_loss": 69.33330178260803, "training_acc": 47.0, "val_loss": 17.332419753074646, "val_acc": 56.0}
{"epoch": 46, "training_loss": 69.28002262115479, "training_acc": 54.0, "val_loss": 17.27556884288788, "val_acc": 56.0}
{"epoch": 47, "training_loss": 69.15200567245483, "training_acc": 53.0, "val_loss": 17.1945258975029, "val_acc": 56.0}
{"epoch": 48, "training_loss": 69.23206806182861, "training_acc": 52.0, "val_loss": 17.168666422367096, "val_acc": 56.0}
{"epoch": 49, "training_loss": 69.1380934715271, "training_acc": 52.0, "val_loss": 17.20947176218033, "val_acc": 56.0}
