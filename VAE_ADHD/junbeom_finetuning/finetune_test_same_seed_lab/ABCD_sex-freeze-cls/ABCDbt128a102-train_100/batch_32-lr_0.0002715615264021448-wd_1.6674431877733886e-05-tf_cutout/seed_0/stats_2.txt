"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.46760106086731, "training_acc": 53.0, "val_loss": 17.402207851409912, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.47435331344604, "training_acc": 53.0, "val_loss": 17.389564216136932, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.34591960906982, "training_acc": 53.0, "val_loss": 17.371344566345215, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.23966884613037, "training_acc": 53.0, "val_loss": 17.34325885772705, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.35312342643738, "training_acc": 53.0, "val_loss": 17.355738580226898, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.50655221939087, "training_acc": 45.0, "val_loss": 17.36440658569336, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.31360268592834, "training_acc": 44.0, "val_loss": 17.338939011096954, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.24288511276245, "training_acc": 53.0, "val_loss": 17.344406247138977, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.23696708679199, "training_acc": 53.0, "val_loss": 17.34646111726761, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.4333348274231, "training_acc": 53.0, "val_loss": 17.341512441635132, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.28919100761414, "training_acc": 52.0, "val_loss": 17.338599264621735, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.25245189666748, "training_acc": 53.0, "val_loss": 17.337213456630707, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.19876456260681, "training_acc": 53.0, "val_loss": 17.35568791627884, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.24759936332703, "training_acc": 53.0, "val_loss": 17.385020852088928, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.27401971817017, "training_acc": 53.0, "val_loss": 17.41735190153122, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.4558937549591, "training_acc": 53.0, "val_loss": 17.403100430965424, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.3058021068573, "training_acc": 53.0, "val_loss": 17.338263988494873, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.28233933448792, "training_acc": 51.0, "val_loss": 17.351078987121582, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.34087896347046, "training_acc": 56.0, "val_loss": 17.336459457874298, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.30147695541382, "training_acc": 53.0, "val_loss": 17.340990900993347, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.19419956207275, "training_acc": 53.0, "val_loss": 17.34912097454071, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.21142816543579, "training_acc": 53.0, "val_loss": 17.345382273197174, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.21625185012817, "training_acc": 53.0, "val_loss": 17.348666489124298, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.23677945137024, "training_acc": 53.0, "val_loss": 17.36067682504654, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.23183917999268, "training_acc": 53.0, "val_loss": 17.34776496887207, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.18724513053894, "training_acc": 53.0, "val_loss": 17.35907644033432, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.224196434021, "training_acc": 53.0, "val_loss": 17.37363338470459, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.21553230285645, "training_acc": 53.0, "val_loss": 17.346535623073578, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.1625759601593, "training_acc": 53.0, "val_loss": 17.342527210712433, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.16757583618164, "training_acc": 53.0, "val_loss": 17.367583513259888, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.22416877746582, "training_acc": 53.0, "val_loss": 17.36869215965271, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.31321549415588, "training_acc": 53.0, "val_loss": 17.45806336402893, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.53897857666016, "training_acc": 53.0, "val_loss": 17.60307401418686, "val_acc": 52.0}
{"epoch": 33, "training_loss": 70.0942120552063, "training_acc": 53.0, "val_loss": 17.64593869447708, "val_acc": 52.0}
{"epoch": 34, "training_loss": 70.09185028076172, "training_acc": 53.0, "val_loss": 17.621149122714996, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.95678877830505, "training_acc": 53.0, "val_loss": 17.54273623228073, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.70702743530273, "training_acc": 53.0, "val_loss": 17.431670427322388, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.3135666847229, "training_acc": 53.0, "val_loss": 17.408350110054016, "val_acc": 52.0}
