"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.4911687374115, "training_acc": 47.0, "val_loss": 17.30780601501465, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.19662714004517, "training_acc": 53.0, "val_loss": 17.290237545967102, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.15450429916382, "training_acc": 53.0, "val_loss": 17.286215722560883, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.17660093307495, "training_acc": 53.0, "val_loss": 17.305514216423035, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.53214931488037, "training_acc": 53.0, "val_loss": 17.369933426380157, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.30171012878418, "training_acc": 53.0, "val_loss": 17.347519099712372, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.25591516494751, "training_acc": 53.0, "val_loss": 17.323753237724304, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.17914772033691, "training_acc": 53.0, "val_loss": 17.290866374969482, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.19265151023865, "training_acc": 53.0, "val_loss": 17.286096513271332, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.13206481933594, "training_acc": 53.0, "val_loss": 17.286212742328644, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.29080247879028, "training_acc": 53.0, "val_loss": 17.289337515830994, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.12988209724426, "training_acc": 53.0, "val_loss": 17.28651374578476, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.30773854255676, "training_acc": 53.0, "val_loss": 17.2896608710289, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.20983648300171, "training_acc": 53.0, "val_loss": 17.28930026292801, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.12065553665161, "training_acc": 53.0, "val_loss": 17.29879379272461, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.2654938697815, "training_acc": 53.0, "val_loss": 17.30685532093048, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.30157971382141, "training_acc": 50.0, "val_loss": 17.294633388519287, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.1823959350586, "training_acc": 53.0, "val_loss": 17.287351191043854, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.0437273979187, "training_acc": 53.0, "val_loss": 17.296527326107025, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.08674263954163, "training_acc": 53.0, "val_loss": 17.32126772403717, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.09153437614441, "training_acc": 53.0, "val_loss": 17.373038828372955, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.3464674949646, "training_acc": 53.0, "val_loss": 17.438596487045288, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.51897072792053, "training_acc": 53.0, "val_loss": 17.39918440580368, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.2771544456482, "training_acc": 53.0, "val_loss": 17.3044353723526, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.03699445724487, "training_acc": 53.0, "val_loss": 17.289836704730988, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.20206308364868, "training_acc": 54.0, "val_loss": 17.301559448242188, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.17185640335083, "training_acc": 53.0, "val_loss": 17.29464679956436, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.10518550872803, "training_acc": 53.0, "val_loss": 17.296123504638672, "val_acc": 52.0}
