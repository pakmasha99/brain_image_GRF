"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 92.67023420333862, "training_acc": 47.0, "val_loss": 18.547385931015015, "val_acc": 48.0}
{"epoch": 1, "training_loss": 80.42432403564453, "training_acc": 43.0, "val_loss": 23.298141360282898, "val_acc": 52.0}
{"epoch": 2, "training_loss": 92.7543044090271, "training_acc": 53.0, "val_loss": 20.420771837234497, "val_acc": 52.0}
{"epoch": 3, "training_loss": 76.1415913105011, "training_acc": 53.0, "val_loss": 17.74224042892456, "val_acc": 52.0}
{"epoch": 4, "training_loss": 75.64384865760803, "training_acc": 47.0, "val_loss": 20.78535556793213, "val_acc": 48.0}
{"epoch": 5, "training_loss": 82.84379935264587, "training_acc": 47.0, "val_loss": 18.285834789276123, "val_acc": 52.0}
{"epoch": 6, "training_loss": 71.71599125862122, "training_acc": 47.0, "val_loss": 17.688968777656555, "val_acc": 52.0}
{"epoch": 7, "training_loss": 71.37236857414246, "training_acc": 53.0, "val_loss": 19.453145563602448, "val_acc": 52.0}
{"epoch": 8, "training_loss": 76.60556149482727, "training_acc": 53.0, "val_loss": 18.390384316444397, "val_acc": 52.0}
{"epoch": 9, "training_loss": 71.62129187583923, "training_acc": 53.0, "val_loss": 17.308105528354645, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.35096192359924, "training_acc": 53.0, "val_loss": 18.065857887268066, "val_acc": 52.0}
{"epoch": 11, "training_loss": 73.94544315338135, "training_acc": 47.0, "val_loss": 17.8730770945549, "val_acc": 52.0}
{"epoch": 12, "training_loss": 70.7749981880188, "training_acc": 47.0, "val_loss": 17.406153678894043, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.46629071235657, "training_acc": 53.0, "val_loss": 18.335609138011932, "val_acc": 52.0}
{"epoch": 14, "training_loss": 72.92226505279541, "training_acc": 53.0, "val_loss": 17.99570620059967, "val_acc": 52.0}
{"epoch": 15, "training_loss": 70.28061008453369, "training_acc": 53.0, "val_loss": 17.314307391643524, "val_acc": 52.0}
{"epoch": 16, "training_loss": 68.43816208839417, "training_acc": 63.0, "val_loss": 18.043340742588043, "val_acc": 52.0}
{"epoch": 17, "training_loss": 72.90761351585388, "training_acc": 47.0, "val_loss": 18.124482035636902, "val_acc": 52.0}
{"epoch": 18, "training_loss": 72.13652038574219, "training_acc": 47.0, "val_loss": 17.31126606464386, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.35933256149292, "training_acc": 53.0, "val_loss": 17.882271111011505, "val_acc": 52.0}
{"epoch": 20, "training_loss": 71.27783703804016, "training_acc": 53.0, "val_loss": 17.872846126556396, "val_acc": 52.0}
{"epoch": 21, "training_loss": 70.67662310600281, "training_acc": 53.0, "val_loss": 17.313160002231598, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.76734638214111, "training_acc": 47.0, "val_loss": 17.494671046733856, "val_acc": 52.0}
{"epoch": 23, "training_loss": 70.14366602897644, "training_acc": 47.0, "val_loss": 17.353439331054688, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.10076451301575, "training_acc": 53.0, "val_loss": 17.416587471961975, "val_acc": 52.0}
{"epoch": 25, "training_loss": 70.6693263053894, "training_acc": 53.0, "val_loss": 17.64174997806549, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.7152259349823, "training_acc": 53.0, "val_loss": 17.30804741382599, "val_acc": 52.0}
{"epoch": 27, "training_loss": 70.22618532180786, "training_acc": 45.0, "val_loss": 17.546622455120087, "val_acc": 52.0}
{"epoch": 28, "training_loss": 70.6783344745636, "training_acc": 47.0, "val_loss": 17.32797920703888, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.18661451339722, "training_acc": 58.0, "val_loss": 17.33255386352539, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.14948391914368, "training_acc": 53.0, "val_loss": 17.455628514289856, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.50735425949097, "training_acc": 53.0, "val_loss": 17.51684844493866, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.89343309402466, "training_acc": 53.0, "val_loss": 17.353181540966034, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.17232203483582, "training_acc": 53.0, "val_loss": 17.308570444583893, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.15721321105957, "training_acc": 53.0, "val_loss": 17.315198481082916, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.22147345542908, "training_acc": 53.0, "val_loss": 17.308731377124786, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.14980816841125, "training_acc": 53.0, "val_loss": 17.35805869102478, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.2533962726593, "training_acc": 53.0, "val_loss": 17.35503226518631, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.51852250099182, "training_acc": 53.0, "val_loss": 17.34399050474167, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.21932196617126, "training_acc": 53.0, "val_loss": 17.381244897842407, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.53985500335693, "training_acc": 53.0, "val_loss": 17.32104867696762, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.30115151405334, "training_acc": 53.0, "val_loss": 17.31865108013153, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.45011925697327, "training_acc": 53.0, "val_loss": 17.32666790485382, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.7036201953888, "training_acc": 47.0, "val_loss": 17.460422217845917, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.89259004592896, "training_acc": 47.0, "val_loss": 17.310847342014313, "val_acc": 52.0}
{"epoch": 45, "training_loss": 69.30788850784302, "training_acc": 53.0, "val_loss": 17.48635470867157, "val_acc": 52.0}
