"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 70.47146916389465, "training_acc": 47.0, "val_loss": 17.524412274360657, "val_acc": 52.0}
{"epoch": 1, "training_loss": 70.33432841300964, "training_acc": 47.0, "val_loss": 17.488594353199005, "val_acc": 52.0}
{"epoch": 2, "training_loss": 70.10438966751099, "training_acc": 47.0, "val_loss": 17.4601212143898, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.96378016471863, "training_acc": 47.0, "val_loss": 17.43503361940384, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.89737725257874, "training_acc": 47.0, "val_loss": 17.411337792873383, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.72966384887695, "training_acc": 47.0, "val_loss": 17.392100393772125, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.61553764343262, "training_acc": 47.0, "val_loss": 17.374809086322784, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.5772602558136, "training_acc": 47.0, "val_loss": 17.358487844467163, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.48062109947205, "training_acc": 47.0, "val_loss": 17.344382405281067, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.4516716003418, "training_acc": 47.0, "val_loss": 17.33214110136032, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.38013052940369, "training_acc": 46.0, "val_loss": 17.32250154018402, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.29296803474426, "training_acc": 55.0, "val_loss": 17.3151895403862, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.26708912849426, "training_acc": 53.0, "val_loss": 17.30974167585373, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.19624209403992, "training_acc": 53.0, "val_loss": 17.30611026287079, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.16763019561768, "training_acc": 53.0, "val_loss": 17.303870618343353, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.19883275032043, "training_acc": 53.0, "val_loss": 17.30276495218277, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.15066647529602, "training_acc": 53.0, "val_loss": 17.302775382995605, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.16162991523743, "training_acc": 53.0, "val_loss": 17.303667962551117, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.16271710395813, "training_acc": 53.0, "val_loss": 17.304931581020355, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.14916396141052, "training_acc": 53.0, "val_loss": 17.305858433246613, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.15300035476685, "training_acc": 53.0, "val_loss": 17.306876182556152, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.14147591590881, "training_acc": 53.0, "val_loss": 17.307591438293457, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.1517105102539, "training_acc": 53.0, "val_loss": 17.308957874774933, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.17396116256714, "training_acc": 53.0, "val_loss": 17.31015294790268, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.13048148155212, "training_acc": 53.0, "val_loss": 17.311252653598785, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.1575574874878, "training_acc": 53.0, "val_loss": 17.312638461589813, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.13273620605469, "training_acc": 53.0, "val_loss": 17.31318235397339, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.14626622200012, "training_acc": 53.0, "val_loss": 17.313534021377563, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.14031887054443, "training_acc": 53.0, "val_loss": 17.31308102607727, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.161128282547, "training_acc": 53.0, "val_loss": 17.31325089931488, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.14448237419128, "training_acc": 53.0, "val_loss": 17.31361299753189, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.14503788948059, "training_acc": 53.0, "val_loss": 17.31356382369995, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.14661359786987, "training_acc": 53.0, "val_loss": 17.31332540512085, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.13943290710449, "training_acc": 53.0, "val_loss": 17.31351763010025, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.14462280273438, "training_acc": 53.0, "val_loss": 17.31369197368622, "val_acc": 52.0}
