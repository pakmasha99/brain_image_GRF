"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.26534748077393, "training_acc": 53.0, "val_loss": 17.317982017993927, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.25262188911438, "training_acc": 53.0, "val_loss": 17.31589436531067, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.23175382614136, "training_acc": 53.0, "val_loss": 17.31414496898651, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.20757007598877, "training_acc": 53.0, "val_loss": 17.312927544116974, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.24043202400208, "training_acc": 53.0, "val_loss": 17.311736941337585, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.185467004776, "training_acc": 53.0, "val_loss": 17.31088161468506, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.17542695999146, "training_acc": 53.0, "val_loss": 17.310096323490143, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.19151616096497, "training_acc": 53.0, "val_loss": 17.30964630842209, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.1731264591217, "training_acc": 53.0, "val_loss": 17.309194803237915, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.1723120212555, "training_acc": 53.0, "val_loss": 17.30884313583374, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.17106699943542, "training_acc": 53.0, "val_loss": 17.30867028236389, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.16947913169861, "training_acc": 53.0, "val_loss": 17.308521270751953, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.1725971698761, "training_acc": 53.0, "val_loss": 17.30842888355255, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.14899945259094, "training_acc": 53.0, "val_loss": 17.30835735797882, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.18099164962769, "training_acc": 53.0, "val_loss": 17.308318614959717, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.15607953071594, "training_acc": 53.0, "val_loss": 17.308251559734344, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.14197659492493, "training_acc": 53.0, "val_loss": 17.308174073696136, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.18121266365051, "training_acc": 53.0, "val_loss": 17.30809211730957, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.17317914962769, "training_acc": 53.0, "val_loss": 17.308039963245392, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.1498191356659, "training_acc": 53.0, "val_loss": 17.30799823999405, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.15950345993042, "training_acc": 53.0, "val_loss": 17.30796843767166, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.16255259513855, "training_acc": 53.0, "val_loss": 17.307952046394348, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.15892910957336, "training_acc": 53.0, "val_loss": 17.30795055627823, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.13851642608643, "training_acc": 53.0, "val_loss": 17.30796843767166, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.13408708572388, "training_acc": 53.0, "val_loss": 17.308029532432556, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.13207340240479, "training_acc": 53.0, "val_loss": 17.308111488819122, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.14067339897156, "training_acc": 53.0, "val_loss": 17.30816215276718, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.13606858253479, "training_acc": 53.0, "val_loss": 17.308196425437927, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.13157653808594, "training_acc": 53.0, "val_loss": 17.308272421360016, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.12452745437622, "training_acc": 53.0, "val_loss": 17.30838567018509, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.15629935264587, "training_acc": 53.0, "val_loss": 17.30850636959076, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.15310549736023, "training_acc": 53.0, "val_loss": 17.30867028236389, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.1530373096466, "training_acc": 53.0, "val_loss": 17.308799922466278, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.12387990951538, "training_acc": 53.0, "val_loss": 17.308826744556427, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.13464546203613, "training_acc": 53.0, "val_loss": 17.308883368968964, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.16167950630188, "training_acc": 53.0, "val_loss": 17.308953404426575, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.1412603855133, "training_acc": 53.0, "val_loss": 17.308984696865082, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.13460564613342, "training_acc": 53.0, "val_loss": 17.309047281742096, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.17221474647522, "training_acc": 53.0, "val_loss": 17.30916202068329, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.14337992668152, "training_acc": 53.0, "val_loss": 17.30932891368866, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.12294149398804, "training_acc": 53.0, "val_loss": 17.30949580669403, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.1534104347229, "training_acc": 53.0, "val_loss": 17.309628427028656, "val_acc": 52.0}
