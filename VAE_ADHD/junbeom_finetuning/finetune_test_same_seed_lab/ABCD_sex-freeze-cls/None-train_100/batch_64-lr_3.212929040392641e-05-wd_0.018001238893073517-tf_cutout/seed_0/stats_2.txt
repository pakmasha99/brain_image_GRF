"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.3822033405304, "training_acc": 44.0, "val_loss": 17.330121994018555, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.31487822532654, "training_acc": 50.0, "val_loss": 17.323970794677734, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.29457020759583, "training_acc": 52.0, "val_loss": 17.318402230739594, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.24866342544556, "training_acc": 53.0, "val_loss": 17.314381897449493, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.213791847229, "training_acc": 53.0, "val_loss": 17.311376333236694, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.20806956291199, "training_acc": 53.0, "val_loss": 17.309311032295227, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.1587188243866, "training_acc": 53.0, "val_loss": 17.308175563812256, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.14228439331055, "training_acc": 53.0, "val_loss": 17.307618260383606, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.16319394111633, "training_acc": 53.0, "val_loss": 17.307689785957336, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.13379311561584, "training_acc": 53.0, "val_loss": 17.308184504508972, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.12285232543945, "training_acc": 53.0, "val_loss": 17.30908304452896, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.11881947517395, "training_acc": 53.0, "val_loss": 17.310069501399994, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.1029543876648, "training_acc": 53.0, "val_loss": 17.310969531536102, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.14763498306274, "training_acc": 53.0, "val_loss": 17.311854660511017, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.15835642814636, "training_acc": 53.0, "val_loss": 17.311957478523254, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.09268736839294, "training_acc": 53.0, "val_loss": 17.311716079711914, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.14384007453918, "training_acc": 53.0, "val_loss": 17.311488091945648, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.1135482788086, "training_acc": 53.0, "val_loss": 17.311786115169525, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.12074255943298, "training_acc": 53.0, "val_loss": 17.311935126781464, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.13532757759094, "training_acc": 53.0, "val_loss": 17.31140911579132, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.122309923172, "training_acc": 53.0, "val_loss": 17.311526834964752, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.12874674797058, "training_acc": 53.0, "val_loss": 17.312091588974, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.10343527793884, "training_acc": 53.0, "val_loss": 17.312002182006836, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.11892056465149, "training_acc": 53.0, "val_loss": 17.31164902448654, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.13004994392395, "training_acc": 53.0, "val_loss": 17.311282455921173, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.1150369644165, "training_acc": 53.0, "val_loss": 17.31187552213669, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.14763927459717, "training_acc": 53.0, "val_loss": 17.312492430210114, "val_acc": 52.0}
