"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 70.42798733711243, "training_acc": 47.0, "val_loss": 17.495761811733246, "val_acc": 52.0}
{"epoch": 1, "training_loss": 70.21952390670776, "training_acc": 47.0, "val_loss": 17.442822456359863, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.89341402053833, "training_acc": 47.0, "val_loss": 17.405076324939728, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.68488621711731, "training_acc": 47.0, "val_loss": 17.37532764673233, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.62853169441223, "training_acc": 47.0, "val_loss": 17.35002100467682, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.4201648235321, "training_acc": 47.0, "val_loss": 17.332948744297028, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.30183625221252, "training_acc": 48.0, "val_loss": 17.32008457183838, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.28285622596741, "training_acc": 53.0, "val_loss": 17.310328781604767, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.20495438575745, "training_acc": 53.0, "val_loss": 17.304691672325134, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.2139105796814, "training_acc": 53.0, "val_loss": 17.30266660451889, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.17716598510742, "training_acc": 53.0, "val_loss": 17.30380952358246, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.13071823120117, "training_acc": 53.0, "val_loss": 17.30692684650421, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.14586687088013, "training_acc": 53.0, "val_loss": 17.311276495456696, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.11525344848633, "training_acc": 53.0, "val_loss": 17.315714061260223, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.1250729560852, "training_acc": 53.0, "val_loss": 17.320159077644348, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.17715811729431, "training_acc": 53.0, "val_loss": 17.32545793056488, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.16680788993835, "training_acc": 53.0, "val_loss": 17.329223453998566, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.18510508537292, "training_acc": 53.0, "val_loss": 17.33306795358658, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.19453239440918, "training_acc": 53.0, "val_loss": 17.334182560443878, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.19882583618164, "training_acc": 53.0, "val_loss": 17.331115901470184, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.18726301193237, "training_acc": 53.0, "val_loss": 17.328134179115295, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.21084141731262, "training_acc": 53.0, "val_loss": 17.324337363243103, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.17624831199646, "training_acc": 53.0, "val_loss": 17.323271930217743, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.1992244720459, "training_acc": 53.0, "val_loss": 17.321689426898956, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.15950012207031, "training_acc": 53.0, "val_loss": 17.320124804973602, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.16538071632385, "training_acc": 53.0, "val_loss": 17.31959879398346, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.1430995464325, "training_acc": 53.0, "val_loss": 17.317508161067963, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.14566802978516, "training_acc": 53.0, "val_loss": 17.315565049648285, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.15849781036377, "training_acc": 53.0, "val_loss": 17.312750220298767, "val_acc": 52.0}
