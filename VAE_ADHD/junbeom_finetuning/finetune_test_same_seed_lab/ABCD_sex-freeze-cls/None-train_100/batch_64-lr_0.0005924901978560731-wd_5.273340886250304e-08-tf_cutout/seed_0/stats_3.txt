"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 70.27333402633667, "training_acc": 45.0, "val_loss": 17.346099019050598, "val_acc": 52.0}
{"epoch": 1, "training_loss": 68.998774766922, "training_acc": 55.0, "val_loss": 17.372632026672363, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.22298884391785, "training_acc": 53.0, "val_loss": 17.58154034614563, "val_acc": 52.0}
{"epoch": 3, "training_loss": 70.06432461738586, "training_acc": 53.0, "val_loss": 17.583394050598145, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.82232737541199, "training_acc": 53.0, "val_loss": 17.38635152578354, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.09736251831055, "training_acc": 53.0, "val_loss": 17.30901747941971, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.68589496612549, "training_acc": 43.0, "val_loss": 17.366445064544678, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.80012440681458, "training_acc": 47.0, "val_loss": 17.362038791179657, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.35344982147217, "training_acc": 46.0, "val_loss": 17.309795320034027, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.05834341049194, "training_acc": 53.0, "val_loss": 17.345908284187317, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.18600726127625, "training_acc": 53.0, "val_loss": 17.44154691696167, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.52249121665955, "training_acc": 53.0, "val_loss": 17.48926192522049, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.65687489509583, "training_acc": 53.0, "val_loss": 17.448891699314117, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.5209093093872, "training_acc": 53.0, "val_loss": 17.378732562065125, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.22940754890442, "training_acc": 53.0, "val_loss": 17.32895076274872, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.1969301700592, "training_acc": 53.0, "val_loss": 17.30918437242508, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.21082639694214, "training_acc": 53.0, "val_loss": 17.319488525390625, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.38053894042969, "training_acc": 43.0, "val_loss": 17.31944978237152, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.46288704872131, "training_acc": 53.0, "val_loss": 17.309260368347168, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.13064908981323, "training_acc": 53.0, "val_loss": 17.313598096370697, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.12961554527283, "training_acc": 53.0, "val_loss": 17.328758537769318, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.41354656219482, "training_acc": 53.0, "val_loss": 17.34083443880081, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.14490127563477, "training_acc": 53.0, "val_loss": 17.317897081375122, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.09505724906921, "training_acc": 53.0, "val_loss": 17.309007048606873, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.11288571357727, "training_acc": 53.0, "val_loss": 17.31865108013153, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.2335774898529, "training_acc": 53.0, "val_loss": 17.33347326517105, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.39553451538086, "training_acc": 46.0, "val_loss": 17.340683937072754, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.34646654129028, "training_acc": 47.0, "val_loss": 17.31862723827362, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.18883538246155, "training_acc": 53.0, "val_loss": 17.310595512390137, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.25196075439453, "training_acc": 53.0, "val_loss": 17.3406183719635, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.1872832775116, "training_acc": 53.0, "val_loss": 17.357206344604492, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.21442770957947, "training_acc": 53.0, "val_loss": 17.34919548034668, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.18861103057861, "training_acc": 53.0, "val_loss": 17.32800304889679, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.11413240432739, "training_acc": 53.0, "val_loss": 17.31100082397461, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.43844676017761, "training_acc": 53.0, "val_loss": 17.31097251176834, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.1907606124878, "training_acc": 53.0, "val_loss": 17.309190332889557, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.17809414863586, "training_acc": 53.0, "val_loss": 17.32420176267624, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.41504645347595, "training_acc": 53.0, "val_loss": 17.341895401477814, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.14986157417297, "training_acc": 53.0, "val_loss": 17.320407927036285, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.26920485496521, "training_acc": 53.0, "val_loss": 17.309115827083588, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.19706273078918, "training_acc": 53.0, "val_loss": 17.309419810771942, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.18618941307068, "training_acc": 53.0, "val_loss": 17.310190200805664, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.13021469116211, "training_acc": 53.0, "val_loss": 17.31637865304947, "val_acc": 52.0}
