"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 70.98516511917114, "training_acc": 47.0, "val_loss": 17.653587460517883, "val_acc": 52.0}
{"epoch": 1, "training_loss": 70.8222382068634, "training_acc": 47.0, "val_loss": 17.6217183470726, "val_acc": 52.0}
{"epoch": 2, "training_loss": 70.69971823692322, "training_acc": 47.0, "val_loss": 17.591412365436554, "val_acc": 52.0}
{"epoch": 3, "training_loss": 70.55001592636108, "training_acc": 47.0, "val_loss": 17.562974989414215, "val_acc": 52.0}
{"epoch": 4, "training_loss": 70.41561961174011, "training_acc": 47.0, "val_loss": 17.53610521554947, "val_acc": 52.0}
{"epoch": 5, "training_loss": 70.30824589729309, "training_acc": 47.0, "val_loss": 17.51106083393097, "val_acc": 52.0}
{"epoch": 6, "training_loss": 70.19310998916626, "training_acc": 47.0, "val_loss": 17.48785674571991, "val_acc": 52.0}
{"epoch": 7, "training_loss": 70.07167959213257, "training_acc": 47.0, "val_loss": 17.466247081756592, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.99868607521057, "training_acc": 47.0, "val_loss": 17.44695007801056, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.87928318977356, "training_acc": 47.0, "val_loss": 17.4305722117424, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.77740716934204, "training_acc": 47.0, "val_loss": 17.41536855697632, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.7778217792511, "training_acc": 47.0, "val_loss": 17.401470243930817, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.67075824737549, "training_acc": 47.0, "val_loss": 17.389419674873352, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.62852907180786, "training_acc": 47.0, "val_loss": 17.379073798656464, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.58096241950989, "training_acc": 47.0, "val_loss": 17.36949235200882, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.52733993530273, "training_acc": 47.0, "val_loss": 17.360801994800568, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.47673344612122, "training_acc": 47.0, "val_loss": 17.352838814258575, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.40954971313477, "training_acc": 47.0, "val_loss": 17.345203459262848, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.39580178260803, "training_acc": 47.0, "val_loss": 17.33863800764084, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.35980582237244, "training_acc": 47.0, "val_loss": 17.333312332630157, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.3189013004303, "training_acc": 49.0, "val_loss": 17.328988015651703, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.29450988769531, "training_acc": 51.0, "val_loss": 17.32526868581772, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.279700756073, "training_acc": 53.0, "val_loss": 17.321892082691193, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.27302479743958, "training_acc": 53.0, "val_loss": 17.319273948669434, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.22938251495361, "training_acc": 53.0, "val_loss": 17.317187786102295, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.22275900840759, "training_acc": 53.0, "val_loss": 17.315152287483215, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.22356343269348, "training_acc": 53.0, "val_loss": 17.313767969608307, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.18992948532104, "training_acc": 53.0, "val_loss": 17.31271892786026, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.18293857574463, "training_acc": 53.0, "val_loss": 17.311784625053406, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.20208287239075, "training_acc": 53.0, "val_loss": 17.311154305934906, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.17404794692993, "training_acc": 53.0, "val_loss": 17.310772836208344, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.16348385810852, "training_acc": 53.0, "val_loss": 17.310601472854614, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.18201398849487, "training_acc": 53.0, "val_loss": 17.310553789138794, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.12832260131836, "training_acc": 53.0, "val_loss": 17.31063425540924, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.15571355819702, "training_acc": 53.0, "val_loss": 17.310813069343567, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.15323185920715, "training_acc": 53.0, "val_loss": 17.311038076877594, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.12390637397766, "training_acc": 53.0, "val_loss": 17.311306297779083, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.12813782691956, "training_acc": 53.0, "val_loss": 17.311595380306244, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.15054106712341, "training_acc": 53.0, "val_loss": 17.311936616897583, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.11271214485168, "training_acc": 53.0, "val_loss": 17.312325537204742, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.14400577545166, "training_acc": 53.0, "val_loss": 17.31277108192444, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.1269314289093, "training_acc": 53.0, "val_loss": 17.313072085380554, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.12739610671997, "training_acc": 53.0, "val_loss": 17.313411831855774, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.10107135772705, "training_acc": 53.0, "val_loss": 17.313793301582336, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.1245367527008, "training_acc": 53.0, "val_loss": 17.314109206199646, "val_acc": 52.0}
{"epoch": 45, "training_loss": 69.12326169013977, "training_acc": 53.0, "val_loss": 17.314696311950684, "val_acc": 52.0}
{"epoch": 46, "training_loss": 69.15362668037415, "training_acc": 53.0, "val_loss": 17.314985394477844, "val_acc": 52.0}
{"epoch": 47, "training_loss": 69.14240431785583, "training_acc": 53.0, "val_loss": 17.315080761909485, "val_acc": 52.0}
{"epoch": 48, "training_loss": 69.1521828174591, "training_acc": 53.0, "val_loss": 17.315027117729187, "val_acc": 52.0}
{"epoch": 49, "training_loss": 69.1273775100708, "training_acc": 53.0, "val_loss": 17.314673960208893, "val_acc": 52.0}
{"epoch": 50, "training_loss": 69.14967083930969, "training_acc": 53.0, "val_loss": 17.314468324184418, "val_acc": 52.0}
{"epoch": 51, "training_loss": 69.15428280830383, "training_acc": 53.0, "val_loss": 17.314426600933075, "val_acc": 52.0}
