"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 1699768.580318451, "training_acc": 47.0, "val_loss": 450900.634765625, "val_acc": 48.0}
{"epoch": 1, "training_loss": 1684665.21875, "training_acc": 47.0, "val_loss": 553333.0078125, "val_acc": 52.0}
{"epoch": 2, "training_loss": 2047471.7109375, "training_acc": 53.0, "val_loss": 222392.7734375, "val_acc": 52.0}
{"epoch": 3, "training_loss": 823737.6328125, "training_acc": 51.0, "val_loss": 336858.4228515625, "val_acc": 48.0}
{"epoch": 4, "training_loss": 1252529.875, "training_acc": 47.0, "val_loss": 33600.99182128906, "val_acc": 48.0}
{"epoch": 5, "training_loss": 488836.55078125, "training_acc": 51.0, "val_loss": 471501.07421875, "val_acc": 52.0}
{"epoch": 6, "training_loss": 1847792.4140625, "training_acc": 53.0, "val_loss": 381883.837890625, "val_acc": 52.0}
{"epoch": 7, "training_loss": 1163196.064453125, "training_acc": 53.0, "val_loss": 140055.712890625, "val_acc": 48.0}
{"epoch": 8, "training_loss": 800356.828125, "training_acc": 47.0, "val_loss": 295613.9404296875, "val_acc": 48.0}
{"epoch": 9, "training_loss": 986148.009765625, "training_acc": 47.0, "val_loss": 79997.19848632812, "val_acc": 52.0}
{"epoch": 10, "training_loss": 451330.36328125, "training_acc": 53.0, "val_loss": 168256.6162109375, "val_acc": 52.0}
{"epoch": 11, "training_loss": 516080.72314453125, "training_acc": 53.0, "val_loss": 164745.8984375, "val_acc": 48.0}
{"epoch": 12, "training_loss": 722609.48046875, "training_acc": 47.0, "val_loss": 159587.12158203125, "val_acc": 48.0}
{"epoch": 13, "training_loss": 465008.8681640625, "training_acc": 45.0, "val_loss": 55932.257080078125, "val_acc": 52.0}
{"epoch": 14, "training_loss": 131452.08331298828, "training_acc": 55.0, "val_loss": 44604.48303222656, "val_acc": 52.0}
{"epoch": 15, "training_loss": 98987.68276977539, "training_acc": 53.0, "val_loss": 170167.88330078125, "val_acc": 48.0}
{"epoch": 16, "training_loss": 742214.9453125, "training_acc": 47.0, "val_loss": 122362.9638671875, "val_acc": 48.0}
{"epoch": 17, "training_loss": 389023.1064453125, "training_acc": 51.0, "val_loss": 132204.40673828125, "val_acc": 52.0}
{"epoch": 18, "training_loss": 486246.333984375, "training_acc": 53.0, "val_loss": 24687.745666503906, "val_acc": 48.0}
{"epoch": 19, "training_loss": 131494.86083984375, "training_acc": 47.0, "val_loss": 43529.827880859375, "val_acc": 52.0}
{"epoch": 20, "training_loss": 142570.76123046875, "training_acc": 53.0, "val_loss": 87117.75512695312, "val_acc": 48.0}
{"epoch": 21, "training_loss": 324194.28125, "training_acc": 47.0, "val_loss": 51492.230224609375, "val_acc": 52.0}
{"epoch": 22, "training_loss": 194225.35400390625, "training_acc": 53.0, "val_loss": 28045.41015625, "val_acc": 48.0}
{"epoch": 23, "training_loss": 72709.39154052734, "training_acc": 55.0, "val_loss": 45941.461181640625, "val_acc": 48.0}
{"epoch": 24, "training_loss": 166825.78173828125, "training_acc": 45.0, "val_loss": 18989.97802734375, "val_acc": 48.0}
{"epoch": 25, "training_loss": 136255.9521484375, "training_acc": 45.0, "val_loss": 15048.321533203125, "val_acc": 52.0}
{"epoch": 26, "training_loss": 184238.2734375, "training_acc": 55.0, "val_loss": 133074.7802734375, "val_acc": 48.0}
{"epoch": 27, "training_loss": 406523.2678222656, "training_acc": 47.0, "val_loss": 155322.8759765625, "val_acc": 52.0}
{"epoch": 28, "training_loss": 760606.97265625, "training_acc": 53.0, "val_loss": 187915.90576171875, "val_acc": 52.0}
{"epoch": 29, "training_loss": 527614.919921875, "training_acc": 53.0, "val_loss": 182876.806640625, "val_acc": 48.0}
{"epoch": 30, "training_loss": 947831.6875, "training_acc": 47.0, "val_loss": 265716.357421875, "val_acc": 48.0}
{"epoch": 31, "training_loss": 877429.3408203125, "training_acc": 47.0, "val_loss": 113614.58740234375, "val_acc": 52.0}
{"epoch": 32, "training_loss": 545539.1640625, "training_acc": 53.0, "val_loss": 213835.64453125, "val_acc": 52.0}
{"epoch": 33, "training_loss": 703665.5546875, "training_acc": 53.0, "val_loss": 43471.57287597656, "val_acc": 48.0}
{"epoch": 34, "training_loss": 280429.291015625, "training_acc": 47.0, "val_loss": 46287.43591308594, "val_acc": 48.0}
{"epoch": 35, "training_loss": 261917.1396484375, "training_acc": 51.0, "val_loss": 163122.509765625, "val_acc": 52.0}
{"epoch": 36, "training_loss": 568532.345703125, "training_acc": 53.0, "val_loss": 17227.7099609375, "val_acc": 48.0}
{"epoch": 37, "training_loss": 73973.98046875, "training_acc": 47.0, "val_loss": 58636.859130859375, "val_acc": 52.0}
{"epoch": 38, "training_loss": 203195.0302734375, "training_acc": 53.0, "val_loss": 80989.99633789062, "val_acc": 48.0}
{"epoch": 39, "training_loss": 344681.84765625, "training_acc": 47.0, "val_loss": 24407.164001464844, "val_acc": 52.0}
{"epoch": 40, "training_loss": 110423.08740234375, "training_acc": 53.0, "val_loss": 58525.506591796875, "val_acc": 48.0}
{"epoch": 41, "training_loss": 197462.19482421875, "training_acc": 47.0, "val_loss": 95625.75073242188, "val_acc": 52.0}
{"epoch": 42, "training_loss": 427970.146484375, "training_acc": 53.0, "val_loss": 44538.3544921875, "val_acc": 52.0}
{"epoch": 43, "training_loss": 301363.07421875, "training_acc": 51.0, "val_loss": 185122.998046875, "val_acc": 48.0}
{"epoch": 44, "training_loss": 647275.15234375, "training_acc": 47.0, "val_loss": 47998.883056640625, "val_acc": 52.0}
