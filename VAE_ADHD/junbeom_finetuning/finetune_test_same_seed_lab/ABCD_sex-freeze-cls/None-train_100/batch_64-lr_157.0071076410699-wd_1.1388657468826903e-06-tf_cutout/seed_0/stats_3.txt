"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 1017914.8219528198, "training_acc": 47.0, "val_loss": 353052.05078125, "val_acc": 52.0}
{"epoch": 1, "training_loss": 1078293.30078125, "training_acc": 51.0, "val_loss": 216744.921875, "val_acc": 48.0}
{"epoch": 2, "training_loss": 694346.7387695312, "training_acc": 47.0, "val_loss": 203931.93359375, "val_acc": 52.0}
{"epoch": 3, "training_loss": 911287.75390625, "training_acc": 53.0, "val_loss": 209700.830078125, "val_acc": 52.0}
{"epoch": 4, "training_loss": 558076.8466796875, "training_acc": 53.0, "val_loss": 218889.74609375, "val_acc": 48.0}
{"epoch": 5, "training_loss": 1022680.56640625, "training_acc": 47.0, "val_loss": 316255.6396484375, "val_acc": 48.0}
{"epoch": 6, "training_loss": 1126866.806640625, "training_acc": 47.0, "val_loss": 4805.826187133789, "val_acc": 52.0}
{"epoch": 7, "training_loss": 113542.88671875, "training_acc": 53.0, "val_loss": 64607.623291015625, "val_acc": 52.0}
{"epoch": 8, "training_loss": 213017.251953125, "training_acc": 49.0, "val_loss": 19350.271606445312, "val_acc": 48.0}
{"epoch": 9, "training_loss": 146601.5478515625, "training_acc": 51.0, "val_loss": 84507.90405273438, "val_acc": 52.0}
{"epoch": 10, "training_loss": 237569.18310546875, "training_acc": 53.0, "val_loss": 117836.62109375, "val_acc": 48.0}
{"epoch": 11, "training_loss": 536691.8984375, "training_acc": 47.0, "val_loss": 109733.6669921875, "val_acc": 48.0}
{"epoch": 12, "training_loss": 311421.5490722656, "training_acc": 49.0, "val_loss": 55768.218994140625, "val_acc": 52.0}
{"epoch": 13, "training_loss": 167838.95629882812, "training_acc": 53.0, "val_loss": 87605.57861328125, "val_acc": 48.0}
{"epoch": 14, "training_loss": 373990.8095703125, "training_acc": 47.0, "val_loss": 35759.326171875, "val_acc": 48.0}
{"epoch": 15, "training_loss": 165849.3408203125, "training_acc": 59.0, "val_loss": 146267.7978515625, "val_acc": 52.0}
{"epoch": 16, "training_loss": 549549.40234375, "training_acc": 53.0, "val_loss": 52019.586181640625, "val_acc": 52.0}
{"epoch": 17, "training_loss": 244821.740234375, "training_acc": 53.0, "val_loss": 138313.00048828125, "val_acc": 48.0}
{"epoch": 18, "training_loss": 508514.458984375, "training_acc": 47.0, "val_loss": 12772.196960449219, "val_acc": 52.0}
{"epoch": 19, "training_loss": 86594.97119140625, "training_acc": 53.0, "val_loss": 19450.11749267578, "val_acc": 48.0}
{"epoch": 20, "training_loss": 48965.71894836426, "training_acc": 55.0, "val_loss": 38256.0546875, "val_acc": 48.0}
{"epoch": 21, "training_loss": 96499.18716430664, "training_acc": 53.0, "val_loss": 25277.59246826172, "val_acc": 48.0}
{"epoch": 22, "training_loss": 106868.6708984375, "training_acc": 49.0, "val_loss": 9353.034210205078, "val_acc": 52.0}
{"epoch": 23, "training_loss": 162652.060546875, "training_acc": 47.0, "val_loss": 84129.1259765625, "val_acc": 48.0}
{"epoch": 24, "training_loss": 236029.3970336914, "training_acc": 45.0, "val_loss": 1262.798023223877, "val_acc": 52.0}
{"epoch": 25, "training_loss": 57376.81213378906, "training_acc": 63.0, "val_loss": 40615.98205566406, "val_acc": 48.0}
{"epoch": 26, "training_loss": 160875.361328125, "training_acc": 51.0, "val_loss": 55288.592529296875, "val_acc": 52.0}
{"epoch": 27, "training_loss": 138279.95001220703, "training_acc": 51.0, "val_loss": 22982.875061035156, "val_acc": 52.0}
{"epoch": 28, "training_loss": 77216.99340820312, "training_acc": 55.0, "val_loss": 12897.148132324219, "val_acc": 52.0}
{"epoch": 29, "training_loss": 79519.86083984375, "training_acc": 49.0, "val_loss": 12745.952606201172, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69320.69384765625, "training_acc": 47.0, "val_loss": 31111.5966796875, "val_acc": 52.0}
{"epoch": 31, "training_loss": 91564.28308105469, "training_acc": 53.0, "val_loss": 78194.68383789062, "val_acc": 48.0}
{"epoch": 32, "training_loss": 312829.4853515625, "training_acc": 47.0, "val_loss": 6510.779571533203, "val_acc": 52.0}
{"epoch": 33, "training_loss": 31363.773559570312, "training_acc": 53.0, "val_loss": 58839.92919921875, "val_acc": 48.0}
{"epoch": 34, "training_loss": 211352.92822265625, "training_acc": 47.0, "val_loss": 47898.101806640625, "val_acc": 52.0}
{"epoch": 35, "training_loss": 190075.71142578125, "training_acc": 53.0, "val_loss": 1245.4721450805664, "val_acc": 48.0}
{"epoch": 36, "training_loss": 45203.29150390625, "training_acc": 47.0, "val_loss": 16169.013977050781, "val_acc": 48.0}
{"epoch": 37, "training_loss": 70155.47265625, "training_acc": 49.0, "val_loss": 15335.182189941406, "val_acc": 48.0}
{"epoch": 38, "training_loss": 62114.288818359375, "training_acc": 55.0, "val_loss": 1551.6904830932617, "val_acc": 52.0}
{"epoch": 39, "training_loss": 134107.939453125, "training_acc": 51.0, "val_loss": 93052.93579101562, "val_acc": 48.0}
{"epoch": 40, "training_loss": 258883.60473632812, "training_acc": 47.0, "val_loss": 122765.6005859375, "val_acc": 52.0}
{"epoch": 41, "training_loss": 568614.294921875, "training_acc": 53.0, "val_loss": 175350.29296875, "val_acc": 52.0}
{"epoch": 42, "training_loss": 572135.69140625, "training_acc": 53.0, "val_loss": 31123.709106445312, "val_acc": 48.0}
{"epoch": 43, "training_loss": 218748.203125, "training_acc": 47.0, "val_loss": 63357.073974609375, "val_acc": 48.0}
{"epoch": 44, "training_loss": 196327.0927734375, "training_acc": 53.0, "val_loss": 59644.171142578125, "val_acc": 52.0}
{"epoch": 45, "training_loss": 176498.24365234375, "training_acc": 53.0, "val_loss": 90761.73095703125, "val_acc": 48.0}
{"epoch": 46, "training_loss": 401366.216796875, "training_acc": 47.0, "val_loss": 71057.56225585938, "val_acc": 48.0}
{"epoch": 47, "training_loss": 238224.20556640625, "training_acc": 51.0, "val_loss": 80875.50048828125, "val_acc": 52.0}
{"epoch": 48, "training_loss": 270951.4541015625, "training_acc": 53.0, "val_loss": 49119.23828125, "val_acc": 48.0}
{"epoch": 49, "training_loss": 237182.103515625, "training_acc": 47.0, "val_loss": 5487.479400634766, "val_acc": 52.0}
{"epoch": 50, "training_loss": 41836.489990234375, "training_acc": 53.0, "val_loss": 66336.1328125, "val_acc": 48.0}
{"epoch": 51, "training_loss": 270894.8447265625, "training_acc": 47.0, "val_loss": 17006.979370117188, "val_acc": 52.0}
{"epoch": 52, "training_loss": 91547.8994140625, "training_acc": 53.0, "val_loss": 37663.433837890625, "val_acc": 48.0}
{"epoch": 53, "training_loss": 134712.91064453125, "training_acc": 47.0, "val_loss": 62100.421142578125, "val_acc": 52.0}
{"epoch": 54, "training_loss": 264328.3564453125, "training_acc": 53.0, "val_loss": 21985.647583007812, "val_acc": 52.0}
