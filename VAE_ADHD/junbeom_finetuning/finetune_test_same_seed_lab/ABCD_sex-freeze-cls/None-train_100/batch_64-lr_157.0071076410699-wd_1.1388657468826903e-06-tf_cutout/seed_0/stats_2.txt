"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 1314930.0939865112, "training_acc": 53.0, "val_loss": 272633.349609375, "val_acc": 52.0}
{"epoch": 1, "training_loss": 1075377.51953125, "training_acc": 53.0, "val_loss": 466648.388671875, "val_acc": 48.0}
{"epoch": 2, "training_loss": 1755377.6171875, "training_acc": 47.0, "val_loss": 115300.25634765625, "val_acc": 48.0}
{"epoch": 3, "training_loss": 596966.73046875, "training_acc": 51.0, "val_loss": 400233.5205078125, "val_acc": 52.0}
{"epoch": 4, "training_loss": 1556111.44140625, "training_acc": 53.0, "val_loss": 335132.6904296875, "val_acc": 52.0}
{"epoch": 5, "training_loss": 1076750.755859375, "training_acc": 53.0, "val_loss": 78728.30810546875, "val_acc": 48.0}
{"epoch": 6, "training_loss": 424185.298828125, "training_acc": 47.0, "val_loss": 162015.0146484375, "val_acc": 48.0}
{"epoch": 7, "training_loss": 474960.544921875, "training_acc": 47.0, "val_loss": 142384.50927734375, "val_acc": 52.0}
{"epoch": 8, "training_loss": 678587.08203125, "training_acc": 53.0, "val_loss": 231705.224609375, "val_acc": 52.0}
{"epoch": 9, "training_loss": 782477.09375, "training_acc": 53.0, "val_loss": 2808.804130554199, "val_acc": 48.0}
{"epoch": 10, "training_loss": 61530.18994140625, "training_acc": 47.0, "val_loss": 2528.426170349121, "val_acc": 48.0}
{"epoch": 11, "training_loss": 159197.294921875, "training_acc": 51.0, "val_loss": 148906.2744140625, "val_acc": 52.0}
{"epoch": 12, "training_loss": 513714.7841796875, "training_acc": 53.0, "val_loss": 12162.849426269531, "val_acc": 48.0}
{"epoch": 13, "training_loss": 70314.880859375, "training_acc": 47.0, "val_loss": 39308.27331542969, "val_acc": 52.0}
{"epoch": 14, "training_loss": 128332.97119140625, "training_acc": 53.0, "val_loss": 66981.0791015625, "val_acc": 48.0}
{"epoch": 15, "training_loss": 264237.1181640625, "training_acc": 47.0, "val_loss": 34450.99182128906, "val_acc": 52.0}
{"epoch": 16, "training_loss": 154741.5087890625, "training_acc": 53.0, "val_loss": 15754.112243652344, "val_acc": 48.0}
{"epoch": 17, "training_loss": 64783.053955078125, "training_acc": 45.0, "val_loss": 43051.03759765625, "val_acc": 48.0}
{"epoch": 18, "training_loss": 124841.49938964844, "training_acc": 47.0, "val_loss": 97813.34228515625, "val_acc": 52.0}
{"epoch": 19, "training_loss": 415294.59375, "training_acc": 53.0, "val_loss": 77272.998046875, "val_acc": 52.0}
{"epoch": 20, "training_loss": 271123.7587890625, "training_acc": 47.0, "val_loss": 61309.4970703125, "val_acc": 48.0}
{"epoch": 21, "training_loss": 166668.6690673828, "training_acc": 49.0, "val_loss": 8863.724517822266, "val_acc": 48.0}
{"epoch": 22, "training_loss": 65625.30712890625, "training_acc": 59.0, "val_loss": 46395.44372558594, "val_acc": 52.0}
{"epoch": 23, "training_loss": 225142.1591796875, "training_acc": 37.0, "val_loss": 10678.06625366211, "val_acc": 48.0}
{"epoch": 24, "training_loss": 147614.99609375, "training_acc": 51.0, "val_loss": 117974.76806640625, "val_acc": 52.0}
{"epoch": 25, "training_loss": 394593.2939453125, "training_acc": 53.0, "val_loss": 37354.66613769531, "val_acc": 48.0}
{"epoch": 26, "training_loss": 206283.208984375, "training_acc": 47.0, "val_loss": 5206.989288330078, "val_acc": 48.0}
{"epoch": 27, "training_loss": 205910.1484375, "training_acc": 47.0, "val_loss": 176174.169921875, "val_acc": 52.0}
{"epoch": 28, "training_loss": 646358.138671875, "training_acc": 53.0, "val_loss": 59239.569091796875, "val_acc": 52.0}
{"epoch": 29, "training_loss": 310752.45703125, "training_acc": 47.0, "val_loss": 149472.81494140625, "val_acc": 48.0}
{"epoch": 30, "training_loss": 554602.978515625, "training_acc": 47.0, "val_loss": 1043.527889251709, "val_acc": 48.0}
{"epoch": 31, "training_loss": 192273.50842285156, "training_acc": 55.0, "val_loss": 259375.439453125, "val_acc": 52.0}
{"epoch": 32, "training_loss": 1037335.5234375, "training_acc": 53.0, "val_loss": 223999.9267578125, "val_acc": 52.0}
{"epoch": 33, "training_loss": 709282.78125, "training_acc": 53.0, "val_loss": 77163.90380859375, "val_acc": 48.0}
{"epoch": 34, "training_loss": 456665.076171875, "training_acc": 47.0, "val_loss": 157757.55615234375, "val_acc": 48.0}
{"epoch": 35, "training_loss": 504158.2939453125, "training_acc": 47.0, "val_loss": 90888.96484375, "val_acc": 52.0}
{"epoch": 36, "training_loss": 425073.630859375, "training_acc": 53.0, "val_loss": 153534.375, "val_acc": 52.0}
{"epoch": 37, "training_loss": 498248.3583984375, "training_acc": 53.0, "val_loss": 46790.76843261719, "val_acc": 48.0}
{"epoch": 38, "training_loss": 252451.4814453125, "training_acc": 47.0, "val_loss": 48777.5146484375, "val_acc": 48.0}
{"epoch": 39, "training_loss": 208242.37890625, "training_acc": 51.0, "val_loss": 99683.72192382812, "val_acc": 52.0}
{"epoch": 40, "training_loss": 335365.3623046875, "training_acc": 53.0, "val_loss": 41023.01330566406, "val_acc": 48.0}
{"epoch": 41, "training_loss": 181094.04736328125, "training_acc": 47.0, "val_loss": 9101.545715332031, "val_acc": 52.0}
{"epoch": 42, "training_loss": 27789.017578125, "training_acc": 53.0, "val_loss": 56565.521240234375, "val_acc": 52.0}
{"epoch": 43, "training_loss": 197457.53857421875, "training_acc": 53.0, "val_loss": 40873.956298828125, "val_acc": 48.0}
{"epoch": 44, "training_loss": 148270.09765625, "training_acc": 47.0, "val_loss": 54981.94580078125, "val_acc": 52.0}
{"epoch": 45, "training_loss": 221841.3935546875, "training_acc": 53.0, "val_loss": 2241.1937713623047, "val_acc": 48.0}
{"epoch": 46, "training_loss": 30438.2041015625, "training_acc": 49.0, "val_loss": 34565.667724609375, "val_acc": 48.0}
{"epoch": 47, "training_loss": 95385.99172973633, "training_acc": 47.0, "val_loss": 42445.330810546875, "val_acc": 48.0}
{"epoch": 48, "training_loss": 109204.07897949219, "training_acc": 47.0, "val_loss": 111977.57568359375, "val_acc": 52.0}
{"epoch": 49, "training_loss": 491321.787109375, "training_acc": 53.0, "val_loss": 113635.85205078125, "val_acc": 52.0}
