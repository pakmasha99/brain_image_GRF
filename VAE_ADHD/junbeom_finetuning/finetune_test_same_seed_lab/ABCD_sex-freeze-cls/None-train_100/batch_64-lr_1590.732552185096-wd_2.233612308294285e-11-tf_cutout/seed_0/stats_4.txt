"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 10945340.407173157, "training_acc": 51.0, "val_loss": 2409838.4765625, "val_acc": 52.0}
{"epoch": 1, "training_loss": 11947128.5625, "training_acc": 55.0, "val_loss": 6580021.875, "val_acc": 48.0}
{"epoch": 2, "training_loss": 24621966.8125, "training_acc": 47.0, "val_loss": 1671997.0703125, "val_acc": 48.0}
{"epoch": 3, "training_loss": 7477248.1875, "training_acc": 53.0, "val_loss": 4865475.0, "val_acc": 52.0}
{"epoch": 4, "training_loss": 19864478.8125, "training_acc": 53.0, "val_loss": 4337058.984375, "val_acc": 52.0}
{"epoch": 5, "training_loss": 13988518.3125, "training_acc": 53.0, "val_loss": 837208.88671875, "val_acc": 48.0}
{"epoch": 6, "training_loss": 6422642.96875, "training_acc": 47.0, "val_loss": 2213675.390625, "val_acc": 48.0}
{"epoch": 7, "training_loss": 7105689.2421875, "training_acc": 47.0, "val_loss": 1596159.5703125, "val_acc": 52.0}
{"epoch": 8, "training_loss": 7632351.9375, "training_acc": 53.0, "val_loss": 2483638.4765625, "val_acc": 52.0}
{"epoch": 9, "training_loss": 8508261.84375, "training_acc": 53.0, "val_loss": 384419.921875, "val_acc": 48.0}
{"epoch": 10, "training_loss": 2347924.015625, "training_acc": 47.0, "val_loss": 310105.0048828125, "val_acc": 48.0}
{"epoch": 11, "training_loss": 2509817.140625, "training_acc": 49.0, "val_loss": 1654110.7421875, "val_acc": 52.0}
{"epoch": 12, "training_loss": 5849692.21875, "training_acc": 53.0, "val_loss": 54753.997802734375, "val_acc": 48.0}
{"epoch": 13, "training_loss": 626190.15234375, "training_acc": 47.0, "val_loss": 481243.603515625, "val_acc": 52.0}
{"epoch": 14, "training_loss": 1627839.68359375, "training_acc": 53.0, "val_loss": 673180.712890625, "val_acc": 48.0}
{"epoch": 15, "training_loss": 2492678.03125, "training_acc": 47.0, "val_loss": 568611.376953125, "val_acc": 52.0}
{"epoch": 16, "training_loss": 2420555.0390625, "training_acc": 53.0, "val_loss": 54913.134765625, "val_acc": 48.0}
{"epoch": 17, "training_loss": 417956.04296875, "training_acc": 49.0, "val_loss": 432531.591796875, "val_acc": 48.0}
{"epoch": 18, "training_loss": 1165542.2893066406, "training_acc": 47.0, "val_loss": 1217400.48828125, "val_acc": 52.0}
{"epoch": 19, "training_loss": 5151161.3125, "training_acc": 53.0, "val_loss": 1009568.5546875, "val_acc": 52.0}
{"epoch": 20, "training_loss": 3041787.8671875, "training_acc": 49.0, "val_loss": 496144.04296875, "val_acc": 48.0}
{"epoch": 21, "training_loss": 1573585.10546875, "training_acc": 51.0, "val_loss": 108660.64453125, "val_acc": 52.0}
{"epoch": 22, "training_loss": 1308748.796875, "training_acc": 55.0, "val_loss": 775196.2890625, "val_acc": 48.0}
{"epoch": 23, "training_loss": 2210220.873046875, "training_acc": 51.0, "val_loss": 239479.8583984375, "val_acc": 52.0}
{"epoch": 24, "training_loss": 1497432.2578125, "training_acc": 49.0, "val_loss": 303018.9208984375, "val_acc": 48.0}
{"epoch": 25, "training_loss": 1975873.53125, "training_acc": 49.0, "val_loss": 1044822.8515625, "val_acc": 52.0}
{"epoch": 26, "training_loss": 3329923.73046875, "training_acc": 53.0, "val_loss": 975231.0546875, "val_acc": 48.0}
{"epoch": 27, "training_loss": 4168563.578125, "training_acc": 47.0, "val_loss": 464597.998046875, "val_acc": 48.0}
{"epoch": 28, "training_loss": 2973309.90625, "training_acc": 45.0, "val_loss": 1525653.125, "val_acc": 52.0}
{"epoch": 29, "training_loss": 5471506.265625, "training_acc": 53.0, "val_loss": 40940.10009765625, "val_acc": 52.0}
{"epoch": 30, "training_loss": 2204435.875, "training_acc": 59.0, "val_loss": 2770744.7265625, "val_acc": 48.0}
{"epoch": 31, "training_loss": 11150236.40625, "training_acc": 47.0, "val_loss": 1582626.85546875, "val_acc": 48.0}
{"epoch": 32, "training_loss": 4427548.61328125, "training_acc": 51.0, "val_loss": 1158485.83984375, "val_acc": 52.0}
{"epoch": 33, "training_loss": 4799770.3125, "training_acc": 53.0, "val_loss": 352821.044921875, "val_acc": 52.0}
{"epoch": 34, "training_loss": 2908530.203125, "training_acc": 51.0, "val_loss": 2023257.03125, "val_acc": 48.0}
{"epoch": 35, "training_loss": 7711748.4375, "training_acc": 47.0, "val_loss": 342715.8447265625, "val_acc": 48.0}
{"epoch": 36, "training_loss": 3487782.5625, "training_acc": 47.0, "val_loss": 2704312.5, "val_acc": 52.0}
{"epoch": 37, "training_loss": 10575911.15625, "training_acc": 53.0, "val_loss": 2123103.90625, "val_acc": 52.0}
{"epoch": 38, "training_loss": 6718439.6171875, "training_acc": 53.0, "val_loss": 1221189.74609375, "val_acc": 48.0}
{"epoch": 39, "training_loss": 6047213.40625, "training_acc": 47.0, "val_loss": 1923853.3203125, "val_acc": 48.0}
{"epoch": 40, "training_loss": 6383792.234375, "training_acc": 47.0, "val_loss": 967278.3203125, "val_acc": 52.0}
{"epoch": 41, "training_loss": 5078730.875, "training_acc": 53.0, "val_loss": 1637726.7578125, "val_acc": 52.0}
{"epoch": 42, "training_loss": 4999300.53125, "training_acc": 53.0, "val_loss": 936840.4296875, "val_acc": 48.0}
{"epoch": 43, "training_loss": 4344240.578125, "training_acc": 47.0, "val_loss": 1286327.24609375, "val_acc": 48.0}
{"epoch": 44, "training_loss": 3664846.30859375, "training_acc": 47.0, "val_loss": 1642972.265625, "val_acc": 52.0}
{"epoch": 45, "training_loss": 7124187.484375, "training_acc": 53.0, "val_loss": 2404977.734375, "val_acc": 52.0}
{"epoch": 46, "training_loss": 8353775.96875, "training_acc": 53.0, "val_loss": 269255.419921875, "val_acc": 52.0}
{"epoch": 47, "training_loss": 4284718.03125, "training_acc": 45.0, "val_loss": 3140395.8984375, "val_acc": 48.0}
{"epoch": 48, "training_loss": 12682755.5625, "training_acc": 47.0, "val_loss": 2160775.390625, "val_acc": 48.0}
