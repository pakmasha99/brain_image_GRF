"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 12943302.078781128, "training_acc": 50.0, "val_loss": 2510818.359375, "val_acc": 44.0}
{"epoch": 1, "training_loss": 13348712.0, "training_acc": 48.0, "val_loss": 6374775.0, "val_acc": 56.0}
{"epoch": 2, "training_loss": 27175082.0, "training_acc": 52.0, "val_loss": 3900282.03125, "val_acc": 56.0}
{"epoch": 3, "training_loss": 11648215.40625, "training_acc": 52.0, "val_loss": 4075045.3125, "val_acc": 44.0}
{"epoch": 4, "training_loss": 18550660.125, "training_acc": 48.0, "val_loss": 7101703.125, "val_acc": 44.0}
{"epoch": 5, "training_loss": 25031398.8125, "training_acc": 48.0, "val_loss": 3700485.546875, "val_acc": 44.0}
{"epoch": 6, "training_loss": 9298518.19921875, "training_acc": 48.0, "val_loss": 2930016.9921875, "val_acc": 56.0}
{"epoch": 7, "training_loss": 15541586.125, "training_acc": 52.0, "val_loss": 5658023.046875, "val_acc": 56.0}
{"epoch": 8, "training_loss": 24612642.125, "training_acc": 52.0, "val_loss": 4534282.8125, "val_acc": 56.0}
{"epoch": 9, "training_loss": 16862600.125, "training_acc": 52.0, "val_loss": 294740.7958984375, "val_acc": 56.0}
{"epoch": 10, "training_loss": 5517785.25, "training_acc": 50.0, "val_loss": 5714716.015625, "val_acc": 44.0}
{"epoch": 11, "training_loss": 22345463.875, "training_acc": 48.0, "val_loss": 6326977.734375, "val_acc": 44.0}
{"epoch": 12, "training_loss": 21831017.625, "training_acc": 48.0, "val_loss": 2762152.734375, "val_acc": 44.0}
{"epoch": 13, "training_loss": 6564122.6484375, "training_acc": 52.0, "val_loss": 1567325.1953125, "val_acc": 56.0}
{"epoch": 14, "training_loss": 7495056.46875, "training_acc": 52.0, "val_loss": 1716336.71875, "val_acc": 56.0}
{"epoch": 15, "training_loss": 6012031.1640625, "training_acc": 52.0, "val_loss": 1275823.92578125, "val_acc": 44.0}
{"epoch": 16, "training_loss": 5762729.59375, "training_acc": 48.0, "val_loss": 1794985.7421875, "val_acc": 44.0}
{"epoch": 17, "training_loss": 4934885.359375, "training_acc": 48.0, "val_loss": 1305028.7109375, "val_acc": 56.0}
{"epoch": 18, "training_loss": 6916771.15625, "training_acc": 52.0, "val_loss": 2075611.1328125, "val_acc": 56.0}
{"epoch": 19, "training_loss": 7843021.546875, "training_acc": 52.0, "val_loss": 134318.98193359375, "val_acc": 44.0}
{"epoch": 20, "training_loss": 1445267.5546875, "training_acc": 48.0, "val_loss": 310683.837890625, "val_acc": 44.0}
{"epoch": 21, "training_loss": 1836049.4140625, "training_acc": 54.0, "val_loss": 1184000.5859375, "val_acc": 56.0}
{"epoch": 22, "training_loss": 4502034.703125, "training_acc": 52.0, "val_loss": 458534.66796875, "val_acc": 44.0}
{"epoch": 23, "training_loss": 2042813.6640625, "training_acc": 48.0, "val_loss": 78776.94091796875, "val_acc": 44.0}
{"epoch": 24, "training_loss": 2332844.140625, "training_acc": 46.0, "val_loss": 1603414.35546875, "val_acc": 56.0}
{"epoch": 25, "training_loss": 6370332.21875, "training_acc": 52.0, "val_loss": 109364.80712890625, "val_acc": 56.0}
{"epoch": 26, "training_loss": 1958376.546875, "training_acc": 62.0, "val_loss": 2929122.4609375, "val_acc": 44.0}
{"epoch": 27, "training_loss": 10796712.90625, "training_acc": 48.0, "val_loss": 1846778.515625, "val_acc": 44.0}
{"epoch": 28, "training_loss": 4104280.206298828, "training_acc": 54.0, "val_loss": 696697.4609375, "val_acc": 56.0}
{"epoch": 29, "training_loss": 2819769.8984375, "training_acc": 52.0, "val_loss": 346295.8984375, "val_acc": 44.0}
{"epoch": 30, "training_loss": 1060696.1767578125, "training_acc": 48.0, "val_loss": 654125.244140625, "val_acc": 56.0}
{"epoch": 31, "training_loss": 2665244.44140625, "training_acc": 52.0, "val_loss": 260235.9375, "val_acc": 44.0}
{"epoch": 32, "training_loss": 658237.0112304688, "training_acc": 48.0, "val_loss": 913880.56640625, "val_acc": 56.0}
{"epoch": 33, "training_loss": 4095119.953125, "training_acc": 52.0, "val_loss": 373992.87109375, "val_acc": 56.0}
{"epoch": 34, "training_loss": 2334037.328125, "training_acc": 54.0, "val_loss": 1621625.68359375, "val_acc": 44.0}
{"epoch": 35, "training_loss": 5272582.453125, "training_acc": 48.0, "val_loss": 352104.1748046875, "val_acc": 56.0}
{"epoch": 36, "training_loss": 1986894.7265625, "training_acc": 52.0, "val_loss": 260367.28515625, "val_acc": 56.0}
{"epoch": 37, "training_loss": 2785081.5, "training_acc": 42.0, "val_loss": 1320383.49609375, "val_acc": 44.0}
{"epoch": 38, "training_loss": 3643615.2265625, "training_acc": 48.0, "val_loss": 1069533.3984375, "val_acc": 56.0}
{"epoch": 39, "training_loss": 5927846.25, "training_acc": 52.0, "val_loss": 1543414.453125, "val_acc": 56.0}
{"epoch": 40, "training_loss": 5293748.28125, "training_acc": 52.0, "val_loss": 1202054.00390625, "val_acc": 44.0}
{"epoch": 41, "training_loss": 5523030.65625, "training_acc": 48.0, "val_loss": 1677364.453125, "val_acc": 44.0}
{"epoch": 42, "training_loss": 4666206.48828125, "training_acc": 48.0, "val_loss": 1389062.98828125, "val_acc": 56.0}
