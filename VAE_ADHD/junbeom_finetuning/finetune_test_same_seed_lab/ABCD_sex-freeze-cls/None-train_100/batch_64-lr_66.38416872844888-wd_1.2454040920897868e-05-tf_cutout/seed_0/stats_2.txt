"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 625605.7471313477, "training_acc": 45.0, "val_loss": 113020.34912109375, "val_acc": 48.0}
{"epoch": 1, "training_loss": 424721.876953125, "training_acc": 59.0, "val_loss": 300507.9345703125, "val_acc": 52.0}
{"epoch": 2, "training_loss": 1162256.681640625, "training_acc": 53.0, "val_loss": 220856.0791015625, "val_acc": 52.0}
{"epoch": 3, "training_loss": 702871.3115234375, "training_acc": 53.0, "val_loss": 66581.06689453125, "val_acc": 48.0}
{"epoch": 4, "training_loss": 357453.296875, "training_acc": 47.0, "val_loss": 128836.09619140625, "val_acc": 48.0}
{"epoch": 5, "training_loss": 423861.134765625, "training_acc": 47.0, "val_loss": 43925.933837890625, "val_acc": 52.0}
{"epoch": 6, "training_loss": 246060.1259765625, "training_acc": 53.0, "val_loss": 89300.94604492188, "val_acc": 52.0}
{"epoch": 7, "training_loss": 291450.587890625, "training_acc": 53.0, "val_loss": 42809.97619628906, "val_acc": 48.0}
{"epoch": 8, "training_loss": 219641.4853515625, "training_acc": 47.0, "val_loss": 42142.90771484375, "val_acc": 48.0}
{"epoch": 9, "training_loss": 142371.39404296875, "training_acc": 51.0, "val_loss": 48745.59326171875, "val_acc": 52.0}
{"epoch": 10, "training_loss": 165971.95239257812, "training_acc": 53.0, "val_loss": 24476.8798828125, "val_acc": 48.0}
{"epoch": 11, "training_loss": 100315.666015625, "training_acc": 47.0, "val_loss": 13069.894409179688, "val_acc": 52.0}
{"epoch": 12, "training_loss": 44716.40002441406, "training_acc": 53.0, "val_loss": 32363.4765625, "val_acc": 48.0}
{"epoch": 13, "training_loss": 125482.13037109375, "training_acc": 47.0, "val_loss": 20383.035278320312, "val_acc": 52.0}
{"epoch": 14, "training_loss": 89825.78125, "training_acc": 53.0, "val_loss": 1106.7276000976562, "val_acc": 48.0}
{"epoch": 15, "training_loss": 13125.048950195312, "training_acc": 55.0, "val_loss": 11386.396026611328, "val_acc": 48.0}
{"epoch": 16, "training_loss": 48495.62353515625, "training_acc": 47.0, "val_loss": 4640.245819091797, "val_acc": 48.0}
{"epoch": 17, "training_loss": 39812.51611328125, "training_acc": 49.0, "val_loss": 6951.953887939453, "val_acc": 52.0}
{"epoch": 18, "training_loss": 83158.91650390625, "training_acc": 49.0, "val_loss": 44048.968505859375, "val_acc": 48.0}
{"epoch": 19, "training_loss": 123393.30645751953, "training_acc": 47.0, "val_loss": 65795.99609375, "val_acc": 52.0}
{"epoch": 20, "training_loss": 306914.228515625, "training_acc": 53.0, "val_loss": 87467.08984375, "val_acc": 52.0}
{"epoch": 21, "training_loss": 281785.9833984375, "training_acc": 53.0, "val_loss": 26771.636962890625, "val_acc": 48.0}
{"epoch": 22, "training_loss": 160076.095703125, "training_acc": 47.0, "val_loss": 42232.21435546875, "val_acc": 48.0}
{"epoch": 23, "training_loss": 132456.53881835938, "training_acc": 47.0, "val_loss": 23887.828063964844, "val_acc": 52.0}
{"epoch": 24, "training_loss": 61709.71807861328, "training_acc": 53.0, "val_loss": 56079.193115234375, "val_acc": 48.0}
{"epoch": 25, "training_loss": 245581.5556640625, "training_acc": 47.0, "val_loss": 38359.1552734375, "val_acc": 48.0}
{"epoch": 26, "training_loss": 143454.16357421875, "training_acc": 47.0, "val_loss": 48041.51916503906, "val_acc": 52.0}
{"epoch": 27, "training_loss": 168750.60546875, "training_acc": 53.0, "val_loss": 11280.168914794922, "val_acc": 48.0}
{"epoch": 28, "training_loss": 46920.636474609375, "training_acc": 47.0, "val_loss": 22982.5927734375, "val_acc": 52.0}
{"epoch": 29, "training_loss": 87407.13647460938, "training_acc": 53.0, "val_loss": 14359.194946289062, "val_acc": 48.0}
{"epoch": 30, "training_loss": 50519.66467285156, "training_acc": 47.0, "val_loss": 34221.319580078125, "val_acc": 52.0}
{"epoch": 31, "training_loss": 144648.99169921875, "training_acc": 53.0, "val_loss": 13110.568237304688, "val_acc": 52.0}
{"epoch": 32, "training_loss": 97131.96728515625, "training_acc": 53.0, "val_loss": 65042.388916015625, "val_acc": 48.0}
{"epoch": 33, "training_loss": 230352.99462890625, "training_acc": 47.0, "val_loss": 18086.51580810547, "val_acc": 52.0}
