"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 627441.9787864685, "training_acc": 53.0, "val_loss": 127540.39306640625, "val_acc": 52.0}
{"epoch": 1, "training_loss": 463836.4375, "training_acc": 57.0, "val_loss": 232211.23046875, "val_acc": 48.0}
{"epoch": 2, "training_loss": 886583.87890625, "training_acc": 47.0, "val_loss": 70565.12451171875, "val_acc": 48.0}
{"epoch": 3, "training_loss": 306902.734375, "training_acc": 51.0, "val_loss": 173254.052734375, "val_acc": 52.0}
{"epoch": 4, "training_loss": 688580.90625, "training_acc": 53.0, "val_loss": 137725.6591796875, "val_acc": 52.0}
{"epoch": 5, "training_loss": 407057.0, "training_acc": 53.0, "val_loss": 75828.44848632812, "val_acc": 48.0}
{"epoch": 6, "training_loss": 396891.583984375, "training_acc": 47.0, "val_loss": 135147.900390625, "val_acc": 48.0}
{"epoch": 7, "training_loss": 475347.6943359375, "training_acc": 47.0, "val_loss": 9968.37158203125, "val_acc": 52.0}
{"epoch": 8, "training_loss": 96816.8232421875, "training_acc": 53.0, "val_loss": 41137.042236328125, "val_acc": 52.0}
{"epoch": 9, "training_loss": 102889.53729248047, "training_acc": 53.0, "val_loss": 782.7792167663574, "val_acc": 48.0}
{"epoch": 10, "training_loss": 38377.640869140625, "training_acc": 59.0, "val_loss": 43808.624267578125, "val_acc": 52.0}
{"epoch": 11, "training_loss": 131921.70275878906, "training_acc": 53.0, "val_loss": 55166.680908203125, "val_acc": 48.0}
{"epoch": 12, "training_loss": 242640.466796875, "training_acc": 47.0, "val_loss": 38197.77526855469, "val_acc": 48.0}
{"epoch": 13, "training_loss": 177937.52783203125, "training_acc": 39.0, "val_loss": 51414.2822265625, "val_acc": 52.0}
{"epoch": 14, "training_loss": 174377.46044921875, "training_acc": 53.0, "val_loss": 20750.35858154297, "val_acc": 48.0}
{"epoch": 15, "training_loss": 108197.75927734375, "training_acc": 47.0, "val_loss": 333.0177307128906, "val_acc": 52.0}
{"epoch": 16, "training_loss": 11990.131774902344, "training_acc": 47.0, "val_loss": 28914.2333984375, "val_acc": 52.0}
{"epoch": 17, "training_loss": 111156.66650390625, "training_acc": 53.0, "val_loss": 7688.005065917969, "val_acc": 48.0}
{"epoch": 18, "training_loss": 24088.81719970703, "training_acc": 47.0, "val_loss": 41510.97412109375, "val_acc": 52.0}
{"epoch": 19, "training_loss": 177388.4306640625, "training_acc": 53.0, "val_loss": 22577.349853515625, "val_acc": 52.0}
{"epoch": 20, "training_loss": 82371.08959960938, "training_acc": 61.0, "val_loss": 54992.1875, "val_acc": 48.0}
{"epoch": 21, "training_loss": 194565.685546875, "training_acc": 47.0, "val_loss": 21671.97265625, "val_acc": 52.0}
{"epoch": 22, "training_loss": 108123.16650390625, "training_acc": 53.0, "val_loss": 20000.392150878906, "val_acc": 52.0}
{"epoch": 23, "training_loss": 84739.49462890625, "training_acc": 55.0, "val_loss": 39128.78723144531, "val_acc": 48.0}
{"epoch": 24, "training_loss": 113341.75573730469, "training_acc": 47.0, "val_loss": 55068.914794921875, "val_acc": 52.0}
{"epoch": 25, "training_loss": 243400.9521484375, "training_acc": 53.0, "val_loss": 68432.09228515625, "val_acc": 52.0}
{"epoch": 26, "training_loss": 214376.74145507812, "training_acc": 53.0, "val_loss": 45598.91662597656, "val_acc": 48.0}
{"epoch": 27, "training_loss": 223487.572265625, "training_acc": 47.0, "val_loss": 54681.146240234375, "val_acc": 48.0}
{"epoch": 28, "training_loss": 149048.94943237305, "training_acc": 47.0, "val_loss": 13094.244384765625, "val_acc": 52.0}
{"epoch": 29, "training_loss": 39926.534912109375, "training_acc": 55.0, "val_loss": 8420.177459716797, "val_acc": 52.0}
{"epoch": 30, "training_loss": 48493.64453125, "training_acc": 45.0, "val_loss": 8439.796447753906, "val_acc": 52.0}
{"epoch": 31, "training_loss": 24620.715789794922, "training_acc": 53.0, "val_loss": 21595.803833007812, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69672.5107421875, "training_acc": 53.0, "val_loss": 33561.83776855469, "val_acc": 48.0}
{"epoch": 33, "training_loss": 144711.4111328125, "training_acc": 47.0, "val_loss": 2980.194091796875, "val_acc": 52.0}
{"epoch": 34, "training_loss": 14510.581420898438, "training_acc": 53.0, "val_loss": 27847.085571289062, "val_acc": 48.0}
