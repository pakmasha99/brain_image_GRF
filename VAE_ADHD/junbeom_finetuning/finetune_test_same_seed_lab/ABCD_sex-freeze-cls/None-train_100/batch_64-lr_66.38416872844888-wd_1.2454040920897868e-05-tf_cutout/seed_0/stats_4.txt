"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 456811.03607940674, "training_acc": 51.0, "val_loss": 100283.31298828125, "val_acc": 52.0}
{"epoch": 1, "training_loss": 498432.900390625, "training_acc": 55.0, "val_loss": 274826.2451171875, "val_acc": 48.0}
{"epoch": 2, "training_loss": 1028124.95703125, "training_acc": 47.0, "val_loss": 69603.2470703125, "val_acc": 48.0}
{"epoch": 3, "training_loss": 311880.419921875, "training_acc": 53.0, "val_loss": 203164.29443359375, "val_acc": 52.0}
{"epoch": 4, "training_loss": 829181.78125, "training_acc": 53.0, "val_loss": 180759.130859375, "val_acc": 52.0}
{"epoch": 5, "training_loss": 582637.5966796875, "training_acc": 53.0, "val_loss": 35436.66076660156, "val_acc": 48.0}
{"epoch": 6, "training_loss": 270006.041015625, "training_acc": 47.0, "val_loss": 92764.32495117188, "val_acc": 48.0}
{"epoch": 7, "training_loss": 298002.3835449219, "training_acc": 47.0, "val_loss": 66348.4375, "val_acc": 52.0}
{"epoch": 8, "training_loss": 317407.73828125, "training_acc": 53.0, "val_loss": 103241.2109375, "val_acc": 52.0}
{"epoch": 9, "training_loss": 353384.9521484375, "training_acc": 53.0, "val_loss": 16624.777221679688, "val_acc": 48.0}
{"epoch": 10, "training_loss": 100335.61865234375, "training_acc": 47.0, "val_loss": 13477.590942382812, "val_acc": 48.0}
{"epoch": 11, "training_loss": 105455.24462890625, "training_acc": 49.0, "val_loss": 68503.2958984375, "val_acc": 52.0}
{"epoch": 12, "training_loss": 241970.6240234375, "training_acc": 53.0, "val_loss": 2962.462615966797, "val_acc": 48.0}
{"epoch": 13, "training_loss": 28888.73876953125, "training_acc": 47.0, "val_loss": 19470.66192626953, "val_acc": 52.0}
{"epoch": 14, "training_loss": 65515.99572753906, "training_acc": 53.0, "val_loss": 28783.322143554688, "val_acc": 48.0}
{"epoch": 15, "training_loss": 106804.23364257812, "training_acc": 47.0, "val_loss": 23130.148315429688, "val_acc": 52.0}
{"epoch": 16, "training_loss": 98640.58544921875, "training_acc": 53.0, "val_loss": 2986.893081665039, "val_acc": 48.0}
{"epoch": 17, "training_loss": 18353.832763671875, "training_acc": 49.0, "val_loss": 18749.790954589844, "val_acc": 48.0}
{"epoch": 18, "training_loss": 51467.6725769043, "training_acc": 47.0, "val_loss": 50174.27978515625, "val_acc": 52.0}
{"epoch": 19, "training_loss": 212445.998046875, "training_acc": 53.0, "val_loss": 41408.1787109375, "val_acc": 52.0}
{"epoch": 20, "training_loss": 126333.82861328125, "training_acc": 49.0, "val_loss": 21514.907836914062, "val_acc": 48.0}
{"epoch": 21, "training_loss": 66749.04711914062, "training_acc": 51.0, "val_loss": 3795.948028564453, "val_acc": 52.0}
{"epoch": 22, "training_loss": 53944.80029296875, "training_acc": 55.0, "val_loss": 33125.482177734375, "val_acc": 48.0}
{"epoch": 23, "training_loss": 93283.87524414062, "training_acc": 51.0, "val_loss": 9298.043060302734, "val_acc": 52.0}
{"epoch": 24, "training_loss": 61865.25634765625, "training_acc": 49.0, "val_loss": 13387.457275390625, "val_acc": 48.0}
{"epoch": 25, "training_loss": 83439.90625, "training_acc": 49.0, "val_loss": 42899.53918457031, "val_acc": 52.0}
{"epoch": 26, "training_loss": 136175.45178222656, "training_acc": 53.0, "val_loss": 41510.20202636719, "val_acc": 48.0}
{"epoch": 27, "training_loss": 177227.4052734375, "training_acc": 47.0, "val_loss": 20126.54266357422, "val_acc": 48.0}
{"epoch": 28, "training_loss": 125070.765625, "training_acc": 45.0, "val_loss": 62966.265869140625, "val_acc": 52.0}
{"epoch": 29, "training_loss": 225506.7900390625, "training_acc": 53.0, "val_loss": 913.3623123168945, "val_acc": 52.0}
{"epoch": 30, "training_loss": 91268.79809570312, "training_acc": 59.0, "val_loss": 116426.55029296875, "val_acc": 48.0}
{"epoch": 31, "training_loss": 468422.81640625, "training_acc": 47.0, "val_loss": 66655.09033203125, "val_acc": 48.0}
{"epoch": 32, "training_loss": 185634.54846191406, "training_acc": 51.0, "val_loss": 47824.127197265625, "val_acc": 52.0}
{"epoch": 33, "training_loss": 198188.2705078125, "training_acc": 53.0, "val_loss": 14118.470764160156, "val_acc": 52.0}
{"epoch": 34, "training_loss": 120842.4052734375, "training_acc": 51.0, "val_loss": 85054.38842773438, "val_acc": 48.0}
{"epoch": 35, "training_loss": 324237.3076171875, "training_acc": 47.0, "val_loss": 14793.222045898438, "val_acc": 48.0}
{"epoch": 36, "training_loss": 146210.5302734375, "training_acc": 47.0, "val_loss": 112355.322265625, "val_acc": 52.0}
{"epoch": 37, "training_loss": 439302.6923828125, "training_acc": 53.0, "val_loss": 87915.22216796875, "val_acc": 52.0}
{"epoch": 38, "training_loss": 277605.4387207031, "training_acc": 53.0, "val_loss": 51811.810302734375, "val_acc": 48.0}
{"epoch": 39, "training_loss": 255764.0986328125, "training_acc": 47.0, "val_loss": 81021.39892578125, "val_acc": 48.0}
{"epoch": 40, "training_loss": 269316.568359375, "training_acc": 47.0, "val_loss": 39775.5615234375, "val_acc": 52.0}
{"epoch": 41, "training_loss": 209582.5361328125, "training_acc": 53.0, "val_loss": 67660.72387695312, "val_acc": 52.0}
{"epoch": 42, "training_loss": 205855.29443359375, "training_acc": 53.0, "val_loss": 39928.83605957031, "val_acc": 48.0}
{"epoch": 43, "training_loss": 184651.41015625, "training_acc": 47.0, "val_loss": 54429.443359375, "val_acc": 48.0}
{"epoch": 44, "training_loss": 155929.1982421875, "training_acc": 47.0, "val_loss": 67920.03173828125, "val_acc": 52.0}
{"epoch": 45, "training_loss": 294730.9443359375, "training_acc": 53.0, "val_loss": 99583.79516601562, "val_acc": 52.0}
{"epoch": 46, "training_loss": 345429.5419921875, "training_acc": 53.0, "val_loss": 10313.652038574219, "val_acc": 52.0}
{"epoch": 47, "training_loss": 177982.021484375, "training_acc": 45.0, "val_loss": 131990.56396484375, "val_acc": 48.0}
{"epoch": 48, "training_loss": 532953.810546875, "training_acc": 47.0, "val_loss": 90894.4091796875, "val_acc": 48.0}
