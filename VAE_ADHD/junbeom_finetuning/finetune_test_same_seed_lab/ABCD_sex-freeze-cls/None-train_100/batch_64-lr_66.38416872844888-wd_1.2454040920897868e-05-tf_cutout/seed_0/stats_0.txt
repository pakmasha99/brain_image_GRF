"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 540188.5748748779, "training_acc": 50.0, "val_loss": 104450.732421875, "val_acc": 44.0}
{"epoch": 1, "training_loss": 556799.80859375, "training_acc": 48.0, "val_loss": 266205.0537109375, "val_acc": 56.0}
{"epoch": 2, "training_loss": 1134445.24609375, "training_acc": 52.0, "val_loss": 162512.841796875, "val_acc": 56.0}
{"epoch": 3, "training_loss": 484765.01416015625, "training_acc": 52.0, "val_loss": 170586.51123046875, "val_acc": 44.0}
{"epoch": 4, "training_loss": 775920.1875, "training_acc": 48.0, "val_loss": 296525.6103515625, "val_acc": 44.0}
{"epoch": 5, "training_loss": 1044914.626953125, "training_acc": 48.0, "val_loss": 154136.90185546875, "val_acc": 44.0}
{"epoch": 6, "training_loss": 386790.7219238281, "training_acc": 48.0, "val_loss": 122615.00244140625, "val_acc": 56.0}
{"epoch": 7, "training_loss": 649915.033203125, "training_acc": 52.0, "val_loss": 236191.6748046875, "val_acc": 56.0}
{"epoch": 8, "training_loss": 1027072.25390625, "training_acc": 52.0, "val_loss": 188907.04345703125, "val_acc": 56.0}
{"epoch": 9, "training_loss": 702068.712890625, "training_acc": 52.0, "val_loss": 11732.698822021484, "val_acc": 56.0}
{"epoch": 10, "training_loss": 229665.041015625, "training_acc": 50.0, "val_loss": 239100.3173828125, "val_acc": 44.0}
{"epoch": 11, "training_loss": 934546.140625, "training_acc": 48.0, "val_loss": 264225.6103515625, "val_acc": 44.0}
{"epoch": 12, "training_loss": 911486.689453125, "training_acc": 48.0, "val_loss": 115069.5068359375, "val_acc": 44.0}
{"epoch": 13, "training_loss": 273813.11779785156, "training_acc": 52.0, "val_loss": 65632.92236328125, "val_acc": 56.0}
{"epoch": 14, "training_loss": 313687.8056640625, "training_acc": 52.0, "val_loss": 71727.42309570312, "val_acc": 56.0}
{"epoch": 15, "training_loss": 251255.1328125, "training_acc": 52.0, "val_loss": 53216.241455078125, "val_acc": 44.0}
{"epoch": 16, "training_loss": 240336.404296875, "training_acc": 48.0, "val_loss": 74766.29638671875, "val_acc": 44.0}
{"epoch": 17, "training_loss": 205333.3095703125, "training_acc": 48.0, "val_loss": 54634.86328125, "val_acc": 56.0}
{"epoch": 18, "training_loss": 289337.5126953125, "training_acc": 52.0, "val_loss": 86675.10986328125, "val_acc": 56.0}
{"epoch": 19, "training_loss": 327431.423828125, "training_acc": 52.0, "val_loss": 5683.392333984375, "val_acc": 44.0}
{"epoch": 20, "training_loss": 60598.75830078125, "training_acc": 48.0, "val_loss": 13011.090087890625, "val_acc": 44.0}
{"epoch": 21, "training_loss": 76686.12060546875, "training_acc": 54.0, "val_loss": 49353.68957519531, "val_acc": 56.0}
{"epoch": 22, "training_loss": 187556.20556640625, "training_acc": 52.0, "val_loss": 19295.179748535156, "val_acc": 44.0}
{"epoch": 23, "training_loss": 85826.67431640625, "training_acc": 48.0, "val_loss": 3405.542755126953, "val_acc": 44.0}
{"epoch": 24, "training_loss": 97494.2119140625, "training_acc": 46.0, "val_loss": 66779.8828125, "val_acc": 56.0}
{"epoch": 25, "training_loss": 265183.0927734375, "training_acc": 52.0, "val_loss": 4334.9456787109375, "val_acc": 56.0}
{"epoch": 26, "training_loss": 81481.40478515625, "training_acc": 62.0, "val_loss": 122469.384765625, "val_acc": 44.0}
{"epoch": 27, "training_loss": 451300.642578125, "training_acc": 48.0, "val_loss": 77100.78735351562, "val_acc": 44.0}
{"epoch": 28, "training_loss": 171385.0937652588, "training_acc": 54.0, "val_loss": 29098.846435546875, "val_acc": 56.0}
{"epoch": 29, "training_loss": 117746.18017578125, "training_acc": 52.0, "val_loss": 14474.349975585938, "val_acc": 44.0}
{"epoch": 30, "training_loss": 44341.4599609375, "training_acc": 48.0, "val_loss": 27292.10205078125, "val_acc": 56.0}
{"epoch": 31, "training_loss": 111174.0771484375, "training_acc": 52.0, "val_loss": 10916.973114013672, "val_acc": 44.0}
{"epoch": 32, "training_loss": 27673.112503051758, "training_acc": 48.0, "val_loss": 38100.299072265625, "val_acc": 56.0}
{"epoch": 33, "training_loss": 170687.31787109375, "training_acc": 52.0, "val_loss": 15504.278564453125, "val_acc": 56.0}
{"epoch": 34, "training_loss": 97312.873046875, "training_acc": 54.0, "val_loss": 67781.51245117188, "val_acc": 44.0}
{"epoch": 35, "training_loss": 220365.38525390625, "training_acc": 48.0, "val_loss": 14681.321716308594, "val_acc": 56.0}
{"epoch": 36, "training_loss": 82844.7919921875, "training_acc": 52.0, "val_loss": 10818.021392822266, "val_acc": 56.0}
{"epoch": 37, "training_loss": 116194.5576171875, "training_acc": 42.0, "val_loss": 55131.591796875, "val_acc": 44.0}
{"epoch": 38, "training_loss": 152095.63330078125, "training_acc": 48.0, "val_loss": 44660.21728515625, "val_acc": 56.0}
{"epoch": 39, "training_loss": 247429.1396484375, "training_acc": 52.0, "val_loss": 64338.41552734375, "val_acc": 56.0}
{"epoch": 40, "training_loss": 220527.18017578125, "training_acc": 52.0, "val_loss": 50347.479248046875, "val_acc": 44.0}
{"epoch": 41, "training_loss": 231119.5, "training_acc": 48.0, "val_loss": 70072.72338867188, "val_acc": 44.0}
{"epoch": 42, "training_loss": 194943.0577392578, "training_acc": 48.0, "val_loss": 57963.470458984375, "val_acc": 56.0}
