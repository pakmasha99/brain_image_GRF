"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 16709473.578781128, "training_acc": 50.0, "val_loss": 2929600.390625, "val_acc": 44.0}
{"epoch": 1, "training_loss": 16971998.375, "training_acc": 48.0, "val_loss": 8380301.5625, "val_acc": 56.0}
{"epoch": 2, "training_loss": 35373187.875, "training_acc": 52.0, "val_loss": 4779062.890625, "val_acc": 56.0}
{"epoch": 3, "training_loss": 13710906.484375, "training_acc": 52.0, "val_loss": 5761569.53125, "val_acc": 44.0}
{"epoch": 4, "training_loss": 25610209.125, "training_acc": 48.0, "val_loss": 9298528.125, "val_acc": 44.0}
{"epoch": 5, "training_loss": 32530261.0625, "training_acc": 48.0, "val_loss": 4481084.765625, "val_acc": 44.0}
{"epoch": 6, "training_loss": 10751057.625, "training_acc": 48.0, "val_loss": 4107109.765625, "val_acc": 56.0}
{"epoch": 7, "training_loss": 21328638.8125, "training_acc": 52.0, "val_loss": 7361717.96875, "val_acc": 56.0}
{"epoch": 8, "training_loss": 31668836.125, "training_acc": 52.0, "val_loss": 5544354.6875, "val_acc": 56.0}
{"epoch": 9, "training_loss": 20189603.3125, "training_acc": 52.0, "val_loss": 188103.80859375, "val_acc": 44.0}
{"epoch": 10, "training_loss": 3565285.0, "training_acc": 48.0, "val_loss": 2661791.40625, "val_acc": 44.0}
{"epoch": 11, "training_loss": 8408494.734375, "training_acc": 48.0, "val_loss": 655128.61328125, "val_acc": 56.0}
{"epoch": 12, "training_loss": 3985817.71875, "training_acc": 52.0, "val_loss": 768342.236328125, "val_acc": 56.0}
{"epoch": 13, "training_loss": 3580127.8125, "training_acc": 48.0, "val_loss": 1155468.45703125, "val_acc": 44.0}
{"epoch": 14, "training_loss": 2696310.60546875, "training_acc": 48.0, "val_loss": 1852662.5, "val_acc": 56.0}
{"epoch": 15, "training_loss": 8905866.625, "training_acc": 52.0, "val_loss": 2140033.7890625, "val_acc": 56.0}
{"epoch": 16, "training_loss": 7337394.71875, "training_acc": 52.0, "val_loss": 1470228.80859375, "val_acc": 44.0}
{"epoch": 17, "training_loss": 6803265.15625, "training_acc": 48.0, "val_loss": 1989539.2578125, "val_acc": 44.0}
{"epoch": 18, "training_loss": 5055432.19140625, "training_acc": 48.0, "val_loss": 2006588.4765625, "val_acc": 56.0}
{"epoch": 19, "training_loss": 10422917.8125, "training_acc": 52.0, "val_loss": 2924387.6953125, "val_acc": 56.0}
{"epoch": 20, "training_loss": 11194416.90625, "training_acc": 52.0, "val_loss": 22261.585998535156, "val_acc": 56.0}
{"epoch": 21, "training_loss": 5004479.53125, "training_acc": 46.0, "val_loss": 4734473.828125, "val_acc": 44.0}
{"epoch": 22, "training_loss": 17410120.0, "training_acc": 48.0, "val_loss": 3346529.296875, "val_acc": 44.0}
{"epoch": 23, "training_loss": 9454198.65625, "training_acc": 48.0, "val_loss": 1899258.59375, "val_acc": 56.0}
{"epoch": 24, "training_loss": 10710293.125, "training_acc": 52.0, "val_loss": 3496233.59375, "val_acc": 56.0}
{"epoch": 25, "training_loss": 14080185.59375, "training_acc": 52.0, "val_loss": 1106482.71484375, "val_acc": 56.0}
{"epoch": 26, "training_loss": 3755682.703125, "training_acc": 62.0, "val_loss": 2851707.6171875, "val_acc": 44.0}
{"epoch": 27, "training_loss": 10581362.09375, "training_acc": 48.0, "val_loss": 1594195.703125, "val_acc": 44.0}
{"epoch": 28, "training_loss": 4294201.0234375, "training_acc": 54.0, "val_loss": 1339720.703125, "val_acc": 56.0}
{"epoch": 29, "training_loss": 5407563.09375, "training_acc": 52.0, "val_loss": 127793.1396484375, "val_acc": 44.0}
{"epoch": 30, "training_loss": 475334.154296875, "training_acc": 40.0, "val_loss": 1247177.9296875, "val_acc": 44.0}
{"epoch": 31, "training_loss": 4697189.0, "training_acc": 48.0, "val_loss": 200357.5927734375, "val_acc": 56.0}
{"epoch": 32, "training_loss": 1127137.93359375, "training_acc": 52.0, "val_loss": 610542.626953125, "val_acc": 44.0}
{"epoch": 33, "training_loss": 1830959.20703125, "training_acc": 48.0, "val_loss": 1043344.7265625, "val_acc": 56.0}
{"epoch": 34, "training_loss": 4979617.015625, "training_acc": 52.0, "val_loss": 482855.908203125, "val_acc": 56.0}
{"epoch": 35, "training_loss": 3372589.578125, "training_acc": 50.0, "val_loss": 2019518.5546875, "val_acc": 44.0}
{"epoch": 36, "training_loss": 6402276.34375, "training_acc": 48.0, "val_loss": 619377.44140625, "val_acc": 56.0}
{"epoch": 37, "training_loss": 3113448.7578125, "training_acc": 52.0, "val_loss": 572789.84375, "val_acc": 56.0}
{"epoch": 38, "training_loss": 3559013.921875, "training_acc": 42.0, "val_loss": 1051981.73828125, "val_acc": 44.0}
{"epoch": 39, "training_loss": 3176841.625, "training_acc": 44.0, "val_loss": 146861.23046875, "val_acc": 56.0}
