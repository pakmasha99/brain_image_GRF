"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.2358512878418, "training_acc": 52.0, "val_loss": 17.23688840866089, "val_acc": 56.0}
{"epoch": 1, "training_loss": 69.23069095611572, "training_acc": 52.0, "val_loss": 17.22463220357895, "val_acc": 56.0}
{"epoch": 2, "training_loss": 69.24860000610352, "training_acc": 52.0, "val_loss": 17.22344160079956, "val_acc": 56.0}
{"epoch": 3, "training_loss": 69.26392388343811, "training_acc": 52.0, "val_loss": 17.230111360549927, "val_acc": 56.0}
{"epoch": 4, "training_loss": 69.239506483078, "training_acc": 52.0, "val_loss": 17.241016030311584, "val_acc": 56.0}
{"epoch": 5, "training_loss": 69.22336912155151, "training_acc": 52.0, "val_loss": 17.245033383369446, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.2562370300293, "training_acc": 52.0, "val_loss": 17.253795266151428, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.25172877311707, "training_acc": 52.0, "val_loss": 17.258480191230774, "val_acc": 56.0}
{"epoch": 8, "training_loss": 69.25679874420166, "training_acc": 52.0, "val_loss": 17.255820333957672, "val_acc": 56.0}
{"epoch": 9, "training_loss": 69.25251984596252, "training_acc": 52.0, "val_loss": 17.259277403354645, "val_acc": 56.0}
{"epoch": 10, "training_loss": 69.22961020469666, "training_acc": 52.0, "val_loss": 17.261573672294617, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.260089635849, "training_acc": 52.0, "val_loss": 17.26074367761612, "val_acc": 56.0}
{"epoch": 12, "training_loss": 69.2299907207489, "training_acc": 52.0, "val_loss": 17.259137332439423, "val_acc": 56.0}
{"epoch": 13, "training_loss": 69.23950695991516, "training_acc": 52.0, "val_loss": 17.25928783416748, "val_acc": 56.0}
{"epoch": 14, "training_loss": 69.24965143203735, "training_acc": 52.0, "val_loss": 17.255328595638275, "val_acc": 56.0}
{"epoch": 15, "training_loss": 69.25485372543335, "training_acc": 52.0, "val_loss": 17.2488272190094, "val_acc": 56.0}
{"epoch": 16, "training_loss": 69.25235867500305, "training_acc": 52.0, "val_loss": 17.238731682300568, "val_acc": 56.0}
{"epoch": 17, "training_loss": 69.23448157310486, "training_acc": 52.0, "val_loss": 17.23136156797409, "val_acc": 56.0}
{"epoch": 18, "training_loss": 69.24032258987427, "training_acc": 52.0, "val_loss": 17.224939167499542, "val_acc": 56.0}
{"epoch": 19, "training_loss": 69.25449657440186, "training_acc": 52.0, "val_loss": 17.219586670398712, "val_acc": 56.0}
{"epoch": 20, "training_loss": 69.20140147209167, "training_acc": 52.0, "val_loss": 17.21709668636322, "val_acc": 56.0}
{"epoch": 21, "training_loss": 69.2205240726471, "training_acc": 52.0, "val_loss": 17.214232683181763, "val_acc": 56.0}
{"epoch": 22, "training_loss": 69.28353452682495, "training_acc": 52.0, "val_loss": 17.20937341451645, "val_acc": 56.0}
{"epoch": 23, "training_loss": 69.25973892211914, "training_acc": 52.0, "val_loss": 17.21007078886032, "val_acc": 56.0}
{"epoch": 24, "training_loss": 69.26059126853943, "training_acc": 52.0, "val_loss": 17.213739454746246, "val_acc": 56.0}
{"epoch": 25, "training_loss": 69.24634647369385, "training_acc": 52.0, "val_loss": 17.22010225057602, "val_acc": 56.0}
{"epoch": 26, "training_loss": 69.27740597724915, "training_acc": 52.0, "val_loss": 17.222587764263153, "val_acc": 56.0}
{"epoch": 27, "training_loss": 69.22886347770691, "training_acc": 52.0, "val_loss": 17.232297360897064, "val_acc": 56.0}
{"epoch": 28, "training_loss": 69.23837780952454, "training_acc": 52.0, "val_loss": 17.24160760641098, "val_acc": 56.0}
{"epoch": 29, "training_loss": 69.25333619117737, "training_acc": 52.0, "val_loss": 17.24582463502884, "val_acc": 56.0}
{"epoch": 30, "training_loss": 69.25794339179993, "training_acc": 52.0, "val_loss": 17.24603772163391, "val_acc": 56.0}
{"epoch": 31, "training_loss": 69.26794123649597, "training_acc": 52.0, "val_loss": 17.252010107040405, "val_acc": 56.0}
{"epoch": 32, "training_loss": 69.24052214622498, "training_acc": 52.0, "val_loss": 17.25032776594162, "val_acc": 56.0}
{"epoch": 33, "training_loss": 69.2530779838562, "training_acc": 52.0, "val_loss": 17.249013483524323, "val_acc": 56.0}
{"epoch": 34, "training_loss": 69.25303649902344, "training_acc": 52.0, "val_loss": 17.245346307754517, "val_acc": 56.0}
{"epoch": 35, "training_loss": 69.23967385292053, "training_acc": 52.0, "val_loss": 17.243801057338715, "val_acc": 56.0}
{"epoch": 36, "training_loss": 69.2393524646759, "training_acc": 52.0, "val_loss": 17.241649329662323, "val_acc": 56.0}
{"epoch": 37, "training_loss": 69.23953199386597, "training_acc": 52.0, "val_loss": 17.238357663154602, "val_acc": 56.0}
{"epoch": 38, "training_loss": 69.2338981628418, "training_acc": 52.0, "val_loss": 17.231078445911407, "val_acc": 56.0}
{"epoch": 39, "training_loss": 69.24173879623413, "training_acc": 52.0, "val_loss": 17.22080558538437, "val_acc": 56.0}
{"epoch": 40, "training_loss": 69.22672963142395, "training_acc": 52.0, "val_loss": 17.21588522195816, "val_acc": 56.0}
{"epoch": 41, "training_loss": 69.2493667602539, "training_acc": 52.0, "val_loss": 17.21220761537552, "val_acc": 56.0}
