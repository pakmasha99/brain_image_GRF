"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.15153956413269, "training_acc": 53.0, "val_loss": 17.31962412595749, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.15895342826843, "training_acc": 53.0, "val_loss": 17.32085347175598, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.19331765174866, "training_acc": 53.0, "val_loss": 17.320087552070618, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.14674043655396, "training_acc": 53.0, "val_loss": 17.32504963874817, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.15666556358337, "training_acc": 53.0, "val_loss": 17.32545644044876, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.13083863258362, "training_acc": 53.0, "val_loss": 17.322242259979248, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.14317274093628, "training_acc": 53.0, "val_loss": 17.31875389814377, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.14313411712646, "training_acc": 53.0, "val_loss": 17.315810918807983, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.12563371658325, "training_acc": 53.0, "val_loss": 17.31458306312561, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.14656114578247, "training_acc": 53.0, "val_loss": 17.314492166042328, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.16014862060547, "training_acc": 53.0, "val_loss": 17.315475642681122, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.14395260810852, "training_acc": 53.0, "val_loss": 17.31448769569397, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.14417219161987, "training_acc": 53.0, "val_loss": 17.31448322534561, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.15565085411072, "training_acc": 53.0, "val_loss": 17.315320670604706, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.1701283454895, "training_acc": 53.0, "val_loss": 17.315423488616943, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.17277455329895, "training_acc": 53.0, "val_loss": 17.31879562139511, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.13039970397949, "training_acc": 53.0, "val_loss": 17.319031059741974, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.15898561477661, "training_acc": 53.0, "val_loss": 17.317427694797516, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.12559604644775, "training_acc": 53.0, "val_loss": 17.315246164798737, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.13403367996216, "training_acc": 53.0, "val_loss": 17.31223315000534, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.120121717453, "training_acc": 53.0, "val_loss": 17.311111092567444, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.19163537025452, "training_acc": 53.0, "val_loss": 17.31097251176834, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.16715979576111, "training_acc": 53.0, "val_loss": 17.31051206588745, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.16323566436768, "training_acc": 53.0, "val_loss": 17.310620844364166, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.1726303100586, "training_acc": 53.0, "val_loss": 17.310580611228943, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.18054580688477, "training_acc": 53.0, "val_loss": 17.310553789138794, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.16162061691284, "training_acc": 53.0, "val_loss": 17.310722172260284, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.14303612709045, "training_acc": 53.0, "val_loss": 17.312486469745636, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.16434931755066, "training_acc": 53.0, "val_loss": 17.314666509628296, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.13828992843628, "training_acc": 53.0, "val_loss": 17.31615662574768, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.16385102272034, "training_acc": 53.0, "val_loss": 17.316608130931854, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.16925024986267, "training_acc": 53.0, "val_loss": 17.31541007757187, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.12266898155212, "training_acc": 53.0, "val_loss": 17.312683165073395, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.20031332969666, "training_acc": 53.0, "val_loss": 17.31114536523819, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.14464068412781, "training_acc": 53.0, "val_loss": 17.311109602451324, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.15424394607544, "training_acc": 53.0, "val_loss": 17.31082648038864, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.17162728309631, "training_acc": 53.0, "val_loss": 17.311057448387146, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.12833881378174, "training_acc": 53.0, "val_loss": 17.31100231409073, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.13439345359802, "training_acc": 53.0, "val_loss": 17.31134057044983, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.14532113075256, "training_acc": 53.0, "val_loss": 17.31203943490982, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.15072679519653, "training_acc": 53.0, "val_loss": 17.31298416852951, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.1358253955841, "training_acc": 53.0, "val_loss": 17.312896251678467, "val_acc": 52.0}
