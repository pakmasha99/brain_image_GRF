"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.80144119262695, "training_acc": 50.0, "val_loss": 17.351633310317993, "val_acc": 56.0}
{"epoch": 1, "training_loss": 69.56708192825317, "training_acc": 48.0, "val_loss": 17.158187925815582, "val_acc": 56.0}
{"epoch": 2, "training_loss": 69.90903973579407, "training_acc": 52.0, "val_loss": 17.150503396987915, "val_acc": 56.0}
{"epoch": 3, "training_loss": 69.22889065742493, "training_acc": 52.0, "val_loss": 17.375165224075317, "val_acc": 56.0}
{"epoch": 4, "training_loss": 69.55307674407959, "training_acc": 48.0, "val_loss": 17.64376014471054, "val_acc": 56.0}
{"epoch": 5, "training_loss": 69.96905016899109, "training_acc": 48.0, "val_loss": 17.45796948671341, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.47433972358704, "training_acc": 48.0, "val_loss": 17.266719043254852, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.18009662628174, "training_acc": 52.0, "val_loss": 17.162467539310455, "val_acc": 56.0}
{"epoch": 8, "training_loss": 69.78616404533386, "training_acc": 52.0, "val_loss": 17.14749038219452, "val_acc": 56.0}
{"epoch": 9, "training_loss": 69.47737216949463, "training_acc": 52.0, "val_loss": 17.186343669891357, "val_acc": 56.0}
{"epoch": 10, "training_loss": 69.2602047920227, "training_acc": 52.0, "val_loss": 17.32669025659561, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.32754254341125, "training_acc": 53.0, "val_loss": 17.41946041584015, "val_acc": 56.0}
{"epoch": 12, "training_loss": 69.44792747497559, "training_acc": 48.0, "val_loss": 17.382486164569855, "val_acc": 56.0}
{"epoch": 13, "training_loss": 69.3645851612091, "training_acc": 48.0, "val_loss": 17.296481132507324, "val_acc": 56.0}
{"epoch": 14, "training_loss": 69.21598267555237, "training_acc": 52.0, "val_loss": 17.19076931476593, "val_acc": 56.0}
{"epoch": 15, "training_loss": 69.22797966003418, "training_acc": 52.0, "val_loss": 17.15276539325714, "val_acc": 56.0}
{"epoch": 16, "training_loss": 69.49638962745667, "training_acc": 52.0, "val_loss": 17.148201167583466, "val_acc": 56.0}
{"epoch": 17, "training_loss": 69.48393082618713, "training_acc": 52.0, "val_loss": 17.173422873020172, "val_acc": 56.0}
{"epoch": 18, "training_loss": 69.29675388336182, "training_acc": 52.0, "val_loss": 17.2652930021286, "val_acc": 56.0}
{"epoch": 19, "training_loss": 69.25348997116089, "training_acc": 52.0, "val_loss": 17.35118180513382, "val_acc": 56.0}
{"epoch": 20, "training_loss": 69.36438012123108, "training_acc": 48.0, "val_loss": 17.356601357460022, "val_acc": 56.0}
{"epoch": 21, "training_loss": 69.29345369338989, "training_acc": 47.0, "val_loss": 17.246834933757782, "val_acc": 56.0}
{"epoch": 22, "training_loss": 69.5459291934967, "training_acc": 52.0, "val_loss": 17.16172695159912, "val_acc": 56.0}
{"epoch": 23, "training_loss": 69.41562342643738, "training_acc": 52.0, "val_loss": 17.168407142162323, "val_acc": 56.0}
{"epoch": 24, "training_loss": 69.27580761909485, "training_acc": 52.0, "val_loss": 17.238937318325043, "val_acc": 56.0}
{"epoch": 25, "training_loss": 69.35402321815491, "training_acc": 51.0, "val_loss": 17.369811236858368, "val_acc": 56.0}
{"epoch": 26, "training_loss": 69.65077996253967, "training_acc": 38.0, "val_loss": 17.357821762561798, "val_acc": 56.0}
{"epoch": 27, "training_loss": 69.35227108001709, "training_acc": 48.0, "val_loss": 17.40843504667282, "val_acc": 56.0}
