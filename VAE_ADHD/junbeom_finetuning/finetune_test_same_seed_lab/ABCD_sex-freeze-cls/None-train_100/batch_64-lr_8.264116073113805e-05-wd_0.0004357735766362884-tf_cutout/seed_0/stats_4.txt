"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.15155029296875, "training_acc": 53.0, "val_loss": 17.319171130657196, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.15791916847229, "training_acc": 53.0, "val_loss": 17.320677638053894, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.19151020050049, "training_acc": 53.0, "val_loss": 17.32006072998047, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.1464250087738, "training_acc": 53.0, "val_loss": 17.324937880039215, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.15645146369934, "training_acc": 53.0, "val_loss": 17.32541024684906, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.13083219528198, "training_acc": 53.0, "val_loss": 17.32235848903656, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.14338493347168, "training_acc": 53.0, "val_loss": 17.318977415561676, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.14310503005981, "training_acc": 53.0, "val_loss": 17.31606423854828, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.12556791305542, "training_acc": 53.0, "val_loss": 17.31480360031128, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.14653182029724, "training_acc": 53.0, "val_loss": 17.31465458869934, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.15940928459167, "training_acc": 53.0, "val_loss": 17.31555461883545, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.14396548271179, "training_acc": 53.0, "val_loss": 17.314526438713074, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.1442186832428, "training_acc": 53.0, "val_loss": 17.314469814300537, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.15564727783203, "training_acc": 53.0, "val_loss": 17.31523424386978, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.16961765289307, "training_acc": 53.0, "val_loss": 17.31530725955963, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.17154788970947, "training_acc": 53.0, "val_loss": 17.318519949913025, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.12992548942566, "training_acc": 53.0, "val_loss": 17.318759858608246, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.15884065628052, "training_acc": 53.0, "val_loss": 17.31726825237274, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.12568163871765, "training_acc": 53.0, "val_loss": 17.315205931663513, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.13371396064758, "training_acc": 53.0, "val_loss": 17.312297224998474, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.11967206001282, "training_acc": 53.0, "val_loss": 17.311184108257294, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.19017148017883, "training_acc": 53.0, "val_loss": 17.311036586761475, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.16653108596802, "training_acc": 53.0, "val_loss": 17.310521006584167, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.16180109977722, "training_acc": 53.0, "val_loss": 17.310579121112823, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.17120361328125, "training_acc": 53.0, "val_loss": 17.310553789138794, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.1791639328003, "training_acc": 53.0, "val_loss": 17.31053739786148, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.16151523590088, "training_acc": 53.0, "val_loss": 17.310731112957, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.14210104942322, "training_acc": 53.0, "val_loss": 17.31240451335907, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.16386151313782, "training_acc": 53.0, "val_loss": 17.31444150209427, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.13794016838074, "training_acc": 53.0, "val_loss": 17.315837740898132, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.16316151618958, "training_acc": 53.0, "val_loss": 17.31627583503723, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.1688084602356, "training_acc": 53.0, "val_loss": 17.315176129341125, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.1229932308197, "training_acc": 53.0, "val_loss": 17.312614619731903, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.19837260246277, "training_acc": 53.0, "val_loss": 17.311154305934906, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.14454102516174, "training_acc": 53.0, "val_loss": 17.311125993728638, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.15350413322449, "training_acc": 53.0, "val_loss": 17.31085181236267, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.1710433959961, "training_acc": 53.0, "val_loss": 17.311085760593414, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.12793827056885, "training_acc": 53.0, "val_loss": 17.311033606529236, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.1342089176178, "training_acc": 53.0, "val_loss": 17.31136441230774, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.14521718025208, "training_acc": 53.0, "val_loss": 17.312045395374298, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.1498498916626, "training_acc": 53.0, "val_loss": 17.312952876091003, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.13577008247375, "training_acc": 53.0, "val_loss": 17.312869429588318, "val_acc": 52.0}
