"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 73439243.32878113, "training_acc": 50.0, "val_loss": 14241709.375, "val_acc": 44.0}
{"epoch": 1, "training_loss": 75736085.5, "training_acc": 48.0, "val_loss": 36172421.875, "val_acc": 56.0}
{"epoch": 2, "training_loss": 154194770.0, "training_acc": 52.0, "val_loss": 22126460.9375, "val_acc": 56.0}
{"epoch": 3, "training_loss": 66072829.1875, "training_acc": 52.0, "val_loss": 23128785.9375, "val_acc": 44.0}
{"epoch": 4, "training_loss": 105279521.0, "training_acc": 48.0, "val_loss": 40296800.0, "val_acc": 44.0}
{"epoch": 5, "training_loss": 142030834.5, "training_acc": 48.0, "val_loss": 20992346.875, "val_acc": 44.0}
{"epoch": 6, "training_loss": 52742097.1875, "training_acc": 48.0, "val_loss": 16629364.0625, "val_acc": 56.0}
{"epoch": 7, "training_loss": 88200160.5, "training_acc": 52.0, "val_loss": 32104218.75, "val_acc": 56.0}
{"epoch": 8, "training_loss": 139649631.0, "training_acc": 52.0, "val_loss": 25722859.375, "val_acc": 56.0}
{"epoch": 9, "training_loss": 95654737.0, "training_acc": 52.0, "val_loss": 1664537.890625, "val_acc": 56.0}
{"epoch": 10, "training_loss": 31299273.5, "training_acc": 50.0, "val_loss": 32433393.75, "val_acc": 44.0}
{"epoch": 11, "training_loss": 126814669.5, "training_acc": 48.0, "val_loss": 35901493.75, "val_acc": 44.0}
{"epoch": 12, "training_loss": 123873947.75, "training_acc": 48.0, "val_loss": 15669584.375, "val_acc": 44.0}
{"epoch": 13, "training_loss": 37242822.0625, "training_acc": 52.0, "val_loss": 8895950.78125, "val_acc": 56.0}
{"epoch": 14, "training_loss": 42538689.0, "training_acc": 52.0, "val_loss": 9739735.15625, "val_acc": 56.0}
{"epoch": 15, "training_loss": 34116664.8125, "training_acc": 52.0, "val_loss": 7238626.5625, "val_acc": 44.0}
{"epoch": 16, "training_loss": 32695421.625, "training_acc": 48.0, "val_loss": 10182732.03125, "val_acc": 44.0}
{"epoch": 17, "training_loss": 27992037.90625, "training_acc": 48.0, "val_loss": 7406992.96875, "val_acc": 56.0}
{"epoch": 18, "training_loss": 39254588.0, "training_acc": 52.0, "val_loss": 11777617.96875, "val_acc": 56.0}
{"epoch": 19, "training_loss": 44502398.625, "training_acc": 52.0, "val_loss": 763224.755859375, "val_acc": 44.0}
{"epoch": 20, "training_loss": 8204405.5625, "training_acc": 48.0, "val_loss": 1763465.625, "val_acc": 44.0}
{"epoch": 21, "training_loss": 10418550.5, "training_acc": 54.0, "val_loss": 6717129.6875, "val_acc": 56.0}
{"epoch": 22, "training_loss": 25539689.875, "training_acc": 52.0, "val_loss": 2603926.953125, "val_acc": 44.0}
{"epoch": 23, "training_loss": 11598851.0, "training_acc": 48.0, "val_loss": 448638.818359375, "val_acc": 44.0}
{"epoch": 24, "training_loss": 13238377.0, "training_acc": 46.0, "val_loss": 9095798.4375, "val_acc": 56.0}
{"epoch": 25, "training_loss": 36135593.75, "training_acc": 52.0, "val_loss": 617347.802734375, "val_acc": 56.0}
{"epoch": 26, "training_loss": 11108295.125, "training_acc": 62.0, "val_loss": 16622879.6875, "val_acc": 44.0}
{"epoch": 27, "training_loss": 61270111.25, "training_acc": 48.0, "val_loss": 10478989.84375, "val_acc": 44.0}
{"epoch": 28, "training_loss": 23288937.0546875, "training_acc": 54.0, "val_loss": 3953300.78125, "val_acc": 56.0}
{"epoch": 29, "training_loss": 15999961.0, "training_acc": 52.0, "val_loss": 1965233.7890625, "val_acc": 44.0}
{"epoch": 30, "training_loss": 6019600.078125, "training_acc": 48.0, "val_loss": 3711335.546875, "val_acc": 56.0}
{"epoch": 31, "training_loss": 15121501.15625, "training_acc": 52.0, "val_loss": 1477403.125, "val_acc": 44.0}
{"epoch": 32, "training_loss": 3737818.884765625, "training_acc": 48.0, "val_loss": 5184732.421875, "val_acc": 56.0}
{"epoch": 33, "training_loss": 23232347.125, "training_acc": 52.0, "val_loss": 2120545.8984375, "val_acc": 56.0}
{"epoch": 34, "training_loss": 13241873.3125, "training_acc": 54.0, "val_loss": 9202546.09375, "val_acc": 44.0}
{"epoch": 35, "training_loss": 29921072.3125, "training_acc": 48.0, "val_loss": 1997588.671875, "val_acc": 56.0}
{"epoch": 36, "training_loss": 11272286.0625, "training_acc": 52.0, "val_loss": 1476599.70703125, "val_acc": 56.0}
{"epoch": 37, "training_loss": 15801868.75, "training_acc": 42.0, "val_loss": 7492242.96875, "val_acc": 44.0}
{"epoch": 38, "training_loss": 20674455.125, "training_acc": 48.0, "val_loss": 6068770.3125, "val_acc": 56.0}
{"epoch": 39, "training_loss": 33634638.25, "training_acc": 52.0, "val_loss": 8756200.78125, "val_acc": 56.0}
{"epoch": 40, "training_loss": 30030775.8125, "training_acc": 52.0, "val_loss": 6822962.5, "val_acc": 44.0}
{"epoch": 41, "training_loss": 31346262.125, "training_acc": 48.0, "val_loss": 9518333.59375, "val_acc": 44.0}
{"epoch": 42, "training_loss": 26478956.578125, "training_acc": 48.0, "val_loss": 7881328.125, "val_acc": 56.0}
