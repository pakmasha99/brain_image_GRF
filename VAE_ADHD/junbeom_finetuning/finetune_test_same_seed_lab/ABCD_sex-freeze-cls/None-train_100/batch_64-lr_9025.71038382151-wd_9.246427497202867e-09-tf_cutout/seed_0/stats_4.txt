"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 62102924.40717316, "training_acc": 51.0, "val_loss": 13669381.25, "val_acc": 52.0}
{"epoch": 1, "training_loss": 67785269.75, "training_acc": 55.0, "val_loss": 37337756.25, "val_acc": 48.0}
{"epoch": 2, "training_loss": 139711687.5, "training_acc": 47.0, "val_loss": 9484437.5, "val_acc": 48.0}
{"epoch": 3, "training_loss": 42423233.25, "training_acc": 53.0, "val_loss": 27608015.625, "val_acc": 52.0}
{"epoch": 4, "training_loss": 112712483.0, "training_acc": 53.0, "val_loss": 24604975.0, "val_acc": 52.0}
{"epoch": 5, "training_loss": 79354458.5, "training_acc": 53.0, "val_loss": 4757105.078125, "val_acc": 48.0}
{"epoch": 6, "training_loss": 36468784.5, "training_acc": 47.0, "val_loss": 12565519.53125, "val_acc": 48.0}
{"epoch": 7, "training_loss": 40337405.40625, "training_acc": 47.0, "val_loss": 9052889.0625, "val_acc": 52.0}
{"epoch": 8, "training_loss": 43290238.25, "training_acc": 53.0, "val_loss": 14086414.0625, "val_acc": 52.0}
{"epoch": 9, "training_loss": 48252172.3125, "training_acc": 53.0, "val_loss": 2189192.96875, "val_acc": 48.0}
{"epoch": 10, "training_loss": 13354377.375, "training_acc": 47.0, "val_loss": 1766917.3828125, "val_acc": 48.0}
{"epoch": 11, "training_loss": 14250408.375, "training_acc": 49.0, "val_loss": 9378050.0, "val_acc": 52.0}
{"epoch": 12, "training_loss": 33161090.5, "training_acc": 53.0, "val_loss": 320031.1279296875, "val_acc": 48.0}
{"epoch": 13, "training_loss": 3591061.40625, "training_acc": 47.0, "val_loss": 2722066.6015625, "val_acc": 52.0}
{"epoch": 14, "training_loss": 9202802.203125, "training_acc": 53.0, "val_loss": 3829146.09375, "val_acc": 48.0}
{"epoch": 15, "training_loss": 14181824.0, "training_acc": 47.0, "val_loss": 3217945.5078125, "val_acc": 52.0}
{"epoch": 16, "training_loss": 13701105.8125, "training_acc": 53.0, "val_loss": 321232.6904296875, "val_acc": 48.0}
{"epoch": 17, "training_loss": 2384111.75, "training_acc": 49.0, "val_loss": 2463886.1328125, "val_acc": 48.0}
{"epoch": 18, "training_loss": 6652559.77734375, "training_acc": 47.0, "val_loss": 6898667.96875, "val_acc": 52.0}
{"epoch": 19, "training_loss": 29192208.625, "training_acc": 53.0, "val_loss": 5718157.03125, "val_acc": 52.0}
{"epoch": 20, "training_loss": 17250468.5, "training_acc": 49.0, "val_loss": 2826378.515625, "val_acc": 48.0}
{"epoch": 21, "training_loss": 8943463.75, "training_acc": 51.0, "val_loss": 606219.53125, "val_acc": 52.0}
{"epoch": 22, "training_loss": 7416383.6875, "training_acc": 55.0, "val_loss": 4409257.421875, "val_acc": 48.0}
{"epoch": 23, "training_loss": 12555280.734375, "training_acc": 51.0, "val_loss": 1349036.23046875, "val_acc": 52.0}
{"epoch": 24, "training_loss": 8487572.5, "training_acc": 49.0, "val_loss": 1729730.6640625, "val_acc": 48.0}
{"epoch": 25, "training_loss": 11224771.125, "training_acc": 49.0, "val_loss": 5918375.78125, "val_acc": 52.0}
{"epoch": 26, "training_loss": 18854575.953125, "training_acc": 53.0, "val_loss": 5544809.375, "val_acc": 48.0}
{"epoch": 27, "training_loss": 23698074.8125, "training_acc": 47.0, "val_loss": 2646512.6953125, "val_acc": 48.0}
{"epoch": 28, "training_loss": 16884297.375, "training_acc": 45.0, "val_loss": 8646542.1875, "val_acc": 52.0}
{"epoch": 29, "training_loss": 31005035.75, "training_acc": 53.0, "val_loss": 221087.6708984375, "val_acc": 52.0}
{"epoch": 30, "training_loss": 12497596.21875, "training_acc": 59.0, "val_loss": 15732310.9375, "val_acc": 48.0}
{"epoch": 31, "training_loss": 63309661.25, "training_acc": 47.0, "val_loss": 8988430.46875, "val_acc": 48.0}
{"epoch": 32, "training_loss": 25133936.578125, "training_acc": 51.0, "val_loss": 6565678.125, "val_acc": 52.0}
{"epoch": 33, "training_loss": 27203227.875, "training_acc": 53.0, "val_loss": 1993229.6875, "val_acc": 52.0}
{"epoch": 34, "training_loss": 16495145.625, "training_acc": 51.0, "val_loss": 11488722.65625, "val_acc": 48.0}
{"epoch": 35, "training_loss": 43790638.5, "training_acc": 47.0, "val_loss": 1951686.71875, "val_acc": 48.0}
{"epoch": 36, "training_loss": 19799007.0, "training_acc": 47.0, "val_loss": 15336840.625, "val_acc": 52.0}
{"epoch": 37, "training_loss": 59977404.5, "training_acc": 53.0, "val_loss": 12036551.5625, "val_acc": 52.0}
{"epoch": 38, "training_loss": 38080484.75, "training_acc": 53.0, "val_loss": 6941036.71875, "val_acc": 48.0}
{"epoch": 39, "training_loss": 34359958.75, "training_acc": 47.0, "val_loss": 10926359.375, "val_acc": 48.0}
{"epoch": 40, "training_loss": 36262985.0625, "training_acc": 47.0, "val_loss": 5479734.765625, "val_acc": 52.0}
{"epoch": 41, "training_loss": 28782267.125, "training_acc": 53.0, "val_loss": 9282515.625, "val_acc": 52.0}
{"epoch": 42, "training_loss": 28325878.0, "training_acc": 53.0, "val_loss": 5327492.96875, "val_acc": 48.0}
{"epoch": 43, "training_loss": 24697044.0, "training_acc": 47.0, "val_loss": 7309321.09375, "val_acc": 48.0}
{"epoch": 44, "training_loss": 20837205.3984375, "training_acc": 47.0, "val_loss": 9312789.0625, "val_acc": 52.0}
{"epoch": 45, "training_loss": 40384926.625, "training_acc": 53.0, "val_loss": 13634478.125, "val_acc": 52.0}
{"epoch": 46, "training_loss": 47353050.125, "training_acc": 53.0, "val_loss": 1514559.47265625, "val_acc": 52.0}
{"epoch": 47, "training_loss": 24299401.25, "training_acc": 45.0, "val_loss": 17831820.3125, "val_acc": 48.0}
{"epoch": 48, "training_loss": 72013954.25, "training_acc": 47.0, "val_loss": 12270593.75, "val_acc": 48.0}
