"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 85052798.57525635, "training_acc": 45.0, "val_loss": 15404015.625, "val_acc": 48.0}
{"epoch": 1, "training_loss": 57784539.25, "training_acc": 59.0, "val_loss": 40829812.5, "val_acc": 52.0}
{"epoch": 2, "training_loss": 157938966.25, "training_acc": 53.0, "val_loss": 30059156.25, "val_acc": 52.0}
{"epoch": 3, "training_loss": 95715469.875, "training_acc": 53.0, "val_loss": 8983679.6875, "val_acc": 48.0}
{"epoch": 4, "training_loss": 48328292.25, "training_acc": 47.0, "val_loss": 17468096.875, "val_acc": 48.0}
{"epoch": 5, "training_loss": 57452466.625, "training_acc": 47.0, "val_loss": 5998738.28125, "val_acc": 52.0}
{"epoch": 6, "training_loss": 33565171.25, "training_acc": 53.0, "val_loss": 12181867.96875, "val_acc": 52.0}
{"epoch": 7, "training_loss": 39794721.0625, "training_acc": 53.0, "val_loss": 5762736.71875, "val_acc": 48.0}
{"epoch": 8, "training_loss": 29634251.375, "training_acc": 47.0, "val_loss": 5683525.78125, "val_acc": 48.0}
{"epoch": 9, "training_loss": 19291282.1875, "training_acc": 51.0, "val_loss": 6669024.21875, "val_acc": 52.0}
{"epoch": 10, "training_loss": 22734072.03125, "training_acc": 53.0, "val_loss": 3274657.6171875, "val_acc": 48.0}
{"epoch": 11, "training_loss": 13425187.9375, "training_acc": 47.0, "val_loss": 1821564.2578125, "val_acc": 52.0}
{"epoch": 12, "training_loss": 6255975.953125, "training_acc": 53.0, "val_loss": 4349607.8125, "val_acc": 48.0}
{"epoch": 13, "training_loss": 16859801.875, "training_acc": 47.0, "val_loss": 2812342.96875, "val_acc": 52.0}
{"epoch": 14, "training_loss": 12376490.375, "training_acc": 53.0, "val_loss": 100803.076171875, "val_acc": 48.0}
{"epoch": 15, "training_loss": 1719386.5, "training_acc": 55.0, "val_loss": 1497579.8828125, "val_acc": 48.0}
{"epoch": 16, "training_loss": 6525860.90625, "training_acc": 47.0, "val_loss": 580217.236328125, "val_acc": 48.0}
{"epoch": 17, "training_loss": 5346078.5, "training_acc": 49.0, "val_loss": 993521.875, "val_acc": 52.0}
{"epoch": 18, "training_loss": 11349548.75, "training_acc": 49.0, "val_loss": 5940166.40625, "val_acc": 48.0}
{"epoch": 19, "training_loss": 16583904.7890625, "training_acc": 47.0, "val_loss": 8986362.5, "val_acc": 52.0}
{"epoch": 20, "training_loss": 41898195.5, "training_acc": 53.0, "val_loss": 11949237.5, "val_acc": 52.0}
{"epoch": 21, "training_loss": 38548821.25, "training_acc": 53.0, "val_loss": 3563413.28125, "val_acc": 48.0}
{"epoch": 22, "training_loss": 21456644.0, "training_acc": 47.0, "val_loss": 5674073.828125, "val_acc": 48.0}
{"epoch": 23, "training_loss": 17914269.15625, "training_acc": 47.0, "val_loss": 3307826.5625, "val_acc": 52.0}
{"epoch": 24, "training_loss": 8628542.578125, "training_acc": 53.0, "val_loss": 7556885.15625, "val_acc": 48.0}
{"epoch": 25, "training_loss": 33121761.5, "training_acc": 47.0, "val_loss": 5160155.46875, "val_acc": 48.0}
{"epoch": 26, "training_loss": 19426634.3125, "training_acc": 47.0, "val_loss": 6581917.1875, "val_acc": 52.0}
{"epoch": 27, "training_loss": 23145919.9375, "training_acc": 53.0, "val_loss": 1470592.1875, "val_acc": 48.0}
{"epoch": 28, "training_loss": 6123732.046875, "training_acc": 47.0, "val_loss": 3180813.671875, "val_acc": 52.0}
{"epoch": 29, "training_loss": 12106893.15625, "training_acc": 53.0, "val_loss": 1886763.28125, "val_acc": 48.0}
{"epoch": 30, "training_loss": 6603694.59375, "training_acc": 47.0, "val_loss": 4711095.703125, "val_acc": 52.0}
{"epoch": 31, "training_loss": 19900256.1875, "training_acc": 53.0, "val_loss": 1848463.8671875, "val_acc": 52.0}
{"epoch": 32, "training_loss": 13264644.125, "training_acc": 53.0, "val_loss": 8775225.0, "val_acc": 48.0}
{"epoch": 33, "training_loss": 31051253.6875, "training_acc": 47.0, "val_loss": 2511979.4921875, "val_acc": 52.0}
