"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 913.4986038208008, "training_acc": 50.0, "val_loss": 167.73775815963745, "val_acc": 44.0}
{"epoch": 1, "training_loss": 896.6756019592285, "training_acc": 48.0, "val_loss": 429.8037052154541, "val_acc": 56.0}
{"epoch": 2, "training_loss": 1832.356674194336, "training_acc": 52.0, "val_loss": 263.3234739303589, "val_acc": 56.0}
{"epoch": 3, "training_loss": 788.0303936004639, "training_acc": 52.0, "val_loss": 271.97792530059814, "val_acc": 44.0}
{"epoch": 4, "training_loss": 1238.3867263793945, "training_acc": 48.0, "val_loss": 473.32797050476074, "val_acc": 44.0}
{"epoch": 5, "training_loss": 1666.1627616882324, "training_acc": 48.0, "val_loss": 242.29233264923096, "val_acc": 44.0}
{"epoch": 6, "training_loss": 606.6162190437317, "training_acc": 48.0, "val_loss": 184.60978269577026, "val_acc": 56.0}
{"epoch": 7, "training_loss": 964.7818336486816, "training_acc": 52.0, "val_loss": 332.77318477630615, "val_acc": 56.0}
{"epoch": 8, "training_loss": 1415.4284439086914, "training_acc": 52.0, "val_loss": 224.89013671875, "val_acc": 56.0}
{"epoch": 9, "training_loss": 760.2529048919678, "training_acc": 52.0, "val_loss": 113.83914947509766, "val_acc": 44.0}
{"epoch": 10, "training_loss": 592.0053405761719, "training_acc": 48.0, "val_loss": 272.0712184906006, "val_acc": 44.0}
{"epoch": 11, "training_loss": 949.4400367736816, "training_acc": 48.0, "val_loss": 111.58758401870728, "val_acc": 44.0}
{"epoch": 12, "training_loss": 368.66569805145264, "training_acc": 46.0, "val_loss": 119.33181285858154, "val_acc": 56.0}
{"epoch": 13, "training_loss": 529.4107933044434, "training_acc": 52.0, "val_loss": 82.4083924293518, "val_acc": 56.0}
{"epoch": 14, "training_loss": 269.89242482185364, "training_acc": 48.0, "val_loss": 82.50182867050171, "val_acc": 44.0}
{"epoch": 15, "training_loss": 300.9716796875, "training_acc": 48.0, "val_loss": 17.1609029173851, "val_acc": 56.0}
{"epoch": 16, "training_loss": 140.17844486236572, "training_acc": 52.0, "val_loss": 55.43025732040405, "val_acc": 56.0}
{"epoch": 17, "training_loss": 181.09602808952332, "training_acc": 52.0, "val_loss": 61.34999990463257, "val_acc": 44.0}
{"epoch": 18, "training_loss": 210.14337253570557, "training_acc": 48.0, "val_loss": 27.98161208629608, "val_acc": 56.0}
{"epoch": 19, "training_loss": 144.3838653564453, "training_acc": 52.0, "val_loss": 17.19399243593216, "val_acc": 56.0}
{"epoch": 20, "training_loss": 118.26974868774414, "training_acc": 48.0, "val_loss": 30.30160665512085, "val_acc": 44.0}
{"epoch": 21, "training_loss": 116.80972623825073, "training_acc": 54.0, "val_loss": 44.1798210144043, "val_acc": 56.0}
{"epoch": 22, "training_loss": 139.8869185447693, "training_acc": 52.0, "val_loss": 62.83412575721741, "val_acc": 44.0}
{"epoch": 23, "training_loss": 218.9636549949646, "training_acc": 48.0, "val_loss": 22.466115653514862, "val_acc": 56.0}
{"epoch": 24, "training_loss": 125.6072850227356, "training_acc": 52.0, "val_loss": 17.237049341201782, "val_acc": 56.0}
{"epoch": 25, "training_loss": 109.74836206436157, "training_acc": 48.0, "val_loss": 21.38904482126236, "val_acc": 44.0}
{"epoch": 26, "training_loss": 159.9042501449585, "training_acc": 38.0, "val_loss": 33.32587480545044, "val_acc": 56.0}
{"epoch": 27, "training_loss": 140.93048095703125, "training_acc": 52.0, "val_loss": 47.12027609348297, "val_acc": 44.0}
{"epoch": 28, "training_loss": 129.28378033638, "training_acc": 54.0, "val_loss": 39.08312916755676, "val_acc": 56.0}
{"epoch": 29, "training_loss": 142.59629225730896, "training_acc": 52.0, "val_loss": 43.75098943710327, "val_acc": 44.0}
{"epoch": 30, "training_loss": 145.3081705570221, "training_acc": 48.0, "val_loss": 25.643402338027954, "val_acc": 56.0}
{"epoch": 31, "training_loss": 103.27387499809265, "training_acc": 52.0, "val_loss": 20.650529861450195, "val_acc": 44.0}
{"epoch": 32, "training_loss": 76.28765487670898, "training_acc": 48.0, "val_loss": 17.596934735774994, "val_acc": 56.0}
{"epoch": 33, "training_loss": 74.82030653953552, "training_acc": 48.0, "val_loss": 17.250579595565796, "val_acc": 56.0}
{"epoch": 34, "training_loss": 72.3291962146759, "training_acc": 52.0, "val_loss": 23.020172119140625, "val_acc": 44.0}
