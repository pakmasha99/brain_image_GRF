"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.1515953540802, "training_acc": 53.0, "val_loss": 17.318420112133026, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.15630340576172, "training_acc": 53.0, "val_loss": 17.320334911346436, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.18851470947266, "training_acc": 53.0, "val_loss": 17.319953441619873, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.14582180976868, "training_acc": 53.0, "val_loss": 17.3246830701828, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.156001329422, "training_acc": 53.0, "val_loss": 17.325282096862793, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.13073778152466, "training_acc": 53.0, "val_loss": 17.322513461112976, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.14369320869446, "training_acc": 53.0, "val_loss": 17.31933504343033, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.14305520057678, "training_acc": 53.0, "val_loss": 17.31649488210678, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.125492811203, "training_acc": 53.0, "val_loss": 17.315198481082916, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.1464991569519, "training_acc": 53.0, "val_loss": 17.314960062503815, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.15825390815735, "training_acc": 53.0, "val_loss": 17.315727472305298, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.14397406578064, "training_acc": 53.0, "val_loss": 17.31463223695755, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.14427995681763, "training_acc": 53.0, "val_loss": 17.31448769569397, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.1556122303009, "training_acc": 53.0, "val_loss": 17.315125465393066, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.1688392162323, "training_acc": 53.0, "val_loss": 17.3151433467865, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.16948819160461, "training_acc": 53.0, "val_loss": 17.31809377670288, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.12916159629822, "training_acc": 53.0, "val_loss": 17.318332195281982, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.1585681438446, "training_acc": 53.0, "val_loss": 17.316994071006775, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.12572932243347, "training_acc": 53.0, "val_loss": 17.31511801481247, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.13321781158447, "training_acc": 53.0, "val_loss": 17.312385141849518, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.11904215812683, "training_acc": 53.0, "val_loss": 17.31128990650177, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.18786549568176, "training_acc": 53.0, "val_loss": 17.31114089488983, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.16559529304504, "training_acc": 53.0, "val_loss": 17.310550808906555, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.15960741043091, "training_acc": 53.0, "val_loss": 17.310531437397003, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.16902732849121, "training_acc": 53.0, "val_loss": 17.31051802635193, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.17687153816223, "training_acc": 53.0, "val_loss": 17.31051653623581, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.16127490997314, "training_acc": 53.0, "val_loss": 17.310744524002075, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.1406180858612, "training_acc": 53.0, "val_loss": 17.312277853488922, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.16307926177979, "training_acc": 53.0, "val_loss": 17.31410175561905, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.13741874694824, "training_acc": 53.0, "val_loss": 17.31535643339157, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.16206431388855, "training_acc": 53.0, "val_loss": 17.31576919555664, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.16805696487427, "training_acc": 53.0, "val_loss": 17.314809560775757, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.12355399131775, "training_acc": 53.0, "val_loss": 17.31250286102295, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.1952645778656, "training_acc": 53.0, "val_loss": 17.311163246631622, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.14437055587769, "training_acc": 53.0, "val_loss": 17.311149835586548, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.15237951278687, "training_acc": 53.0, "val_loss": 17.310886085033417, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.17010569572449, "training_acc": 53.0, "val_loss": 17.31112450361252, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.1273844242096, "training_acc": 53.0, "val_loss": 17.31107085943222, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.13397717475891, "training_acc": 53.0, "val_loss": 17.311395704746246, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.14507603645325, "training_acc": 53.0, "val_loss": 17.31204241514206, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.1484866142273, "training_acc": 53.0, "val_loss": 17.312894761562347, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.13571214675903, "training_acc": 53.0, "val_loss": 17.3128142952919, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.16038584709167, "training_acc": 53.0, "val_loss": 17.312346398830414, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.14891242980957, "training_acc": 53.0, "val_loss": 17.312762141227722, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.14418745040894, "training_acc": 53.0, "val_loss": 17.31298863887787, "val_acc": 52.0}
