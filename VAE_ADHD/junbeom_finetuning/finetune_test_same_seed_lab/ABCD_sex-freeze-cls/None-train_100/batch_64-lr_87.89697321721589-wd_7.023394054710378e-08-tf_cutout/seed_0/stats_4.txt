"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 604833.5087356567, "training_acc": 51.0, "val_loss": 133154.06494140625, "val_acc": 52.0}
{"epoch": 1, "training_loss": 660144.765625, "training_acc": 55.0, "val_loss": 363586.1083984375, "val_acc": 48.0}
{"epoch": 2, "training_loss": 1360510.65234375, "training_acc": 47.0, "val_loss": 92386.02905273438, "val_acc": 48.0}
{"epoch": 3, "training_loss": 413159.259765625, "training_acc": 53.0, "val_loss": 268845.80078125, "val_acc": 52.0}
{"epoch": 4, "training_loss": 1097625.203125, "training_acc": 53.0, "val_loss": 239644.3359375, "val_acc": 52.0}
{"epoch": 5, "training_loss": 772932.16015625, "training_acc": 53.0, "val_loss": 46265.850830078125, "val_acc": 48.0}
{"epoch": 6, "training_loss": 354908.529296875, "training_acc": 47.0, "val_loss": 122322.2900390625, "val_acc": 48.0}
{"epoch": 7, "training_loss": 392645.71923828125, "training_acc": 47.0, "val_loss": 88193.8720703125, "val_acc": 52.0}
{"epoch": 8, "training_loss": 421718.189453125, "training_acc": 53.0, "val_loss": 137230.6884765625, "val_acc": 52.0}
{"epoch": 9, "training_loss": 470111.48828125, "training_acc": 53.0, "val_loss": 21247.540283203125, "val_acc": 48.0}
{"epoch": 10, "training_loss": 129760.9951171875, "training_acc": 47.0, "val_loss": 17140.780639648438, "val_acc": 48.0}
{"epoch": 11, "training_loss": 138689.203125, "training_acc": 49.0, "val_loss": 91393.37158203125, "val_acc": 52.0}
{"epoch": 12, "training_loss": 323205.78515625, "training_acc": 53.0, "val_loss": 3032.596206665039, "val_acc": 48.0}
{"epoch": 13, "training_loss": 34629.5458984375, "training_acc": 47.0, "val_loss": 26584.9609375, "val_acc": 52.0}
{"epoch": 14, "training_loss": 89921.85607910156, "training_acc": 53.0, "val_loss": 37204.31823730469, "val_acc": 48.0}
{"epoch": 15, "training_loss": 137763.921875, "training_acc": 47.0, "val_loss": 31412.637329101562, "val_acc": 52.0}
{"epoch": 16, "training_loss": 133724.2275390625, "training_acc": 53.0, "val_loss": 3041.606903076172, "val_acc": 48.0}
{"epoch": 17, "training_loss": 23104.067504882812, "training_acc": 49.0, "val_loss": 23907.215881347656, "val_acc": 48.0}
{"epoch": 18, "training_loss": 64432.73327636719, "training_acc": 47.0, "val_loss": 67261.58447265625, "val_acc": 52.0}
{"epoch": 19, "training_loss": 284604.0947265625, "training_acc": 53.0, "val_loss": 55776.77001953125, "val_acc": 52.0}
{"epoch": 20, "training_loss": 168069.56762695312, "training_acc": 49.0, "val_loss": 27423.281860351562, "val_acc": 48.0}
{"epoch": 21, "training_loss": 86960.79858398438, "training_acc": 51.0, "val_loss": 5996.326065063477, "val_acc": 52.0}
{"epoch": 22, "training_loss": 72308.72998046875, "training_acc": 55.0, "val_loss": 42842.1875, "val_acc": 48.0}
{"epoch": 23, "training_loss": 122138.27917480469, "training_acc": 51.0, "val_loss": 13225.218200683594, "val_acc": 52.0}
{"epoch": 24, "training_loss": 82734.91259765625, "training_acc": 49.0, "val_loss": 16751.416015625, "val_acc": 48.0}
{"epoch": 25, "training_loss": 109188.5869140625, "training_acc": 49.0, "val_loss": 57724.884033203125, "val_acc": 52.0}
{"epoch": 26, "training_loss": 183967.49560546875, "training_acc": 53.0, "val_loss": 53895.654296875, "val_acc": 48.0}
{"epoch": 27, "training_loss": 230371.4228515625, "training_acc": 47.0, "val_loss": 25679.57763671875, "val_acc": 48.0}
{"epoch": 28, "training_loss": 164302.7939453125, "training_acc": 45.0, "val_loss": 84293.45092773438, "val_acc": 52.0}
{"epoch": 29, "training_loss": 302301.50390625, "training_acc": 53.0, "val_loss": 2253.7363052368164, "val_acc": 52.0}
{"epoch": 30, "training_loss": 121799.8662109375, "training_acc": 59.0, "val_loss": 153107.8369140625, "val_acc": 48.0}
{"epoch": 31, "training_loss": 616146.8828125, "training_acc": 47.0, "val_loss": 87455.78002929688, "val_acc": 48.0}
{"epoch": 32, "training_loss": 244656.4921875, "training_acc": 51.0, "val_loss": 64007.11669921875, "val_acc": 52.0}
{"epoch": 33, "training_loss": 265191.103515625, "training_acc": 53.0, "val_loss": 19488.7451171875, "val_acc": 52.0}
{"epoch": 34, "training_loss": 160706.8359375, "training_acc": 51.0, "val_loss": 111803.21044921875, "val_acc": 48.0}
{"epoch": 35, "training_loss": 426144.400390625, "training_acc": 47.0, "val_loss": 18942.552185058594, "val_acc": 48.0}
{"epoch": 36, "training_loss": 192727.1650390625, "training_acc": 47.0, "val_loss": 149422.94921875, "val_acc": 52.0}
{"epoch": 37, "training_loss": 584356.099609375, "training_acc": 53.0, "val_loss": 117306.06689453125, "val_acc": 52.0}
{"epoch": 38, "training_loss": 371201.9091796875, "training_acc": 53.0, "val_loss": 67486.73095703125, "val_acc": 48.0}
{"epoch": 39, "training_loss": 334179.1875, "training_acc": 47.0, "val_loss": 106311.767578125, "val_acc": 48.0}
{"epoch": 40, "training_loss": 352772.2880859375, "training_acc": 47.0, "val_loss": 53441.070556640625, "val_acc": 52.0}
{"epoch": 41, "training_loss": 280602.53125, "training_acc": 53.0, "val_loss": 90486.2060546875, "val_acc": 52.0}
{"epoch": 42, "training_loss": 276209.560546875, "training_acc": 53.0, "val_loss": 51774.725341796875, "val_acc": 48.0}
{"epoch": 43, "training_loss": 240080.1357421875, "training_acc": 47.0, "val_loss": 71085.04028320312, "val_acc": 48.0}
{"epoch": 44, "training_loss": 202536.2540283203, "training_acc": 47.0, "val_loss": 90776.40991210938, "val_acc": 52.0}
{"epoch": 45, "training_loss": 393623.39453125, "training_acc": 53.0, "val_loss": 132880.18798828125, "val_acc": 52.0}
{"epoch": 46, "training_loss": 461559.0, "training_acc": 53.0, "val_loss": 14868.020629882812, "val_acc": 52.0}
{"epoch": 47, "training_loss": 236746.09765625, "training_acc": 45.0, "val_loss": 173534.716796875, "val_acc": 48.0}
{"epoch": 48, "training_loss": 700833.654296875, "training_acc": 47.0, "val_loss": 119403.03955078125, "val_acc": 48.0}
