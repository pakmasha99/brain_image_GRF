"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 830759.6428489685, "training_acc": 53.0, "val_loss": 169243.59130859375, "val_acc": 52.0}
{"epoch": 1, "training_loss": 614305.822265625, "training_acc": 57.0, "val_loss": 307066.1865234375, "val_acc": 48.0}
{"epoch": 2, "training_loss": 1172666.38671875, "training_acc": 47.0, "val_loss": 93498.47412109375, "val_acc": 48.0}
{"epoch": 3, "training_loss": 406350.728515625, "training_acc": 51.0, "val_loss": 229372.4853515625, "val_acc": 52.0}
{"epoch": 4, "training_loss": 911870.52734375, "training_acc": 53.0, "val_loss": 182715.2099609375, "val_acc": 52.0}
{"epoch": 5, "training_loss": 540606.7041015625, "training_acc": 53.0, "val_loss": 99788.70239257812, "val_acc": 48.0}
{"epoch": 6, "training_loss": 523131.51171875, "training_acc": 47.0, "val_loss": 178561.58447265625, "val_acc": 48.0}
{"epoch": 7, "training_loss": 628044.578125, "training_acc": 47.0, "val_loss": 13333.1787109375, "val_acc": 52.0}
{"epoch": 8, "training_loss": 128734.1015625, "training_acc": 53.0, "val_loss": 54666.07666015625, "val_acc": 52.0}
{"epoch": 9, "training_loss": 136346.77737426758, "training_acc": 53.0, "val_loss": 776.3910293579102, "val_acc": 48.0}
{"epoch": 10, "training_loss": 50473.71789550781, "training_acc": 59.0, "val_loss": 58284.5947265625, "val_acc": 52.0}
{"epoch": 11, "training_loss": 175821.14807128906, "training_acc": 53.0, "val_loss": 72678.79638671875, "val_acc": 48.0}
{"epoch": 12, "training_loss": 319865.1259765625, "training_acc": 47.0, "val_loss": 50344.677734375, "val_acc": 48.0}
{"epoch": 13, "training_loss": 235227.875, "training_acc": 39.0, "val_loss": 68289.04418945312, "val_acc": 52.0}
{"epoch": 14, "training_loss": 231809.25, "training_acc": 53.0, "val_loss": 27142.95654296875, "val_acc": 48.0}
{"epoch": 15, "training_loss": 141946.6875, "training_acc": 47.0, "val_loss": 691.7799472808838, "val_acc": 52.0}
{"epoch": 16, "training_loss": 16104.536010742188, "training_acc": 47.0, "val_loss": 38528.62548828125, "val_acc": 52.0}
{"epoch": 17, "training_loss": 148178.66943359375, "training_acc": 53.0, "val_loss": 9848.52294921875, "val_acc": 48.0}
{"epoch": 18, "training_loss": 30560.35089111328, "training_acc": 47.0, "val_loss": 55257.904052734375, "val_acc": 52.0}
{"epoch": 19, "training_loss": 236097.3349609375, "training_acc": 53.0, "val_loss": 30288.983154296875, "val_acc": 52.0}
{"epoch": 20, "training_loss": 109397.203125, "training_acc": 61.0, "val_loss": 72394.54345703125, "val_acc": 48.0}
{"epoch": 21, "training_loss": 256003.9228515625, "training_acc": 47.0, "val_loss": 28990.789794921875, "val_acc": 52.0}
{"epoch": 22, "training_loss": 144351.52490234375, "training_acc": 53.0, "val_loss": 26842.486572265625, "val_acc": 52.0}
{"epoch": 23, "training_loss": 112499.65478515625, "training_acc": 55.0, "val_loss": 51426.727294921875, "val_acc": 48.0}
{"epoch": 24, "training_loss": 148579.630859375, "training_acc": 47.0, "val_loss": 73219.62280273438, "val_acc": 52.0}
{"epoch": 25, "training_loss": 323544.8935546875, "training_acc": 53.0, "val_loss": 91057.2265625, "val_acc": 52.0}
{"epoch": 26, "training_loss": 285695.99462890625, "training_acc": 53.0, "val_loss": 59779.522705078125, "val_acc": 48.0}
{"epoch": 27, "training_loss": 293555.2177734375, "training_acc": 47.0, "val_loss": 71932.06176757812, "val_acc": 48.0}
{"epoch": 28, "training_loss": 196653.92443847656, "training_acc": 47.0, "val_loss": 17717.727661132812, "val_acc": 52.0}
{"epoch": 29, "training_loss": 53193.03381347656, "training_acc": 55.0, "val_loss": 11535.972595214844, "val_acc": 52.0}
{"epoch": 30, "training_loss": 64545.5615234375, "training_acc": 45.0, "val_loss": 11556.04476928711, "val_acc": 52.0}
{"epoch": 31, "training_loss": 32933.92022705078, "training_acc": 53.0, "val_loss": 28981.53076171875, "val_acc": 52.0}
{"epoch": 32, "training_loss": 93797.9501953125, "training_acc": 53.0, "val_loss": 43980.419921875, "val_acc": 48.0}
{"epoch": 33, "training_loss": 189804.3310546875, "training_acc": 47.0, "val_loss": 4296.806716918945, "val_acc": 52.0}
{"epoch": 34, "training_loss": 20587.288940429688, "training_acc": 53.0, "val_loss": 36482.31201171875, "val_acc": 48.0}
