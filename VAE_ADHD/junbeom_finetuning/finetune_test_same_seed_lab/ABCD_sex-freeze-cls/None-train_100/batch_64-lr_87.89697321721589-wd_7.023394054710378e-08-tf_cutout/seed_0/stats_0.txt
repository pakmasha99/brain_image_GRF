"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 715231.1569061279, "training_acc": 50.0, "val_loss": 138732.5927734375, "val_acc": 44.0}
{"epoch": 1, "training_loss": 737587.96875, "training_acc": 48.0, "val_loss": 352244.970703125, "val_acc": 56.0}
{"epoch": 2, "training_loss": 1501584.6953125, "training_acc": 52.0, "val_loss": 215511.03515625, "val_acc": 56.0}
{"epoch": 3, "training_loss": 643620.5791015625, "training_acc": 52.0, "val_loss": 225173.33984375, "val_acc": 44.0}
{"epoch": 4, "training_loss": 1025042.0390625, "training_acc": 48.0, "val_loss": 392409.7412109375, "val_acc": 44.0}
{"epoch": 5, "training_loss": 1383125.25, "training_acc": 48.0, "val_loss": 204468.81103515625, "val_acc": 44.0}
{"epoch": 6, "training_loss": 513779.2917480469, "training_acc": 48.0, "val_loss": 161904.23583984375, "val_acc": 56.0}
{"epoch": 7, "training_loss": 858777.47265625, "training_acc": 52.0, "val_loss": 312639.306640625, "val_acc": 56.0}
{"epoch": 8, "training_loss": 1359991.0546875, "training_acc": 52.0, "val_loss": 250542.578125, "val_acc": 56.0}
{"epoch": 9, "training_loss": 931742.009765625, "training_acc": 52.0, "val_loss": 16281.411743164062, "val_acc": 56.0}
{"epoch": 10, "training_loss": 304883.91015625, "training_acc": 50.0, "val_loss": 315775.341796875, "val_acc": 44.0}
{"epoch": 11, "training_loss": 1234729.015625, "training_acc": 48.0, "val_loss": 349602.0263671875, "val_acc": 44.0}
{"epoch": 12, "training_loss": 1206287.390625, "training_acc": 48.0, "val_loss": 152621.533203125, "val_acc": 44.0}
{"epoch": 13, "training_loss": 362702.4645996094, "training_acc": 52.0, "val_loss": 86606.65283203125, "val_acc": 56.0}
{"epoch": 14, "training_loss": 414156.9375, "training_acc": 52.0, "val_loss": 94839.17236328125, "val_acc": 56.0}
{"epoch": 15, "training_loss": 332206.05322265625, "training_acc": 52.0, "val_loss": 70495.14770507812, "val_acc": 44.0}
{"epoch": 16, "training_loss": 318417.8623046875, "training_acc": 48.0, "val_loss": 99180.6396484375, "val_acc": 44.0}
{"epoch": 17, "training_loss": 272670.3046875, "training_acc": 48.0, "val_loss": 72112.77465820312, "val_acc": 56.0}
{"epoch": 18, "training_loss": 382201.228515625, "training_acc": 52.0, "val_loss": 114690.673828125, "val_acc": 56.0}
{"epoch": 19, "training_loss": 433376.4091796875, "training_acc": 52.0, "val_loss": 7421.5484619140625, "val_acc": 44.0}
{"epoch": 20, "training_loss": 79857.8447265625, "training_acc": 48.0, "val_loss": 17166.390991210938, "val_acc": 44.0}
{"epoch": 21, "training_loss": 101451.46826171875, "training_acc": 54.0, "val_loss": 65423.040771484375, "val_acc": 56.0}
{"epoch": 22, "training_loss": 248763.560546875, "training_acc": 52.0, "val_loss": 25337.112426757812, "val_acc": 44.0}
{"epoch": 23, "training_loss": 112878.58984375, "training_acc": 48.0, "val_loss": 4352.942276000977, "val_acc": 44.0}
{"epoch": 24, "training_loss": 128902.9921875, "training_acc": 46.0, "val_loss": 88597.265625, "val_acc": 56.0}
{"epoch": 25, "training_loss": 351994.1669921875, "training_acc": 52.0, "val_loss": 6041.630554199219, "val_acc": 56.0}
{"epoch": 26, "training_loss": 108209.9248046875, "training_acc": 62.0, "val_loss": 161851.7822265625, "val_acc": 44.0}
{"epoch": 27, "training_loss": 596582.568359375, "training_acc": 48.0, "val_loss": 102044.18334960938, "val_acc": 44.0}
{"epoch": 28, "training_loss": 226784.4853515625, "training_acc": 54.0, "val_loss": 38497.56164550781, "val_acc": 56.0}
{"epoch": 29, "training_loss": 155812.6875, "training_acc": 52.0, "val_loss": 19133.946228027344, "val_acc": 44.0}
{"epoch": 30, "training_loss": 58606.14825439453, "training_acc": 48.0, "val_loss": 36144.90966796875, "val_acc": 56.0}
{"epoch": 31, "training_loss": 147273.07446289062, "training_acc": 52.0, "val_loss": 14378.982543945312, "val_acc": 44.0}
{"epoch": 32, "training_loss": 36369.2453918457, "training_acc": 48.0, "val_loss": 50497.552490234375, "val_acc": 56.0}
{"epoch": 33, "training_loss": 226280.236328125, "training_acc": 52.0, "val_loss": 20665.057373046875, "val_acc": 56.0}
{"epoch": 34, "training_loss": 128968.8251953125, "training_acc": 54.0, "val_loss": 89603.96728515625, "val_acc": 44.0}
{"epoch": 35, "training_loss": 291339.21484375, "training_acc": 48.0, "val_loss": 19456.4697265625, "val_acc": 56.0}
{"epoch": 36, "training_loss": 109790.20166015625, "training_acc": 52.0, "val_loss": 14387.144470214844, "val_acc": 56.0}
{"epoch": 37, "training_loss": 153892.1064453125, "training_acc": 42.0, "val_loss": 72957.86743164062, "val_acc": 44.0}
{"epoch": 38, "training_loss": 201326.67041015625, "training_acc": 48.0, "val_loss": 59098.870849609375, "val_acc": 56.0}
{"epoch": 39, "training_loss": 327551.228515625, "training_acc": 52.0, "val_loss": 85282.53173828125, "val_acc": 56.0}
{"epoch": 40, "training_loss": 292509.4248046875, "training_acc": 52.0, "val_loss": 66420.99609375, "val_acc": 44.0}
{"epoch": 41, "training_loss": 305180.89453125, "training_acc": 48.0, "val_loss": 92683.52661132812, "val_acc": 44.0}
{"epoch": 42, "training_loss": 257832.36840820312, "training_acc": 48.0, "val_loss": 76754.38842773438, "val_acc": 56.0}
