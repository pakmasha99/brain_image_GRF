"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 288713.36979675293, "training_acc": 50.0, "val_loss": 55887.51220703125, "val_acc": 44.0}
{"epoch": 1, "training_loss": 297623.845703125, "training_acc": 48.0, "val_loss": 142233.77685546875, "val_acc": 56.0}
{"epoch": 2, "training_loss": 606209.087890625, "training_acc": 52.0, "val_loss": 86903.57666015625, "val_acc": 56.0}
{"epoch": 3, "training_loss": 259345.75927734375, "training_acc": 52.0, "val_loss": 91059.5947265625, "val_acc": 44.0}
{"epoch": 4, "training_loss": 414316.619140625, "training_acc": 48.0, "val_loss": 158439.80712890625, "val_acc": 44.0}
{"epoch": 5, "training_loss": 558369.8828125, "training_acc": 48.0, "val_loss": 82433.544921875, "val_acc": 44.0}
{"epoch": 6, "training_loss": 206963.0684814453, "training_acc": 48.0, "val_loss": 65461.26708984375, "val_acc": 56.0}
{"epoch": 7, "training_loss": 347067.9140625, "training_acc": 52.0, "val_loss": 126214.2333984375, "val_acc": 56.0}
{"epoch": 8, "training_loss": 548913.755859375, "training_acc": 52.0, "val_loss": 101022.27783203125, "val_acc": 56.0}
{"epoch": 9, "training_loss": 375540.30078125, "training_acc": 52.0, "val_loss": 6385.25390625, "val_acc": 56.0}
{"epoch": 10, "training_loss": 122862.197265625, "training_acc": 50.0, "val_loss": 127658.10546875, "val_acc": 44.0}
{"epoch": 11, "training_loss": 499039.248046875, "training_acc": 48.0, "val_loss": 141171.728515625, "val_acc": 44.0}
{"epoch": 12, "training_loss": 487036.576171875, "training_acc": 48.0, "val_loss": 61536.62109375, "val_acc": 44.0}
{"epoch": 13, "training_loss": 146357.82354736328, "training_acc": 52.0, "val_loss": 35031.158447265625, "val_acc": 56.0}
{"epoch": 14, "training_loss": 167463.65478515625, "training_acc": 52.0, "val_loss": 38313.29345703125, "val_acc": 56.0}
{"epoch": 15, "training_loss": 134207.36791992188, "training_acc": 52.0, "val_loss": 28445.138549804688, "val_acc": 44.0}
{"epoch": 16, "training_loss": 128471.9541015625, "training_acc": 48.0, "val_loss": 39985.43701171875, "val_acc": 44.0}
{"epoch": 17, "training_loss": 109857.0126953125, "training_acc": 48.0, "val_loss": 29164.016723632812, "val_acc": 56.0}
{"epoch": 18, "training_loss": 154494.275390625, "training_acc": 52.0, "val_loss": 46311.04736328125, "val_acc": 56.0}
{"epoch": 19, "training_loss": 174965.91259765625, "training_acc": 52.0, "val_loss": 3021.0214614868164, "val_acc": 44.0}
{"epoch": 20, "training_loss": 32325.989990234375, "training_acc": 48.0, "val_loss": 6943.705749511719, "val_acc": 44.0}
{"epoch": 21, "training_loss": 40969.720947265625, "training_acc": 54.0, "val_loss": 26388.064575195312, "val_acc": 56.0}
{"epoch": 22, "training_loss": 100302.83569335938, "training_acc": 52.0, "val_loss": 10279.164123535156, "val_acc": 44.0}
{"epoch": 23, "training_loss": 45750.082763671875, "training_acc": 48.0, "val_loss": 1795.681381225586, "val_acc": 44.0}
{"epoch": 24, "training_loss": 52074.9921875, "training_acc": 46.0, "val_loss": 35716.56188964844, "val_acc": 56.0}
{"epoch": 25, "training_loss": 141857.40869140625, "training_acc": 52.0, "val_loss": 2363.283157348633, "val_acc": 56.0}
{"epoch": 26, "training_loss": 43596.072265625, "training_acc": 62.0, "val_loss": 65404.278564453125, "val_acc": 44.0}
{"epoch": 27, "training_loss": 241039.4697265625, "training_acc": 48.0, "val_loss": 41198.34289550781, "val_acc": 44.0}
{"epoch": 28, "training_loss": 91571.67161941528, "training_acc": 54.0, "val_loss": 15546.7041015625, "val_acc": 56.0}
{"epoch": 29, "training_loss": 62914.01599121094, "training_acc": 52.0, "val_loss": 7730.559539794922, "val_acc": 44.0}
{"epoch": 30, "training_loss": 23680.542114257812, "training_acc": 48.0, "val_loss": 14587.196350097656, "val_acc": 56.0}
{"epoch": 31, "training_loss": 59426.56774902344, "training_acc": 52.0, "val_loss": 5822.528076171875, "val_acc": 44.0}
{"epoch": 32, "training_loss": 14746.882507324219, "training_acc": 48.0, "val_loss": 20369.84405517578, "val_acc": 56.0}
{"epoch": 33, "training_loss": 91264.09448242188, "training_acc": 52.0, "val_loss": 8307.029724121094, "val_acc": 56.0}
{"epoch": 34, "training_loss": 52025.642333984375, "training_acc": 54.0, "val_loss": 36202.41394042969, "val_acc": 44.0}
{"epoch": 35, "training_loss": 117702.16723632812, "training_acc": 48.0, "val_loss": 7848.935699462891, "val_acc": 56.0}
{"epoch": 36, "training_loss": 44290.38720703125, "training_acc": 52.0, "val_loss": 5791.309356689453, "val_acc": 56.0}
{"epoch": 37, "training_loss": 62104.66845703125, "training_acc": 42.0, "val_loss": 29457.77587890625, "val_acc": 44.0}
{"epoch": 38, "training_loss": 81275.26904296875, "training_acc": 48.0, "val_loss": 23862.65106201172, "val_acc": 56.0}
{"epoch": 39, "training_loss": 132224.82958984375, "training_acc": 52.0, "val_loss": 34398.992919921875, "val_acc": 56.0}
{"epoch": 40, "training_loss": 117936.2666015625, "training_acc": 52.0, "val_loss": 26869.976806640625, "val_acc": 44.0}
{"epoch": 41, "training_loss": 123388.7744140625, "training_acc": 48.0, "val_loss": 37434.039306640625, "val_acc": 44.0}
{"epoch": 42, "training_loss": 104139.66668701172, "training_acc": 48.0, "val_loss": 30978.582763671875, "val_acc": 56.0}
