"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 244154.37787628174, "training_acc": 51.0, "val_loss": 53651.422119140625, "val_acc": 52.0}
{"epoch": 1, "training_loss": 266406.5146484375, "training_acc": 55.0, "val_loss": 146829.65087890625, "val_acc": 48.0}
{"epoch": 2, "training_loss": 549341.20703125, "training_acc": 47.0, "val_loss": 37233.233642578125, "val_acc": 48.0}
{"epoch": 3, "training_loss": 166711.052734375, "training_acc": 53.0, "val_loss": 108552.9296875, "val_acc": 52.0}
{"epoch": 4, "training_loss": 443098.671875, "training_acc": 53.0, "val_loss": 96650.18310546875, "val_acc": 52.0}
{"epoch": 5, "training_loss": 311605.84619140625, "training_acc": 53.0, "val_loss": 18838.368225097656, "val_acc": 48.0}
{"epoch": 6, "training_loss": 143902.642578125, "training_acc": 47.0, "val_loss": 49499.28894042969, "val_acc": 48.0}
{"epoch": 7, "training_loss": 158967.609375, "training_acc": 47.0, "val_loss": 35510.986328125, "val_acc": 52.0}
{"epoch": 8, "training_loss": 169853.0146484375, "training_acc": 53.0, "val_loss": 55256.463623046875, "val_acc": 52.0}
{"epoch": 9, "training_loss": 189196.2294921875, "training_acc": 53.0, "val_loss": 8768.153381347656, "val_acc": 48.0}
{"epoch": 10, "training_loss": 53151.262451171875, "training_acc": 47.0, "val_loss": 7095.56884765625, "val_acc": 48.0}
{"epoch": 11, "training_loss": 56215.232421875, "training_acc": 49.0, "val_loss": 36715.380859375, "val_acc": 52.0}
{"epoch": 12, "training_loss": 129745.97900390625, "training_acc": 53.0, "val_loss": 1447.7381706237793, "val_acc": 48.0}
{"epoch": 13, "training_loss": 14887.805541992188, "training_acc": 47.0, "val_loss": 10528.072357177734, "val_acc": 52.0}
{"epoch": 14, "training_loss": 35496.4775390625, "training_acc": 53.0, "val_loss": 15244.906616210938, "val_acc": 48.0}
{"epoch": 15, "training_loss": 56524.660888671875, "training_acc": 47.0, "val_loss": 12480.841827392578, "val_acc": 52.0}
{"epoch": 16, "training_loss": 53189.58837890625, "training_acc": 53.0, "val_loss": 1457.760238647461, "val_acc": 48.0}
{"epoch": 17, "training_loss": 9627.096801757812, "training_acc": 49.0, "val_loss": 9881.237030029297, "val_acc": 48.0}
{"epoch": 18, "training_loss": 26943.251724243164, "training_acc": 47.0, "val_loss": 26939.932250976562, "val_acc": 52.0}
{"epoch": 19, "training_loss": 114038.99951171875, "training_acc": 53.0, "val_loss": 22273.651123046875, "val_acc": 52.0}
{"epoch": 20, "training_loss": 67637.14123535156, "training_acc": 49.0, "val_loss": 11337.187194824219, "val_acc": 48.0}
{"epoch": 21, "training_loss": 35457.735900878906, "training_acc": 51.0, "val_loss": 2175.3700256347656, "val_acc": 52.0}
{"epoch": 22, "training_loss": 28963.27197265625, "training_acc": 55.0, "val_loss": 17549.57733154297, "val_acc": 48.0}
{"epoch": 23, "training_loss": 49645.91845703125, "training_acc": 51.0, "val_loss": 5106.940841674805, "val_acc": 52.0}
{"epoch": 24, "training_loss": 33186.553466796875, "training_acc": 49.0, "val_loss": 7007.97119140625, "val_acc": 48.0}
{"epoch": 25, "training_loss": 44398.287109375, "training_acc": 49.0, "val_loss": 23065.733337402344, "val_acc": 52.0}
{"epoch": 26, "training_loss": 73327.52124023438, "training_acc": 53.0, "val_loss": 22023.81591796875, "val_acc": 48.0}
{"epoch": 27, "training_loss": 94070.53662109375, "training_acc": 47.0, "val_loss": 10610.808563232422, "val_acc": 48.0}
{"epoch": 28, "training_loss": 66646.55859375, "training_acc": 45.0, "val_loss": 33789.422607421875, "val_acc": 52.0}
{"epoch": 29, "training_loss": 121075.45166015625, "training_acc": 53.0, "val_loss": 645.054817199707, "val_acc": 52.0}
{"epoch": 30, "training_loss": 48920.317138671875, "training_acc": 59.0, "val_loss": 62064.83154296875, "val_acc": 48.0}
{"epoch": 31, "training_loss": 249729.7197265625, "training_acc": 47.0, "val_loss": 35503.71398925781, "val_acc": 48.0}
{"epoch": 32, "training_loss": 99039.60174560547, "training_acc": 51.0, "val_loss": 25660.064697265625, "val_acc": 52.0}
{"epoch": 33, "training_loss": 106329.41015625, "training_acc": 53.0, "val_loss": 7663.5009765625, "val_acc": 52.0}
{"epoch": 34, "training_loss": 64686.43798828125, "training_acc": 51.0, "val_loss": 45335.015869140625, "val_acc": 48.0}
{"epoch": 35, "training_loss": 172813.9931640625, "training_acc": 47.0, "val_loss": 7811.423492431641, "val_acc": 48.0}
{"epoch": 36, "training_loss": 78012.34228515625, "training_acc": 47.0, "val_loss": 60142.85888671875, "val_acc": 52.0}
{"epoch": 37, "training_loss": 235173.56201171875, "training_acc": 53.0, "val_loss": 47118.54553222656, "val_acc": 52.0}
{"epoch": 38, "training_loss": 148901.828125, "training_acc": 53.0, "val_loss": 27523.443603515625, "val_acc": 48.0}
{"epoch": 39, "training_loss": 136021.98828125, "training_acc": 47.0, "val_loss": 43157.06787109375, "val_acc": 48.0}
{"epoch": 40, "training_loss": 143365.06494140625, "training_acc": 47.0, "val_loss": 21371.627807617188, "val_acc": 52.0}
{"epoch": 41, "training_loss": 112464.8876953125, "training_acc": 53.0, "val_loss": 36293.12438964844, "val_acc": 52.0}
{"epoch": 42, "training_loss": 110555.6572265625, "training_acc": 53.0, "val_loss": 21176.751708984375, "val_acc": 48.0}
{"epoch": 43, "training_loss": 98028.57543945312, "training_acc": 47.0, "val_loss": 28943.447875976562, "val_acc": 48.0}
{"epoch": 44, "training_loss": 82753.84753417969, "training_acc": 47.0, "val_loss": 36423.12927246094, "val_acc": 52.0}
{"epoch": 45, "training_loss": 158011.517578125, "training_acc": 53.0, "val_loss": 53372.5341796875, "val_acc": 52.0}
{"epoch": 46, "training_loss": 185229.68603515625, "training_acc": 53.0, "val_loss": 5692.380142211914, "val_acc": 52.0}
{"epoch": 47, "training_loss": 95280.90234375, "training_acc": 45.0, "val_loss": 70357.39135742188, "val_acc": 48.0}
{"epoch": 48, "training_loss": 284110.9208984375, "training_acc": 47.0, "val_loss": 48437.81433105469, "val_acc": 48.0}
