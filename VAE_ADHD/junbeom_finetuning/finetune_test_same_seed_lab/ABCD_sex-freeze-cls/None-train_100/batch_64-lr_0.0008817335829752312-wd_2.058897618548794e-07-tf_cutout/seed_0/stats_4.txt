"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.67312240600586, "training_acc": 49.0, "val_loss": 17.493727803230286, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.50665736198425, "training_acc": 53.0, "val_loss": 17.317989468574524, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.67755055427551, "training_acc": 47.0, "val_loss": 17.356765270233154, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.3015923500061, "training_acc": 47.0, "val_loss": 17.32648015022278, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.2208321094513, "training_acc": 53.0, "val_loss": 17.5254687666893, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.93520474433899, "training_acc": 53.0, "val_loss": 17.51582622528076, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.47056221961975, "training_acc": 53.0, "val_loss": 17.33054369688034, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.17242860794067, "training_acc": 53.0, "val_loss": 17.377379536628723, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.78099036216736, "training_acc": 47.0, "val_loss": 17.43110567331314, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.81600260734558, "training_acc": 47.0, "val_loss": 17.32982248067856, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.22510099411011, "training_acc": 53.0, "val_loss": 17.332199215888977, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.12966394424438, "training_acc": 53.0, "val_loss": 17.389167845249176, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.67319178581238, "training_acc": 53.0, "val_loss": 17.48400330543518, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.55817866325378, "training_acc": 53.0, "val_loss": 17.38673746585846, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.60162496566772, "training_acc": 53.0, "val_loss": 17.319682240486145, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.09995937347412, "training_acc": 53.0, "val_loss": 17.3177570104599, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.1448221206665, "training_acc": 53.0, "val_loss": 17.31829345226288, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.19532060623169, "training_acc": 53.0, "val_loss": 17.317502200603485, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.18052697181702, "training_acc": 53.0, "val_loss": 17.31787621974945, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.19469118118286, "training_acc": 53.0, "val_loss": 17.32395589351654, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.1841402053833, "training_acc": 53.0, "val_loss": 17.35672354698181, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.19952344894409, "training_acc": 53.0, "val_loss": 17.35529601573944, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.19368553161621, "training_acc": 53.0, "val_loss": 17.325949668884277, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.13048505783081, "training_acc": 53.0, "val_loss": 17.317458987236023, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.18532061576843, "training_acc": 53.0, "val_loss": 17.31972098350525, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.16970467567444, "training_acc": 53.0, "val_loss": 17.317602038383484, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.13392972946167, "training_acc": 53.0, "val_loss": 17.333194613456726, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.3066828250885, "training_acc": 53.0, "val_loss": 17.360858619213104, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.21789145469666, "training_acc": 53.0, "val_loss": 17.334818840026855, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.19490051269531, "training_acc": 53.0, "val_loss": 17.320895195007324, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.12928032875061, "training_acc": 53.0, "val_loss": 17.318548262119293, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.34190011024475, "training_acc": 53.0, "val_loss": 17.318597435951233, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.39881467819214, "training_acc": 53.0, "val_loss": 17.34640896320343, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.20819449424744, "training_acc": 53.0, "val_loss": 17.33395904302597, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.11121678352356, "training_acc": 53.0, "val_loss": 17.317479848861694, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.1329276561737, "training_acc": 53.0, "val_loss": 17.33381301164627, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.39556121826172, "training_acc": 49.0, "val_loss": 17.370375990867615, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.41173481941223, "training_acc": 47.0, "val_loss": 17.329297959804535, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.92766237258911, "training_acc": 53.0, "val_loss": 17.330116033554077, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.14536762237549, "training_acc": 53.0, "val_loss": 17.32669025659561, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.17261624336243, "training_acc": 53.0, "val_loss": 17.321830987930298, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.15103840827942, "training_acc": 53.0, "val_loss": 17.326532304286957, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.37521123886108, "training_acc": 53.0, "val_loss": 17.323166131973267, "val_acc": 52.0}
