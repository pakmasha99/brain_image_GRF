"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.67095565795898, "training_acc": 53.0, "val_loss": 17.368561029434204, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.58951568603516, "training_acc": 49.0, "val_loss": 17.41626262664795, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.64902973175049, "training_acc": 47.0, "val_loss": 17.31618493795395, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.35240125656128, "training_acc": 53.0, "val_loss": 17.422790825366974, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.41190814971924, "training_acc": 53.0, "val_loss": 17.4026757478714, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.40521097183228, "training_acc": 53.0, "val_loss": 17.34204888343811, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.1490650177002, "training_acc": 53.0, "val_loss": 17.31812208890915, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.11908268928528, "training_acc": 53.0, "val_loss": 17.31880158185959, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.2290735244751, "training_acc": 53.0, "val_loss": 17.334073781967163, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.27479195594788, "training_acc": 57.0, "val_loss": 17.315444350242615, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.10622072219849, "training_acc": 53.0, "val_loss": 17.334921658039093, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.15661597251892, "training_acc": 53.0, "val_loss": 17.404739558696747, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.58773756027222, "training_acc": 53.0, "val_loss": 17.374764382839203, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.62773513793945, "training_acc": 53.0, "val_loss": 17.374177277088165, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.24910688400269, "training_acc": 53.0, "val_loss": 17.43290275335312, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.49495673179626, "training_acc": 53.0, "val_loss": 17.405281960964203, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.50772190093994, "training_acc": 53.0, "val_loss": 17.322787642478943, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.09078550338745, "training_acc": 53.0, "val_loss": 17.315009236335754, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.37353491783142, "training_acc": 53.0, "val_loss": 17.321953177452087, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.18493747711182, "training_acc": 53.0, "val_loss": 17.31526106595993, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.10031080245972, "training_acc": 53.0, "val_loss": 17.350228130817413, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.15726613998413, "training_acc": 53.0, "val_loss": 17.395217716693878, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.45581698417664, "training_acc": 53.0, "val_loss": 17.418299615383148, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.36901187896729, "training_acc": 53.0, "val_loss": 17.340348660945892, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.28267884254456, "training_acc": 53.0, "val_loss": 17.314815521240234, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.13906264305115, "training_acc": 53.0, "val_loss": 17.316055297851562, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.27183508872986, "training_acc": 53.0, "val_loss": 17.314884066581726, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.2057876586914, "training_acc": 53.0, "val_loss": 17.33402907848358, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.14542412757874, "training_acc": 53.0, "val_loss": 17.350928485393524, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.19185066223145, "training_acc": 53.0, "val_loss": 17.340850830078125, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.1862120628357, "training_acc": 53.0, "val_loss": 17.32691526412964, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.12602281570435, "training_acc": 53.0, "val_loss": 17.314966022968292, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.14133024215698, "training_acc": 53.0, "val_loss": 17.323783040046692, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.35988283157349, "training_acc": 47.0, "val_loss": 17.327623069286346, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.27039790153503, "training_acc": 53.0, "val_loss": 17.31630265712738, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.14600658416748, "training_acc": 53.0, "val_loss": 17.33732968568802, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.14147782325745, "training_acc": 53.0, "val_loss": 17.350749671459198, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.16418290138245, "training_acc": 53.0, "val_loss": 17.335613071918488, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.21562266349792, "training_acc": 53.0, "val_loss": 17.315754294395447, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.1356852054596, "training_acc": 53.0, "val_loss": 17.31528788805008, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.13406085968018, "training_acc": 53.0, "val_loss": 17.315003275871277, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.12976884841919, "training_acc": 53.0, "val_loss": 17.316505312919617, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.16454696655273, "training_acc": 53.0, "val_loss": 17.327414453029633, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.15161108970642, "training_acc": 53.0, "val_loss": 17.326320707798004, "val_acc": 52.0}
