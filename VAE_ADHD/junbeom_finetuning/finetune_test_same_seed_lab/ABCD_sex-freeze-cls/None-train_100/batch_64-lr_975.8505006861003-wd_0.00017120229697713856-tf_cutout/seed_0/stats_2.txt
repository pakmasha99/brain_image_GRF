"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 7528970.485626221, "training_acc": 53.0, "val_loss": 560168.65234375, "val_acc": 52.0}
{"epoch": 1, "training_loss": 7365921.9375, "training_acc": 51.0, "val_loss": 4449045.3125, "val_acc": 48.0}
{"epoch": 2, "training_loss": 15774403.875, "training_acc": 47.0, "val_loss": 521226.123046875, "val_acc": 48.0}
{"epoch": 3, "training_loss": 4339589.25, "training_acc": 49.0, "val_loss": 2951242.96875, "val_acc": 52.0}
{"epoch": 4, "training_loss": 11135443.5625, "training_acc": 53.0, "val_loss": 1627088.671875, "val_acc": 52.0}
{"epoch": 5, "training_loss": 5085358.369140625, "training_acc": 53.0, "val_loss": 1592895.5078125, "val_acc": 48.0}
{"epoch": 6, "training_loss": 7068790.0625, "training_acc": 47.0, "val_loss": 1377098.046875, "val_acc": 48.0}
{"epoch": 7, "training_loss": 3981441.75390625, "training_acc": 47.0, "val_loss": 1324422.4609375, "val_acc": 52.0}
{"epoch": 8, "training_loss": 5641216.484375, "training_acc": 53.0, "val_loss": 1498614.84375, "val_acc": 52.0}
{"epoch": 9, "training_loss": 4767112.5234375, "training_acc": 53.0, "val_loss": 415898.92578125, "val_acc": 48.0}
{"epoch": 10, "training_loss": 2199807.484375, "training_acc": 47.0, "val_loss": 164579.6875, "val_acc": 48.0}
{"epoch": 11, "training_loss": 1391435.2109375, "training_acc": 53.0, "val_loss": 1011832.2265625, "val_acc": 52.0}
{"epoch": 12, "training_loss": 3441441.0390625, "training_acc": 53.0, "val_loss": 188308.59375, "val_acc": 48.0}
{"epoch": 13, "training_loss": 870327.87890625, "training_acc": 47.0, "val_loss": 393494.1162109375, "val_acc": 52.0}
{"epoch": 14, "training_loss": 1521735.5078125, "training_acc": 53.0, "val_loss": 215625.2685546875, "val_acc": 48.0}
{"epoch": 15, "training_loss": 679618.3486328125, "training_acc": 47.0, "val_loss": 596694.43359375, "val_acc": 52.0}
{"epoch": 16, "training_loss": 2298622.3203125, "training_acc": 53.0, "val_loss": 95457.87963867188, "val_acc": 52.0}
{"epoch": 17, "training_loss": 1426531.375, "training_acc": 51.0, "val_loss": 885251.46484375, "val_acc": 48.0}
{"epoch": 18, "training_loss": 2747648.375, "training_acc": 47.0, "val_loss": 638326.85546875, "val_acc": 52.0}
{"epoch": 19, "training_loss": 2890309.75, "training_acc": 53.0, "val_loss": 562745.751953125, "val_acc": 52.0}
{"epoch": 20, "training_loss": 1694465.0, "training_acc": 51.0, "val_loss": 208689.2333984375, "val_acc": 48.0}
{"epoch": 21, "training_loss": 971316.82421875, "training_acc": 51.0, "val_loss": 262855.6884765625, "val_acc": 52.0}
{"epoch": 22, "training_loss": 1022757.4921875, "training_acc": 53.0, "val_loss": 142526.9287109375, "val_acc": 48.0}
{"epoch": 23, "training_loss": 1250438.7109375, "training_acc": 45.0, "val_loss": 496111.81640625, "val_acc": 52.0}
{"epoch": 24, "training_loss": 1210935.9310302734, "training_acc": 53.0, "val_loss": 124180.3466796875, "val_acc": 52.0}
{"epoch": 25, "training_loss": 835752.98828125, "training_acc": 47.0, "val_loss": 606.466007232666, "val_acc": 52.0}
{"epoch": 26, "training_loss": 365818.18298339844, "training_acc": 53.0, "val_loss": 86352.15454101562, "val_acc": 52.0}
{"epoch": 27, "training_loss": 444889.798828125, "training_acc": 51.0, "val_loss": 220607.3974609375, "val_acc": 52.0}
{"epoch": 28, "training_loss": 572578.9106445312, "training_acc": 53.0, "val_loss": 671242.236328125, "val_acc": 48.0}
{"epoch": 29, "training_loss": 2599633.71875, "training_acc": 47.0, "val_loss": 54425.341796875, "val_acc": 52.0}
{"epoch": 30, "training_loss": 172858.71875, "training_acc": 47.0, "val_loss": 521186.376953125, "val_acc": 52.0}
{"epoch": 31, "training_loss": 1945540.3671875, "training_acc": 53.0, "val_loss": 112241.40625, "val_acc": 48.0}
{"epoch": 32, "training_loss": 314044.90234375, "training_acc": 49.0, "val_loss": 405309.3017578125, "val_acc": 48.0}
{"epoch": 33, "training_loss": 1258843.16015625, "training_acc": 47.0, "val_loss": 600148.92578125, "val_acc": 52.0}
{"epoch": 34, "training_loss": 2481463.078125, "training_acc": 53.0, "val_loss": 236511.42578125, "val_acc": 52.0}
{"epoch": 35, "training_loss": 1416451.65625, "training_acc": 53.0, "val_loss": 702724.365234375, "val_acc": 48.0}
{"epoch": 36, "training_loss": 2099089.5078125, "training_acc": 47.0, "val_loss": 730990.380859375, "val_acc": 52.0}
{"epoch": 37, "training_loss": 3302151.015625, "training_acc": 53.0, "val_loss": 565525.1953125, "val_acc": 52.0}
{"epoch": 38, "training_loss": 1600250.259765625, "training_acc": 55.0, "val_loss": 314972.1435546875, "val_acc": 48.0}
{"epoch": 39, "training_loss": 1079113.791015625, "training_acc": 49.0, "val_loss": 54683.087158203125, "val_acc": 52.0}
{"epoch": 40, "training_loss": 1178324.2734375, "training_acc": 45.0, "val_loss": 429731.982421875, "val_acc": 48.0}
{"epoch": 41, "training_loss": 1580086.83203125, "training_acc": 47.0, "val_loss": 284245.9228515625, "val_acc": 52.0}
{"epoch": 42, "training_loss": 775678.8828125, "training_acc": 59.0, "val_loss": 16383.892822265625, "val_acc": 48.0}
{"epoch": 43, "training_loss": 625562.26171875, "training_acc": 57.0, "val_loss": 588017.87109375, "val_acc": 52.0}
{"epoch": 44, "training_loss": 1722371.9873046875, "training_acc": 53.0, "val_loss": 735277.9296875, "val_acc": 48.0}
