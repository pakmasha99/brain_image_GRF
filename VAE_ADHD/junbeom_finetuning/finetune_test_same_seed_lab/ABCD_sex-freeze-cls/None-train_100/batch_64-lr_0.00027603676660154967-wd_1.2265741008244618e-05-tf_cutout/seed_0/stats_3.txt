"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.28700685501099, "training_acc": 53.0, "val_loss": 17.31521785259247, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.16249775886536, "training_acc": 53.0, "val_loss": 17.369386553764343, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.70170950889587, "training_acc": 47.0, "val_loss": 17.382125556468964, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.5413978099823, "training_acc": 47.0, "val_loss": 17.31427013874054, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.08031153678894, "training_acc": 53.0, "val_loss": 17.317195236682892, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.35828828811646, "training_acc": 53.0, "val_loss": 17.37317591905594, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.26368260383606, "training_acc": 53.0, "val_loss": 17.376220226287842, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.30893898010254, "training_acc": 53.0, "val_loss": 17.360785603523254, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.16517353057861, "training_acc": 53.0, "val_loss": 17.321549355983734, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.13960480690002, "training_acc": 53.0, "val_loss": 17.308366298675537, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.17271709442139, "training_acc": 53.0, "val_loss": 17.32173264026642, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.2862434387207, "training_acc": 49.0, "val_loss": 17.33148694038391, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.33436107635498, "training_acc": 45.0, "val_loss": 17.31811612844467, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.13510823249817, "training_acc": 53.0, "val_loss": 17.308953404426575, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.20205163955688, "training_acc": 53.0, "val_loss": 17.340895533561707, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.22163558006287, "training_acc": 53.0, "val_loss": 17.3682302236557, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.35732650756836, "training_acc": 53.0, "val_loss": 17.3649862408638, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.403972864151, "training_acc": 53.0, "val_loss": 17.329277098178864, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.3803322315216, "training_acc": 53.0, "val_loss": 17.31792986392975, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.1359543800354, "training_acc": 53.0, "val_loss": 17.324639856815338, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.11872839927673, "training_acc": 53.0, "val_loss": 17.332351207733154, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.14206624031067, "training_acc": 53.0, "val_loss": 17.33880639076233, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.18766331672668, "training_acc": 53.0, "val_loss": 17.338138818740845, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.20746278762817, "training_acc": 53.0, "val_loss": 17.331160604953766, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.17751383781433, "training_acc": 53.0, "val_loss": 17.32560694217682, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.09907650947571, "training_acc": 53.0, "val_loss": 17.32303649187088, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.15942907333374, "training_acc": 53.0, "val_loss": 17.312663793563843, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.13249182701111, "training_acc": 53.0, "val_loss": 17.309263348579407, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.25261378288269, "training_acc": 53.0, "val_loss": 17.308759689331055, "val_acc": 52.0}
