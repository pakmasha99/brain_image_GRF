"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.24867391586304, "training_acc": 53.0, "val_loss": 17.313602566719055, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.2206609249115, "training_acc": 53.0, "val_loss": 17.313656210899353, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.21022868156433, "training_acc": 53.0, "val_loss": 17.313331365585327, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.19403648376465, "training_acc": 53.0, "val_loss": 17.312805354595184, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.23446369171143, "training_acc": 53.0, "val_loss": 17.31240004301071, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.19587707519531, "training_acc": 53.0, "val_loss": 17.311933636665344, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.18225145339966, "training_acc": 53.0, "val_loss": 17.3115074634552, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.19254803657532, "training_acc": 53.0, "val_loss": 17.31102168560028, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.18420720100403, "training_acc": 53.0, "val_loss": 17.31073707342148, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.18667268753052, "training_acc": 53.0, "val_loss": 17.310434579849243, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.18386197090149, "training_acc": 53.0, "val_loss": 17.310184240341187, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.15158987045288, "training_acc": 53.0, "val_loss": 17.310062050819397, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.1812596321106, "training_acc": 53.0, "val_loss": 17.3099547624588, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.16744327545166, "training_acc": 53.0, "val_loss": 17.309890687465668, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.18289923667908, "training_acc": 53.0, "val_loss": 17.309841513633728, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.20192670822144, "training_acc": 53.0, "val_loss": 17.3098161816597, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.18841981887817, "training_acc": 53.0, "val_loss": 17.30976402759552, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.17965054512024, "training_acc": 53.0, "val_loss": 17.309699952602386, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.14196729660034, "training_acc": 53.0, "val_loss": 17.309625446796417, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.21255493164062, "training_acc": 53.0, "val_loss": 17.309577763080597, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.17690896987915, "training_acc": 53.0, "val_loss": 17.309536039829254, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.16983151435852, "training_acc": 53.0, "val_loss": 17.309504747390747, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.15418124198914, "training_acc": 53.0, "val_loss": 17.309483885765076, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.13597011566162, "training_acc": 53.0, "val_loss": 17.30947643518448, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.17525172233582, "training_acc": 53.0, "val_loss": 17.309477925300598, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.17407584190369, "training_acc": 53.0, "val_loss": 17.309516668319702, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.14268589019775, "training_acc": 53.0, "val_loss": 17.309579253196716, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.13440346717834, "training_acc": 53.0, "val_loss": 17.30961799621582, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.16339874267578, "training_acc": 53.0, "val_loss": 17.30964183807373, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.16993927955627, "training_acc": 53.0, "val_loss": 17.309705913066864, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.1816475391388, "training_acc": 53.0, "val_loss": 17.309798300266266, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.14883279800415, "training_acc": 53.0, "val_loss": 17.309899628162384, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.14311289787292, "training_acc": 53.0, "val_loss": 17.310039699077606, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.14015889167786, "training_acc": 53.0, "val_loss": 17.310157418251038, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.13949155807495, "training_acc": 53.0, "val_loss": 17.31017678976059, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.15499186515808, "training_acc": 53.0, "val_loss": 17.310230433940887, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.16372489929199, "training_acc": 53.0, "val_loss": 17.310291528701782, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.15180850028992, "training_acc": 53.0, "val_loss": 17.31031984090805, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.17059707641602, "training_acc": 53.0, "val_loss": 17.310374975204468, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.16302800178528, "training_acc": 53.0, "val_loss": 17.310477793216705, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.15097260475159, "training_acc": 53.0, "val_loss": 17.310629785060883, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.13652873039246, "training_acc": 53.0, "val_loss": 17.31078177690506, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.18055033683777, "training_acc": 53.0, "val_loss": 17.31090545654297, "val_acc": 52.0}
