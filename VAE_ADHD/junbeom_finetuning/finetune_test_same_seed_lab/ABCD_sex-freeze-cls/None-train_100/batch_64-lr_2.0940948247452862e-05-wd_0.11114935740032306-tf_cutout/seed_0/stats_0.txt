"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 70.0569794178009, "training_acc": 52.0, "val_loss": 17.196358740329742, "val_acc": 56.0}
{"epoch": 1, "training_loss": 69.94720005989075, "training_acc": 52.0, "val_loss": 17.18735247850418, "val_acc": 56.0}
{"epoch": 2, "training_loss": 69.87321043014526, "training_acc": 52.0, "val_loss": 17.18013882637024, "val_acc": 56.0}
{"epoch": 3, "training_loss": 69.76646542549133, "training_acc": 52.0, "val_loss": 17.174488306045532, "val_acc": 56.0}
{"epoch": 4, "training_loss": 69.65354228019714, "training_acc": 52.0, "val_loss": 17.17018634080887, "val_acc": 56.0}
{"epoch": 5, "training_loss": 69.60538339614868, "training_acc": 52.0, "val_loss": 17.167407274246216, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.57452321052551, "training_acc": 52.0, "val_loss": 17.16577261686325, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.5456690788269, "training_acc": 52.0, "val_loss": 17.16531366109848, "val_acc": 56.0}
{"epoch": 8, "training_loss": 69.44818830490112, "training_acc": 52.0, "val_loss": 17.165791988372803, "val_acc": 56.0}
{"epoch": 9, "training_loss": 69.46590304374695, "training_acc": 52.0, "val_loss": 17.167145013809204, "val_acc": 56.0}
{"epoch": 10, "training_loss": 69.37140607833862, "training_acc": 52.0, "val_loss": 17.169298231601715, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.38370084762573, "training_acc": 52.0, "val_loss": 17.172060906887054, "val_acc": 56.0}
{"epoch": 12, "training_loss": 69.36145639419556, "training_acc": 52.0, "val_loss": 17.17538833618164, "val_acc": 56.0}
{"epoch": 13, "training_loss": 69.30649662017822, "training_acc": 52.0, "val_loss": 17.179325222969055, "val_acc": 56.0}
{"epoch": 14, "training_loss": 69.3024115562439, "training_acc": 52.0, "val_loss": 17.183341085910797, "val_acc": 56.0}
{"epoch": 15, "training_loss": 69.28048181533813, "training_acc": 52.0, "val_loss": 17.1872615814209, "val_acc": 56.0}
{"epoch": 16, "training_loss": 69.28003692626953, "training_acc": 52.0, "val_loss": 17.19076931476593, "val_acc": 56.0}
{"epoch": 17, "training_loss": 69.26922678947449, "training_acc": 52.0, "val_loss": 17.19450354576111, "val_acc": 56.0}
{"epoch": 18, "training_loss": 69.22910141944885, "training_acc": 52.0, "val_loss": 17.19818115234375, "val_acc": 56.0}
{"epoch": 19, "training_loss": 69.24418616294861, "training_acc": 52.0, "val_loss": 17.201733589172363, "val_acc": 56.0}
{"epoch": 20, "training_loss": 69.22182846069336, "training_acc": 52.0, "val_loss": 17.205414175987244, "val_acc": 56.0}
{"epoch": 21, "training_loss": 69.22833681106567, "training_acc": 52.0, "val_loss": 17.208558320999146, "val_acc": 56.0}
{"epoch": 22, "training_loss": 69.24226880073547, "training_acc": 52.0, "val_loss": 17.21072793006897, "val_acc": 56.0}
{"epoch": 23, "training_loss": 69.22488355636597, "training_acc": 52.0, "val_loss": 17.2136127948761, "val_acc": 56.0}
{"epoch": 24, "training_loss": 69.21914029121399, "training_acc": 52.0, "val_loss": 17.21677929162979, "val_acc": 56.0}
{"epoch": 25, "training_loss": 69.23349499702454, "training_acc": 52.0, "val_loss": 17.220252752304077, "val_acc": 56.0}
{"epoch": 26, "training_loss": 69.2199490070343, "training_acc": 52.0, "val_loss": 17.222487926483154, "val_acc": 56.0}
