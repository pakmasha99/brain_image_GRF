"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.20972013473511, "training_acc": 53.0, "val_loss": 17.314784228801727, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.2252471446991, "training_acc": 53.0, "val_loss": 17.31477528810501, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.19606971740723, "training_acc": 53.0, "val_loss": 17.31470674276352, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.19778823852539, "training_acc": 53.0, "val_loss": 17.31460988521576, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.19952988624573, "training_acc": 53.0, "val_loss": 17.3143669962883, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.19330263137817, "training_acc": 53.0, "val_loss": 17.314089834690094, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.19757533073425, "training_acc": 53.0, "val_loss": 17.31373518705368, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.17718124389648, "training_acc": 53.0, "val_loss": 17.31325536966324, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.18412280082703, "training_acc": 53.0, "val_loss": 17.312754690647125, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.17107439041138, "training_acc": 53.0, "val_loss": 17.31235980987549, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.17358732223511, "training_acc": 53.0, "val_loss": 17.312034964561462, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.15881419181824, "training_acc": 53.0, "val_loss": 17.311833798885345, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.1602373123169, "training_acc": 53.0, "val_loss": 17.311643064022064, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.12238645553589, "training_acc": 53.0, "val_loss": 17.31145828962326, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.14793109893799, "training_acc": 53.0, "val_loss": 17.311351001262665, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.13306951522827, "training_acc": 53.0, "val_loss": 17.311328649520874, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.1350827217102, "training_acc": 53.0, "val_loss": 17.31138378381729, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.12699961662292, "training_acc": 53.0, "val_loss": 17.3115074634552, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.14746451377869, "training_acc": 53.0, "val_loss": 17.31169819831848, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.15400958061218, "training_acc": 53.0, "val_loss": 17.311961948871613, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.13630652427673, "training_acc": 53.0, "val_loss": 17.31220632791519, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.13794445991516, "training_acc": 53.0, "val_loss": 17.31248050928116, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.13573908805847, "training_acc": 53.0, "val_loss": 17.31276661157608, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.12213444709778, "training_acc": 53.0, "val_loss": 17.313165962696075, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.09606170654297, "training_acc": 53.0, "val_loss": 17.31339991092682, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.11776661872864, "training_acc": 53.0, "val_loss": 17.313693463802338, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.13073563575745, "training_acc": 53.0, "val_loss": 17.313972115516663, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.1291069984436, "training_acc": 53.0, "val_loss": 17.314033210277557, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.13985466957092, "training_acc": 53.0, "val_loss": 17.314068973064423, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.13661098480225, "training_acc": 53.0, "val_loss": 17.314007878303528, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.12984490394592, "training_acc": 53.0, "val_loss": 17.31390655040741, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.1449658870697, "training_acc": 53.0, "val_loss": 17.31373518705368, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.13321805000305, "training_acc": 53.0, "val_loss": 17.313644289970398, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.15604829788208, "training_acc": 53.0, "val_loss": 17.31361746788025, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.09902119636536, "training_acc": 53.0, "val_loss": 17.313650250434875, "val_acc": 52.0}
