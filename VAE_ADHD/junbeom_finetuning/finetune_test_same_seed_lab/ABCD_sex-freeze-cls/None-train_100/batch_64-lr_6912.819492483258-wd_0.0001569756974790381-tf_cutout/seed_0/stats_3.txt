"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 53489478.7166481, "training_acc": 53.0, "val_loss": 30253484.375, "val_acc": 48.0}
{"epoch": 1, "training_loss": 83885050.4375, "training_acc": 51.0, "val_loss": 10085512.5, "val_acc": 48.0}
{"epoch": 2, "training_loss": 33196695.4375, "training_acc": 49.0, "val_loss": 5682927.34375, "val_acc": 48.0}
{"epoch": 3, "training_loss": 18028892.5, "training_acc": 55.0, "val_loss": 3159874.0234375, "val_acc": 48.0}
{"epoch": 4, "training_loss": 13427781.3125, "training_acc": 53.0, "val_loss": 1953342.3828125, "val_acc": 48.0}
{"epoch": 5, "training_loss": 17339604.75, "training_acc": 37.0, "val_loss": 2439992.96875, "val_acc": 48.0}
{"epoch": 6, "training_loss": 9839789.6875, "training_acc": 55.0, "val_loss": 1275166.2109375, "val_acc": 48.0}
{"epoch": 7, "training_loss": 10721930.5625, "training_acc": 45.0, "val_loss": 1348261.42578125, "val_acc": 48.0}
{"epoch": 8, "training_loss": 9803866.25, "training_acc": 47.0, "val_loss": 1168661.62109375, "val_acc": 48.0}
{"epoch": 9, "training_loss": 7976063.3125, "training_acc": 51.0, "val_loss": 790904.6875, "val_acc": 48.0}
{"epoch": 10, "training_loss": 8921586.125, "training_acc": 45.0, "val_loss": 925199.31640625, "val_acc": 48.0}
{"epoch": 11, "training_loss": 7817336.75, "training_acc": 49.0, "val_loss": 725015.478515625, "val_acc": 48.0}
{"epoch": 12, "training_loss": 9650109.3125, "training_acc": 41.0, "val_loss": 1090097.4609375, "val_acc": 48.0}
{"epoch": 13, "training_loss": 8340072.6875, "training_acc": 47.0, "val_loss": 947934.27734375, "val_acc": 48.0}
{"epoch": 14, "training_loss": 8499858.5, "training_acc": 45.0, "val_loss": 1003860.7421875, "val_acc": 48.0}
{"epoch": 15, "training_loss": 7439165.4375, "training_acc": 49.0, "val_loss": 799345.99609375, "val_acc": 48.0}
{"epoch": 16, "training_loss": 7568120.6875, "training_acc": 47.0, "val_loss": 785318.505859375, "val_acc": 48.0}
{"epoch": 17, "training_loss": 5126676.15625, "training_acc": 57.0, "val_loss": 174835.43701171875, "val_acc": 48.0}
{"epoch": 18, "training_loss": 5536245.0625, "training_acc": 51.0, "val_loss": 112005.84716796875, "val_acc": 48.0}
{"epoch": 19, "training_loss": 6506508.34375, "training_acc": 47.0, "val_loss": 248902.880859375, "val_acc": 48.0}
{"epoch": 20, "training_loss": 5726456.0, "training_acc": 51.0, "val_loss": 92778.13110351562, "val_acc": 48.0}
{"epoch": 21, "training_loss": 5451402.0, "training_acc": 51.0, "val_loss": 1412.7294540405273, "val_acc": 52.0}
{"epoch": 22, "training_loss": 3641351.46484375, "training_acc": 57.0, "val_loss": 970018.65234375, "val_acc": 52.0}
{"epoch": 23, "training_loss": 6846504.65625, "training_acc": 51.0, "val_loss": 1594369.3359375, "val_acc": 52.0}
{"epoch": 24, "training_loss": 5751822.28125, "training_acc": 59.0, "val_loss": 1630287.98828125, "val_acc": 52.0}
{"epoch": 25, "training_loss": 8063897.875, "training_acc": 49.0, "val_loss": 2247103.7109375, "val_acc": 52.0}
{"epoch": 26, "training_loss": 6037270.984375, "training_acc": 61.0, "val_loss": 2041375.78125, "val_acc": 52.0}
{"epoch": 27, "training_loss": 6575672.234375, "training_acc": 57.0, "val_loss": 2176177.5390625, "val_acc": 52.0}
{"epoch": 28, "training_loss": 8729346.53125, "training_acc": 49.0, "val_loss": 2681592.96875, "val_acc": 52.0}
{"epoch": 29, "training_loss": 8858902.296875, "training_acc": 51.0, "val_loss": 2924235.7421875, "val_acc": 52.0}
{"epoch": 30, "training_loss": 11298983.8125, "training_acc": 43.0, "val_loss": 3547587.890625, "val_acc": 52.0}
{"epoch": 31, "training_loss": 9307129.5390625, "training_acc": 53.0, "val_loss": 3473636.71875, "val_acc": 52.0}
{"epoch": 32, "training_loss": 8615285.23828125, "training_acc": 55.0, "val_loss": 3382202.734375, "val_acc": 52.0}
{"epoch": 33, "training_loss": 10017062.96875, "training_acc": 49.0, "val_loss": 3636109.375, "val_acc": 52.0}
{"epoch": 34, "training_loss": 8837708.05859375, "training_acc": 55.0, "val_loss": 3485953.515625, "val_acc": 52.0}
{"epoch": 35, "training_loss": 11239772.828125, "training_acc": 45.0, "val_loss": 3927131.25, "val_acc": 52.0}
{"epoch": 36, "training_loss": 11855633.2421875, "training_acc": 45.0, "val_loss": 4206914.84375, "val_acc": 52.0}
{"epoch": 37, "training_loss": 11009453.985351562, "training_acc": 49.0, "val_loss": 4224398.046875, "val_acc": 52.0}
{"epoch": 38, "training_loss": 10350083.61328125, "training_acc": 53.0, "val_loss": 3465088.28125, "val_acc": 48.0}
{"epoch": 39, "training_loss": 9295180.3125, "training_acc": 55.0, "val_loss": 2506319.53125, "val_acc": 48.0}
{"epoch": 40, "training_loss": 7908760.265625, "training_acc": 55.0, "val_loss": 1756732.6171875, "val_acc": 48.0}
