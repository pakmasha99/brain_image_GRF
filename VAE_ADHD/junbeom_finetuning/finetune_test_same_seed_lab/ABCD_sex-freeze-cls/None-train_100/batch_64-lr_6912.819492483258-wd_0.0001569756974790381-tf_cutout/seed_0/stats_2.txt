"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 65164139.68117142, "training_acc": 45.0, "val_loss": 27365503.125, "val_acc": 52.0}
{"epoch": 1, "training_loss": 73444895.28125, "training_acc": 49.0, "val_loss": 12569705.46875, "val_acc": 52.0}
{"epoch": 2, "training_loss": 36420962.578125, "training_acc": 45.0, "val_loss": 9223172.65625, "val_acc": 52.0}
{"epoch": 3, "training_loss": 27094243.7578125, "training_acc": 43.0, "val_loss": 7787100.78125, "val_acc": 52.0}
{"epoch": 4, "training_loss": 22068628.3354187, "training_acc": 53.0, "val_loss": 3509410.546875, "val_acc": 48.0}
{"epoch": 5, "training_loss": 14674887.5625, "training_acc": 45.0, "val_loss": 3065901.953125, "val_acc": 48.0}
{"epoch": 6, "training_loss": 12771654.90625, "training_acc": 47.0, "val_loss": 2443418.359375, "val_acc": 48.0}
{"epoch": 7, "training_loss": 10033833.9375, "training_acc": 51.0, "val_loss": 1760922.0703125, "val_acc": 48.0}
{"epoch": 8, "training_loss": 9287187.4375, "training_acc": 49.0, "val_loss": 1418029.296875, "val_acc": 48.0}
{"epoch": 9, "training_loss": 8575905.75, "training_acc": 49.0, "val_loss": 1141033.59375, "val_acc": 48.0}
{"epoch": 10, "training_loss": 7992663.8125, "training_acc": 49.0, "val_loss": 928772.265625, "val_acc": 48.0}
{"epoch": 11, "training_loss": 7022945.34375, "training_acc": 51.0, "val_loss": 642784.619140625, "val_acc": 48.0}
{"epoch": 12, "training_loss": 7553186.875, "training_acc": 47.0, "val_loss": 675577.978515625, "val_acc": 48.0}
{"epoch": 13, "training_loss": 7545295.8125, "training_acc": 47.0, "val_loss": 667819.482421875, "val_acc": 48.0}
{"epoch": 14, "training_loss": 7983661.125, "training_acc": 45.0, "val_loss": 777845.751953125, "val_acc": 48.0}
{"epoch": 15, "training_loss": 7027970.5, "training_acc": 49.0, "val_loss": 617912.98828125, "val_acc": 48.0}
{"epoch": 16, "training_loss": 7218401.875, "training_acc": 47.0, "val_loss": 637318.994140625, "val_acc": 48.0}
{"epoch": 17, "training_loss": 5758594.71875, "training_acc": 53.0, "val_loss": 292197.265625, "val_acc": 48.0}
{"epoch": 18, "training_loss": 8255418.625, "training_acc": 41.0, "val_loss": 749467.28515625, "val_acc": 48.0}
{"epoch": 19, "training_loss": 5928792.25, "training_acc": 53.0, "val_loss": 339835.6201171875, "val_acc": 48.0}
{"epoch": 20, "training_loss": 5219452.4375, "training_acc": 53.0, "val_loss": 100441.38793945312, "val_acc": 48.0}
{"epoch": 21, "training_loss": 4445254.71875, "training_acc": 55.0, "val_loss": 193051.52587890625, "val_acc": 52.0}
{"epoch": 22, "training_loss": 4224541.375, "training_acc": 55.0, "val_loss": 1188981.73828125, "val_acc": 52.0}
{"epoch": 23, "training_loss": 7520074.0625, "training_acc": 49.0, "val_loss": 1850029.4921875, "val_acc": 52.0}
{"epoch": 24, "training_loss": 7808711.5625, "training_acc": 51.0, "val_loss": 2232251.953125, "val_acc": 52.0}
{"epoch": 25, "training_loss": 9760889.46875, "training_acc": 45.0, "val_loss": 2860453.515625, "val_acc": 52.0}
{"epoch": 26, "training_loss": 9426210.4375, "training_acc": 49.0, "val_loss": 3119423.2421875, "val_acc": 52.0}
{"epoch": 27, "training_loss": 8632604.265625, "training_acc": 53.0, "val_loss": 3155256.8359375, "val_acc": 52.0}
{"epoch": 28, "training_loss": 8625547.1953125, "training_acc": 53.0, "val_loss": 3214300.390625, "val_acc": 52.0}
{"epoch": 29, "training_loss": 10777214.546875, "training_acc": 45.0, "val_loss": 3665023.046875, "val_acc": 52.0}
{"epoch": 30, "training_loss": 11369859.890625, "training_acc": 45.0, "val_loss": 3968216.015625, "val_acc": 52.0}
{"epoch": 31, "training_loss": 9328776.76171875, "training_acc": 53.0, "val_loss": 3657106.640625, "val_acc": 48.0}
{"epoch": 32, "training_loss": 12008741.5625, "training_acc": 45.0, "val_loss": 3209351.171875, "val_acc": 48.0}
{"epoch": 33, "training_loss": 8424037.2578125, "training_acc": 57.0, "val_loss": 2149261.9140625, "val_acc": 48.0}
{"epoch": 34, "training_loss": 7274994.5625, "training_acc": 55.0, "val_loss": 1489731.93359375, "val_acc": 48.0}
{"epoch": 35, "training_loss": 7787373.875, "training_acc": 49.0, "val_loss": 1277017.3828125, "val_acc": 48.0}
{"epoch": 36, "training_loss": 8590115.75, "training_acc": 45.0, "val_loss": 1284487.890625, "val_acc": 48.0}
{"epoch": 37, "training_loss": 7605226.3125, "training_acc": 49.0, "val_loss": 1033669.62890625, "val_acc": 48.0}
{"epoch": 38, "training_loss": 8223636.5625, "training_acc": 45.0, "val_loss": 1087501.953125, "val_acc": 48.0}
{"epoch": 39, "training_loss": 7310720.71875, "training_acc": 49.0, "val_loss": 874508.203125, "val_acc": 48.0}
