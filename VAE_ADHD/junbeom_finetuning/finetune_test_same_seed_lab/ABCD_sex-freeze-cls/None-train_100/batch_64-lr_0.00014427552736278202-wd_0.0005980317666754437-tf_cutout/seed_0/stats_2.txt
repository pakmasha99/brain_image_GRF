"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.2376983165741, "training_acc": 53.0, "val_loss": 17.314621806144714, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.13023781776428, "training_acc": 53.0, "val_loss": 17.33170449733734, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.14296388626099, "training_acc": 53.0, "val_loss": 17.355796694755554, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.19970345497131, "training_acc": 53.0, "val_loss": 17.366695404052734, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.2327356338501, "training_acc": 53.0, "val_loss": 17.35258400440216, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.13253784179688, "training_acc": 53.0, "val_loss": 17.32938289642334, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.21768283843994, "training_acc": 53.0, "val_loss": 17.31662005186081, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.12860918045044, "training_acc": 53.0, "val_loss": 17.314592003822327, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.17157006263733, "training_acc": 53.0, "val_loss": 17.314960062503815, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.19700455665588, "training_acc": 53.0, "val_loss": 17.315350472927094, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.11468243598938, "training_acc": 53.0, "val_loss": 17.314817011356354, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.17255353927612, "training_acc": 53.0, "val_loss": 17.314548790454865, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.20147800445557, "training_acc": 53.0, "val_loss": 17.31600910425186, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.14204812049866, "training_acc": 53.0, "val_loss": 17.319168150424957, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.13163304328918, "training_acc": 53.0, "val_loss": 17.322880029678345, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.15665459632874, "training_acc": 53.0, "val_loss": 17.327643930912018, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.13154530525208, "training_acc": 53.0, "val_loss": 17.327849566936493, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.1390209197998, "training_acc": 53.0, "val_loss": 17.324015498161316, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.19781231880188, "training_acc": 53.0, "val_loss": 17.321406304836273, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.13419795036316, "training_acc": 53.0, "val_loss": 17.316022515296936, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.13347625732422, "training_acc": 53.0, "val_loss": 17.31465458869934, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.16867423057556, "training_acc": 53.0, "val_loss": 17.315955460071564, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.17142605781555, "training_acc": 53.0, "val_loss": 17.315049469470978, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.26395726203918, "training_acc": 53.0, "val_loss": 17.31458306312561, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.18524670600891, "training_acc": 53.0, "val_loss": 17.314620316028595, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.12004160881042, "training_acc": 53.0, "val_loss": 17.31509417295456, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.23110246658325, "training_acc": 53.0, "val_loss": 17.319385707378387, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.15045261383057, "training_acc": 53.0, "val_loss": 17.32158660888672, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.14736747741699, "training_acc": 53.0, "val_loss": 17.321307957172394, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.19967007637024, "training_acc": 53.0, "val_loss": 17.3201322555542, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.10953330993652, "training_acc": 53.0, "val_loss": 17.325308918952942, "val_acc": 52.0}
