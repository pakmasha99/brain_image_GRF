"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 70.15392422676086, "training_acc": 47.0, "val_loss": 17.428994178771973, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.81901812553406, "training_acc": 47.0, "val_loss": 17.39114671945572, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.63715410232544, "training_acc": 47.0, "val_loss": 17.354869842529297, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.48380756378174, "training_acc": 47.0, "val_loss": 17.327874898910522, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.40064573287964, "training_acc": 43.0, "val_loss": 17.312659323215485, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.2341206073761, "training_acc": 53.0, "val_loss": 17.305465042591095, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.17780065536499, "training_acc": 53.0, "val_loss": 17.30278730392456, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.1199381351471, "training_acc": 53.0, "val_loss": 17.303892970085144, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.1107485294342, "training_acc": 53.0, "val_loss": 17.307651042938232, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.12402653694153, "training_acc": 53.0, "val_loss": 17.314569652080536, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.14502096176147, "training_acc": 53.0, "val_loss": 17.32185333967209, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.16130685806274, "training_acc": 53.0, "val_loss": 17.326238751411438, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.16891813278198, "training_acc": 53.0, "val_loss": 17.326989769935608, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.19002413749695, "training_acc": 53.0, "val_loss": 17.32599586248398, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.16568446159363, "training_acc": 53.0, "val_loss": 17.325450479984283, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.15272092819214, "training_acc": 53.0, "val_loss": 17.322151362895966, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.20216107368469, "training_acc": 53.0, "val_loss": 17.318496108055115, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.14197516441345, "training_acc": 53.0, "val_loss": 17.3182412981987, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.13996744155884, "training_acc": 53.0, "val_loss": 17.315639555454254, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.14415502548218, "training_acc": 53.0, "val_loss": 17.31308102607727, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.11095690727234, "training_acc": 53.0, "val_loss": 17.312684655189514, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.12506604194641, "training_acc": 53.0, "val_loss": 17.312034964561462, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.13798832893372, "training_acc": 53.0, "val_loss": 17.31128692626953, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.12957429885864, "training_acc": 53.0, "val_loss": 17.309600114822388, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.13122653961182, "training_acc": 53.0, "val_loss": 17.308516800403595, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.1311457157135, "training_acc": 53.0, "val_loss": 17.308740317821503, "val_acc": 52.0}
