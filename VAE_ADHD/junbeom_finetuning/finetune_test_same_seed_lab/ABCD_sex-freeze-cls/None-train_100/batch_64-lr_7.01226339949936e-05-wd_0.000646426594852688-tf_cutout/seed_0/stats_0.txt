"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.23996043205261, "training_acc": 52.0, "val_loss": 17.240914702415466, "val_acc": 56.0}
{"epoch": 1, "training_loss": 69.23614144325256, "training_acc": 52.0, "val_loss": 17.221465706825256, "val_acc": 56.0}
{"epoch": 2, "training_loss": 69.25470519065857, "training_acc": 52.0, "val_loss": 17.219610512256622, "val_acc": 56.0}
{"epoch": 3, "training_loss": 69.26376795768738, "training_acc": 52.0, "val_loss": 17.229509353637695, "val_acc": 56.0}
{"epoch": 4, "training_loss": 69.2403872013092, "training_acc": 52.0, "val_loss": 17.245517671108246, "val_acc": 56.0}
{"epoch": 5, "training_loss": 69.22606563568115, "training_acc": 52.0, "val_loss": 17.25132018327713, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.25983381271362, "training_acc": 52.0, "val_loss": 17.26333349943161, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.25902104377747, "training_acc": 52.0, "val_loss": 17.268502712249756, "val_acc": 56.0}
{"epoch": 8, "training_loss": 69.27212452888489, "training_acc": 52.0, "val_loss": 17.262747883796692, "val_acc": 56.0}
{"epoch": 9, "training_loss": 69.25670433044434, "training_acc": 52.0, "val_loss": 17.265745997428894, "val_acc": 56.0}
{"epoch": 10, "training_loss": 69.23359227180481, "training_acc": 52.0, "val_loss": 17.26684868335724, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.26403021812439, "training_acc": 52.0, "val_loss": 17.26342886686325, "val_acc": 56.0}
{"epoch": 12, "training_loss": 69.23401260375977, "training_acc": 52.0, "val_loss": 17.25919544696808, "val_acc": 56.0}
{"epoch": 13, "training_loss": 69.2392680644989, "training_acc": 52.0, "val_loss": 17.257829010486603, "val_acc": 56.0}
{"epoch": 14, "training_loss": 69.24766540527344, "training_acc": 52.0, "val_loss": 17.251186072826385, "val_acc": 56.0}
{"epoch": 15, "training_loss": 69.25108289718628, "training_acc": 52.0, "val_loss": 17.24170595407486, "val_acc": 56.0}
{"epoch": 16, "training_loss": 69.25149512290955, "training_acc": 52.0, "val_loss": 17.22825914621353, "val_acc": 56.0}
{"epoch": 17, "training_loss": 69.23338317871094, "training_acc": 52.0, "val_loss": 17.219458520412445, "val_acc": 56.0}
{"epoch": 18, "training_loss": 69.2415726184845, "training_acc": 52.0, "val_loss": 17.212744057178497, "val_acc": 56.0}
{"epoch": 19, "training_loss": 69.25858426094055, "training_acc": 52.0, "val_loss": 17.20808744430542, "val_acc": 56.0}
{"epoch": 20, "training_loss": 69.20787858963013, "training_acc": 52.0, "val_loss": 17.207588255405426, "val_acc": 56.0}
{"epoch": 21, "training_loss": 69.22881317138672, "training_acc": 52.0, "val_loss": 17.206719517707825, "val_acc": 56.0}
{"epoch": 22, "training_loss": 69.28973889350891, "training_acc": 52.0, "val_loss": 17.2030508518219, "val_acc": 56.0}
{"epoch": 23, "training_loss": 69.26206493377686, "training_acc": 52.0, "val_loss": 17.20672696828842, "val_acc": 56.0}
{"epoch": 24, "training_loss": 69.25923275947571, "training_acc": 52.0, "val_loss": 17.214420437812805, "val_acc": 56.0}
{"epoch": 25, "training_loss": 69.24976348876953, "training_acc": 52.0, "val_loss": 17.225761711597443, "val_acc": 56.0}
{"epoch": 26, "training_loss": 69.27870726585388, "training_acc": 52.0, "val_loss": 17.230762541294098, "val_acc": 56.0}
{"epoch": 27, "training_loss": 69.22718548774719, "training_acc": 52.0, "val_loss": 17.245864868164062, "val_acc": 56.0}
{"epoch": 28, "training_loss": 69.24836325645447, "training_acc": 52.0, "val_loss": 17.259597778320312, "val_acc": 56.0}
{"epoch": 29, "training_loss": 69.2626576423645, "training_acc": 52.0, "val_loss": 17.264382541179657, "val_acc": 56.0}
{"epoch": 30, "training_loss": 69.27687168121338, "training_acc": 52.0, "val_loss": 17.262235283851624, "val_acc": 56.0}
{"epoch": 31, "training_loss": 69.28221917152405, "training_acc": 52.0, "val_loss": 17.268390953540802, "val_acc": 56.0}
{"epoch": 32, "training_loss": 69.25073170661926, "training_acc": 52.0, "val_loss": 17.26253032684326, "val_acc": 56.0}
{"epoch": 33, "training_loss": 69.2574610710144, "training_acc": 52.0, "val_loss": 17.257188260555267, "val_acc": 56.0}
{"epoch": 34, "training_loss": 69.26047444343567, "training_acc": 52.0, "val_loss": 17.248722910881042, "val_acc": 56.0}
{"epoch": 35, "training_loss": 69.24013495445251, "training_acc": 52.0, "val_loss": 17.24383383989334, "val_acc": 56.0}
{"epoch": 36, "training_loss": 69.237619638443, "training_acc": 52.0, "val_loss": 17.238670587539673, "val_acc": 56.0}
{"epoch": 37, "training_loss": 69.23730826377869, "training_acc": 52.0, "val_loss": 17.232635617256165, "val_acc": 56.0}
{"epoch": 38, "training_loss": 69.2290723323822, "training_acc": 52.0, "val_loss": 17.2219917178154, "val_acc": 56.0}
{"epoch": 39, "training_loss": 69.2515754699707, "training_acc": 52.0, "val_loss": 17.20840483903885, "val_acc": 56.0}
{"epoch": 40, "training_loss": 69.231929063797, "training_acc": 52.0, "val_loss": 17.202678322792053, "val_acc": 56.0}
{"epoch": 41, "training_loss": 69.25729990005493, "training_acc": 52.0, "val_loss": 17.1991765499115, "val_acc": 56.0}
{"epoch": 42, "training_loss": 69.27058005332947, "training_acc": 52.0, "val_loss": 17.197318375110626, "val_acc": 56.0}
{"epoch": 43, "training_loss": 69.24137306213379, "training_acc": 52.0, "val_loss": 17.200078070163727, "val_acc": 56.0}
{"epoch": 44, "training_loss": 69.25500440597534, "training_acc": 52.0, "val_loss": 17.209823429584503, "val_acc": 56.0}
{"epoch": 45, "training_loss": 69.24142599105835, "training_acc": 52.0, "val_loss": 17.217525839805603, "val_acc": 56.0}
{"epoch": 46, "training_loss": 69.2547516822815, "training_acc": 52.0, "val_loss": 17.227226495742798, "val_acc": 56.0}
{"epoch": 47, "training_loss": 69.23922038078308, "training_acc": 52.0, "val_loss": 17.237025499343872, "val_acc": 56.0}
{"epoch": 48, "training_loss": 69.2251513004303, "training_acc": 52.0, "val_loss": 17.241157591342926, "val_acc": 56.0}
{"epoch": 49, "training_loss": 69.23203468322754, "training_acc": 52.0, "val_loss": 17.244701087474823, "val_acc": 56.0}
{"epoch": 50, "training_loss": 69.2535126209259, "training_acc": 52.0, "val_loss": 17.251043021678925, "val_acc": 56.0}
{"epoch": 51, "training_loss": 69.25179648399353, "training_acc": 52.0, "val_loss": 17.25791096687317, "val_acc": 56.0}
{"epoch": 52, "training_loss": 69.25779819488525, "training_acc": 52.0, "val_loss": 17.262963950634003, "val_acc": 56.0}
{"epoch": 53, "training_loss": 69.26706004142761, "training_acc": 52.0, "val_loss": 17.268122732639313, "val_acc": 56.0}
{"epoch": 54, "training_loss": 69.22368288040161, "training_acc": 52.0, "val_loss": 17.271442711353302, "val_acc": 56.0}
{"epoch": 55, "training_loss": 69.25893115997314, "training_acc": 52.0, "val_loss": 17.26868599653244, "val_acc": 56.0}
{"epoch": 56, "training_loss": 69.24892330169678, "training_acc": 52.0, "val_loss": 17.268824577331543, "val_acc": 56.0}
{"epoch": 57, "training_loss": 69.24754667282104, "training_acc": 52.0, "val_loss": 17.26982295513153, "val_acc": 56.0}
{"epoch": 58, "training_loss": 69.23846554756165, "training_acc": 52.0, "val_loss": 17.265520989894867, "val_acc": 56.0}
{"epoch": 59, "training_loss": 69.26132917404175, "training_acc": 52.0, "val_loss": 17.26142019033432, "val_acc": 56.0}
{"epoch": 60, "training_loss": 69.28171586990356, "training_acc": 52.0, "val_loss": 17.251834273338318, "val_acc": 56.0}
{"epoch": 61, "training_loss": 69.24004292488098, "training_acc": 52.0, "val_loss": 17.24766194820404, "val_acc": 56.0}
