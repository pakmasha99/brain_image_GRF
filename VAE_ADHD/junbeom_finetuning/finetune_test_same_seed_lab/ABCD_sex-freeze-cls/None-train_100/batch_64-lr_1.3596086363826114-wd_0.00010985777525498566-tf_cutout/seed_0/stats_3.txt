"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 10505.811447143555, "training_acc": 53.0, "val_loss": 2041.1605834960938, "val_acc": 52.0}
{"epoch": 1, "training_loss": 11597.9140625, "training_acc": 49.0, "val_loss": 5421.444320678711, "val_acc": 48.0}
{"epoch": 2, "training_loss": 20423.329162597656, "training_acc": 47.0, "val_loss": 1562.7347946166992, "val_acc": 48.0}
{"epoch": 3, "training_loss": 6875.473846435547, "training_acc": 49.0, "val_loss": 3584.591293334961, "val_acc": 52.0}
{"epoch": 4, "training_loss": 14249.504150390625, "training_acc": 53.0, "val_loss": 2961.138153076172, "val_acc": 52.0}
{"epoch": 5, "training_loss": 9324.516677856445, "training_acc": 53.0, "val_loss": 1224.7881889343262, "val_acc": 48.0}
{"epoch": 6, "training_loss": 6592.523986816406, "training_acc": 47.0, "val_loss": 2193.307876586914, "val_acc": 48.0}
{"epoch": 7, "training_loss": 7143.168640136719, "training_acc": 47.0, "val_loss": 864.4167900085449, "val_acc": 52.0}
{"epoch": 8, "training_loss": 4419.72297668457, "training_acc": 53.0, "val_loss": 1728.140640258789, "val_acc": 52.0}
{"epoch": 9, "training_loss": 5632.63427734375, "training_acc": 53.0, "val_loss": 444.7157382965088, "val_acc": 48.0}
{"epoch": 10, "training_loss": 2916.694839477539, "training_acc": 47.0, "val_loss": 641.9600009918213, "val_acc": 48.0}
{"epoch": 11, "training_loss": 2787.8802490234375, "training_acc": 45.0, "val_loss": 935.8158111572266, "val_acc": 52.0}
{"epoch": 12, "training_loss": 3044.6943969726562, "training_acc": 53.0, "val_loss": 641.392707824707, "val_acc": 48.0}
{"epoch": 13, "training_loss": 3002.2930297851562, "training_acc": 47.0, "val_loss": 237.85159587860107, "val_acc": 48.0}
{"epoch": 14, "training_loss": 2454.623825073242, "training_acc": 45.0, "val_loss": 1585.1780891418457, "val_acc": 52.0}
{"epoch": 15, "training_loss": 5828.064971923828, "training_acc": 53.0, "val_loss": 399.27239418029785, "val_acc": 52.0}
{"epoch": 16, "training_loss": 2129.2919311523438, "training_acc": 59.0, "val_loss": 1824.4007110595703, "val_acc": 48.0}
{"epoch": 17, "training_loss": 7167.557342529297, "training_acc": 47.0, "val_loss": 681.6446781158447, "val_acc": 48.0}
{"epoch": 18, "training_loss": 3392.2727813720703, "training_acc": 45.0, "val_loss": 1621.596336364746, "val_acc": 52.0}
{"epoch": 19, "training_loss": 6267.103958129883, "training_acc": 53.0, "val_loss": 844.0743446350098, "val_acc": 52.0}
{"epoch": 20, "training_loss": 2705.2142333984375, "training_acc": 53.0, "val_loss": 971.1483955383301, "val_acc": 48.0}
{"epoch": 21, "training_loss": 3472.640914916992, "training_acc": 47.0, "val_loss": 480.2187442779541, "val_acc": 52.0}
{"epoch": 22, "training_loss": 2319.881706237793, "training_acc": 53.0, "val_loss": 487.1311664581299, "val_acc": 52.0}
{"epoch": 23, "training_loss": 1837.503562927246, "training_acc": 53.0, "val_loss": 622.9198455810547, "val_acc": 48.0}
{"epoch": 24, "training_loss": 1621.1237263679504, "training_acc": 51.0, "val_loss": 84.34246778488159, "val_acc": 52.0}
{"epoch": 25, "training_loss": 1125.2450942993164, "training_acc": 49.0, "val_loss": 357.73186683654785, "val_acc": 48.0}
{"epoch": 26, "training_loss": 2135.827980041504, "training_acc": 43.0, "val_loss": 794.6788787841797, "val_acc": 52.0}
{"epoch": 27, "training_loss": 2339.918109893799, "training_acc": 53.0, "val_loss": 971.5901374816895, "val_acc": 48.0}
{"epoch": 28, "training_loss": 4215.546325683594, "training_acc": 47.0, "val_loss": 761.040735244751, "val_acc": 48.0}
{"epoch": 29, "training_loss": 2430.9667892456055, "training_acc": 51.0, "val_loss": 723.5782146453857, "val_acc": 52.0}
{"epoch": 30, "training_loss": 2406.3293685913086, "training_acc": 53.0, "val_loss": 615.1566028594971, "val_acc": 48.0}
{"epoch": 31, "training_loss": 2703.844352722168, "training_acc": 47.0, "val_loss": 247.44348526000977, "val_acc": 48.0}
{"epoch": 32, "training_loss": 1862.3328399658203, "training_acc": 51.0, "val_loss": 1348.3348846435547, "val_acc": 52.0}
{"epoch": 33, "training_loss": 4888.925079345703, "training_acc": 53.0, "val_loss": 167.7346110343933, "val_acc": 52.0}
{"epoch": 34, "training_loss": 3132.727264404297, "training_acc": 43.0, "val_loss": 2053.071975708008, "val_acc": 48.0}
{"epoch": 35, "training_loss": 7915.77392578125, "training_acc": 47.0, "val_loss": 678.9989948272705, "val_acc": 48.0}
{"epoch": 36, "training_loss": 3492.791793823242, "training_acc": 45.0, "val_loss": 1769.1497802734375, "val_acc": 52.0}
{"epoch": 37, "training_loss": 6932.301055908203, "training_acc": 53.0, "val_loss": 1163.0512237548828, "val_acc": 52.0}
{"epoch": 38, "training_loss": 2737.4701550006866, "training_acc": 53.0, "val_loss": 1297.0338821411133, "val_acc": 48.0}
{"epoch": 39, "training_loss": 6031.6300048828125, "training_acc": 47.0, "val_loss": 1207.687759399414, "val_acc": 48.0}
{"epoch": 40, "training_loss": 2905.1206047534943, "training_acc": 51.0, "val_loss": 1208.2489967346191, "val_acc": 52.0}
{"epoch": 41, "training_loss": 5273.150924682617, "training_acc": 53.0, "val_loss": 1362.201976776123, "val_acc": 52.0}
{"epoch": 42, "training_loss": 3992.8469009399414, "training_acc": 53.0, "val_loss": 1065.8222198486328, "val_acc": 48.0}
{"epoch": 43, "training_loss": 5576.179931640625, "training_acc": 47.0, "val_loss": 1516.6741371154785, "val_acc": 48.0}
