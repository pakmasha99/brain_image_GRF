"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 9935.851810455322, "training_acc": 49.0, "val_loss": 2365.1166915893555, "val_acc": 52.0}
{"epoch": 1, "training_loss": 9181.716766357422, "training_acc": 59.0, "val_loss": 5219.584655761719, "val_acc": 48.0}
{"epoch": 2, "training_loss": 19510.45245361328, "training_acc": 47.0, "val_loss": 1139.5805358886719, "val_acc": 48.0}
{"epoch": 3, "training_loss": 5660.8001708984375, "training_acc": 55.0, "val_loss": 4414.420700073242, "val_acc": 52.0}
{"epoch": 4, "training_loss": 17622.552795410156, "training_acc": 53.0, "val_loss": 3892.0928955078125, "val_acc": 52.0}
{"epoch": 5, "training_loss": 12323.031188964844, "training_acc": 53.0, "val_loss": 359.94255542755127, "val_acc": 48.0}
{"epoch": 6, "training_loss": 3465.1719360351562, "training_acc": 47.0, "val_loss": 1599.8932838439941, "val_acc": 48.0}
{"epoch": 7, "training_loss": 4923.108612060547, "training_acc": 47.0, "val_loss": 1425.8959770202637, "val_acc": 52.0}
{"epoch": 8, "training_loss": 6400.719573974609, "training_acc": 53.0, "val_loss": 2070.7706451416016, "val_acc": 52.0}
{"epoch": 9, "training_loss": 6904.5526123046875, "training_acc": 53.0, "val_loss": 357.82787799835205, "val_acc": 48.0}
{"epoch": 10, "training_loss": 2233.2628631591797, "training_acc": 47.0, "val_loss": 428.08475494384766, "val_acc": 48.0}
{"epoch": 11, "training_loss": 2294.467529296875, "training_acc": 49.0, "val_loss": 1173.3803749084473, "val_acc": 52.0}
{"epoch": 12, "training_loss": 3991.726173400879, "training_acc": 53.0, "val_loss": 410.84461212158203, "val_acc": 48.0}
{"epoch": 13, "training_loss": 2037.492561340332, "training_acc": 47.0, "val_loss": 119.63938474655151, "val_acc": 52.0}
{"epoch": 14, "training_loss": 385.5995149612427, "training_acc": 53.0, "val_loss": 794.6530818939209, "val_acc": 48.0}
{"epoch": 15, "training_loss": 3078.849540710449, "training_acc": 47.0, "val_loss": 214.81082439422607, "val_acc": 52.0}
{"epoch": 16, "training_loss": 887.6796379089355, "training_acc": 53.0, "val_loss": 410.0436210632324, "val_acc": 48.0}
{"epoch": 17, "training_loss": 1285.5781984329224, "training_acc": 47.0, "val_loss": 824.598217010498, "val_acc": 52.0}
{"epoch": 18, "training_loss": 3304.1352615356445, "training_acc": 53.0, "val_loss": 374.043345451355, "val_acc": 52.0}
{"epoch": 19, "training_loss": 2162.6492767333984, "training_acc": 51.0, "val_loss": 1120.9049224853516, "val_acc": 48.0}
{"epoch": 20, "training_loss": 3713.8069229125977, "training_acc": 47.0, "val_loss": 755.9479713439941, "val_acc": 52.0}
{"epoch": 21, "training_loss": 3523.9667358398438, "training_acc": 53.0, "val_loss": 913.658332824707, "val_acc": 52.0}
{"epoch": 22, "training_loss": 2520.8297691345215, "training_acc": 47.0, "val_loss": 102.60133743286133, "val_acc": 48.0}
{"epoch": 23, "training_loss": 818.6169586181641, "training_acc": 57.0, "val_loss": 586.9602680206299, "val_acc": 52.0}
{"epoch": 24, "training_loss": 1542.1579427719116, "training_acc": 51.0, "val_loss": 118.02675724029541, "val_acc": 52.0}
{"epoch": 25, "training_loss": 657.1164855957031, "training_acc": 59.0, "val_loss": 122.30445146560669, "val_acc": 48.0}
{"epoch": 26, "training_loss": 1986.3553466796875, "training_acc": 41.0, "val_loss": 1027.5490760803223, "val_acc": 52.0}
{"epoch": 27, "training_loss": 3193.1341552734375, "training_acc": 53.0, "val_loss": 830.5263519287109, "val_acc": 48.0}
{"epoch": 28, "training_loss": 4029.695098876953, "training_acc": 47.0, "val_loss": 680.6110858917236, "val_acc": 48.0}
{"epoch": 29, "training_loss": 2878.903434753418, "training_acc": 45.0, "val_loss": 1004.7572135925293, "val_acc": 52.0}
{"epoch": 30, "training_loss": 3467.848777770996, "training_acc": 53.0, "val_loss": 323.32146167755127, "val_acc": 48.0}
{"epoch": 31, "training_loss": 1586.7245178222656, "training_acc": 47.0, "val_loss": 292.43805408477783, "val_acc": 52.0}
{"epoch": 32, "training_loss": 1111.105484008789, "training_acc": 53.0, "val_loss": 418.80717277526855, "val_acc": 48.0}
{"epoch": 33, "training_loss": 1443.8851699829102, "training_acc": 47.0, "val_loss": 678.6597728729248, "val_acc": 52.0}
{"epoch": 34, "training_loss": 2746.994789123535, "training_acc": 53.0, "val_loss": 272.2869873046875, "val_acc": 52.0}
{"epoch": 35, "training_loss": 1869.7310943603516, "training_acc": 53.0, "val_loss": 1157.4311256408691, "val_acc": 48.0}
{"epoch": 36, "training_loss": 3984.360023498535, "training_acc": 47.0, "val_loss": 682.3239803314209, "val_acc": 52.0}
{"epoch": 37, "training_loss": 2993.2125396728516, "training_acc": 53.0, "val_loss": 653.6196231842041, "val_acc": 52.0}
{"epoch": 38, "training_loss": 2204.5223541259766, "training_acc": 49.0, "val_loss": 418.07379722595215, "val_acc": 48.0}
{"epoch": 39, "training_loss": 2110.356964111328, "training_acc": 37.0, "val_loss": 261.54818534851074, "val_acc": 52.0}
{"epoch": 40, "training_loss": 1060.5681266784668, "training_acc": 61.0, "val_loss": 570.6456184387207, "val_acc": 48.0}
{"epoch": 41, "training_loss": 1252.5528619289398, "training_acc": 61.0, "val_loss": 348.5438346862793, "val_acc": 52.0}
