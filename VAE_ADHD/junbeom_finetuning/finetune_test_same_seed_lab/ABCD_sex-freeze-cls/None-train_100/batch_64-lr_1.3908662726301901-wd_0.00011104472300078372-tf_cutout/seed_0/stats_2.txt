"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 10760.595039367676, "training_acc": 53.0, "val_loss": 2267.9025650024414, "val_acc": 52.0}
{"epoch": 1, "training_loss": 11882.261291503906, "training_acc": 49.0, "val_loss": 5193.681335449219, "val_acc": 48.0}
{"epoch": 2, "training_loss": 19362.689453125, "training_acc": 47.0, "val_loss": 1249.4205474853516, "val_acc": 48.0}
{"epoch": 3, "training_loss": 7407.606597900391, "training_acc": 45.0, "val_loss": 4150.739288330078, "val_acc": 52.0}
{"epoch": 4, "training_loss": 16597.368225097656, "training_acc": 53.0, "val_loss": 3637.6922607421875, "val_acc": 52.0}
{"epoch": 5, "training_loss": 12107.167022705078, "training_acc": 53.0, "val_loss": 449.7797966003418, "val_acc": 48.0}
{"epoch": 6, "training_loss": 3378.805160522461, "training_acc": 47.0, "val_loss": 1350.6895065307617, "val_acc": 48.0}
{"epoch": 7, "training_loss": 3653.9417419433594, "training_acc": 47.0, "val_loss": 1703.9411544799805, "val_acc": 52.0}
{"epoch": 8, "training_loss": 8079.307434082031, "training_acc": 53.0, "val_loss": 2625.6290435791016, "val_acc": 52.0}
{"epoch": 9, "training_loss": 9158.652587890625, "training_acc": 53.0, "val_loss": 354.23901081085205, "val_acc": 52.0}
{"epoch": 10, "training_loss": 3819.8297729492188, "training_acc": 49.0, "val_loss": 2941.0125732421875, "val_acc": 48.0}
{"epoch": 11, "training_loss": 11993.938720703125, "training_acc": 47.0, "val_loss": 2109.280204772949, "val_acc": 48.0}
{"epoch": 12, "training_loss": 6408.34566116333, "training_acc": 47.0, "val_loss": 1699.907112121582, "val_acc": 52.0}
{"epoch": 13, "training_loss": 8452.401306152344, "training_acc": 53.0, "val_loss": 3215.3480529785156, "val_acc": 52.0}
{"epoch": 14, "training_loss": 12010.36245727539, "training_acc": 53.0, "val_loss": 1706.6232681274414, "val_acc": 52.0}
{"epoch": 15, "training_loss": 4260.137054443359, "training_acc": 53.0, "val_loss": 2276.7738342285156, "val_acc": 48.0}
{"epoch": 16, "training_loss": 11139.122802734375, "training_acc": 47.0, "val_loss": 3844.6895599365234, "val_acc": 48.0}
{"epoch": 17, "training_loss": 14826.588806152344, "training_acc": 47.0, "val_loss": 1910.1068496704102, "val_acc": 48.0}
{"epoch": 18, "training_loss": 5369.162931442261, "training_acc": 47.0, "val_loss": 1146.3241577148438, "val_acc": 52.0}
{"epoch": 19, "training_loss": 4700.117172241211, "training_acc": 53.0, "val_loss": 862.809944152832, "val_acc": 52.0}
{"epoch": 20, "training_loss": 2389.465934753418, "training_acc": 53.0, "val_loss": 471.76852226257324, "val_acc": 48.0}
{"epoch": 21, "training_loss": 1409.8166942596436, "training_acc": 49.0, "val_loss": 59.31875705718994, "val_acc": 48.0}
{"epoch": 22, "training_loss": 835.378791809082, "training_acc": 51.0, "val_loss": 383.40232372283936, "val_acc": 52.0}
{"epoch": 23, "training_loss": 1512.5913391113281, "training_acc": 53.0, "val_loss": 393.67735385894775, "val_acc": 48.0}
{"epoch": 24, "training_loss": 1734.8860778808594, "training_acc": 47.0, "val_loss": 461.7467403411865, "val_acc": 52.0}
{"epoch": 25, "training_loss": 1478.730842590332, "training_acc": 51.0, "val_loss": 47.63323366641998, "val_acc": 48.0}
{"epoch": 26, "training_loss": 1741.919677734375, "training_acc": 41.0, "val_loss": 941.8455123901367, "val_acc": 52.0}
{"epoch": 27, "training_loss": 2731.9703369140625, "training_acc": 53.0, "val_loss": 1040.5652046203613, "val_acc": 48.0}
{"epoch": 28, "training_loss": 4885.285415649414, "training_acc": 47.0, "val_loss": 1061.9050979614258, "val_acc": 48.0}
{"epoch": 29, "training_loss": 2601.4537267684937, "training_acc": 55.0, "val_loss": 451.8287658691406, "val_acc": 52.0}
{"epoch": 30, "training_loss": 1323.0112648010254, "training_acc": 53.0, "val_loss": 899.9564170837402, "val_acc": 48.0}
{"epoch": 31, "training_loss": 3709.35107421875, "training_acc": 47.0, "val_loss": 324.29285049438477, "val_acc": 48.0}
{"epoch": 32, "training_loss": 1703.4271774291992, "training_acc": 57.0, "val_loss": 1430.3488731384277, "val_acc": 52.0}
{"epoch": 33, "training_loss": 5310.328063964844, "training_acc": 53.0, "val_loss": 395.21403312683105, "val_acc": 52.0}
{"epoch": 34, "training_loss": 1981.329330444336, "training_acc": 59.0, "val_loss": 1588.4310722351074, "val_acc": 48.0}
{"epoch": 35, "training_loss": 6040.162826538086, "training_acc": 47.0, "val_loss": 219.02000904083252, "val_acc": 48.0}
{"epoch": 36, "training_loss": 2593.5670471191406, "training_acc": 49.0, "val_loss": 2210.8837127685547, "val_acc": 52.0}
{"epoch": 37, "training_loss": 8549.050842285156, "training_acc": 53.0, "val_loss": 1560.9098434448242, "val_acc": 52.0}
{"epoch": 38, "training_loss": 4553.10461807251, "training_acc": 53.0, "val_loss": 1416.7194366455078, "val_acc": 48.0}
{"epoch": 39, "training_loss": 7287.612457275391, "training_acc": 47.0, "val_loss": 2156.608772277832, "val_acc": 48.0}
{"epoch": 40, "training_loss": 7243.897232055664, "training_acc": 47.0, "val_loss": 459.98382568359375, "val_acc": 52.0}
{"epoch": 41, "training_loss": 2573.1902618408203, "training_acc": 53.0, "val_loss": 1294.9225425720215, "val_acc": 52.0}
{"epoch": 42, "training_loss": 4243.665054321289, "training_acc": 53.0, "val_loss": 498.1104373931885, "val_acc": 48.0}
{"epoch": 43, "training_loss": 2504.8543701171875, "training_acc": 47.0, "val_loss": 372.7153539657593, "val_acc": 48.0}
{"epoch": 44, "training_loss": 1996.9489669799805, "training_acc": 51.0, "val_loss": 1170.0043678283691, "val_acc": 52.0}
