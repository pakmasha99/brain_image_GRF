"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.15155029296875, "training_acc": 53.0, "val_loss": 17.31916069984436, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.1579020023346, "training_acc": 53.0, "val_loss": 17.320670187473297, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.19147729873657, "training_acc": 53.0, "val_loss": 17.320050299167633, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.14640069007874, "training_acc": 53.0, "val_loss": 17.324919998645782, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.15641903877258, "training_acc": 53.0, "val_loss": 17.325395345687866, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.13081169128418, "training_acc": 53.0, "val_loss": 17.322345077991486, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.14336395263672, "training_acc": 53.0, "val_loss": 17.31896996498108, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.14309763908386, "training_acc": 53.0, "val_loss": 17.3160582780838, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.12557530403137, "training_acc": 53.0, "val_loss": 17.31480062007904, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.14652347564697, "training_acc": 53.0, "val_loss": 17.31465458869934, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.15938878059387, "training_acc": 53.0, "val_loss": 17.31555163860321, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.14395809173584, "training_acc": 53.0, "val_loss": 17.314523458480835, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.14421200752258, "training_acc": 53.0, "val_loss": 17.3144668340683, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.15562081336975, "training_acc": 53.0, "val_loss": 17.315226793289185, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.16959500312805, "training_acc": 53.0, "val_loss": 17.315299808979034, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.17150831222534, "training_acc": 53.0, "val_loss": 17.31850802898407, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.12992811203003, "training_acc": 53.0, "val_loss": 17.31874644756317, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.15879535675049, "training_acc": 53.0, "val_loss": 17.317253351211548, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.12568688392639, "training_acc": 53.0, "val_loss": 17.315194010734558, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.13372445106506, "training_acc": 53.0, "val_loss": 17.312289774417877, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.11972761154175, "training_acc": 53.0, "val_loss": 17.311178147792816, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.19013714790344, "training_acc": 53.0, "val_loss": 17.311030626296997, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.16649675369263, "training_acc": 53.0, "val_loss": 17.31051653623581, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.16181302070618, "training_acc": 53.0, "val_loss": 17.310574650764465, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.17118430137634, "training_acc": 53.0, "val_loss": 17.310546338558197, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.17917513847351, "training_acc": 53.0, "val_loss": 17.310532927513123, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.16150093078613, "training_acc": 53.0, "val_loss": 17.310725152492523, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.14216613769531, "training_acc": 53.0, "val_loss": 17.312392592430115, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.1638343334198, "training_acc": 53.0, "val_loss": 17.314426600933075, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.13794827461243, "training_acc": 53.0, "val_loss": 17.3158198595047, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.1631166934967, "training_acc": 53.0, "val_loss": 17.31625646352768, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.16874170303345, "training_acc": 53.0, "val_loss": 17.315158247947693, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.12301421165466, "training_acc": 53.0, "val_loss": 17.31260120868683, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.19833493232727, "training_acc": 53.0, "val_loss": 17.31114536523819, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.14457774162292, "training_acc": 53.0, "val_loss": 17.31111854314804, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.1535255908966, "training_acc": 53.0, "val_loss": 17.310844361782074, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.17100834846497, "training_acc": 53.0, "val_loss": 17.311078310012817, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.12802958488464, "training_acc": 53.0, "val_loss": 17.3110231757164, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.13425779342651, "training_acc": 53.0, "val_loss": 17.311355471611023, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.14521527290344, "training_acc": 53.0, "val_loss": 17.312034964561462, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.14989829063416, "training_acc": 53.0, "val_loss": 17.312942445278168, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.13579297065735, "training_acc": 53.0, "val_loss": 17.312857508659363, "val_acc": 52.0}
