"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.4079864025116, "training_acc": 49.0, "val_loss": 17.310670018196106, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.53821301460266, "training_acc": 53.0, "val_loss": 17.332763969898224, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.19198989868164, "training_acc": 53.0, "val_loss": 17.31858402490616, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.21174573898315, "training_acc": 53.0, "val_loss": 17.310598492622375, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.48703384399414, "training_acc": 53.0, "val_loss": 17.316676676273346, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.22434854507446, "training_acc": 53.0, "val_loss": 17.30964481830597, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.13444375991821, "training_acc": 53.0, "val_loss": 17.321258783340454, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.12110209465027, "training_acc": 53.0, "val_loss": 17.341364920139313, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.24652814865112, "training_acc": 53.0, "val_loss": 17.361806333065033, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.27607989311218, "training_acc": 53.0, "val_loss": 17.35091209411621, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.15689039230347, "training_acc": 53.0, "val_loss": 17.318637669086456, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.06945872306824, "training_acc": 53.0, "val_loss": 17.30991303920746, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.42846417427063, "training_acc": 53.0, "val_loss": 17.326219379901886, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.30284118652344, "training_acc": 52.0, "val_loss": 17.318882048130035, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.2912483215332, "training_acc": 53.0, "val_loss": 17.311939597129822, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.20581531524658, "training_acc": 53.0, "val_loss": 17.309676110744476, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.13581800460815, "training_acc": 53.0, "val_loss": 17.309440672397614, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.12207007408142, "training_acc": 53.0, "val_loss": 17.314548790454865, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.15365600585938, "training_acc": 53.0, "val_loss": 17.325027287006378, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.13973617553711, "training_acc": 53.0, "val_loss": 17.357029020786285, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.42199635505676, "training_acc": 53.0, "val_loss": 17.38041192293167, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.39010834693909, "training_acc": 53.0, "val_loss": 17.347915470600128, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.17811369895935, "training_acc": 53.0, "val_loss": 17.33330339193344, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.16414761543274, "training_acc": 53.0, "val_loss": 17.318502068519592, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.12812161445618, "training_acc": 53.0, "val_loss": 17.310935258865356, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.19115710258484, "training_acc": 53.0, "val_loss": 17.310819029808044, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.19050407409668, "training_acc": 53.0, "val_loss": 17.313118278980255, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.20512795448303, "training_acc": 53.0, "val_loss": 17.310279607772827, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.1785945892334, "training_acc": 53.0, "val_loss": 17.30937510728836, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.11909103393555, "training_acc": 53.0, "val_loss": 17.3159196972847, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.1656231880188, "training_acc": 53.0, "val_loss": 17.336569726467133, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.20868015289307, "training_acc": 53.0, "val_loss": 17.345601320266724, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.17639398574829, "training_acc": 53.0, "val_loss": 17.330434918403625, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.16221499443054, "training_acc": 53.0, "val_loss": 17.316368222236633, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.20055317878723, "training_acc": 53.0, "val_loss": 17.31007993221283, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.14385342597961, "training_acc": 53.0, "val_loss": 17.309819161891937, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.26864242553711, "training_acc": 53.0, "val_loss": 17.31007695198059, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.19419312477112, "training_acc": 53.0, "val_loss": 17.32008457183838, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.10350608825684, "training_acc": 53.0, "val_loss": 17.37067401409149, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.36788773536682, "training_acc": 53.0, "val_loss": 17.41717904806137, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.44337487220764, "training_acc": 53.0, "val_loss": 17.39565283060074, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.33259177207947, "training_acc": 53.0, "val_loss": 17.368018627166748, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.33146667480469, "training_acc": 53.0, "val_loss": 17.328175902366638, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.18504977226257, "training_acc": 53.0, "val_loss": 17.314353585243225, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.13858890533447, "training_acc": 53.0, "val_loss": 17.311088740825653, "val_acc": 52.0}
{"epoch": 45, "training_loss": 69.17464065551758, "training_acc": 53.0, "val_loss": 17.310720682144165, "val_acc": 52.0}
{"epoch": 46, "training_loss": 69.16066288948059, "training_acc": 53.0, "val_loss": 17.315921187400818, "val_acc": 52.0}
{"epoch": 47, "training_loss": 69.1225700378418, "training_acc": 53.0, "val_loss": 17.316776514053345, "val_acc": 52.0}
