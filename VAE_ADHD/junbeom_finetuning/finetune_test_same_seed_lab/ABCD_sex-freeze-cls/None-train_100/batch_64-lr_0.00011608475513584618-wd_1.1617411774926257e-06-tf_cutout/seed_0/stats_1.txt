"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 70.58351063728333, "training_acc": 47.0, "val_loss": 17.44576394557953, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.81796360015869, "training_acc": 47.0, "val_loss": 17.351314425468445, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.36790537834167, "training_acc": 46.0, "val_loss": 17.30962097644806, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.32271552085876, "training_acc": 53.0, "val_loss": 17.308372259140015, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.1823456287384, "training_acc": 53.0, "val_loss": 17.320114374160767, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.1270763874054, "training_acc": 53.0, "val_loss": 17.329776287078857, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.2528612613678, "training_acc": 53.0, "val_loss": 17.341017723083496, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.20132422447205, "training_acc": 53.0, "val_loss": 17.340579628944397, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.19335532188416, "training_acc": 53.0, "val_loss": 17.337791621685028, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.20717740058899, "training_acc": 53.0, "val_loss": 17.337635159492493, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.19494247436523, "training_acc": 53.0, "val_loss": 17.336829006671906, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.17671346664429, "training_acc": 53.0, "val_loss": 17.337046563625336, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.20597195625305, "training_acc": 53.0, "val_loss": 17.334337532520294, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.21236157417297, "training_acc": 53.0, "val_loss": 17.329604923725128, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.14619040489197, "training_acc": 53.0, "val_loss": 17.325088381767273, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.1715157032013, "training_acc": 53.0, "val_loss": 17.32017993927002, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.17886853218079, "training_acc": 53.0, "val_loss": 17.317241430282593, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.1552186012268, "training_acc": 53.0, "val_loss": 17.317622900009155, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.16708779335022, "training_acc": 53.0, "val_loss": 17.317062616348267, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.20465040206909, "training_acc": 53.0, "val_loss": 17.318537831306458, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.16152477264404, "training_acc": 53.0, "val_loss": 17.317703366279602, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.17958569526672, "training_acc": 53.0, "val_loss": 17.31286644935608, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.1572732925415, "training_acc": 53.0, "val_loss": 17.31012314558029, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.21598839759827, "training_acc": 53.0, "val_loss": 17.30785369873047, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.16043210029602, "training_acc": 53.0, "val_loss": 17.308619618415833, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.1523048877716, "training_acc": 53.0, "val_loss": 17.309436202049255, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.1380341053009, "training_acc": 53.0, "val_loss": 17.310620844364166, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.19718766212463, "training_acc": 53.0, "val_loss": 17.31354296207428, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.16391777992249, "training_acc": 53.0, "val_loss": 17.314037680625916, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.17300844192505, "training_acc": 53.0, "val_loss": 17.314258217811584, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.17176675796509, "training_acc": 53.0, "val_loss": 17.312103509902954, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.14313077926636, "training_acc": 53.0, "val_loss": 17.31254905462265, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.14830708503723, "training_acc": 53.0, "val_loss": 17.314013838768005, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.13017702102661, "training_acc": 53.0, "val_loss": 17.314305901527405, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.14351391792297, "training_acc": 53.0, "val_loss": 17.314067482948303, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.14275860786438, "training_acc": 53.0, "val_loss": 17.315571010112762, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.14586567878723, "training_acc": 53.0, "val_loss": 17.317111790180206, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.1657145023346, "training_acc": 53.0, "val_loss": 17.318835854530334, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.1502161026001, "training_acc": 53.0, "val_loss": 17.316192388534546, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.14902186393738, "training_acc": 53.0, "val_loss": 17.31443554162979, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.15591049194336, "training_acc": 53.0, "val_loss": 17.312727868556976, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.1415798664093, "training_acc": 53.0, "val_loss": 17.311891913414, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.1687741279602, "training_acc": 53.0, "val_loss": 17.310510575771332, "val_acc": 52.0}
