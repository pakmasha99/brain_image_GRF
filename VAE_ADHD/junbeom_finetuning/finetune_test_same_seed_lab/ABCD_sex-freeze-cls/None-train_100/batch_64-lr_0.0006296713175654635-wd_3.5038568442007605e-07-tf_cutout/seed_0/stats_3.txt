"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.3807098865509, "training_acc": 53.0, "val_loss": 17.3470601439476, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.54910778999329, "training_acc": 53.0, "val_loss": 17.315302789211273, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.07810664176941, "training_acc": 53.0, "val_loss": 17.363257706165314, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.49818110466003, "training_acc": 47.0, "val_loss": 17.401321232318878, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.67185068130493, "training_acc": 47.0, "val_loss": 17.36772507429123, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.6320698261261, "training_acc": 45.0, "val_loss": 17.318862676620483, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.21802115440369, "training_acc": 53.0, "val_loss": 17.31037348508835, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.11131262779236, "training_acc": 53.0, "val_loss": 17.33747571706772, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.18124127388, "training_acc": 53.0, "val_loss": 17.375417053699493, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.35973381996155, "training_acc": 53.0, "val_loss": 17.379727959632874, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.27616310119629, "training_acc": 53.0, "val_loss": 17.3280268907547, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.12073063850403, "training_acc": 53.0, "val_loss": 17.30971336364746, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.17905139923096, "training_acc": 53.0, "val_loss": 17.317071557044983, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.22178769111633, "training_acc": 53.0, "val_loss": 17.320510745048523, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.46351742744446, "training_acc": 41.0, "val_loss": 17.31499880552292, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.07844233512878, "training_acc": 53.0, "val_loss": 17.32330322265625, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.25397968292236, "training_acc": 53.0, "val_loss": 17.40536093711853, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.7259590625763, "training_acc": 53.0, "val_loss": 17.432963848114014, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.32567954063416, "training_acc": 53.0, "val_loss": 17.342014610767365, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.40114545822144, "training_acc": 53.0, "val_loss": 17.311154305934906, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.30707669258118, "training_acc": 53.0, "val_loss": 17.324455082416534, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.26079201698303, "training_acc": 53.0, "val_loss": 17.316412925720215, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.21476125717163, "training_acc": 53.0, "val_loss": 17.309825122356415, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.25917363166809, "training_acc": 53.0, "val_loss": 17.3293337225914, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.42875027656555, "training_acc": 53.0, "val_loss": 17.343847453594208, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.20229029655457, "training_acc": 53.0, "val_loss": 17.316962778568268, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.1203670501709, "training_acc": 53.0, "val_loss": 17.30998605489731, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.1485481262207, "training_acc": 53.0, "val_loss": 17.310424149036407, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.21271300315857, "training_acc": 53.0, "val_loss": 17.313234508037567, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.21901297569275, "training_acc": 53.0, "val_loss": 17.310187220573425, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.1330201625824, "training_acc": 53.0, "val_loss": 17.316889762878418, "val_acc": 52.0}
