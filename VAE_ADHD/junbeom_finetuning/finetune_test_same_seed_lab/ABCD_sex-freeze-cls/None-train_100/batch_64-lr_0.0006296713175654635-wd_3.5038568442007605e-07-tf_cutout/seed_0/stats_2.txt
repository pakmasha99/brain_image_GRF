"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 70.15426015853882, "training_acc": 47.0, "val_loss": 17.342983186244965, "val_acc": 52.0}
{"epoch": 1, "training_loss": 68.91186738014221, "training_acc": 57.0, "val_loss": 17.394673824310303, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.50623345375061, "training_acc": 53.0, "val_loss": 17.651408910751343, "val_acc": 52.0}
{"epoch": 3, "training_loss": 70.17113256454468, "training_acc": 53.0, "val_loss": 17.550988495349884, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.58134484291077, "training_acc": 53.0, "val_loss": 17.357562482357025, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.29336214065552, "training_acc": 53.0, "val_loss": 17.322638630867004, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.4328761100769, "training_acc": 50.0, "val_loss": 17.389287054538727, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.62922191619873, "training_acc": 47.0, "val_loss": 17.364460229873657, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.4688949584961, "training_acc": 47.0, "val_loss": 17.316241562366486, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.12313795089722, "training_acc": 53.0, "val_loss": 17.327146232128143, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.28420376777649, "training_acc": 53.0, "val_loss": 17.41923838853836, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.51428723335266, "training_acc": 53.0, "val_loss": 17.439986765384674, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.42239785194397, "training_acc": 53.0, "val_loss": 17.35955774784088, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.16707134246826, "training_acc": 53.0, "val_loss": 17.31066256761551, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.2662091255188, "training_acc": 53.0, "val_loss": 17.316295206546783, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.25278973579407, "training_acc": 53.0, "val_loss": 17.316699028015137, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.365079164505, "training_acc": 53.0, "val_loss": 17.309512197971344, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.10684084892273, "training_acc": 53.0, "val_loss": 17.324870824813843, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.12766242027283, "training_acc": 53.0, "val_loss": 17.43520349264145, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.64634442329407, "training_acc": 53.0, "val_loss": 17.51965433359146, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.75812721252441, "training_acc": 53.0, "val_loss": 17.448526620864868, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.46022009849548, "training_acc": 53.0, "val_loss": 17.37026870250702, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.40671229362488, "training_acc": 53.0, "val_loss": 17.312927544116974, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.22838234901428, "training_acc": 53.0, "val_loss": 17.3093318939209, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.1702835559845, "training_acc": 53.0, "val_loss": 17.3100084066391, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.22008061408997, "training_acc": 53.0, "val_loss": 17.308516800403595, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.23023343086243, "training_acc": 53.0, "val_loss": 17.323295772075653, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.13103556632996, "training_acc": 53.0, "val_loss": 17.33444333076477, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.14771342277527, "training_acc": 53.0, "val_loss": 17.339777946472168, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.17937088012695, "training_acc": 53.0, "val_loss": 17.342984676361084, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.22551345825195, "training_acc": 53.0, "val_loss": 17.33059287071228, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.15490508079529, "training_acc": 53.0, "val_loss": 17.328983545303345, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.15048289299011, "training_acc": 53.0, "val_loss": 17.31695532798767, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.16213583946228, "training_acc": 53.0, "val_loss": 17.310211062431335, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.14135694503784, "training_acc": 53.0, "val_loss": 17.309430241584778, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.15876388549805, "training_acc": 53.0, "val_loss": 17.308852076530457, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.15261816978455, "training_acc": 53.0, "val_loss": 17.308595776557922, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.21829104423523, "training_acc": 53.0, "val_loss": 17.30889528989792, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.16204047203064, "training_acc": 53.0, "val_loss": 17.310871183872223, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.13461685180664, "training_acc": 53.0, "val_loss": 17.317233979701996, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.136479139328, "training_acc": 53.0, "val_loss": 17.322979867458344, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.1400499343872, "training_acc": 53.0, "val_loss": 17.323195934295654, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.17172360420227, "training_acc": 53.0, "val_loss": 17.314918339252472, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.16987180709839, "training_acc": 53.0, "val_loss": 17.31553226709366, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.1381287574768, "training_acc": 53.0, "val_loss": 17.31214076280594, "val_acc": 52.0}
