"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 70.14825081825256, "training_acc": 47.0, "val_loss": 17.434239387512207, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.84630012512207, "training_acc": 47.0, "val_loss": 17.398831248283386, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.67673993110657, "training_acc": 47.0, "val_loss": 17.36382246017456, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.52676582336426, "training_acc": 47.0, "val_loss": 17.336279153823853, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.44235467910767, "training_acc": 45.0, "val_loss": 17.319130897521973, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.27414608001709, "training_acc": 52.0, "val_loss": 17.30949580669403, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.20874452590942, "training_acc": 53.0, "val_loss": 17.304126918315887, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.13986563682556, "training_acc": 53.0, "val_loss": 17.30276048183441, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.11856460571289, "training_acc": 53.0, "val_loss": 17.30445772409439, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.12551641464233, "training_acc": 53.0, "val_loss": 17.309334874153137, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.1389172077179, "training_acc": 53.0, "val_loss": 17.315544188022614, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.15145945549011, "training_acc": 53.0, "val_loss": 17.3202782869339, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.15743613243103, "training_acc": 53.0, "val_loss": 17.322474718093872, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.17773079872131, "training_acc": 53.0, "val_loss": 17.323240637779236, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.16300201416016, "training_acc": 53.0, "val_loss": 17.324216663837433, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.15088224411011, "training_acc": 53.0, "val_loss": 17.322543263435364, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.1941146850586, "training_acc": 53.0, "val_loss": 17.32008010149002, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.14648723602295, "training_acc": 53.0, "val_loss": 17.32042282819748, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.14418911933899, "training_acc": 53.0, "val_loss": 17.318283021450043, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.14468836784363, "training_acc": 53.0, "val_loss": 17.315907776355743, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.11494183540344, "training_acc": 53.0, "val_loss": 17.315493524074554, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.12886548042297, "training_acc": 53.0, "val_loss": 17.314693331718445, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.14068078994751, "training_acc": 53.0, "val_loss": 17.313703894615173, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.13221669197083, "training_acc": 53.0, "val_loss": 17.31170266866684, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.13285064697266, "training_acc": 53.0, "val_loss": 17.310282588005066, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.1314127445221, "training_acc": 53.0, "val_loss": 17.31017231941223, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.14408564567566, "training_acc": 53.0, "val_loss": 17.309580743312836, "val_acc": 52.0}
