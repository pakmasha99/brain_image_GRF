"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 100829.14128112793, "training_acc": 50.0, "val_loss": 19545.458984375, "val_acc": 44.0}
{"epoch": 1, "training_loss": 103938.54150390625, "training_acc": 48.0, "val_loss": 49642.3583984375, "val_acc": 56.0}
{"epoch": 2, "training_loss": 211615.951171875, "training_acc": 52.0, "val_loss": 30368.069458007812, "val_acc": 56.0}
{"epoch": 3, "training_loss": 90687.6806640625, "training_acc": 52.0, "val_loss": 31737.686157226562, "val_acc": 44.0}
{"epoch": 4, "training_loss": 144470.2978515625, "training_acc": 48.0, "val_loss": 55300.5126953125, "val_acc": 44.0}
{"epoch": 5, "training_loss": 194914.16796875, "training_acc": 48.0, "val_loss": 28809.695434570312, "val_acc": 44.0}
{"epoch": 6, "training_loss": 72383.85736083984, "training_acc": 48.0, "val_loss": 22821.060180664062, "val_acc": 56.0}
{"epoch": 7, "training_loss": 121041.65478515625, "training_acc": 52.0, "val_loss": 44059.7412109375, "val_acc": 56.0}
{"epoch": 8, "training_loss": 191656.67578125, "training_acc": 52.0, "val_loss": 35304.01611328125, "val_acc": 56.0}
{"epoch": 9, "training_loss": 131286.796875, "training_acc": 52.0, "val_loss": 2288.1017684936523, "val_acc": 56.0}
{"epoch": 10, "training_loss": 42958.448486328125, "training_acc": 50.0, "val_loss": 44506.695556640625, "val_acc": 44.0}
{"epoch": 11, "training_loss": 174023.35107421875, "training_acc": 48.0, "val_loss": 49268.3349609375, "val_acc": 44.0}
{"epoch": 12, "training_loss": 169995.35888671875, "training_acc": 48.0, "val_loss": 21504.388427734375, "val_acc": 44.0}
{"epoch": 13, "training_loss": 51110.44155883789, "training_acc": 52.0, "val_loss": 12208.455657958984, "val_acc": 56.0}
{"epoch": 14, "training_loss": 58378.9521484375, "training_acc": 52.0, "val_loss": 13367.054748535156, "val_acc": 56.0}
{"epoch": 15, "training_loss": 46823.38494873047, "training_acc": 52.0, "val_loss": 9933.06655883789, "val_acc": 44.0}
{"epoch": 16, "training_loss": 44866.77685546875, "training_acc": 48.0, "val_loss": 13974.064636230469, "val_acc": 44.0}
{"epoch": 17, "training_loss": 38414.164001464844, "training_acc": 48.0, "val_loss": 10165.293884277344, "val_acc": 56.0}
{"epoch": 18, "training_loss": 53872.965087890625, "training_acc": 52.0, "val_loss": 16164.024353027344, "val_acc": 56.0}
{"epoch": 19, "training_loss": 61077.722900390625, "training_acc": 52.0, "val_loss": 1045.849609375, "val_acc": 44.0}
{"epoch": 20, "training_loss": 11253.610046386719, "training_acc": 48.0, "val_loss": 2418.717384338379, "val_acc": 44.0}
{"epoch": 21, "training_loss": 14296.489868164062, "training_acc": 54.0, "val_loss": 9219.671630859375, "val_acc": 56.0}
{"epoch": 22, "training_loss": 35055.912109375, "training_acc": 52.0, "val_loss": 3571.5896606445312, "val_acc": 44.0}
{"epoch": 23, "training_loss": 15910.692321777344, "training_acc": 48.0, "val_loss": 613.920783996582, "val_acc": 44.0}
{"epoch": 24, "training_loss": 18166.0322265625, "training_acc": 46.0, "val_loss": 12484.484100341797, "val_acc": 56.0}
{"epoch": 25, "training_loss": 49599.14697265625, "training_acc": 52.0, "val_loss": 849.3043899536133, "val_acc": 56.0}
{"epoch": 26, "training_loss": 15247.040649414062, "training_acc": 62.0, "val_loss": 22810.55450439453, "val_acc": 44.0}
{"epoch": 27, "training_loss": 84077.7041015625, "training_acc": 48.0, "val_loss": 14379.786682128906, "val_acc": 44.0}
{"epoch": 28, "training_loss": 31959.315460205078, "training_acc": 54.0, "val_loss": 5426.873397827148, "val_acc": 56.0}
{"epoch": 29, "training_loss": 21965.337341308594, "training_acc": 52.0, "val_loss": 2693.6279296875, "val_acc": 44.0}
{"epoch": 30, "training_loss": 8247.9105758667, "training_acc": 48.0, "val_loss": 5096.836853027344, "val_acc": 56.0}
{"epoch": 31, "training_loss": 20768.210998535156, "training_acc": 52.0, "val_loss": 2022.028923034668, "val_acc": 44.0}
{"epoch": 32, "training_loss": 5108.6796951293945, "training_acc": 48.0, "val_loss": 7120.226287841797, "val_acc": 56.0}
{"epoch": 33, "training_loss": 31905.109619140625, "training_acc": 52.0, "val_loss": 2915.651321411133, "val_acc": 56.0}
{"epoch": 34, "training_loss": 18178.550903320312, "training_acc": 54.0, "val_loss": 12622.224426269531, "val_acc": 44.0}
{"epoch": 35, "training_loss": 41036.58215332031, "training_acc": 48.0, "val_loss": 2746.9539642333984, "val_acc": 56.0}
{"epoch": 36, "training_loss": 15494.145874023438, "training_acc": 52.0, "val_loss": 2032.3083877563477, "val_acc": 56.0}
{"epoch": 37, "training_loss": 21692.307250976562, "training_acc": 42.0, "val_loss": 10274.7314453125, "val_acc": 44.0}
{"epoch": 38, "training_loss": 28345.597900390625, "training_acc": 48.0, "val_loss": 8334.410095214844, "val_acc": 56.0}
{"epoch": 39, "training_loss": 46185.0498046875, "training_acc": 52.0, "val_loss": 12023.162078857422, "val_acc": 56.0}
{"epoch": 40, "training_loss": 41241.743713378906, "training_acc": 52.0, "val_loss": 9355.104064941406, "val_acc": 44.0}
{"epoch": 41, "training_loss": 42987.248291015625, "training_acc": 48.0, "val_loss": 13054.644775390625, "val_acc": 44.0}
{"epoch": 42, "training_loss": 36309.21823120117, "training_acc": 48.0, "val_loss": 10822.361755371094, "val_acc": 56.0}
