"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 117111.6565208435, "training_acc": 53.0, "val_loss": 23847.15118408203, "val_acc": 52.0}
{"epoch": 1, "training_loss": 86568.63330078125, "training_acc": 57.0, "val_loss": 43276.019287109375, "val_acc": 48.0}
{"epoch": 2, "training_loss": 165265.55615234375, "training_acc": 47.0, "val_loss": 13175.035095214844, "val_acc": 48.0}
{"epoch": 3, "training_loss": 57263.6630859375, "training_acc": 51.0, "val_loss": 32324.484252929688, "val_acc": 52.0}
{"epoch": 4, "training_loss": 128503.5830078125, "training_acc": 53.0, "val_loss": 25745.736694335938, "val_acc": 52.0}
{"epoch": 5, "training_loss": 76169.77014160156, "training_acc": 53.0, "val_loss": 14067.950439453125, "val_acc": 48.0}
{"epoch": 6, "training_loss": 73742.205078125, "training_acc": 47.0, "val_loss": 25166.668701171875, "val_acc": 48.0}
{"epoch": 7, "training_loss": 88517.0908203125, "training_acc": 47.0, "val_loss": 1878.0349731445312, "val_acc": 52.0}
{"epoch": 8, "training_loss": 18137.819946289062, "training_acc": 53.0, "val_loss": 7702.180480957031, "val_acc": 52.0}
{"epoch": 9, "training_loss": 19213.64247894287, "training_acc": 53.0, "val_loss": 111.50519847869873, "val_acc": 48.0}
{"epoch": 10, "training_loss": 7113.109329223633, "training_acc": 59.0, "val_loss": 8206.414031982422, "val_acc": 52.0}
{"epoch": 11, "training_loss": 24745.816436767578, "training_acc": 53.0, "val_loss": 10255.197143554688, "val_acc": 48.0}
{"epoch": 12, "training_loss": 45131.48095703125, "training_acc": 47.0, "val_loss": 7109.961700439453, "val_acc": 48.0}
{"epoch": 13, "training_loss": 33167.44274902344, "training_acc": 39.0, "val_loss": 9607.11898803711, "val_acc": 52.0}
{"epoch": 14, "training_loss": 32600.570678710938, "training_acc": 53.0, "val_loss": 3846.1009979248047, "val_acc": 48.0}
{"epoch": 15, "training_loss": 20090.521606445312, "training_acc": 47.0, "val_loss": 77.07327604293823, "val_acc": 52.0}
{"epoch": 16, "training_loss": 2203.28515625, "training_acc": 47.0, "val_loss": 5461.665725708008, "val_acc": 52.0}
{"epoch": 17, "training_loss": 21038.06561279297, "training_acc": 53.0, "val_loss": 1309.5508575439453, "val_acc": 48.0}
{"epoch": 18, "training_loss": 3956.099395751953, "training_acc": 47.0, "val_loss": 7894.138336181641, "val_acc": 52.0}
{"epoch": 19, "training_loss": 33712.90393066406, "training_acc": 53.0, "val_loss": 4401.119613647461, "val_acc": 52.0}
{"epoch": 20, "training_loss": 15526.2421875, "training_acc": 61.0, "val_loss": 10036.085510253906, "val_acc": 48.0}
{"epoch": 21, "training_loss": 35383.25354003906, "training_acc": 47.0, "val_loss": 4258.707046508789, "val_acc": 52.0}
{"epoch": 22, "training_loss": 21031.732055664062, "training_acc": 53.0, "val_loss": 3969.6502685546875, "val_acc": 52.0}
{"epoch": 23, "training_loss": 16018.161987304688, "training_acc": 55.0, "val_loss": 7033.160400390625, "val_acc": 48.0}
{"epoch": 24, "training_loss": 20056.07551574707, "training_acc": 47.0, "val_loss": 10527.339935302734, "val_acc": 52.0}
{"epoch": 25, "training_loss": 46417.978515625, "training_acc": 53.0, "val_loss": 13047.505187988281, "val_acc": 52.0}
{"epoch": 26, "training_loss": 41107.184509277344, "training_acc": 53.0, "val_loss": 8187.248992919922, "val_acc": 48.0}
{"epoch": 27, "training_loss": 40398.157470703125, "training_acc": 47.0, "val_loss": 9893.387603759766, "val_acc": 48.0}
{"epoch": 28, "training_loss": 27399.72055053711, "training_acc": 47.0, "val_loss": 2727.06241607666, "val_acc": 52.0}
{"epoch": 29, "training_loss": 7705.46826171875, "training_acc": 55.0, "val_loss": 1859.0091705322266, "val_acc": 52.0}
{"epoch": 30, "training_loss": 9308.092712402344, "training_acc": 45.0, "val_loss": 1864.4939422607422, "val_acc": 52.0}
{"epoch": 31, "training_loss": 4856.166778564453, "training_acc": 53.0, "val_loss": 4322.36328125, "val_acc": 52.0}
{"epoch": 32, "training_loss": 14151.735900878906, "training_acc": 53.0, "val_loss": 5939.072036743164, "val_acc": 48.0}
{"epoch": 33, "training_loss": 25691.357666015625, "training_acc": 47.0, "val_loss": 846.8667984008789, "val_acc": 52.0}
{"epoch": 34, "training_loss": 3847.2063751220703, "training_acc": 53.0, "val_loss": 4879.012680053711, "val_acc": 48.0}
