"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 73.64352416992188, "training_acc": 45.0, "val_loss": 17.50914752483368, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.96615147590637, "training_acc": 49.0, "val_loss": 18.103213608264923, "val_acc": 52.0}
{"epoch": 2, "training_loss": 72.30757451057434, "training_acc": 53.0, "val_loss": 18.224716186523438, "val_acc": 52.0}
{"epoch": 3, "training_loss": 71.51188111305237, "training_acc": 53.0, "val_loss": 17.375710606575012, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.04069519042969, "training_acc": 52.0, "val_loss": 17.535169422626495, "val_acc": 52.0}
{"epoch": 5, "training_loss": 71.84728598594666, "training_acc": 47.0, "val_loss": 17.782969772815704, "val_acc": 52.0}
{"epoch": 6, "training_loss": 70.83433055877686, "training_acc": 47.0, "val_loss": 17.338332533836365, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.54270029067993, "training_acc": 47.0, "val_loss": 17.525866627693176, "val_acc": 52.0}
{"epoch": 8, "training_loss": 71.01298642158508, "training_acc": 53.0, "val_loss": 17.820054292678833, "val_acc": 52.0}
{"epoch": 9, "training_loss": 70.52325677871704, "training_acc": 53.0, "val_loss": 17.46901422739029, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.9152283668518, "training_acc": 53.0, "val_loss": 17.314231395721436, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.57092261314392, "training_acc": 47.0, "val_loss": 17.4095019698143, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.75080180168152, "training_acc": 47.0, "val_loss": 17.360928654670715, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.35396432876587, "training_acc": 47.0, "val_loss": 17.306555807590485, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.4152946472168, "training_acc": 53.0, "val_loss": 17.398105561733246, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.86029934883118, "training_acc": 53.0, "val_loss": 17.424768209457397, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.43258261680603, "training_acc": 53.0, "val_loss": 17.312118411064148, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.14743900299072, "training_acc": 53.0, "val_loss": 17.321504652500153, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.27204203605652, "training_acc": 53.0, "val_loss": 17.336612939834595, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.39028596878052, "training_acc": 47.0, "val_loss": 17.32243299484253, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.21961450576782, "training_acc": 53.0, "val_loss": 17.31068044900894, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.06088995933533, "training_acc": 53.0, "val_loss": 17.38867163658142, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.38112282752991, "training_acc": 53.0, "val_loss": 17.459125816822052, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.52332282066345, "training_acc": 53.0, "val_loss": 17.37714856863022, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.1271665096283, "training_acc": 53.0, "val_loss": 17.306512594223022, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.6022584438324, "training_acc": 45.0, "val_loss": 17.362557351589203, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.50563478469849, "training_acc": 47.0, "val_loss": 17.322583496570587, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.25717544555664, "training_acc": 52.0, "val_loss": 17.310400307178497, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.07318234443665, "training_acc": 53.0, "val_loss": 17.356908321380615, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.27123665809631, "training_acc": 53.0, "val_loss": 17.429694533348083, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.46071529388428, "training_acc": 53.0, "val_loss": 17.374327778816223, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.24370455741882, "training_acc": 53.0, "val_loss": 17.320720851421356, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.22092032432556, "training_acc": 53.0, "val_loss": 17.30808913707733, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.35243201255798, "training_acc": 49.0, "val_loss": 17.376643419265747, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.63297390937805, "training_acc": 47.0, "val_loss": 17.336350679397583, "val_acc": 52.0}
{"epoch": 35, "training_loss": 70.27953362464905, "training_acc": 37.0, "val_loss": 17.323875427246094, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.20047307014465, "training_acc": 53.0, "val_loss": 17.321202158927917, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.18314051628113, "training_acc": 53.0, "val_loss": 17.306698858737946, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.15214467048645, "training_acc": 53.0, "val_loss": 17.30673909187317, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.36322784423828, "training_acc": 53.0, "val_loss": 17.30814278125763, "val_acc": 52.0}
{"epoch": 40, "training_loss": 70.04692816734314, "training_acc": 53.0, "val_loss": 17.359107732772827, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.33238935470581, "training_acc": 53.0, "val_loss": 17.310933768749237, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.25934338569641, "training_acc": 53.0, "val_loss": 17.306728661060333, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.25162053108215, "training_acc": 53.0, "val_loss": 17.32804775238037, "val_acc": 52.0}
