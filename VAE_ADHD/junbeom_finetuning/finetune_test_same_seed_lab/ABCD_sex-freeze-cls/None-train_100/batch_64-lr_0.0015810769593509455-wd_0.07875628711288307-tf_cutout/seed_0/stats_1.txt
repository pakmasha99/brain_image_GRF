"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 72.54284739494324, "training_acc": 43.0, "val_loss": 17.52735823392868, "val_acc": 52.0}
{"epoch": 1, "training_loss": 70.20598983764648, "training_acc": 53.0, "val_loss": 17.427268624305725, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.84588265419006, "training_acc": 47.0, "val_loss": 17.38121062517166, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.44275522232056, "training_acc": 49.0, "val_loss": 17.339232563972473, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.42542457580566, "training_acc": 53.0, "val_loss": 17.4422949552536, "val_acc": 52.0}
{"epoch": 5, "training_loss": 70.19181203842163, "training_acc": 53.0, "val_loss": 17.638571560382843, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.95718693733215, "training_acc": 53.0, "val_loss": 17.37733781337738, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.06323647499084, "training_acc": 53.0, "val_loss": 17.38761067390442, "val_acc": 52.0}
{"epoch": 8, "training_loss": 70.1143639087677, "training_acc": 47.0, "val_loss": 17.55700707435608, "val_acc": 52.0}
{"epoch": 9, "training_loss": 70.31503796577454, "training_acc": 47.0, "val_loss": 17.360755801200867, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.26978707313538, "training_acc": 51.0, "val_loss": 17.343120276927948, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.18269109725952, "training_acc": 53.0, "val_loss": 17.521731555461884, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.73967051506042, "training_acc": 53.0, "val_loss": 17.520172894001007, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.66747689247131, "training_acc": 53.0, "val_loss": 17.393888533115387, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.1660988330841, "training_acc": 53.0, "val_loss": 17.31562316417694, "val_acc": 52.0}
{"epoch": 15, "training_loss": 70.080561876297, "training_acc": 41.0, "val_loss": 17.383792996406555, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.4632580280304, "training_acc": 47.0, "val_loss": 17.314055562019348, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.02349019050598, "training_acc": 53.0, "val_loss": 17.387881875038147, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.50083255767822, "training_acc": 53.0, "val_loss": 17.50807911157608, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.70861029624939, "training_acc": 53.0, "val_loss": 17.389290034770966, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.00573468208313, "training_acc": 53.0, "val_loss": 17.32492744922638, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.30400037765503, "training_acc": 53.0, "val_loss": 17.565737664699554, "val_acc": 52.0}
{"epoch": 22, "training_loss": 70.4715085029602, "training_acc": 47.0, "val_loss": 17.527228593826294, "val_acc": 52.0}
{"epoch": 23, "training_loss": 70.06167125701904, "training_acc": 47.0, "val_loss": 17.317090928554535, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.18812966346741, "training_acc": 53.0, "val_loss": 17.44380295276642, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.50742292404175, "training_acc": 53.0, "val_loss": 17.56126880645752, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.86814498901367, "training_acc": 53.0, "val_loss": 17.429988086223602, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.4002525806427, "training_acc": 53.0, "val_loss": 17.32112467288971, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.32384157180786, "training_acc": 53.0, "val_loss": 17.32296347618103, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.44789624214172, "training_acc": 53.0, "val_loss": 17.31695532798767, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.20408773422241, "training_acc": 53.0, "val_loss": 17.320674657821655, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.4074432849884, "training_acc": 45.0, "val_loss": 17.31729507446289, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.65895986557007, "training_acc": 53.0, "val_loss": 17.33885556459427, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.15314483642578, "training_acc": 53.0, "val_loss": 17.328578233718872, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.35698056221008, "training_acc": 53.0, "val_loss": 17.32288748025894, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.11083507537842, "training_acc": 53.0, "val_loss": 17.356085777282715, "val_acc": 52.0}
