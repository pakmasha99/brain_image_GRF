"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.24240016937256, "training_acc": 53.0, "val_loss": 17.347627878189087, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.18491053581238, "training_acc": 53.0, "val_loss": 17.34348088502884, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.16721820831299, "training_acc": 53.0, "val_loss": 17.338870465755463, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.20056128501892, "training_acc": 53.0, "val_loss": 17.3345148563385, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.21080732345581, "training_acc": 53.0, "val_loss": 17.330600321292877, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.1517117023468, "training_acc": 53.0, "val_loss": 17.328380048274994, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.12715196609497, "training_acc": 53.0, "val_loss": 17.32614040374756, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.15688610076904, "training_acc": 53.0, "val_loss": 17.32371300458908, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.15059113502502, "training_acc": 53.0, "val_loss": 17.321491241455078, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.16339087486267, "training_acc": 53.0, "val_loss": 17.319756746292114, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.1610848903656, "training_acc": 53.0, "val_loss": 17.318446934223175, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.10355186462402, "training_acc": 53.0, "val_loss": 17.317982017993927, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.13174104690552, "training_acc": 53.0, "val_loss": 17.317691445350647, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.1432478427887, "training_acc": 53.0, "val_loss": 17.316903173923492, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.11766529083252, "training_acc": 53.0, "val_loss": 17.3166424036026, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.12732028961182, "training_acc": 53.0, "val_loss": 17.316356301307678, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.13014006614685, "training_acc": 53.0, "val_loss": 17.31608957052231, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.14281582832336, "training_acc": 53.0, "val_loss": 17.315496504306793, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.12205600738525, "training_acc": 53.0, "val_loss": 17.315061390399933, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.13547611236572, "training_acc": 53.0, "val_loss": 17.31477379798889, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.14169692993164, "training_acc": 53.0, "val_loss": 17.314383387565613, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.1388008594513, "training_acc": 53.0, "val_loss": 17.31421798467636, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.15272641181946, "training_acc": 53.0, "val_loss": 17.314253747463226, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.11266231536865, "training_acc": 53.0, "val_loss": 17.314162850379944, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.10609889030457, "training_acc": 53.0, "val_loss": 17.31385439634323, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.1558620929718, "training_acc": 53.0, "val_loss": 17.313572764396667, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.14087677001953, "training_acc": 53.0, "val_loss": 17.313309013843536, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.11695408821106, "training_acc": 53.0, "val_loss": 17.313265800476074, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.12518334388733, "training_acc": 53.0, "val_loss": 17.313167452812195, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.11925601959229, "training_acc": 53.0, "val_loss": 17.313453555107117, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.09312176704407, "training_acc": 53.0, "val_loss": 17.314311861991882, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.12243056297302, "training_acc": 53.0, "val_loss": 17.315135896205902, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.09770011901855, "training_acc": 53.0, "val_loss": 17.315557599067688, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.09035992622375, "training_acc": 53.0, "val_loss": 17.31618493795395, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.16164708137512, "training_acc": 53.0, "val_loss": 17.316508293151855, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.12116599082947, "training_acc": 53.0, "val_loss": 17.31702983379364, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.12698125839233, "training_acc": 53.0, "val_loss": 17.31761395931244, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.11576390266418, "training_acc": 53.0, "val_loss": 17.31809973716736, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.14213609695435, "training_acc": 53.0, "val_loss": 17.31880009174347, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.14080834388733, "training_acc": 53.0, "val_loss": 17.319004237651825, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.13419318199158, "training_acc": 53.0, "val_loss": 17.31916069984436, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.1375744342804, "training_acc": 53.0, "val_loss": 17.319433391094208, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.14237594604492, "training_acc": 53.0, "val_loss": 17.319495975971222, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.16558384895325, "training_acc": 53.0, "val_loss": 17.31983572244644, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.12872314453125, "training_acc": 53.0, "val_loss": 17.31983870267868, "val_acc": 52.0}
{"epoch": 45, "training_loss": 69.13848972320557, "training_acc": 53.0, "val_loss": 17.3196941614151, "val_acc": 52.0}
{"epoch": 46, "training_loss": 69.13744306564331, "training_acc": 53.0, "val_loss": 17.319586873054504, "val_acc": 52.0}
{"epoch": 47, "training_loss": 69.13524222373962, "training_acc": 53.0, "val_loss": 17.319245636463165, "val_acc": 52.0}
