"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.28155875205994, "training_acc": 53.0, "val_loss": 17.316481471061707, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.17413449287415, "training_acc": 52.0, "val_loss": 17.371000349521637, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.70573878288269, "training_acc": 47.0, "val_loss": 17.38126277923584, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.53761172294617, "training_acc": 47.0, "val_loss": 17.313304543495178, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.0739095211029, "training_acc": 53.0, "val_loss": 17.318718135356903, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.36390423774719, "training_acc": 53.0, "val_loss": 17.376261949539185, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.27434873580933, "training_acc": 53.0, "val_loss": 17.3769012093544, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.30725979804993, "training_acc": 53.0, "val_loss": 17.359313368797302, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.15833520889282, "training_acc": 53.0, "val_loss": 17.320023477077484, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.13806414604187, "training_acc": 53.0, "val_loss": 17.308855056762695, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.17913746833801, "training_acc": 53.0, "val_loss": 17.323173582553864, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.29314064979553, "training_acc": 51.0, "val_loss": 17.332065105438232, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.33256077766418, "training_acc": 46.0, "val_loss": 17.317549884319305, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.12908554077148, "training_acc": 53.0, "val_loss": 17.309463024139404, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.20382761955261, "training_acc": 53.0, "val_loss": 17.343921959400177, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.23059868812561, "training_acc": 53.0, "val_loss": 17.37123429775238, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.36362147331238, "training_acc": 53.0, "val_loss": 17.365863919258118, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.41166377067566, "training_acc": 53.0, "val_loss": 17.328475415706635, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.38542175292969, "training_acc": 53.0, "val_loss": 17.316922545433044, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.13444900512695, "training_acc": 53.0, "val_loss": 17.32337474822998, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.11524248123169, "training_acc": 53.0, "val_loss": 17.331287264823914, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.13906049728394, "training_acc": 53.0, "val_loss": 17.338372766971588, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.18627405166626, "training_acc": 53.0, "val_loss": 17.338401079177856, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.20758152008057, "training_acc": 53.0, "val_loss": 17.33177900314331, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.17871499061584, "training_acc": 53.0, "val_loss": 17.326270043849945, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.10082674026489, "training_acc": 53.0, "val_loss": 17.323581874370575, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.16060137748718, "training_acc": 53.0, "val_loss": 17.31279343366623, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.13282704353333, "training_acc": 53.0, "val_loss": 17.30925291776657, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.25448274612427, "training_acc": 53.0, "val_loss": 17.308732867240906, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.22982358932495, "training_acc": 53.0, "val_loss": 17.314034700393677, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.15618062019348, "training_acc": 53.0, "val_loss": 17.312011122703552, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.12004041671753, "training_acc": 53.0, "val_loss": 17.313727736473083, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.22674036026001, "training_acc": 53.0, "val_loss": 17.315149307250977, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.14712166786194, "training_acc": 53.0, "val_loss": 17.30884164571762, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.05310845375061, "training_acc": 53.0, "val_loss": 17.31046289205551, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.14794635772705, "training_acc": 53.0, "val_loss": 17.31887012720108, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.23950695991516, "training_acc": 53.0, "val_loss": 17.325927317142487, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.25876307487488, "training_acc": 54.0, "val_loss": 17.32964664697647, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.24015426635742, "training_acc": 52.0, "val_loss": 17.316429316997528, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.15671157836914, "training_acc": 53.0, "val_loss": 17.30904132127762, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.24087691307068, "training_acc": 53.0, "val_loss": 17.323173582553864, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.1681580543518, "training_acc": 53.0, "val_loss": 17.33453720808029, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.17670226097107, "training_acc": 53.0, "val_loss": 17.33517348766327, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.16340827941895, "training_acc": 53.0, "val_loss": 17.327016592025757, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.11664056777954, "training_acc": 53.0, "val_loss": 17.314812541007996, "val_acc": 52.0}
{"epoch": 45, "training_loss": 69.34740400314331, "training_acc": 53.0, "val_loss": 17.309100925922394, "val_acc": 52.0}
{"epoch": 46, "training_loss": 69.16683864593506, "training_acc": 53.0, "val_loss": 17.309924960136414, "val_acc": 52.0}
{"epoch": 47, "training_loss": 69.16465330123901, "training_acc": 53.0, "val_loss": 17.318183183670044, "val_acc": 52.0}
