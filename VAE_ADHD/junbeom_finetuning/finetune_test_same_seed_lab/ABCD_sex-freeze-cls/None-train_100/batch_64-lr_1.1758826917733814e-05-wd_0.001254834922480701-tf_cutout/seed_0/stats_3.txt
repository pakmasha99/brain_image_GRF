"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.24189114570618, "training_acc": 53.0, "val_loss": 17.31352210044861, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.20308136940002, "training_acc": 53.0, "val_loss": 17.31303036212921, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.21255993843079, "training_acc": 53.0, "val_loss": 17.31276512145996, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.22174787521362, "training_acc": 53.0, "val_loss": 17.312225699424744, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.20658421516418, "training_acc": 53.0, "val_loss": 17.311854660511017, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.17948913574219, "training_acc": 53.0, "val_loss": 17.31158345937729, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.16778564453125, "training_acc": 53.0, "val_loss": 17.311294376850128, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.19135236740112, "training_acc": 53.0, "val_loss": 17.311054468154907, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.16901969909668, "training_acc": 53.0, "val_loss": 17.310836911201477, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.15063619613647, "training_acc": 53.0, "val_loss": 17.310625314712524, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.1773796081543, "training_acc": 53.0, "val_loss": 17.31048971414566, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.16361570358276, "training_acc": 53.0, "val_loss": 17.310458421707153, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.16868305206299, "training_acc": 53.0, "val_loss": 17.310509085655212, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.17506337165833, "training_acc": 53.0, "val_loss": 17.310579121112823, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.17925524711609, "training_acc": 53.0, "val_loss": 17.310702800750732, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.1323595046997, "training_acc": 53.0, "val_loss": 17.31095016002655, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.14830803871155, "training_acc": 53.0, "val_loss": 17.311276495456696, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.13565850257874, "training_acc": 53.0, "val_loss": 17.311687767505646, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.15020680427551, "training_acc": 53.0, "val_loss": 17.312146723270416, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.1429169178009, "training_acc": 53.0, "val_loss": 17.312635481357574, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.14798331260681, "training_acc": 53.0, "val_loss": 17.31318086385727, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.15206170082092, "training_acc": 53.0, "val_loss": 17.313796281814575, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.14907789230347, "training_acc": 53.0, "val_loss": 17.314250767230988, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.14452791213989, "training_acc": 53.0, "val_loss": 17.31470823287964, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.1464774608612, "training_acc": 53.0, "val_loss": 17.315134406089783, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.16947484016418, "training_acc": 53.0, "val_loss": 17.31574982404709, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.14618992805481, "training_acc": 53.0, "val_loss": 17.316007614135742, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.12642812728882, "training_acc": 53.0, "val_loss": 17.316347360610962, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.14193105697632, "training_acc": 53.0, "val_loss": 17.31664687395096, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.12892436981201, "training_acc": 53.0, "val_loss": 17.316561937332153, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.1333909034729, "training_acc": 53.0, "val_loss": 17.316439747810364, "val_acc": 52.0}
