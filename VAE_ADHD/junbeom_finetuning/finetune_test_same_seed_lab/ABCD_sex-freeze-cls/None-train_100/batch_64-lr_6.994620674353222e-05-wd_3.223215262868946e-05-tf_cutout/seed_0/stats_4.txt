"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 70.1537868976593, "training_acc": 47.0, "val_loss": 17.42912083864212, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.81966471672058, "training_acc": 47.0, "val_loss": 17.391328513622284, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.6380832195282, "training_acc": 47.0, "val_loss": 17.355075478553772, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.48479104042053, "training_acc": 47.0, "val_loss": 17.328061163425446, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.4015805721283, "training_acc": 43.0, "val_loss": 17.31279492378235, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.23498630523682, "training_acc": 53.0, "val_loss": 17.305539548397064, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.17843556404114, "training_acc": 53.0, "val_loss": 17.302799224853516, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.12030386924744, "training_acc": 53.0, "val_loss": 17.303849756717682, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.1108295917511, "training_acc": 53.0, "val_loss": 17.30756312608719, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.12398672103882, "training_acc": 53.0, "val_loss": 17.31444001197815, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.14483428001404, "training_acc": 53.0, "val_loss": 17.32170581817627, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.1610631942749, "training_acc": 53.0, "val_loss": 17.32611060142517, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.1686646938324, "training_acc": 53.0, "val_loss": 17.326901853084564, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.18976640701294, "training_acc": 53.0, "val_loss": 17.325954139232635, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.16566348075867, "training_acc": 53.0, "val_loss": 17.325443029403687, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.15272617340088, "training_acc": 53.0, "val_loss": 17.322182655334473, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.20201635360718, "training_acc": 53.0, "val_loss": 17.318551242351532, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.14210438728333, "training_acc": 53.0, "val_loss": 17.318305373191833, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.1400842666626, "training_acc": 53.0, "val_loss": 17.315708100795746, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.14418625831604, "training_acc": 53.0, "val_loss": 17.313152551651, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.1110417842865, "training_acc": 53.0, "val_loss": 17.312751710414886, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.12514543533325, "training_acc": 53.0, "val_loss": 17.31209307909012, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.13802337646484, "training_acc": 53.0, "val_loss": 17.31133759021759, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.1296226978302, "training_acc": 53.0, "val_loss": 17.30964183807373, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.13125872612, "training_acc": 53.0, "val_loss": 17.30855107307434, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.13113832473755, "training_acc": 53.0, "val_loss": 17.308765649795532, "val_acc": 52.0}
