"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.23588132858276, "training_acc": 52.0, "val_loss": 17.236922681331635, "val_acc": 56.0}
{"epoch": 1, "training_loss": 69.23073863983154, "training_acc": 52.0, "val_loss": 17.224611341953278, "val_acc": 56.0}
{"epoch": 2, "training_loss": 69.24864554405212, "training_acc": 52.0, "val_loss": 17.223413288593292, "val_acc": 56.0}
{"epoch": 3, "training_loss": 69.26392531394958, "training_acc": 52.0, "val_loss": 17.230108380317688, "val_acc": 56.0}
{"epoch": 4, "training_loss": 69.23951840400696, "training_acc": 52.0, "val_loss": 17.24105477333069, "val_acc": 56.0}
{"epoch": 5, "training_loss": 69.223393201828, "training_acc": 52.0, "val_loss": 17.245082557201385, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.25626111030579, "training_acc": 52.0, "val_loss": 17.253875732421875, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.25178575515747, "training_acc": 52.0, "val_loss": 17.25856512784958, "val_acc": 56.0}
{"epoch": 8, "training_loss": 69.25691556930542, "training_acc": 52.0, "val_loss": 17.255885899066925, "val_acc": 56.0}
{"epoch": 9, "training_loss": 69.25255513191223, "training_acc": 52.0, "val_loss": 17.25934147834778, "val_acc": 56.0}
{"epoch": 10, "training_loss": 69.22964787483215, "training_acc": 52.0, "val_loss": 17.261633276939392, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.26012754440308, "training_acc": 52.0, "val_loss": 17.260783910751343, "val_acc": 56.0}
{"epoch": 12, "training_loss": 69.23003315925598, "training_acc": 52.0, "val_loss": 17.259158194065094, "val_acc": 56.0}
{"epoch": 13, "training_loss": 69.23951745033264, "training_acc": 52.0, "val_loss": 17.259296774864197, "val_acc": 56.0}
{"epoch": 14, "training_loss": 69.2496497631073, "training_acc": 52.0, "val_loss": 17.25531369447708, "val_acc": 56.0}
{"epoch": 15, "training_loss": 69.25482892990112, "training_acc": 52.0, "val_loss": 17.2487810254097, "val_acc": 56.0}
{"epoch": 16, "training_loss": 69.25235486030579, "training_acc": 52.0, "val_loss": 17.23865121603012, "val_acc": 56.0}
{"epoch": 17, "training_loss": 69.23445916175842, "training_acc": 52.0, "val_loss": 17.23126322031021, "val_acc": 56.0}
{"epoch": 18, "training_loss": 69.2403154373169, "training_acc": 52.0, "val_loss": 17.224830389022827, "val_acc": 56.0}
{"epoch": 19, "training_loss": 69.25451636314392, "training_acc": 52.0, "val_loss": 17.21947491168976, "val_acc": 56.0}
{"epoch": 20, "training_loss": 69.20143628120422, "training_acc": 52.0, "val_loss": 17.216995358467102, "val_acc": 56.0}
{"epoch": 21, "training_loss": 69.22058415412903, "training_acc": 52.0, "val_loss": 17.21414178609848, "val_acc": 56.0}
{"epoch": 22, "training_loss": 69.28359603881836, "training_acc": 52.0, "val_loss": 17.209286987781525, "val_acc": 56.0}
{"epoch": 23, "training_loss": 69.2597770690918, "training_acc": 52.0, "val_loss": 17.210008203983307, "val_acc": 56.0}
{"epoch": 24, "training_loss": 69.26060366630554, "training_acc": 52.0, "val_loss": 17.21370667219162, "val_acc": 56.0}
{"epoch": 25, "training_loss": 69.2463870048523, "training_acc": 52.0, "val_loss": 17.220109701156616, "val_acc": 56.0}
{"epoch": 26, "training_loss": 69.27740979194641, "training_acc": 52.0, "val_loss": 17.22261756658554, "val_acc": 56.0}
{"epoch": 27, "training_loss": 69.22884392738342, "training_acc": 52.0, "val_loss": 17.232373356819153, "val_acc": 56.0}
{"epoch": 28, "training_loss": 69.23844170570374, "training_acc": 52.0, "val_loss": 17.24172532558441, "val_acc": 56.0}
{"epoch": 29, "training_loss": 69.25338912010193, "training_acc": 52.0, "val_loss": 17.245955765247345, "val_acc": 56.0}
{"epoch": 30, "training_loss": 69.25804543495178, "training_acc": 52.0, "val_loss": 17.246165871620178, "val_acc": 56.0}
{"epoch": 31, "training_loss": 69.26805853843689, "training_acc": 52.0, "val_loss": 17.252151668071747, "val_acc": 56.0}
{"epoch": 32, "training_loss": 69.24058794975281, "training_acc": 52.0, "val_loss": 17.25044846534729, "val_acc": 56.0}
{"epoch": 33, "training_loss": 69.25311660766602, "training_acc": 52.0, "val_loss": 17.249108850955963, "val_acc": 56.0}
{"epoch": 34, "training_loss": 69.25309872627258, "training_acc": 52.0, "val_loss": 17.24540889263153, "val_acc": 56.0}
{"epoch": 35, "training_loss": 69.23968935012817, "training_acc": 52.0, "val_loss": 17.243842780590057, "val_acc": 56.0}
{"epoch": 36, "training_loss": 69.23934960365295, "training_acc": 52.0, "val_loss": 17.241668701171875, "val_acc": 56.0}
{"epoch": 37, "training_loss": 69.23952984809875, "training_acc": 52.0, "val_loss": 17.238351702690125, "val_acc": 56.0}
{"epoch": 38, "training_loss": 69.23386096954346, "training_acc": 52.0, "val_loss": 17.231041193008423, "val_acc": 56.0}
{"epoch": 39, "training_loss": 69.24181485176086, "training_acc": 52.0, "val_loss": 17.22072958946228, "val_acc": 56.0}
{"epoch": 40, "training_loss": 69.22674512863159, "training_acc": 52.0, "val_loss": 17.21579283475876, "val_acc": 56.0}
{"epoch": 41, "training_loss": 69.24939751625061, "training_acc": 52.0, "val_loss": 17.21210777759552, "val_acc": 56.0}
