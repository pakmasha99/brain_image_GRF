"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.23528265953064, "training_acc": 52.0, "val_loss": 17.236220836639404, "val_acc": 56.0}
{"epoch": 1, "training_loss": 69.22989845275879, "training_acc": 52.0, "val_loss": 17.225001752376556, "val_acc": 56.0}
{"epoch": 2, "training_loss": 69.24771070480347, "training_acc": 52.0, "val_loss": 17.223933339118958, "val_acc": 56.0}
{"epoch": 3, "training_loss": 69.26394391059875, "training_acc": 52.0, "val_loss": 17.23015308380127, "val_acc": 56.0}
{"epoch": 4, "training_loss": 69.23938035964966, "training_acc": 52.0, "val_loss": 17.24032759666443, "val_acc": 56.0}
{"epoch": 5, "training_loss": 69.22295355796814, "training_acc": 52.0, "val_loss": 17.244094610214233, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.25576877593994, "training_acc": 52.0, "val_loss": 17.252349853515625, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.25071263313293, "training_acc": 52.0, "val_loss": 17.256882786750793, "val_acc": 56.0}
{"epoch": 8, "training_loss": 69.25463461875916, "training_acc": 52.0, "val_loss": 17.254604399204254, "val_acc": 56.0}
{"epoch": 9, "training_loss": 69.25187635421753, "training_acc": 52.0, "val_loss": 17.258045077323914, "val_acc": 56.0}
{"epoch": 10, "training_loss": 69.22888994216919, "training_acc": 52.0, "val_loss": 17.260432243347168, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.25933003425598, "training_acc": 52.0, "val_loss": 17.259927093982697, "val_acc": 56.0}
{"epoch": 12, "training_loss": 69.22917103767395, "training_acc": 52.0, "val_loss": 17.25868582725525, "val_acc": 56.0}
{"epoch": 13, "training_loss": 69.23927283287048, "training_acc": 52.0, "val_loss": 17.259058356285095, "val_acc": 56.0}
{"epoch": 14, "training_loss": 69.24970602989197, "training_acc": 52.0, "val_loss": 17.255546152591705, "val_acc": 56.0}
{"epoch": 15, "training_loss": 69.255291223526, "training_acc": 52.0, "val_loss": 17.249584197998047, "val_acc": 56.0}
{"epoch": 16, "training_loss": 69.25244641304016, "training_acc": 52.0, "val_loss": 17.240142822265625, "val_acc": 56.0}
{"epoch": 17, "training_loss": 69.23485565185547, "training_acc": 52.0, "val_loss": 17.23315566778183, "val_acc": 56.0}
{"epoch": 18, "training_loss": 69.24047493934631, "training_acc": 52.0, "val_loss": 17.22695529460907, "val_acc": 56.0}
{"epoch": 19, "training_loss": 69.25417637825012, "training_acc": 52.0, "val_loss": 17.221669852733612, "val_acc": 56.0}
{"epoch": 20, "training_loss": 69.20081043243408, "training_acc": 52.0, "val_loss": 17.219024896621704, "val_acc": 56.0}
{"epoch": 21, "training_loss": 69.21955800056458, "training_acc": 52.0, "val_loss": 17.21598356962204, "val_acc": 56.0}
{"epoch": 22, "training_loss": 69.28229546546936, "training_acc": 52.0, "val_loss": 17.211036384105682, "val_acc": 56.0}
{"epoch": 23, "training_loss": 69.25901508331299, "training_acc": 52.0, "val_loss": 17.211349308490753, "val_acc": 56.0}
{"epoch": 24, "training_loss": 69.26036381721497, "training_acc": 52.0, "val_loss": 17.21445471048355, "val_acc": 56.0}
{"epoch": 25, "training_loss": 69.24553060531616, "training_acc": 52.0, "val_loss": 17.2201007604599, "val_acc": 56.0}
{"epoch": 26, "training_loss": 69.27719330787659, "training_acc": 52.0, "val_loss": 17.222172021865845, "val_acc": 56.0}
{"epoch": 27, "training_loss": 69.2292218208313, "training_acc": 52.0, "val_loss": 17.231038212776184, "val_acc": 56.0}
{"epoch": 28, "training_loss": 69.23714542388916, "training_acc": 52.0, "val_loss": 17.239581048488617, "val_acc": 56.0}
{"epoch": 29, "training_loss": 69.2524905204773, "training_acc": 52.0, "val_loss": 17.24349558353424, "val_acc": 56.0}
{"epoch": 30, "training_loss": 69.2561867237091, "training_acc": 52.0, "val_loss": 17.24378913640976, "val_acc": 56.0}
{"epoch": 31, "training_loss": 69.26575493812561, "training_acc": 52.0, "val_loss": 17.24948137998581, "val_acc": 56.0}
{"epoch": 32, "training_loss": 69.23934268951416, "training_acc": 52.0, "val_loss": 17.248156666755676, "val_acc": 56.0}
{"epoch": 33, "training_loss": 69.25232672691345, "training_acc": 52.0, "val_loss": 17.247208952903748, "val_acc": 56.0}
{"epoch": 34, "training_loss": 69.25190615653992, "training_acc": 52.0, "val_loss": 17.244087159633636, "val_acc": 56.0}
{"epoch": 35, "training_loss": 69.23934388160706, "training_acc": 52.0, "val_loss": 17.242932319641113, "val_acc": 56.0}
{"epoch": 36, "training_loss": 69.23929071426392, "training_acc": 52.0, "val_loss": 17.241187393665314, "val_acc": 56.0}
{"epoch": 37, "training_loss": 69.23957943916321, "training_acc": 52.0, "val_loss": 17.238333821296692, "val_acc": 56.0}
{"epoch": 38, "training_loss": 69.23459482192993, "training_acc": 52.0, "val_loss": 17.23168194293976, "val_acc": 56.0}
{"epoch": 39, "training_loss": 69.24034810066223, "training_acc": 52.0, "val_loss": 17.222106456756592, "val_acc": 56.0}
{"epoch": 40, "training_loss": 69.22651100158691, "training_acc": 52.0, "val_loss": 17.21748262643814, "val_acc": 56.0}
{"epoch": 41, "training_loss": 69.24885630607605, "training_acc": 52.0, "val_loss": 17.213965952396393, "val_acc": 56.0}
