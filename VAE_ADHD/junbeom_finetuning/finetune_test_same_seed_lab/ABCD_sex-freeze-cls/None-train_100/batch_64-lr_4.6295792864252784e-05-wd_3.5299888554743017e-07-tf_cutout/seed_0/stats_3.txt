"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.42776894569397, "training_acc": 39.0, "val_loss": 17.31812059879303, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.26784539222717, "training_acc": 53.0, "val_loss": 17.316436767578125, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.2817075252533, "training_acc": 53.0, "val_loss": 17.3130601644516, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.23656034469604, "training_acc": 53.0, "val_loss": 17.310838401317596, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.20222735404968, "training_acc": 53.0, "val_loss": 17.308036983013153, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.2211811542511, "training_acc": 53.0, "val_loss": 17.30506420135498, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.17730665206909, "training_acc": 53.0, "val_loss": 17.303359508514404, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.17187023162842, "training_acc": 53.0, "val_loss": 17.30298548936844, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.15277147293091, "training_acc": 53.0, "val_loss": 17.303848266601562, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.14774203300476, "training_acc": 53.0, "val_loss": 17.305636405944824, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.14012598991394, "training_acc": 53.0, "val_loss": 17.30821579694748, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.15263342857361, "training_acc": 53.0, "val_loss": 17.311450839042664, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.14081048965454, "training_acc": 53.0, "val_loss": 17.313891649246216, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.15400815010071, "training_acc": 53.0, "val_loss": 17.316173017024994, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.15483069419861, "training_acc": 53.0, "val_loss": 17.31799989938736, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.16893410682678, "training_acc": 53.0, "val_loss": 17.320652306079865, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.15345335006714, "training_acc": 53.0, "val_loss": 17.320838570594788, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.15564703941345, "training_acc": 53.0, "val_loss": 17.3211932182312, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.16578578948975, "training_acc": 53.0, "val_loss": 17.321136593818665, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.16093921661377, "training_acc": 53.0, "val_loss": 17.31884926557541, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.16164088249207, "training_acc": 53.0, "val_loss": 17.31647104024887, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.13594603538513, "training_acc": 53.0, "val_loss": 17.313575744628906, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.13874769210815, "training_acc": 53.0, "val_loss": 17.310865223407745, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.17623925209045, "training_acc": 53.0, "val_loss": 17.30823516845703, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.14921188354492, "training_acc": 53.0, "val_loss": 17.30659455060959, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.14455819129944, "training_acc": 53.0, "val_loss": 17.30561852455139, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.123215675354, "training_acc": 53.0, "val_loss": 17.30511039495468, "val_acc": 52.0}
