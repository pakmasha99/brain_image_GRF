"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 1098.7550659179688, "training_acc": 43.0, "val_loss": 260.6174945831299, "val_acc": 52.0}
{"epoch": 1, "training_loss": 954.3173484802246, "training_acc": 53.0, "val_loss": 356.7185640335083, "val_acc": 48.0}
{"epoch": 2, "training_loss": 1300.8498992919922, "training_acc": 47.0, "val_loss": 19.408927857875824, "val_acc": 48.0}
{"epoch": 3, "training_loss": 370.0076713562012, "training_acc": 51.0, "val_loss": 335.04326343536377, "val_acc": 52.0}
{"epoch": 4, "training_loss": 1265.6036567687988, "training_acc": 53.0, "val_loss": 176.42769813537598, "val_acc": 52.0}
{"epoch": 5, "training_loss": 460.8773422241211, "training_acc": 53.0, "val_loss": 135.0805640220642, "val_acc": 48.0}
{"epoch": 6, "training_loss": 530.714729309082, "training_acc": 47.0, "val_loss": 19.32366192340851, "val_acc": 48.0}
{"epoch": 7, "training_loss": 153.37207126617432, "training_acc": 59.0, "val_loss": 149.99090433120728, "val_acc": 52.0}
{"epoch": 8, "training_loss": 524.5560598373413, "training_acc": 53.0, "val_loss": 22.356511652469635, "val_acc": 48.0}
{"epoch": 9, "training_loss": 184.76192665100098, "training_acc": 47.0, "val_loss": 33.745431900024414, "val_acc": 48.0}
{"epoch": 10, "training_loss": 219.30977153778076, "training_acc": 47.0, "val_loss": 107.99492597579956, "val_acc": 52.0}
{"epoch": 11, "training_loss": 345.27412605285645, "training_acc": 53.0, "val_loss": 70.16726732254028, "val_acc": 48.0}
{"epoch": 12, "training_loss": 304.52057456970215, "training_acc": 47.0, "val_loss": 47.418010234832764, "val_acc": 48.0}
{"epoch": 13, "training_loss": 230.08853244781494, "training_acc": 47.0, "val_loss": 94.74619030952454, "val_acc": 52.0}
{"epoch": 14, "training_loss": 308.2448830604553, "training_acc": 53.0, "val_loss": 61.16749048233032, "val_acc": 48.0}
{"epoch": 15, "training_loss": 280.3305196762085, "training_acc": 47.0, "val_loss": 24.996359646320343, "val_acc": 48.0}
{"epoch": 16, "training_loss": 184.16888523101807, "training_acc": 49.0, "val_loss": 109.09208059310913, "val_acc": 52.0}
{"epoch": 17, "training_loss": 365.77270889282227, "training_acc": 53.0, "val_loss": 44.904935359954834, "val_acc": 48.0}
{"epoch": 18, "training_loss": 229.33573627471924, "training_acc": 47.0, "val_loss": 20.265398919582367, "val_acc": 48.0}
{"epoch": 19, "training_loss": 148.72540855407715, "training_acc": 51.0, "val_loss": 85.86587905883789, "val_acc": 52.0}
{"epoch": 20, "training_loss": 266.79332065582275, "training_acc": 53.0, "val_loss": 76.84292197227478, "val_acc": 48.0}
{"epoch": 21, "training_loss": 319.9236783981323, "training_acc": 47.0, "val_loss": 28.428760170936584, "val_acc": 48.0}
{"epoch": 22, "training_loss": 213.35412883758545, "training_acc": 45.0, "val_loss": 109.55883264541626, "val_acc": 52.0}
{"epoch": 23, "training_loss": 370.963915348053, "training_acc": 53.0, "val_loss": 42.55290925502777, "val_acc": 48.0}
{"epoch": 24, "training_loss": 186.47833156585693, "training_acc": 47.0, "val_loss": 17.877669632434845, "val_acc": 52.0}
{"epoch": 25, "training_loss": 114.34957122802734, "training_acc": 51.0, "val_loss": 41.70564413070679, "val_acc": 52.0}
{"epoch": 26, "training_loss": 192.14127922058105, "training_acc": 43.0, "val_loss": 30.557164549827576, "val_acc": 48.0}
{"epoch": 27, "training_loss": 126.93200588226318, "training_acc": 53.0, "val_loss": 46.98273241519928, "val_acc": 52.0}
{"epoch": 28, "training_loss": 167.55037927627563, "training_acc": 45.0, "val_loss": 23.421165347099304, "val_acc": 48.0}
{"epoch": 29, "training_loss": 105.879647731781, "training_acc": 49.0, "val_loss": 21.919088065624237, "val_acc": 52.0}
{"epoch": 30, "training_loss": 109.4840612411499, "training_acc": 51.0, "val_loss": 20.5250546336174, "val_acc": 48.0}
{"epoch": 31, "training_loss": 108.7849669456482, "training_acc": 51.0, "val_loss": 32.639044523239136, "val_acc": 52.0}
{"epoch": 32, "training_loss": 139.29494953155518, "training_acc": 51.0, "val_loss": 31.81290030479431, "val_acc": 48.0}
{"epoch": 33, "training_loss": 127.16884088516235, "training_acc": 53.0, "val_loss": 43.6614453792572, "val_acc": 52.0}
{"epoch": 34, "training_loss": 173.7522430419922, "training_acc": 43.0, "val_loss": 20.117801427841187, "val_acc": 48.0}
{"epoch": 35, "training_loss": 103.96282052993774, "training_acc": 49.0, "val_loss": 22.228820621967316, "val_acc": 52.0}
{"epoch": 36, "training_loss": 143.79924392700195, "training_acc": 45.0, "val_loss": 23.932044208049774, "val_acc": 48.0}
{"epoch": 37, "training_loss": 161.5258207321167, "training_acc": 45.0, "val_loss": 56.99240565299988, "val_acc": 52.0}
{"epoch": 38, "training_loss": 143.20243501663208, "training_acc": 61.0, "val_loss": 71.58405184745789, "val_acc": 48.0}
{"epoch": 39, "training_loss": 267.4244575500488, "training_acc": 47.0, "val_loss": 40.265318751335144, "val_acc": 52.0}
{"epoch": 40, "training_loss": 189.61917877197266, "training_acc": 53.0, "val_loss": 19.438037276268005, "val_acc": 52.0}
{"epoch": 41, "training_loss": 208.77612686157227, "training_acc": 39.0, "val_loss": 56.54439926147461, "val_acc": 48.0}
{"epoch": 42, "training_loss": 207.207190990448, "training_acc": 49.0, "val_loss": 62.84491419792175, "val_acc": 52.0}
{"epoch": 43, "training_loss": 190.72180342674255, "training_acc": 53.0, "val_loss": 64.55520987510681, "val_acc": 48.0}
