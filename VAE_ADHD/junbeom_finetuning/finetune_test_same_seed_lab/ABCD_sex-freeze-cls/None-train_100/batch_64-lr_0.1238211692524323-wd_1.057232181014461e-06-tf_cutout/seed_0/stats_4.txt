"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 1001.7785835266113, "training_acc": 53.0, "val_loss": 183.1823706626892, "val_acc": 52.0}
{"epoch": 1, "training_loss": 1015.156852722168, "training_acc": 51.0, "val_loss": 512.6852989196777, "val_acc": 48.0}
{"epoch": 2, "training_loss": 1945.4905471801758, "training_acc": 47.0, "val_loss": 175.28316974639893, "val_acc": 48.0}
{"epoch": 3, "training_loss": 775.2188510894775, "training_acc": 43.0, "val_loss": 307.7335834503174, "val_acc": 52.0}
{"epoch": 4, "training_loss": 1250.9256629943848, "training_acc": 53.0, "val_loss": 257.03558921813965, "val_acc": 52.0}
{"epoch": 5, "training_loss": 767.9166221618652, "training_acc": 53.0, "val_loss": 126.86811685562134, "val_acc": 48.0}
{"epoch": 6, "training_loss": 629.5453433990479, "training_acc": 47.0, "val_loss": 235.09631156921387, "val_acc": 48.0}
{"epoch": 7, "training_loss": 835.3212070465088, "training_acc": 47.0, "val_loss": 18.223442137241364, "val_acc": 52.0}
{"epoch": 8, "training_loss": 210.29388236999512, "training_acc": 53.0, "val_loss": 156.51872158050537, "val_acc": 52.0}
{"epoch": 9, "training_loss": 561.1263027191162, "training_acc": 53.0, "val_loss": 19.635696709156036, "val_acc": 52.0}
{"epoch": 10, "training_loss": 245.7665672302246, "training_acc": 51.0, "val_loss": 180.3882122039795, "val_acc": 48.0}
{"epoch": 11, "training_loss": 675.4323253631592, "training_acc": 47.0, "val_loss": 25.403335690498352, "val_acc": 48.0}
{"epoch": 12, "training_loss": 271.8997211456299, "training_acc": 47.0, "val_loss": 204.46507930755615, "val_acc": 52.0}
{"epoch": 13, "training_loss": 785.764482498169, "training_acc": 53.0, "val_loss": 129.38871383666992, "val_acc": 52.0}
{"epoch": 14, "training_loss": 379.81318640708923, "training_acc": 53.0, "val_loss": 115.64050912857056, "val_acc": 48.0}
{"epoch": 15, "training_loss": 501.7085819244385, "training_acc": 47.0, "val_loss": 88.98672461509705, "val_acc": 48.0}
{"epoch": 16, "training_loss": 304.4642343521118, "training_acc": 45.0, "val_loss": 76.01014375686646, "val_acc": 52.0}
{"epoch": 17, "training_loss": 261.59620094299316, "training_acc": 53.0, "val_loss": 41.5984183549881, "val_acc": 48.0}
{"epoch": 18, "training_loss": 177.18483114242554, "training_acc": 47.0, "val_loss": 17.400632798671722, "val_acc": 52.0}
{"epoch": 19, "training_loss": 147.93343925476074, "training_acc": 53.0, "val_loss": 21.778538823127747, "val_acc": 52.0}
{"epoch": 20, "training_loss": 132.3141860961914, "training_acc": 53.0, "val_loss": 51.65519714355469, "val_acc": 48.0}
{"epoch": 21, "training_loss": 139.91308212280273, "training_acc": 59.0, "val_loss": 56.630516052246094, "val_acc": 52.0}
{"epoch": 22, "training_loss": 188.89678955078125, "training_acc": 53.0, "val_loss": 56.71522617340088, "val_acc": 48.0}
{"epoch": 23, "training_loss": 257.65759658813477, "training_acc": 47.0, "val_loss": 17.398814857006073, "val_acc": 52.0}
{"epoch": 24, "training_loss": 158.06275177001953, "training_acc": 53.0, "val_loss": 52.55938768386841, "val_acc": 52.0}
{"epoch": 25, "training_loss": 193.78704023361206, "training_acc": 47.0, "val_loss": 37.920624017715454, "val_acc": 48.0}
{"epoch": 26, "training_loss": 133.45167303085327, "training_acc": 51.0, "val_loss": 33.761391043663025, "val_acc": 52.0}
{"epoch": 27, "training_loss": 118.60291624069214, "training_acc": 51.0, "val_loss": 22.863173484802246, "val_acc": 48.0}
{"epoch": 28, "training_loss": 131.90608263015747, "training_acc": 41.0, "val_loss": 19.31959092617035, "val_acc": 52.0}
{"epoch": 29, "training_loss": 97.13501644134521, "training_acc": 55.0, "val_loss": 23.472613096237183, "val_acc": 48.0}
{"epoch": 30, "training_loss": 103.4365963935852, "training_acc": 55.0, "val_loss": 40.443626046180725, "val_acc": 52.0}
{"epoch": 31, "training_loss": 132.3283085823059, "training_acc": 53.0, "val_loss": 30.234289169311523, "val_acc": 48.0}
{"epoch": 32, "training_loss": 121.85921955108643, "training_acc": 49.0, "val_loss": 25.602856278419495, "val_acc": 52.0}
{"epoch": 33, "training_loss": 89.30349469184875, "training_acc": 59.0, "val_loss": 27.240628004074097, "val_acc": 48.0}
{"epoch": 34, "training_loss": 120.94940280914307, "training_acc": 49.0, "val_loss": 27.479541301727295, "val_acc": 52.0}
{"epoch": 35, "training_loss": 151.69540405273438, "training_acc": 45.0, "val_loss": 24.224941432476044, "val_acc": 48.0}
{"epoch": 36, "training_loss": 121.33877182006836, "training_acc": 53.0, "val_loss": 51.185810565948486, "val_acc": 52.0}
{"epoch": 37, "training_loss": 131.2723195552826, "training_acc": 61.0, "val_loss": 66.13314151763916, "val_acc": 48.0}
{"epoch": 38, "training_loss": 238.05717611312866, "training_acc": 47.0, "val_loss": 51.7422616481781, "val_acc": 52.0}
{"epoch": 39, "training_loss": 232.7918825149536, "training_acc": 53.0, "val_loss": 23.667949438095093, "val_acc": 52.0}
{"epoch": 40, "training_loss": 207.66644191741943, "training_acc": 45.0, "val_loss": 84.78521704673767, "val_acc": 48.0}
{"epoch": 41, "training_loss": 247.17264986038208, "training_acc": 47.0, "val_loss": 77.79731750488281, "val_acc": 52.0}
{"epoch": 42, "training_loss": 312.2250051498413, "training_acc": 53.0, "val_loss": 24.772851169109344, "val_acc": 52.0}
