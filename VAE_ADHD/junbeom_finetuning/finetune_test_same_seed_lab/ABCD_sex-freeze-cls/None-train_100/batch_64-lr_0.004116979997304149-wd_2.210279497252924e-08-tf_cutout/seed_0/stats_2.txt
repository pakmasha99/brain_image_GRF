"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 82.54007768630981, "training_acc": 53.0, "val_loss": 18.20598393678665, "val_acc": 52.0}
{"epoch": 1, "training_loss": 70.69137597084045, "training_acc": 55.0, "val_loss": 22.176329791545868, "val_acc": 48.0}
{"epoch": 2, "training_loss": 88.43761372566223, "training_acc": 47.0, "val_loss": 18.327073752880096, "val_acc": 52.0}
{"epoch": 3, "training_loss": 70.81188893318176, "training_acc": 53.0, "val_loss": 19.059592485427856, "val_acc": 52.0}
{"epoch": 4, "training_loss": 77.74071097373962, "training_acc": 53.0, "val_loss": 20.484736561775208, "val_acc": 52.0}
{"epoch": 5, "training_loss": 79.42917561531067, "training_acc": 53.0, "val_loss": 17.53760129213333, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.25699949264526, "training_acc": 53.0, "val_loss": 18.08253824710846, "val_acc": 52.0}
{"epoch": 7, "training_loss": 73.55377960205078, "training_acc": 47.0, "val_loss": 18.40469390153885, "val_acc": 48.0}
{"epoch": 8, "training_loss": 72.89645743370056, "training_acc": 47.0, "val_loss": 17.31632798910141, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.8835825920105, "training_acc": 53.0, "val_loss": 18.140608072280884, "val_acc": 52.0}
{"epoch": 10, "training_loss": 72.50901436805725, "training_acc": 53.0, "val_loss": 17.967717349529266, "val_acc": 52.0}
{"epoch": 11, "training_loss": 70.4561083316803, "training_acc": 53.0, "val_loss": 17.325927317142487, "val_acc": 52.0}
{"epoch": 12, "training_loss": 70.42011308670044, "training_acc": 47.0, "val_loss": 17.847728729248047, "val_acc": 52.0}
{"epoch": 13, "training_loss": 71.47459363937378, "training_acc": 47.0, "val_loss": 17.41042137145996, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.51824879646301, "training_acc": 49.0, "val_loss": 17.45339334011078, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.74656629562378, "training_acc": 53.0, "val_loss": 17.689815163612366, "val_acc": 52.0}
{"epoch": 16, "training_loss": 70.09809446334839, "training_acc": 53.0, "val_loss": 17.360714077949524, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.66920447349548, "training_acc": 47.0, "val_loss": 17.412051558494568, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.7201087474823, "training_acc": 47.0, "val_loss": 17.36207902431488, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.37064099311829, "training_acc": 49.0, "val_loss": 17.32834279537201, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.11261081695557, "training_acc": 53.0, "val_loss": 17.436112463474274, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.50170516967773, "training_acc": 53.0, "val_loss": 17.377138137817383, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.12669324874878, "training_acc": 53.0, "val_loss": 17.327198386192322, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.36786961555481, "training_acc": 51.0, "val_loss": 17.38167405128479, "val_acc": 52.0}
{"epoch": 24, "training_loss": 70.05834579467773, "training_acc": 41.0, "val_loss": 17.31274425983429, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.26790738105774, "training_acc": 53.0, "val_loss": 17.312829196453094, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.14130711555481, "training_acc": 53.0, "val_loss": 17.31351763010025, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.32576036453247, "training_acc": 53.0, "val_loss": 17.320434749126434, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.34882402420044, "training_acc": 53.0, "val_loss": 17.431695759296417, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.74732327461243, "training_acc": 53.0, "val_loss": 17.320039868354797, "val_acc": 52.0}
{"epoch": 30, "training_loss": 70.54364252090454, "training_acc": 43.0, "val_loss": 17.454902827739716, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.71485137939453, "training_acc": 47.0, "val_loss": 17.32846349477768, "val_acc": 52.0}
{"epoch": 32, "training_loss": 70.85038995742798, "training_acc": 53.0, "val_loss": 17.694121599197388, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.94210696220398, "training_acc": 53.0, "val_loss": 17.313265800476074, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.36323142051697, "training_acc": 51.0, "val_loss": 17.607063055038452, "val_acc": 52.0}
{"epoch": 35, "training_loss": 72.00501751899719, "training_acc": 47.0, "val_loss": 17.336638271808624, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.1685643196106, "training_acc": 50.0, "val_loss": 18.150456249713898, "val_acc": 52.0}
{"epoch": 37, "training_loss": 71.97210097312927, "training_acc": 53.0, "val_loss": 17.985229194164276, "val_acc": 52.0}
{"epoch": 38, "training_loss": 71.77404642105103, "training_acc": 53.0, "val_loss": 17.314280569553375, "val_acc": 52.0}
{"epoch": 39, "training_loss": 70.27757740020752, "training_acc": 43.0, "val_loss": 17.381981015205383, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.76409935951233, "training_acc": 45.0, "val_loss": 17.338721454143524, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.2930269241333, "training_acc": 53.0, "val_loss": 17.370468378067017, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.2880482673645, "training_acc": 53.0, "val_loss": 17.313438653945923, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.16577887535095, "training_acc": 53.0, "val_loss": 17.313899099826813, "val_acc": 52.0}
