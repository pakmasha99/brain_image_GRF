"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.2652702331543, "training_acc": 53.0, "val_loss": 17.317917943000793, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.2520489692688, "training_acc": 53.0, "val_loss": 17.315803468227386, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.23117995262146, "training_acc": 53.0, "val_loss": 17.314033210277557, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.20664834976196, "training_acc": 53.0, "val_loss": 17.312805354595184, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.23938751220703, "training_acc": 53.0, "val_loss": 17.311613261699677, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.18426966667175, "training_acc": 53.0, "val_loss": 17.31075942516327, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.174480676651, "training_acc": 53.0, "val_loss": 17.30997860431671, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.19027709960938, "training_acc": 53.0, "val_loss": 17.309536039829254, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.17191004753113, "training_acc": 53.0, "val_loss": 17.309094965457916, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.17125391960144, "training_acc": 53.0, "val_loss": 17.308755218982697, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.16986560821533, "training_acc": 53.0, "val_loss": 17.308589816093445, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.16832375526428, "training_acc": 53.0, "val_loss": 17.308449745178223, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.17142176628113, "training_acc": 53.0, "val_loss": 17.3083633184433, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.14786458015442, "training_acc": 53.0, "val_loss": 17.308299243450165, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.17990136146545, "training_acc": 53.0, "val_loss": 17.3082634806633, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.15502858161926, "training_acc": 53.0, "val_loss": 17.308203876018524, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.14094543457031, "training_acc": 53.0, "val_loss": 17.308132350444794, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.18023157119751, "training_acc": 53.0, "val_loss": 17.308059334754944, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.17221403121948, "training_acc": 53.0, "val_loss": 17.30801612138748, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.14889192581177, "training_acc": 53.0, "val_loss": 17.307981848716736, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.15859961509705, "training_acc": 53.0, "val_loss": 17.307958006858826, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.16168880462646, "training_acc": 53.0, "val_loss": 17.30795055627823, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.15819263458252, "training_acc": 53.0, "val_loss": 17.307955026626587, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.13772320747375, "training_acc": 53.0, "val_loss": 17.307983338832855, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.13332414627075, "training_acc": 53.0, "val_loss": 17.308054864406586, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.13144612312317, "training_acc": 53.0, "val_loss": 17.308154702186584, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.1399736404419, "training_acc": 53.0, "val_loss": 17.308208346366882, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.1354489326477, "training_acc": 53.0, "val_loss": 17.308244109153748, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.13095593452454, "training_acc": 53.0, "val_loss": 17.308326065540314, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.12391400337219, "training_acc": 53.0, "val_loss": 17.308448255062103, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.1557469367981, "training_acc": 53.0, "val_loss": 17.30857491493225, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.15258765220642, "training_acc": 53.0, "val_loss": 17.30874925851822, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.15258717536926, "training_acc": 53.0, "val_loss": 17.308887839317322, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.12341952323914, "training_acc": 53.0, "val_loss": 17.308910191059113, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.13419032096863, "training_acc": 53.0, "val_loss": 17.308971285820007, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.16122937202454, "training_acc": 53.0, "val_loss": 17.3090398311615, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.14086532592773, "training_acc": 53.0, "val_loss": 17.309068143367767, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.13423228263855, "training_acc": 53.0, "val_loss": 17.30913072824478, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.17186260223389, "training_acc": 53.0, "val_loss": 17.309246957302094, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.14302515983582, "training_acc": 53.0, "val_loss": 17.30942130088806, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.12262678146362, "training_acc": 53.0, "val_loss": 17.30959266424179, "val_acc": 52.0}
