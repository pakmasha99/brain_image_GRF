"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.23831605911255, "training_acc": 53.0, "val_loss": 17.30809360742569, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.21743607521057, "training_acc": 53.0, "val_loss": 17.30826497077942, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.21407008171082, "training_acc": 53.0, "val_loss": 17.307914793491364, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.20876026153564, "training_acc": 53.0, "val_loss": 17.30749160051346, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.18320560455322, "training_acc": 53.0, "val_loss": 17.30726957321167, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.16173791885376, "training_acc": 53.0, "val_loss": 17.30688065290451, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.16398739814758, "training_acc": 53.0, "val_loss": 17.306503653526306, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.16944599151611, "training_acc": 53.0, "val_loss": 17.306146025657654, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.17811894416809, "training_acc": 53.0, "val_loss": 17.305894196033478, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.21591520309448, "training_acc": 53.0, "val_loss": 17.30562150478363, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.15683770179749, "training_acc": 53.0, "val_loss": 17.30533540248871, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.16371488571167, "training_acc": 53.0, "val_loss": 17.305102944374084, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.16294574737549, "training_acc": 53.0, "val_loss": 17.304858565330505, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.1874668598175, "training_acc": 53.0, "val_loss": 17.304618656635284, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.20115947723389, "training_acc": 53.0, "val_loss": 17.304441332817078, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.14771509170532, "training_acc": 53.0, "val_loss": 17.304319143295288, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.14794254302979, "training_acc": 53.0, "val_loss": 17.304222285747528, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.16231107711792, "training_acc": 53.0, "val_loss": 17.30414479970932, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.13515400886536, "training_acc": 53.0, "val_loss": 17.304083704948425, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.18030118942261, "training_acc": 53.0, "val_loss": 17.304058372974396, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.10415291786194, "training_acc": 53.0, "val_loss": 17.304058372974396, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.16610646247864, "training_acc": 53.0, "val_loss": 17.30411946773529, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.1347143650055, "training_acc": 53.0, "val_loss": 17.304235696792603, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.16960263252258, "training_acc": 53.0, "val_loss": 17.30436235666275, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.16040515899658, "training_acc": 53.0, "val_loss": 17.304545640945435, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.14618611335754, "training_acc": 53.0, "val_loss": 17.30472594499588, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.15160250663757, "training_acc": 53.0, "val_loss": 17.304958403110504, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.12588143348694, "training_acc": 53.0, "val_loss": 17.30523109436035, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.1056342124939, "training_acc": 53.0, "val_loss": 17.305514216423035, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.1650185585022, "training_acc": 53.0, "val_loss": 17.30586439371109, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.1409523487091, "training_acc": 53.0, "val_loss": 17.306137084960938, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.12351942062378, "training_acc": 53.0, "val_loss": 17.306415736675262, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.14598035812378, "training_acc": 53.0, "val_loss": 17.30673313140869, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.12663626670837, "training_acc": 53.0, "val_loss": 17.307011783123016, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.14988017082214, "training_acc": 53.0, "val_loss": 17.307376861572266, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.15202617645264, "training_acc": 53.0, "val_loss": 17.30765849351883, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.15164089202881, "training_acc": 53.0, "val_loss": 17.307904362678528, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.1441900730133, "training_acc": 53.0, "val_loss": 17.3081636428833, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.15490794181824, "training_acc": 53.0, "val_loss": 17.308349907398224, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.14015889167786, "training_acc": 53.0, "val_loss": 17.308419942855835, "val_acc": 52.0}
