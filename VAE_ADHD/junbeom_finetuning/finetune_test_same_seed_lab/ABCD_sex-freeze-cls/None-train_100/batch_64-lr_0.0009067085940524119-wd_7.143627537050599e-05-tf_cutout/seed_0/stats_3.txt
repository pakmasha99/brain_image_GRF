"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 70.80762362480164, "training_acc": 53.0, "val_loss": 17.416954040527344, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.15438795089722, "training_acc": 53.0, "val_loss": 17.36346334218979, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.49074506759644, "training_acc": 47.0, "val_loss": 17.45695024728775, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.91643834114075, "training_acc": 47.0, "val_loss": 17.348967492580414, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.45464062690735, "training_acc": 47.0, "val_loss": 17.337150871753693, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.60442280769348, "training_acc": 53.0, "val_loss": 17.433536052703857, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.43180012702942, "training_acc": 53.0, "val_loss": 17.351126670837402, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.03324675559998, "training_acc": 53.0, "val_loss": 17.309334874153137, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.5005464553833, "training_acc": 47.0, "val_loss": 17.37990379333496, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.60393857955933, "training_acc": 47.0, "val_loss": 17.346030473709106, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.34559369087219, "training_acc": 44.0, "val_loss": 17.307309806346893, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.36526584625244, "training_acc": 53.0, "val_loss": 17.35963225364685, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.22945928573608, "training_acc": 53.0, "val_loss": 17.38797128200531, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.39073753356934, "training_acc": 53.0, "val_loss": 17.376242578029633, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.17936635017395, "training_acc": 53.0, "val_loss": 17.313843965530396, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.12498950958252, "training_acc": 53.0, "val_loss": 17.323851585388184, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.30276441574097, "training_acc": 50.0, "val_loss": 17.34372228384018, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.41619992256165, "training_acc": 47.0, "val_loss": 17.321477830410004, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.20819473266602, "training_acc": 53.0, "val_loss": 17.312826216220856, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.01221418380737, "training_acc": 53.0, "val_loss": 17.40640252828598, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.5250313282013, "training_acc": 53.0, "val_loss": 17.48683899641037, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.55451703071594, "training_acc": 53.0, "val_loss": 17.384496331214905, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.16675639152527, "training_acc": 53.0, "val_loss": 17.308340966701508, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.67145156860352, "training_acc": 43.0, "val_loss": 17.34599471092224, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.77868270874023, "training_acc": 47.0, "val_loss": 17.324155569076538, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.1267671585083, "training_acc": 54.0, "val_loss": 17.340955138206482, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.11200070381165, "training_acc": 53.0, "val_loss": 17.4922913312912, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.67802143096924, "training_acc": 53.0, "val_loss": 17.55084991455078, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.82090926170349, "training_acc": 53.0, "val_loss": 17.42914766073227, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.50421237945557, "training_acc": 53.0, "val_loss": 17.31921285390854, "val_acc": 52.0}
