"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.3167450428009, "training_acc": 46.0, "val_loss": 17.331115901470184, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.30877876281738, "training_acc": 46.0, "val_loss": 17.32509583234787, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.23977565765381, "training_acc": 53.0, "val_loss": 17.320246994495392, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.22575759887695, "training_acc": 53.0, "val_loss": 17.31712818145752, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.16505551338196, "training_acc": 53.0, "val_loss": 17.315927147865295, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.1769757270813, "training_acc": 53.0, "val_loss": 17.31472760438919, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.14359784126282, "training_acc": 53.0, "val_loss": 17.31397658586502, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.14405179023743, "training_acc": 53.0, "val_loss": 17.313475906848907, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.14673280715942, "training_acc": 53.0, "val_loss": 17.313310503959656, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.16779589653015, "training_acc": 53.0, "val_loss": 17.31346845626831, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.12900495529175, "training_acc": 53.0, "val_loss": 17.31378585100174, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.13512825965881, "training_acc": 53.0, "val_loss": 17.31419265270233, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.10420846939087, "training_acc": 53.0, "val_loss": 17.314478754997253, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.12500023841858, "training_acc": 53.0, "val_loss": 17.314855754375458, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.13742923736572, "training_acc": 53.0, "val_loss": 17.315341532230377, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.12015461921692, "training_acc": 53.0, "val_loss": 17.315825819969177, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.14030051231384, "training_acc": 53.0, "val_loss": 17.316187918186188, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.14582777023315, "training_acc": 53.0, "val_loss": 17.316962778568268, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.14736557006836, "training_acc": 53.0, "val_loss": 17.317481338977814, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.12419080734253, "training_acc": 53.0, "val_loss": 17.317664623260498, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.12179589271545, "training_acc": 53.0, "val_loss": 17.317743599414825, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.1123857498169, "training_acc": 53.0, "val_loss": 17.317962646484375, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.11655950546265, "training_acc": 53.0, "val_loss": 17.31806844472885, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.12874722480774, "training_acc": 53.0, "val_loss": 17.318230867385864, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.1291491985321, "training_acc": 53.0, "val_loss": 17.318424582481384, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.11596035957336, "training_acc": 53.0, "val_loss": 17.318831384181976, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.11914658546448, "training_acc": 53.0, "val_loss": 17.31908917427063, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.11009311676025, "training_acc": 53.0, "val_loss": 17.3191636800766, "val_acc": 52.0}
