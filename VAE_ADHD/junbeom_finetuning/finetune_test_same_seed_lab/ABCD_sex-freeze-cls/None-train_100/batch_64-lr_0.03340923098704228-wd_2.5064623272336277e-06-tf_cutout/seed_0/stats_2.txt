"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 300.8007049560547, "training_acc": 51.0, "val_loss": 38.55501711368561, "val_acc": 48.0}
{"epoch": 1, "training_loss": 273.0252227783203, "training_acc": 47.0, "val_loss": 155.2442193031311, "val_acc": 52.0}
{"epoch": 2, "training_loss": 592.24342918396, "training_acc": 53.0, "val_loss": 94.80700492858887, "val_acc": 52.0}
{"epoch": 3, "training_loss": 272.11820578575134, "training_acc": 53.0, "val_loss": 70.37912011146545, "val_acc": 48.0}
{"epoch": 4, "training_loss": 339.90061950683594, "training_acc": 47.0, "val_loss": 110.27758121490479, "val_acc": 48.0}
{"epoch": 5, "training_loss": 403.4706735610962, "training_acc": 47.0, "val_loss": 29.34090793132782, "val_acc": 48.0}
{"epoch": 6, "training_loss": 161.64087009429932, "training_acc": 41.0, "val_loss": 75.80934166908264, "val_acc": 52.0}
{"epoch": 7, "training_loss": 304.03136444091797, "training_acc": 53.0, "val_loss": 68.08295845985413, "val_acc": 52.0}
{"epoch": 8, "training_loss": 227.27151536941528, "training_acc": 53.0, "val_loss": 20.11762410402298, "val_acc": 48.0}
{"epoch": 9, "training_loss": 129.60525226593018, "training_acc": 47.0, "val_loss": 52.091896533966064, "val_acc": 48.0}
{"epoch": 10, "training_loss": 188.04537963867188, "training_acc": 47.0, "val_loss": 17.44595170021057, "val_acc": 52.0}
{"epoch": 11, "training_loss": 88.22963905334473, "training_acc": 53.0, "val_loss": 40.7540887594223, "val_acc": 52.0}
{"epoch": 12, "training_loss": 148.0263156890869, "training_acc": 53.0, "val_loss": 17.809367179870605, "val_acc": 52.0}
{"epoch": 13, "training_loss": 92.52999448776245, "training_acc": 45.0, "val_loss": 32.555294036865234, "val_acc": 48.0}
{"epoch": 14, "training_loss": 117.0088050365448, "training_acc": 47.0, "val_loss": 18.76773238182068, "val_acc": 52.0}
{"epoch": 15, "training_loss": 89.95877981185913, "training_acc": 53.0, "val_loss": 27.97250747680664, "val_acc": 52.0}
{"epoch": 16, "training_loss": 96.04888653755188, "training_acc": 53.0, "val_loss": 20.874622464179993, "val_acc": 48.0}
{"epoch": 17, "training_loss": 97.92562580108643, "training_acc": 47.0, "val_loss": 21.230536699295044, "val_acc": 48.0}
{"epoch": 18, "training_loss": 75.79798889160156, "training_acc": 55.0, "val_loss": 26.09330415725708, "val_acc": 52.0}
{"epoch": 19, "training_loss": 105.92772722244263, "training_acc": 53.0, "val_loss": 18.521663546562195, "val_acc": 52.0}
{"epoch": 20, "training_loss": 73.2379105091095, "training_acc": 55.0, "val_loss": 26.778113842010498, "val_acc": 48.0}
{"epoch": 21, "training_loss": 101.38702249526978, "training_acc": 47.0, "val_loss": 18.12223643064499, "val_acc": 52.0}
{"epoch": 22, "training_loss": 77.98814010620117, "training_acc": 53.0, "val_loss": 26.408395171165466, "val_acc": 52.0}
{"epoch": 23, "training_loss": 93.43753170967102, "training_acc": 53.0, "val_loss": 19.683551788330078, "val_acc": 48.0}
{"epoch": 24, "training_loss": 83.24566388130188, "training_acc": 47.0, "val_loss": 19.859889149665833, "val_acc": 48.0}
{"epoch": 25, "training_loss": 76.75543189048767, "training_acc": 49.0, "val_loss": 21.71645164489746, "val_acc": 52.0}
{"epoch": 26, "training_loss": 84.5057418346405, "training_acc": 53.0, "val_loss": 17.36220270395279, "val_acc": 52.0}
{"epoch": 27, "training_loss": 74.5676965713501, "training_acc": 49.0, "val_loss": 20.223532617092133, "val_acc": 48.0}
{"epoch": 28, "training_loss": 76.06587529182434, "training_acc": 51.0, "val_loss": 20.39540410041809, "val_acc": 52.0}
{"epoch": 29, "training_loss": 79.92073583602905, "training_acc": 53.0, "val_loss": 17.813213169574738, "val_acc": 52.0}
{"epoch": 30, "training_loss": 71.49166893959045, "training_acc": 49.0, "val_loss": 18.229953944683075, "val_acc": 52.0}
{"epoch": 31, "training_loss": 72.3476333618164, "training_acc": 46.0, "val_loss": 17.678460478782654, "val_acc": 52.0}
{"epoch": 32, "training_loss": 72.00917911529541, "training_acc": 53.0, "val_loss": 17.349053919315338, "val_acc": 52.0}
{"epoch": 33, "training_loss": 70.78336024284363, "training_acc": 54.0, "val_loss": 17.823873460292816, "val_acc": 52.0}
{"epoch": 34, "training_loss": 70.12399625778198, "training_acc": 51.0, "val_loss": 18.54933500289917, "val_acc": 52.0}
{"epoch": 35, "training_loss": 75.18367338180542, "training_acc": 53.0, "val_loss": 17.479661107063293, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.80494809150696, "training_acc": 53.0, "val_loss": 17.418308556079865, "val_acc": 52.0}
{"epoch": 37, "training_loss": 70.0779640674591, "training_acc": 47.0, "val_loss": 17.37147718667984, "val_acc": 52.0}
{"epoch": 38, "training_loss": 70.19715690612793, "training_acc": 47.0, "val_loss": 17.595428228378296, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.15076088905334, "training_acc": 53.0, "val_loss": 17.86472350358963, "val_acc": 52.0}
{"epoch": 40, "training_loss": 72.20036387443542, "training_acc": 47.0, "val_loss": 17.547599971294403, "val_acc": 52.0}
{"epoch": 41, "training_loss": 76.32250332832336, "training_acc": 53.0, "val_loss": 17.386506497859955, "val_acc": 52.0}
{"epoch": 42, "training_loss": 73.37616682052612, "training_acc": 49.0, "val_loss": 18.52688044309616, "val_acc": 48.0}
{"epoch": 43, "training_loss": 69.50281405448914, "training_acc": 59.0, "val_loss": 21.564528346061707, "val_acc": 52.0}
{"epoch": 44, "training_loss": 83.47954154014587, "training_acc": 53.0, "val_loss": 17.3541858792305, "val_acc": 52.0}
{"epoch": 45, "training_loss": 76.39663076400757, "training_acc": 45.0, "val_loss": 17.816540598869324, "val_acc": 52.0}
{"epoch": 46, "training_loss": 69.66257834434509, "training_acc": 53.0, "val_loss": 20.299287140369415, "val_acc": 52.0}
{"epoch": 47, "training_loss": 76.8830828666687, "training_acc": 53.0, "val_loss": 18.296366930007935, "val_acc": 52.0}
{"epoch": 48, "training_loss": 77.45537233352661, "training_acc": 47.0, "val_loss": 17.445212602615356, "val_acc": 52.0}
{"epoch": 49, "training_loss": 81.41054964065552, "training_acc": 53.0, "val_loss": 18.121108412742615, "val_acc": 52.0}
{"epoch": 50, "training_loss": 69.14790439605713, "training_acc": 57.0, "val_loss": 22.00707644224167, "val_acc": 48.0}
{"epoch": 51, "training_loss": 83.22534704208374, "training_acc": 47.0, "val_loss": 19.236847758293152, "val_acc": 52.0}
