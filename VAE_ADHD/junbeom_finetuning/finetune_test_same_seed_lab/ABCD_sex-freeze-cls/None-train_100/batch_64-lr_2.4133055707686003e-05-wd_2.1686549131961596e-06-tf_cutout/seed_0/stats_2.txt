"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.494393825531, "training_acc": 53.0, "val_loss": 17.44001656770706, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.4475371837616, "training_acc": 53.0, "val_loss": 17.428985238075256, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.37881064414978, "training_acc": 53.0, "val_loss": 17.419539391994476, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.37369632720947, "training_acc": 53.0, "val_loss": 17.407318949699402, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.32298970222473, "training_acc": 53.0, "val_loss": 17.394763231277466, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.34683966636658, "training_acc": 53.0, "val_loss": 17.383304238319397, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.23193049430847, "training_acc": 53.0, "val_loss": 17.372342944145203, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.23185110092163, "training_acc": 53.0, "val_loss": 17.361637949943542, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.21786618232727, "training_acc": 53.0, "val_loss": 17.35215336084366, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.19163465499878, "training_acc": 53.0, "val_loss": 17.34500080347061, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.14746069908142, "training_acc": 53.0, "val_loss": 17.3385351896286, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.13284039497375, "training_acc": 53.0, "val_loss": 17.33311116695404, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.11225867271423, "training_acc": 53.0, "val_loss": 17.328712344169617, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.1228380203247, "training_acc": 53.0, "val_loss": 17.32497364282608, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.11540532112122, "training_acc": 53.0, "val_loss": 17.322589457035065, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.10927605628967, "training_acc": 53.0, "val_loss": 17.32049584388733, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.12666630744934, "training_acc": 53.0, "val_loss": 17.31889247894287, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.10988998413086, "training_acc": 53.0, "val_loss": 17.317883670330048, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.13095998764038, "training_acc": 53.0, "val_loss": 17.316994071006775, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.10142993927002, "training_acc": 53.0, "val_loss": 17.3162043094635, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.15685200691223, "training_acc": 53.0, "val_loss": 17.315758764743805, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.14878964424133, "training_acc": 53.0, "val_loss": 17.31504797935486, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.0977213382721, "training_acc": 53.0, "val_loss": 17.31441020965576, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.13626170158386, "training_acc": 53.0, "val_loss": 17.314012348651886, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.12094831466675, "training_acc": 53.0, "val_loss": 17.314013838768005, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.15798306465149, "training_acc": 53.0, "val_loss": 17.314033210277557, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.14345002174377, "training_acc": 53.0, "val_loss": 17.31380820274353, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.13770294189453, "training_acc": 53.0, "val_loss": 17.31392592191696, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.14734649658203, "training_acc": 53.0, "val_loss": 17.314304411411285, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.14116048812866, "training_acc": 53.0, "val_loss": 17.31441468000412, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.10387349128723, "training_acc": 53.0, "val_loss": 17.314413189888, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.1187961101532, "training_acc": 53.0, "val_loss": 17.314399778842926, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.139892578125, "training_acc": 53.0, "val_loss": 17.31492727994919, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.12985348701477, "training_acc": 53.0, "val_loss": 17.315536737442017, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.10613203048706, "training_acc": 53.0, "val_loss": 17.31637865304947, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.14610600471497, "training_acc": 53.0, "val_loss": 17.317557334899902, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.12682056427002, "training_acc": 53.0, "val_loss": 17.319169640541077, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.15925192832947, "training_acc": 53.0, "val_loss": 17.320391535758972, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.12964797019958, "training_acc": 53.0, "val_loss": 17.32126623392105, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.1537013053894, "training_acc": 53.0, "val_loss": 17.322134971618652, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.13662195205688, "training_acc": 53.0, "val_loss": 17.322808504104614, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.11785554885864, "training_acc": 53.0, "val_loss": 17.32327938079834, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.13001871109009, "training_acc": 53.0, "val_loss": 17.3235684633255, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.11407136917114, "training_acc": 53.0, "val_loss": 17.323903739452362, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.11926698684692, "training_acc": 53.0, "val_loss": 17.32388287782669, "val_acc": 52.0}
{"epoch": 45, "training_loss": 69.1489098072052, "training_acc": 53.0, "val_loss": 17.323589324951172, "val_acc": 52.0}
