"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 1559.5924072265625, "training_acc": 50.0, "val_loss": 257.2742223739624, "val_acc": 44.0}
{"epoch": 1, "training_loss": 1532.68310546875, "training_acc": 48.0, "val_loss": 764.7821426391602, "val_acc": 56.0}
{"epoch": 2, "training_loss": 3219.6556701660156, "training_acc": 52.0, "val_loss": 427.9249668121338, "val_acc": 56.0}
{"epoch": 3, "training_loss": 1214.5889501571655, "training_acc": 52.0, "val_loss": 531.5620422363281, "val_acc": 44.0}
{"epoch": 4, "training_loss": 2347.399917602539, "training_acc": 48.0, "val_loss": 837.4308586120605, "val_acc": 44.0}
{"epoch": 5, "training_loss": 2918.61808013916, "training_acc": 48.0, "val_loss": 384.75608825683594, "val_acc": 44.0}
{"epoch": 6, "training_loss": 934.524356842041, "training_acc": 50.0, "val_loss": 234.66339111328125, "val_acc": 56.0}
{"epoch": 7, "training_loss": 1127.825008392334, "training_acc": 52.0, "val_loss": 264.1857147216797, "val_acc": 56.0}
{"epoch": 8, "training_loss": 902.2048416137695, "training_acc": 52.0, "val_loss": 133.16302299499512, "val_acc": 44.0}
{"epoch": 9, "training_loss": 640.271541595459, "training_acc": 48.0, "val_loss": 210.71949005126953, "val_acc": 44.0}
{"epoch": 10, "training_loss": 558.9066400527954, "training_acc": 48.0, "val_loss": 175.26285648345947, "val_acc": 56.0}
{"epoch": 11, "training_loss": 908.1303291320801, "training_acc": 52.0, "val_loss": 238.53075504302979, "val_acc": 56.0}
{"epoch": 12, "training_loss": 849.6741390228271, "training_acc": 52.0, "val_loss": 89.74246978759766, "val_acc": 44.0}
{"epoch": 13, "training_loss": 469.6050205230713, "training_acc": 48.0, "val_loss": 132.5728416442871, "val_acc": 44.0}
{"epoch": 14, "training_loss": 344.68531036376953, "training_acc": 52.0, "val_loss": 71.99109196662903, "val_acc": 56.0}
{"epoch": 15, "training_loss": 244.06601643562317, "training_acc": 52.0, "val_loss": 95.23471593856812, "val_acc": 44.0}
{"epoch": 16, "training_loss": 315.52727222442627, "training_acc": 48.0, "val_loss": 61.44775748252869, "val_acc": 56.0}
{"epoch": 17, "training_loss": 279.8173532485962, "training_acc": 52.0, "val_loss": 22.983165085315704, "val_acc": 44.0}
{"epoch": 18, "training_loss": 113.3603196144104, "training_acc": 48.0, "val_loss": 40.91882109642029, "val_acc": 56.0}
{"epoch": 19, "training_loss": 149.75280570983887, "training_acc": 52.0, "val_loss": 83.86866450309753, "val_acc": 44.0}
{"epoch": 20, "training_loss": 276.79230880737305, "training_acc": 48.0, "val_loss": 71.49454951286316, "val_acc": 56.0}
{"epoch": 21, "training_loss": 320.61534118652344, "training_acc": 52.0, "val_loss": 17.36072152853012, "val_acc": 56.0}
{"epoch": 22, "training_loss": 145.50201225280762, "training_acc": 56.0, "val_loss": 65.20180106163025, "val_acc": 44.0}
{"epoch": 23, "training_loss": 277.4571180343628, "training_acc": 46.0, "val_loss": 69.43240165710449, "val_acc": 56.0}
{"epoch": 24, "training_loss": 212.55632901191711, "training_acc": 54.0, "val_loss": 52.466100454330444, "val_acc": 44.0}
{"epoch": 25, "training_loss": 159.6822190284729, "training_acc": 52.0, "val_loss": 21.167798340320587, "val_acc": 56.0}
{"epoch": 26, "training_loss": 87.4907398223877, "training_acc": 62.0, "val_loss": 23.031120002269745, "val_acc": 44.0}
{"epoch": 27, "training_loss": 153.15082454681396, "training_acc": 48.0, "val_loss": 27.834075689315796, "val_acc": 56.0}
{"epoch": 28, "training_loss": 246.11590385437012, "training_acc": 46.0, "val_loss": 89.30705189704895, "val_acc": 44.0}
{"epoch": 29, "training_loss": 261.73147916793823, "training_acc": 52.0, "val_loss": 54.513561725616455, "val_acc": 56.0}
{"epoch": 30, "training_loss": 153.4708034992218, "training_acc": 60.0, "val_loss": 62.129372358322144, "val_acc": 44.0}
{"epoch": 31, "training_loss": 152.79999351501465, "training_acc": 58.0, "val_loss": 63.38239312171936, "val_acc": 56.0}
{"epoch": 32, "training_loss": 217.68115901947021, "training_acc": 52.0, "val_loss": 111.65746450424194, "val_acc": 44.0}
{"epoch": 33, "training_loss": 415.70508766174316, "training_acc": 48.0, "val_loss": 23.07424247264862, "val_acc": 56.0}
{"epoch": 34, "training_loss": 154.05783367156982, "training_acc": 52.0, "val_loss": 26.29248797893524, "val_acc": 44.0}
{"epoch": 35, "training_loss": 94.91656947135925, "training_acc": 48.0, "val_loss": 47.92167842388153, "val_acc": 56.0}
{"epoch": 36, "training_loss": 164.23354816436768, "training_acc": 52.0, "val_loss": 55.51940202713013, "val_acc": 44.0}
{"epoch": 37, "training_loss": 139.90957188606262, "training_acc": 58.0, "val_loss": 64.43498134613037, "val_acc": 56.0}
{"epoch": 38, "training_loss": 229.6391260623932, "training_acc": 52.0, "val_loss": 71.77426218986511, "val_acc": 44.0}
{"epoch": 39, "training_loss": 209.4508445262909, "training_acc": 48.0, "val_loss": 52.45024561882019, "val_acc": 56.0}
{"epoch": 40, "training_loss": 174.07211303710938, "training_acc": 52.0, "val_loss": 75.57480931282043, "val_acc": 44.0}
