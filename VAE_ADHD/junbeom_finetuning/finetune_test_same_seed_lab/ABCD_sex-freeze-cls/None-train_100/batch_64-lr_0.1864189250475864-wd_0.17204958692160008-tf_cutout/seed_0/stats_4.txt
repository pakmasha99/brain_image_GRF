"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 1626.6796379089355, "training_acc": 49.0, "val_loss": 226.70493125915527, "val_acc": 48.0}
{"epoch": 1, "training_loss": 1185.147201538086, "training_acc": 57.0, "val_loss": 941.0016059875488, "val_acc": 52.0}
{"epoch": 2, "training_loss": 3669.5374603271484, "training_acc": 53.0, "val_loss": 656.5686702728271, "val_acc": 52.0}
{"epoch": 3, "training_loss": 1933.7533416748047, "training_acc": 53.0, "val_loss": 279.540753364563, "val_acc": 48.0}
{"epoch": 4, "training_loss": 1437.8528518676758, "training_acc": 47.0, "val_loss": 529.6095848083496, "val_acc": 48.0}
{"epoch": 5, "training_loss": 1959.2002716064453, "training_acc": 47.0, "val_loss": 81.47051930427551, "val_acc": 48.0}
{"epoch": 6, "training_loss": 573.1700592041016, "training_acc": 51.0, "val_loss": 453.7221431732178, "val_acc": 52.0}
{"epoch": 7, "training_loss": 1811.877082824707, "training_acc": 53.0, "val_loss": 389.41361904144287, "val_acc": 52.0}
{"epoch": 8, "training_loss": 1196.7749671936035, "training_acc": 53.0, "val_loss": 82.70953297615051, "val_acc": 48.0}
{"epoch": 9, "training_loss": 540.1494789123535, "training_acc": 47.0, "val_loss": 210.7133388519287, "val_acc": 48.0}
{"epoch": 10, "training_loss": 648.1969337463379, "training_acc": 47.0, "val_loss": 154.9418330192566, "val_acc": 52.0}
{"epoch": 11, "training_loss": 756.7536926269531, "training_acc": 53.0, "val_loss": 233.40075016021729, "val_acc": 52.0}
{"epoch": 12, "training_loss": 780.017954826355, "training_acc": 53.0, "val_loss": 95.9636390209198, "val_acc": 48.0}
{"epoch": 13, "training_loss": 502.7506332397461, "training_acc": 47.0, "val_loss": 81.53994083404541, "val_acc": 48.0}
{"epoch": 14, "training_loss": 389.2122631072998, "training_acc": 45.0, "val_loss": 150.4913330078125, "val_acc": 52.0}
{"epoch": 15, "training_loss": 513.9339923858643, "training_acc": 53.0, "val_loss": 49.30531978607178, "val_acc": 48.0}
{"epoch": 16, "training_loss": 228.48631191253662, "training_acc": 47.0, "val_loss": 19.386887550354004, "val_acc": 52.0}
{"epoch": 17, "training_loss": 139.49743366241455, "training_acc": 53.0, "val_loss": 29.249081015586853, "val_acc": 48.0}
{"epoch": 18, "training_loss": 119.96137285232544, "training_acc": 37.0, "val_loss": 31.354376673698425, "val_acc": 48.0}
{"epoch": 19, "training_loss": 134.42432355880737, "training_acc": 47.0, "val_loss": 17.90827512741089, "val_acc": 52.0}
{"epoch": 20, "training_loss": 103.0421142578125, "training_acc": 47.0, "val_loss": 68.29425096511841, "val_acc": 52.0}
{"epoch": 21, "training_loss": 266.0206699371338, "training_acc": 53.0, "val_loss": 48.94166588783264, "val_acc": 48.0}
{"epoch": 22, "training_loss": 207.9857416152954, "training_acc": 47.0, "val_loss": 65.28244018554688, "val_acc": 52.0}
{"epoch": 23, "training_loss": 283.032208442688, "training_acc": 53.0, "val_loss": 22.37449586391449, "val_acc": 48.0}
{"epoch": 24, "training_loss": 134.2609634399414, "training_acc": 47.0, "val_loss": 42.327311635017395, "val_acc": 52.0}
{"epoch": 25, "training_loss": 147.69297885894775, "training_acc": 53.0, "val_loss": 61.212241649627686, "val_acc": 48.0}
{"epoch": 26, "training_loss": 201.3030309677124, "training_acc": 47.0, "val_loss": 86.24303340911865, "val_acc": 52.0}
{"epoch": 27, "training_loss": 361.4660301208496, "training_acc": 53.0, "val_loss": 18.209698796272278, "val_acc": 52.0}
{"epoch": 28, "training_loss": 136.67802715301514, "training_acc": 47.0, "val_loss": 24.418267607688904, "val_acc": 48.0}
{"epoch": 29, "training_loss": 154.91783809661865, "training_acc": 55.0, "val_loss": 90.38625955581665, "val_acc": 52.0}
{"epoch": 30, "training_loss": 244.22510409355164, "training_acc": 53.0, "val_loss": 99.24084544181824, "val_acc": 48.0}
{"epoch": 31, "training_loss": 365.7279987335205, "training_acc": 47.0, "val_loss": 61.6476833820343, "val_acc": 52.0}
{"epoch": 32, "training_loss": 294.06749153137207, "training_acc": 53.0, "val_loss": 17.397305369377136, "val_acc": 52.0}
{"epoch": 33, "training_loss": 157.6015911102295, "training_acc": 47.0, "val_loss": 29.671519994735718, "val_acc": 48.0}
{"epoch": 34, "training_loss": 178.93737602233887, "training_acc": 55.0, "val_loss": 114.1349196434021, "val_acc": 52.0}
{"epoch": 35, "training_loss": 344.76518058776855, "training_acc": 53.0, "val_loss": 123.29612970352173, "val_acc": 48.0}
{"epoch": 36, "training_loss": 502.03500270843506, "training_acc": 47.0, "val_loss": 36.76212728023529, "val_acc": 48.0}
{"epoch": 37, "training_loss": 268.1617069244385, "training_acc": 51.0, "val_loss": 181.89072608947754, "val_acc": 52.0}
{"epoch": 38, "training_loss": 638.0112705230713, "training_acc": 53.0, "val_loss": 21.97088897228241, "val_acc": 48.0}
{"epoch": 39, "training_loss": 220.3984889984131, "training_acc": 47.0, "val_loss": 36.18292808532715, "val_acc": 48.0}
{"epoch": 40, "training_loss": 258.0760746002197, "training_acc": 49.0, "val_loss": 146.31747007369995, "val_acc": 52.0}
{"epoch": 41, "training_loss": 474.14618396759033, "training_acc": 53.0, "val_loss": 88.22247385978699, "val_acc": 48.0}
{"epoch": 42, "training_loss": 404.3561477661133, "training_acc": 47.0, "val_loss": 30.723434686660767, "val_acc": 48.0}
{"epoch": 43, "training_loss": 253.65529823303223, "training_acc": 51.0, "val_loss": 177.53221988677979, "val_acc": 52.0}
{"epoch": 44, "training_loss": 616.3379211425781, "training_acc": 53.0, "val_loss": 31.459331512451172, "val_acc": 48.0}
{"epoch": 45, "training_loss": 170.51830577850342, "training_acc": 47.0, "val_loss": 22.518078982830048, "val_acc": 52.0}
{"epoch": 46, "training_loss": 101.27577352523804, "training_acc": 53.0, "val_loss": 51.59391760826111, "val_acc": 48.0}
{"epoch": 47, "training_loss": 161.18519401550293, "training_acc": 47.0, "val_loss": 76.05682015419006, "val_acc": 52.0}
{"epoch": 48, "training_loss": 278.72949504852295, "training_acc": 53.0, "val_loss": 64.77476358413696, "val_acc": 48.0}
{"epoch": 49, "training_loss": 280.8813533782959, "training_acc": 47.0, "val_loss": 48.7593948841095, "val_acc": 52.0}
{"epoch": 50, "training_loss": 203.6163558959961, "training_acc": 53.0, "val_loss": 34.687912464141846, "val_acc": 48.0}
{"epoch": 51, "training_loss": 121.27316761016846, "training_acc": 47.0, "val_loss": 58.426469564437866, "val_acc": 52.0}
