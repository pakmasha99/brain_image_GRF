"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 496289.18451690674, "training_acc": 51.0, "val_loss": 109255.3955078125, "val_acc": 52.0}
{"epoch": 1, "training_loss": 541665.20703125, "training_acc": 55.0, "val_loss": 298332.4951171875, "val_acc": 48.0}
{"epoch": 2, "training_loss": 1116336.01953125, "training_acc": 47.0, "val_loss": 75804.66918945312, "val_acc": 48.0}
{"epoch": 3, "training_loss": 339007.380859375, "training_acc": 53.0, "val_loss": 220595.1416015625, "val_acc": 52.0}
{"epoch": 4, "training_loss": 900629.92578125, "training_acc": 53.0, "val_loss": 196633.5693359375, "val_acc": 52.0}
{"epoch": 5, "training_loss": 634207.044921875, "training_acc": 53.0, "val_loss": 37963.87023925781, "val_acc": 48.0}
{"epoch": 6, "training_loss": 291217.728515625, "training_acc": 47.0, "val_loss": 100369.79370117188, "val_acc": 48.0}
{"epoch": 7, "training_loss": 322180.5905761719, "training_acc": 47.0, "val_loss": 72364.4775390625, "val_acc": 52.0}
{"epoch": 8, "training_loss": 346027.03125, "training_acc": 53.0, "val_loss": 112600.01220703125, "val_acc": 52.0}
{"epoch": 9, "training_loss": 385733.23681640625, "training_acc": 53.0, "val_loss": 17435.964965820312, "val_acc": 48.0}
{"epoch": 10, "training_loss": 106479.54150390625, "training_acc": 47.0, "val_loss": 14066.12548828125, "val_acc": 48.0}
{"epoch": 11, "training_loss": 113800.279296875, "training_acc": 49.0, "val_loss": 74988.96484375, "val_acc": 52.0}
{"epoch": 12, "training_loss": 265191.9208984375, "training_acc": 53.0, "val_loss": 2490.4279708862305, "val_acc": 48.0}
{"epoch": 13, "training_loss": 28422.9912109375, "training_acc": 47.0, "val_loss": 21811.73553466797, "val_acc": 52.0}
{"epoch": 14, "training_loss": 73775.65795898438, "training_acc": 53.0, "val_loss": 30529.248046875, "val_acc": 48.0}
{"epoch": 15, "training_loss": 113047.49780273438, "training_acc": 47.0, "val_loss": 25773.004150390625, "val_acc": 52.0}
{"epoch": 16, "training_loss": 109716.75830078125, "training_acc": 53.0, "val_loss": 2497.8866577148438, "val_acc": 48.0}
{"epoch": 17, "training_loss": 18960.325805664062, "training_acc": 49.0, "val_loss": 19618.67218017578, "val_acc": 48.0}
{"epoch": 18, "training_loss": 52877.554595947266, "training_acc": 47.0, "val_loss": 55187.872314453125, "val_acc": 52.0}
{"epoch": 19, "training_loss": 233517.0703125, "training_acc": 53.0, "val_loss": 45764.007568359375, "val_acc": 52.0}
{"epoch": 20, "training_loss": 137903.49560546875, "training_acc": 49.0, "val_loss": 22504.03594970703, "val_acc": 48.0}
{"epoch": 21, "training_loss": 71356.92834472656, "training_acc": 51.0, "val_loss": 4917.829132080078, "val_acc": 52.0}
{"epoch": 22, "training_loss": 59329.072265625, "training_acc": 55.0, "val_loss": 35155.53894042969, "val_acc": 48.0}
{"epoch": 23, "training_loss": 100220.86926269531, "training_acc": 51.0, "val_loss": 10849.443054199219, "val_acc": 52.0}
{"epoch": 24, "training_loss": 67884.1396484375, "training_acc": 49.0, "val_loss": 13747.306823730469, "val_acc": 48.0}
{"epoch": 25, "training_loss": 89595.123046875, "training_acc": 49.0, "val_loss": 47362.530517578125, "val_acc": 52.0}
{"epoch": 26, "training_loss": 150941.25830078125, "training_acc": 53.0, "val_loss": 44225.323486328125, "val_acc": 48.0}
{"epoch": 27, "training_loss": 189035.95751953125, "training_acc": 47.0, "val_loss": 21073.08807373047, "val_acc": 48.0}
{"epoch": 28, "training_loss": 134817.77734375, "training_acc": 45.0, "val_loss": 69162.7197265625, "val_acc": 52.0}
{"epoch": 29, "training_loss": 248037.2109375, "training_acc": 53.0, "val_loss": 1846.7424392700195, "val_acc": 52.0}
{"epoch": 30, "training_loss": 99937.63623046875, "training_acc": 59.0, "val_loss": 125631.4453125, "val_acc": 48.0}
{"epoch": 31, "training_loss": 505574.193359375, "training_acc": 47.0, "val_loss": 71761.67602539062, "val_acc": 48.0}
{"epoch": 32, "training_loss": 200749.69653320312, "training_acc": 51.0, "val_loss": 52517.779541015625, "val_acc": 52.0}
{"epoch": 33, "training_loss": 217589.2470703125, "training_acc": 53.0, "val_loss": 15989.065551757812, "val_acc": 52.0}
{"epoch": 34, "training_loss": 131862.3388671875, "training_acc": 51.0, "val_loss": 91739.41040039062, "val_acc": 48.0}
{"epoch": 35, "training_loss": 349670.2724609375, "training_acc": 47.0, "val_loss": 15544.482421875, "val_acc": 48.0}
{"epoch": 36, "training_loss": 158139.759765625, "training_acc": 47.0, "val_loss": 122603.69873046875, "val_acc": 52.0}
{"epoch": 37, "training_loss": 479472.4111328125, "training_acc": 53.0, "val_loss": 96250.43334960938, "val_acc": 52.0}
{"epoch": 38, "training_loss": 304571.8037109375, "training_acc": 53.0, "val_loss": 55377.301025390625, "val_acc": 48.0}
{"epoch": 39, "training_loss": 274213.4404296875, "training_acc": 47.0, "val_loss": 87233.91723632812, "val_acc": 48.0}
{"epoch": 40, "training_loss": 289468.16748046875, "training_acc": 47.0, "val_loss": 43847.8271484375, "val_acc": 52.0}
{"epoch": 41, "training_loss": 230233.8076171875, "training_acc": 53.0, "val_loss": 74244.03076171875, "val_acc": 52.0}
{"epoch": 42, "training_loss": 226628.06298828125, "training_acc": 53.0, "val_loss": 42485.17150878906, "val_acc": 48.0}
{"epoch": 43, "training_loss": 197002.712890625, "training_acc": 47.0, "val_loss": 58329.52880859375, "val_acc": 48.0}
{"epoch": 44, "training_loss": 166195.9405517578, "training_acc": 47.0, "val_loss": 74482.2509765625, "val_acc": 52.0}
{"epoch": 45, "training_loss": 322969.658203125, "training_acc": 53.0, "val_loss": 109029.06494140625, "val_acc": 52.0}
{"epoch": 46, "training_loss": 378710.705078125, "training_acc": 53.0, "val_loss": 12196.637725830078, "val_acc": 52.0}
{"epoch": 47, "training_loss": 194253.625, "training_acc": 45.0, "val_loss": 142392.7001953125, "val_acc": 48.0}
{"epoch": 48, "training_loss": 575063.947265625, "training_acc": 47.0, "val_loss": 97975.6591796875, "val_acc": 48.0}
