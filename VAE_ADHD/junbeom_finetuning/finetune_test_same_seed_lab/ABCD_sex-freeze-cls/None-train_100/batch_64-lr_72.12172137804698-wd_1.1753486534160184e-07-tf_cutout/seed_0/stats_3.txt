"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 681667.6115989685, "training_acc": 53.0, "val_loss": 138867.9443359375, "val_acc": 52.0}
{"epoch": 1, "training_loss": 504053.373046875, "training_acc": 57.0, "val_loss": 251956.494140625, "val_acc": 48.0}
{"epoch": 2, "training_loss": 962205.1328125, "training_acc": 47.0, "val_loss": 76717.63916015625, "val_acc": 48.0}
{"epoch": 3, "training_loss": 333421.0751953125, "training_acc": 51.0, "val_loss": 188206.16455078125, "val_acc": 52.0}
{"epoch": 4, "training_loss": 748213.189453125, "training_acc": 53.0, "val_loss": 149921.78955078125, "val_acc": 52.0}
{"epoch": 5, "training_loss": 443578.30078125, "training_acc": 53.0, "val_loss": 81880.47485351562, "val_acc": 48.0}
{"epoch": 6, "training_loss": 429247.8203125, "training_acc": 47.0, "val_loss": 146515.12451171875, "val_acc": 48.0}
{"epoch": 7, "training_loss": 515329.3408203125, "training_acc": 47.0, "val_loss": 10939.997863769531, "val_acc": 52.0}
{"epoch": 8, "training_loss": 105628.732421875, "training_acc": 53.0, "val_loss": 44854.55322265625, "val_acc": 52.0}
{"epoch": 9, "training_loss": 111875.84643554688, "training_acc": 53.0, "val_loss": 637.5467300415039, "val_acc": 48.0}
{"epoch": 10, "training_loss": 41415.634521484375, "training_acc": 59.0, "val_loss": 47823.455810546875, "val_acc": 52.0}
{"epoch": 11, "training_loss": 144263.47692871094, "training_acc": 53.0, "val_loss": 59635.55908203125, "val_acc": 48.0}
{"epoch": 12, "training_loss": 262460.341796875, "training_acc": 47.0, "val_loss": 41309.53674316406, "val_acc": 48.0}
{"epoch": 13, "training_loss": 193011.23291015625, "training_acc": 39.0, "val_loss": 56032.50732421875, "val_acc": 52.0}
{"epoch": 14, "training_loss": 190203.67578125, "training_acc": 53.0, "val_loss": 22272.15576171875, "val_acc": 48.0}
{"epoch": 15, "training_loss": 116473.50927734375, "training_acc": 47.0, "val_loss": 567.1370029449463, "val_acc": 52.0}
{"epoch": 16, "training_loss": 13213.736206054688, "training_acc": 47.0, "val_loss": 31613.253784179688, "val_acc": 52.0}
{"epoch": 17, "training_loss": 121582.44189453125, "training_acc": 53.0, "val_loss": 8081.62841796875, "val_acc": 48.0}
{"epoch": 18, "training_loss": 25078.21759033203, "training_acc": 47.0, "val_loss": 45339.94140625, "val_acc": 52.0}
{"epoch": 19, "training_loss": 193721.4501953125, "training_acc": 53.0, "val_loss": 24852.069091796875, "val_acc": 52.0}
{"epoch": 20, "training_loss": 89762.51904296875, "training_acc": 61.0, "val_loss": 59402.44140625, "val_acc": 48.0}
{"epoch": 21, "training_loss": 210061.09228515625, "training_acc": 47.0, "val_loss": 23787.09259033203, "val_acc": 52.0}
{"epoch": 22, "training_loss": 118441.73046875, "training_acc": 53.0, "val_loss": 22024.20654296875, "val_acc": 52.0}
{"epoch": 23, "training_loss": 92308.24951171875, "training_acc": 55.0, "val_loss": 42197.73254394531, "val_acc": 48.0}
{"epoch": 24, "training_loss": 121916.44689941406, "training_acc": 47.0, "val_loss": 60077.96630859375, "val_acc": 52.0}
{"epoch": 25, "training_loss": 265474.2958984375, "training_acc": 53.0, "val_loss": 74713.84887695312, "val_acc": 52.0}
{"epoch": 26, "training_loss": 234416.9931640625, "training_acc": 53.0, "val_loss": 49051.90124511719, "val_acc": 48.0}
{"epoch": 27, "training_loss": 240874.58203125, "training_acc": 47.0, "val_loss": 59023.077392578125, "val_acc": 48.0}
{"epoch": 28, "training_loss": 161361.0509033203, "training_acc": 47.0, "val_loss": 14537.063598632812, "val_acc": 52.0}
{"epoch": 29, "training_loss": 43645.57409667969, "training_acc": 55.0, "val_loss": 9464.75601196289, "val_acc": 52.0}
{"epoch": 30, "training_loss": 52960.589111328125, "training_acc": 45.0, "val_loss": 9481.236267089844, "val_acc": 52.0}
{"epoch": 31, "training_loss": 27022.430694580078, "training_acc": 53.0, "val_loss": 23779.28009033203, "val_acc": 52.0}
{"epoch": 32, "training_loss": 76960.37487792969, "training_acc": 53.0, "val_loss": 36088.031005859375, "val_acc": 48.0}
{"epoch": 33, "training_loss": 155743.1181640625, "training_acc": 47.0, "val_loss": 3524.906539916992, "val_acc": 52.0}
{"epoch": 34, "training_loss": 16889.521545410156, "training_acc": 53.0, "val_loss": 29935.488891601562, "val_acc": 48.0}
