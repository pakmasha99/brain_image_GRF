"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 3776.6430053710938, "training_acc": 45.0, "val_loss": 675.0685691833496, "val_acc": 48.0}
{"epoch": 1, "training_loss": 2535.4950103759766, "training_acc": 59.0, "val_loss": 1794.0641403198242, "val_acc": 52.0}
{"epoch": 2, "training_loss": 6940.018768310547, "training_acc": 53.0, "val_loss": 1321.419906616211, "val_acc": 52.0}
{"epoch": 3, "training_loss": 4208.999443054199, "training_acc": 53.0, "val_loss": 392.6332950592041, "val_acc": 48.0}
{"epoch": 4, "training_loss": 2114.543472290039, "training_acc": 47.0, "val_loss": 765.2773380279541, "val_acc": 48.0}
{"epoch": 5, "training_loss": 2515.704620361328, "training_acc": 47.0, "val_loss": 264.9190664291382, "val_acc": 52.0}
{"epoch": 6, "training_loss": 1479.7662734985352, "training_acc": 53.0, "val_loss": 536.4850521087646, "val_acc": 52.0}
{"epoch": 7, "training_loss": 1753.6014995574951, "training_acc": 53.0, "val_loss": 251.2561321258545, "val_acc": 48.0}
{"epoch": 8, "training_loss": 1293.9529037475586, "training_acc": 47.0, "val_loss": 247.8339433670044, "val_acc": 48.0}
{"epoch": 9, "training_loss": 844.7440128326416, "training_acc": 51.0, "val_loss": 294.4265365600586, "val_acc": 52.0}
{"epoch": 10, "training_loss": 1004.5302448272705, "training_acc": 53.0, "val_loss": 142.02511310577393, "val_acc": 48.0}
{"epoch": 11, "training_loss": 582.2919254302979, "training_acc": 47.0, "val_loss": 81.60226345062256, "val_acc": 52.0}
{"epoch": 12, "training_loss": 282.0997176170349, "training_acc": 53.0, "val_loss": 182.70416259765625, "val_acc": 48.0}
{"epoch": 13, "training_loss": 697.4711875915527, "training_acc": 47.0, "val_loss": 141.10406637191772, "val_acc": 52.0}
{"epoch": 14, "training_loss": 618.3105792999268, "training_acc": 53.0, "val_loss": 25.167563557624817, "val_acc": 52.0}
{"epoch": 15, "training_loss": 603.4085502624512, "training_acc": 45.0, "val_loss": 329.40995693206787, "val_acc": 48.0}
{"epoch": 16, "training_loss": 1005.2984972000122, "training_acc": 47.0, "val_loss": 337.1098041534424, "val_acc": 52.0}
{"epoch": 17, "training_loss": 1565.0291290283203, "training_acc": 53.0, "val_loss": 462.7317428588867, "val_acc": 52.0}
{"epoch": 18, "training_loss": 1479.8334922790527, "training_acc": 53.0, "val_loss": 217.7743673324585, "val_acc": 48.0}
{"epoch": 19, "training_loss": 1095.207721710205, "training_acc": 47.0, "val_loss": 272.6574659347534, "val_acc": 48.0}
{"epoch": 20, "training_loss": 856.674656867981, "training_acc": 45.0, "val_loss": 127.60543823242188, "val_acc": 52.0}
{"epoch": 21, "training_loss": 334.31834721565247, "training_acc": 53.0, "val_loss": 82.50060677528381, "val_acc": 48.0}
{"epoch": 22, "training_loss": 293.29090690612793, "training_acc": 53.0, "val_loss": 31.850817799568176, "val_acc": 52.0}
{"epoch": 23, "training_loss": 330.80746269226074, "training_acc": 53.0, "val_loss": 146.09049558639526, "val_acc": 48.0}
{"epoch": 24, "training_loss": 554.0318241119385, "training_acc": 49.0, "val_loss": 151.73689126968384, "val_acc": 52.0}
{"epoch": 25, "training_loss": 417.28809928894043, "training_acc": 51.0, "val_loss": 24.17685091495514, "val_acc": 48.0}
{"epoch": 26, "training_loss": 271.1306266784668, "training_acc": 47.0, "val_loss": 64.04441595077515, "val_acc": 52.0}
{"epoch": 27, "training_loss": 545.3589897155762, "training_acc": 47.0, "val_loss": 233.1634759902954, "val_acc": 48.0}
{"epoch": 28, "training_loss": 659.51154255867, "training_acc": 47.0, "val_loss": 103.03972959518433, "val_acc": 52.0}
{"epoch": 29, "training_loss": 293.1102559566498, "training_acc": 51.0, "val_loss": 23.946459591388702, "val_acc": 52.0}
{"epoch": 30, "training_loss": 201.73724174499512, "training_acc": 49.0, "val_loss": 60.913026332855225, "val_acc": 52.0}
{"epoch": 31, "training_loss": 172.15896344184875, "training_acc": 53.0, "val_loss": 57.09987282752991, "val_acc": 48.0}
{"epoch": 32, "training_loss": 350.3438491821289, "training_acc": 47.0, "val_loss": 81.19058012962341, "val_acc": 52.0}
{"epoch": 33, "training_loss": 464.85143661499023, "training_acc": 51.0, "val_loss": 173.4378457069397, "val_acc": 48.0}
{"epoch": 34, "training_loss": 679.8154296875, "training_acc": 41.0, "val_loss": 82.57258534431458, "val_acc": 52.0}
{"epoch": 35, "training_loss": 360.7348575592041, "training_acc": 55.0, "val_loss": 106.24775886535645, "val_acc": 48.0}
{"epoch": 36, "training_loss": 485.5280170440674, "training_acc": 49.0, "val_loss": 165.9141182899475, "val_acc": 52.0}
{"epoch": 37, "training_loss": 524.7607102394104, "training_acc": 45.0, "val_loss": 25.651919841766357, "val_acc": 52.0}
{"epoch": 38, "training_loss": 151.92252922058105, "training_acc": 53.0, "val_loss": 77.32922434806824, "val_acc": 52.0}
{"epoch": 39, "training_loss": 219.73149156570435, "training_acc": 53.0, "val_loss": 246.43242359161377, "val_acc": 48.0}
{"epoch": 40, "training_loss": 1061.2706909179688, "training_acc": 47.0, "val_loss": 21.18082046508789, "val_acc": 48.0}
{"epoch": 41, "training_loss": 407.0296859741211, "training_acc": 55.0, "val_loss": 424.49822425842285, "val_acc": 52.0}
{"epoch": 42, "training_loss": 1547.161865234375, "training_acc": 53.0, "val_loss": 50.49750208854675, "val_acc": 52.0}
{"epoch": 43, "training_loss": 696.2099075317383, "training_acc": 55.0, "val_loss": 670.7236766815186, "val_acc": 48.0}
{"epoch": 44, "training_loss": 2674.6307220458984, "training_acc": 47.0, "val_loss": 349.8741149902344, "val_acc": 48.0}
{"epoch": 45, "training_loss": 801.5487928390503, "training_acc": 63.0, "val_loss": 344.86613273620605, "val_acc": 52.0}
{"epoch": 46, "training_loss": 1379.2134590148926, "training_acc": 53.0, "val_loss": 195.0441837310791, "val_acc": 52.0}
{"epoch": 47, "training_loss": 865.7737579345703, "training_acc": 45.0, "val_loss": 276.4702320098877, "val_acc": 48.0}
{"epoch": 48, "training_loss": 872.9749240875244, "training_acc": 47.0, "val_loss": 267.427921295166, "val_acc": 52.0}
{"epoch": 49, "training_loss": 1182.938850402832, "training_acc": 53.0, "val_loss": 351.85770988464355, "val_acc": 52.0}
{"epoch": 50, "training_loss": 1024.7277278900146, "training_acc": 53.0, "val_loss": 321.0747718811035, "val_acc": 48.0}
{"epoch": 51, "training_loss": 1532.9169082641602, "training_acc": 47.0, "val_loss": 399.07727241516113, "val_acc": 48.0}
{"epoch": 52, "training_loss": 1145.3116598129272, "training_acc": 47.0, "val_loss": 414.4439697265625, "val_acc": 52.0}
{"epoch": 53, "training_loss": 1971.8272705078125, "training_acc": 53.0, "val_loss": 682.142448425293, "val_acc": 52.0}
{"epoch": 54, "training_loss": 2417.7138671875, "training_acc": 53.0, "val_loss": 141.0676121711731, "val_acc": 52.0}
{"epoch": 55, "training_loss": 959.285774230957, "training_acc": 51.0, "val_loss": 683.8846206665039, "val_acc": 48.0}
{"epoch": 56, "training_loss": 2795.0893478393555, "training_acc": 47.0, "val_loss": 439.13750648498535, "val_acc": 48.0}
{"epoch": 57, "training_loss": 1233.416154384613, "training_acc": 47.0, "val_loss": 241.85378551483154, "val_acc": 52.0}
{"epoch": 58, "training_loss": 942.2885627746582, "training_acc": 53.0, "val_loss": 27.32768952846527, "val_acc": 52.0}
{"epoch": 59, "training_loss": 496.0281105041504, "training_acc": 55.0, "val_loss": 434.6126079559326, "val_acc": 48.0}
