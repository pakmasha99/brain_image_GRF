"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 328.05020332336426, "training_acc": 49.0, "val_loss": 73.61639142036438, "val_acc": 52.0}
{"epoch": 1, "training_loss": 306.9559144973755, "training_acc": 53.0, "val_loss": 128.88778448104858, "val_acc": 48.0}
{"epoch": 2, "training_loss": 469.28215980529785, "training_acc": 47.0, "val_loss": 20.035284757614136, "val_acc": 48.0}
{"epoch": 3, "training_loss": 164.90177249908447, "training_acc": 49.0, "val_loss": 116.60302877426147, "val_acc": 52.0}
{"epoch": 4, "training_loss": 450.844690322876, "training_acc": 53.0, "val_loss": 78.01092267036438, "val_acc": 52.0}
{"epoch": 5, "training_loss": 241.84322690963745, "training_acc": 53.0, "val_loss": 55.06557822227478, "val_acc": 48.0}
{"epoch": 6, "training_loss": 254.4668369293213, "training_acc": 47.0, "val_loss": 73.45606684684753, "val_acc": 48.0}
{"epoch": 7, "training_loss": 247.36301803588867, "training_acc": 47.0, "val_loss": 25.49351453781128, "val_acc": 52.0}
{"epoch": 8, "training_loss": 152.51317882537842, "training_acc": 53.0, "val_loss": 57.94154405593872, "val_acc": 52.0}
{"epoch": 9, "training_loss": 201.7781581878662, "training_acc": 53.0, "val_loss": 18.131549656391144, "val_acc": 52.0}
{"epoch": 10, "training_loss": 107.51854991912842, "training_acc": 47.0, "val_loss": 37.357836961746216, "val_acc": 48.0}
{"epoch": 11, "training_loss": 127.32041692733765, "training_acc": 47.0, "val_loss": 26.170414686203003, "val_acc": 52.0}
{"epoch": 12, "training_loss": 115.33613634109497, "training_acc": 53.0, "val_loss": 26.26577317714691, "val_acc": 52.0}
{"epoch": 13, "training_loss": 99.5383358001709, "training_acc": 45.0, "val_loss": 26.298385858535767, "val_acc": 48.0}
{"epoch": 14, "training_loss": 101.34643650054932, "training_acc": 47.0, "val_loss": 17.814800143241882, "val_acc": 52.0}
{"epoch": 15, "training_loss": 78.65561294555664, "training_acc": 53.0, "val_loss": 23.281340301036835, "val_acc": 52.0}
{"epoch": 16, "training_loss": 79.50047016143799, "training_acc": 53.0, "val_loss": 23.500780761241913, "val_acc": 48.0}
{"epoch": 17, "training_loss": 99.25141859054565, "training_acc": 47.0, "val_loss": 17.608308792114258, "val_acc": 52.0}
{"epoch": 18, "training_loss": 77.5221061706543, "training_acc": 49.0, "val_loss": 24.996092915534973, "val_acc": 52.0}
{"epoch": 19, "training_loss": 89.65687704086304, "training_acc": 53.0, "val_loss": 20.18369883298874, "val_acc": 48.0}
{"epoch": 20, "training_loss": 82.95252633094788, "training_acc": 47.0, "val_loss": 17.4521267414093, "val_acc": 52.0}
{"epoch": 21, "training_loss": 70.0797176361084, "training_acc": 53.0, "val_loss": 21.093159914016724, "val_acc": 52.0}
{"epoch": 22, "training_loss": 78.4822952747345, "training_acc": 53.0, "val_loss": 18.90193670988083, "val_acc": 48.0}
{"epoch": 23, "training_loss": 77.02314329147339, "training_acc": 47.0, "val_loss": 17.315949499607086, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.70197343826294, "training_acc": 53.0, "val_loss": 19.266240298748016, "val_acc": 52.0}
{"epoch": 25, "training_loss": 72.50495767593384, "training_acc": 53.0, "val_loss": 19.34940814971924, "val_acc": 48.0}
{"epoch": 26, "training_loss": 82.14131951332092, "training_acc": 47.0, "val_loss": 18.352968990802765, "val_acc": 52.0}
{"epoch": 27, "training_loss": 76.91282629966736, "training_acc": 53.0, "val_loss": 19.86314505338669, "val_acc": 52.0}
{"epoch": 28, "training_loss": 82.04215049743652, "training_acc": 45.0, "val_loss": 19.530826807022095, "val_acc": 48.0}
{"epoch": 29, "training_loss": 73.54857754707336, "training_acc": 47.0, "val_loss": 20.97453624010086, "val_acc": 52.0}
{"epoch": 30, "training_loss": 83.10971164703369, "training_acc": 53.0, "val_loss": 17.437228560447693, "val_acc": 52.0}
{"epoch": 31, "training_loss": 75.35450482368469, "training_acc": 47.0, "val_loss": 18.385668098926544, "val_acc": 48.0}
{"epoch": 32, "training_loss": 68.19011449813843, "training_acc": 59.0, "val_loss": 23.488418757915497, "val_acc": 52.0}
{"epoch": 33, "training_loss": 87.87181949615479, "training_acc": 53.0, "val_loss": 18.087993562221527, "val_acc": 52.0}
{"epoch": 34, "training_loss": 76.8509669303894, "training_acc": 47.0, "val_loss": 17.497950792312622, "val_acc": 52.0}
{"epoch": 35, "training_loss": 74.82526922225952, "training_acc": 47.0, "val_loss": 19.200611114501953, "val_acc": 52.0}
{"epoch": 36, "training_loss": 73.93510031700134, "training_acc": 51.0, "val_loss": 19.547779858112335, "val_acc": 48.0}
{"epoch": 37, "training_loss": 75.64972257614136, "training_acc": 47.0, "val_loss": 18.882863223552704, "val_acc": 52.0}
{"epoch": 38, "training_loss": 74.85617065429688, "training_acc": 53.0, "val_loss": 17.604802548885345, "val_acc": 52.0}
{"epoch": 39, "training_loss": 71.26417350769043, "training_acc": 47.0, "val_loss": 17.628152668476105, "val_acc": 52.0}
{"epoch": 40, "training_loss": 72.0986340045929, "training_acc": 47.0, "val_loss": 17.98030287027359, "val_acc": 52.0}
{"epoch": 41, "training_loss": 72.75785493850708, "training_acc": 47.0, "val_loss": 17.48463660478592, "val_acc": 52.0}
{"epoch": 42, "training_loss": 71.8561520576477, "training_acc": 45.0, "val_loss": 17.397440969944, "val_acc": 52.0}
