"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 34482.21189117432, "training_acc": 53.0, "val_loss": 7084.684753417969, "val_acc": 52.0}
{"epoch": 1, "training_loss": 46744.27734375, "training_acc": 45.0, "val_loss": 19735.003662109375, "val_acc": 48.0}
{"epoch": 2, "training_loss": 72979.78271484375, "training_acc": 47.0, "val_loss": 4482.763671875, "val_acc": 48.0}
{"epoch": 3, "training_loss": 23257.211669921875, "training_acc": 51.0, "val_loss": 15648.2666015625, "val_acc": 52.0}
{"epoch": 4, "training_loss": 62656.249267578125, "training_acc": 53.0, "val_loss": 14599.858093261719, "val_acc": 52.0}
{"epoch": 5, "training_loss": 48709.87658691406, "training_acc": 53.0, "val_loss": 729.7963619232178, "val_acc": 52.0}
{"epoch": 6, "training_loss": 18760.28857421875, "training_acc": 49.0, "val_loss": 16941.415405273438, "val_acc": 48.0}
{"epoch": 7, "training_loss": 71668.65502929688, "training_acc": 47.0, "val_loss": 15884.619140625, "val_acc": 48.0}
{"epoch": 8, "training_loss": 56117.213134765625, "training_acc": 47.0, "val_loss": 1273.0932235717773, "val_acc": 48.0}
{"epoch": 9, "training_loss": 13344.496948242188, "training_acc": 55.0, "val_loss": 15474.082946777344, "val_acc": 52.0}
{"epoch": 10, "training_loss": 65462.3203125, "training_acc": 53.0, "val_loss": 18680.84259033203, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69285.65600585938, "training_acc": 53.0, "val_loss": 10793.212890625, "val_acc": 52.0}
{"epoch": 12, "training_loss": 32474.743682861328, "training_acc": 53.0, "val_loss": 6129.350280761719, "val_acc": 48.0}
{"epoch": 13, "training_loss": 32549.642456054688, "training_acc": 47.0, "val_loss": 13455.259704589844, "val_acc": 48.0}
{"epoch": 14, "training_loss": 52663.504150390625, "training_acc": 47.0, "val_loss": 7460.143280029297, "val_acc": 48.0}
{"epoch": 15, "training_loss": 20639.527236938477, "training_acc": 47.0, "val_loss": 8184.2254638671875, "val_acc": 52.0}
{"epoch": 16, "training_loss": 38268.77697753906, "training_acc": 53.0, "val_loss": 15387.794494628906, "val_acc": 52.0}
{"epoch": 17, "training_loss": 59623.7265625, "training_acc": 53.0, "val_loss": 11878.564453125, "val_acc": 52.0}
{"epoch": 18, "training_loss": 39046.10827636719, "training_acc": 53.0, "val_loss": 737.4902248382568, "val_acc": 48.0}
{"epoch": 19, "training_loss": 8849.210754394531, "training_acc": 47.0, "val_loss": 5274.303436279297, "val_acc": 48.0}
{"epoch": 20, "training_loss": 17834.44024658203, "training_acc": 47.0, "val_loss": 2422.830581665039, "val_acc": 52.0}
{"epoch": 21, "training_loss": 11628.363830566406, "training_acc": 53.0, "val_loss": 3476.040267944336, "val_acc": 52.0}
{"epoch": 22, "training_loss": 9120.954605102539, "training_acc": 53.0, "val_loss": 5257.490539550781, "val_acc": 48.0}
{"epoch": 23, "training_loss": 24585.509521484375, "training_acc": 47.0, "val_loss": 6333.1634521484375, "val_acc": 48.0}
{"epoch": 24, "training_loss": 19822.58706665039, "training_acc": 47.0, "val_loss": 3899.3606567382812, "val_acc": 52.0}
