"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 88.26027631759644, "training_acc": 41.0, "val_loss": 18.29828917980194, "val_acc": 52.0}
{"epoch": 1, "training_loss": 74.21210980415344, "training_acc": 47.0, "val_loss": 20.97735106945038, "val_acc": 52.0}
{"epoch": 2, "training_loss": 82.71156358718872, "training_acc": 53.0, "val_loss": 20.150524377822876, "val_acc": 52.0}
{"epoch": 3, "training_loss": 76.5350992679596, "training_acc": 53.0, "val_loss": 17.354048788547516, "val_acc": 52.0}
{"epoch": 4, "training_loss": 71.75905585289001, "training_acc": 45.0, "val_loss": 18.740546703338623, "val_acc": 48.0}
{"epoch": 5, "training_loss": 75.53287959098816, "training_acc": 47.0, "val_loss": 18.33997070789337, "val_acc": 52.0}
{"epoch": 6, "training_loss": 72.51222729682922, "training_acc": 47.0, "val_loss": 17.308585345745087, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.11276841163635, "training_acc": 53.0, "val_loss": 18.14727485179901, "val_acc": 52.0}
{"epoch": 8, "training_loss": 72.51402354240417, "training_acc": 53.0, "val_loss": 18.343457579612732, "val_acc": 52.0}
{"epoch": 9, "training_loss": 71.81377625465393, "training_acc": 53.0, "val_loss": 17.391283810138702, "val_acc": 52.0}
{"epoch": 10, "training_loss": 68.76953959465027, "training_acc": 55.0, "val_loss": 17.772571742534637, "val_acc": 52.0}
{"epoch": 11, "training_loss": 72.84072613716125, "training_acc": 47.0, "val_loss": 18.124935030937195, "val_acc": 52.0}
{"epoch": 12, "training_loss": 72.3130145072937, "training_acc": 47.0, "val_loss": 17.319273948669434, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.35031032562256, "training_acc": 53.0, "val_loss": 17.72662103176117, "val_acc": 52.0}
{"epoch": 14, "training_loss": 71.65442299842834, "training_acc": 53.0, "val_loss": 17.872770130634308, "val_acc": 52.0}
{"epoch": 15, "training_loss": 70.511403799057, "training_acc": 53.0, "val_loss": 17.308466136455536, "val_acc": 52.0}
{"epoch": 16, "training_loss": 70.11220932006836, "training_acc": 45.0, "val_loss": 17.555424571037292, "val_acc": 52.0}
{"epoch": 17, "training_loss": 70.50099062919617, "training_acc": 47.0, "val_loss": 17.38985776901245, "val_acc": 52.0}
{"epoch": 18, "training_loss": 70.7673761844635, "training_acc": 39.0, "val_loss": 17.382733523845673, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.3205578327179, "training_acc": 53.0, "val_loss": 17.405812442302704, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.34690260887146, "training_acc": 53.0, "val_loss": 17.32865422964096, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.59591960906982, "training_acc": 53.0, "val_loss": 17.31538027524948, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.23336410522461, "training_acc": 53.0, "val_loss": 17.305178940296173, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.1513102054596, "training_acc": 53.0, "val_loss": 17.326638102531433, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.18433880805969, "training_acc": 53.0, "val_loss": 17.33916401863098, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.18966674804688, "training_acc": 53.0, "val_loss": 17.314842343330383, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.24889802932739, "training_acc": 53.0, "val_loss": 17.308436334133148, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.19050121307373, "training_acc": 53.0, "val_loss": 17.322608828544617, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.19315457344055, "training_acc": 53.0, "val_loss": 17.308661341667175, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.20822763442993, "training_acc": 53.0, "val_loss": 17.30712354183197, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.4185619354248, "training_acc": 53.0, "val_loss": 17.305852472782135, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.46408772468567, "training_acc": 52.0, "val_loss": 17.305666208267212, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.1777720451355, "training_acc": 53.0, "val_loss": 17.348647117614746, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.23908281326294, "training_acc": 53.0, "val_loss": 17.359478771686554, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.27251410484314, "training_acc": 53.0, "val_loss": 17.309531569480896, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.27752757072449, "training_acc": 53.0, "val_loss": 17.30518639087677, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.34091591835022, "training_acc": 53.0, "val_loss": 17.337261140346527, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.30421113967896, "training_acc": 53.0, "val_loss": 17.489807307720184, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.95013737678528, "training_acc": 53.0, "val_loss": 17.333541810512543, "val_acc": 52.0}
{"epoch": 39, "training_loss": 68.72617793083191, "training_acc": 53.0, "val_loss": 17.507091164588928, "val_acc": 52.0}
{"epoch": 40, "training_loss": 70.58056688308716, "training_acc": 47.0, "val_loss": 17.640966176986694, "val_acc": 52.0}
{"epoch": 41, "training_loss": 70.25708723068237, "training_acc": 47.0, "val_loss": 17.317843437194824, "val_acc": 52.0}
