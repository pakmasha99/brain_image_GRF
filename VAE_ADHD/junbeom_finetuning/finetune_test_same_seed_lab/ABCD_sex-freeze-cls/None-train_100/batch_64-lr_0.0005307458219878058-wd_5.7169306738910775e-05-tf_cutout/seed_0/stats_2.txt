"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.49466681480408, "training_acc": 49.0, "val_loss": 17.31281280517578, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.65187883377075, "training_acc": 53.0, "val_loss": 17.34555810689926, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.23481917381287, "training_acc": 53.0, "val_loss": 17.323686182498932, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.2275743484497, "training_acc": 53.0, "val_loss": 17.31177419424057, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.56734466552734, "training_acc": 41.0, "val_loss": 17.321650683879852, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.25673365592957, "training_acc": 53.0, "val_loss": 17.309579253196716, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.1298680305481, "training_acc": 53.0, "val_loss": 17.325517535209656, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.12648844718933, "training_acc": 53.0, "val_loss": 17.351806163787842, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.27973890304565, "training_acc": 53.0, "val_loss": 17.373043298721313, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.29723834991455, "training_acc": 53.0, "val_loss": 17.351429164409637, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.13762903213501, "training_acc": 53.0, "val_loss": 17.313551902770996, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.0593729019165, "training_acc": 53.0, "val_loss": 17.315687239170074, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.52501344680786, "training_acc": 43.0, "val_loss": 17.343047261238098, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.40547204017639, "training_acc": 48.0, "val_loss": 17.322582006454468, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.34500098228455, "training_acc": 53.0, "val_loss": 17.310239374637604, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.194575548172, "training_acc": 53.0, "val_loss": 17.309686541557312, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.11101841926575, "training_acc": 53.0, "val_loss": 17.313311994075775, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.11509203910828, "training_acc": 53.0, "val_loss": 17.32248216867447, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.20079731941223, "training_acc": 53.0, "val_loss": 17.331941425800323, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.15782475471497, "training_acc": 53.0, "val_loss": 17.362377047538757, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.43034982681274, "training_acc": 53.0, "val_loss": 17.376606166362762, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.42110776901245, "training_acc": 53.0, "val_loss": 17.334358394145966, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.14469623565674, "training_acc": 53.0, "val_loss": 17.320628464221954, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.14346194267273, "training_acc": 53.0, "val_loss": 17.311838269233704, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.1448302268982, "training_acc": 53.0, "val_loss": 17.30944663286209, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.20137906074524, "training_acc": 53.0, "val_loss": 17.312155663967133, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.19401335716248, "training_acc": 53.0, "val_loss": 17.311856150627136, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.19981503486633, "training_acc": 53.0, "val_loss": 17.309409379959106, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.14925813674927, "training_acc": 53.0, "val_loss": 17.31245517730713, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.1085147857666, "training_acc": 53.0, "val_loss": 17.326313257217407, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.18598675727844, "training_acc": 53.0, "val_loss": 17.35004335641861, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.22847318649292, "training_acc": 53.0, "val_loss": 17.347930371761322, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.17515254020691, "training_acc": 53.0, "val_loss": 17.321962118148804, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.1476559638977, "training_acc": 53.0, "val_loss": 17.310066521167755, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.22317361831665, "training_acc": 53.0, "val_loss": 17.31039434671402, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.17445850372314, "training_acc": 53.0, "val_loss": 17.30949878692627, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.25635170936584, "training_acc": 53.0, "val_loss": 17.3104390501976, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.14343643188477, "training_acc": 53.0, "val_loss": 17.331573367118835, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.13277840614319, "training_acc": 53.0, "val_loss": 17.41514503955841, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.53025388717651, "training_acc": 53.0, "val_loss": 17.46954321861267, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.62595415115356, "training_acc": 53.0, "val_loss": 17.41216480731964, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.35317015647888, "training_acc": 53.0, "val_loss": 17.357555031776428, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.34879326820374, "training_acc": 53.0, "val_loss": 17.314448952674866, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.18918943405151, "training_acc": 53.0, "val_loss": 17.309600114822388, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.16045904159546, "training_acc": 53.0, "val_loss": 17.309793829917908, "val_acc": 52.0}
{"epoch": 45, "training_loss": 69.17495036125183, "training_acc": 53.0, "val_loss": 17.309966683387756, "val_acc": 52.0}
{"epoch": 46, "training_loss": 69.19631242752075, "training_acc": 53.0, "val_loss": 17.321807146072388, "val_acc": 52.0}
