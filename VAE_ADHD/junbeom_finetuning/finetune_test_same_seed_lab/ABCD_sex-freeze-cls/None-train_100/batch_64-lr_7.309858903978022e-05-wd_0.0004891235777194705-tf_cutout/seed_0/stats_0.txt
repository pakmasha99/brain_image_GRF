"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.24061274528503, "training_acc": 52.0, "val_loss": 17.241454124450684, "val_acc": 56.0}
{"epoch": 1, "training_loss": 69.23695421218872, "training_acc": 52.0, "val_loss": 17.22092628479004, "val_acc": 56.0}
{"epoch": 2, "training_loss": 69.25563478469849, "training_acc": 52.0, "val_loss": 17.219001054763794, "val_acc": 56.0}
{"epoch": 3, "training_loss": 69.26373338699341, "training_acc": 52.0, "val_loss": 17.229408025741577, "val_acc": 56.0}
{"epoch": 4, "training_loss": 69.24052166938782, "training_acc": 52.0, "val_loss": 17.246198654174805, "val_acc": 56.0}
{"epoch": 5, "training_loss": 69.22646641731262, "training_acc": 52.0, "val_loss": 17.252276837825775, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.26044774055481, "training_acc": 52.0, "val_loss": 17.26474165916443, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.26017379760742, "training_acc": 52.0, "val_loss": 17.26990044116974, "val_acc": 56.0}
{"epoch": 8, "training_loss": 69.274498462677, "training_acc": 52.0, "val_loss": 17.263610661029816, "val_acc": 56.0}
{"epoch": 9, "training_loss": 69.25728964805603, "training_acc": 52.0, "val_loss": 17.2664612531662, "val_acc": 56.0}
{"epoch": 10, "training_loss": 69.234041929245, "training_acc": 52.0, "val_loss": 17.267318069934845, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.26443147659302, "training_acc": 52.0, "val_loss": 17.263473570346832, "val_acc": 56.0}
{"epoch": 12, "training_loss": 69.23440098762512, "training_acc": 52.0, "val_loss": 17.25884974002838, "val_acc": 56.0}
{"epoch": 13, "training_loss": 69.23903489112854, "training_acc": 52.0, "val_loss": 17.25728213787079, "val_acc": 56.0}
{"epoch": 14, "training_loss": 69.24722695350647, "training_acc": 52.0, "val_loss": 17.250318825244904, "val_acc": 56.0}
{"epoch": 15, "training_loss": 69.25052428245544, "training_acc": 52.0, "val_loss": 17.240510880947113, "val_acc": 56.0}
{"epoch": 16, "training_loss": 69.25141859054565, "training_acc": 52.0, "val_loss": 17.226719856262207, "val_acc": 56.0}
{"epoch": 17, "training_loss": 69.23347425460815, "training_acc": 52.0, "val_loss": 17.217855155467987, "val_acc": 56.0}
{"epoch": 18, "training_loss": 69.24206113815308, "training_acc": 52.0, "val_loss": 17.21123903989792, "val_acc": 56.0}
{"epoch": 19, "training_loss": 69.25935435295105, "training_acc": 52.0, "val_loss": 17.206808924674988, "val_acc": 56.0}
{"epoch": 20, "training_loss": 69.2089991569519, "training_acc": 52.0, "val_loss": 17.206695675849915, "val_acc": 56.0}
{"epoch": 21, "training_loss": 69.23006081581116, "training_acc": 52.0, "val_loss": 17.2061949968338, "val_acc": 56.0}
{"epoch": 22, "training_loss": 69.29024934768677, "training_acc": 52.0, "val_loss": 17.20273345708847, "val_acc": 56.0}
{"epoch": 23, "training_loss": 69.261962890625, "training_acc": 52.0, "val_loss": 17.206867039203644, "val_acc": 56.0}
{"epoch": 24, "training_loss": 69.25865316390991, "training_acc": 52.0, "val_loss": 17.215153574943542, "val_acc": 56.0}
{"epoch": 25, "training_loss": 69.25003480911255, "training_acc": 52.0, "val_loss": 17.227210104465485, "val_acc": 56.0}
{"epoch": 26, "training_loss": 69.2792284488678, "training_acc": 52.0, "val_loss": 17.232494056224823, "val_acc": 56.0}
{"epoch": 27, "training_loss": 69.22722935676575, "training_acc": 52.0, "val_loss": 17.248298227787018, "val_acc": 56.0}
{"epoch": 28, "training_loss": 69.250155210495, "training_acc": 52.0, "val_loss": 17.262522876262665, "val_acc": 56.0}
{"epoch": 29, "training_loss": 69.2644989490509, "training_acc": 52.0, "val_loss": 17.2671377658844, "val_acc": 56.0}
{"epoch": 30, "training_loss": 69.28050923347473, "training_acc": 52.0, "val_loss": 17.264369130134583, "val_acc": 56.0}
{"epoch": 31, "training_loss": 69.28404331207275, "training_acc": 52.0, "val_loss": 17.27031171321869, "val_acc": 56.0}
{"epoch": 32, "training_loss": 69.2522029876709, "training_acc": 52.0, "val_loss": 17.263633012771606, "val_acc": 56.0}
{"epoch": 33, "training_loss": 69.25773620605469, "training_acc": 52.0, "val_loss": 17.257556319236755, "val_acc": 56.0}
{"epoch": 34, "training_loss": 69.2612612247467, "training_acc": 52.0, "val_loss": 17.248331010341644, "val_acc": 56.0}
{"epoch": 35, "training_loss": 69.23985719680786, "training_acc": 52.0, "val_loss": 17.242956161499023, "val_acc": 56.0}
{"epoch": 36, "training_loss": 69.23711824417114, "training_acc": 52.0, "val_loss": 17.237424850463867, "val_acc": 56.0}
{"epoch": 37, "training_loss": 69.23698377609253, "training_acc": 52.0, "val_loss": 17.23111718893051, "val_acc": 56.0}
{"epoch": 38, "training_loss": 69.22866153717041, "training_acc": 52.0, "val_loss": 17.220178246498108, "val_acc": 56.0}
{"epoch": 39, "training_loss": 69.25318789482117, "training_acc": 52.0, "val_loss": 17.206351459026337, "val_acc": 56.0}
{"epoch": 40, "training_loss": 69.23335218429565, "training_acc": 52.0, "val_loss": 17.20070242881775, "val_acc": 56.0}
{"epoch": 41, "training_loss": 69.25910210609436, "training_acc": 52.0, "val_loss": 17.197397351264954, "val_acc": 56.0}
{"epoch": 42, "training_loss": 69.27243733406067, "training_acc": 52.0, "val_loss": 17.195819318294525, "val_acc": 56.0}
{"epoch": 43, "training_loss": 69.24250078201294, "training_acc": 52.0, "val_loss": 17.19900816679001, "val_acc": 56.0}
{"epoch": 44, "training_loss": 69.2565016746521, "training_acc": 52.0, "val_loss": 17.209453880786896, "val_acc": 56.0}
{"epoch": 45, "training_loss": 69.24101233482361, "training_acc": 52.0, "val_loss": 17.217811942100525, "val_acc": 56.0}
{"epoch": 46, "training_loss": 69.25437879562378, "training_acc": 52.0, "val_loss": 17.22823679447174, "val_acc": 56.0}
{"epoch": 47, "training_loss": 69.24015045166016, "training_acc": 52.0, "val_loss": 17.238707840442657, "val_acc": 56.0}
{"epoch": 48, "training_loss": 69.22543859481812, "training_acc": 52.0, "val_loss": 17.243114113807678, "val_acc": 56.0}
{"epoch": 49, "training_loss": 69.23265838623047, "training_acc": 52.0, "val_loss": 17.246778309345245, "val_acc": 56.0}
{"epoch": 50, "training_loss": 69.2542462348938, "training_acc": 52.0, "val_loss": 17.253278195858, "val_acc": 56.0}
{"epoch": 51, "training_loss": 69.25287914276123, "training_acc": 52.0, "val_loss": 17.260226607322693, "val_acc": 56.0}
{"epoch": 52, "training_loss": 69.25927996635437, "training_acc": 52.0, "val_loss": 17.26517081260681, "val_acc": 56.0}
{"epoch": 53, "training_loss": 69.26852488517761, "training_acc": 52.0, "val_loss": 17.270152270793915, "val_acc": 56.0}
{"epoch": 54, "training_loss": 69.22503972053528, "training_acc": 52.0, "val_loss": 17.273147404193878, "val_acc": 56.0}
{"epoch": 55, "training_loss": 69.26097989082336, "training_acc": 52.0, "val_loss": 17.269767820835114, "val_acc": 56.0}
{"epoch": 56, "training_loss": 69.25000333786011, "training_acc": 52.0, "val_loss": 17.269450426101685, "val_acc": 56.0}
{"epoch": 57, "training_loss": 69.24780821800232, "training_acc": 52.0, "val_loss": 17.270083725452423, "val_acc": 56.0}
{"epoch": 58, "training_loss": 69.23869442939758, "training_acc": 52.0, "val_loss": 17.265254259109497, "val_acc": 56.0}
{"epoch": 59, "training_loss": 69.26085495948792, "training_acc": 52.0, "val_loss": 17.260731756687164, "val_acc": 56.0}
{"epoch": 60, "training_loss": 69.28219199180603, "training_acc": 52.0, "val_loss": 17.250628769397736, "val_acc": 56.0}
{"epoch": 61, "training_loss": 69.2394151687622, "training_acc": 52.0, "val_loss": 17.24625676870346, "val_acc": 56.0}
