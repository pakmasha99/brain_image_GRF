"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.34983825683594, "training_acc": 53.0, "val_loss": 17.309558391571045, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.45411729812622, "training_acc": 53.0, "val_loss": 17.32558161020279, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.1662871837616, "training_acc": 53.0, "val_loss": 17.315559089183807, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.20219469070435, "training_acc": 53.0, "val_loss": 17.31002777814865, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.43035864830017, "training_acc": 53.0, "val_loss": 17.314063012599945, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.20431613922119, "training_acc": 53.0, "val_loss": 17.309586703777313, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.13834047317505, "training_acc": 53.0, "val_loss": 17.31817275285721, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.11828064918518, "training_acc": 53.0, "val_loss": 17.33393222093582, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.22338891029358, "training_acc": 53.0, "val_loss": 17.352740466594696, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.25652241706848, "training_acc": 53.0, "val_loss": 17.348824441432953, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.16771817207336, "training_acc": 53.0, "val_loss": 17.32301265001297, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.08469486236572, "training_acc": 53.0, "val_loss": 17.3093318939209, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.36259126663208, "training_acc": 53.0, "val_loss": 17.31596738100052, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.23017930984497, "training_acc": 53.0, "val_loss": 17.314429581165314, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.23936057090759, "training_acc": 53.0, "val_loss": 17.311981320381165, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.20286130905151, "training_acc": 53.0, "val_loss": 17.311008274555206, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.15750861167908, "training_acc": 53.0, "val_loss": 17.309585213661194, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.14164543151855, "training_acc": 53.0, "val_loss": 17.310282588005066, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.1417191028595, "training_acc": 53.0, "val_loss": 17.317096889019012, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.12303233146667, "training_acc": 53.0, "val_loss": 17.343831062316895, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.3843879699707, "training_acc": 53.0, "val_loss": 17.371295392513275, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.33023190498352, "training_acc": 53.0, "val_loss": 17.353180050849915, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.19603419303894, "training_acc": 53.0, "val_loss": 17.344512045383453, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.19114589691162, "training_acc": 53.0, "val_loss": 17.32911616563797, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.14268159866333, "training_acc": 53.0, "val_loss": 17.31667071580887, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.18267798423767, "training_acc": 53.0, "val_loss": 17.309291660785675, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.16603875160217, "training_acc": 53.0, "val_loss": 17.3110231757164, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.18229699134827, "training_acc": 53.0, "val_loss": 17.31085181236267, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.19998550415039, "training_acc": 53.0, "val_loss": 17.309901118278503, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.14720273017883, "training_acc": 53.0, "val_loss": 17.310313880443573, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.15825200080872, "training_acc": 53.0, "val_loss": 17.321527004241943, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.17680478096008, "training_acc": 53.0, "val_loss": 17.332342267036438, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.15055346488953, "training_acc": 53.0, "val_loss": 17.328572273254395, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.16237235069275, "training_acc": 53.0, "val_loss": 17.321181297302246, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.18735599517822, "training_acc": 53.0, "val_loss": 17.314517498016357, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.13660168647766, "training_acc": 53.0, "val_loss": 17.313560843467712, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.2530665397644, "training_acc": 53.0, "val_loss": 17.312487959861755, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.21425700187683, "training_acc": 53.0, "val_loss": 17.32017546892166, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.10853505134583, "training_acc": 53.0, "val_loss": 17.354662716388702, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.29254460334778, "training_acc": 53.0, "val_loss": 17.386874556541443, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.34617304801941, "training_acc": 53.0, "val_loss": 17.375503480434418, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.28362965583801, "training_acc": 53.0, "val_loss": 17.361541092395782, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.2903459072113, "training_acc": 53.0, "val_loss": 17.33299046754837, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.18142700195312, "training_acc": 53.0, "val_loss": 17.320352792739868, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.1400396823883, "training_acc": 53.0, "val_loss": 17.31540709733963, "val_acc": 52.0}
