"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 11141156.703781128, "training_acc": 50.0, "val_loss": 2161225.78125, "val_acc": 44.0}
{"epoch": 1, "training_loss": 11490112.375, "training_acc": 48.0, "val_loss": 5487187.5, "val_acc": 56.0}
{"epoch": 2, "training_loss": 23391379.1875, "training_acc": 52.0, "val_loss": 3357229.296875, "val_acc": 56.0}
{"epoch": 3, "training_loss": 10026384.7578125, "training_acc": 52.0, "val_loss": 3507658.59375, "val_acc": 44.0}
{"epoch": 4, "training_loss": 15967769.25, "training_acc": 48.0, "val_loss": 6112901.953125, "val_acc": 44.0}
{"epoch": 5, "training_loss": 21546165.375, "training_acc": 48.0, "val_loss": 3185250.9765625, "val_acc": 44.0}
{"epoch": 6, "training_loss": 8003844.14453125, "training_acc": 48.0, "val_loss": 2522058.0078125, "val_acc": 56.0}
{"epoch": 7, "training_loss": 13377662.9375, "training_acc": 52.0, "val_loss": 4870231.25, "val_acc": 56.0}
{"epoch": 8, "training_loss": 21185717.8125, "training_acc": 52.0, "val_loss": 3902955.46875, "val_acc": 56.0}
{"epoch": 9, "training_loss": 14514748.5625, "training_acc": 52.0, "val_loss": 253702.9296875, "val_acc": 56.0}
{"epoch": 10, "training_loss": 4749519.75, "training_acc": 50.0, "val_loss": 4919030.859375, "val_acc": 44.0}
{"epoch": 11, "training_loss": 19234205.6875, "training_acc": 48.0, "val_loss": 5446044.53125, "val_acc": 44.0}
{"epoch": 12, "training_loss": 18791388.9375, "training_acc": 48.0, "val_loss": 2377566.2109375, "val_acc": 44.0}
{"epoch": 13, "training_loss": 5650171.333984375, "training_acc": 52.0, "val_loss": 1349099.70703125, "val_acc": 56.0}
{"epoch": 14, "training_loss": 6451487.125, "training_acc": 52.0, "val_loss": 1477363.4765625, "val_acc": 56.0}
{"epoch": 15, "training_loss": 5174949.046875, "training_acc": 52.0, "val_loss": 1098185.44921875, "val_acc": 44.0}
{"epoch": 16, "training_loss": 4960360.0625, "training_acc": 48.0, "val_loss": 1545062.20703125, "val_acc": 44.0}
{"epoch": 17, "training_loss": 4247780.6484375, "training_acc": 48.0, "val_loss": 1123323.73046875, "val_acc": 56.0}
{"epoch": 18, "training_loss": 5953718.15625, "training_acc": 52.0, "val_loss": 1786614.6484375, "val_acc": 56.0}
{"epoch": 19, "training_loss": 6751002.265625, "training_acc": 52.0, "val_loss": 115617.39501953125, "val_acc": 44.0}
{"epoch": 20, "training_loss": 1244037.4296875, "training_acc": 48.0, "val_loss": 267426.171875, "val_acc": 44.0}
{"epoch": 21, "training_loss": 1580408.4609375, "training_acc": 54.0, "val_loss": 1019146.875, "val_acc": 56.0}
{"epoch": 22, "training_loss": 3875195.890625, "training_acc": 52.0, "val_loss": 394691.2109375, "val_acc": 44.0}
{"epoch": 23, "training_loss": 1758384.9453125, "training_acc": 48.0, "val_loss": 67808.80126953125, "val_acc": 44.0}
{"epoch": 24, "training_loss": 2008032.296875, "training_acc": 46.0, "val_loss": 1380163.8671875, "val_acc": 56.0}
{"epoch": 25, "training_loss": 5483362.28125, "training_acc": 52.0, "val_loss": 94137.20092773438, "val_acc": 56.0}
{"epoch": 26, "training_loss": 1685703.15625, "training_acc": 62.0, "val_loss": 2521288.28125, "val_acc": 44.0}
{"epoch": 27, "training_loss": 9293439.59375, "training_acc": 48.0, "val_loss": 1589643.9453125, "val_acc": 44.0}
{"epoch": 28, "training_loss": 3532824.1998291016, "training_acc": 54.0, "val_loss": 599692.919921875, "val_acc": 56.0}
{"epoch": 29, "training_loss": 2427159.40625, "training_acc": 52.0, "val_loss": 298080.0537109375, "val_acc": 44.0}
{"epoch": 30, "training_loss": 913012.3681640625, "training_acc": 48.0, "val_loss": 563048.193359375, "val_acc": 56.0}
{"epoch": 31, "training_loss": 2294149.43359375, "training_acc": 52.0, "val_loss": 224002.5634765625, "val_acc": 44.0}
{"epoch": 32, "training_loss": 566589.2624511719, "training_acc": 48.0, "val_loss": 786636.62109375, "val_acc": 56.0}
{"epoch": 33, "training_loss": 3524937.078125, "training_acc": 52.0, "val_loss": 321919.873046875, "val_acc": 56.0}
{"epoch": 34, "training_loss": 2009058.6015625, "training_acc": 54.0, "val_loss": 1395839.94140625, "val_acc": 44.0}
{"epoch": 35, "training_loss": 4538459.234375, "training_acc": 48.0, "val_loss": 303078.9306640625, "val_acc": 56.0}
{"epoch": 36, "training_loss": 1710249.7109375, "training_acc": 52.0, "val_loss": 224114.9658203125, "val_acc": 56.0}
{"epoch": 37, "training_loss": 2397302.140625, "training_acc": 42.0, "val_loss": 1136540.8203125, "val_acc": 44.0}
{"epoch": 38, "training_loss": 3136299.53125, "training_acc": 48.0, "val_loss": 920617.1875, "val_acc": 56.0}
{"epoch": 39, "training_loss": 5102485.0, "training_acc": 52.0, "val_loss": 1328517.87109375, "val_acc": 56.0}
{"epoch": 40, "training_loss": 4556674.8515625, "training_acc": 52.0, "val_loss": 1034687.01171875, "val_acc": 44.0}
{"epoch": 41, "training_loss": 4754036.328125, "training_acc": 48.0, "val_loss": 1443817.96875, "val_acc": 44.0}
{"epoch": 42, "training_loss": 4016511.1796875, "training_acc": 48.0, "val_loss": 1195657.6171875, "val_acc": 56.0}
