"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 9421378.282173157, "training_acc": 51.0, "val_loss": 2074305.859375, "val_acc": 52.0}
{"epoch": 1, "training_loss": 10283678.0, "training_acc": 55.0, "val_loss": 5663857.03125, "val_acc": 48.0}
{"epoch": 2, "training_loss": 21193745.3125, "training_acc": 47.0, "val_loss": 1439197.94921875, "val_acc": 48.0}
{"epoch": 3, "training_loss": 6436159.15625, "training_acc": 53.0, "val_loss": 4188032.8125, "val_acc": 52.0}
{"epoch": 4, "training_loss": 17098657.75, "training_acc": 53.0, "val_loss": 3733190.625, "val_acc": 52.0}
{"epoch": 5, "training_loss": 12040834.03125, "training_acc": 53.0, "val_loss": 720641.162109375, "val_acc": 48.0}
{"epoch": 6, "training_loss": 5528392.65625, "training_acc": 47.0, "val_loss": 1905456.25, "val_acc": 48.0}
{"epoch": 7, "training_loss": 6116335.5234375, "training_acc": 47.0, "val_loss": 1373919.140625, "val_acc": 52.0}
{"epoch": 8, "training_loss": 6569664.625, "training_acc": 53.0, "val_loss": 2137830.46875, "val_acc": 52.0}
{"epoch": 9, "training_loss": 7323618.328125, "training_acc": 53.0, "val_loss": 330895.751953125, "val_acc": 48.0}
{"epoch": 10, "training_loss": 2021013.375, "training_acc": 47.0, "val_loss": 266927.9541015625, "val_acc": 48.0}
{"epoch": 11, "training_loss": 2160364.640625, "training_acc": 49.0, "val_loss": 1423801.46484375, "val_acc": 52.0}
{"epoch": 12, "training_loss": 5035213.078125, "training_acc": 53.0, "val_loss": 47130.59997558594, "val_acc": 48.0}
{"epoch": 13, "training_loss": 539003.96484375, "training_acc": 47.0, "val_loss": 414237.6953125, "val_acc": 52.0}
{"epoch": 14, "training_loss": 1401187.5703125, "training_acc": 53.0, "val_loss": 579451.123046875, "val_acc": 48.0}
{"epoch": 15, "training_loss": 2145612.640625, "training_acc": 47.0, "val_loss": 489440.91796875, "val_acc": 52.0}
{"epoch": 16, "training_loss": 2083529.90625, "training_acc": 53.0, "val_loss": 47267.53845214844, "val_acc": 48.0}
{"epoch": 17, "training_loss": 359762.453125, "training_acc": 49.0, "val_loss": 372308.5205078125, "val_acc": 48.0}
{"epoch": 18, "training_loss": 1003259.5432128906, "training_acc": 47.0, "val_loss": 1047896.38671875, "val_acc": 52.0}
{"epoch": 19, "training_loss": 4433941.890625, "training_acc": 53.0, "val_loss": 869001.66015625, "val_acc": 52.0}
{"epoch": 20, "training_loss": 2618266.05859375, "training_acc": 49.0, "val_loss": 427063.916015625, "val_acc": 48.0}
{"epoch": 21, "training_loss": 1354488.212890625, "training_acc": 51.0, "val_loss": 93531.2255859375, "val_acc": 52.0}
{"epoch": 22, "training_loss": 1126525.8203125, "training_acc": 55.0, "val_loss": 667262.40234375, "val_acc": 48.0}
{"epoch": 23, "training_loss": 1902482.18359375, "training_acc": 51.0, "val_loss": 206135.8642578125, "val_acc": 52.0}
{"epoch": 24, "training_loss": 1288937.9296875, "training_acc": 49.0, "val_loss": 260828.3935546875, "val_acc": 48.0}
{"epoch": 25, "training_loss": 1700764.2265625, "training_acc": 49.0, "val_loss": 899347.55859375, "val_acc": 52.0}
{"epoch": 26, "training_loss": 2866283.86328125, "training_acc": 53.0, "val_loss": 839445.5078125, "val_acc": 48.0}
{"epoch": 27, "training_loss": 3588156.671875, "training_acc": 47.0, "val_loss": 399910.0830078125, "val_acc": 48.0}
{"epoch": 28, "training_loss": 2559323.078125, "training_acc": 45.0, "val_loss": 1313229.6875, "val_acc": 52.0}
{"epoch": 29, "training_loss": 4709684.328125, "training_acc": 53.0, "val_loss": 35239.7216796875, "val_acc": 52.0}
{"epoch": 30, "training_loss": 1897502.3671875, "training_acc": 59.0, "val_loss": 2384961.9140625, "val_acc": 48.0}
{"epoch": 31, "training_loss": 9597740.21875, "training_acc": 47.0, "val_loss": 1362270.703125, "val_acc": 48.0}
{"epoch": 32, "training_loss": 3811081.53515625, "training_acc": 51.0, "val_loss": 997184.765625, "val_acc": 52.0}
{"epoch": 33, "training_loss": 4131476.703125, "training_acc": 53.0, "val_loss": 303696.240234375, "val_acc": 52.0}
{"epoch": 34, "training_loss": 2503562.90625, "training_acc": 51.0, "val_loss": 1741550.0, "val_acc": 48.0}
{"epoch": 35, "training_loss": 6638008.203125, "training_acc": 47.0, "val_loss": 294997.8759765625, "val_acc": 48.0}
{"epoch": 36, "training_loss": 3002163.359375, "training_acc": 47.0, "val_loss": 2327779.4921875, "val_acc": 52.0}
{"epoch": 37, "training_loss": 9103380.75, "training_acc": 53.0, "val_loss": 1827495.1171875, "val_acc": 52.0}
{"epoch": 38, "training_loss": 5783003.03125, "training_acc": 53.0, "val_loss": 1051157.6171875, "val_acc": 48.0}
{"epoch": 39, "training_loss": 5205231.546875, "training_acc": 47.0, "val_loss": 1655986.328125, "val_acc": 48.0}
{"epoch": 40, "training_loss": 5494946.75, "training_acc": 47.0, "val_loss": 832600.390625, "val_acc": 52.0}
{"epoch": 41, "training_loss": 4371598.40625, "training_acc": 53.0, "val_loss": 1409699.21875, "val_acc": 52.0}
{"epoch": 42, "training_loss": 4303227.8125, "training_acc": 53.0, "val_loss": 806399.658203125, "val_acc": 48.0}
{"epoch": 43, "training_loss": 3739371.546875, "training_acc": 47.0, "val_loss": 1107225.87890625, "val_acc": 48.0}
{"epoch": 44, "training_loss": 3154571.93359375, "training_acc": 47.0, "val_loss": 1414214.35546875, "val_acc": 52.0}
{"epoch": 45, "training_loss": 6132256.984375, "training_acc": 53.0, "val_loss": 2070122.4609375, "val_acc": 52.0}
{"epoch": 46, "training_loss": 7190644.4375, "training_acc": 53.0, "val_loss": 231766.064453125, "val_acc": 52.0}
{"epoch": 47, "training_loss": 3688138.34375, "training_acc": 45.0, "val_loss": 2703144.140625, "val_acc": 48.0}
{"epoch": 48, "training_loss": 10916877.5, "training_acc": 47.0, "val_loss": 1859920.8984375, "val_acc": 48.0}
