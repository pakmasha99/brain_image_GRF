"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.23994541168213, "training_acc": 52.0, "val_loss": 17.240901291370392, "val_acc": 56.0}
{"epoch": 1, "training_loss": 69.2361192703247, "training_acc": 52.0, "val_loss": 17.22148060798645, "val_acc": 56.0}
{"epoch": 2, "training_loss": 69.25467872619629, "training_acc": 52.0, "val_loss": 17.219626903533936, "val_acc": 56.0}
{"epoch": 3, "training_loss": 69.26376461982727, "training_acc": 52.0, "val_loss": 17.229510843753815, "val_acc": 56.0}
{"epoch": 4, "training_loss": 69.24038290977478, "training_acc": 52.0, "val_loss": 17.245498299598694, "val_acc": 56.0}
{"epoch": 5, "training_loss": 69.22605848312378, "training_acc": 52.0, "val_loss": 17.25129336118698, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.25981879234314, "training_acc": 52.0, "val_loss": 17.263297736644745, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.25899195671082, "training_acc": 52.0, "val_loss": 17.268462479114532, "val_acc": 56.0}
{"epoch": 8, "training_loss": 69.27206349372864, "training_acc": 52.0, "val_loss": 17.26272404193878, "val_acc": 56.0}
{"epoch": 9, "training_loss": 69.2566921710968, "training_acc": 52.0, "val_loss": 17.265725135803223, "val_acc": 56.0}
{"epoch": 10, "training_loss": 69.2335798740387, "training_acc": 52.0, "val_loss": 17.266833782196045, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.26401710510254, "training_acc": 52.0, "val_loss": 17.263425886631012, "val_acc": 56.0}
{"epoch": 12, "training_loss": 69.23400616645813, "training_acc": 52.0, "val_loss": 17.259204387664795, "val_acc": 56.0}
{"epoch": 13, "training_loss": 69.23927712440491, "training_acc": 52.0, "val_loss": 17.257842421531677, "val_acc": 56.0}
{"epoch": 14, "training_loss": 69.24767756462097, "training_acc": 52.0, "val_loss": 17.251208424568176, "val_acc": 56.0}
{"epoch": 15, "training_loss": 69.25109839439392, "training_acc": 52.0, "val_loss": 17.241740226745605, "val_acc": 56.0}
{"epoch": 16, "training_loss": 69.25149607658386, "training_acc": 52.0, "val_loss": 17.228303849697113, "val_acc": 56.0}
{"epoch": 17, "training_loss": 69.2333824634552, "training_acc": 52.0, "val_loss": 17.219503223896027, "val_acc": 56.0}
{"epoch": 18, "training_loss": 69.24156212806702, "training_acc": 52.0, "val_loss": 17.21278727054596, "val_acc": 56.0}
{"epoch": 19, "training_loss": 69.25856423377991, "training_acc": 52.0, "val_loss": 17.208123207092285, "val_acc": 56.0}
{"epoch": 20, "training_loss": 69.20784854888916, "training_acc": 52.0, "val_loss": 17.207612097263336, "val_acc": 56.0}
{"epoch": 21, "training_loss": 69.22878193855286, "training_acc": 52.0, "val_loss": 17.206737399101257, "val_acc": 56.0}
{"epoch": 22, "training_loss": 69.28972244262695, "training_acc": 52.0, "val_loss": 17.203059792518616, "val_acc": 56.0}
{"epoch": 23, "training_loss": 69.26206517219543, "training_acc": 52.0, "val_loss": 17.206725478172302, "val_acc": 56.0}
{"epoch": 24, "training_loss": 69.25924777984619, "training_acc": 52.0, "val_loss": 17.214402556419373, "val_acc": 56.0}
{"epoch": 25, "training_loss": 69.24975323677063, "training_acc": 52.0, "val_loss": 17.225724458694458, "val_acc": 56.0}
{"epoch": 26, "training_loss": 69.27868986129761, "training_acc": 52.0, "val_loss": 17.230716347694397, "val_acc": 56.0}
{"epoch": 27, "training_loss": 69.22718548774719, "training_acc": 52.0, "val_loss": 17.24579930305481, "val_acc": 56.0}
{"epoch": 28, "training_loss": 69.24831795692444, "training_acc": 52.0, "val_loss": 17.259517312049866, "val_acc": 56.0}
{"epoch": 29, "training_loss": 69.26260757446289, "training_acc": 52.0, "val_loss": 17.26430505514145, "val_acc": 56.0}
{"epoch": 30, "training_loss": 69.276771068573, "training_acc": 52.0, "val_loss": 17.26217418909073, "val_acc": 56.0}
{"epoch": 31, "training_loss": 69.28216743469238, "training_acc": 52.0, "val_loss": 17.268332839012146, "val_acc": 56.0}
{"epoch": 32, "training_loss": 69.2506856918335, "training_acc": 52.0, "val_loss": 17.262496054172516, "val_acc": 56.0}
{"epoch": 33, "training_loss": 69.25745010375977, "training_acc": 52.0, "val_loss": 17.257174849510193, "val_acc": 56.0}
{"epoch": 34, "training_loss": 69.26044774055481, "training_acc": 52.0, "val_loss": 17.24873185157776, "val_acc": 56.0}
{"epoch": 35, "training_loss": 69.24014353752136, "training_acc": 52.0, "val_loss": 17.243854701519012, "val_acc": 56.0}
{"epoch": 36, "training_loss": 69.23762893676758, "training_acc": 52.0, "val_loss": 17.23870038986206, "val_acc": 56.0}
{"epoch": 37, "training_loss": 69.23731923103333, "training_acc": 52.0, "val_loss": 17.23267287015915, "val_acc": 56.0}
{"epoch": 38, "training_loss": 69.22908425331116, "training_acc": 52.0, "val_loss": 17.22204238176346, "val_acc": 56.0}
{"epoch": 39, "training_loss": 69.25153088569641, "training_acc": 52.0, "val_loss": 17.208461463451385, "val_acc": 56.0}
{"epoch": 40, "training_loss": 69.23189520835876, "training_acc": 52.0, "val_loss": 17.20273494720459, "val_acc": 56.0}
{"epoch": 41, "training_loss": 69.25725245475769, "training_acc": 52.0, "val_loss": 17.19922572374344, "val_acc": 56.0}
{"epoch": 42, "training_loss": 69.27052640914917, "training_acc": 52.0, "val_loss": 17.197363078594208, "val_acc": 56.0}
{"epoch": 43, "training_loss": 69.2413444519043, "training_acc": 52.0, "val_loss": 17.200110852718353, "val_acc": 56.0}
{"epoch": 44, "training_loss": 69.25495982170105, "training_acc": 52.0, "val_loss": 17.209836840629578, "val_acc": 56.0}
{"epoch": 45, "training_loss": 69.24143648147583, "training_acc": 52.0, "val_loss": 17.217521369457245, "val_acc": 56.0}
{"epoch": 46, "training_loss": 69.2547607421875, "training_acc": 52.0, "val_loss": 17.22719967365265, "val_acc": 56.0}
{"epoch": 47, "training_loss": 69.23919987678528, "training_acc": 52.0, "val_loss": 17.23698228597641, "val_acc": 56.0}
{"epoch": 48, "training_loss": 69.22514867782593, "training_acc": 52.0, "val_loss": 17.241105437278748, "val_acc": 56.0}
{"epoch": 49, "training_loss": 69.23202347755432, "training_acc": 52.0, "val_loss": 17.244647443294525, "val_acc": 56.0}
{"epoch": 50, "training_loss": 69.25349068641663, "training_acc": 52.0, "val_loss": 17.25098341703415, "val_acc": 56.0}
{"epoch": 51, "training_loss": 69.2517671585083, "training_acc": 52.0, "val_loss": 17.257848381996155, "val_acc": 56.0}
{"epoch": 52, "training_loss": 69.2577588558197, "training_acc": 52.0, "val_loss": 17.262902855873108, "val_acc": 56.0}
{"epoch": 53, "training_loss": 69.26701831817627, "training_acc": 52.0, "val_loss": 17.268066108226776, "val_acc": 56.0}
{"epoch": 54, "training_loss": 69.2236442565918, "training_acc": 52.0, "val_loss": 17.27139502763748, "val_acc": 56.0}
{"epoch": 55, "training_loss": 69.2588722705841, "training_acc": 52.0, "val_loss": 17.268653213977814, "val_acc": 56.0}
{"epoch": 56, "training_loss": 69.24889016151428, "training_acc": 52.0, "val_loss": 17.26880371570587, "val_acc": 56.0}
{"epoch": 57, "training_loss": 69.24753665924072, "training_acc": 52.0, "val_loss": 17.269812524318695, "val_acc": 56.0}
{"epoch": 58, "training_loss": 69.23845791816711, "training_acc": 52.0, "val_loss": 17.265525460243225, "val_acc": 56.0}
{"epoch": 59, "training_loss": 69.26134037971497, "training_acc": 52.0, "val_loss": 17.261438071727753, "val_acc": 56.0}
{"epoch": 60, "training_loss": 69.28169775009155, "training_acc": 52.0, "val_loss": 17.251865565776825, "val_acc": 56.0}
{"epoch": 61, "training_loss": 69.24006009101868, "training_acc": 52.0, "val_loss": 17.247697710990906, "val_acc": 56.0}
