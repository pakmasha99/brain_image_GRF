"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 70.37515759468079, "training_acc": 53.0, "val_loss": 17.65754669904709, "val_acc": 52.0}
{"epoch": 1, "training_loss": 70.24382090568542, "training_acc": 53.0, "val_loss": 17.580829560756683, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.90017247200012, "training_acc": 53.0, "val_loss": 17.521995306015015, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.67477512359619, "training_acc": 53.0, "val_loss": 17.476296424865723, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.66041541099548, "training_acc": 53.0, "val_loss": 17.43205189704895, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.43102955818176, "training_acc": 53.0, "val_loss": 17.399658262729645, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.39274024963379, "training_acc": 53.0, "val_loss": 17.37276166677475, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.30261731147766, "training_acc": 53.0, "val_loss": 17.35326200723648, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.26036429405212, "training_acc": 53.0, "val_loss": 17.34049916267395, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.179518699646, "training_acc": 53.0, "val_loss": 17.330873012542725, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.2084538936615, "training_acc": 53.0, "val_loss": 17.324306070804596, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.12378811836243, "training_acc": 53.0, "val_loss": 17.31889694929123, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.14581036567688, "training_acc": 53.0, "val_loss": 17.31499582529068, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.13256025314331, "training_acc": 53.0, "val_loss": 17.31255054473877, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.13793897628784, "training_acc": 53.0, "val_loss": 17.31131374835968, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.15691614151001, "training_acc": 53.0, "val_loss": 17.310646176338196, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.15191698074341, "training_acc": 53.0, "val_loss": 17.31017231941223, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.14269781112671, "training_acc": 53.0, "val_loss": 17.310035228729248, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.1563560962677, "training_acc": 53.0, "val_loss": 17.309992015361786, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.13584637641907, "training_acc": 53.0, "val_loss": 17.31000542640686, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.14045691490173, "training_acc": 53.0, "val_loss": 17.310243844985962, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.13380813598633, "training_acc": 53.0, "val_loss": 17.31075942516327, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.13540816307068, "training_acc": 53.0, "val_loss": 17.310455441474915, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.2062439918518, "training_acc": 53.0, "val_loss": 17.310282588005066, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.13237452507019, "training_acc": 53.0, "val_loss": 17.310252785682678, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.14705920219421, "training_acc": 53.0, "val_loss": 17.309975624084473, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.14358949661255, "training_acc": 53.0, "val_loss": 17.309390008449554, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.12930417060852, "training_acc": 53.0, "val_loss": 17.309066653251648, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.15645241737366, "training_acc": 53.0, "val_loss": 17.308905720710754, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.14020895957947, "training_acc": 53.0, "val_loss": 17.309054732322693, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.14666032791138, "training_acc": 53.0, "val_loss": 17.309890687465668, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.18606495857239, "training_acc": 53.0, "val_loss": 17.31092780828476, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.17376756668091, "training_acc": 53.0, "val_loss": 17.311540246009827, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.1323652267456, "training_acc": 53.0, "val_loss": 17.311565577983856, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.139484167099, "training_acc": 53.0, "val_loss": 17.311540246009827, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.14777278900146, "training_acc": 53.0, "val_loss": 17.312091588974, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.10229420661926, "training_acc": 53.0, "val_loss": 17.311997711658478, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.17462587356567, "training_acc": 53.0, "val_loss": 17.31182038784027, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.13707542419434, "training_acc": 53.0, "val_loss": 17.313167452812195, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.15840554237366, "training_acc": 53.0, "val_loss": 17.313332855701447, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.13316750526428, "training_acc": 53.0, "val_loss": 17.313319444656372, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.12570261955261, "training_acc": 53.0, "val_loss": 17.314571142196655, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.15785217285156, "training_acc": 53.0, "val_loss": 17.315539717674255, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.13219285011292, "training_acc": 53.0, "val_loss": 17.316220700740814, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.14995193481445, "training_acc": 53.0, "val_loss": 17.315761744976044, "val_acc": 52.0}
{"epoch": 45, "training_loss": 69.15626859664917, "training_acc": 53.0, "val_loss": 17.315472662448883, "val_acc": 52.0}
{"epoch": 46, "training_loss": 69.12316823005676, "training_acc": 53.0, "val_loss": 17.316170036792755, "val_acc": 52.0}
{"epoch": 47, "training_loss": 69.14290189743042, "training_acc": 53.0, "val_loss": 17.316097021102905, "val_acc": 52.0}
