"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 401443.4344444275, "training_acc": 49.0, "val_loss": 89054.0771484375, "val_acc": 48.0}
{"epoch": 1, "training_loss": 341152.462890625, "training_acc": 55.0, "val_loss": 182924.30419921875, "val_acc": 52.0}
{"epoch": 2, "training_loss": 694057.53515625, "training_acc": 53.0, "val_loss": 101629.833984375, "val_acc": 52.0}
{"epoch": 3, "training_loss": 267649.4104614258, "training_acc": 49.0, "val_loss": 38394.354248046875, "val_acc": 48.0}
{"epoch": 4, "training_loss": 125961.3095703125, "training_acc": 47.0, "val_loss": 41333.67614746094, "val_acc": 52.0}
{"epoch": 5, "training_loss": 181610.244140625, "training_acc": 53.0, "val_loss": 35003.729248046875, "val_acc": 52.0}
{"epoch": 6, "training_loss": 127967.8671875, "training_acc": 45.0, "val_loss": 24111.00311279297, "val_acc": 48.0}
{"epoch": 7, "training_loss": 76090.1533203125, "training_acc": 47.0, "val_loss": 3293.901824951172, "val_acc": 52.0}
{"epoch": 8, "training_loss": 52471.734375, "training_acc": 51.0, "val_loss": 26366.238403320312, "val_acc": 48.0}
{"epoch": 9, "training_loss": 77591.69763183594, "training_acc": 51.0, "val_loss": 11309.191131591797, "val_acc": 52.0}
{"epoch": 10, "training_loss": 43469.255126953125, "training_acc": 53.0, "val_loss": 2957.925796508789, "val_acc": 48.0}
{"epoch": 11, "training_loss": 62282.77734375, "training_acc": 45.0, "val_loss": 38843.73779296875, "val_acc": 52.0}
{"epoch": 12, "training_loss": 120806.958984375, "training_acc": 53.0, "val_loss": 27312.060546875, "val_acc": 48.0}
{"epoch": 13, "training_loss": 131987.2529296875, "training_acc": 47.0, "val_loss": 19438.85498046875, "val_acc": 48.0}
{"epoch": 14, "training_loss": 91953.0166015625, "training_acc": 47.0, "val_loss": 40546.42028808594, "val_acc": 52.0}
{"epoch": 15, "training_loss": 140918.94384765625, "training_acc": 53.0, "val_loss": 5561.708068847656, "val_acc": 48.0}
{"epoch": 16, "training_loss": 27233.990844726562, "training_acc": 47.0, "val_loss": 15566.6015625, "val_acc": 52.0}
{"epoch": 17, "training_loss": 57860.2646484375, "training_acc": 53.0, "val_loss": 13679.304504394531, "val_acc": 48.0}
{"epoch": 18, "training_loss": 52651.157470703125, "training_acc": 47.0, "val_loss": 19334.007263183594, "val_acc": 52.0}
{"epoch": 19, "training_loss": 81315.19506835938, "training_acc": 53.0, "val_loss": 2723.210906982422, "val_acc": 52.0}
{"epoch": 20, "training_loss": 67641.111328125, "training_acc": 51.0, "val_loss": 52666.51611328125, "val_acc": 48.0}
{"epoch": 21, "training_loss": 187213.97607421875, "training_acc": 47.0, "val_loss": 9337.635803222656, "val_acc": 52.0}
{"epoch": 22, "training_loss": 58038.542236328125, "training_acc": 53.0, "val_loss": 12167.325592041016, "val_acc": 52.0}
{"epoch": 23, "training_loss": 59650.824462890625, "training_acc": 53.0, "val_loss": 26238.510131835938, "val_acc": 48.0}
{"epoch": 24, "training_loss": 71372.01724243164, "training_acc": 47.0, "val_loss": 43765.728759765625, "val_acc": 52.0}
{"epoch": 25, "training_loss": 205356.916015625, "training_acc": 53.0, "val_loss": 53168.707275390625, "val_acc": 52.0}
{"epoch": 26, "training_loss": 157710.7431640625, "training_acc": 53.0, "val_loss": 33275.13732910156, "val_acc": 48.0}
{"epoch": 27, "training_loss": 167965.8974609375, "training_acc": 47.0, "val_loss": 47085.174560546875, "val_acc": 48.0}
{"epoch": 28, "training_loss": 136232.46997070312, "training_acc": 47.0, "val_loss": 43040.35949707031, "val_acc": 52.0}
{"epoch": 29, "training_loss": 207359.986328125, "training_acc": 53.0, "val_loss": 70526.76391601562, "val_acc": 52.0}
{"epoch": 30, "training_loss": 239389.4111328125, "training_acc": 53.0, "val_loss": 4759.127426147461, "val_acc": 52.0}
{"epoch": 31, "training_loss": 130300.810546875, "training_acc": 43.0, "val_loss": 94646.35620117188, "val_acc": 48.0}
{"epoch": 32, "training_loss": 385324.384765625, "training_acc": 47.0, "val_loss": 62610.8154296875, "val_acc": 48.0}
{"epoch": 33, "training_loss": 187059.62701416016, "training_acc": 47.0, "val_loss": 64464.85595703125, "val_acc": 52.0}
{"epoch": 34, "training_loss": 310901.681640625, "training_acc": 53.0, "val_loss": 111770.7763671875, "val_acc": 52.0}
{"epoch": 35, "training_loss": 412329.6455078125, "training_acc": 53.0, "val_loss": 60209.9853515625, "val_acc": 52.0}
{"epoch": 36, "training_loss": 184302.0303955078, "training_acc": 53.0, "val_loss": 70817.7001953125, "val_acc": 48.0}
{"epoch": 37, "training_loss": 343268.0859375, "training_acc": 47.0, "val_loss": 108817.37060546875, "val_acc": 48.0}
{"epoch": 38, "training_loss": 403293.3125, "training_acc": 47.0, "val_loss": 33083.82873535156, "val_acc": 48.0}
