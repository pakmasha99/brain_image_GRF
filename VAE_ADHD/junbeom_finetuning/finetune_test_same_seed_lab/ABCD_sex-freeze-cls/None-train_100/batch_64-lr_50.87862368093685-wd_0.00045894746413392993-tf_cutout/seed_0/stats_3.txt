"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 453318.81590270996, "training_acc": 53.0, "val_loss": 87314.35546875, "val_acc": 52.0}
{"epoch": 1, "training_loss": 539808.34765625, "training_acc": 45.0, "val_loss": 223840.4541015625, "val_acc": 48.0}
{"epoch": 2, "training_loss": 816236.443359375, "training_acc": 47.0, "val_loss": 45980.694580078125, "val_acc": 48.0}
{"epoch": 3, "training_loss": 271791.861328125, "training_acc": 49.0, "val_loss": 178828.80859375, "val_acc": 52.0}
{"epoch": 4, "training_loss": 707454.603515625, "training_acc": 53.0, "val_loss": 152599.90234375, "val_acc": 52.0}
{"epoch": 5, "training_loss": 487778.3408203125, "training_acc": 53.0, "val_loss": 20562.01934814453, "val_acc": 48.0}
{"epoch": 6, "training_loss": 124489.69921875, "training_acc": 47.0, "val_loss": 59594.842529296875, "val_acc": 48.0}
{"epoch": 7, "training_loss": 174312.48071289062, "training_acc": 47.0, "val_loss": 60843.06640625, "val_acc": 52.0}
{"epoch": 8, "training_loss": 281436.0498046875, "training_acc": 53.0, "val_loss": 83738.76342773438, "val_acc": 52.0}
{"epoch": 9, "training_loss": 266980.2490234375, "training_acc": 53.0, "val_loss": 26326.052856445312, "val_acc": 48.0}
{"epoch": 10, "training_loss": 147825.4287109375, "training_acc": 47.0, "val_loss": 36178.51867675781, "val_acc": 48.0}
{"epoch": 11, "training_loss": 105292.73278808594, "training_acc": 53.0, "val_loss": 27565.921020507812, "val_acc": 52.0}
{"epoch": 12, "training_loss": 81182.99212646484, "training_acc": 53.0, "val_loss": 43231.41174316406, "val_acc": 48.0}
{"epoch": 13, "training_loss": 182902.5322265625, "training_acc": 47.0, "val_loss": 17369.3603515625, "val_acc": 48.0}
{"epoch": 14, "training_loss": 112929.08349609375, "training_acc": 47.0, "val_loss": 65126.3671875, "val_acc": 52.0}
{"epoch": 15, "training_loss": 238877.720703125, "training_acc": 53.0, "val_loss": 13488.066101074219, "val_acc": 52.0}
{"epoch": 16, "training_loss": 139013.5830078125, "training_acc": 45.0, "val_loss": 84870.3125, "val_acc": 48.0}
{"epoch": 17, "training_loss": 320662.1572265625, "training_acc": 47.0, "val_loss": 21034.85870361328, "val_acc": 48.0}
{"epoch": 18, "training_loss": 127259.61181640625, "training_acc": 49.0, "val_loss": 84935.49194335938, "val_acc": 52.0}
{"epoch": 19, "training_loss": 329020.173828125, "training_acc": 53.0, "val_loss": 55863.079833984375, "val_acc": 52.0}
{"epoch": 20, "training_loss": 144773.13446044922, "training_acc": 53.0, "val_loss": 77903.32641601562, "val_acc": 48.0}
{"epoch": 21, "training_loss": 375813.9921875, "training_acc": 47.0, "val_loss": 110559.130859375, "val_acc": 48.0}
{"epoch": 22, "training_loss": 387251.0029296875, "training_acc": 47.0, "val_loss": 469.2970275878906, "val_acc": 52.0}
{"epoch": 23, "training_loss": 62912.689880371094, "training_acc": 53.0, "val_loss": 25832.568359375, "val_acc": 52.0}
{"epoch": 24, "training_loss": 84249.49963378906, "training_acc": 49.0, "val_loss": 9974.331665039062, "val_acc": 48.0}
{"epoch": 25, "training_loss": 52020.65283203125, "training_acc": 53.0, "val_loss": 24712.782287597656, "val_acc": 52.0}
{"epoch": 26, "training_loss": 56792.31805419922, "training_acc": 53.0, "val_loss": 57537.02392578125, "val_acc": 48.0}
{"epoch": 27, "training_loss": 253832.1328125, "training_acc": 47.0, "val_loss": 58724.176025390625, "val_acc": 48.0}
{"epoch": 28, "training_loss": 163574.93774414062, "training_acc": 47.0, "val_loss": 61973.49853515625, "val_acc": 52.0}
{"epoch": 29, "training_loss": 291134.6279296875, "training_acc": 53.0, "val_loss": 98888.57421875, "val_acc": 52.0}
{"epoch": 30, "training_loss": 354020.8056640625, "training_acc": 53.0, "val_loss": 19628.140258789062, "val_acc": 52.0}
{"epoch": 31, "training_loss": 89110.2353515625, "training_acc": 63.0, "val_loss": 90258.7646484375, "val_acc": 48.0}
{"epoch": 32, "training_loss": 365811.654296875, "training_acc": 47.0, "val_loss": 54427.05078125, "val_acc": 48.0}
{"epoch": 33, "training_loss": 185804.40649414062, "training_acc": 41.0, "val_loss": 38841.56799316406, "val_acc": 52.0}
{"epoch": 34, "training_loss": 139144.2666015625, "training_acc": 53.0, "val_loss": 6147.275161743164, "val_acc": 48.0}
{"epoch": 35, "training_loss": 21540.95489501953, "training_acc": 47.0, "val_loss": 28563.705444335938, "val_acc": 52.0}
{"epoch": 36, "training_loss": 111256.25732421875, "training_acc": 53.0, "val_loss": 506.2286376953125, "val_acc": 48.0}
{"epoch": 37, "training_loss": 6644.7269287109375, "training_acc": 53.0, "val_loss": 17953.323364257812, "val_acc": 48.0}
{"epoch": 38, "training_loss": 50920.25183105469, "training_acc": 47.0, "val_loss": 40998.81896972656, "val_acc": 52.0}
{"epoch": 39, "training_loss": 175595.67041015625, "training_acc": 53.0, "val_loss": 32533.36181640625, "val_acc": 52.0}
{"epoch": 40, "training_loss": 117968.14135742188, "training_acc": 45.0, "val_loss": 22864.602661132812, "val_acc": 48.0}
{"epoch": 41, "training_loss": 61317.20831298828, "training_acc": 53.0, "val_loss": 1984.6565246582031, "val_acc": 52.0}
