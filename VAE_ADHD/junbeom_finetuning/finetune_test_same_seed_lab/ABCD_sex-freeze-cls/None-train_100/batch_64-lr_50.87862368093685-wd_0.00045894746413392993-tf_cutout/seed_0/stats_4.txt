"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 381096.7575302124, "training_acc": 49.0, "val_loss": 99654.66918945312, "val_acc": 52.0}
{"epoch": 1, "training_loss": 390706.21484375, "training_acc": 55.0, "val_loss": 173154.833984375, "val_acc": 48.0}
{"epoch": 2, "training_loss": 611571.916015625, "training_acc": 47.0, "val_loss": 676.4634132385254, "val_acc": 48.0}
{"epoch": 3, "training_loss": 187332.3203125, "training_acc": 51.0, "val_loss": 209599.8291015625, "val_acc": 52.0}
{"epoch": 4, "training_loss": 826749.77734375, "training_acc": 53.0, "val_loss": 183338.46435546875, "val_acc": 52.0}
{"epoch": 5, "training_loss": 629387.8125, "training_acc": 53.0, "val_loss": 14321.707153320312, "val_acc": 52.0}
{"epoch": 6, "training_loss": 162465.64453125, "training_acc": 57.0, "val_loss": 175908.11767578125, "val_acc": 48.0}
{"epoch": 7, "training_loss": 719420.935546875, "training_acc": 47.0, "val_loss": 159029.2724609375, "val_acc": 48.0}
{"epoch": 8, "training_loss": 551654.1220703125, "training_acc": 47.0, "val_loss": 187.25075721740723, "val_acc": 48.0}
{"epoch": 9, "training_loss": 192864.9210205078, "training_acc": 41.0, "val_loss": 168211.7919921875, "val_acc": 52.0}
{"epoch": 10, "training_loss": 676394.724609375, "training_acc": 53.0, "val_loss": 174942.9931640625, "val_acc": 52.0}
{"epoch": 11, "training_loss": 624298.384765625, "training_acc": 53.0, "val_loss": 70844.40307617188, "val_acc": 52.0}
{"epoch": 12, "training_loss": 148505.41046142578, "training_acc": 63.0, "val_loss": 59722.967529296875, "val_acc": 48.0}
{"epoch": 13, "training_loss": 252744.87109375, "training_acc": 47.0, "val_loss": 42247.271728515625, "val_acc": 48.0}
{"epoch": 14, "training_loss": 117180.93957519531, "training_acc": 53.0, "val_loss": 32690.774536132812, "val_acc": 52.0}
{"epoch": 15, "training_loss": 112868.0400390625, "training_acc": 53.0, "val_loss": 15541.5283203125, "val_acc": 48.0}
{"epoch": 16, "training_loss": 74599.7197265625, "training_acc": 47.0, "val_loss": 12651.433563232422, "val_acc": 52.0}
{"epoch": 17, "training_loss": 48138.75964355469, "training_acc": 53.0, "val_loss": 12279.94384765625, "val_acc": 48.0}
{"epoch": 18, "training_loss": 34661.06791687012, "training_acc": 45.0, "val_loss": 20628.456115722656, "val_acc": 48.0}
{"epoch": 19, "training_loss": 62086.960876464844, "training_acc": 47.0, "val_loss": 35448.065185546875, "val_acc": 52.0}
{"epoch": 20, "training_loss": 153386.44482421875, "training_acc": 53.0, "val_loss": 24197.401428222656, "val_acc": 52.0}
{"epoch": 21, "training_loss": 74066.25854492188, "training_acc": 59.0, "val_loss": 33701.51062011719, "val_acc": 48.0}
{"epoch": 22, "training_loss": 109665.42504882812, "training_acc": 47.0, "val_loss": 34193.20983886719, "val_acc": 52.0}
{"epoch": 23, "training_loss": 148114.01025390625, "training_acc": 53.0, "val_loss": 25927.322387695312, "val_acc": 52.0}
{"epoch": 24, "training_loss": 116953.64306640625, "training_acc": 41.0, "val_loss": 27799.9755859375, "val_acc": 48.0}
{"epoch": 25, "training_loss": 82619.30932617188, "training_acc": 43.0, "val_loss": 3196.50821685791, "val_acc": 48.0}
{"epoch": 26, "training_loss": 28764.325927734375, "training_acc": 55.0, "val_loss": 16216.395568847656, "val_acc": 52.0}
{"epoch": 27, "training_loss": 48720.23083496094, "training_acc": 57.0, "val_loss": 7780.310821533203, "val_acc": 48.0}
