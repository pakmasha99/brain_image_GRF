"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.94367480278015, "training_acc": 47.0, "val_loss": 17.29867160320282, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.02527093887329, "training_acc": 53.0, "val_loss": 17.360879480838776, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.46170592308044, "training_acc": 53.0, "val_loss": 17.462266981601715, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.57915496826172, "training_acc": 53.0, "val_loss": 17.415355145931244, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.45862603187561, "training_acc": 53.0, "val_loss": 17.33592301607132, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.30169987678528, "training_acc": 53.0, "val_loss": 17.299163341522217, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.16334009170532, "training_acc": 53.0, "val_loss": 17.296233773231506, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.17680144309998, "training_acc": 53.0, "val_loss": 17.297843098640442, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.19453549385071, "training_acc": 53.0, "val_loss": 17.29690134525299, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.16267156600952, "training_acc": 53.0, "val_loss": 17.29609966278076, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.16608667373657, "training_acc": 53.0, "val_loss": 17.301565408706665, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.14373803138733, "training_acc": 53.0, "val_loss": 17.30951964855194, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.15221691131592, "training_acc": 53.0, "val_loss": 17.3173651099205, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.29966259002686, "training_acc": 53.0, "val_loss": 17.323243618011475, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.14562916755676, "training_acc": 53.0, "val_loss": 17.304380238056183, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.12712121009827, "training_acc": 53.0, "val_loss": 17.296382784843445, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.13890385627747, "training_acc": 53.0, "val_loss": 17.296229302883148, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.16718983650208, "training_acc": 53.0, "val_loss": 17.299212515354156, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.19487261772156, "training_acc": 53.0, "val_loss": 17.304906249046326, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.23432993888855, "training_acc": 53.0, "val_loss": 17.302902042865753, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.22092151641846, "training_acc": 53.0, "val_loss": 17.297159135341644, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.15483665466309, "training_acc": 53.0, "val_loss": 17.297732830047607, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.14867520332336, "training_acc": 53.0, "val_loss": 17.322254180908203, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.29586148262024, "training_acc": 53.0, "val_loss": 17.35011637210846, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.29151129722595, "training_acc": 53.0, "val_loss": 17.344875633716583, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.21511459350586, "training_acc": 53.0, "val_loss": 17.31744557619095, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.21612691879272, "training_acc": 53.0, "val_loss": 17.29980558156967, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.1185462474823, "training_acc": 53.0, "val_loss": 17.296192049980164, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.15883302688599, "training_acc": 53.0, "val_loss": 17.29690283536911, "val_acc": 52.0}
