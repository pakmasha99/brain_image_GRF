"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 104145.68216705322, "training_acc": 49.0, "val_loss": 24650.6591796875, "val_acc": 52.0}
{"epoch": 1, "training_loss": 132655.86669921875, "training_acc": 45.0, "val_loss": 47766.82434082031, "val_acc": 48.0}
{"epoch": 2, "training_loss": 173663.69677734375, "training_acc": 47.0, "val_loss": 3155.843162536621, "val_acc": 48.0}
{"epoch": 3, "training_loss": 59569.85888671875, "training_acc": 47.0, "val_loss": 51262.24365234375, "val_acc": 52.0}
{"epoch": 4, "training_loss": 201878.60986328125, "training_acc": 53.0, "val_loss": 45439.92004394531, "val_acc": 52.0}
{"epoch": 5, "training_loss": 155979.87744140625, "training_acc": 53.0, "val_loss": 5261.349868774414, "val_acc": 52.0}
{"epoch": 6, "training_loss": 58197.375, "training_acc": 47.0, "val_loss": 43240.21911621094, "val_acc": 48.0}
{"epoch": 7, "training_loss": 179986.8984375, "training_acc": 47.0, "val_loss": 38333.48693847656, "val_acc": 48.0}
{"epoch": 8, "training_loss": 134668.99975585938, "training_acc": 47.0, "val_loss": 2877.4906158447266, "val_acc": 52.0}
{"epoch": 9, "training_loss": 36483.899169921875, "training_acc": 53.0, "val_loss": 16758.480834960938, "val_acc": 52.0}
{"epoch": 10, "training_loss": 55370.982177734375, "training_acc": 53.0, "val_loss": 4679.52766418457, "val_acc": 48.0}
{"epoch": 11, "training_loss": 25427.811157226562, "training_acc": 47.0, "val_loss": 3753.7593841552734, "val_acc": 48.0}
{"epoch": 12, "training_loss": 22856.118408203125, "training_acc": 49.0, "val_loss": 13077.401733398438, "val_acc": 52.0}
{"epoch": 13, "training_loss": 45762.63830566406, "training_acc": 53.0, "val_loss": 1667.0455932617188, "val_acc": 48.0}
{"epoch": 14, "training_loss": 9513.99365234375, "training_acc": 47.0, "val_loss": 3886.812210083008, "val_acc": 52.0}
{"epoch": 15, "training_loss": 13564.762908935547, "training_acc": 53.0, "val_loss": 5382.537078857422, "val_acc": 48.0}
{"epoch": 16, "training_loss": 20106.614868164062, "training_acc": 47.0, "val_loss": 5486.135482788086, "val_acc": 52.0}
{"epoch": 17, "training_loss": 22533.085021972656, "training_acc": 53.0, "val_loss": 798.8709449768066, "val_acc": 52.0}
{"epoch": 18, "training_loss": 14585.3740234375, "training_acc": 59.0, "val_loss": 15080.461120605469, "val_acc": 48.0}
{"epoch": 19, "training_loss": 54091.88244628906, "training_acc": 47.0, "val_loss": 3068.2018280029297, "val_acc": 52.0}
{"epoch": 20, "training_loss": 15254.965148925781, "training_acc": 53.0, "val_loss": 2500.0667572021484, "val_acc": 52.0}
{"epoch": 21, "training_loss": 13703.0966796875, "training_acc": 59.0, "val_loss": 9238.429260253906, "val_acc": 48.0}
{"epoch": 22, "training_loss": 27533.80889892578, "training_acc": 47.0, "val_loss": 11323.921966552734, "val_acc": 52.0}
{"epoch": 23, "training_loss": 49040.86706542969, "training_acc": 53.0, "val_loss": 14110.310363769531, "val_acc": 52.0}
{"epoch": 24, "training_loss": 44137.14044189453, "training_acc": 53.0, "val_loss": 9813.644409179688, "val_acc": 48.0}
{"epoch": 25, "training_loss": 50062.691650390625, "training_acc": 47.0, "val_loss": 11093.524169921875, "val_acc": 48.0}
{"epoch": 26, "training_loss": 31191.50079345703, "training_acc": 49.0, "val_loss": 5019.754409790039, "val_acc": 52.0}
{"epoch": 27, "training_loss": 14136.437088012695, "training_acc": 53.0, "val_loss": 10204.23812866211, "val_acc": 48.0}
{"epoch": 28, "training_loss": 44481.898681640625, "training_acc": 47.0, "val_loss": 4759.002685546875, "val_acc": 48.0}
{"epoch": 29, "training_loss": 25971.497802734375, "training_acc": 49.0, "val_loss": 14997.819519042969, "val_acc": 52.0}
{"epoch": 30, "training_loss": 55524.55261230469, "training_acc": 53.0, "val_loss": 3899.0482330322266, "val_acc": 52.0}
{"epoch": 31, "training_loss": 33351.389892578125, "training_acc": 43.0, "val_loss": 17335.777282714844, "val_acc": 48.0}
{"epoch": 32, "training_loss": 64399.255859375, "training_acc": 47.0, "val_loss": 736.8311405181885, "val_acc": 48.0}
{"epoch": 33, "training_loss": 21179.630126953125, "training_acc": 55.0, "val_loss": 26433.132934570312, "val_acc": 52.0}
{"epoch": 34, "training_loss": 107117.810546875, "training_acc": 53.0, "val_loss": 22877.171325683594, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69352.29956054688, "training_acc": 53.0, "val_loss": 7664.9505615234375, "val_acc": 48.0}
{"epoch": 36, "training_loss": 46713.80810546875, "training_acc": 47.0, "val_loss": 17789.852905273438, "val_acc": 48.0}
{"epoch": 37, "training_loss": 60505.15466308594, "training_acc": 47.0, "val_loss": 6158.548736572266, "val_acc": 52.0}
{"epoch": 38, "training_loss": 28441.3779296875, "training_acc": 53.0, "val_loss": 10599.356842041016, "val_acc": 52.0}
{"epoch": 39, "training_loss": 31312.98419189453, "training_acc": 53.0, "val_loss": 10871.255493164062, "val_acc": 48.0}
{"epoch": 40, "training_loss": 51633.540771484375, "training_acc": 47.0, "val_loss": 10969.168090820312, "val_acc": 48.0}
{"epoch": 41, "training_loss": 27194.754188537598, "training_acc": 55.0, "val_loss": 5011.787033081055, "val_acc": 52.0}
{"epoch": 42, "training_loss": 15342.65283203125, "training_acc": 53.0, "val_loss": 8763.731384277344, "val_acc": 48.0}
{"epoch": 43, "training_loss": 38176.94482421875, "training_acc": 47.0, "val_loss": 3478.0052185058594, "val_acc": 48.0}
{"epoch": 44, "training_loss": 28339.760498046875, "training_acc": 43.0, "val_loss": 15638.763427734375, "val_acc": 52.0}
{"epoch": 45, "training_loss": 57203.37268066406, "training_acc": 53.0, "val_loss": 3187.08438873291, "val_acc": 52.0}
{"epoch": 46, "training_loss": 29479.224853515625, "training_acc": 47.0, "val_loss": 18362.355041503906, "val_acc": 48.0}
{"epoch": 47, "training_loss": 69420.61303710938, "training_acc": 47.0, "val_loss": 2931.241798400879, "val_acc": 48.0}
{"epoch": 48, "training_loss": 28363.377685546875, "training_acc": 49.0, "val_loss": 23156.285095214844, "val_acc": 52.0}
{"epoch": 49, "training_loss": 90764.328125, "training_acc": 53.0, "val_loss": 17503.915405273438, "val_acc": 52.0}
{"epoch": 50, "training_loss": 52210.50207519531, "training_acc": 53.0, "val_loss": 13333.499145507812, "val_acc": 48.0}
{"epoch": 51, "training_loss": 67842.35888671875, "training_acc": 47.0, "val_loss": 21048.565673828125, "val_acc": 48.0}
