"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 122632.0736694336, "training_acc": 53.0, "val_loss": 24711.338806152344, "val_acc": 52.0}
{"epoch": 1, "training_loss": 110859.455078125, "training_acc": 53.0, "val_loss": 53082.763671875, "val_acc": 48.0}
{"epoch": 2, "training_loss": 200496.00732421875, "training_acc": 47.0, "val_loss": 15035.377502441406, "val_acc": 48.0}
{"epoch": 3, "training_loss": 74143.81274414062, "training_acc": 47.0, "val_loss": 38886.669921875, "val_acc": 52.0}
{"epoch": 4, "training_loss": 155307.7861328125, "training_acc": 53.0, "val_loss": 31800.802612304688, "val_acc": 52.0}
{"epoch": 5, "training_loss": 98646.53491210938, "training_acc": 53.0, "val_loss": 13215.631103515625, "val_acc": 48.0}
{"epoch": 6, "training_loss": 70137.07080078125, "training_acc": 47.0, "val_loss": 24534.783935546875, "val_acc": 48.0}
{"epoch": 7, "training_loss": 82830.16796875, "training_acc": 47.0, "val_loss": 6267.203140258789, "val_acc": 52.0}
{"epoch": 8, "training_loss": 37565.9111328125, "training_acc": 53.0, "val_loss": 13793.667602539062, "val_acc": 52.0}
{"epoch": 9, "training_loss": 41094.561462402344, "training_acc": 53.0, "val_loss": 12385.629272460938, "val_acc": 48.0}
{"epoch": 10, "training_loss": 59711.61279296875, "training_acc": 47.0, "val_loss": 13738.499450683594, "val_acc": 48.0}
{"epoch": 11, "training_loss": 37041.70094347, "training_acc": 47.0, "val_loss": 18517.64373779297, "val_acc": 52.0}
{"epoch": 12, "training_loss": 86385.2978515625, "training_acc": 53.0, "val_loss": 28023.9990234375, "val_acc": 52.0}
{"epoch": 13, "training_loss": 99543.47607421875, "training_acc": 53.0, "val_loss": 6961.58447265625, "val_acc": 52.0}
{"epoch": 14, "training_loss": 29889.347045898438, "training_acc": 59.0, "val_loss": 23961.575317382812, "val_acc": 48.0}
{"epoch": 15, "training_loss": 98136.95361328125, "training_acc": 47.0, "val_loss": 16666.928100585938, "val_acc": 48.0}
{"epoch": 16, "training_loss": 44989.24409484863, "training_acc": 47.0, "val_loss": 20122.74932861328, "val_acc": 52.0}
{"epoch": 17, "training_loss": 91439.8046875, "training_acc": 53.0, "val_loss": 34699.71008300781, "val_acc": 52.0}
{"epoch": 18, "training_loss": 130478.7978515625, "training_acc": 53.0, "val_loss": 20185.33935546875, "val_acc": 52.0}
{"epoch": 19, "training_loss": 51027.256591796875, "training_acc": 53.0, "val_loss": 20522.90802001953, "val_acc": 48.0}
{"epoch": 20, "training_loss": 109615.47705078125, "training_acc": 47.0, "val_loss": 37751.11389160156, "val_acc": 48.0}
{"epoch": 21, "training_loss": 144348.2666015625, "training_acc": 47.0, "val_loss": 17203.81622314453, "val_acc": 48.0}
{"epoch": 22, "training_loss": 59383.285217285156, "training_acc": 41.0, "val_loss": 15360.7177734375, "val_acc": 52.0}
{"epoch": 23, "training_loss": 64305.2275390625, "training_acc": 53.0, "val_loss": 12504.166412353516, "val_acc": 52.0}
{"epoch": 24, "training_loss": 32190.05168914795, "training_acc": 53.0, "val_loss": 18354.77294921875, "val_acc": 48.0}
{"epoch": 25, "training_loss": 95442.29052734375, "training_acc": 47.0, "val_loss": 26313.336181640625, "val_acc": 48.0}
{"epoch": 26, "training_loss": 91403.30786132812, "training_acc": 47.0, "val_loss": 1865.2372360229492, "val_acc": 52.0}
{"epoch": 27, "training_loss": 15874.487182617188, "training_acc": 53.0, "val_loss": 10100.031280517578, "val_acc": 52.0}
{"epoch": 28, "training_loss": 31980.342895507812, "training_acc": 53.0, "val_loss": 9188.648223876953, "val_acc": 48.0}
{"epoch": 29, "training_loss": 44131.0087890625, "training_acc": 47.0, "val_loss": 6187.810897827148, "val_acc": 48.0}
{"epoch": 30, "training_loss": 29940.068481445312, "training_acc": 45.0, "val_loss": 12730.779266357422, "val_acc": 52.0}
{"epoch": 31, "training_loss": 46159.813720703125, "training_acc": 53.0, "val_loss": 743.0156230926514, "val_acc": 52.0}
{"epoch": 32, "training_loss": 26682.237060546875, "training_acc": 49.0, "val_loss": 22419.14825439453, "val_acc": 48.0}
{"epoch": 33, "training_loss": 87371.423828125, "training_acc": 47.0, "val_loss": 8869.78530883789, "val_acc": 48.0}
{"epoch": 34, "training_loss": 35181.61511230469, "training_acc": 49.0, "val_loss": 16501.858520507812, "val_acc": 52.0}
{"epoch": 35, "training_loss": 65135.570556640625, "training_acc": 53.0, "val_loss": 10282.376861572266, "val_acc": 52.0}
{"epoch": 36, "training_loss": 31034.603424072266, "training_acc": 49.0, "val_loss": 6694.526672363281, "val_acc": 48.0}
{"epoch": 37, "training_loss": 20021.49803161621, "training_acc": 47.0, "val_loss": 10279.556274414062, "val_acc": 52.0}
{"epoch": 38, "training_loss": 45955.928466796875, "training_acc": 53.0, "val_loss": 10185.857391357422, "val_acc": 52.0}
{"epoch": 39, "training_loss": 24235.52504348755, "training_acc": 53.0, "val_loss": 17599.966430664062, "val_acc": 48.0}
{"epoch": 40, "training_loss": 82891.53662109375, "training_acc": 47.0, "val_loss": 23714.53857421875, "val_acc": 48.0}
{"epoch": 41, "training_loss": 82821.33911132812, "training_acc": 47.0, "val_loss": 2879.1879653930664, "val_acc": 52.0}
{"epoch": 42, "training_loss": 19569.605224609375, "training_acc": 53.0, "val_loss": 9380.955505371094, "val_acc": 52.0}
{"epoch": 43, "training_loss": 26127.546752929688, "training_acc": 53.0, "val_loss": 11497.705841064453, "val_acc": 48.0}
{"epoch": 44, "training_loss": 55910.4755859375, "training_acc": 47.0, "val_loss": 12891.2841796875, "val_acc": 48.0}
{"epoch": 45, "training_loss": 33702.47758483887, "training_acc": 47.0, "val_loss": 16965.640258789062, "val_acc": 52.0}
{"epoch": 46, "training_loss": 76581.734375, "training_acc": 53.0, "val_loss": 27786.984252929688, "val_acc": 52.0}
{"epoch": 47, "training_loss": 101708.51806640625, "training_acc": 53.0, "val_loss": 11567.571258544922, "val_acc": 52.0}
{"epoch": 48, "training_loss": 39886.25354003906, "training_acc": 49.0, "val_loss": 14759.226989746094, "val_acc": 48.0}
{"epoch": 49, "training_loss": 58688.408447265625, "training_acc": 47.0, "val_loss": 4529.681396484375, "val_acc": 48.0}
{"epoch": 50, "training_loss": 23045.160766601562, "training_acc": 55.0, "val_loss": 17991.539001464844, "val_acc": 52.0}
