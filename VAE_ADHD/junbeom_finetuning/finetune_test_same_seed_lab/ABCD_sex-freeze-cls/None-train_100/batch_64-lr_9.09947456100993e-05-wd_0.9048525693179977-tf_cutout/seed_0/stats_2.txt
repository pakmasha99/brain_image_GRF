"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.20165371894836, "training_acc": 53.0, "val_loss": 17.303091287612915, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.14393734931946, "training_acc": 53.0, "val_loss": 17.30327159166336, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.20737099647522, "training_acc": 53.0, "val_loss": 17.309533059597015, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.12657928466797, "training_acc": 53.0, "val_loss": 17.31429398059845, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.1276581287384, "training_acc": 53.0, "val_loss": 17.318610846996307, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.14446210861206, "training_acc": 53.0, "val_loss": 17.323803901672363, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.16290211677551, "training_acc": 53.0, "val_loss": 17.324937880039215, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.14468097686768, "training_acc": 53.0, "val_loss": 17.32805371284485, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.14081621170044, "training_acc": 53.0, "val_loss": 17.32613295316696, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.16903614997864, "training_acc": 53.0, "val_loss": 17.322024703025818, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.14453220367432, "training_acc": 53.0, "val_loss": 17.318420112133026, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.10629200935364, "training_acc": 53.0, "val_loss": 17.313694953918457, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.11653161048889, "training_acc": 53.0, "val_loss": 17.308400571346283, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.15356183052063, "training_acc": 53.0, "val_loss": 17.304816842079163, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.11584901809692, "training_acc": 53.0, "val_loss": 17.303867638111115, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.12489724159241, "training_acc": 53.0, "val_loss": 17.303241789340973, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.14120626449585, "training_acc": 53.0, "val_loss": 17.30305850505829, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.14992618560791, "training_acc": 53.0, "val_loss": 17.30313003063202, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.14796566963196, "training_acc": 53.0, "val_loss": 17.303165793418884, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.1731424331665, "training_acc": 53.0, "val_loss": 17.304079234600067, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.13564968109131, "training_acc": 53.0, "val_loss": 17.305049300193787, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.14347219467163, "training_acc": 53.0, "val_loss": 17.306262254714966, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.13710927963257, "training_acc": 53.0, "val_loss": 17.308354377746582, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.1625337600708, "training_acc": 53.0, "val_loss": 17.309477925300598, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.14723491668701, "training_acc": 53.0, "val_loss": 17.309334874153137, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.18653059005737, "training_acc": 53.0, "val_loss": 17.30937957763672, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.1388635635376, "training_acc": 53.0, "val_loss": 17.306552827358246, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.12364315986633, "training_acc": 53.0, "val_loss": 17.304101586341858, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.15610146522522, "training_acc": 53.0, "val_loss": 17.303091287612915, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.12275862693787, "training_acc": 53.0, "val_loss": 17.303282022476196, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.16363620758057, "training_acc": 53.0, "val_loss": 17.303530871868134, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.20901393890381, "training_acc": 53.0, "val_loss": 17.303022742271423, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.15922474861145, "training_acc": 53.0, "val_loss": 17.30351150035858, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.18860363960266, "training_acc": 53.0, "val_loss": 17.305469512939453, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.15644764900208, "training_acc": 53.0, "val_loss": 17.306137084960938, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.14207029342651, "training_acc": 53.0, "val_loss": 17.305926978588104, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.19980812072754, "training_acc": 53.0, "val_loss": 17.305544018745422, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.13039183616638, "training_acc": 53.0, "val_loss": 17.308439314365387, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.12981343269348, "training_acc": 53.0, "val_loss": 17.31192171573639, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.15734100341797, "training_acc": 53.0, "val_loss": 17.316876351833344, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.11773014068604, "training_acc": 53.0, "val_loss": 17.323946952819824, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.18330097198486, "training_acc": 53.0, "val_loss": 17.333392798900604, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.16560244560242, "training_acc": 53.0, "val_loss": 17.33717918395996, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.19142961502075, "training_acc": 53.0, "val_loss": 17.336130142211914, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.1698489189148, "training_acc": 53.0, "val_loss": 17.33362525701523, "val_acc": 52.0}
{"epoch": 45, "training_loss": 69.17220640182495, "training_acc": 53.0, "val_loss": 17.329157888889313, "val_acc": 52.0}
{"epoch": 46, "training_loss": 69.17419528961182, "training_acc": 53.0, "val_loss": 17.32379049062729, "val_acc": 52.0}
{"epoch": 47, "training_loss": 69.15960812568665, "training_acc": 53.0, "val_loss": 17.31841266155243, "val_acc": 52.0}
{"epoch": 48, "training_loss": 69.12436604499817, "training_acc": 53.0, "val_loss": 17.314498126506805, "val_acc": 52.0}
{"epoch": 49, "training_loss": 69.13122510910034, "training_acc": 53.0, "val_loss": 17.310598492622375, "val_acc": 52.0}
{"epoch": 50, "training_loss": 69.11921191215515, "training_acc": 53.0, "val_loss": 17.307376861572266, "val_acc": 52.0}
