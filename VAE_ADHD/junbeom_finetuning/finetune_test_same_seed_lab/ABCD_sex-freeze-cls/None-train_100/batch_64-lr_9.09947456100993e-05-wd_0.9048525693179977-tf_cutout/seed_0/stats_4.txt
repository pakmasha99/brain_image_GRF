"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.15155458450317, "training_acc": 53.0, "val_loss": 17.320547997951508, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.16119623184204, "training_acc": 53.0, "val_loss": 17.321132123470306, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.19705200195312, "training_acc": 53.0, "val_loss": 17.320053279399872, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.14730143547058, "training_acc": 53.0, "val_loss": 17.325185239315033, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.15696811676025, "training_acc": 53.0, "val_loss": 17.325463891029358, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.13073468208313, "training_acc": 53.0, "val_loss": 17.32195019721985, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.14268517494202, "training_acc": 53.0, "val_loss": 17.318275570869446, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.14319920539856, "training_acc": 53.0, "val_loss": 17.315299808979034, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.1258270740509, "training_acc": 53.0, "val_loss": 17.314156889915466, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.14662647247314, "training_acc": 53.0, "val_loss": 17.314201593399048, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.16175484657288, "training_acc": 53.0, "val_loss": 17.315368354320526, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.14390921592712, "training_acc": 53.0, "val_loss": 17.314454913139343, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.14404773712158, "training_acc": 53.0, "val_loss": 17.31455624103546, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.15560245513916, "training_acc": 53.0, "val_loss": 17.315541207790375, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.17130994796753, "training_acc": 53.0, "val_loss": 17.315693199634552, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.17527437210083, "training_acc": 53.0, "val_loss": 17.319394648075104, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.13140916824341, "training_acc": 53.0, "val_loss": 17.319580912590027, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.15916514396667, "training_acc": 53.0, "val_loss": 17.317725718021393, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.12531542778015, "training_acc": 53.0, "val_loss": 17.315274477005005, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.13478541374207, "training_acc": 53.0, "val_loss": 17.31206476688385, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.12127661705017, "training_acc": 53.0, "val_loss": 17.31095463037491, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.1948938369751, "training_acc": 53.0, "val_loss": 17.31082946062088, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.16864490509033, "training_acc": 53.0, "val_loss": 17.31051206588745, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.1664764881134, "training_acc": 53.0, "val_loss": 17.31073707342148, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.1758337020874, "training_acc": 53.0, "val_loss": 17.310652136802673, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.18338370323181, "training_acc": 53.0, "val_loss": 17.31059104204178, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.16166663169861, "training_acc": 53.0, "val_loss": 17.310704290866852, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.14506506919861, "training_acc": 53.0, "val_loss": 17.312683165073395, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.1653254032135, "training_acc": 53.0, "val_loss": 17.315183579921722, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.13914823532104, "training_acc": 53.0, "val_loss": 17.316879332065582, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.16532897949219, "training_acc": 53.0, "val_loss": 17.31734275817871, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.17005205154419, "training_acc": 53.0, "val_loss": 17.31591373682022, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.12204480171204, "training_acc": 53.0, "val_loss": 17.312806844711304, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.20450687408447, "training_acc": 53.0, "val_loss": 17.311111092567444, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.14486885070801, "training_acc": 53.0, "val_loss": 17.31105148792267, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.15592932701111, "training_acc": 53.0, "val_loss": 17.31075644493103, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.17286372184753, "training_acc": 53.0, "val_loss": 17.3109769821167, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.12942123413086, "training_acc": 53.0, "val_loss": 17.310921847820282, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.13493871688843, "training_acc": 53.0, "val_loss": 17.31126308441162, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.14560008049011, "training_acc": 53.0, "val_loss": 17.311999201774597, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.15274500846863, "training_acc": 53.0, "val_loss": 17.313016951084137, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.13600564002991, "training_acc": 53.0, "val_loss": 17.312929034233093, "val_acc": 52.0}
