"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 14448621.907173157, "training_acc": 51.0, "val_loss": 3180955.46875, "val_acc": 52.0}
{"epoch": 1, "training_loss": 15770966.125, "training_acc": 55.0, "val_loss": 8686267.96875, "val_acc": 48.0}
{"epoch": 2, "training_loss": 32503209.125, "training_acc": 47.0, "val_loss": 2207033.7890625, "val_acc": 48.0}
{"epoch": 3, "training_loss": 9870392.96875, "training_acc": 53.0, "val_loss": 6422860.9375, "val_acc": 52.0}
{"epoch": 4, "training_loss": 26222677.25, "training_acc": 53.0, "val_loss": 5725059.765625, "val_acc": 52.0}
{"epoch": 5, "training_loss": 18465033.375, "training_acc": 53.0, "val_loss": 1105536.328125, "val_acc": 48.0}
{"epoch": 6, "training_loss": 8479781.5625, "training_acc": 47.0, "val_loss": 2922489.2578125, "val_acc": 48.0}
{"epoch": 7, "training_loss": 9381088.234375, "training_acc": 47.0, "val_loss": 2106854.296875, "val_acc": 52.0}
{"epoch": 8, "training_loss": 10074448.25, "training_acc": 53.0, "val_loss": 3278286.71875, "val_acc": 52.0}
{"epoch": 9, "training_loss": 11230297.328125, "training_acc": 53.0, "val_loss": 507884.66796875, "val_acc": 48.0}
{"epoch": 10, "training_loss": 3101136.96875, "training_acc": 47.0, "val_loss": 409751.171875, "val_acc": 48.0}
{"epoch": 11, "training_loss": 3313659.453125, "training_acc": 49.0, "val_loss": 2183161.9140625, "val_acc": 52.0}
{"epoch": 12, "training_loss": 7720449.65625, "training_acc": 53.0, "val_loss": 72772.49145507812, "val_acc": 48.0}
{"epoch": 13, "training_loss": 828623.8828125, "training_acc": 47.0, "val_loss": 634829.345703125, "val_acc": 52.0}
{"epoch": 14, "training_loss": 2147102.44921875, "training_acc": 53.0, "val_loss": 889151.07421875, "val_acc": 48.0}
{"epoch": 15, "training_loss": 3292545.640625, "training_acc": 47.0, "val_loss": 750169.23828125, "val_acc": 52.0}
{"epoch": 16, "training_loss": 3193568.359375, "training_acc": 53.0, "val_loss": 72998.53515625, "val_acc": 48.0}
{"epoch": 17, "training_loss": 552399.32421875, "training_acc": 49.0, "val_loss": 571485.83984375, "val_acc": 48.0}
{"epoch": 18, "training_loss": 1540674.6020507812, "training_acc": 47.0, "val_loss": 1606593.06640625, "val_acc": 52.0}
{"epoch": 19, "training_loss": 6798049.875, "training_acc": 53.0, "val_loss": 1332172.265625, "val_acc": 52.0}
{"epoch": 20, "training_loss": 4014933.50390625, "training_acc": 49.0, "val_loss": 655540.966796875, "val_acc": 48.0}
{"epoch": 21, "training_loss": 2078038.375, "training_acc": 51.0, "val_loss": 142896.1669921875, "val_acc": 52.0}
{"epoch": 22, "training_loss": 1727148.765625, "training_acc": 55.0, "val_loss": 1023886.62109375, "val_acc": 48.0}
{"epoch": 23, "training_loss": 2918421.8359375, "training_acc": 51.0, "val_loss": 315615.9912109375, "val_acc": 52.0}
{"epoch": 24, "training_loss": 1976256.1875, "training_acc": 49.0, "val_loss": 400556.5185546875, "val_acc": 48.0}
{"epoch": 25, "training_loss": 2609022.9375, "training_acc": 49.0, "val_loss": 1378720.1171875, "val_acc": 52.0}
{"epoch": 26, "training_loss": 4393671.38671875, "training_acc": 53.0, "val_loss": 1287977.24609375, "val_acc": 48.0}
{"epoch": 27, "training_loss": 5505225.71875, "training_acc": 47.0, "val_loss": 613852.44140625, "val_acc": 48.0}
{"epoch": 28, "training_loss": 3925718.015625, "training_acc": 45.0, "val_loss": 2013448.6328125, "val_acc": 52.0}
{"epoch": 29, "training_loss": 7220674.875, "training_acc": 53.0, "val_loss": 53452.84423828125, "val_acc": 52.0}
{"epoch": 30, "training_loss": 2909474.421875, "training_acc": 59.0, "val_loss": 3658177.34375, "val_acc": 48.0}
{"epoch": 31, "training_loss": 14721434.375, "training_acc": 47.0, "val_loss": 2089641.796875, "val_acc": 48.0}
{"epoch": 32, "training_loss": 5845333.31640625, "training_acc": 51.0, "val_loss": 1528888.18359375, "val_acc": 52.0}
{"epoch": 33, "training_loss": 6334436.875, "training_acc": 53.0, "val_loss": 465291.845703125, "val_acc": 52.0}
{"epoch": 34, "training_loss": 3839064.03125, "training_acc": 51.0, "val_loss": 2671314.84375, "val_acc": 48.0}
{"epoch": 35, "training_loss": 10181898.09375, "training_acc": 47.0, "val_loss": 452787.79296875, "val_acc": 48.0}
{"epoch": 36, "training_loss": 4604629.9375, "training_acc": 47.0, "val_loss": 3569502.734375, "val_acc": 52.0}
{"epoch": 37, "training_loss": 13959395.4375, "training_acc": 53.0, "val_loss": 2802132.2265625, "val_acc": 52.0}
{"epoch": 38, "training_loss": 8866736.734375, "training_acc": 53.0, "val_loss": 1612695.99609375, "val_acc": 48.0}
{"epoch": 39, "training_loss": 7985315.4375, "training_acc": 47.0, "val_loss": 2540181.640625, "val_acc": 48.0}
{"epoch": 40, "training_loss": 8429271.5, "training_acc": 47.0, "val_loss": 1276425.1953125, "val_acc": 52.0}
{"epoch": 41, "training_loss": 6702485.5, "training_acc": 53.0, "val_loss": 2161396.875, "val_acc": 52.0}
{"epoch": 42, "training_loss": 6597332.046875, "training_acc": 53.0, "val_loss": 1237326.7578125, "val_acc": 48.0}
{"epoch": 43, "training_loss": 5737252.859375, "training_acc": 47.0, "val_loss": 1698614.84375, "val_acc": 48.0}
{"epoch": 44, "training_loss": 4840140.697265625, "training_acc": 47.0, "val_loss": 2168348.046875, "val_acc": 52.0}
{"epoch": 45, "training_loss": 9402469.40625, "training_acc": 53.0, "val_loss": 3174151.171875, "val_acc": 52.0}
{"epoch": 46, "training_loss": 11025168.4375, "training_acc": 53.0, "val_loss": 354740.4052734375, "val_acc": 52.0}
{"epoch": 47, "training_loss": 5655512.78125, "training_acc": 45.0, "val_loss": 4146257.8125, "val_acc": 48.0}
{"epoch": 48, "training_loss": 16744942.1875, "training_acc": 47.0, "val_loss": 2852934.9609375, "val_acc": 48.0}
