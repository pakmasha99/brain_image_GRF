"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 1711270.6735153198, "training_acc": 47.0, "val_loss": 593404.98046875, "val_acc": 52.0}
{"epoch": 1, "training_loss": 1812792.67578125, "training_acc": 51.0, "val_loss": 364592.0654296875, "val_acc": 48.0}
{"epoch": 2, "training_loss": 1168100.2470703125, "training_acc": 47.0, "val_loss": 342709.423828125, "val_acc": 52.0}
{"epoch": 3, "training_loss": 1531436.109375, "training_acc": 53.0, "val_loss": 352306.3720703125, "val_acc": 52.0}
{"epoch": 4, "training_loss": 937218.0341796875, "training_acc": 53.0, "val_loss": 368313.623046875, "val_acc": 48.0}
{"epoch": 5, "training_loss": 1720543.0859375, "training_acc": 47.0, "val_loss": 531887.841796875, "val_acc": 48.0}
{"epoch": 6, "training_loss": 1895212.8125, "training_acc": 47.0, "val_loss": 7989.6148681640625, "val_acc": 52.0}
{"epoch": 7, "training_loss": 190533.517578125, "training_acc": 53.0, "val_loss": 108509.92431640625, "val_acc": 52.0}
{"epoch": 8, "training_loss": 358044.5048828125, "training_acc": 49.0, "val_loss": 32656.536865234375, "val_acc": 48.0}
{"epoch": 9, "training_loss": 246630.935546875, "training_acc": 51.0, "val_loss": 141946.3623046875, "val_acc": 52.0}
{"epoch": 10, "training_loss": 398873.84375, "training_acc": 53.0, "val_loss": 198267.85888671875, "val_acc": 48.0}
{"epoch": 11, "training_loss": 902897.26953125, "training_acc": 47.0, "val_loss": 184586.68212890625, "val_acc": 48.0}
{"epoch": 12, "training_loss": 523721.8837890625, "training_acc": 49.0, "val_loss": 93677.50854492188, "val_acc": 52.0}
{"epoch": 13, "training_loss": 281844.84619140625, "training_acc": 53.0, "val_loss": 147384.3994140625, "val_acc": 48.0}
{"epoch": 14, "training_loss": 629136.345703125, "training_acc": 47.0, "val_loss": 60181.53076171875, "val_acc": 48.0}
{"epoch": 15, "training_loss": 278911.732421875, "training_acc": 59.0, "val_loss": 245832.177734375, "val_acc": 52.0}
{"epoch": 16, "training_loss": 923584.921875, "training_acc": 53.0, "val_loss": 87326.57470703125, "val_acc": 52.0}
{"epoch": 17, "training_loss": 411488.65234375, "training_acc": 53.0, "val_loss": 232661.279296875, "val_acc": 48.0}
{"epoch": 18, "training_loss": 855404.69140625, "training_acc": 47.0, "val_loss": 21394.70672607422, "val_acc": 52.0}
{"epoch": 19, "training_loss": 145280.21875, "training_acc": 53.0, "val_loss": 32794.49157714844, "val_acc": 48.0}
{"epoch": 20, "training_loss": 82446.80477905273, "training_acc": 55.0, "val_loss": 64406.951904296875, "val_acc": 48.0}
{"epoch": 21, "training_loss": 162357.283203125, "training_acc": 53.0, "val_loss": 42580.17883300781, "val_acc": 48.0}
{"epoch": 22, "training_loss": 179778.51025390625, "training_acc": 49.0, "val_loss": 15645.146179199219, "val_acc": 52.0}
{"epoch": 23, "training_loss": 273380.92578125, "training_acc": 47.0, "val_loss": 141505.16357421875, "val_acc": 48.0}
{"epoch": 24, "training_loss": 396921.3108520508, "training_acc": 45.0, "val_loss": 2074.3648529052734, "val_acc": 52.0}
{"epoch": 25, "training_loss": 94736.51611328125, "training_acc": 63.0, "val_loss": 65287.85400390625, "val_acc": 48.0}
{"epoch": 26, "training_loss": 268218.4560546875, "training_acc": 51.0, "val_loss": 97971.53930664062, "val_acc": 52.0}
{"epoch": 27, "training_loss": 250079.0291748047, "training_acc": 53.0, "val_loss": 207666.064453125, "val_acc": 48.0}
{"epoch": 28, "training_loss": 901878.51171875, "training_acc": 47.0, "val_loss": 175717.3095703125, "val_acc": 48.0}
{"epoch": 29, "training_loss": 475590.3215332031, "training_acc": 51.0, "val_loss": 87395.9716796875, "val_acc": 52.0}
{"epoch": 30, "training_loss": 263126.6359863281, "training_acc": 53.0, "val_loss": 139083.837890625, "val_acc": 48.0}
{"epoch": 31, "training_loss": 599540.9140625, "training_acc": 47.0, "val_loss": 32195.758056640625, "val_acc": 48.0}
{"epoch": 32, "training_loss": 368249.4140625, "training_acc": 49.0, "val_loss": 297965.4052734375, "val_acc": 52.0}
{"epoch": 33, "training_loss": 1139060.09765625, "training_acc": 53.0, "val_loss": 145202.4658203125, "val_acc": 52.0}
{"epoch": 34, "training_loss": 449367.8876953125, "training_acc": 55.0, "val_loss": 183597.6318359375, "val_acc": 48.0}
{"epoch": 35, "training_loss": 684154.474609375, "training_acc": 47.0, "val_loss": 36656.01501464844, "val_acc": 52.0}
{"epoch": 36, "training_loss": 213185.4111328125, "training_acc": 53.0, "val_loss": 12253.189086914062, "val_acc": 52.0}
{"epoch": 37, "training_loss": 288179.45703125, "training_acc": 51.0, "val_loss": 216125.341796875, "val_acc": 48.0}
{"epoch": 38, "training_loss": 724736.283203125, "training_acc": 47.0, "val_loss": 99819.00634765625, "val_acc": 52.0}
{"epoch": 39, "training_loss": 508868.833984375, "training_acc": 53.0, "val_loss": 150703.77197265625, "val_acc": 52.0}
{"epoch": 40, "training_loss": 404878.99768066406, "training_acc": 53.0, "val_loss": 246885.986328125, "val_acc": 48.0}
{"epoch": 41, "training_loss": 1131727.05078125, "training_acc": 47.0, "val_loss": 284208.7646484375, "val_acc": 48.0}
{"epoch": 42, "training_loss": 935724.4248046875, "training_acc": 47.0, "val_loss": 181389.78271484375, "val_acc": 52.0}
{"epoch": 43, "training_loss": 866686.4375, "training_acc": 53.0, "val_loss": 302273.6572265625, "val_acc": 52.0}
