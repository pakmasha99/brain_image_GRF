"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 2676139.8766212463, "training_acc": 47.0, "val_loss": 479469.775390625, "val_acc": 48.0}
{"epoch": 1, "training_loss": 2427557.390625, "training_acc": 49.0, "val_loss": 1301083.30078125, "val_acc": 52.0}
{"epoch": 2, "training_loss": 4938422.9921875, "training_acc": 53.0, "val_loss": 868870.60546875, "val_acc": 52.0}
{"epoch": 3, "training_loss": 2467853.12890625, "training_acc": 53.0, "val_loss": 464549.755859375, "val_acc": 48.0}
{"epoch": 4, "training_loss": 2398912.5625, "training_acc": 47.0, "val_loss": 861353.02734375, "val_acc": 48.0}
{"epoch": 5, "training_loss": 3068063.390625, "training_acc": 47.0, "val_loss": 163797.119140625, "val_acc": 48.0}
{"epoch": 6, "training_loss": 1050877.5625, "training_acc": 49.0, "val_loss": 786643.359375, "val_acc": 52.0}
{"epoch": 7, "training_loss": 3199165.125, "training_acc": 53.0, "val_loss": 810963.037109375, "val_acc": 52.0}
{"epoch": 8, "training_loss": 2761689.6640625, "training_acc": 53.0, "val_loss": 162230.77392578125, "val_acc": 52.0}
{"epoch": 9, "training_loss": 1030156.2421875, "training_acc": 49.0, "val_loss": 704789.74609375, "val_acc": 48.0}
{"epoch": 10, "training_loss": 2958074.890625, "training_acc": 47.0, "val_loss": 628090.087890625, "val_acc": 48.0}
{"epoch": 11, "training_loss": 2055575.42578125, "training_acc": 47.0, "val_loss": 178945.751953125, "val_acc": 52.0}
{"epoch": 12, "training_loss": 1059275.2109375, "training_acc": 53.0, "val_loss": 481862.255859375, "val_acc": 52.0}
{"epoch": 13, "training_loss": 1721736.1328125, "training_acc": 53.0, "val_loss": 129857.6904296875, "val_acc": 52.0}
{"epoch": 14, "training_loss": 702279.89453125, "training_acc": 51.0, "val_loss": 444229.8828125, "val_acc": 48.0}
{"epoch": 15, "training_loss": 1745822.8125, "training_acc": 47.0, "val_loss": 213196.09375, "val_acc": 48.0}
{"epoch": 16, "training_loss": 719730.103515625, "training_acc": 51.0, "val_loss": 302755.615234375, "val_acc": 52.0}
{"epoch": 17, "training_loss": 1158313.80078125, "training_acc": 53.0, "val_loss": 146049.853515625, "val_acc": 52.0}
{"epoch": 18, "training_loss": 513028.134765625, "training_acc": 55.0, "val_loss": 239920.9716796875, "val_acc": 48.0}
{"epoch": 19, "training_loss": 833092.6328125, "training_acc": 47.0, "val_loss": 102891.0888671875, "val_acc": 52.0}
{"epoch": 20, "training_loss": 486959.84765625, "training_acc": 53.0, "val_loss": 92606.201171875, "val_acc": 52.0}
{"epoch": 21, "training_loss": 424966.74609375, "training_acc": 51.0, "val_loss": 167833.1787109375, "val_acc": 48.0}
{"epoch": 22, "training_loss": 443122.978515625, "training_acc": 47.0, "val_loss": 273751.0009765625, "val_acc": 52.0}
{"epoch": 23, "training_loss": 1299643.8046875, "training_acc": 53.0, "val_loss": 375486.865234375, "val_acc": 52.0}
{"epoch": 24, "training_loss": 1179034.44921875, "training_acc": 53.0, "val_loss": 132574.560546875, "val_acc": 48.0}
{"epoch": 25, "training_loss": 689835.5, "training_acc": 47.0, "val_loss": 207759.1552734375, "val_acc": 48.0}
{"epoch": 26, "training_loss": 651772.5966796875, "training_acc": 41.0, "val_loss": 52328.961181640625, "val_acc": 52.0}
{"epoch": 27, "training_loss": 262970.4638671875, "training_acc": 49.0, "val_loss": 27316.357421875, "val_acc": 48.0}
{"epoch": 28, "training_loss": 306873.908203125, "training_acc": 51.0, "val_loss": 230665.673828125, "val_acc": 52.0}
{"epoch": 29, "training_loss": 757282.45703125, "training_acc": 53.0, "val_loss": 109443.505859375, "val_acc": 48.0}
{"epoch": 30, "training_loss": 550644.1875, "training_acc": 47.0, "val_loss": 79242.31567382812, "val_acc": 48.0}
{"epoch": 31, "training_loss": 398079.263671875, "training_acc": 53.0, "val_loss": 258743.6279296875, "val_acc": 52.0}
{"epoch": 32, "training_loss": 913334.91015625, "training_acc": 53.0, "val_loss": 5566.030120849609, "val_acc": 48.0}
{"epoch": 33, "training_loss": 111412.4580078125, "training_acc": 47.0, "val_loss": 111230.96923828125, "val_acc": 52.0}
{"epoch": 34, "training_loss": 416070.3193359375, "training_acc": 53.0, "val_loss": 34719.622802734375, "val_acc": 48.0}
{"epoch": 35, "training_loss": 130590.62426757812, "training_acc": 45.0, "val_loss": 97448.5595703125, "val_acc": 48.0}
{"epoch": 36, "training_loss": 291091.43408203125, "training_acc": 47.0, "val_loss": 188187.9638671875, "val_acc": 52.0}
{"epoch": 37, "training_loss": 812307.8671875, "training_acc": 53.0, "val_loss": 126843.29833984375, "val_acc": 52.0}
{"epoch": 38, "training_loss": 396929.9228515625, "training_acc": 59.0, "val_loss": 195451.904296875, "val_acc": 48.0}
{"epoch": 39, "training_loss": 653523.0532226562, "training_acc": 47.0, "val_loss": 162607.14111328125, "val_acc": 52.0}
{"epoch": 40, "training_loss": 716197.61328125, "training_acc": 53.0, "val_loss": 120793.7255859375, "val_acc": 52.0}
{"epoch": 41, "training_loss": 618195.7578125, "training_acc": 41.0, "val_loss": 179527.84423828125, "val_acc": 48.0}
{"epoch": 42, "training_loss": 519608.87463378906, "training_acc": 47.0, "val_loss": 259645.01953125, "val_acc": 52.0}
{"epoch": 43, "training_loss": 1102897.1015625, "training_acc": 53.0, "val_loss": 307277.44140625, "val_acc": 52.0}
{"epoch": 44, "training_loss": 917627.5546875, "training_acc": 53.0, "val_loss": 193266.8701171875, "val_acc": 48.0}
{"epoch": 45, "training_loss": 1006831.58203125, "training_acc": 47.0, "val_loss": 276449.609375, "val_acc": 48.0}
{"epoch": 46, "training_loss": 761706.5932617188, "training_acc": 47.0, "val_loss": 313207.470703125, "val_acc": 52.0}
{"epoch": 47, "training_loss": 1464462.296875, "training_acc": 53.0, "val_loss": 507193.359375, "val_acc": 52.0}
{"epoch": 48, "training_loss": 1771161.75, "training_acc": 53.0, "val_loss": 118982.8857421875, "val_acc": 52.0}
{"epoch": 49, "training_loss": 640696.703125, "training_acc": 55.0, "val_loss": 508735.05859375, "val_acc": 48.0}
{"epoch": 50, "training_loss": 2102089.4765625, "training_acc": 47.0, "val_loss": 374676.513671875, "val_acc": 48.0}
{"epoch": 51, "training_loss": 1043639.2294921875, "training_acc": 47.0, "val_loss": 394420.8740234375, "val_acc": 52.0}
