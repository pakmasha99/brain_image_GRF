"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.24310612678528, "training_acc": 53.0, "val_loss": 17.347678542137146, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.18491315841675, "training_acc": 53.0, "val_loss": 17.34345704317093, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.16701221466064, "training_acc": 53.0, "val_loss": 17.338772118091583, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.20016598701477, "training_acc": 53.0, "val_loss": 17.33434796333313, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.21071147918701, "training_acc": 53.0, "val_loss": 17.330382764339447, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.1511697769165, "training_acc": 53.0, "val_loss": 17.328141629695892, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.12656998634338, "training_acc": 53.0, "val_loss": 17.325885593891144, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.15632677078247, "training_acc": 53.0, "val_loss": 17.323440313339233, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.15011954307556, "training_acc": 53.0, "val_loss": 17.321211099624634, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.16293668746948, "training_acc": 53.0, "val_loss": 17.31947660446167, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.16084289550781, "training_acc": 53.0, "val_loss": 17.31817275285721, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.10316467285156, "training_acc": 53.0, "val_loss": 17.317716777324677, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.13148808479309, "training_acc": 53.0, "val_loss": 17.31743961572647, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.14300274848938, "training_acc": 53.0, "val_loss": 17.316661775112152, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.11738181114197, "training_acc": 53.0, "val_loss": 17.316411435604095, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.12706327438354, "training_acc": 53.0, "val_loss": 17.316138744354248, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.12997603416443, "training_acc": 53.0, "val_loss": 17.315883934497833, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.14259767532349, "training_acc": 53.0, "val_loss": 17.315301299095154, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.12186574935913, "training_acc": 53.0, "val_loss": 17.31487810611725, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.13533473014832, "training_acc": 53.0, "val_loss": 17.314599454402924, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.14155960083008, "training_acc": 53.0, "val_loss": 17.31421947479248, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.13866758346558, "training_acc": 53.0, "val_loss": 17.314067482948303, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.15266156196594, "training_acc": 53.0, "val_loss": 17.314116656780243, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.11262488365173, "training_acc": 53.0, "val_loss": 17.314034700393677, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.10602140426636, "training_acc": 53.0, "val_loss": 17.31373518705368, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.15580081939697, "training_acc": 53.0, "val_loss": 17.313465476036072, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.14081931114197, "training_acc": 53.0, "val_loss": 17.313207685947418, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.11691331863403, "training_acc": 53.0, "val_loss": 17.31317490339279, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.12517857551575, "training_acc": 53.0, "val_loss": 17.31308400630951, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.11921739578247, "training_acc": 53.0, "val_loss": 17.313377559185028, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.09305310249329, "training_acc": 53.0, "val_loss": 17.314256727695465, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.12249398231506, "training_acc": 53.0, "val_loss": 17.315101623535156, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.09762239456177, "training_acc": 53.0, "val_loss": 17.315536737442017, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.09037327766418, "training_acc": 53.0, "val_loss": 17.31618046760559, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.16161775588989, "training_acc": 53.0, "val_loss": 17.316517233848572, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.12114071846008, "training_acc": 53.0, "val_loss": 17.317049205303192, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.1269941329956, "training_acc": 53.0, "val_loss": 17.317645251750946, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.11579179763794, "training_acc": 53.0, "val_loss": 17.31814593076706, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.14226508140564, "training_acc": 53.0, "val_loss": 17.318862676620483, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.1408920288086, "training_acc": 53.0, "val_loss": 17.31906831264496, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.13429546356201, "training_acc": 53.0, "val_loss": 17.31923073530197, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.13767552375793, "training_acc": 53.0, "val_loss": 17.31950491666794, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.14251399040222, "training_acc": 53.0, "val_loss": 17.319568991661072, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.16570997238159, "training_acc": 53.0, "val_loss": 17.31991171836853, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.1288366317749, "training_acc": 53.0, "val_loss": 17.31991171836853, "val_acc": 52.0}
{"epoch": 45, "training_loss": 69.13861107826233, "training_acc": 53.0, "val_loss": 17.319759726524353, "val_acc": 52.0}
{"epoch": 46, "training_loss": 69.13754200935364, "training_acc": 53.0, "val_loss": 17.3196479678154, "val_acc": 52.0}
{"epoch": 47, "training_loss": 69.13532662391663, "training_acc": 53.0, "val_loss": 17.319293320178986, "val_acc": 52.0}
