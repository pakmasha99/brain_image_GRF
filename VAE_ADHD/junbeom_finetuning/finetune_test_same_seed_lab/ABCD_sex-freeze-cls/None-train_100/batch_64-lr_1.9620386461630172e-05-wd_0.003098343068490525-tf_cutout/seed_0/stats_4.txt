"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 70.69473385810852, "training_acc": 47.0, "val_loss": 17.589071393013, "val_acc": 52.0}
{"epoch": 1, "training_loss": 70.50859308242798, "training_acc": 47.0, "val_loss": 17.56264716386795, "val_acc": 52.0}
{"epoch": 2, "training_loss": 70.39982914924622, "training_acc": 47.0, "val_loss": 17.53634661436081, "val_acc": 52.0}
{"epoch": 3, "training_loss": 70.30110669136047, "training_acc": 47.0, "val_loss": 17.511029541492462, "val_acc": 52.0}
{"epoch": 4, "training_loss": 70.17898559570312, "training_acc": 47.0, "val_loss": 17.487354576587677, "val_acc": 52.0}
{"epoch": 5, "training_loss": 70.07417702674866, "training_acc": 47.0, "val_loss": 17.465098202228546, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.97075581550598, "training_acc": 47.0, "val_loss": 17.444613575935364, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.85989737510681, "training_acc": 47.0, "val_loss": 17.425939440727234, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.77269291877747, "training_acc": 47.0, "val_loss": 17.408829927444458, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.71725940704346, "training_acc": 47.0, "val_loss": 17.39387810230255, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.65172266960144, "training_acc": 47.0, "val_loss": 17.3816978931427, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.59520864486694, "training_acc": 47.0, "val_loss": 17.37065315246582, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.53998565673828, "training_acc": 47.0, "val_loss": 17.36082285642624, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.46666979789734, "training_acc": 47.0, "val_loss": 17.35263168811798, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.43808102607727, "training_acc": 47.0, "val_loss": 17.34592914581299, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.37837934494019, "training_acc": 47.0, "val_loss": 17.33989268541336, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.34901857376099, "training_acc": 47.0, "val_loss": 17.33459234237671, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.29341125488281, "training_acc": 55.0, "val_loss": 17.329876124858856, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.29693245887756, "training_acc": 50.0, "val_loss": 17.32545495033264, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.30493879318237, "training_acc": 53.0, "val_loss": 17.321880161762238, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.29234218597412, "training_acc": 53.0, "val_loss": 17.319229245185852, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.22616529464722, "training_acc": 53.0, "val_loss": 17.31727123260498, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.2132568359375, "training_acc": 53.0, "val_loss": 17.315728962421417, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.22818684577942, "training_acc": 53.0, "val_loss": 17.31443703174591, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.20260715484619, "training_acc": 53.0, "val_loss": 17.313596606254578, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.16548466682434, "training_acc": 53.0, "val_loss": 17.31303781270981, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.18951869010925, "training_acc": 53.0, "val_loss": 17.31259673833847, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.15994620323181, "training_acc": 53.0, "val_loss": 17.312440276145935, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.13313221931458, "training_acc": 53.0, "val_loss": 17.31242835521698, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.13185048103333, "training_acc": 53.0, "val_loss": 17.3125758767128, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.15479278564453, "training_acc": 53.0, "val_loss": 17.312854528427124, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.16776585578918, "training_acc": 53.0, "val_loss": 17.31322705745697, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.12716221809387, "training_acc": 53.0, "val_loss": 17.31356382369995, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.12061643600464, "training_acc": 53.0, "val_loss": 17.313949763774872, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.17386269569397, "training_acc": 53.0, "val_loss": 17.314498126506805, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.12453126907349, "training_acc": 53.0, "val_loss": 17.314982414245605, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.12690281867981, "training_acc": 53.0, "val_loss": 17.315374314785004, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.11521363258362, "training_acc": 53.0, "val_loss": 17.315734922885895, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.14438939094543, "training_acc": 53.0, "val_loss": 17.316055297851562, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.13133382797241, "training_acc": 53.0, "val_loss": 17.316412925720215, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.15108156204224, "training_acc": 53.0, "val_loss": 17.316803336143494, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.14179062843323, "training_acc": 53.0, "val_loss": 17.317233979701996, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.14848446846008, "training_acc": 53.0, "val_loss": 17.317406833171844, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.15187096595764, "training_acc": 53.0, "val_loss": 17.317628860473633, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.14208602905273, "training_acc": 53.0, "val_loss": 17.31790155172348, "val_acc": 52.0}
{"epoch": 45, "training_loss": 69.16100668907166, "training_acc": 53.0, "val_loss": 17.318081855773926, "val_acc": 52.0}
{"epoch": 46, "training_loss": 69.12118148803711, "training_acc": 53.0, "val_loss": 17.318643629550934, "val_acc": 52.0}
{"epoch": 47, "training_loss": 69.13730239868164, "training_acc": 53.0, "val_loss": 17.318789660930634, "val_acc": 52.0}
