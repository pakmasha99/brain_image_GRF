"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 70.38880133628845, "training_acc": 47.0, "val_loss": 17.47119128704071, "val_acc": 52.0}
{"epoch": 1, "training_loss": 70.12419486045837, "training_acc": 47.0, "val_loss": 17.40700453519821, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.72821354866028, "training_acc": 47.0, "val_loss": 17.366519570350647, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.48626637458801, "training_acc": 47.0, "val_loss": 17.338721454143524, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.460364818573, "training_acc": 49.0, "val_loss": 17.31846332550049, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.24661993980408, "training_acc": 53.0, "val_loss": 17.308448255062103, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.15333986282349, "training_acc": 53.0, "val_loss": 17.303569614887238, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.16688275337219, "training_acc": 53.0, "val_loss": 17.30298548936844, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.12431120872498, "training_acc": 53.0, "val_loss": 17.306533455848694, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.17043781280518, "training_acc": 53.0, "val_loss": 17.313572764396667, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.16758012771606, "training_acc": 53.0, "val_loss": 17.321588099002838, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.15175652503967, "training_acc": 53.0, "val_loss": 17.328697443008423, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.18354797363281, "training_acc": 53.0, "val_loss": 17.334558069705963, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.16966271400452, "training_acc": 53.0, "val_loss": 17.337597906589508, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.18926787376404, "training_acc": 53.0, "val_loss": 17.339327931404114, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.22038626670837, "training_acc": 53.0, "val_loss": 17.341934144496918, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.21773219108582, "training_acc": 53.0, "val_loss": 17.341384291648865, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.20830941200256, "training_acc": 53.0, "val_loss": 17.341218888759613, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.20036363601685, "training_acc": 53.0, "val_loss": 17.337575554847717, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.20881962776184, "training_acc": 53.0, "val_loss": 17.32916533946991, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.17716526985168, "training_acc": 53.0, "val_loss": 17.322459816932678, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.22522449493408, "training_acc": 53.0, "val_loss": 17.316322028636932, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.16019892692566, "training_acc": 53.0, "val_loss": 17.314325273036957, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.18386554718018, "training_acc": 53.0, "val_loss": 17.312385141849518, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.14748525619507, "training_acc": 53.0, "val_loss": 17.31094717979431, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.15386891365051, "training_acc": 53.0, "val_loss": 17.310847342014313, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.12988638877869, "training_acc": 53.0, "val_loss": 17.30947196483612, "val_acc": 52.0}
