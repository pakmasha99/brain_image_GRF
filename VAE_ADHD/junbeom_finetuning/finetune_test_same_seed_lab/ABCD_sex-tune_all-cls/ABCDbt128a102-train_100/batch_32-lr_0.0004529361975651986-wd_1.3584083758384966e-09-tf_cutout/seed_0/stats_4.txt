"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 71.85091257095337, "training_acc": 53.0, "val_loss": 44.15098428726196, "val_acc": 48.0}
{"epoch": 1, "training_loss": 120.17348003387451, "training_acc": 53.0, "val_loss": 24.87073391675949, "val_acc": 52.0}
{"epoch": 2, "training_loss": 74.9359016418457, "training_acc": 53.0, "val_loss": 17.31610894203186, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.6652603149414, "training_acc": 49.0, "val_loss": 18.059737980365753, "val_acc": 52.0}
{"epoch": 4, "training_loss": 72.459951877594, "training_acc": 53.0, "val_loss": 17.3281729221344, "val_acc": 52.0}
{"epoch": 5, "training_loss": 71.05511450767517, "training_acc": 48.0, "val_loss": 17.364177107810974, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.9290509223938, "training_acc": 39.0, "val_loss": 17.615778744220734, "val_acc": 52.0}
{"epoch": 7, "training_loss": 71.20681405067444, "training_acc": 47.0, "val_loss": 17.43367463350296, "val_acc": 52.0}
{"epoch": 8, "training_loss": 71.34898066520691, "training_acc": 41.0, "val_loss": 17.32557713985443, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.47494602203369, "training_acc": 49.0, "val_loss": 17.340564727783203, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.16969060897827, "training_acc": 53.0, "val_loss": 17.340900003910065, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.03897857666016, "training_acc": 53.0, "val_loss": 17.518487572669983, "val_acc": 52.0}
{"epoch": 12, "training_loss": 71.70721101760864, "training_acc": 47.0, "val_loss": 17.361833155155182, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.38039302825928, "training_acc": 51.0, "val_loss": 17.36106127500534, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.036292552948, "training_acc": 53.0, "val_loss": 17.33744740486145, "val_acc": 52.0}
{"epoch": 15, "training_loss": 68.78009510040283, "training_acc": 57.0, "val_loss": 17.537832260131836, "val_acc": 52.0}
{"epoch": 16, "training_loss": 70.37174868583679, "training_acc": 53.0, "val_loss": 17.77857542037964, "val_acc": 52.0}
{"epoch": 17, "training_loss": 70.44514346122742, "training_acc": 53.0, "val_loss": 17.309226095676422, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.44233703613281, "training_acc": 51.0, "val_loss": 17.608542740345, "val_acc": 52.0}
{"epoch": 19, "training_loss": 70.65081191062927, "training_acc": 45.0, "val_loss": 17.35714226961136, "val_acc": 52.0}
{"epoch": 20, "training_loss": 70.25601863861084, "training_acc": 53.0, "val_loss": 17.55048781633377, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.57996582984924, "training_acc": 53.0, "val_loss": 17.315585911273956, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.15365815162659, "training_acc": 53.0, "val_loss": 17.365533113479614, "val_acc": 52.0}
{"epoch": 23, "training_loss": 70.35035443305969, "training_acc": 47.0, "val_loss": 17.30784922838211, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.45249223709106, "training_acc": 53.0, "val_loss": 17.724932730197906, "val_acc": 52.0}
{"epoch": 25, "training_loss": 70.75261116027832, "training_acc": 53.0, "val_loss": 17.86813735961914, "val_acc": 52.0}
{"epoch": 26, "training_loss": 71.69825601577759, "training_acc": 53.0, "val_loss": 17.90015399456024, "val_acc": 52.0}
{"epoch": 27, "training_loss": 70.57518815994263, "training_acc": 53.0, "val_loss": 17.371664941310883, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.24510335922241, "training_acc": 53.0, "val_loss": 17.3359677195549, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.3935694694519, "training_acc": 47.0, "val_loss": 17.32303947210312, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.98515748977661, "training_acc": 53.0, "val_loss": 17.40581840276718, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.63810443878174, "training_acc": 53.0, "val_loss": 17.566867172718048, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.92211723327637, "training_acc": 53.0, "val_loss": 17.530183494091034, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.8703498840332, "training_acc": 53.0, "val_loss": 17.474661767482758, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.42583227157593, "training_acc": 53.0, "val_loss": 17.333179712295532, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.41041660308838, "training_acc": 53.0, "val_loss": 17.400966584682465, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.51880192756653, "training_acc": 53.0, "val_loss": 17.494291067123413, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.58889555931091, "training_acc": 53.0, "val_loss": 17.32766628265381, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.29744911193848, "training_acc": 53.0, "val_loss": 17.361703515052795, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.46796226501465, "training_acc": 47.0, "val_loss": 17.418548464775085, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.70917510986328, "training_acc": 47.0, "val_loss": 17.364059388637543, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.7004714012146, "training_acc": 47.0, "val_loss": 17.402561008930206, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.74853730201721, "training_acc": 47.0, "val_loss": 17.372986674308777, "val_acc": 52.0}
