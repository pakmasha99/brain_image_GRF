"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 75.54986572265625, "training_acc": 49.0, "val_loss": 20.52631676197052, "val_acc": 56.0}
{"epoch": 1, "training_loss": 86.9518768787384, "training_acc": 52.0, "val_loss": 26.78695321083069, "val_acc": 44.0}
{"epoch": 2, "training_loss": 95.6534514427185, "training_acc": 58.0, "val_loss": 17.40071028470993, "val_acc": 56.0}
{"epoch": 3, "training_loss": 69.81623578071594, "training_acc": 54.0, "val_loss": 20.098918676376343, "val_acc": 44.0}
{"epoch": 4, "training_loss": 75.86787223815918, "training_acc": 48.0, "val_loss": 17.118871212005615, "val_acc": 56.0}
{"epoch": 5, "training_loss": 71.82011842727661, "training_acc": 52.0, "val_loss": 17.182771861553192, "val_acc": 56.0}
{"epoch": 6, "training_loss": 70.98281669616699, "training_acc": 46.0, "val_loss": 17.189185321331024, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.2652759552002, "training_acc": 52.0, "val_loss": 17.178721725940704, "val_acc": 56.0}
{"epoch": 8, "training_loss": 70.1341450214386, "training_acc": 52.0, "val_loss": 17.49272346496582, "val_acc": 56.0}
{"epoch": 9, "training_loss": 71.13201642036438, "training_acc": 48.0, "val_loss": 17.920441925525665, "val_acc": 56.0}
{"epoch": 10, "training_loss": 71.18193578720093, "training_acc": 48.0, "val_loss": 17.768996953964233, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.39302182197571, "training_acc": 48.0, "val_loss": 17.192812263965607, "val_acc": 56.0}
{"epoch": 12, "training_loss": 72.84348821640015, "training_acc": 52.0, "val_loss": 17.5530806183815, "val_acc": 56.0}
{"epoch": 13, "training_loss": 72.00083518028259, "training_acc": 52.0, "val_loss": 17.162151634693146, "val_acc": 56.0}
{"epoch": 14, "training_loss": 69.89362859725952, "training_acc": 52.0, "val_loss": 17.167411744594574, "val_acc": 56.0}
{"epoch": 15, "training_loss": 69.60379314422607, "training_acc": 48.0, "val_loss": 17.263057827949524, "val_acc": 56.0}
{"epoch": 16, "training_loss": 69.64368653297424, "training_acc": 52.0, "val_loss": 17.147983610630035, "val_acc": 56.0}
{"epoch": 17, "training_loss": 69.33900308609009, "training_acc": 52.0, "val_loss": 17.25192219018936, "val_acc": 56.0}
{"epoch": 18, "training_loss": 69.17235326766968, "training_acc": 50.0, "val_loss": 17.566314339637756, "val_acc": 56.0}
{"epoch": 19, "training_loss": 70.03514385223389, "training_acc": 48.0, "val_loss": 17.69118309020996, "val_acc": 56.0}
{"epoch": 20, "training_loss": 69.64653778076172, "training_acc": 48.0, "val_loss": 17.30579286813736, "val_acc": 56.0}
{"epoch": 21, "training_loss": 69.17369961738586, "training_acc": 52.0, "val_loss": 17.19214916229248, "val_acc": 56.0}
{"epoch": 22, "training_loss": 69.4872522354126, "training_acc": 52.0, "val_loss": 17.187680304050446, "val_acc": 56.0}
{"epoch": 23, "training_loss": 69.22666525840759, "training_acc": 52.0, "val_loss": 17.25296378135681, "val_acc": 56.0}
