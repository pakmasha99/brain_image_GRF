"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 73.74615716934204, "training_acc": 52.0, "val_loss": 66.19516015052795, "val_acc": 52.0}
{"epoch": 1, "training_loss": 144.77615070343018, "training_acc": 47.0, "val_loss": 17.36396551132202, "val_acc": 52.0}
{"epoch": 2, "training_loss": 73.61452031135559, "training_acc": 53.0, "val_loss": 17.905807495117188, "val_acc": 52.0}
{"epoch": 3, "training_loss": 71.2841489315033, "training_acc": 51.0, "val_loss": 17.650337517261505, "val_acc": 52.0}
{"epoch": 4, "training_loss": 77.67264032363892, "training_acc": 47.0, "val_loss": 17.465972900390625, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.55449628829956, "training_acc": 48.0, "val_loss": 17.373713850975037, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.92522883415222, "training_acc": 53.0, "val_loss": 17.41209328174591, "val_acc": 52.0}
{"epoch": 7, "training_loss": 70.94376039505005, "training_acc": 53.0, "val_loss": 17.411276698112488, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.04941654205322, "training_acc": 57.0, "val_loss": 17.709243297576904, "val_acc": 52.0}
{"epoch": 9, "training_loss": 74.33724117279053, "training_acc": 47.0, "val_loss": 17.845460772514343, "val_acc": 52.0}
{"epoch": 10, "training_loss": 70.37469387054443, "training_acc": 47.0, "val_loss": 17.349116504192352, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.2714114189148, "training_acc": 53.0, "val_loss": 17.42366999387741, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.41486740112305, "training_acc": 53.0, "val_loss": 17.33376234769821, "val_acc": 52.0}
{"epoch": 13, "training_loss": 70.58806467056274, "training_acc": 45.0, "val_loss": 17.327262461185455, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.16276669502258, "training_acc": 52.0, "val_loss": 17.380698025226593, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.13760566711426, "training_acc": 53.0, "val_loss": 17.31998771429062, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.52389407157898, "training_acc": 45.0, "val_loss": 17.31347292661667, "val_acc": 52.0}
{"epoch": 17, "training_loss": 68.99330711364746, "training_acc": 53.0, "val_loss": 17.356909811496735, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.13556623458862, "training_acc": 53.0, "val_loss": 17.30859726667404, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.16726565361023, "training_acc": 53.0, "val_loss": 17.30792224407196, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.2332010269165, "training_acc": 53.0, "val_loss": 17.33202636241913, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.16823983192444, "training_acc": 53.0, "val_loss": 17.334410548210144, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.22053909301758, "training_acc": 53.0, "val_loss": 17.324866354465485, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.21539258956909, "training_acc": 53.0, "val_loss": 17.36634373664856, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.43209457397461, "training_acc": 53.0, "val_loss": 17.363090813159943, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.13596868515015, "training_acc": 53.0, "val_loss": 17.469733953475952, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.57154941558838, "training_acc": 53.0, "val_loss": 17.443351447582245, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.34531784057617, "training_acc": 53.0, "val_loss": 17.33342707157135, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.32531571388245, "training_acc": 53.0, "val_loss": 17.332853376865387, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.43858599662781, "training_acc": 47.0, "val_loss": 17.347480356693268, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.29023218154907, "training_acc": 47.0, "val_loss": 17.311914265155792, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.14705872535706, "training_acc": 53.0, "val_loss": 17.31368452310562, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.16641211509705, "training_acc": 53.0, "val_loss": 17.31029450893402, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.43869495391846, "training_acc": 51.0, "val_loss": 17.32870638370514, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.32164812088013, "training_acc": 53.0, "val_loss": 17.310649156570435, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.1562659740448, "training_acc": 53.0, "val_loss": 17.311595380306244, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.20411229133606, "training_acc": 53.0, "val_loss": 17.360956966876984, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.25699782371521, "training_acc": 53.0, "val_loss": 17.392604053020477, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.32522225379944, "training_acc": 53.0, "val_loss": 17.403626441955566, "val_acc": 52.0}
