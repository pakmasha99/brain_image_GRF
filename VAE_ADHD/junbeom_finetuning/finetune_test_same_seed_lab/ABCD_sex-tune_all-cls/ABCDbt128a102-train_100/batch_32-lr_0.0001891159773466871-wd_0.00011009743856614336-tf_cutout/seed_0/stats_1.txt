"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 70.45745468139648, "training_acc": 42.0, "val_loss": 17.343558371067047, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.41827201843262, "training_acc": 53.0, "val_loss": 17.31906533241272, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.48517537117004, "training_acc": 45.0, "val_loss": 17.374178767204285, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.62193727493286, "training_acc": 47.0, "val_loss": 17.34369546175003, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.38055419921875, "training_acc": 53.0, "val_loss": 17.317458987236023, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.6211519241333, "training_acc": 53.0, "val_loss": 17.35556423664093, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.15513753890991, "training_acc": 53.0, "val_loss": 17.32250154018402, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.2702169418335, "training_acc": 53.0, "val_loss": 17.31623113155365, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.1334228515625, "training_acc": 53.0, "val_loss": 17.31175035238266, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.33134889602661, "training_acc": 53.0, "val_loss": 17.312175035476685, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.13772320747375, "training_acc": 53.0, "val_loss": 17.311465740203857, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.32604169845581, "training_acc": 53.0, "val_loss": 17.30927973985672, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.14768934249878, "training_acc": 53.0, "val_loss": 17.31606125831604, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.45362687110901, "training_acc": 53.0, "val_loss": 17.30918437242508, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.22263097763062, "training_acc": 53.0, "val_loss": 17.310582101345062, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.17259573936462, "training_acc": 53.0, "val_loss": 17.32962429523468, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.43390727043152, "training_acc": 43.0, "val_loss": 17.33628213405609, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.44794130325317, "training_acc": 47.0, "val_loss": 17.311452329158783, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.2019374370575, "training_acc": 53.0, "val_loss": 17.311424016952515, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.08064246177673, "training_acc": 53.0, "val_loss": 17.33998954296112, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.18956351280212, "training_acc": 53.0, "val_loss": 17.362216114997864, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.2187819480896, "training_acc": 53.0, "val_loss": 17.42016226053238, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.50231742858887, "training_acc": 53.0, "val_loss": 17.47044175863266, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.513680934906, "training_acc": 53.0, "val_loss": 17.357371747493744, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.20554494857788, "training_acc": 53.0, "val_loss": 17.31126755475998, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.2297613620758, "training_acc": 53.0, "val_loss": 17.32545793056488, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.34817147254944, "training_acc": 45.0, "val_loss": 17.313456535339355, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.12606644630432, "training_acc": 53.0, "val_loss": 17.312273383140564, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.12992191314697, "training_acc": 53.0, "val_loss": 17.30906218290329, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.26045846939087, "training_acc": 53.0, "val_loss": 17.308959364891052, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.21705484390259, "training_acc": 53.0, "val_loss": 17.336644232273102, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.33825588226318, "training_acc": 53.0, "val_loss": 17.334069311618805, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.2588791847229, "training_acc": 53.0, "val_loss": 17.31080561876297, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.28189206123352, "training_acc": 53.0, "val_loss": 17.313018441200256, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.13766741752625, "training_acc": 53.0, "val_loss": 17.32049584388733, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.30728960037231, "training_acc": 53.0, "val_loss": 17.387035489082336, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.31853413581848, "training_acc": 53.0, "val_loss": 17.325012385845184, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.18637752532959, "training_acc": 53.0, "val_loss": 17.322298884391785, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.16989803314209, "training_acc": 53.0, "val_loss": 17.318598926067352, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.15977954864502, "training_acc": 53.0, "val_loss": 17.314301431179047, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.29687285423279, "training_acc": 53.0, "val_loss": 17.329658567905426, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.42224788665771, "training_acc": 53.0, "val_loss": 17.309360206127167, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.21423101425171, "training_acc": 53.0, "val_loss": 17.313101887702942, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.21612048149109, "training_acc": 53.0, "val_loss": 17.31073409318924, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.18902730941772, "training_acc": 53.0, "val_loss": 17.308688163757324, "val_acc": 52.0}
{"epoch": 45, "training_loss": 69.21451759338379, "training_acc": 53.0, "val_loss": 17.30881631374359, "val_acc": 52.0}
{"epoch": 46, "training_loss": 69.14995861053467, "training_acc": 53.0, "val_loss": 17.327140271663666, "val_acc": 52.0}
{"epoch": 47, "training_loss": 69.1763277053833, "training_acc": 53.0, "val_loss": 17.318224906921387, "val_acc": 52.0}
{"epoch": 48, "training_loss": 69.3221664428711, "training_acc": 53.0, "val_loss": 17.33049899339676, "val_acc": 52.0}
{"epoch": 49, "training_loss": 69.1670401096344, "training_acc": 53.0, "val_loss": 17.311695218086243, "val_acc": 52.0}
{"epoch": 50, "training_loss": 69.273766040802, "training_acc": 51.0, "val_loss": 17.405109107494354, "val_acc": 52.0}
{"epoch": 51, "training_loss": 69.94925498962402, "training_acc": 47.0, "val_loss": 17.400437593460083, "val_acc": 52.0}
{"epoch": 52, "training_loss": 69.58208560943604, "training_acc": 45.0, "val_loss": 17.31194257736206, "val_acc": 52.0}
{"epoch": 53, "training_loss": 69.21224761009216, "training_acc": 53.0, "val_loss": 17.31417626142502, "val_acc": 52.0}
{"epoch": 54, "training_loss": 69.27960228919983, "training_acc": 51.0, "val_loss": 17.351096868515015, "val_acc": 52.0}
{"epoch": 55, "training_loss": 69.44675064086914, "training_acc": 47.0, "val_loss": 17.348620295524597, "val_acc": 52.0}
{"epoch": 56, "training_loss": 69.4142599105835, "training_acc": 47.0, "val_loss": 17.336708307266235, "val_acc": 52.0}
{"epoch": 57, "training_loss": 69.3726634979248, "training_acc": 47.0, "val_loss": 17.334292829036713, "val_acc": 52.0}
{"epoch": 58, "training_loss": 69.4399242401123, "training_acc": 47.0, "val_loss": 17.31959879398346, "val_acc": 52.0}
{"epoch": 59, "training_loss": 69.09476470947266, "training_acc": 53.0, "val_loss": 17.32456535100937, "val_acc": 52.0}
{"epoch": 60, "training_loss": 69.0727150440216, "training_acc": 53.0, "val_loss": 17.42546111345291, "val_acc": 52.0}
{"epoch": 61, "training_loss": 69.41060614585876, "training_acc": 53.0, "val_loss": 17.514297366142273, "val_acc": 52.0}
{"epoch": 62, "training_loss": 69.70829057693481, "training_acc": 53.0, "val_loss": 17.46748238801956, "val_acc": 52.0}
{"epoch": 63, "training_loss": 69.4621934890747, "training_acc": 53.0, "val_loss": 17.36990064382553, "val_acc": 52.0}
