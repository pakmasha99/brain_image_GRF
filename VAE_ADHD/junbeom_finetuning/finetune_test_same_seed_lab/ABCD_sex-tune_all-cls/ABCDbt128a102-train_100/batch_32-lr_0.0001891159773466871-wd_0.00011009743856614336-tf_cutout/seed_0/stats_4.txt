"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.5750150680542, "training_acc": 53.0, "val_loss": 17.329242825508118, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.2087652683258, "training_acc": 53.0, "val_loss": 17.316745221614838, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.21849727630615, "training_acc": 53.0, "val_loss": 17.364543676376343, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.24915218353271, "training_acc": 53.0, "val_loss": 17.467336356639862, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.8539023399353, "training_acc": 53.0, "val_loss": 17.48308539390564, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.52810335159302, "training_acc": 53.0, "val_loss": 17.3956498503685, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.42956352233887, "training_acc": 53.0, "val_loss": 17.36191064119339, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.39014959335327, "training_acc": 53.0, "val_loss": 17.332030832767487, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.15014004707336, "training_acc": 53.0, "val_loss": 17.309869825839996, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.18160033226013, "training_acc": 53.0, "val_loss": 17.34994649887085, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.63086605072021, "training_acc": 41.0, "val_loss": 17.342713475227356, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.51853823661804, "training_acc": 47.0, "val_loss": 17.354530096054077, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.32518243789673, "training_acc": 51.0, "val_loss": 17.310789227485657, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.34829950332642, "training_acc": 53.0, "val_loss": 17.324058711528778, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.55757665634155, "training_acc": 47.0, "val_loss": 17.359532415866852, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.34006023406982, "training_acc": 53.0, "val_loss": 17.3115074634552, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.20226669311523, "training_acc": 53.0, "val_loss": 17.309218645095825, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.06731176376343, "training_acc": 53.0, "val_loss": 17.34037697315216, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.19939494132996, "training_acc": 53.0, "val_loss": 17.444273829460144, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.6402645111084, "training_acc": 53.0, "val_loss": 17.47598648071289, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.36846256256104, "training_acc": 53.0, "val_loss": 17.34008491039276, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.08406209945679, "training_acc": 53.0, "val_loss": 17.31320023536682, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.23130369186401, "training_acc": 53.0, "val_loss": 17.319101095199585, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.26860761642456, "training_acc": 53.0, "val_loss": 17.31695532798767, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.12917017936707, "training_acc": 53.0, "val_loss": 17.309388518333435, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.4586615562439, "training_acc": 53.0, "val_loss": 17.308799922466278, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.21082639694214, "training_acc": 53.0, "val_loss": 17.397300899028778, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.38774299621582, "training_acc": 53.0, "val_loss": 17.562220990657806, "val_acc": 52.0}
{"epoch": 28, "training_loss": 70.4107403755188, "training_acc": 53.0, "val_loss": 17.741423845291138, "val_acc": 52.0}
{"epoch": 29, "training_loss": 70.40179395675659, "training_acc": 53.0, "val_loss": 17.54133552312851, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.65001630783081, "training_acc": 53.0, "val_loss": 17.36511141061783, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.17544293403625, "training_acc": 53.0, "val_loss": 17.317992448806763, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.357013463974, "training_acc": 53.0, "val_loss": 17.342017590999603, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.2493348121643, "training_acc": 53.0, "val_loss": 17.38351732492447, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.288893699646, "training_acc": 53.0, "val_loss": 17.425082623958588, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.55417656898499, "training_acc": 53.0, "val_loss": 17.466457188129425, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.5257625579834, "training_acc": 53.0, "val_loss": 17.411501705646515, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.52538633346558, "training_acc": 53.0, "val_loss": 17.44816154241562, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.58353114128113, "training_acc": 53.0, "val_loss": 17.476320266723633, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.54424977302551, "training_acc": 53.0, "val_loss": 17.33921617269516, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.25162649154663, "training_acc": 53.0, "val_loss": 17.32255518436432, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.18558931350708, "training_acc": 57.0, "val_loss": 17.370600998401642, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.54848527908325, "training_acc": 47.0, "val_loss": 17.37177222967148, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.7658281326294, "training_acc": 47.0, "val_loss": 17.42950528860092, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.90485954284668, "training_acc": 47.0, "val_loss": 17.400914430618286, "val_acc": 52.0}
