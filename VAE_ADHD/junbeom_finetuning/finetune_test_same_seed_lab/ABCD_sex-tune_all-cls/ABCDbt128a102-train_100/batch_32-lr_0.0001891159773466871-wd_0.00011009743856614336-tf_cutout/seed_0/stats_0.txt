"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.2599139213562, "training_acc": 51.0, "val_loss": 17.279911041259766, "val_acc": 56.0}
{"epoch": 1, "training_loss": 69.34688305854797, "training_acc": 52.0, "val_loss": 17.245973646640778, "val_acc": 56.0}
{"epoch": 2, "training_loss": 69.58503770828247, "training_acc": 43.0, "val_loss": 17.17759519815445, "val_acc": 56.0}
{"epoch": 3, "training_loss": 69.4180588722229, "training_acc": 52.0, "val_loss": 17.187151312828064, "val_acc": 56.0}
{"epoch": 4, "training_loss": 69.35167217254639, "training_acc": 52.0, "val_loss": 17.195267975330353, "val_acc": 56.0}
{"epoch": 5, "training_loss": 69.39532804489136, "training_acc": 52.0, "val_loss": 17.191722989082336, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.2972731590271, "training_acc": 52.0, "val_loss": 17.222250998020172, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.36551356315613, "training_acc": 52.0, "val_loss": 17.248588800430298, "val_acc": 56.0}
{"epoch": 8, "training_loss": 69.509108543396, "training_acc": 52.0, "val_loss": 17.230544984340668, "val_acc": 56.0}
{"epoch": 9, "training_loss": 69.46395564079285, "training_acc": 52.0, "val_loss": 17.35498458147049, "val_acc": 56.0}
{"epoch": 10, "training_loss": 69.45856142044067, "training_acc": 48.0, "val_loss": 17.442676424980164, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.4153459072113, "training_acc": 48.0, "val_loss": 17.236460745334625, "val_acc": 56.0}
{"epoch": 12, "training_loss": 69.72249603271484, "training_acc": 52.0, "val_loss": 17.153796553611755, "val_acc": 56.0}
{"epoch": 13, "training_loss": 69.48889994621277, "training_acc": 52.0, "val_loss": 17.1532079577446, "val_acc": 56.0}
{"epoch": 14, "training_loss": 69.48401975631714, "training_acc": 52.0, "val_loss": 17.1516090631485, "val_acc": 56.0}
{"epoch": 15, "training_loss": 69.57126116752625, "training_acc": 52.0, "val_loss": 17.15240180492401, "val_acc": 56.0}
{"epoch": 16, "training_loss": 69.78044748306274, "training_acc": 52.0, "val_loss": 17.152397334575653, "val_acc": 56.0}
{"epoch": 17, "training_loss": 69.4343478679657, "training_acc": 52.0, "val_loss": 17.216424643993378, "val_acc": 56.0}
{"epoch": 18, "training_loss": 69.15081977844238, "training_acc": 54.0, "val_loss": 17.452318966388702, "val_acc": 56.0}
{"epoch": 19, "training_loss": 69.72205066680908, "training_acc": 48.0, "val_loss": 17.532263696193695, "val_acc": 56.0}
{"epoch": 20, "training_loss": 69.56226754188538, "training_acc": 48.0, "val_loss": 17.300771176815033, "val_acc": 56.0}
{"epoch": 21, "training_loss": 69.22675013542175, "training_acc": 52.0, "val_loss": 17.217916250228882, "val_acc": 56.0}
{"epoch": 22, "training_loss": 69.41845297813416, "training_acc": 52.0, "val_loss": 17.219872772693634, "val_acc": 56.0}
{"epoch": 23, "training_loss": 69.2147798538208, "training_acc": 51.0, "val_loss": 17.2470360994339, "val_acc": 56.0}
{"epoch": 24, "training_loss": 69.57191896438599, "training_acc": 52.0, "val_loss": 17.190250754356384, "val_acc": 56.0}
{"epoch": 25, "training_loss": 69.26568603515625, "training_acc": 52.0, "val_loss": 17.258180677890778, "val_acc": 56.0}
{"epoch": 26, "training_loss": 69.4587390422821, "training_acc": 52.0, "val_loss": 17.24116802215576, "val_acc": 56.0}
{"epoch": 27, "training_loss": 69.3000111579895, "training_acc": 52.0, "val_loss": 17.251479625701904, "val_acc": 56.0}
{"epoch": 28, "training_loss": 69.31970572471619, "training_acc": 52.0, "val_loss": 17.21932590007782, "val_acc": 56.0}
{"epoch": 29, "training_loss": 69.32024002075195, "training_acc": 52.0, "val_loss": 17.14736521244049, "val_acc": 56.0}
{"epoch": 30, "training_loss": 70.11582040786743, "training_acc": 52.0, "val_loss": 17.193754017353058, "val_acc": 56.0}
{"epoch": 31, "training_loss": 70.2794303894043, "training_acc": 52.0, "val_loss": 17.18626767396927, "val_acc": 56.0}
{"epoch": 32, "training_loss": 70.36639308929443, "training_acc": 52.0, "val_loss": 17.224279046058655, "val_acc": 56.0}
{"epoch": 33, "training_loss": 70.37102007865906, "training_acc": 52.0, "val_loss": 17.153285443782806, "val_acc": 56.0}
{"epoch": 34, "training_loss": 69.25733613967896, "training_acc": 52.0, "val_loss": 17.267657816410065, "val_acc": 56.0}
{"epoch": 35, "training_loss": 69.43687438964844, "training_acc": 51.0, "val_loss": 17.407913506031036, "val_acc": 56.0}
{"epoch": 36, "training_loss": 69.30705308914185, "training_acc": 48.0, "val_loss": 17.231671512126923, "val_acc": 56.0}
{"epoch": 37, "training_loss": 69.1579372882843, "training_acc": 52.0, "val_loss": 17.16129332780838, "val_acc": 56.0}
{"epoch": 38, "training_loss": 69.43979978561401, "training_acc": 52.0, "val_loss": 17.200912535190582, "val_acc": 56.0}
{"epoch": 39, "training_loss": 69.13039898872375, "training_acc": 52.0, "val_loss": 17.358005046844482, "val_acc": 56.0}
{"epoch": 40, "training_loss": 69.5121340751648, "training_acc": 52.0, "val_loss": 17.533273994922638, "val_acc": 56.0}
{"epoch": 41, "training_loss": 69.53639078140259, "training_acc": 49.0, "val_loss": 17.25039780139923, "val_acc": 56.0}
{"epoch": 42, "training_loss": 69.22981834411621, "training_acc": 52.0, "val_loss": 17.29111671447754, "val_acc": 56.0}
{"epoch": 43, "training_loss": 69.36209392547607, "training_acc": 52.0, "val_loss": 17.328917980194092, "val_acc": 56.0}
{"epoch": 44, "training_loss": 69.48617935180664, "training_acc": 50.0, "val_loss": 17.453910410404205, "val_acc": 56.0}
{"epoch": 45, "training_loss": 69.52032041549683, "training_acc": 48.0, "val_loss": 17.38918274641037, "val_acc": 56.0}
{"epoch": 46, "training_loss": 69.37299251556396, "training_acc": 50.0, "val_loss": 17.27011799812317, "val_acc": 56.0}
{"epoch": 47, "training_loss": 69.20973300933838, "training_acc": 52.0, "val_loss": 17.176347970962524, "val_acc": 56.0}
{"epoch": 48, "training_loss": 69.37024354934692, "training_acc": 52.0, "val_loss": 17.17126965522766, "val_acc": 56.0}
