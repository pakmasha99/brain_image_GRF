"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 71.69334888458252, "training_acc": 48.0, "val_loss": 18.75484585762024, "val_acc": 52.0}
{"epoch": 1, "training_loss": 74.4997501373291, "training_acc": 43.0, "val_loss": 18.558529019355774, "val_acc": 48.0}
{"epoch": 2, "training_loss": 73.24523758888245, "training_acc": 47.0, "val_loss": 18.524131178855896, "val_acc": 52.0}
{"epoch": 3, "training_loss": 72.16949796676636, "training_acc": 45.0, "val_loss": 18.39529573917389, "val_acc": 56.0}
{"epoch": 4, "training_loss": 71.66655111312866, "training_acc": 48.0, "val_loss": 18.3845654129982, "val_acc": 56.0}
{"epoch": 5, "training_loss": 70.65832543373108, "training_acc": 49.0, "val_loss": 18.3611661195755, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.91203308105469, "training_acc": 53.0, "val_loss": 18.210473656654358, "val_acc": 56.0}
{"epoch": 7, "training_loss": 68.63555002212524, "training_acc": 52.0, "val_loss": 18.112267553806305, "val_acc": 56.0}
{"epoch": 8, "training_loss": 68.33750581741333, "training_acc": 54.0, "val_loss": 18.049076199531555, "val_acc": 56.0}
{"epoch": 9, "training_loss": 68.37461805343628, "training_acc": 55.0, "val_loss": 18.00575852394104, "val_acc": 56.0}
{"epoch": 10, "training_loss": 67.83700227737427, "training_acc": 54.0, "val_loss": 17.9653137922287, "val_acc": 56.0}
{"epoch": 11, "training_loss": 67.38575649261475, "training_acc": 55.0, "val_loss": 17.834120988845825, "val_acc": 56.0}
{"epoch": 12, "training_loss": 67.08923721313477, "training_acc": 60.0, "val_loss": 17.753085494041443, "val_acc": 56.0}
{"epoch": 13, "training_loss": 67.16487765312195, "training_acc": 56.0, "val_loss": 17.82742589712143, "val_acc": 56.0}
{"epoch": 14, "training_loss": 67.0199065208435, "training_acc": 58.0, "val_loss": 18.00924688577652, "val_acc": 56.0}
{"epoch": 15, "training_loss": 67.32667303085327, "training_acc": 59.0, "val_loss": 17.883481085300446, "val_acc": 52.0}
{"epoch": 16, "training_loss": 66.3073182106018, "training_acc": 61.0, "val_loss": 17.5333172082901, "val_acc": 52.0}
{"epoch": 17, "training_loss": 67.05313777923584, "training_acc": 61.0, "val_loss": 17.500288784503937, "val_acc": 52.0}
{"epoch": 18, "training_loss": 66.49318194389343, "training_acc": 62.0, "val_loss": 17.38404482603073, "val_acc": 56.0}
{"epoch": 19, "training_loss": 66.23353862762451, "training_acc": 59.0, "val_loss": 17.484360933303833, "val_acc": 56.0}
{"epoch": 20, "training_loss": 65.8911554813385, "training_acc": 60.0, "val_loss": 17.641372978687286, "val_acc": 56.0}
{"epoch": 21, "training_loss": 65.8540244102478, "training_acc": 65.0, "val_loss": 17.665910720825195, "val_acc": 56.0}
{"epoch": 22, "training_loss": 66.51061248779297, "training_acc": 61.0, "val_loss": 17.695041000843048, "val_acc": 56.0}
{"epoch": 23, "training_loss": 66.01435875892639, "training_acc": 63.0, "val_loss": 17.735740542411804, "val_acc": 56.0}
{"epoch": 24, "training_loss": 65.9167218208313, "training_acc": 63.0, "val_loss": 17.55100190639496, "val_acc": 56.0}
{"epoch": 25, "training_loss": 65.40333652496338, "training_acc": 64.0, "val_loss": 17.432577908039093, "val_acc": 56.0}
{"epoch": 26, "training_loss": 64.82108354568481, "training_acc": 67.0, "val_loss": 17.411616444587708, "val_acc": 56.0}
{"epoch": 27, "training_loss": 64.97161316871643, "training_acc": 61.0, "val_loss": 17.413343489170074, "val_acc": 52.0}
{"epoch": 28, "training_loss": 64.58925676345825, "training_acc": 65.0, "val_loss": 17.413993179798126, "val_acc": 56.0}
{"epoch": 29, "training_loss": 64.03104972839355, "training_acc": 68.0, "val_loss": 17.661769688129425, "val_acc": 56.0}
{"epoch": 30, "training_loss": 64.66514921188354, "training_acc": 64.0, "val_loss": 17.752231657505035, "val_acc": 56.0}
{"epoch": 31, "training_loss": 63.65468072891235, "training_acc": 67.0, "val_loss": 17.942005395889282, "val_acc": 56.0}
{"epoch": 32, "training_loss": 63.89493179321289, "training_acc": 65.0, "val_loss": 18.09738427400589, "val_acc": 52.0}
{"epoch": 33, "training_loss": 63.905083656311035, "training_acc": 63.0, "val_loss": 18.19363683462143, "val_acc": 52.0}
{"epoch": 34, "training_loss": 63.750152349472046, "training_acc": 61.0, "val_loss": 18.386292457580566, "val_acc": 52.0}
{"epoch": 35, "training_loss": 63.22264051437378, "training_acc": 63.0, "val_loss": 18.606090545654297, "val_acc": 52.0}
{"epoch": 36, "training_loss": 63.07760691642761, "training_acc": 62.0, "val_loss": 18.63448917865753, "val_acc": 52.0}
{"epoch": 37, "training_loss": 63.90921711921692, "training_acc": 59.0, "val_loss": 18.654264509677887, "val_acc": 56.0}
