"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 72.04785823822021, "training_acc": 48.0, "val_loss": 17.457078397274017, "val_acc": 44.0}
{"epoch": 1, "training_loss": 73.41372609138489, "training_acc": 39.0, "val_loss": 17.33759194612503, "val_acc": 44.0}
{"epoch": 2, "training_loss": 72.71468687057495, "training_acc": 47.0, "val_loss": 17.35469549894333, "val_acc": 44.0}
{"epoch": 3, "training_loss": 72.31486463546753, "training_acc": 47.0, "val_loss": 17.407724261283875, "val_acc": 40.0}
{"epoch": 4, "training_loss": 71.16759157180786, "training_acc": 48.0, "val_loss": 17.382703721523285, "val_acc": 52.0}
{"epoch": 5, "training_loss": 70.58134078979492, "training_acc": 52.0, "val_loss": 17.279723286628723, "val_acc": 56.0}
{"epoch": 6, "training_loss": 70.47144269943237, "training_acc": 52.0, "val_loss": 17.23414957523346, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.8642168045044, "training_acc": 55.0, "val_loss": 17.241409420967102, "val_acc": 56.0}
{"epoch": 8, "training_loss": 69.80297136306763, "training_acc": 55.0, "val_loss": 17.23572462797165, "val_acc": 56.0}
{"epoch": 9, "training_loss": 69.71554803848267, "training_acc": 54.0, "val_loss": 17.18766540288925, "val_acc": 56.0}
{"epoch": 10, "training_loss": 69.43088579177856, "training_acc": 58.0, "val_loss": 17.15967506170273, "val_acc": 56.0}
{"epoch": 11, "training_loss": 68.84835481643677, "training_acc": 58.0, "val_loss": 17.142531275749207, "val_acc": 56.0}
{"epoch": 12, "training_loss": 68.71795272827148, "training_acc": 56.0, "val_loss": 17.148423194885254, "val_acc": 56.0}
{"epoch": 13, "training_loss": 68.88712644577026, "training_acc": 55.0, "val_loss": 17.16599613428116, "val_acc": 52.0}
{"epoch": 14, "training_loss": 68.74158215522766, "training_acc": 52.0, "val_loss": 17.214542627334595, "val_acc": 52.0}
{"epoch": 15, "training_loss": 68.83941507339478, "training_acc": 51.0, "val_loss": 17.229542136192322, "val_acc": 52.0}
{"epoch": 16, "training_loss": 68.6292142868042, "training_acc": 53.0, "val_loss": 17.241641879081726, "val_acc": 52.0}
{"epoch": 17, "training_loss": 68.1840033531189, "training_acc": 53.0, "val_loss": 17.286500334739685, "val_acc": 52.0}
{"epoch": 18, "training_loss": 67.72385859489441, "training_acc": 56.0, "val_loss": 17.292630672454834, "val_acc": 52.0}
{"epoch": 19, "training_loss": 67.6120195388794, "training_acc": 58.0, "val_loss": 17.304107546806335, "val_acc": 52.0}
{"epoch": 20, "training_loss": 68.18498134613037, "training_acc": 58.0, "val_loss": 17.283789813518524, "val_acc": 52.0}
{"epoch": 21, "training_loss": 67.38229560852051, "training_acc": 60.0, "val_loss": 17.236274480819702, "val_acc": 52.0}
{"epoch": 22, "training_loss": 67.25918054580688, "training_acc": 60.0, "val_loss": 17.258255183696747, "val_acc": 52.0}
{"epoch": 23, "training_loss": 67.54410600662231, "training_acc": 60.0, "val_loss": 17.263180017471313, "val_acc": 52.0}
{"epoch": 24, "training_loss": 67.2035186290741, "training_acc": 60.0, "val_loss": 17.28500723838806, "val_acc": 52.0}
{"epoch": 25, "training_loss": 66.62555193901062, "training_acc": 59.0, "val_loss": 17.310607433319092, "val_acc": 48.0}
{"epoch": 26, "training_loss": 66.84872102737427, "training_acc": 59.0, "val_loss": 17.341308295726776, "val_acc": 48.0}
{"epoch": 27, "training_loss": 66.32663989067078, "training_acc": 61.0, "val_loss": 17.36724078655243, "val_acc": 48.0}
{"epoch": 28, "training_loss": 66.55076360702515, "training_acc": 59.0, "val_loss": 17.329204082489014, "val_acc": 48.0}
{"epoch": 29, "training_loss": 66.31460428237915, "training_acc": 62.0, "val_loss": 17.339925467967987, "val_acc": 48.0}
{"epoch": 30, "training_loss": 65.61721897125244, "training_acc": 63.0, "val_loss": 17.32562929391861, "val_acc": 48.0}
