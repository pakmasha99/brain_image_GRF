"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 70.94439458847046, "training_acc": 47.0, "val_loss": 17.407120764255524, "val_acc": 52.0}
{"epoch": 1, "training_loss": 71.44614148139954, "training_acc": 51.0, "val_loss": 17.285123467445374, "val_acc": 56.0}
{"epoch": 2, "training_loss": 69.84559535980225, "training_acc": 54.0, "val_loss": 17.158538103103638, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.58505177497864, "training_acc": 49.0, "val_loss": 17.038027942180634, "val_acc": 52.0}
{"epoch": 4, "training_loss": 68.26415085792542, "training_acc": 51.0, "val_loss": 16.96716845035553, "val_acc": 56.0}
{"epoch": 5, "training_loss": 67.97055292129517, "training_acc": 52.0, "val_loss": 16.98179990053177, "val_acc": 56.0}
{"epoch": 6, "training_loss": 67.58439207077026, "training_acc": 57.0, "val_loss": 17.095428705215454, "val_acc": 56.0}
{"epoch": 7, "training_loss": 66.53161680698395, "training_acc": 58.0, "val_loss": 17.18384176492691, "val_acc": 56.0}
{"epoch": 8, "training_loss": 66.62571859359741, "training_acc": 63.0, "val_loss": 17.13668406009674, "val_acc": 56.0}
{"epoch": 9, "training_loss": 66.7358946800232, "training_acc": 60.0, "val_loss": 17.02180951833725, "val_acc": 56.0}
{"epoch": 10, "training_loss": 66.40877318382263, "training_acc": 59.0, "val_loss": 16.965599358081818, "val_acc": 56.0}
{"epoch": 11, "training_loss": 66.1792962551117, "training_acc": 61.0, "val_loss": 17.02360063791275, "val_acc": 56.0}
{"epoch": 12, "training_loss": 65.76778507232666, "training_acc": 60.0, "val_loss": 17.061424255371094, "val_acc": 56.0}
{"epoch": 13, "training_loss": 65.55182313919067, "training_acc": 62.0, "val_loss": 17.050762474536896, "val_acc": 56.0}
{"epoch": 14, "training_loss": 65.30799341201782, "training_acc": 62.0, "val_loss": 17.005541920661926, "val_acc": 56.0}
{"epoch": 15, "training_loss": 65.41541528701782, "training_acc": 62.0, "val_loss": 17.001891136169434, "val_acc": 56.0}
{"epoch": 16, "training_loss": 65.2486572265625, "training_acc": 62.0, "val_loss": 17.0161172747612, "val_acc": 56.0}
{"epoch": 17, "training_loss": 64.54172098636627, "training_acc": 65.0, "val_loss": 17.03449934720993, "val_acc": 56.0}
{"epoch": 18, "training_loss": 64.63459467887878, "training_acc": 65.0, "val_loss": 17.071329057216644, "val_acc": 56.0}
{"epoch": 19, "training_loss": 64.79925298690796, "training_acc": 63.0, "val_loss": 17.096200585365295, "val_acc": 56.0}
{"epoch": 20, "training_loss": 64.56257772445679, "training_acc": 64.0, "val_loss": 17.076025903224945, "val_acc": 56.0}
{"epoch": 21, "training_loss": 64.08934688568115, "training_acc": 63.0, "val_loss": 17.069771885871887, "val_acc": 56.0}
{"epoch": 22, "training_loss": 63.71187448501587, "training_acc": 64.0, "val_loss": 17.060527205467224, "val_acc": 56.0}
{"epoch": 23, "training_loss": 64.25581336021423, "training_acc": 63.0, "val_loss": 17.073480784893036, "val_acc": 56.0}
{"epoch": 24, "training_loss": 64.18176579475403, "training_acc": 62.0, "val_loss": 17.09897816181183, "val_acc": 52.0}
{"epoch": 25, "training_loss": 63.49920845031738, "training_acc": 65.0, "val_loss": 17.133106291294098, "val_acc": 52.0}
{"epoch": 26, "training_loss": 63.80350589752197, "training_acc": 67.0, "val_loss": 17.171375453472137, "val_acc": 52.0}
{"epoch": 27, "training_loss": 63.91535568237305, "training_acc": 66.0, "val_loss": 17.20145344734192, "val_acc": 52.0}
{"epoch": 28, "training_loss": 63.234068870544434, "training_acc": 66.0, "val_loss": 17.244800925254822, "val_acc": 52.0}
{"epoch": 29, "training_loss": 62.91078567504883, "training_acc": 68.0, "val_loss": 17.208759486675262, "val_acc": 52.0}
