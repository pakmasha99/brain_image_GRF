"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 74.20618009567261, "training_acc": 52.0, "val_loss": 18.11576932668686, "val_acc": 52.0}
{"epoch": 1, "training_loss": 73.51679039001465, "training_acc": 54.0, "val_loss": 17.766375839710236, "val_acc": 52.0}
{"epoch": 2, "training_loss": 72.665536403656, "training_acc": 54.0, "val_loss": 17.59844869375229, "val_acc": 52.0}
{"epoch": 3, "training_loss": 71.70037198066711, "training_acc": 53.0, "val_loss": 17.51886010169983, "val_acc": 52.0}
{"epoch": 4, "training_loss": 71.05203938484192, "training_acc": 53.0, "val_loss": 17.52251833677292, "val_acc": 52.0}
{"epoch": 5, "training_loss": 70.56703925132751, "training_acc": 53.0, "val_loss": 17.527729272842407, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.33892250061035, "training_acc": 53.0, "val_loss": 17.535148561000824, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.44275283813477, "training_acc": 53.0, "val_loss": 17.514578998088837, "val_acc": 52.0}
{"epoch": 8, "training_loss": 68.49135637283325, "training_acc": 53.0, "val_loss": 17.50132590532303, "val_acc": 52.0}
{"epoch": 9, "training_loss": 68.54570603370667, "training_acc": 53.0, "val_loss": 17.523711919784546, "val_acc": 52.0}
{"epoch": 10, "training_loss": 68.28271007537842, "training_acc": 53.0, "val_loss": 17.517684400081635, "val_acc": 52.0}
{"epoch": 11, "training_loss": 67.92680382728577, "training_acc": 53.0, "val_loss": 17.479625344276428, "val_acc": 52.0}
{"epoch": 12, "training_loss": 67.74209785461426, "training_acc": 55.0, "val_loss": 17.51893013715744, "val_acc": 52.0}
{"epoch": 13, "training_loss": 67.71549415588379, "training_acc": 54.0, "val_loss": 17.48611479997635, "val_acc": 52.0}
{"epoch": 14, "training_loss": 67.6398093700409, "training_acc": 54.0, "val_loss": 17.46605634689331, "val_acc": 52.0}
{"epoch": 15, "training_loss": 67.11004996299744, "training_acc": 53.0, "val_loss": 17.50844717025757, "val_acc": 52.0}
{"epoch": 16, "training_loss": 66.9472074508667, "training_acc": 56.0, "val_loss": 17.53159612417221, "val_acc": 52.0}
{"epoch": 17, "training_loss": 67.19391679763794, "training_acc": 54.0, "val_loss": 17.474913597106934, "val_acc": 52.0}
{"epoch": 18, "training_loss": 66.51586246490479, "training_acc": 55.0, "val_loss": 17.384634912014008, "val_acc": 52.0}
{"epoch": 19, "training_loss": 66.2137246131897, "training_acc": 53.0, "val_loss": 17.387494444847107, "val_acc": 52.0}
{"epoch": 20, "training_loss": 66.34546995162964, "training_acc": 54.0, "val_loss": 17.413945496082306, "val_acc": 52.0}
{"epoch": 21, "training_loss": 66.13364887237549, "training_acc": 59.0, "val_loss": 17.39790439605713, "val_acc": 52.0}
{"epoch": 22, "training_loss": 66.08565473556519, "training_acc": 58.0, "val_loss": 17.354603111743927, "val_acc": 52.0}
{"epoch": 23, "training_loss": 65.79977774620056, "training_acc": 57.0, "val_loss": 17.33754128217697, "val_acc": 52.0}
{"epoch": 24, "training_loss": 65.73882627487183, "training_acc": 58.0, "val_loss": 17.35718846321106, "val_acc": 52.0}
{"epoch": 25, "training_loss": 65.5097725391388, "training_acc": 59.0, "val_loss": 17.336547374725342, "val_acc": 52.0}
{"epoch": 26, "training_loss": 65.62890553474426, "training_acc": 59.0, "val_loss": 17.327049374580383, "val_acc": 52.0}
{"epoch": 27, "training_loss": 65.60469961166382, "training_acc": 60.0, "val_loss": 17.328602075576782, "val_acc": 52.0}
{"epoch": 28, "training_loss": 65.4588623046875, "training_acc": 62.0, "val_loss": 17.26665049791336, "val_acc": 52.0}
{"epoch": 29, "training_loss": 64.94674396514893, "training_acc": 60.0, "val_loss": 17.239123582839966, "val_acc": 52.0}
{"epoch": 30, "training_loss": 65.05880689620972, "training_acc": 58.0, "val_loss": 17.24333167076111, "val_acc": 52.0}
{"epoch": 31, "training_loss": 64.75839972496033, "training_acc": 60.0, "val_loss": 17.26168841123581, "val_acc": 52.0}
{"epoch": 32, "training_loss": 64.39442539215088, "training_acc": 62.0, "val_loss": 17.2866091132164, "val_acc": 52.0}
{"epoch": 33, "training_loss": 64.8753833770752, "training_acc": 56.0, "val_loss": 17.329494655132294, "val_acc": 52.0}
{"epoch": 34, "training_loss": 64.37292885780334, "training_acc": 60.0, "val_loss": 17.377161979675293, "val_acc": 52.0}
{"epoch": 35, "training_loss": 63.86288070678711, "training_acc": 61.0, "val_loss": 17.431029677391052, "val_acc": 52.0}
{"epoch": 36, "training_loss": 63.884103775024414, "training_acc": 63.0, "val_loss": 17.43703931570053, "val_acc": 52.0}
{"epoch": 37, "training_loss": 63.81138229370117, "training_acc": 64.0, "val_loss": 17.466357350349426, "val_acc": 52.0}
{"epoch": 38, "training_loss": 63.653520584106445, "training_acc": 62.0, "val_loss": 17.51922369003296, "val_acc": 52.0}
{"epoch": 39, "training_loss": 63.818148612976074, "training_acc": 62.0, "val_loss": 17.583949863910675, "val_acc": 52.0}
{"epoch": 40, "training_loss": 63.52241826057434, "training_acc": 65.0, "val_loss": 17.68018752336502, "val_acc": 52.0}
{"epoch": 41, "training_loss": 63.48426008224487, "training_acc": 63.0, "val_loss": 17.693302035331726, "val_acc": 52.0}
{"epoch": 42, "training_loss": 63.44452929496765, "training_acc": 65.0, "val_loss": 17.609675228595734, "val_acc": 52.0}
{"epoch": 43, "training_loss": 63.329968214035034, "training_acc": 64.0, "val_loss": 17.589041590690613, "val_acc": 52.0}
{"epoch": 44, "training_loss": 63.2992467880249, "training_acc": 63.0, "val_loss": 17.599715292453766, "val_acc": 52.0}
{"epoch": 45, "training_loss": 62.987714529037476, "training_acc": 65.0, "val_loss": 17.57809817790985, "val_acc": 52.0}
{"epoch": 46, "training_loss": 62.86011075973511, "training_acc": 65.0, "val_loss": 17.498482763767242, "val_acc": 52.0}
{"epoch": 47, "training_loss": 62.548300981521606, "training_acc": 66.0, "val_loss": 17.514720559120178, "val_acc": 52.0}
{"epoch": 48, "training_loss": 63.39522075653076, "training_acc": 59.0, "val_loss": 17.587199807167053, "val_acc": 52.0}
