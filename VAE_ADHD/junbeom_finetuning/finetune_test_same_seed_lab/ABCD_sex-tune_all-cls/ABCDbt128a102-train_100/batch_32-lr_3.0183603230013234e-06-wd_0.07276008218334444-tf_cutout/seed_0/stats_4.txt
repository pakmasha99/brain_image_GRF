"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 71.00047636032104, "training_acc": 47.0, "val_loss": 17.05479770898819, "val_acc": 52.0}
{"epoch": 1, "training_loss": 70.84107065200806, "training_acc": 47.0, "val_loss": 16.899096965789795, "val_acc": 44.0}
{"epoch": 2, "training_loss": 69.50971341133118, "training_acc": 57.0, "val_loss": 16.814464330673218, "val_acc": 44.0}
{"epoch": 3, "training_loss": 69.04883813858032, "training_acc": 58.0, "val_loss": 16.80818498134613, "val_acc": 48.0}
{"epoch": 4, "training_loss": 69.10505819320679, "training_acc": 59.0, "val_loss": 16.66986495256424, "val_acc": 48.0}
{"epoch": 5, "training_loss": 69.28791785240173, "training_acc": 58.0, "val_loss": 16.51497632265091, "val_acc": 56.0}
{"epoch": 6, "training_loss": 68.82140779495239, "training_acc": 57.0, "val_loss": 16.47351086139679, "val_acc": 56.0}
{"epoch": 7, "training_loss": 68.65003871917725, "training_acc": 56.0, "val_loss": 16.3778156042099, "val_acc": 52.0}
{"epoch": 8, "training_loss": 68.51881980895996, "training_acc": 54.0, "val_loss": 16.30488932132721, "val_acc": 56.0}
{"epoch": 9, "training_loss": 68.2292206287384, "training_acc": 56.0, "val_loss": 16.376720368862152, "val_acc": 64.0}
{"epoch": 10, "training_loss": 68.34948110580444, "training_acc": 60.0, "val_loss": 16.276702284812927, "val_acc": 64.0}
{"epoch": 11, "training_loss": 67.48520231246948, "training_acc": 61.0, "val_loss": 16.19446873664856, "val_acc": 60.0}
{"epoch": 12, "training_loss": 67.4241771697998, "training_acc": 58.0, "val_loss": 16.09903872013092, "val_acc": 60.0}
{"epoch": 13, "training_loss": 66.82287979125977, "training_acc": 59.0, "val_loss": 16.064222157001495, "val_acc": 56.0}
{"epoch": 14, "training_loss": 66.4764175415039, "training_acc": 62.0, "val_loss": 16.095048189163208, "val_acc": 48.0}
{"epoch": 15, "training_loss": 66.16618871688843, "training_acc": 62.0, "val_loss": 16.108500957489014, "val_acc": 48.0}
{"epoch": 16, "training_loss": 65.93794012069702, "training_acc": 61.0, "val_loss": 16.04125052690506, "val_acc": 52.0}
{"epoch": 17, "training_loss": 65.9037926197052, "training_acc": 60.0, "val_loss": 16.1189004778862, "val_acc": 52.0}
{"epoch": 18, "training_loss": 65.15003538131714, "training_acc": 64.0, "val_loss": 16.207560896873474, "val_acc": 48.0}
{"epoch": 19, "training_loss": 65.32789182662964, "training_acc": 59.0, "val_loss": 16.279035806655884, "val_acc": 48.0}
{"epoch": 20, "training_loss": 65.72402286529541, "training_acc": 62.0, "val_loss": 16.32981449365616, "val_acc": 48.0}
{"epoch": 21, "training_loss": 65.15687203407288, "training_acc": 61.0, "val_loss": 16.3661926984787, "val_acc": 48.0}
{"epoch": 22, "training_loss": 65.26843118667603, "training_acc": 63.0, "val_loss": 16.194285452365875, "val_acc": 48.0}
{"epoch": 23, "training_loss": 64.69173288345337, "training_acc": 64.0, "val_loss": 16.113239526748657, "val_acc": 52.0}
{"epoch": 24, "training_loss": 65.20877242088318, "training_acc": 61.0, "val_loss": 15.956693887710571, "val_acc": 52.0}
{"epoch": 25, "training_loss": 64.44477319717407, "training_acc": 61.0, "val_loss": 15.949490666389465, "val_acc": 52.0}
{"epoch": 26, "training_loss": 64.31766247749329, "training_acc": 59.0, "val_loss": 15.882481634616852, "val_acc": 56.0}
{"epoch": 27, "training_loss": 63.984612464904785, "training_acc": 58.0, "val_loss": 15.886016190052032, "val_acc": 60.0}
{"epoch": 28, "training_loss": 64.46411871910095, "training_acc": 61.0, "val_loss": 15.860094130039215, "val_acc": 60.0}
{"epoch": 29, "training_loss": 63.78099083900452, "training_acc": 60.0, "val_loss": 15.887787938117981, "val_acc": 56.0}
{"epoch": 30, "training_loss": 63.1190619468689, "training_acc": 58.0, "val_loss": 16.023819148540497, "val_acc": 52.0}
{"epoch": 31, "training_loss": 63.25872302055359, "training_acc": 65.0, "val_loss": 16.114015877246857, "val_acc": 52.0}
{"epoch": 32, "training_loss": 62.701162576675415, "training_acc": 66.0, "val_loss": 15.908758342266083, "val_acc": 52.0}
{"epoch": 33, "training_loss": 63.009888887405396, "training_acc": 62.0, "val_loss": 15.791173279285431, "val_acc": 52.0}
{"epoch": 34, "training_loss": 62.904456615448, "training_acc": 66.0, "val_loss": 15.850421786308289, "val_acc": 52.0}
{"epoch": 35, "training_loss": 62.8720166683197, "training_acc": 61.0, "val_loss": 15.813404321670532, "val_acc": 52.0}
{"epoch": 36, "training_loss": 62.0976437330246, "training_acc": 62.0, "val_loss": 15.72958379983902, "val_acc": 56.0}
{"epoch": 37, "training_loss": 61.39108347892761, "training_acc": 63.0, "val_loss": 15.918509662151337, "val_acc": 52.0}
{"epoch": 38, "training_loss": 62.642817974090576, "training_acc": 65.0, "val_loss": 16.011281311511993, "val_acc": 52.0}
{"epoch": 39, "training_loss": 62.46943020820618, "training_acc": 64.0, "val_loss": 15.894804894924164, "val_acc": 48.0}
{"epoch": 40, "training_loss": 62.13488149642944, "training_acc": 65.0, "val_loss": 15.794311463832855, "val_acc": 52.0}
{"epoch": 41, "training_loss": 60.77599763870239, "training_acc": 66.0, "val_loss": 15.714192390441895, "val_acc": 56.0}
{"epoch": 42, "training_loss": 61.44574785232544, "training_acc": 60.0, "val_loss": 15.716704726219177, "val_acc": 52.0}
{"epoch": 43, "training_loss": 61.26807713508606, "training_acc": 64.0, "val_loss": 15.757891535758972, "val_acc": 52.0}
{"epoch": 44, "training_loss": 61.27755570411682, "training_acc": 66.0, "val_loss": 15.917493402957916, "val_acc": 48.0}
{"epoch": 45, "training_loss": 60.8225154876709, "training_acc": 67.0, "val_loss": 15.845456719398499, "val_acc": 52.0}
{"epoch": 46, "training_loss": 60.66521167755127, "training_acc": 62.0, "val_loss": 15.78305959701538, "val_acc": 48.0}
{"epoch": 47, "training_loss": 59.598750591278076, "training_acc": 66.0, "val_loss": 15.825816988945007, "val_acc": 48.0}
{"epoch": 48, "training_loss": 60.641124963760376, "training_acc": 69.0, "val_loss": 15.987153351306915, "val_acc": 48.0}
{"epoch": 49, "training_loss": 60.562058210372925, "training_acc": 70.0, "val_loss": 16.07747972011566, "val_acc": 52.0}
{"epoch": 50, "training_loss": 60.30166840553284, "training_acc": 68.0, "val_loss": 15.918421745300293, "val_acc": 52.0}
{"epoch": 51, "training_loss": 59.35682916641235, "training_acc": 65.0, "val_loss": 15.767194330692291, "val_acc": 48.0}
{"epoch": 52, "training_loss": 59.753517150878906, "training_acc": 68.0, "val_loss": 15.888741612434387, "val_acc": 52.0}
{"epoch": 53, "training_loss": 58.74592638015747, "training_acc": 71.0, "val_loss": 16.004109382629395, "val_acc": 48.0}
{"epoch": 54, "training_loss": 59.29147744178772, "training_acc": 68.0, "val_loss": 16.29912555217743, "val_acc": 52.0}
{"epoch": 55, "training_loss": 60.219993233680725, "training_acc": 69.0, "val_loss": 16.380324959754944, "val_acc": 52.0}
{"epoch": 56, "training_loss": 58.935283184051514, "training_acc": 66.0, "val_loss": 16.284044086933136, "val_acc": 48.0}
{"epoch": 57, "training_loss": 58.838335037231445, "training_acc": 67.0, "val_loss": 16.154643893241882, "val_acc": 48.0}
{"epoch": 58, "training_loss": 58.154479026794434, "training_acc": 70.0, "val_loss": 16.05108231306076, "val_acc": 48.0}
{"epoch": 59, "training_loss": 57.84347200393677, "training_acc": 71.0, "val_loss": 16.04476571083069, "val_acc": 48.0}
{"epoch": 60, "training_loss": 58.0428284406662, "training_acc": 69.0, "val_loss": 16.188418865203857, "val_acc": 48.0}
