"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.91174149513245, "training_acc": 52.0, "val_loss": 17.230582237243652, "val_acc": 56.0}
{"epoch": 1, "training_loss": 68.15848278999329, "training_acc": 57.0, "val_loss": 17.379161715507507, "val_acc": 52.0}
{"epoch": 2, "training_loss": 67.0455412864685, "training_acc": 58.0, "val_loss": 17.57410019636154, "val_acc": 52.0}
{"epoch": 3, "training_loss": 66.81969022750854, "training_acc": 59.0, "val_loss": 17.817233502864838, "val_acc": 52.0}
{"epoch": 4, "training_loss": 66.5603346824646, "training_acc": 61.0, "val_loss": 17.9651141166687, "val_acc": 52.0}
{"epoch": 5, "training_loss": 65.4603967666626, "training_acc": 62.0, "val_loss": 17.57861077785492, "val_acc": 48.0}
{"epoch": 6, "training_loss": 65.9369444847107, "training_acc": 59.0, "val_loss": 17.294849455356598, "val_acc": 48.0}
{"epoch": 7, "training_loss": 65.29125833511353, "training_acc": 62.0, "val_loss": 17.19747632741928, "val_acc": 48.0}
{"epoch": 8, "training_loss": 64.61948013305664, "training_acc": 61.0, "val_loss": 17.14624911546707, "val_acc": 52.0}
{"epoch": 9, "training_loss": 65.03833055496216, "training_acc": 60.0, "val_loss": 17.246422171592712, "val_acc": 52.0}
{"epoch": 10, "training_loss": 63.93037271499634, "training_acc": 61.0, "val_loss": 17.50514954328537, "val_acc": 48.0}
{"epoch": 11, "training_loss": 63.53205966949463, "training_acc": 64.0, "val_loss": 17.683814465999603, "val_acc": 48.0}
{"epoch": 12, "training_loss": 63.74053764343262, "training_acc": 61.0, "val_loss": 17.741389572620392, "val_acc": 52.0}
{"epoch": 13, "training_loss": 63.90457248687744, "training_acc": 62.0, "val_loss": 17.24892407655716, "val_acc": 56.0}
{"epoch": 14, "training_loss": 63.47721242904663, "training_acc": 65.0, "val_loss": 16.959932446479797, "val_acc": 56.0}
{"epoch": 15, "training_loss": 64.25286960601807, "training_acc": 64.0, "val_loss": 16.979530453681946, "val_acc": 56.0}
{"epoch": 16, "training_loss": 64.6714928150177, "training_acc": 65.0, "val_loss": 17.338834702968597, "val_acc": 56.0}
{"epoch": 17, "training_loss": 63.21214485168457, "training_acc": 70.0, "val_loss": 17.525115609169006, "val_acc": 56.0}
{"epoch": 18, "training_loss": 62.044044971466064, "training_acc": 69.0, "val_loss": 17.4773707985878, "val_acc": 56.0}
{"epoch": 19, "training_loss": 61.49992370605469, "training_acc": 68.0, "val_loss": 17.63620525598526, "val_acc": 52.0}
{"epoch": 20, "training_loss": 62.305702924728394, "training_acc": 72.0, "val_loss": 17.8095281124115, "val_acc": 56.0}
{"epoch": 21, "training_loss": 62.48989987373352, "training_acc": 67.0, "val_loss": 17.953698337078094, "val_acc": 56.0}
{"epoch": 22, "training_loss": 61.64457964897156, "training_acc": 70.0, "val_loss": 17.691002786159515, "val_acc": 56.0}
{"epoch": 23, "training_loss": 61.36958312988281, "training_acc": 71.0, "val_loss": 17.23587214946747, "val_acc": 56.0}
{"epoch": 24, "training_loss": 61.394922494888306, "training_acc": 74.0, "val_loss": 17.277608811855316, "val_acc": 60.0}
{"epoch": 25, "training_loss": 60.936275482177734, "training_acc": 71.0, "val_loss": 17.5805926322937, "val_acc": 56.0}
{"epoch": 26, "training_loss": 60.49868106842041, "training_acc": 69.0, "val_loss": 17.717865109443665, "val_acc": 52.0}
{"epoch": 27, "training_loss": 63.011107206344604, "training_acc": 68.0, "val_loss": 17.793460190296173, "val_acc": 52.0}
{"epoch": 28, "training_loss": 62.79719853401184, "training_acc": 65.0, "val_loss": 18.097274005413055, "val_acc": 52.0}
{"epoch": 29, "training_loss": 60.21428036689758, "training_acc": 64.0, "val_loss": 18.135802447795868, "val_acc": 56.0}
{"epoch": 30, "training_loss": 60.31460976600647, "training_acc": 72.0, "val_loss": 17.874282598495483, "val_acc": 60.0}
{"epoch": 31, "training_loss": 60.79104280471802, "training_acc": 68.0, "val_loss": 17.576071619987488, "val_acc": 60.0}
{"epoch": 32, "training_loss": 61.112882137298584, "training_acc": 74.0, "val_loss": 17.506538331508636, "val_acc": 60.0}
{"epoch": 33, "training_loss": 61.036335468292236, "training_acc": 74.0, "val_loss": 17.867596447467804, "val_acc": 56.0}
