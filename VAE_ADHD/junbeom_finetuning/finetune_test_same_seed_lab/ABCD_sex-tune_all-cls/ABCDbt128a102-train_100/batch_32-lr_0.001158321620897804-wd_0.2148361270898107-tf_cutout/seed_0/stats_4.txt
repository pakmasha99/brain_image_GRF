"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 81.89387440681458, "training_acc": 52.0, "val_loss": 37879.42810058594, "val_acc": 48.0}
{"epoch": 1, "training_loss": 43453.29344749451, "training_acc": 55.0, "val_loss": 268.43042373657227, "val_acc": 52.0}
{"epoch": 2, "training_loss": 416.66971588134766, "training_acc": 57.0, "val_loss": 34.48725938796997, "val_acc": 52.0}
{"epoch": 3, "training_loss": 110.70087480545044, "training_acc": 45.0, "val_loss": 17.345881462097168, "val_acc": 52.0}
{"epoch": 4, "training_loss": 70.18216705322266, "training_acc": 53.0, "val_loss": 18.155278265476227, "val_acc": 52.0}
{"epoch": 5, "training_loss": 72.13567972183228, "training_acc": 53.0, "val_loss": 18.624506890773773, "val_acc": 48.0}
{"epoch": 6, "training_loss": 72.24609065055847, "training_acc": 51.0, "val_loss": 22.719377279281616, "val_acc": 52.0}
{"epoch": 7, "training_loss": 80.10683965682983, "training_acc": 53.0, "val_loss": 17.676909267902374, "val_acc": 52.0}
{"epoch": 8, "training_loss": 73.90731906890869, "training_acc": 47.0, "val_loss": 17.40347295999527, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.13115167617798, "training_acc": 57.0, "val_loss": 17.595742642879486, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.27860879898071, "training_acc": 53.0, "val_loss": 17.318373918533325, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.60449886322021, "training_acc": 53.0, "val_loss": 17.327643930912018, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.1209774017334, "training_acc": 53.0, "val_loss": 17.313958704471588, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.3564522266388, "training_acc": 47.0, "val_loss": 17.31061488389969, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.42748856544495, "training_acc": 53.0, "val_loss": 17.35444813966751, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.3726544380188, "training_acc": 53.0, "val_loss": 17.30915606021881, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.3118269443512, "training_acc": 53.0, "val_loss": 17.31729805469513, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.2714946269989, "training_acc": 53.0, "val_loss": 17.31017827987671, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.16864776611328, "training_acc": 53.0, "val_loss": 17.312774062156677, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.07624101638794, "training_acc": 53.0, "val_loss": 17.360839247703552, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.30309534072876, "training_acc": 53.0, "val_loss": 17.430751025676727, "val_acc": 52.0}
{"epoch": 21, "training_loss": 70.47008609771729, "training_acc": 53.0, "val_loss": 17.595362663269043, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.88804841041565, "training_acc": 53.0, "val_loss": 17.372439801692963, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.19660234451294, "training_acc": 53.0, "val_loss": 17.33529269695282, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.2006676197052, "training_acc": 47.0, "val_loss": 17.514212429523468, "val_acc": 52.0}
{"epoch": 25, "training_loss": 70.19336724281311, "training_acc": 47.0, "val_loss": 17.705413699150085, "val_acc": 52.0}
{"epoch": 26, "training_loss": 71.28087091445923, "training_acc": 47.0, "val_loss": 17.520716786384583, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.54028558731079, "training_acc": 47.0, "val_loss": 17.31162816286087, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.35875701904297, "training_acc": 53.0, "val_loss": 17.42166429758072, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.61549758911133, "training_acc": 53.0, "val_loss": 17.46796816587448, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.47791147232056, "training_acc": 53.0, "val_loss": 17.31998771429062, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.13267421722412, "training_acc": 53.0, "val_loss": 17.320100963115692, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.34707403182983, "training_acc": 49.0, "val_loss": 17.426909506320953, "val_acc": 52.0}
{"epoch": 33, "training_loss": 70.08650970458984, "training_acc": 47.0, "val_loss": 17.53913015127182, "val_acc": 52.0}
{"epoch": 34, "training_loss": 70.3854808807373, "training_acc": 47.0, "val_loss": 17.477065324783325, "val_acc": 52.0}
