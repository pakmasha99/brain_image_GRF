"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 76.6986448764801, "training_acc": 49.0, "val_loss": 115.2589201927185, "val_acc": 44.0}
{"epoch": 1, "training_loss": 11263.796004533768, "training_acc": 48.0, "val_loss": 19.431445002555847, "val_acc": 44.0}
{"epoch": 2, "training_loss": 83.23294305801392, "training_acc": 44.0, "val_loss": 17.859451472759247, "val_acc": 56.0}
{"epoch": 3, "training_loss": 77.15218877792358, "training_acc": 52.0, "val_loss": 22.91269749403, "val_acc": 44.0}
{"epoch": 4, "training_loss": 82.31639862060547, "training_acc": 50.0, "val_loss": 18.25747638940811, "val_acc": 56.0}
{"epoch": 5, "training_loss": 72.36722564697266, "training_acc": 52.0, "val_loss": 18.499068915843964, "val_acc": 56.0}
{"epoch": 6, "training_loss": 70.10607695579529, "training_acc": 50.0, "val_loss": 17.192786931991577, "val_acc": 56.0}
{"epoch": 7, "training_loss": 70.41728019714355, "training_acc": 48.0, "val_loss": 17.325177788734436, "val_acc": 56.0}
{"epoch": 8, "training_loss": 70.99376964569092, "training_acc": 41.0, "val_loss": 17.249780893325806, "val_acc": 56.0}
{"epoch": 9, "training_loss": 70.5524971485138, "training_acc": 42.0, "val_loss": 17.903724312782288, "val_acc": 56.0}
{"epoch": 10, "training_loss": 71.32009601593018, "training_acc": 48.0, "val_loss": 17.819571495056152, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.4983057975769, "training_acc": 48.0, "val_loss": 17.184314131736755, "val_acc": 56.0}
{"epoch": 12, "training_loss": 72.79399585723877, "training_acc": 52.0, "val_loss": 17.23896414041519, "val_acc": 56.0}
{"epoch": 13, "training_loss": 70.93910646438599, "training_acc": 48.0, "val_loss": 17.22237765789032, "val_acc": 56.0}
{"epoch": 14, "training_loss": 69.43869686126709, "training_acc": 52.0, "val_loss": 17.1627938747406, "val_acc": 56.0}
{"epoch": 15, "training_loss": 69.82576251029968, "training_acc": 52.0, "val_loss": 17.16996282339096, "val_acc": 56.0}
{"epoch": 16, "training_loss": 69.97084045410156, "training_acc": 52.0, "val_loss": 17.169879376888275, "val_acc": 56.0}
{"epoch": 17, "training_loss": 69.54084897041321, "training_acc": 52.0, "val_loss": 17.238354682922363, "val_acc": 56.0}
{"epoch": 18, "training_loss": 69.15970349311829, "training_acc": 50.0, "val_loss": 17.685863375663757, "val_acc": 56.0}
{"epoch": 19, "training_loss": 70.50139999389648, "training_acc": 48.0, "val_loss": 17.76156574487686, "val_acc": 56.0}
{"epoch": 20, "training_loss": 69.75072288513184, "training_acc": 48.0, "val_loss": 17.301341891288757, "val_acc": 56.0}
{"epoch": 21, "training_loss": 69.17286920547485, "training_acc": 52.0, "val_loss": 17.18910038471222, "val_acc": 56.0}
{"epoch": 22, "training_loss": 69.46426963806152, "training_acc": 52.0, "val_loss": 17.18979924917221, "val_acc": 56.0}
{"epoch": 23, "training_loss": 69.2287654876709, "training_acc": 52.0, "val_loss": 17.254434525966644, "val_acc": 56.0}
{"epoch": 24, "training_loss": 69.53617000579834, "training_acc": 52.0, "val_loss": 17.181430757045746, "val_acc": 56.0}
{"epoch": 25, "training_loss": 69.27953600883484, "training_acc": 52.0, "val_loss": 17.235973477363586, "val_acc": 56.0}
{"epoch": 26, "training_loss": 69.23883247375488, "training_acc": 52.0, "val_loss": 17.263944447040558, "val_acc": 56.0}
{"epoch": 27, "training_loss": 69.21956777572632, "training_acc": 52.0, "val_loss": 17.30097532272339, "val_acc": 56.0}
{"epoch": 28, "training_loss": 69.29724764823914, "training_acc": 52.0, "val_loss": 17.29101687669754, "val_acc": 56.0}
{"epoch": 29, "training_loss": 69.23514223098755, "training_acc": 52.0, "val_loss": 17.19101518392563, "val_acc": 56.0}
{"epoch": 30, "training_loss": 69.66035509109497, "training_acc": 52.0, "val_loss": 17.155371606349945, "val_acc": 56.0}
{"epoch": 31, "training_loss": 69.76391410827637, "training_acc": 52.0, "val_loss": 17.186513543128967, "val_acc": 56.0}
{"epoch": 32, "training_loss": 70.46133279800415, "training_acc": 52.0, "val_loss": 17.272883653640747, "val_acc": 56.0}
{"epoch": 33, "training_loss": 70.7830605506897, "training_acc": 52.0, "val_loss": 17.210926115512848, "val_acc": 56.0}
{"epoch": 34, "training_loss": 69.85655307769775, "training_acc": 52.0, "val_loss": 17.156586050987244, "val_acc": 56.0}
{"epoch": 35, "training_loss": 69.42524719238281, "training_acc": 52.0, "val_loss": 17.237986624240875, "val_acc": 56.0}
{"epoch": 36, "training_loss": 69.25432348251343, "training_acc": 52.0, "val_loss": 17.24625676870346, "val_acc": 56.0}
{"epoch": 37, "training_loss": 69.22727656364441, "training_acc": 52.0, "val_loss": 17.211109399795532, "val_acc": 56.0}
{"epoch": 38, "training_loss": 69.27490997314453, "training_acc": 52.0, "val_loss": 17.224235832691193, "val_acc": 56.0}
{"epoch": 39, "training_loss": 69.20713257789612, "training_acc": 52.0, "val_loss": 17.283061146736145, "val_acc": 56.0}
{"epoch": 40, "training_loss": 69.41714072227478, "training_acc": 50.0, "val_loss": 17.360642552375793, "val_acc": 56.0}
{"epoch": 41, "training_loss": 69.35027527809143, "training_acc": 48.0, "val_loss": 17.344941198825836, "val_acc": 56.0}
{"epoch": 42, "training_loss": 69.33368468284607, "training_acc": 48.0, "val_loss": 17.34711080789566, "val_acc": 56.0}
{"epoch": 43, "training_loss": 69.37089490890503, "training_acc": 46.0, "val_loss": 17.34405905008316, "val_acc": 56.0}
{"epoch": 44, "training_loss": 69.3800139427185, "training_acc": 48.0, "val_loss": 17.3834428191185, "val_acc": 56.0}
{"epoch": 45, "training_loss": 69.39059019088745, "training_acc": 48.0, "val_loss": 17.371268570423126, "val_acc": 56.0}
{"epoch": 46, "training_loss": 69.36270642280579, "training_acc": 48.0, "val_loss": 17.33107715845108, "val_acc": 56.0}
{"epoch": 47, "training_loss": 69.27064418792725, "training_acc": 51.0, "val_loss": 17.26715862751007, "val_acc": 56.0}
{"epoch": 48, "training_loss": 69.27216005325317, "training_acc": 52.0, "val_loss": 17.2366201877594, "val_acc": 56.0}
{"epoch": 49, "training_loss": 69.23198366165161, "training_acc": 52.0, "val_loss": 17.25281924009323, "val_acc": 56.0}
