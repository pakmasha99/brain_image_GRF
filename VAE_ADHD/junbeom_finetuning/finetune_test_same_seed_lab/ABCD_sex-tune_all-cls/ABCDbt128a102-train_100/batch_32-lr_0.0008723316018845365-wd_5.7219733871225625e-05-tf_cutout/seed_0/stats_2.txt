"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 87.58827018737793, "training_acc": 46.0, "val_loss": 291.28222465515137, "val_acc": 52.0}
{"epoch": 1, "training_loss": 481.86528396606445, "training_acc": 45.0, "val_loss": 19.92400735616684, "val_acc": 48.0}
{"epoch": 2, "training_loss": 74.76562213897705, "training_acc": 49.0, "val_loss": 74.31872487068176, "val_acc": 48.0}
{"epoch": 3, "training_loss": 165.43972992897034, "training_acc": 53.0, "val_loss": 17.367511987686157, "val_acc": 52.0}
{"epoch": 4, "training_loss": 72.18519878387451, "training_acc": 47.0, "val_loss": 17.35438108444214, "val_acc": 52.0}
{"epoch": 5, "training_loss": 71.61231470108032, "training_acc": 53.0, "val_loss": 17.311881482601166, "val_acc": 52.0}
{"epoch": 6, "training_loss": 73.28317475318909, "training_acc": 43.0, "val_loss": 17.336244881153107, "val_acc": 52.0}
{"epoch": 7, "training_loss": 68.8113443851471, "training_acc": 53.0, "val_loss": 17.97618418931961, "val_acc": 52.0}
{"epoch": 8, "training_loss": 70.70732688903809, "training_acc": 53.0, "val_loss": 17.354412376880646, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.05132031440735, "training_acc": 53.0, "val_loss": 17.37263798713684, "val_acc": 52.0}
{"epoch": 10, "training_loss": 70.34233617782593, "training_acc": 47.0, "val_loss": 17.55829155445099, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.99074745178223, "training_acc": 49.0, "val_loss": 17.345809936523438, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.04724097251892, "training_acc": 53.0, "val_loss": 17.63816922903061, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.92245864868164, "training_acc": 53.0, "val_loss": 17.3288956284523, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.23785495758057, "training_acc": 53.0, "val_loss": 17.357730865478516, "val_acc": 52.0}
{"epoch": 15, "training_loss": 70.46120429039001, "training_acc": 47.0, "val_loss": 17.51158833503723, "val_acc": 52.0}
{"epoch": 16, "training_loss": 70.01052474975586, "training_acc": 47.0, "val_loss": 17.309890687465668, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.0774154663086, "training_acc": 53.0, "val_loss": 17.401057481765747, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.75538301467896, "training_acc": 53.0, "val_loss": 17.48337149620056, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.50541734695435, "training_acc": 53.0, "val_loss": 17.36810952425003, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.3152232170105, "training_acc": 53.0, "val_loss": 17.337635159492493, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.18109321594238, "training_acc": 53.0, "val_loss": 17.31228530406952, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.20792245864868, "training_acc": 53.0, "val_loss": 17.365162074565887, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.7586555480957, "training_acc": 47.0, "val_loss": 17.40361750125885, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.58483338356018, "training_acc": 47.0, "val_loss": 17.30930656194687, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.29730272293091, "training_acc": 53.0, "val_loss": 17.34602302312851, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.216796875, "training_acc": 53.0, "val_loss": 17.341944575309753, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.2623655796051, "training_acc": 53.0, "val_loss": 17.314542829990387, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.15207624435425, "training_acc": 53.0, "val_loss": 17.31242537498474, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.17515897750854, "training_acc": 53.0, "val_loss": 17.32119917869568, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.15011072158813, "training_acc": 53.0, "val_loss": 17.31298863887787, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.14089870452881, "training_acc": 53.0, "val_loss": 17.32419580221176, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.18526792526245, "training_acc": 53.0, "val_loss": 17.340348660945892, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.15223264694214, "training_acc": 53.0, "val_loss": 17.317964136600494, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.14636015892029, "training_acc": 53.0, "val_loss": 17.314918339252472, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.17571496963501, "training_acc": 53.0, "val_loss": 17.335687577724457, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.17787218093872, "training_acc": 53.0, "val_loss": 17.335182428359985, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.23091411590576, "training_acc": 53.0, "val_loss": 17.4092099070549, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.48292589187622, "training_acc": 53.0, "val_loss": 17.535080015659332, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.94975328445435, "training_acc": 53.0, "val_loss": 17.56311058998108, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.88234186172485, "training_acc": 53.0, "val_loss": 17.53923147916794, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.76650643348694, "training_acc": 53.0, "val_loss": 17.480401694774628, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.61354494094849, "training_acc": 53.0, "val_loss": 17.39802062511444, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.33377027511597, "training_acc": 53.0, "val_loss": 17.38293468952179, "val_acc": 52.0}
