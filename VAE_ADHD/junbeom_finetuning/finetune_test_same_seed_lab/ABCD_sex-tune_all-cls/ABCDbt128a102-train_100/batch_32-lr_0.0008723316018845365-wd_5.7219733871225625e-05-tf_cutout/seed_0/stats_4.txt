"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 84.02231979370117, "training_acc": 40.0, "val_loss": 522.1813678741455, "val_acc": 48.0}
{"epoch": 1, "training_loss": 662.899209022522, "training_acc": 53.0, "val_loss": 21.09144926071167, "val_acc": 48.0}
{"epoch": 2, "training_loss": 81.74493622779846, "training_acc": 53.0, "val_loss": 17.977257072925568, "val_acc": 52.0}
{"epoch": 3, "training_loss": 72.2889518737793, "training_acc": 53.0, "val_loss": 17.387838661670685, "val_acc": 52.0}
{"epoch": 4, "training_loss": 70.307199716568, "training_acc": 45.0, "val_loss": 17.419852316379547, "val_acc": 52.0}
{"epoch": 5, "training_loss": 70.0864839553833, "training_acc": 47.0, "val_loss": 17.37934499979019, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.2498390674591, "training_acc": 49.0, "val_loss": 17.68360435962677, "val_acc": 52.0}
{"epoch": 7, "training_loss": 74.66866683959961, "training_acc": 53.0, "val_loss": 17.98821985721588, "val_acc": 52.0}
{"epoch": 8, "training_loss": 70.29726457595825, "training_acc": 53.0, "val_loss": 17.3403799533844, "val_acc": 52.0}
{"epoch": 9, "training_loss": 71.37036800384521, "training_acc": 47.0, "val_loss": 17.516450583934784, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.87474393844604, "training_acc": 47.0, "val_loss": 17.666251957416534, "val_acc": 52.0}
{"epoch": 11, "training_loss": 71.69157862663269, "training_acc": 53.0, "val_loss": 17.648887634277344, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.81909465789795, "training_acc": 53.0, "val_loss": 17.31226146221161, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.68248224258423, "training_acc": 47.0, "val_loss": 17.319202423095703, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.01916408538818, "training_acc": 53.0, "val_loss": 17.506328225135803, "val_acc": 52.0}
{"epoch": 15, "training_loss": 70.51432800292969, "training_acc": 53.0, "val_loss": 17.447760701179504, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.37584447860718, "training_acc": 53.0, "val_loss": 17.32363849878311, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.2324812412262, "training_acc": 53.0, "val_loss": 17.315059900283813, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.20840954780579, "training_acc": 53.0, "val_loss": 17.36309826374054, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.219318151474, "training_acc": 53.0, "val_loss": 17.314134538173676, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.36776161193848, "training_acc": 49.0, "val_loss": 17.309650778770447, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.09466505050659, "training_acc": 53.0, "val_loss": 17.447741329669952, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.89387536048889, "training_acc": 53.0, "val_loss": 17.684344947338104, "val_acc": 52.0}
{"epoch": 23, "training_loss": 70.28523015975952, "training_acc": 53.0, "val_loss": 17.600424587726593, "val_acc": 52.0}
{"epoch": 24, "training_loss": 70.02243185043335, "training_acc": 53.0, "val_loss": 17.462657392024994, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.4254834651947, "training_acc": 53.0, "val_loss": 17.34268218278885, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.26145505905151, "training_acc": 53.0, "val_loss": 17.32151210308075, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.23499727249146, "training_acc": 53.0, "val_loss": 17.31891632080078, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.18473768234253, "training_acc": 53.0, "val_loss": 17.309005558490753, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.11210060119629, "training_acc": 53.0, "val_loss": 17.339864373207092, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.41941499710083, "training_acc": 47.0, "val_loss": 17.351695895195007, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.53701686859131, "training_acc": 47.0, "val_loss": 17.369243502616882, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.42440462112427, "training_acc": 45.0, "val_loss": 17.321664094924927, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.3912615776062, "training_acc": 53.0, "val_loss": 17.320150136947632, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.40057229995728, "training_acc": 47.0, "val_loss": 17.34233945608139, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.31861758232117, "training_acc": 53.0, "val_loss": 17.314088344573975, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.22143006324768, "training_acc": 53.0, "val_loss": 17.30923056602478, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.08565139770508, "training_acc": 53.0, "val_loss": 17.316964268684387, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.09460997581482, "training_acc": 53.0, "val_loss": 17.37956553697586, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.44214677810669, "training_acc": 53.0, "val_loss": 17.433129251003265, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.29458427429199, "training_acc": 53.0, "val_loss": 17.345428466796875, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.12185311317444, "training_acc": 53.0, "val_loss": 17.321579158306122, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.1775918006897, "training_acc": 53.0, "val_loss": 17.323236167430878, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.19080686569214, "training_acc": 53.0, "val_loss": 17.320404946804047, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.13067626953125, "training_acc": 53.0, "val_loss": 17.313754558563232, "val_acc": 52.0}
{"epoch": 45, "training_loss": 69.28054094314575, "training_acc": 53.0, "val_loss": 17.311282455921173, "val_acc": 52.0}
{"epoch": 46, "training_loss": 69.16127252578735, "training_acc": 53.0, "val_loss": 17.336003482341766, "val_acc": 52.0}
{"epoch": 47, "training_loss": 69.15322041511536, "training_acc": 53.0, "val_loss": 17.38666743040085, "val_acc": 52.0}
