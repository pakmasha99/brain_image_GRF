"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 70.85903716087341, "training_acc": 54.0, "val_loss": 39.29054141044617, "val_acc": 44.0}
{"epoch": 1, "training_loss": 156.9316120147705, "training_acc": 48.0, "val_loss": 76.61291360855103, "val_acc": 56.0}
{"epoch": 2, "training_loss": 162.9856653213501, "training_acc": 41.0, "val_loss": 27.858051657676697, "val_acc": 56.0}
{"epoch": 3, "training_loss": 87.59248065948486, "training_acc": 54.0, "val_loss": 27.164730429649353, "val_acc": 44.0}
{"epoch": 4, "training_loss": 81.21429014205933, "training_acc": 50.0, "val_loss": 19.533643126487732, "val_acc": 56.0}
{"epoch": 5, "training_loss": 75.97680044174194, "training_acc": 52.0, "val_loss": 17.816922068595886, "val_acc": 56.0}
{"epoch": 6, "training_loss": 72.2014217376709, "training_acc": 48.0, "val_loss": 17.535948753356934, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.0404794216156, "training_acc": 52.0, "val_loss": 17.149314284324646, "val_acc": 56.0}
{"epoch": 8, "training_loss": 70.85231518745422, "training_acc": 52.0, "val_loss": 17.226532101631165, "val_acc": 56.0}
{"epoch": 9, "training_loss": 70.3973777294159, "training_acc": 42.0, "val_loss": 18.150128424167633, "val_acc": 56.0}
{"epoch": 10, "training_loss": 72.16691541671753, "training_acc": 48.0, "val_loss": 18.01845133304596, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.62290239334106, "training_acc": 48.0, "val_loss": 17.200125753879547, "val_acc": 56.0}
{"epoch": 12, "training_loss": 73.39892864227295, "training_acc": 52.0, "val_loss": 17.553605139255524, "val_acc": 56.0}
{"epoch": 13, "training_loss": 71.95989632606506, "training_acc": 52.0, "val_loss": 17.149588465690613, "val_acc": 56.0}
{"epoch": 14, "training_loss": 69.51990723609924, "training_acc": 52.0, "val_loss": 17.191696166992188, "val_acc": 56.0}
{"epoch": 15, "training_loss": 69.4707555770874, "training_acc": 48.0, "val_loss": 17.220424115657806, "val_acc": 56.0}
{"epoch": 16, "training_loss": 69.69544267654419, "training_acc": 52.0, "val_loss": 17.148619890213013, "val_acc": 56.0}
{"epoch": 17, "training_loss": 69.43731617927551, "training_acc": 52.0, "val_loss": 17.21983253955841, "val_acc": 56.0}
{"epoch": 18, "training_loss": 69.14526319503784, "training_acc": 51.0, "val_loss": 17.50801056623459, "val_acc": 56.0}
{"epoch": 19, "training_loss": 69.9162278175354, "training_acc": 48.0, "val_loss": 17.690829932689667, "val_acc": 56.0}
{"epoch": 20, "training_loss": 69.77022957801819, "training_acc": 48.0, "val_loss": 17.360900342464447, "val_acc": 56.0}
{"epoch": 21, "training_loss": 69.21920347213745, "training_acc": 52.0, "val_loss": 17.204536497592926, "val_acc": 56.0}
{"epoch": 22, "training_loss": 69.50037932395935, "training_acc": 52.0, "val_loss": 17.17562973499298, "val_acc": 56.0}
{"epoch": 23, "training_loss": 69.24194478988647, "training_acc": 52.0, "val_loss": 17.229652404785156, "val_acc": 56.0}
{"epoch": 24, "training_loss": 69.46672058105469, "training_acc": 52.0, "val_loss": 17.177097499370575, "val_acc": 56.0}
{"epoch": 25, "training_loss": 69.2787389755249, "training_acc": 52.0, "val_loss": 17.228779196739197, "val_acc": 56.0}
{"epoch": 26, "training_loss": 69.23963832855225, "training_acc": 52.0, "val_loss": 17.26008504629135, "val_acc": 56.0}
{"epoch": 27, "training_loss": 69.23030185699463, "training_acc": 52.0, "val_loss": 17.30048954486847, "val_acc": 56.0}
{"epoch": 28, "training_loss": 69.30884194374084, "training_acc": 52.0, "val_loss": 17.295728623867035, "val_acc": 56.0}
{"epoch": 29, "training_loss": 69.24336647987366, "training_acc": 52.0, "val_loss": 17.19270646572113, "val_acc": 56.0}
{"epoch": 30, "training_loss": 69.6822464466095, "training_acc": 52.0, "val_loss": 17.16427206993103, "val_acc": 56.0}
{"epoch": 31, "training_loss": 69.87196063995361, "training_acc": 52.0, "val_loss": 17.22961813211441, "val_acc": 56.0}
{"epoch": 32, "training_loss": 70.90287065505981, "training_acc": 52.0, "val_loss": 17.351680994033813, "val_acc": 56.0}
{"epoch": 33, "training_loss": 71.28260469436646, "training_acc": 52.0, "val_loss": 17.2408327460289, "val_acc": 56.0}
{"epoch": 34, "training_loss": 69.93698191642761, "training_acc": 52.0, "val_loss": 17.162685096263885, "val_acc": 56.0}
{"epoch": 35, "training_loss": 69.40631079673767, "training_acc": 50.0, "val_loss": 17.346127331256866, "val_acc": 56.0}
