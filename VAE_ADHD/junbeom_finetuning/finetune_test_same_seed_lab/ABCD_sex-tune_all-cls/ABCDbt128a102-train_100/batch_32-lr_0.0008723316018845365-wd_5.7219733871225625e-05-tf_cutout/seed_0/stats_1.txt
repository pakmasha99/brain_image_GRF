"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 76.19866752624512, "training_acc": 50.0, "val_loss": 1017.5881385803223, "val_acc": 48.0}
{"epoch": 1, "training_loss": 1497.3399982452393, "training_acc": 50.0, "val_loss": 77.13626027107239, "val_acc": 52.0}
{"epoch": 2, "training_loss": 150.14647102355957, "training_acc": 51.0, "val_loss": 18.284596502780914, "val_acc": 52.0}
{"epoch": 3, "training_loss": 72.35973405838013, "training_acc": 57.0, "val_loss": 17.520780861377716, "val_acc": 52.0}
{"epoch": 4, "training_loss": 79.86303853988647, "training_acc": 47.0, "val_loss": 17.582832276821136, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.76652455329895, "training_acc": 51.0, "val_loss": 17.327766120433807, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.04309320449829, "training_acc": 53.0, "val_loss": 17.49184876680374, "val_acc": 52.0}
{"epoch": 7, "training_loss": 70.1360273361206, "training_acc": 53.0, "val_loss": 17.39194095134735, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.38614177703857, "training_acc": 53.0, "val_loss": 17.344021797180176, "val_acc": 52.0}
{"epoch": 9, "training_loss": 71.83374214172363, "training_acc": 39.0, "val_loss": 17.362448573112488, "val_acc": 52.0}
{"epoch": 10, "training_loss": 68.7063376903534, "training_acc": 55.0, "val_loss": 17.57378727197647, "val_acc": 52.0}
{"epoch": 11, "training_loss": 70.18064022064209, "training_acc": 53.0, "val_loss": 17.30303466320038, "val_acc": 52.0}
{"epoch": 12, "training_loss": 70.88672232627869, "training_acc": 39.0, "val_loss": 17.336010932922363, "val_acc": 52.0}
{"epoch": 13, "training_loss": 68.74427890777588, "training_acc": 54.0, "val_loss": 17.567168176174164, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.95646357536316, "training_acc": 53.0, "val_loss": 17.396903038024902, "val_acc": 52.0}
{"epoch": 15, "training_loss": 71.90748524665833, "training_acc": 47.0, "val_loss": 18.016280233860016, "val_acc": 52.0}
{"epoch": 16, "training_loss": 71.69420099258423, "training_acc": 47.0, "val_loss": 17.302800714969635, "val_acc": 52.0}
{"epoch": 17, "training_loss": 70.9278359413147, "training_acc": 53.0, "val_loss": 18.11123639345169, "val_acc": 52.0}
{"epoch": 18, "training_loss": 72.1017472743988, "training_acc": 53.0, "val_loss": 17.369039356708527, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.1645393371582, "training_acc": 53.0, "val_loss": 17.424491047859192, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.80962538719177, "training_acc": 45.0, "val_loss": 17.33122617006302, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.31441926956177, "training_acc": 51.0, "val_loss": 17.323985695838928, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.32302117347717, "training_acc": 53.0, "val_loss": 17.314913868904114, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.09140253067017, "training_acc": 53.0, "val_loss": 17.376916110515594, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.54661417007446, "training_acc": 53.0, "val_loss": 17.319035530090332, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.15083026885986, "training_acc": 53.0, "val_loss": 17.322689294815063, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.60750651359558, "training_acc": 53.0, "val_loss": 17.318694293498993, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.27344107627869, "training_acc": 53.0, "val_loss": 17.326252162456512, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.28758668899536, "training_acc": 51.0, "val_loss": 17.360955476760864, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.56756973266602, "training_acc": 47.0, "val_loss": 17.36120581626892, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.54631114006042, "training_acc": 41.0, "val_loss": 17.31720119714737, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.19823694229126, "training_acc": 53.0, "val_loss": 17.318861186504364, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.06936359405518, "training_acc": 53.0, "val_loss": 17.362655699253082, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.22226166725159, "training_acc": 53.0, "val_loss": 17.397132515907288, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.30610370635986, "training_acc": 53.0, "val_loss": 17.449980974197388, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.56992363929749, "training_acc": 53.0, "val_loss": 17.494766414165497, "val_acc": 52.0}
