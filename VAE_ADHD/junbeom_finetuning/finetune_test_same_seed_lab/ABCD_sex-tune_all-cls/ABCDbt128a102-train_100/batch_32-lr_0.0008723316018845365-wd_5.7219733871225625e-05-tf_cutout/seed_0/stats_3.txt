"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 84.16277551651001, "training_acc": 48.0, "val_loss": 44.38257813453674, "val_acc": 48.0}
{"epoch": 1, "training_loss": 1412.2631530761719, "training_acc": 51.0, "val_loss": 91.00651144981384, "val_acc": 48.0}
{"epoch": 2, "training_loss": 181.89744782447815, "training_acc": 47.0, "val_loss": 17.44285225868225, "val_acc": 52.0}
{"epoch": 3, "training_loss": 70.3770489692688, "training_acc": 47.0, "val_loss": 17.573226988315582, "val_acc": 52.0}
{"epoch": 4, "training_loss": 71.79866790771484, "training_acc": 47.0, "val_loss": 17.602983117103577, "val_acc": 52.0}
{"epoch": 5, "training_loss": 74.31410026550293, "training_acc": 47.0, "val_loss": 17.564071714878082, "val_acc": 52.0}
{"epoch": 6, "training_loss": 70.80718207359314, "training_acc": 53.0, "val_loss": 17.490339279174805, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.23882865905762, "training_acc": 53.0, "val_loss": 17.402786016464233, "val_acc": 52.0}
{"epoch": 8, "training_loss": 70.61248564720154, "training_acc": 47.0, "val_loss": 17.786428332328796, "val_acc": 52.0}
{"epoch": 9, "training_loss": 71.5268144607544, "training_acc": 47.0, "val_loss": 17.337678372859955, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.45986652374268, "training_acc": 53.0, "val_loss": 17.30589270591736, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.12216138839722, "training_acc": 53.0, "val_loss": 17.40959882736206, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.37320351600647, "training_acc": 53.0, "val_loss": 17.33875572681427, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.36574625968933, "training_acc": 53.0, "val_loss": 17.317454516887665, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.58315706253052, "training_acc": 53.0, "val_loss": 17.578430473804474, "val_acc": 52.0}
{"epoch": 15, "training_loss": 70.53770971298218, "training_acc": 53.0, "val_loss": 17.41381287574768, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.6510717868805, "training_acc": 55.0, "val_loss": 17.41534173488617, "val_acc": 52.0}
{"epoch": 17, "training_loss": 70.10064792633057, "training_acc": 47.0, "val_loss": 17.59244054555893, "val_acc": 52.0}
{"epoch": 18, "training_loss": 70.49009895324707, "training_acc": 47.0, "val_loss": 17.339496314525604, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.0465338230133, "training_acc": 57.0, "val_loss": 17.469123005867004, "val_acc": 52.0}
{"epoch": 20, "training_loss": 70.36470556259155, "training_acc": 53.0, "val_loss": 17.621728777885437, "val_acc": 52.0}
{"epoch": 21, "training_loss": 70.05511045455933, "training_acc": 53.0, "val_loss": 17.50600039958954, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.89314317703247, "training_acc": 53.0, "val_loss": 17.386706173419952, "val_acc": 52.0}
{"epoch": 23, "training_loss": 68.96166467666626, "training_acc": 53.0, "val_loss": 17.317786812782288, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.34615325927734, "training_acc": 53.0, "val_loss": 17.313571274280548, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.44967603683472, "training_acc": 45.0, "val_loss": 17.351670563220978, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.45825457572937, "training_acc": 47.0, "val_loss": 17.34890341758728, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.54681539535522, "training_acc": 47.0, "val_loss": 17.411750555038452, "val_acc": 52.0}
{"epoch": 28, "training_loss": 70.36181020736694, "training_acc": 47.0, "val_loss": 17.503440380096436, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.94375491142273, "training_acc": 47.0, "val_loss": 17.341388761997223, "val_acc": 52.0}
