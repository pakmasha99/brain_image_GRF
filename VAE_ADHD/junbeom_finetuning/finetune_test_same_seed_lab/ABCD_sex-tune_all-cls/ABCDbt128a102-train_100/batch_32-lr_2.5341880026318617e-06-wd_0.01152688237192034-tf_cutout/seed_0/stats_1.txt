"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 72.0349793434143, "training_acc": 48.0, "val_loss": 17.483556270599365, "val_acc": 44.0}
{"epoch": 1, "training_loss": 73.41624093055725, "training_acc": 38.0, "val_loss": 17.329344153404236, "val_acc": 44.0}
{"epoch": 2, "training_loss": 72.27174210548401, "training_acc": 49.0, "val_loss": 17.390021681785583, "val_acc": 44.0}
{"epoch": 3, "training_loss": 71.67250442504883, "training_acc": 49.0, "val_loss": 17.42665022611618, "val_acc": 48.0}
{"epoch": 4, "training_loss": 70.50394439697266, "training_acc": 55.0, "val_loss": 17.34892576932907, "val_acc": 56.0}
{"epoch": 5, "training_loss": 69.97518801689148, "training_acc": 51.0, "val_loss": 17.212297022342682, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.88332200050354, "training_acc": 53.0, "val_loss": 17.205893993377686, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.24822998046875, "training_acc": 54.0, "val_loss": 17.234784364700317, "val_acc": 56.0}
{"epoch": 8, "training_loss": 69.3514256477356, "training_acc": 56.0, "val_loss": 17.246869206428528, "val_acc": 56.0}
{"epoch": 9, "training_loss": 69.25615310668945, "training_acc": 54.0, "val_loss": 17.174312472343445, "val_acc": 56.0}
{"epoch": 10, "training_loss": 68.82117795944214, "training_acc": 58.0, "val_loss": 17.145122587680817, "val_acc": 56.0}
{"epoch": 11, "training_loss": 68.26372241973877, "training_acc": 56.0, "val_loss": 17.160920798778534, "val_acc": 52.0}
{"epoch": 12, "training_loss": 67.97807192802429, "training_acc": 54.0, "val_loss": 17.2016903758049, "val_acc": 52.0}
{"epoch": 13, "training_loss": 68.20249724388123, "training_acc": 52.0, "val_loss": 17.282448709011078, "val_acc": 48.0}
{"epoch": 14, "training_loss": 67.99498701095581, "training_acc": 54.0, "val_loss": 17.413444817066193, "val_acc": 52.0}
{"epoch": 15, "training_loss": 68.08036422729492, "training_acc": 54.0, "val_loss": 17.37937331199646, "val_acc": 52.0}
{"epoch": 16, "training_loss": 68.03026056289673, "training_acc": 57.0, "val_loss": 17.33364462852478, "val_acc": 48.0}
{"epoch": 17, "training_loss": 67.30592203140259, "training_acc": 59.0, "val_loss": 17.354662716388702, "val_acc": 48.0}
{"epoch": 18, "training_loss": 66.68631029129028, "training_acc": 59.0, "val_loss": 17.33626276254654, "val_acc": 52.0}
{"epoch": 19, "training_loss": 66.44937419891357, "training_acc": 59.0, "val_loss": 17.35793501138687, "val_acc": 48.0}
{"epoch": 20, "training_loss": 67.01036262512207, "training_acc": 58.0, "val_loss": 17.349925637245178, "val_acc": 48.0}
{"epoch": 21, "training_loss": 66.16169309616089, "training_acc": 62.0, "val_loss": 17.314502596855164, "val_acc": 48.0}
{"epoch": 22, "training_loss": 65.84308195114136, "training_acc": 62.0, "val_loss": 17.383721470832825, "val_acc": 48.0}
{"epoch": 23, "training_loss": 66.38505387306213, "training_acc": 58.0, "val_loss": 17.43980050086975, "val_acc": 52.0}
{"epoch": 24, "training_loss": 65.93512606620789, "training_acc": 60.0, "val_loss": 17.48739928007126, "val_acc": 52.0}
{"epoch": 25, "training_loss": 65.36610698699951, "training_acc": 61.0, "val_loss": 17.518478631973267, "val_acc": 52.0}
{"epoch": 26, "training_loss": 65.44154262542725, "training_acc": 59.0, "val_loss": 17.538748681545258, "val_acc": 52.0}
{"epoch": 27, "training_loss": 64.98013496398926, "training_acc": 60.0, "val_loss": 17.54896342754364, "val_acc": 52.0}
{"epoch": 28, "training_loss": 65.1671781539917, "training_acc": 59.0, "val_loss": 17.50093102455139, "val_acc": 52.0}
{"epoch": 29, "training_loss": 64.8562707901001, "training_acc": 62.0, "val_loss": 17.50205159187317, "val_acc": 52.0}
