"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 70.06372475624084, "training_acc": 51.0, "val_loss": 19.072160124778748, "val_acc": 44.0}
{"epoch": 1, "training_loss": 73.46017146110535, "training_acc": 51.0, "val_loss": 18.860991299152374, "val_acc": 48.0}
{"epoch": 2, "training_loss": 72.37927484512329, "training_acc": 53.0, "val_loss": 18.24907213449478, "val_acc": 52.0}
{"epoch": 3, "training_loss": 71.0805435180664, "training_acc": 57.0, "val_loss": 17.52527505159378, "val_acc": 56.0}
{"epoch": 4, "training_loss": 69.71337938308716, "training_acc": 58.0, "val_loss": 17.3766627907753, "val_acc": 60.0}
{"epoch": 5, "training_loss": 68.84050846099854, "training_acc": 58.0, "val_loss": 17.506036162376404, "val_acc": 52.0}
{"epoch": 6, "training_loss": 67.0124409198761, "training_acc": 56.0, "val_loss": 17.504827678203583, "val_acc": 56.0}
{"epoch": 7, "training_loss": 67.62251687049866, "training_acc": 58.0, "val_loss": 17.512355744838715, "val_acc": 56.0}
{"epoch": 8, "training_loss": 66.74223136901855, "training_acc": 60.0, "val_loss": 17.561504244804382, "val_acc": 56.0}
{"epoch": 9, "training_loss": 67.09651470184326, "training_acc": 62.0, "val_loss": 17.529882490634918, "val_acc": 52.0}
{"epoch": 10, "training_loss": 65.93883538246155, "training_acc": 60.0, "val_loss": 17.486746609210968, "val_acc": 52.0}
{"epoch": 11, "training_loss": 66.1547441482544, "training_acc": 61.0, "val_loss": 17.38920509815216, "val_acc": 52.0}
{"epoch": 12, "training_loss": 66.29281830787659, "training_acc": 62.0, "val_loss": 17.3286035656929, "val_acc": 52.0}
{"epoch": 13, "training_loss": 66.28090381622314, "training_acc": 63.0, "val_loss": 17.406143248081207, "val_acc": 52.0}
{"epoch": 14, "training_loss": 65.3060073852539, "training_acc": 65.0, "val_loss": 17.476342618465424, "val_acc": 56.0}
{"epoch": 15, "training_loss": 65.3740541934967, "training_acc": 64.0, "val_loss": 17.495720088481903, "val_acc": 56.0}
{"epoch": 16, "training_loss": 65.35799074172974, "training_acc": 66.0, "val_loss": 17.454680800437927, "val_acc": 56.0}
{"epoch": 17, "training_loss": 64.89030361175537, "training_acc": 65.0, "val_loss": 17.40258038043976, "val_acc": 56.0}
{"epoch": 18, "training_loss": 65.19972372055054, "training_acc": 64.0, "val_loss": 17.47583895921707, "val_acc": 56.0}
{"epoch": 19, "training_loss": 64.85872054100037, "training_acc": 63.0, "val_loss": 17.511847615242004, "val_acc": 56.0}
{"epoch": 20, "training_loss": 64.33777236938477, "training_acc": 66.0, "val_loss": 17.410850524902344, "val_acc": 56.0}
{"epoch": 21, "training_loss": 65.11518573760986, "training_acc": 66.0, "val_loss": 17.38111674785614, "val_acc": 56.0}
{"epoch": 22, "training_loss": 64.50413227081299, "training_acc": 68.0, "val_loss": 17.336799204349518, "val_acc": 56.0}
{"epoch": 23, "training_loss": 63.67713975906372, "training_acc": 65.0, "val_loss": 17.356492578983307, "val_acc": 56.0}
{"epoch": 24, "training_loss": 63.95574498176575, "training_acc": 62.0, "val_loss": 17.413292825222015, "val_acc": 56.0}
{"epoch": 25, "training_loss": 62.90182161331177, "training_acc": 66.0, "val_loss": 17.380738258361816, "val_acc": 60.0}
{"epoch": 26, "training_loss": 63.39773368835449, "training_acc": 62.0, "val_loss": 17.32744723558426, "val_acc": 60.0}
{"epoch": 27, "training_loss": 62.124011516571045, "training_acc": 68.0, "val_loss": 17.298291623592377, "val_acc": 60.0}
{"epoch": 28, "training_loss": 62.698317527770996, "training_acc": 70.0, "val_loss": 17.240367829799652, "val_acc": 60.0}
{"epoch": 29, "training_loss": 62.285303592681885, "training_acc": 67.0, "val_loss": 17.24148839712143, "val_acc": 60.0}
{"epoch": 30, "training_loss": 61.767282009124756, "training_acc": 69.0, "val_loss": 17.17987060546875, "val_acc": 64.0}
{"epoch": 31, "training_loss": 61.67394304275513, "training_acc": 69.0, "val_loss": 17.057739198207855, "val_acc": 60.0}
{"epoch": 32, "training_loss": 62.796157360076904, "training_acc": 67.0, "val_loss": 17.05879122018814, "val_acc": 60.0}
{"epoch": 33, "training_loss": 62.59120845794678, "training_acc": 67.0, "val_loss": 17.134055495262146, "val_acc": 64.0}
{"epoch": 34, "training_loss": 61.99267053604126, "training_acc": 68.0, "val_loss": 17.153796553611755, "val_acc": 60.0}
{"epoch": 35, "training_loss": 61.92128610610962, "training_acc": 64.0, "val_loss": 17.185187339782715, "val_acc": 60.0}
{"epoch": 36, "training_loss": 60.44795274734497, "training_acc": 70.0, "val_loss": 17.284999787807465, "val_acc": 60.0}
{"epoch": 37, "training_loss": 61.00343680381775, "training_acc": 69.0, "val_loss": 17.445063591003418, "val_acc": 64.0}
{"epoch": 38, "training_loss": 60.71210765838623, "training_acc": 65.0, "val_loss": 17.51631647348404, "val_acc": 64.0}
{"epoch": 39, "training_loss": 59.93394422531128, "training_acc": 68.0, "val_loss": 17.66098588705063, "val_acc": 60.0}
{"epoch": 40, "training_loss": 59.81289577484131, "training_acc": 71.0, "val_loss": 17.72901862859726, "val_acc": 60.0}
{"epoch": 41, "training_loss": 59.769407749176025, "training_acc": 70.0, "val_loss": 17.82252788543701, "val_acc": 60.0}
{"epoch": 42, "training_loss": 60.16016960144043, "training_acc": 69.0, "val_loss": 17.9432675242424, "val_acc": 60.0}
{"epoch": 43, "training_loss": 60.878591537475586, "training_acc": 70.0, "val_loss": 17.848964035511017, "val_acc": 60.0}
{"epoch": 44, "training_loss": 59.18606758117676, "training_acc": 69.0, "val_loss": 17.635028064250946, "val_acc": 60.0}
{"epoch": 45, "training_loss": 59.03421998023987, "training_acc": 70.0, "val_loss": 17.645367980003357, "val_acc": 60.0}
{"epoch": 46, "training_loss": 59.203426003456116, "training_acc": 72.0, "val_loss": 17.792069911956787, "val_acc": 64.0}
{"epoch": 47, "training_loss": 60.99369025230408, "training_acc": 70.0, "val_loss": 17.763042449951172, "val_acc": 64.0}
{"epoch": 48, "training_loss": 59.124406814575195, "training_acc": 76.0, "val_loss": 17.610175907611847, "val_acc": 60.0}
{"epoch": 49, "training_loss": 58.76271986961365, "training_acc": 72.0, "val_loss": 17.637740075588226, "val_acc": 60.0}
{"epoch": 50, "training_loss": 59.03051710128784, "training_acc": 71.0, "val_loss": 17.585566639900208, "val_acc": 60.0}
