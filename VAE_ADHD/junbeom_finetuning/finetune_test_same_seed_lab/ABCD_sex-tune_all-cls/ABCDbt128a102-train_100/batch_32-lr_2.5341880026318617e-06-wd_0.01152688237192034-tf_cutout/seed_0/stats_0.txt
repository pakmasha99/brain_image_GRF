"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 71.67838072776794, "training_acc": 48.0, "val_loss": 18.738284707069397, "val_acc": 52.0}
{"epoch": 1, "training_loss": 74.43386769294739, "training_acc": 43.0, "val_loss": 18.44824105501175, "val_acc": 52.0}
{"epoch": 2, "training_loss": 72.6495361328125, "training_acc": 46.0, "val_loss": 18.299521505832672, "val_acc": 52.0}
{"epoch": 3, "training_loss": 71.02681827545166, "training_acc": 51.0, "val_loss": 18.329866230487823, "val_acc": 56.0}
{"epoch": 4, "training_loss": 70.1445529460907, "training_acc": 50.0, "val_loss": 18.33939254283905, "val_acc": 56.0}
{"epoch": 5, "training_loss": 69.4581389427185, "training_acc": 52.0, "val_loss": 18.24539303779602, "val_acc": 56.0}
{"epoch": 6, "training_loss": 68.7552342414856, "training_acc": 57.0, "val_loss": 18.06516945362091, "val_acc": 56.0}
{"epoch": 7, "training_loss": 67.70088171958923, "training_acc": 56.0, "val_loss": 17.815707623958588, "val_acc": 56.0}
{"epoch": 8, "training_loss": 67.72500658035278, "training_acc": 61.0, "val_loss": 17.6288440823555, "val_acc": 52.0}
{"epoch": 9, "training_loss": 67.60840821266174, "training_acc": 55.0, "val_loss": 18.032924830913544, "val_acc": 52.0}
{"epoch": 10, "training_loss": 66.84392786026001, "training_acc": 57.0, "val_loss": 18.17541867494583, "val_acc": 56.0}
{"epoch": 11, "training_loss": 66.53748512268066, "training_acc": 59.0, "val_loss": 18.16413849592209, "val_acc": 56.0}
{"epoch": 12, "training_loss": 66.19441366195679, "training_acc": 60.0, "val_loss": 18.157431483268738, "val_acc": 56.0}
{"epoch": 13, "training_loss": 66.29589748382568, "training_acc": 59.0, "val_loss": 18.183383345603943, "val_acc": 56.0}
{"epoch": 14, "training_loss": 66.2579779624939, "training_acc": 58.0, "val_loss": 18.173696100711823, "val_acc": 56.0}
{"epoch": 15, "training_loss": 66.3402190208435, "training_acc": 57.0, "val_loss": 17.989376187324524, "val_acc": 52.0}
{"epoch": 16, "training_loss": 65.56857824325562, "training_acc": 63.0, "val_loss": 17.530904710292816, "val_acc": 52.0}
{"epoch": 17, "training_loss": 65.96223783493042, "training_acc": 60.0, "val_loss": 17.456933856010437, "val_acc": 52.0}
{"epoch": 18, "training_loss": 65.6358368396759, "training_acc": 63.0, "val_loss": 17.444811761379242, "val_acc": 56.0}
{"epoch": 19, "training_loss": 65.14838886260986, "training_acc": 65.0, "val_loss": 17.71548241376877, "val_acc": 56.0}
{"epoch": 20, "training_loss": 64.70287990570068, "training_acc": 63.0, "val_loss": 17.995861172676086, "val_acc": 56.0}
{"epoch": 21, "training_loss": 64.59833145141602, "training_acc": 65.0, "val_loss": 18.02232265472412, "val_acc": 60.0}
{"epoch": 22, "training_loss": 65.4036431312561, "training_acc": 65.0, "val_loss": 18.0067777633667, "val_acc": 60.0}
{"epoch": 23, "training_loss": 64.6702094078064, "training_acc": 65.0, "val_loss": 17.978450655937195, "val_acc": 60.0}
{"epoch": 24, "training_loss": 64.24123692512512, "training_acc": 65.0, "val_loss": 17.597512900829315, "val_acc": 56.0}
{"epoch": 25, "training_loss": 64.15404868125916, "training_acc": 63.0, "val_loss": 17.53654032945633, "val_acc": 52.0}
{"epoch": 26, "training_loss": 63.72024655342102, "training_acc": 65.0, "val_loss": 17.61106252670288, "val_acc": 60.0}
{"epoch": 27, "training_loss": 63.55649995803833, "training_acc": 67.0, "val_loss": 17.760463058948517, "val_acc": 60.0}
{"epoch": 28, "training_loss": 63.339401721954346, "training_acc": 67.0, "val_loss": 17.884445190429688, "val_acc": 60.0}
{"epoch": 29, "training_loss": 62.59435319900513, "training_acc": 69.0, "val_loss": 18.160317838191986, "val_acc": 64.0}
{"epoch": 30, "training_loss": 63.11081075668335, "training_acc": 67.0, "val_loss": 18.10993105173111, "val_acc": 60.0}
{"epoch": 31, "training_loss": 61.80736994743347, "training_acc": 71.0, "val_loss": 18.203862011432648, "val_acc": 56.0}
{"epoch": 32, "training_loss": 62.25611639022827, "training_acc": 69.0, "val_loss": 18.41151714324951, "val_acc": 56.0}
{"epoch": 33, "training_loss": 62.28035759925842, "training_acc": 68.0, "val_loss": 18.557660281658173, "val_acc": 56.0}
{"epoch": 34, "training_loss": 62.27725601196289, "training_acc": 65.0, "val_loss": 18.79776269197464, "val_acc": 56.0}
{"epoch": 35, "training_loss": 61.458784103393555, "training_acc": 71.0, "val_loss": 19.057202339172363, "val_acc": 60.0}
{"epoch": 36, "training_loss": 61.354604721069336, "training_acc": 69.0, "val_loss": 19.121067225933075, "val_acc": 60.0}
{"epoch": 37, "training_loss": 62.49741554260254, "training_acc": 64.0, "val_loss": 19.17518526315689, "val_acc": 56.0}
