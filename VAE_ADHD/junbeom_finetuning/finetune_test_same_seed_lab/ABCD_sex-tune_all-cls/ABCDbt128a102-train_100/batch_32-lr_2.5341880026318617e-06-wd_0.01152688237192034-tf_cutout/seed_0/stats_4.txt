"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 70.20958685874939, "training_acc": 49.0, "val_loss": 18.652895092964172, "val_acc": 52.0}
{"epoch": 1, "training_loss": 71.46916389465332, "training_acc": 51.0, "val_loss": 18.615074455738068, "val_acc": 52.0}
{"epoch": 2, "training_loss": 70.95839881896973, "training_acc": 52.0, "val_loss": 18.518315255641937, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.12592077255249, "training_acc": 57.0, "val_loss": 18.413497507572174, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.07281041145325, "training_acc": 59.0, "val_loss": 18.316563963890076, "val_acc": 52.0}
{"epoch": 5, "training_loss": 68.24883651733398, "training_acc": 57.0, "val_loss": 18.362076580524445, "val_acc": 52.0}
{"epoch": 6, "training_loss": 67.6211588382721, "training_acc": 56.0, "val_loss": 18.349361419677734, "val_acc": 52.0}
{"epoch": 7, "training_loss": 67.38841438293457, "training_acc": 58.0, "val_loss": 18.365295231342316, "val_acc": 52.0}
{"epoch": 8, "training_loss": 66.86639928817749, "training_acc": 56.0, "val_loss": 18.320107460021973, "val_acc": 56.0}
{"epoch": 9, "training_loss": 66.20682096481323, "training_acc": 56.0, "val_loss": 18.235158920288086, "val_acc": 56.0}
{"epoch": 10, "training_loss": 66.69873857498169, "training_acc": 60.0, "val_loss": 18.18954050540924, "val_acc": 56.0}
{"epoch": 11, "training_loss": 66.45665383338928, "training_acc": 60.0, "val_loss": 18.178027868270874, "val_acc": 56.0}
{"epoch": 12, "training_loss": 65.70737051963806, "training_acc": 63.0, "val_loss": 18.150468170642853, "val_acc": 56.0}
{"epoch": 13, "training_loss": 65.83815097808838, "training_acc": 58.0, "val_loss": 18.104636669158936, "val_acc": 56.0}
{"epoch": 14, "training_loss": 66.06574201583862, "training_acc": 59.0, "val_loss": 18.08759719133377, "val_acc": 56.0}
{"epoch": 15, "training_loss": 65.79635381698608, "training_acc": 61.0, "val_loss": 18.122628331184387, "val_acc": 56.0}
{"epoch": 16, "training_loss": 65.98154258728027, "training_acc": 60.0, "val_loss": 18.16498190164566, "val_acc": 56.0}
{"epoch": 17, "training_loss": 65.47871971130371, "training_acc": 62.0, "val_loss": 18.22544038295746, "val_acc": 56.0}
{"epoch": 18, "training_loss": 65.54162931442261, "training_acc": 62.0, "val_loss": 18.31100732088089, "val_acc": 56.0}
{"epoch": 19, "training_loss": 65.14698719978333, "training_acc": 61.0, "val_loss": 18.420791625976562, "val_acc": 56.0}
{"epoch": 20, "training_loss": 65.13427782058716, "training_acc": 60.0, "val_loss": 18.43690276145935, "val_acc": 56.0}
{"epoch": 21, "training_loss": 65.09035587310791, "training_acc": 57.0, "val_loss": 18.36300641298294, "val_acc": 56.0}
{"epoch": 22, "training_loss": 64.78032231330872, "training_acc": 60.0, "val_loss": 18.28930228948593, "val_acc": 56.0}
{"epoch": 23, "training_loss": 63.944586992263794, "training_acc": 62.0, "val_loss": 18.2049959897995, "val_acc": 52.0}
{"epoch": 24, "training_loss": 64.76713275909424, "training_acc": 62.0, "val_loss": 18.20995807647705, "val_acc": 52.0}
{"epoch": 25, "training_loss": 64.31896758079529, "training_acc": 61.0, "val_loss": 18.334724009037018, "val_acc": 56.0}
{"epoch": 26, "training_loss": 63.76854062080383, "training_acc": 62.0, "val_loss": 18.469034135341644, "val_acc": 56.0}
{"epoch": 27, "training_loss": 64.47378253936768, "training_acc": 63.0, "val_loss": 18.585722148418427, "val_acc": 56.0}
{"epoch": 28, "training_loss": 64.05350399017334, "training_acc": 62.0, "val_loss": 18.647857010364532, "val_acc": 56.0}
{"epoch": 29, "training_loss": 63.56603121757507, "training_acc": 66.0, "val_loss": 18.59535425901413, "val_acc": 52.0}
{"epoch": 30, "training_loss": 63.133185625076294, "training_acc": 68.0, "val_loss": 18.418963253498077, "val_acc": 48.0}
{"epoch": 31, "training_loss": 63.62744998931885, "training_acc": 66.0, "val_loss": 18.235771358013153, "val_acc": 52.0}
{"epoch": 32, "training_loss": 64.11262226104736, "training_acc": 66.0, "val_loss": 18.10039132833481, "val_acc": 52.0}
{"epoch": 33, "training_loss": 63.39424180984497, "training_acc": 65.0, "val_loss": 18.00900548696518, "val_acc": 56.0}
{"epoch": 34, "training_loss": 63.14880633354187, "training_acc": 67.0, "val_loss": 18.07747185230255, "val_acc": 56.0}
{"epoch": 35, "training_loss": 63.225703954696655, "training_acc": 64.0, "val_loss": 18.158137798309326, "val_acc": 52.0}
{"epoch": 36, "training_loss": 63.234747648239136, "training_acc": 66.0, "val_loss": 18.26406419277191, "val_acc": 52.0}
{"epoch": 37, "training_loss": 62.33586835861206, "training_acc": 69.0, "val_loss": 18.37126910686493, "val_acc": 44.0}
{"epoch": 38, "training_loss": 62.8905553817749, "training_acc": 69.0, "val_loss": 18.419013917446136, "val_acc": 44.0}
{"epoch": 39, "training_loss": 62.370795249938965, "training_acc": 70.0, "val_loss": 18.413621187210083, "val_acc": 48.0}
{"epoch": 40, "training_loss": 62.2174072265625, "training_acc": 69.0, "val_loss": 18.402500450611115, "val_acc": 52.0}
{"epoch": 41, "training_loss": 63.08135175704956, "training_acc": 68.0, "val_loss": 18.389715254306793, "val_acc": 52.0}
{"epoch": 42, "training_loss": 61.52694511413574, "training_acc": 68.0, "val_loss": 18.4313103556633, "val_acc": 44.0}
{"epoch": 43, "training_loss": 62.290268421173096, "training_acc": 68.0, "val_loss": 18.23914796113968, "val_acc": 44.0}
{"epoch": 44, "training_loss": 63.122671365737915, "training_acc": 70.0, "val_loss": 18.39502304792404, "val_acc": 44.0}
{"epoch": 45, "training_loss": 61.716447591781616, "training_acc": 67.0, "val_loss": 18.41752976179123, "val_acc": 48.0}
{"epoch": 46, "training_loss": 60.806031942367554, "training_acc": 70.0, "val_loss": 18.514244258403778, "val_acc": 52.0}
{"epoch": 47, "training_loss": 60.51976466178894, "training_acc": 73.0, "val_loss": 18.589690327644348, "val_acc": 48.0}
{"epoch": 48, "training_loss": 60.25650608539581, "training_acc": 73.0, "val_loss": 18.565286695957184, "val_acc": 48.0}
{"epoch": 49, "training_loss": 60.34526252746582, "training_acc": 73.0, "val_loss": 18.531115353107452, "val_acc": 52.0}
{"epoch": 50, "training_loss": 61.04309105873108, "training_acc": 69.0, "val_loss": 18.708083033561707, "val_acc": 52.0}
{"epoch": 51, "training_loss": 61.43271446228027, "training_acc": 68.0, "val_loss": 18.746165931224823, "val_acc": 52.0}
{"epoch": 52, "training_loss": 60.89423680305481, "training_acc": 68.0, "val_loss": 18.693391978740692, "val_acc": 52.0}
