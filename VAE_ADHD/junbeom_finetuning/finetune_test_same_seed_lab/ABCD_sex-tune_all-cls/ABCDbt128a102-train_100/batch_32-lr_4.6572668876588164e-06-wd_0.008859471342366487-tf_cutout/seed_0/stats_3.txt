"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 73.31347942352295, "training_acc": 46.0, "val_loss": 17.138677835464478, "val_acc": 56.0}
{"epoch": 1, "training_loss": 72.00020551681519, "training_acc": 52.0, "val_loss": 17.31567084789276, "val_acc": 56.0}
{"epoch": 2, "training_loss": 70.43394565582275, "training_acc": 57.0, "val_loss": 17.21861958503723, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.40224146842957, "training_acc": 56.0, "val_loss": 17.383013665676117, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.2071635723114, "training_acc": 55.0, "val_loss": 17.449168860912323, "val_acc": 52.0}
{"epoch": 5, "training_loss": 68.28472256660461, "training_acc": 55.0, "val_loss": 17.45661050081253, "val_acc": 52.0}
{"epoch": 6, "training_loss": 67.30772662162781, "training_acc": 61.0, "val_loss": 17.370764911174774, "val_acc": 52.0}
{"epoch": 7, "training_loss": 67.03188419342041, "training_acc": 61.0, "val_loss": 17.407506704330444, "val_acc": 52.0}
{"epoch": 8, "training_loss": 67.2733097076416, "training_acc": 61.0, "val_loss": 17.521660029888153, "val_acc": 52.0}
{"epoch": 9, "training_loss": 67.26609396934509, "training_acc": 62.0, "val_loss": 17.43186265230179, "val_acc": 52.0}
{"epoch": 10, "training_loss": 66.88804745674133, "training_acc": 62.0, "val_loss": 17.38680750131607, "val_acc": 52.0}
{"epoch": 11, "training_loss": 66.23473072052002, "training_acc": 62.0, "val_loss": 17.29995757341385, "val_acc": 52.0}
{"epoch": 12, "training_loss": 65.51293659210205, "training_acc": 61.0, "val_loss": 17.153161764144897, "val_acc": 52.0}
{"epoch": 13, "training_loss": 66.14400577545166, "training_acc": 59.0, "val_loss": 17.115207016468048, "val_acc": 52.0}
{"epoch": 14, "training_loss": 65.30784893035889, "training_acc": 63.0, "val_loss": 17.163129150867462, "val_acc": 52.0}
{"epoch": 15, "training_loss": 65.62685823440552, "training_acc": 61.0, "val_loss": 17.190702259540558, "val_acc": 52.0}
{"epoch": 16, "training_loss": 65.20918369293213, "training_acc": 59.0, "val_loss": 17.23543107509613, "val_acc": 52.0}
{"epoch": 17, "training_loss": 65.6966757774353, "training_acc": 61.0, "val_loss": 17.243938148021698, "val_acc": 52.0}
{"epoch": 18, "training_loss": 64.03330254554749, "training_acc": 63.0, "val_loss": 17.163333296775818, "val_acc": 52.0}
{"epoch": 19, "training_loss": 64.22149109840393, "training_acc": 63.0, "val_loss": 17.09340214729309, "val_acc": 52.0}
{"epoch": 20, "training_loss": 64.18192601203918, "training_acc": 65.0, "val_loss": 17.122459411621094, "val_acc": 52.0}
{"epoch": 21, "training_loss": 63.562071084976196, "training_acc": 63.0, "val_loss": 17.169900238513947, "val_acc": 52.0}
{"epoch": 22, "training_loss": 64.57750511169434, "training_acc": 61.0, "val_loss": 17.10968017578125, "val_acc": 52.0}
{"epoch": 23, "training_loss": 63.871772050857544, "training_acc": 64.0, "val_loss": 17.330726981163025, "val_acc": 52.0}
{"epoch": 24, "training_loss": 63.690141916275024, "training_acc": 65.0, "val_loss": 17.354348301887512, "val_acc": 52.0}
{"epoch": 25, "training_loss": 63.32063055038452, "training_acc": 69.0, "val_loss": 17.508582770824432, "val_acc": 52.0}
{"epoch": 26, "training_loss": 62.82269251346588, "training_acc": 67.0, "val_loss": 17.304565012454987, "val_acc": 48.0}
{"epoch": 27, "training_loss": 62.90501165390015, "training_acc": 67.0, "val_loss": 17.186172306537628, "val_acc": 52.0}
{"epoch": 28, "training_loss": 62.26266098022461, "training_acc": 63.0, "val_loss": 17.275837063789368, "val_acc": 48.0}
{"epoch": 29, "training_loss": 63.514930725097656, "training_acc": 64.0, "val_loss": 17.377768456935883, "val_acc": 52.0}
{"epoch": 30, "training_loss": 62.68475532531738, "training_acc": 66.0, "val_loss": 17.165912687778473, "val_acc": 48.0}
{"epoch": 31, "training_loss": 62.0485634803772, "training_acc": 65.0, "val_loss": 17.171798646450043, "val_acc": 48.0}
{"epoch": 32, "training_loss": 63.311323165893555, "training_acc": 65.0, "val_loss": 17.067687213420868, "val_acc": 48.0}
{"epoch": 33, "training_loss": 60.83853769302368, "training_acc": 68.0, "val_loss": 17.02234596014023, "val_acc": 48.0}
{"epoch": 34, "training_loss": 63.15182566642761, "training_acc": 67.0, "val_loss": 17.167755961418152, "val_acc": 48.0}
{"epoch": 35, "training_loss": 62.38822031021118, "training_acc": 68.0, "val_loss": 17.070841789245605, "val_acc": 52.0}
{"epoch": 36, "training_loss": 60.335068464279175, "training_acc": 65.0, "val_loss": 17.160341143608093, "val_acc": 52.0}
{"epoch": 37, "training_loss": 61.77346968650818, "training_acc": 64.0, "val_loss": 17.55051016807556, "val_acc": 48.0}
{"epoch": 38, "training_loss": 60.27607488632202, "training_acc": 71.0, "val_loss": 17.60568916797638, "val_acc": 48.0}
{"epoch": 39, "training_loss": 61.19820296764374, "training_acc": 69.0, "val_loss": 17.368027567863464, "val_acc": 48.0}
{"epoch": 40, "training_loss": 60.59532833099365, "training_acc": 67.0, "val_loss": 17.247091233730316, "val_acc": 56.0}
{"epoch": 41, "training_loss": 58.87678909301758, "training_acc": 68.0, "val_loss": 17.035643756389618, "val_acc": 56.0}
{"epoch": 42, "training_loss": 59.92463493347168, "training_acc": 70.0, "val_loss": 16.981925070285797, "val_acc": 56.0}
{"epoch": 43, "training_loss": 59.963998317718506, "training_acc": 67.0, "val_loss": 17.20399558544159, "val_acc": 56.0}
{"epoch": 44, "training_loss": 59.86601996421814, "training_acc": 69.0, "val_loss": 17.56456196308136, "val_acc": 52.0}
{"epoch": 45, "training_loss": 58.69727373123169, "training_acc": 69.0, "val_loss": 17.44026094675064, "val_acc": 48.0}
{"epoch": 46, "training_loss": 59.876593351364136, "training_acc": 66.0, "val_loss": 16.91873073577881, "val_acc": 52.0}
{"epoch": 47, "training_loss": 59.31603217124939, "training_acc": 66.0, "val_loss": 16.631528735160828, "val_acc": 56.0}
{"epoch": 48, "training_loss": 58.452436208724976, "training_acc": 67.0, "val_loss": 17.237941920757294, "val_acc": 56.0}
{"epoch": 49, "training_loss": 58.079071044921875, "training_acc": 68.0, "val_loss": 17.334863543510437, "val_acc": 52.0}
{"epoch": 50, "training_loss": 57.57792127132416, "training_acc": 72.0, "val_loss": 17.384856939315796, "val_acc": 56.0}
{"epoch": 51, "training_loss": 59.76951718330383, "training_acc": 72.0, "val_loss": 17.521415650844574, "val_acc": 56.0}
{"epoch": 52, "training_loss": 57.07766532897949, "training_acc": 71.0, "val_loss": 17.866095900535583, "val_acc": 52.0}
{"epoch": 53, "training_loss": 57.5645055770874, "training_acc": 70.0, "val_loss": 17.747417092323303, "val_acc": 52.0}
{"epoch": 54, "training_loss": 56.5913782119751, "training_acc": 73.0, "val_loss": 16.725201904773712, "val_acc": 56.0}
{"epoch": 55, "training_loss": 57.500364780426025, "training_acc": 74.0, "val_loss": 16.86912477016449, "val_acc": 60.0}
{"epoch": 56, "training_loss": 57.657145977020264, "training_acc": 73.0, "val_loss": 17.369598150253296, "val_acc": 60.0}
{"epoch": 57, "training_loss": 57.02465772628784, "training_acc": 71.0, "val_loss": 17.347025871276855, "val_acc": 64.0}
{"epoch": 58, "training_loss": 56.39365768432617, "training_acc": 68.0, "val_loss": 17.493936419487, "val_acc": 52.0}
{"epoch": 59, "training_loss": 56.58254384994507, "training_acc": 73.0, "val_loss": 17.369525134563446, "val_acc": 56.0}
{"epoch": 60, "training_loss": 56.786052942276, "training_acc": 76.0, "val_loss": 17.162826657295227, "val_acc": 52.0}
{"epoch": 61, "training_loss": 55.02605199813843, "training_acc": 71.0, "val_loss": 17.778882384300232, "val_acc": 48.0}
{"epoch": 62, "training_loss": 55.94034147262573, "training_acc": 70.0, "val_loss": 18.133503198623657, "val_acc": 52.0}
{"epoch": 63, "training_loss": 56.11573123931885, "training_acc": 69.0, "val_loss": 17.78324395418167, "val_acc": 52.0}
{"epoch": 64, "training_loss": 53.45759963989258, "training_acc": 78.0, "val_loss": 16.911152005195618, "val_acc": 68.0}
{"epoch": 65, "training_loss": 56.73558235168457, "training_acc": 69.0, "val_loss": 17.561838030815125, "val_acc": 60.0}
{"epoch": 66, "training_loss": 54.74831795692444, "training_acc": 71.0, "val_loss": 17.819857597351074, "val_acc": 52.0}
