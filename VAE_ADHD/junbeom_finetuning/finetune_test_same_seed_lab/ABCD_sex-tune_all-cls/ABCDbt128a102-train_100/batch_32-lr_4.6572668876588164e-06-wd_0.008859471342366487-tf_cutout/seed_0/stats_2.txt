"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 71.47383785247803, "training_acc": 54.0, "val_loss": 17.854197323322296, "val_acc": 52.0}
{"epoch": 1, "training_loss": 70.91297388076782, "training_acc": 56.0, "val_loss": 17.827793955802917, "val_acc": 52.0}
{"epoch": 2, "training_loss": 68.9651426076889, "training_acc": 58.0, "val_loss": 17.82827377319336, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.21440744400024, "training_acc": 60.0, "val_loss": 17.823508381843567, "val_acc": 52.0}
{"epoch": 4, "training_loss": 67.73734426498413, "training_acc": 60.0, "val_loss": 17.532528936862946, "val_acc": 52.0}
{"epoch": 5, "training_loss": 67.08042526245117, "training_acc": 61.0, "val_loss": 17.42822676897049, "val_acc": 52.0}
{"epoch": 6, "training_loss": 67.32451438903809, "training_acc": 60.0, "val_loss": 17.375609278678894, "val_acc": 52.0}
{"epoch": 7, "training_loss": 66.78854560852051, "training_acc": 61.0, "val_loss": 17.45293438434601, "val_acc": 52.0}
{"epoch": 8, "training_loss": 65.57388210296631, "training_acc": 62.0, "val_loss": 17.806273698806763, "val_acc": 52.0}
{"epoch": 9, "training_loss": 65.57480549812317, "training_acc": 64.0, "val_loss": 17.915521562099457, "val_acc": 52.0}
{"epoch": 10, "training_loss": 65.19881248474121, "training_acc": 64.0, "val_loss": 17.65867918729782, "val_acc": 52.0}
{"epoch": 11, "training_loss": 64.38352918624878, "training_acc": 63.0, "val_loss": 17.566484212875366, "val_acc": 52.0}
{"epoch": 12, "training_loss": 63.54714298248291, "training_acc": 63.0, "val_loss": 17.689162492752075, "val_acc": 52.0}
{"epoch": 13, "training_loss": 63.46064805984497, "training_acc": 63.0, "val_loss": 17.772075533866882, "val_acc": 52.0}
{"epoch": 14, "training_loss": 63.50623941421509, "training_acc": 64.0, "val_loss": 17.902866005897522, "val_acc": 52.0}
{"epoch": 15, "training_loss": 63.39932870864868, "training_acc": 67.0, "val_loss": 18.129046261310577, "val_acc": 52.0}
{"epoch": 16, "training_loss": 62.89632534980774, "training_acc": 68.0, "val_loss": 18.04162561893463, "val_acc": 52.0}
{"epoch": 17, "training_loss": 62.05949330329895, "training_acc": 67.0, "val_loss": 17.733553051948547, "val_acc": 52.0}
{"epoch": 18, "training_loss": 62.3556432723999, "training_acc": 65.0, "val_loss": 17.608489096164703, "val_acc": 52.0}
{"epoch": 19, "training_loss": 63.05051565170288, "training_acc": 67.0, "val_loss": 17.782321572303772, "val_acc": 52.0}
{"epoch": 20, "training_loss": 61.99197864532471, "training_acc": 66.0, "val_loss": 18.19230616092682, "val_acc": 52.0}
{"epoch": 21, "training_loss": 60.753591656684875, "training_acc": 68.0, "val_loss": 18.653449416160583, "val_acc": 52.0}
{"epoch": 22, "training_loss": 61.567484617233276, "training_acc": 66.0, "val_loss": 18.794529139995575, "val_acc": 52.0}
{"epoch": 23, "training_loss": 60.51920032501221, "training_acc": 69.0, "val_loss": 18.831826746463776, "val_acc": 48.0}
{"epoch": 24, "training_loss": 60.820218324661255, "training_acc": 68.0, "val_loss": 18.610458076000214, "val_acc": 48.0}
{"epoch": 25, "training_loss": 60.16839933395386, "training_acc": 67.0, "val_loss": 18.02356094121933, "val_acc": 52.0}
