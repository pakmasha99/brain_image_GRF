"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 71.31647372245789, "training_acc": 52.0, "val_loss": 18.060781061649323, "val_acc": 52.0}
{"epoch": 1, "training_loss": 70.20508193969727, "training_acc": 58.0, "val_loss": 18.01002025604248, "val_acc": 52.0}
{"epoch": 2, "training_loss": 68.58961915969849, "training_acc": 57.0, "val_loss": 17.657314240932465, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.34061241149902, "training_acc": 51.0, "val_loss": 17.410019040107727, "val_acc": 56.0}
{"epoch": 4, "training_loss": 67.89522171020508, "training_acc": 56.0, "val_loss": 17.230582237243652, "val_acc": 56.0}
{"epoch": 5, "training_loss": 67.40231680870056, "training_acc": 60.0, "val_loss": 17.09808111190796, "val_acc": 56.0}
{"epoch": 6, "training_loss": 66.82855796813965, "training_acc": 60.0, "val_loss": 17.35399216413498, "val_acc": 52.0}
{"epoch": 7, "training_loss": 66.7650294303894, "training_acc": 61.0, "val_loss": 17.737606167793274, "val_acc": 52.0}
{"epoch": 8, "training_loss": 67.16709065437317, "training_acc": 58.0, "val_loss": 18.12191754579544, "val_acc": 48.0}
{"epoch": 9, "training_loss": 66.97031688690186, "training_acc": 56.0, "val_loss": 18.259181082248688, "val_acc": 48.0}
{"epoch": 10, "training_loss": 66.1751537322998, "training_acc": 59.0, "val_loss": 17.62199103832245, "val_acc": 52.0}
{"epoch": 11, "training_loss": 64.68109107017517, "training_acc": 62.0, "val_loss": 17.397044599056244, "val_acc": 56.0}
{"epoch": 12, "training_loss": 64.63006448745728, "training_acc": 65.0, "val_loss": 17.297057807445526, "val_acc": 56.0}
{"epoch": 13, "training_loss": 65.17945885658264, "training_acc": 60.0, "val_loss": 16.979388892650604, "val_acc": 56.0}
{"epoch": 14, "training_loss": 64.94083881378174, "training_acc": 58.0, "val_loss": 16.789034008979797, "val_acc": 56.0}
{"epoch": 15, "training_loss": 64.90375328063965, "training_acc": 61.0, "val_loss": 16.619931161403656, "val_acc": 52.0}
{"epoch": 16, "training_loss": 64.20758748054504, "training_acc": 60.0, "val_loss": 16.748638451099396, "val_acc": 52.0}
{"epoch": 17, "training_loss": 64.2277010679245, "training_acc": 62.0, "val_loss": 17.328862845897675, "val_acc": 52.0}
{"epoch": 18, "training_loss": 64.2362494468689, "training_acc": 63.0, "val_loss": 16.873699426651, "val_acc": 52.0}
{"epoch": 19, "training_loss": 63.4582417011261, "training_acc": 64.0, "val_loss": 16.442851722240448, "val_acc": 52.0}
{"epoch": 20, "training_loss": 63.102776765823364, "training_acc": 63.0, "val_loss": 16.477544605731964, "val_acc": 52.0}
{"epoch": 21, "training_loss": 62.71707201004028, "training_acc": 67.0, "val_loss": 16.86442345380783, "val_acc": 52.0}
{"epoch": 22, "training_loss": 62.23795700073242, "training_acc": 65.0, "val_loss": 17.16693490743637, "val_acc": 52.0}
{"epoch": 23, "training_loss": 61.84873390197754, "training_acc": 67.0, "val_loss": 17.412273585796356, "val_acc": 52.0}
{"epoch": 24, "training_loss": 61.18893814086914, "training_acc": 70.0, "val_loss": 17.774459719657898, "val_acc": 48.0}
{"epoch": 25, "training_loss": 61.63952946662903, "training_acc": 63.0, "val_loss": 17.968609929084778, "val_acc": 44.0}
{"epoch": 26, "training_loss": 62.58121681213379, "training_acc": 59.0, "val_loss": 18.1681290268898, "val_acc": 44.0}
{"epoch": 27, "training_loss": 61.68406105041504, "training_acc": 64.0, "val_loss": 18.215779960155487, "val_acc": 48.0}
{"epoch": 28, "training_loss": 61.1144962310791, "training_acc": 70.0, "val_loss": 17.599353194236755, "val_acc": 52.0}
{"epoch": 29, "training_loss": 60.90828776359558, "training_acc": 72.0, "val_loss": 17.15717315673828, "val_acc": 52.0}
{"epoch": 30, "training_loss": 61.072434186935425, "training_acc": 73.0, "val_loss": 17.03580468893051, "val_acc": 52.0}
{"epoch": 31, "training_loss": 59.88145303726196, "training_acc": 70.0, "val_loss": 17.695055902004242, "val_acc": 48.0}
{"epoch": 32, "training_loss": 60.31958198547363, "training_acc": 71.0, "val_loss": 17.838546633720398, "val_acc": 44.0}
{"epoch": 33, "training_loss": 59.39380979537964, "training_acc": 71.0, "val_loss": 18.20424199104309, "val_acc": 44.0}
{"epoch": 34, "training_loss": 59.36702537536621, "training_acc": 72.0, "val_loss": 18.079344928264618, "val_acc": 44.0}
{"epoch": 35, "training_loss": 58.79464864730835, "training_acc": 70.0, "val_loss": 17.698080837726593, "val_acc": 44.0}
{"epoch": 36, "training_loss": 58.0822491645813, "training_acc": 71.0, "val_loss": 18.171115219593048, "val_acc": 44.0}
{"epoch": 37, "training_loss": 58.427978515625, "training_acc": 72.0, "val_loss": 18.159812688827515, "val_acc": 44.0}
{"epoch": 38, "training_loss": 57.60368537902832, "training_acc": 71.0, "val_loss": 18.763796985149384, "val_acc": 44.0}
