"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 72.23598766326904, "training_acc": 49.0, "val_loss": 18.453693389892578, "val_acc": 48.0}
{"epoch": 1, "training_loss": 70.13719868659973, "training_acc": 52.0, "val_loss": 17.906662821769714, "val_acc": 60.0}
{"epoch": 2, "training_loss": 67.65279960632324, "training_acc": 58.0, "val_loss": 18.004675209522247, "val_acc": 64.0}
{"epoch": 3, "training_loss": 65.78363609313965, "training_acc": 66.0, "val_loss": 17.55857765674591, "val_acc": 56.0}
{"epoch": 4, "training_loss": 66.0995671749115, "training_acc": 62.0, "val_loss": 17.497193813323975, "val_acc": 56.0}
{"epoch": 5, "training_loss": 64.62627458572388, "training_acc": 65.0, "val_loss": 18.029731512069702, "val_acc": 56.0}
{"epoch": 6, "training_loss": 63.84763741493225, "training_acc": 65.0, "val_loss": 18.314003944396973, "val_acc": 56.0}
{"epoch": 7, "training_loss": 64.29288697242737, "training_acc": 62.0, "val_loss": 18.45243275165558, "val_acc": 56.0}
{"epoch": 8, "training_loss": 64.6269850730896, "training_acc": 63.0, "val_loss": 18.067069351673126, "val_acc": 56.0}
{"epoch": 9, "training_loss": 62.96464657783508, "training_acc": 62.0, "val_loss": 17.657996714115143, "val_acc": 48.0}
{"epoch": 10, "training_loss": 65.32202935218811, "training_acc": 63.0, "val_loss": 17.525604367256165, "val_acc": 52.0}
{"epoch": 11, "training_loss": 64.19146919250488, "training_acc": 63.0, "val_loss": 18.072383105754852, "val_acc": 48.0}
{"epoch": 12, "training_loss": 63.38879215717316, "training_acc": 63.0, "val_loss": 18.889589607715607, "val_acc": 56.0}
{"epoch": 13, "training_loss": 63.02026152610779, "training_acc": 63.0, "val_loss": 18.144313991069794, "val_acc": 52.0}
{"epoch": 14, "training_loss": 64.8728551864624, "training_acc": 64.0, "val_loss": 17.41410195827484, "val_acc": 48.0}
{"epoch": 15, "training_loss": 64.1932315826416, "training_acc": 60.0, "val_loss": 17.461426556110382, "val_acc": 56.0}
{"epoch": 16, "training_loss": 63.16623497009277, "training_acc": 65.0, "val_loss": 18.3893084526062, "val_acc": 56.0}
{"epoch": 17, "training_loss": 60.19365978240967, "training_acc": 72.0, "val_loss": 18.19770038127899, "val_acc": 52.0}
{"epoch": 18, "training_loss": 61.465516090393066, "training_acc": 69.0, "val_loss": 18.51082146167755, "val_acc": 48.0}
{"epoch": 19, "training_loss": 61.03255558013916, "training_acc": 65.0, "val_loss": 18.432283401489258, "val_acc": 48.0}
{"epoch": 20, "training_loss": 60.31534481048584, "training_acc": 66.0, "val_loss": 18.416091799736023, "val_acc": 52.0}
{"epoch": 21, "training_loss": 58.725483655929565, "training_acc": 69.0, "val_loss": 19.97247338294983, "val_acc": 44.0}
{"epoch": 22, "training_loss": 60.457674980163574, "training_acc": 68.0, "val_loss": 18.511441349983215, "val_acc": 44.0}
{"epoch": 23, "training_loss": 62.80893588066101, "training_acc": 66.0, "val_loss": 18.673844635486603, "val_acc": 56.0}
{"epoch": 24, "training_loss": 63.743016719818115, "training_acc": 66.0, "val_loss": 19.08126026391983, "val_acc": 48.0}
{"epoch": 25, "training_loss": 58.267367362976074, "training_acc": 69.0, "val_loss": 19.009190797805786, "val_acc": 52.0}
{"epoch": 26, "training_loss": 61.969375133514404, "training_acc": 68.0, "val_loss": 18.70322823524475, "val_acc": 52.0}
{"epoch": 27, "training_loss": 61.21427917480469, "training_acc": 68.0, "val_loss": 19.489160180091858, "val_acc": 48.0}
{"epoch": 28, "training_loss": 58.26500344276428, "training_acc": 71.0, "val_loss": 20.56840807199478, "val_acc": 44.0}
{"epoch": 29, "training_loss": 61.34323859214783, "training_acc": 70.0, "val_loss": 19.50564533472061, "val_acc": 48.0}
{"epoch": 30, "training_loss": 64.655841588974, "training_acc": 67.0, "val_loss": 19.813697040081024, "val_acc": 48.0}
{"epoch": 31, "training_loss": 60.88533282279968, "training_acc": 66.0, "val_loss": 19.868366420269012, "val_acc": 52.0}
{"epoch": 32, "training_loss": 58.30931234359741, "training_acc": 70.0, "val_loss": 19.504721462726593, "val_acc": 48.0}
{"epoch": 33, "training_loss": 58.47057390213013, "training_acc": 70.0, "val_loss": 19.140583276748657, "val_acc": 60.0}
