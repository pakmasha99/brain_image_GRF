"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.27805638313293, "training_acc": 51.0, "val_loss": 17.724506556987762, "val_acc": 44.0}
{"epoch": 1, "training_loss": 69.71927189826965, "training_acc": 57.0, "val_loss": 18.148116767406464, "val_acc": 44.0}
{"epoch": 2, "training_loss": 71.1510283946991, "training_acc": 49.0, "val_loss": 17.222534120082855, "val_acc": 52.0}
{"epoch": 3, "training_loss": 72.01027488708496, "training_acc": 50.0, "val_loss": 17.007842659950256, "val_acc": 56.0}
{"epoch": 4, "training_loss": 69.98893880844116, "training_acc": 52.0, "val_loss": 16.789017617702484, "val_acc": 52.0}
{"epoch": 5, "training_loss": 67.1830997467041, "training_acc": 58.0, "val_loss": 16.781795024871826, "val_acc": 56.0}
{"epoch": 6, "training_loss": 65.99449682235718, "training_acc": 58.0, "val_loss": 16.870921850204468, "val_acc": 56.0}
{"epoch": 7, "training_loss": 65.86087894439697, "training_acc": 61.0, "val_loss": 16.801179945468903, "val_acc": 56.0}
{"epoch": 8, "training_loss": 66.22007870674133, "training_acc": 60.0, "val_loss": 16.682372987270355, "val_acc": 56.0}
{"epoch": 9, "training_loss": 64.1346378326416, "training_acc": 62.0, "val_loss": 17.029927670955658, "val_acc": 56.0}
{"epoch": 10, "training_loss": 63.593698263168335, "training_acc": 64.0, "val_loss": 16.89019799232483, "val_acc": 60.0}
{"epoch": 11, "training_loss": 64.89670157432556, "training_acc": 63.0, "val_loss": 16.733942925930023, "val_acc": 60.0}
{"epoch": 12, "training_loss": 62.73316669464111, "training_acc": 70.0, "val_loss": 17.30973720550537, "val_acc": 52.0}
{"epoch": 13, "training_loss": 60.94008541107178, "training_acc": 69.0, "val_loss": 17.56318211555481, "val_acc": 52.0}
{"epoch": 14, "training_loss": 61.614853620529175, "training_acc": 69.0, "val_loss": 16.960512101650238, "val_acc": 48.0}
{"epoch": 15, "training_loss": 62.209394216537476, "training_acc": 66.0, "val_loss": 17.11708754301071, "val_acc": 56.0}
{"epoch": 16, "training_loss": 62.71064305305481, "training_acc": 65.0, "val_loss": 17.209911346435547, "val_acc": 52.0}
{"epoch": 17, "training_loss": 63.45184564590454, "training_acc": 70.0, "val_loss": 17.482447624206543, "val_acc": 60.0}
{"epoch": 18, "training_loss": 62.669567346572876, "training_acc": 67.0, "val_loss": 18.386714160442352, "val_acc": 48.0}
{"epoch": 19, "training_loss": 60.31550121307373, "training_acc": 69.0, "val_loss": 18.25851798057556, "val_acc": 48.0}
{"epoch": 20, "training_loss": 59.81269121170044, "training_acc": 69.0, "val_loss": 18.150165677070618, "val_acc": 44.0}
{"epoch": 21, "training_loss": 59.24659967422485, "training_acc": 71.0, "val_loss": 17.715613543987274, "val_acc": 52.0}
{"epoch": 22, "training_loss": 60.95134234428406, "training_acc": 65.0, "val_loss": 17.866399884223938, "val_acc": 52.0}
{"epoch": 23, "training_loss": 62.256186723709106, "training_acc": 63.0, "val_loss": 17.91394352912903, "val_acc": 52.0}
{"epoch": 24, "training_loss": 59.11300587654114, "training_acc": 67.0, "val_loss": 18.1487038731575, "val_acc": 44.0}
{"epoch": 25, "training_loss": 58.584571838378906, "training_acc": 70.0, "val_loss": 18.729621171951294, "val_acc": 44.0}
{"epoch": 26, "training_loss": 61.59369468688965, "training_acc": 68.0, "val_loss": 19.220781326293945, "val_acc": 44.0}
{"epoch": 27, "training_loss": 58.62151372432709, "training_acc": 69.0, "val_loss": 18.742160499095917, "val_acc": 52.0}
