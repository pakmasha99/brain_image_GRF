"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 71.51828360557556, "training_acc": 50.0, "val_loss": 18.58024001121521, "val_acc": 48.0}
{"epoch": 1, "training_loss": 72.22746467590332, "training_acc": 50.0, "val_loss": 17.359092831611633, "val_acc": 56.0}
{"epoch": 2, "training_loss": 69.93590641021729, "training_acc": 52.0, "val_loss": 18.181025981903076, "val_acc": 60.0}
{"epoch": 3, "training_loss": 67.86352777481079, "training_acc": 53.0, "val_loss": 18.140964210033417, "val_acc": 60.0}
{"epoch": 4, "training_loss": 68.77595806121826, "training_acc": 52.0, "val_loss": 18.2145893573761, "val_acc": 56.0}
{"epoch": 5, "training_loss": 67.64055871963501, "training_acc": 51.0, "val_loss": 18.248139321804047, "val_acc": 56.0}
{"epoch": 6, "training_loss": 67.16566801071167, "training_acc": 55.0, "val_loss": 18.345782160758972, "val_acc": 56.0}
{"epoch": 7, "training_loss": 65.69300723075867, "training_acc": 59.0, "val_loss": 18.22134554386139, "val_acc": 56.0}
{"epoch": 8, "training_loss": 66.03050088882446, "training_acc": 60.0, "val_loss": 18.09227466583252, "val_acc": 56.0}
{"epoch": 9, "training_loss": 65.01757192611694, "training_acc": 58.0, "val_loss": 18.71722787618637, "val_acc": 56.0}
{"epoch": 10, "training_loss": 65.12715101242065, "training_acc": 58.0, "val_loss": 18.988250195980072, "val_acc": 56.0}
{"epoch": 11, "training_loss": 65.4342532157898, "training_acc": 59.0, "val_loss": 18.351882696151733, "val_acc": 56.0}
{"epoch": 12, "training_loss": 65.77962970733643, "training_acc": 58.0, "val_loss": 19.095799326896667, "val_acc": 56.0}
{"epoch": 13, "training_loss": 63.58164310455322, "training_acc": 65.0, "val_loss": 19.475486874580383, "val_acc": 56.0}
{"epoch": 14, "training_loss": 63.66396760940552, "training_acc": 63.0, "val_loss": 19.750376045703888, "val_acc": 56.0}
{"epoch": 15, "training_loss": 62.911354064941406, "training_acc": 65.0, "val_loss": 19.15031373500824, "val_acc": 52.0}
{"epoch": 16, "training_loss": 63.55337142944336, "training_acc": 65.0, "val_loss": 19.265441596508026, "val_acc": 56.0}
{"epoch": 17, "training_loss": 62.320451736450195, "training_acc": 67.0, "val_loss": 19.352827966213226, "val_acc": 56.0}
{"epoch": 18, "training_loss": 61.772907972335815, "training_acc": 65.0, "val_loss": 19.61309462785721, "val_acc": 56.0}
{"epoch": 19, "training_loss": 60.55309784412384, "training_acc": 69.0, "val_loss": 20.05627751350403, "val_acc": 56.0}
{"epoch": 20, "training_loss": 60.277589082717896, "training_acc": 68.0, "val_loss": 20.70033997297287, "val_acc": 52.0}
