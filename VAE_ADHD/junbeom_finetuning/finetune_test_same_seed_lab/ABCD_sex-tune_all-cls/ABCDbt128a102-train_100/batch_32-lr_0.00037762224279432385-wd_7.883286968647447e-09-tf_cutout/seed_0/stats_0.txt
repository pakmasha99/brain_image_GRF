"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 75.23900270462036, "training_acc": 49.0, "val_loss": 18.352919816970825, "val_acc": 56.0}
{"epoch": 1, "training_loss": 86.1742296218872, "training_acc": 52.0, "val_loss": 18.049965798854828, "val_acc": 56.0}
{"epoch": 2, "training_loss": 74.54422998428345, "training_acc": 52.0, "val_loss": 17.961396276950836, "val_acc": 56.0}
{"epoch": 3, "training_loss": 70.81118965148926, "training_acc": 52.0, "val_loss": 18.821370601654053, "val_acc": 72.0}
{"epoch": 4, "training_loss": 73.42984533309937, "training_acc": 48.0, "val_loss": 17.46165007352829, "val_acc": 56.0}
{"epoch": 5, "training_loss": 73.61666536331177, "training_acc": 52.0, "val_loss": 17.196904122829437, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.94257616996765, "training_acc": 46.0, "val_loss": 17.522181570529938, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.25894355773926, "training_acc": 52.0, "val_loss": 17.253322899341583, "val_acc": 56.0}
{"epoch": 8, "training_loss": 71.13503360748291, "training_acc": 52.0, "val_loss": 17.562998831272125, "val_acc": 56.0}
{"epoch": 9, "training_loss": 71.18304824829102, "training_acc": 48.0, "val_loss": 18.026912212371826, "val_acc": 56.0}
{"epoch": 10, "training_loss": 71.3433084487915, "training_acc": 48.0, "val_loss": 17.83008873462677, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.47550582885742, "training_acc": 48.0, "val_loss": 17.18449741601944, "val_acc": 56.0}
{"epoch": 12, "training_loss": 73.16747093200684, "training_acc": 52.0, "val_loss": 17.506255209445953, "val_acc": 56.0}
{"epoch": 13, "training_loss": 71.73333287239075, "training_acc": 52.0, "val_loss": 17.16814637184143, "val_acc": 56.0}
{"epoch": 14, "training_loss": 69.37793612480164, "training_acc": 52.0, "val_loss": 17.216390371322632, "val_acc": 56.0}
{"epoch": 15, "training_loss": 69.46445178985596, "training_acc": 48.0, "val_loss": 17.206470668315887, "val_acc": 56.0}
{"epoch": 16, "training_loss": 70.32445192337036, "training_acc": 52.0, "val_loss": 17.186051607131958, "val_acc": 56.0}
{"epoch": 17, "training_loss": 69.55269527435303, "training_acc": 52.0, "val_loss": 17.246776819229126, "val_acc": 56.0}
{"epoch": 18, "training_loss": 69.19059801101685, "training_acc": 50.0, "val_loss": 17.630945146083832, "val_acc": 56.0}
{"epoch": 19, "training_loss": 70.29680967330933, "training_acc": 48.0, "val_loss": 17.700250446796417, "val_acc": 56.0}
{"epoch": 20, "training_loss": 69.64590787887573, "training_acc": 48.0, "val_loss": 17.256425321102142, "val_acc": 56.0}
{"epoch": 21, "training_loss": 69.17259168624878, "training_acc": 52.0, "val_loss": 17.16238558292389, "val_acc": 56.0}
{"epoch": 22, "training_loss": 69.53990697860718, "training_acc": 52.0, "val_loss": 17.17652529478073, "val_acc": 56.0}
{"epoch": 23, "training_loss": 69.20954251289368, "training_acc": 52.0, "val_loss": 17.276309430599213, "val_acc": 56.0}
{"epoch": 24, "training_loss": 69.47759008407593, "training_acc": 52.0, "val_loss": 17.193835973739624, "val_acc": 56.0}
{"epoch": 25, "training_loss": 69.26075387001038, "training_acc": 52.0, "val_loss": 17.24364012479782, "val_acc": 56.0}
{"epoch": 26, "training_loss": 69.27101230621338, "training_acc": 52.0, "val_loss": 17.266392707824707, "val_acc": 56.0}
{"epoch": 27, "training_loss": 69.22870683670044, "training_acc": 52.0, "val_loss": 17.30153262615204, "val_acc": 56.0}
{"epoch": 28, "training_loss": 69.30480980873108, "training_acc": 52.0, "val_loss": 17.28930026292801, "val_acc": 56.0}
{"epoch": 29, "training_loss": 69.2366144657135, "training_acc": 52.0, "val_loss": 17.18650460243225, "val_acc": 56.0}
{"epoch": 30, "training_loss": 69.72958540916443, "training_acc": 52.0, "val_loss": 17.17425137758255, "val_acc": 56.0}
{"epoch": 31, "training_loss": 69.95330715179443, "training_acc": 52.0, "val_loss": 17.23264306783676, "val_acc": 56.0}
{"epoch": 32, "training_loss": 70.85501742362976, "training_acc": 52.0, "val_loss": 17.336902022361755, "val_acc": 56.0}
{"epoch": 33, "training_loss": 71.16511344909668, "training_acc": 52.0, "val_loss": 17.235510051250458, "val_acc": 56.0}
{"epoch": 34, "training_loss": 69.94115877151489, "training_acc": 52.0, "val_loss": 17.15678870677948, "val_acc": 56.0}
{"epoch": 35, "training_loss": 69.41431856155396, "training_acc": 52.0, "val_loss": 17.302219569683075, "val_acc": 56.0}
{"epoch": 36, "training_loss": 69.30428695678711, "training_acc": 52.0, "val_loss": 17.289607226848602, "val_acc": 56.0}
{"epoch": 37, "training_loss": 69.21273565292358, "training_acc": 52.0, "val_loss": 17.214488983154297, "val_acc": 56.0}
{"epoch": 38, "training_loss": 69.26064348220825, "training_acc": 52.0, "val_loss": 17.221376299858093, "val_acc": 56.0}
{"epoch": 39, "training_loss": 69.20205783843994, "training_acc": 52.0, "val_loss": 17.294982075691223, "val_acc": 56.0}
{"epoch": 40, "training_loss": 69.478520154953, "training_acc": 42.0, "val_loss": 17.39368736743927, "val_acc": 56.0}
{"epoch": 41, "training_loss": 69.37477731704712, "training_acc": 48.0, "val_loss": 17.35376864671707, "val_acc": 56.0}
{"epoch": 42, "training_loss": 69.33832430839539, "training_acc": 48.0, "val_loss": 17.349229753017426, "val_acc": 56.0}
{"epoch": 43, "training_loss": 69.37432861328125, "training_acc": 40.0, "val_loss": 17.343947291374207, "val_acc": 56.0}
{"epoch": 44, "training_loss": 69.38071370124817, "training_acc": 49.0, "val_loss": 17.399123311042786, "val_acc": 56.0}
{"epoch": 45, "training_loss": 69.39568758010864, "training_acc": 48.0, "val_loss": 17.38065630197525, "val_acc": 56.0}
{"epoch": 46, "training_loss": 69.35197496414185, "training_acc": 48.0, "val_loss": 17.325380444526672, "val_acc": 56.0}
{"epoch": 47, "training_loss": 69.23485279083252, "training_acc": 52.0, "val_loss": 17.24521666765213, "val_acc": 56.0}
{"epoch": 48, "training_loss": 69.27081418037415, "training_acc": 52.0, "val_loss": 17.21753627061844, "val_acc": 56.0}
{"epoch": 49, "training_loss": 69.20740699768066, "training_acc": 52.0, "val_loss": 17.248904705047607, "val_acc": 56.0}
{"epoch": 50, "training_loss": 69.22190690040588, "training_acc": 52.0, "val_loss": 17.3293337225914, "val_acc": 56.0}
{"epoch": 51, "training_loss": 69.31975841522217, "training_acc": 50.0, "val_loss": 17.44646728038788, "val_acc": 56.0}
{"epoch": 52, "training_loss": 69.60095357894897, "training_acc": 48.0, "val_loss": 17.551085352897644, "val_acc": 56.0}
{"epoch": 53, "training_loss": 69.71167469024658, "training_acc": 48.0, "val_loss": 17.520034313201904, "val_acc": 56.0}
