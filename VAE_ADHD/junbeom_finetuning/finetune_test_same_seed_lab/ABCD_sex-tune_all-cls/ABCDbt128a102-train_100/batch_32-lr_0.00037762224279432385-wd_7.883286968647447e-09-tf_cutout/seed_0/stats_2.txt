"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 76.59681034088135, "training_acc": 42.0, "val_loss": 20.78181356191635, "val_acc": 44.0}
{"epoch": 1, "training_loss": 414.30097341537476, "training_acc": 50.0, "val_loss": 18.195740878582, "val_acc": 52.0}
{"epoch": 2, "training_loss": 96.83766961097717, "training_acc": 47.0, "val_loss": 19.950996339321136, "val_acc": 52.0}
{"epoch": 3, "training_loss": 72.93225431442261, "training_acc": 53.0, "val_loss": 18.058207631111145, "val_acc": 52.0}
{"epoch": 4, "training_loss": 72.11875629425049, "training_acc": 47.0, "val_loss": 19.19698864221573, "val_acc": 52.0}
{"epoch": 5, "training_loss": 74.8533706665039, "training_acc": 52.0, "val_loss": 17.340047657489777, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.35171866416931, "training_acc": 49.0, "val_loss": 17.3125758767128, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.26844763755798, "training_acc": 51.0, "val_loss": 17.384041845798492, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.46803426742554, "training_acc": 50.0, "val_loss": 17.434222996234894, "val_acc": 52.0}
{"epoch": 9, "training_loss": 71.06833934783936, "training_acc": 53.0, "val_loss": 17.67616868019104, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.7323317527771, "training_acc": 53.0, "val_loss": 17.33444184064865, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.42828512191772, "training_acc": 53.0, "val_loss": 17.658664286136627, "val_acc": 52.0}
{"epoch": 12, "training_loss": 70.81946039199829, "training_acc": 53.0, "val_loss": 17.863091826438904, "val_acc": 52.0}
{"epoch": 13, "training_loss": 70.14766240119934, "training_acc": 53.0, "val_loss": 17.37211048603058, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.21913194656372, "training_acc": 53.0, "val_loss": 17.32587367296219, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.15221858024597, "training_acc": 53.0, "val_loss": 17.353197932243347, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.36735248565674, "training_acc": 53.0, "val_loss": 17.322230339050293, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.53085851669312, "training_acc": 53.0, "val_loss": 17.473281919956207, "val_acc": 52.0}
{"epoch": 18, "training_loss": 70.39197635650635, "training_acc": 53.0, "val_loss": 17.549121379852295, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.8347020149231, "training_acc": 53.0, "val_loss": 17.31719970703125, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.54421329498291, "training_acc": 53.0, "val_loss": 17.312079668045044, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.40297842025757, "training_acc": 53.0, "val_loss": 17.325444519519806, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.35338425636292, "training_acc": 53.0, "val_loss": 17.322389781475067, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.04814195632935, "training_acc": 53.0, "val_loss": 17.330734431743622, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.87788963317871, "training_acc": 48.0, "val_loss": 17.358282208442688, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.23709440231323, "training_acc": 51.0, "val_loss": 17.31027066707611, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.10264587402344, "training_acc": 53.0, "val_loss": 17.325741052627563, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.30611300468445, "training_acc": 53.0, "val_loss": 17.31204390525818, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.17503690719604, "training_acc": 53.0, "val_loss": 17.31041818857193, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.0858588218689, "training_acc": 53.0, "val_loss": 17.33512431383133, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.38128089904785, "training_acc": 47.0, "val_loss": 17.35818386077881, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.46628665924072, "training_acc": 47.0, "val_loss": 17.352786660194397, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.47904348373413, "training_acc": 47.0, "val_loss": 17.377349734306335, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.604820728302, "training_acc": 47.0, "val_loss": 17.346438765525818, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.21474695205688, "training_acc": 53.0, "val_loss": 17.327426373958588, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.14874315261841, "training_acc": 53.0, "val_loss": 17.379961907863617, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.38224577903748, "training_acc": 53.0, "val_loss": 17.39671230316162, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.48586654663086, "training_acc": 53.0, "val_loss": 17.40759015083313, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.27474308013916, "training_acc": 53.0, "val_loss": 17.317499220371246, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.02515935897827, "training_acc": 56.0, "val_loss": 17.39564836025238, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.76878809928894, "training_acc": 47.0, "val_loss": 17.370973527431488, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.51542711257935, "training_acc": 55.0, "val_loss": 17.32182204723358, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.31858396530151, "training_acc": 46.0, "val_loss": 17.35871136188507, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.59041786193848, "training_acc": 47.0, "val_loss": 17.411349713802338, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.70861959457397, "training_acc": 47.0, "val_loss": 17.37675964832306, "val_acc": 52.0}
