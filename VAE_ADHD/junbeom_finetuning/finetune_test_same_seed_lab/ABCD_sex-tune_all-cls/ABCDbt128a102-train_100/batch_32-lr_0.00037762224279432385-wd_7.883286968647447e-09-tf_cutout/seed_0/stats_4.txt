"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 73.08574962615967, "training_acc": 48.0, "val_loss": 20.059001445770264, "val_acc": 52.0}
{"epoch": 1, "training_loss": 96.85683345794678, "training_acc": 51.0, "val_loss": 19.686515629291534, "val_acc": 52.0}
{"epoch": 2, "training_loss": 133.3285927772522, "training_acc": 45.0, "val_loss": 17.638522386550903, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.13655471801758, "training_acc": 55.0, "val_loss": 18.308715522289276, "val_acc": 52.0}
{"epoch": 4, "training_loss": 72.05898451805115, "training_acc": 53.0, "val_loss": 17.864316701889038, "val_acc": 52.0}
{"epoch": 5, "training_loss": 70.32784700393677, "training_acc": 51.0, "val_loss": 18.66939067840576, "val_acc": 52.0}
{"epoch": 6, "training_loss": 73.23492002487183, "training_acc": 53.0, "val_loss": 17.360028624534607, "val_acc": 52.0}
{"epoch": 7, "training_loss": 70.1421799659729, "training_acc": 41.0, "val_loss": 17.308150231838226, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.72466802597046, "training_acc": 53.0, "val_loss": 17.38802045583725, "val_acc": 52.0}
{"epoch": 9, "training_loss": 70.06957197189331, "training_acc": 47.0, "val_loss": 17.602023482322693, "val_acc": 52.0}
{"epoch": 10, "training_loss": 71.07287645339966, "training_acc": 41.0, "val_loss": 17.311130464076996, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.29140090942383, "training_acc": 49.0, "val_loss": 17.313534021377563, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.13120746612549, "training_acc": 53.0, "val_loss": 17.336826026439667, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.16881322860718, "training_acc": 53.0, "val_loss": 17.32688844203949, "val_acc": 52.0}
{"epoch": 14, "training_loss": 70.19902896881104, "training_acc": 47.0, "val_loss": 17.53581315279007, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.77467918395996, "training_acc": 45.0, "val_loss": 17.309604585170746, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.2262191772461, "training_acc": 53.0, "val_loss": 17.321425676345825, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.06424951553345, "training_acc": 53.0, "val_loss": 17.447958886623383, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.74259090423584, "training_acc": 53.0, "val_loss": 17.69012212753296, "val_acc": 52.0}
{"epoch": 19, "training_loss": 70.42245960235596, "training_acc": 53.0, "val_loss": 17.502771317958832, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.13046860694885, "training_acc": 53.0, "val_loss": 17.31630265712738, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.19615340232849, "training_acc": 55.0, "val_loss": 17.337076365947723, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.56895852088928, "training_acc": 49.0, "val_loss": 17.32044219970703, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.4595000743866, "training_acc": 53.0, "val_loss": 17.342522740364075, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.14700174331665, "training_acc": 53.0, "val_loss": 17.31346994638443, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.71779012680054, "training_acc": 53.0, "val_loss": 17.309215664863586, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.25790023803711, "training_acc": 53.0, "val_loss": 17.467622458934784, "val_acc": 52.0}
