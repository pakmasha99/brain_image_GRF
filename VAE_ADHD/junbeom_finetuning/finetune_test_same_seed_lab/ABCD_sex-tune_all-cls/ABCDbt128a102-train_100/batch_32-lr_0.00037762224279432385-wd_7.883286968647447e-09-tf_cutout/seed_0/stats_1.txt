"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 72.89631509780884, "training_acc": 55.0, "val_loss": 33.118125796318054, "val_acc": 52.0}
{"epoch": 1, "training_loss": 97.49917078018188, "training_acc": 53.0, "val_loss": 39.172857999801636, "val_acc": 48.0}
{"epoch": 2, "training_loss": 81.50360345840454, "training_acc": 49.0, "val_loss": 17.70203858613968, "val_acc": 52.0}
{"epoch": 3, "training_loss": 71.97513151168823, "training_acc": 47.0, "val_loss": 17.45806485414505, "val_acc": 52.0}
{"epoch": 4, "training_loss": 70.66825246810913, "training_acc": 53.0, "val_loss": 18.63640993833542, "val_acc": 52.0}
{"epoch": 5, "training_loss": 72.11598062515259, "training_acc": 53.0, "val_loss": 17.802295088768005, "val_acc": 52.0}
{"epoch": 6, "training_loss": 75.4005115032196, "training_acc": 47.0, "val_loss": 17.33258366584778, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.4189624786377, "training_acc": 51.0, "val_loss": 17.328235507011414, "val_acc": 52.0}
{"epoch": 8, "training_loss": 72.88609862327576, "training_acc": 35.0, "val_loss": 17.360660433769226, "val_acc": 52.0}
{"epoch": 9, "training_loss": 70.16515159606934, "training_acc": 47.0, "val_loss": 17.32659488916397, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.01910781860352, "training_acc": 55.0, "val_loss": 17.432475090026855, "val_acc": 52.0}
{"epoch": 11, "training_loss": 70.04784846305847, "training_acc": 47.0, "val_loss": 17.324763536453247, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.91310358047485, "training_acc": 53.0, "val_loss": 17.408882081508636, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.42910885810852, "training_acc": 53.0, "val_loss": 17.3639714717865, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.33131551742554, "training_acc": 53.0, "val_loss": 17.347045242786407, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.26112937927246, "training_acc": 53.0, "val_loss": 17.331664264202118, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.09190249443054, "training_acc": 53.0, "val_loss": 17.42449849843979, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.63914823532104, "training_acc": 53.0, "val_loss": 17.552757263183594, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.62997913360596, "training_acc": 53.0, "val_loss": 17.339296638965607, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.26169776916504, "training_acc": 53.0, "val_loss": 17.39225685596466, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.72923040390015, "training_acc": 47.0, "val_loss": 17.41214692592621, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.52277946472168, "training_acc": 47.0, "val_loss": 17.315563559532166, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.01499557495117, "training_acc": 53.0, "val_loss": 17.390887439250946, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.21083688735962, "training_acc": 53.0, "val_loss": 17.319457232952118, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.36714887619019, "training_acc": 53.0, "val_loss": 17.308250069618225, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.27784872055054, "training_acc": 53.0, "val_loss": 17.34449714422226, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.39574480056763, "training_acc": 53.0, "val_loss": 17.325709760189056, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.36078119277954, "training_acc": 54.0, "val_loss": 17.35314577817917, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.44200897216797, "training_acc": 45.0, "val_loss": 17.324572801589966, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.06460857391357, "training_acc": 53.0, "val_loss": 17.335565388202667, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.4650526046753, "training_acc": 53.0, "val_loss": 17.46469885110855, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.49662637710571, "training_acc": 53.0, "val_loss": 17.327284812927246, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.20112085342407, "training_acc": 53.0, "val_loss": 17.31630712747574, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.15401482582092, "training_acc": 53.0, "val_loss": 17.313918471336365, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.15712070465088, "training_acc": 53.0, "val_loss": 17.313562333583832, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.32552433013916, "training_acc": 53.0, "val_loss": 17.33361780643463, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.4068353176117, "training_acc": 53.0, "val_loss": 17.310012876987457, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.19185161590576, "training_acc": 53.0, "val_loss": 17.313000559806824, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.19855833053589, "training_acc": 53.0, "val_loss": 17.312130331993103, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.18421363830566, "training_acc": 53.0, "val_loss": 17.31126606464386, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.20138168334961, "training_acc": 53.0, "val_loss": 17.311306297779083, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.14965033531189, "training_acc": 53.0, "val_loss": 17.326882481575012, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.16195774078369, "training_acc": 53.0, "val_loss": 17.320969700813293, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.29381942749023, "training_acc": 53.0, "val_loss": 17.33328551054001, "val_acc": 52.0}
