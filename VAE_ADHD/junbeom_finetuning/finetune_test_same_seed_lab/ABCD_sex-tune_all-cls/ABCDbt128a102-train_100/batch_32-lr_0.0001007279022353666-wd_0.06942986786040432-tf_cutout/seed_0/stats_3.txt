"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 71.9918224811554, "training_acc": 46.0, "val_loss": 17.445293068885803, "val_acc": 52.0}
{"epoch": 1, "training_loss": 71.59811067581177, "training_acc": 52.0, "val_loss": 18.64810585975647, "val_acc": 56.0}
{"epoch": 2, "training_loss": 72.06954956054688, "training_acc": 52.0, "val_loss": 18.292926251888275, "val_acc": 52.0}
{"epoch": 3, "training_loss": 72.83330345153809, "training_acc": 53.0, "val_loss": 17.95695275068283, "val_acc": 44.0}
{"epoch": 4, "training_loss": 72.07166290283203, "training_acc": 57.0, "val_loss": 17.59406179189682, "val_acc": 52.0}
{"epoch": 5, "training_loss": 70.00374984741211, "training_acc": 51.0, "val_loss": 17.672231793403625, "val_acc": 52.0}
{"epoch": 6, "training_loss": 70.38923954963684, "training_acc": 49.0, "val_loss": 17.38678365945816, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.37355351448059, "training_acc": 52.0, "val_loss": 17.301470041275024, "val_acc": 52.0}
{"epoch": 8, "training_loss": 68.90210723876953, "training_acc": 62.0, "val_loss": 17.603766918182373, "val_acc": 52.0}
{"epoch": 9, "training_loss": 70.68452882766724, "training_acc": 47.0, "val_loss": 17.315658926963806, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.80389428138733, "training_acc": 53.0, "val_loss": 17.64432042837143, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.78903460502625, "training_acc": 53.0, "val_loss": 17.34323501586914, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.02223372459412, "training_acc": 53.0, "val_loss": 17.32654869556427, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.20110630989075, "training_acc": 53.0, "val_loss": 17.369768023490906, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.19434881210327, "training_acc": 53.0, "val_loss": 17.714187502861023, "val_acc": 52.0}
{"epoch": 15, "training_loss": 70.61701369285583, "training_acc": 53.0, "val_loss": 17.56098121404648, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.24955368041992, "training_acc": 53.0, "val_loss": 17.318210005760193, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.1948013305664, "training_acc": 53.0, "val_loss": 17.335642874240875, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.0695641040802, "training_acc": 53.0, "val_loss": 17.3313170671463, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.21595907211304, "training_acc": 53.0, "val_loss": 17.33727604150772, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.31551885604858, "training_acc": 50.0, "val_loss": 17.35924333333969, "val_acc": 52.0}
{"epoch": 21, "training_loss": 70.59081411361694, "training_acc": 48.0, "val_loss": 17.477543652057648, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.47822856903076, "training_acc": 45.0, "val_loss": 17.3155277967453, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.16267204284668, "training_acc": 53.0, "val_loss": 17.3332080245018, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.580974817276, "training_acc": 53.0, "val_loss": 17.442859709262848, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.48362302780151, "training_acc": 53.0, "val_loss": 17.425504326820374, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.4342532157898, "training_acc": 53.0, "val_loss": 17.351119220256805, "val_acc": 52.0}
