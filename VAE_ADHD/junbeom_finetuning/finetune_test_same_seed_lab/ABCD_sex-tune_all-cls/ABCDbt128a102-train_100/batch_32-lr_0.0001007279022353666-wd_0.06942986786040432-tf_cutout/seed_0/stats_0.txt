"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 70.91168308258057, "training_acc": 52.0, "val_loss": 17.728587985038757, "val_acc": 56.0}
{"epoch": 1, "training_loss": 69.72666358947754, "training_acc": 55.0, "val_loss": 18.334586918354034, "val_acc": 56.0}
{"epoch": 2, "training_loss": 74.83007431030273, "training_acc": 42.0, "val_loss": 17.369818687438965, "val_acc": 56.0}
{"epoch": 3, "training_loss": 72.38091778755188, "training_acc": 52.0, "val_loss": 17.28261113166809, "val_acc": 56.0}
{"epoch": 4, "training_loss": 71.41752409934998, "training_acc": 48.0, "val_loss": 17.485743761062622, "val_acc": 56.0}
{"epoch": 5, "training_loss": 69.98384094238281, "training_acc": 52.0, "val_loss": 17.20692366361618, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.50673294067383, "training_acc": 53.0, "val_loss": 17.494602501392365, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.59616565704346, "training_acc": 46.0, "val_loss": 17.469443380832672, "val_acc": 56.0}
{"epoch": 8, "training_loss": 70.13887739181519, "training_acc": 45.0, "val_loss": 17.34916716814041, "val_acc": 56.0}
{"epoch": 9, "training_loss": 69.47878241539001, "training_acc": 52.0, "val_loss": 17.500999569892883, "val_acc": 56.0}
{"epoch": 10, "training_loss": 69.88452529907227, "training_acc": 49.0, "val_loss": 17.640572786331177, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.7151050567627, "training_acc": 48.0, "val_loss": 17.543785274028778, "val_acc": 56.0}
{"epoch": 12, "training_loss": 72.13580656051636, "training_acc": 52.0, "val_loss": 17.4057275056839, "val_acc": 56.0}
{"epoch": 13, "training_loss": 70.88850235939026, "training_acc": 48.0, "val_loss": 17.137053608894348, "val_acc": 56.0}
{"epoch": 14, "training_loss": 69.73691940307617, "training_acc": 52.0, "val_loss": 17.258046567440033, "val_acc": 56.0}
{"epoch": 15, "training_loss": 70.08022928237915, "training_acc": 48.0, "val_loss": 17.350272834300995, "val_acc": 56.0}
{"epoch": 16, "training_loss": 74.22222089767456, "training_acc": 52.0, "val_loss": 17.349088191986084, "val_acc": 56.0}
{"epoch": 17, "training_loss": 70.06076860427856, "training_acc": 52.0, "val_loss": 17.476455867290497, "val_acc": 56.0}
{"epoch": 18, "training_loss": 69.77677989006042, "training_acc": 48.0, "val_loss": 18.280357122421265, "val_acc": 56.0}
{"epoch": 19, "training_loss": 71.76740646362305, "training_acc": 48.0, "val_loss": 17.62041002511978, "val_acc": 56.0}
{"epoch": 20, "training_loss": 69.21833086013794, "training_acc": 52.0, "val_loss": 17.221899330615997, "val_acc": 56.0}
{"epoch": 21, "training_loss": 70.34284949302673, "training_acc": 52.0, "val_loss": 17.16040074825287, "val_acc": 56.0}
{"epoch": 22, "training_loss": 69.18003559112549, "training_acc": 52.0, "val_loss": 17.354488372802734, "val_acc": 56.0}
{"epoch": 23, "training_loss": 69.41032457351685, "training_acc": 47.0, "val_loss": 17.35634058713913, "val_acc": 56.0}
{"epoch": 24, "training_loss": 71.21635913848877, "training_acc": 51.0, "val_loss": 17.172400653362274, "val_acc": 56.0}
{"epoch": 25, "training_loss": 69.2873387336731, "training_acc": 48.0, "val_loss": 17.427867650985718, "val_acc": 56.0}
{"epoch": 26, "training_loss": 70.08327722549438, "training_acc": 44.0, "val_loss": 17.26837456226349, "val_acc": 56.0}
{"epoch": 27, "training_loss": 69.19408297538757, "training_acc": 52.0, "val_loss": 17.337608337402344, "val_acc": 56.0}
{"epoch": 28, "training_loss": 69.34280061721802, "training_acc": 47.0, "val_loss": 17.26132333278656, "val_acc": 56.0}
{"epoch": 29, "training_loss": 69.25682783126831, "training_acc": 52.0, "val_loss": 17.150750756263733, "val_acc": 56.0}
{"epoch": 30, "training_loss": 71.08266162872314, "training_acc": 52.0, "val_loss": 17.424678802490234, "val_acc": 56.0}
{"epoch": 31, "training_loss": 71.96442008018494, "training_acc": 52.0, "val_loss": 17.341406643390656, "val_acc": 56.0}
{"epoch": 32, "training_loss": 71.48495388031006, "training_acc": 52.0, "val_loss": 17.31981188058853, "val_acc": 56.0}
