"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 73.20437479019165, "training_acc": 48.0, "val_loss": 17.548473179340363, "val_acc": 52.0}
{"epoch": 1, "training_loss": 70.20439338684082, "training_acc": 57.0, "val_loss": 17.715011537075043, "val_acc": 52.0}
{"epoch": 2, "training_loss": 70.03398728370667, "training_acc": 48.0, "val_loss": 21.682414412498474, "val_acc": 52.0}
{"epoch": 3, "training_loss": 75.19018173217773, "training_acc": 53.0, "val_loss": 17.373095452785492, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.58781456947327, "training_acc": 46.0, "val_loss": 17.32812076807022, "val_acc": 52.0}
{"epoch": 5, "training_loss": 70.50687336921692, "training_acc": 33.0, "val_loss": 17.34572947025299, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.97656512260437, "training_acc": 53.0, "val_loss": 17.41340160369873, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.39172077178955, "training_acc": 53.0, "val_loss": 17.356404662132263, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.46618723869324, "training_acc": 53.0, "val_loss": 17.47792363166809, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.38539171218872, "training_acc": 53.0, "val_loss": 17.49636083841324, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.31993794441223, "training_acc": 53.0, "val_loss": 17.467021942138672, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.14472818374634, "training_acc": 53.0, "val_loss": 17.454133927822113, "val_acc": 52.0}
{"epoch": 12, "training_loss": 70.17619514465332, "training_acc": 40.0, "val_loss": 17.3556387424469, "val_acc": 52.0}
{"epoch": 13, "training_loss": 71.4938588142395, "training_acc": 46.0, "val_loss": 17.84031391143799, "val_acc": 52.0}
{"epoch": 14, "training_loss": 70.03754162788391, "training_acc": 53.0, "val_loss": 17.349427938461304, "val_acc": 52.0}
{"epoch": 15, "training_loss": 71.0173168182373, "training_acc": 47.0, "val_loss": 17.409414052963257, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.29468059539795, "training_acc": 54.0, "val_loss": 17.565429210662842, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.95195031166077, "training_acc": 53.0, "val_loss": 17.980030179023743, "val_acc": 52.0}
{"epoch": 18, "training_loss": 68.60815715789795, "training_acc": 53.0, "val_loss": 18.28388422727585, "val_acc": 60.0}
{"epoch": 19, "training_loss": 73.47406935691833, "training_acc": 44.0, "val_loss": 17.787480354309082, "val_acc": 52.0}
{"epoch": 20, "training_loss": 70.94556093215942, "training_acc": 53.0, "val_loss": 17.350955307483673, "val_acc": 52.0}
{"epoch": 21, "training_loss": 71.54267930984497, "training_acc": 53.0, "val_loss": 17.24918782711029, "val_acc": 52.0}
{"epoch": 22, "training_loss": 70.34995126724243, "training_acc": 45.0, "val_loss": 18.829263746738434, "val_acc": 40.0}
{"epoch": 23, "training_loss": 82.93087911605835, "training_acc": 45.0, "val_loss": 17.32485145330429, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.22493863105774, "training_acc": 53.0, "val_loss": 17.32780486345291, "val_acc": 52.0}
{"epoch": 25, "training_loss": 80.35114526748657, "training_acc": 42.0, "val_loss": 22.293052077293396, "val_acc": 52.0}
{"epoch": 26, "training_loss": 77.89062881469727, "training_acc": 53.0, "val_loss": 17.392584681510925, "val_acc": 52.0}
{"epoch": 27, "training_loss": 70.52219009399414, "training_acc": 47.0, "val_loss": 17.3348531126976, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.2299337387085, "training_acc": 51.0, "val_loss": 17.38584041595459, "val_acc": 52.0}
{"epoch": 29, "training_loss": 70.06400156021118, "training_acc": 53.0, "val_loss": 17.334401607513428, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.34934258460999, "training_acc": 53.0, "val_loss": 17.341916263103485, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.31970977783203, "training_acc": 50.0, "val_loss": 17.405645549297333, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.71948862075806, "training_acc": 47.0, "val_loss": 17.376041412353516, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.63871335983276, "training_acc": 43.0, "val_loss": 17.333367466926575, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.17147612571716, "training_acc": 53.0, "val_loss": 17.383308708667755, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.22915172576904, "training_acc": 53.0, "val_loss": 17.445887625217438, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.39526891708374, "training_acc": 53.0, "val_loss": 17.428667843341827, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.31713676452637, "training_acc": 53.0, "val_loss": 17.45557188987732, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.45146369934082, "training_acc": 53.0, "val_loss": 17.505477368831635, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.36058521270752, "training_acc": 53.0, "val_loss": 17.374679446220398, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.23580026626587, "training_acc": 51.0, "val_loss": 17.41965115070343, "val_acc": 52.0}
