"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.25858879089355, "training_acc": 48.0, "val_loss": 17.234808206558228, "val_acc": 56.0}
{"epoch": 1, "training_loss": 69.3103883266449, "training_acc": 52.0, "val_loss": 17.22574234008789, "val_acc": 56.0}
{"epoch": 2, "training_loss": 69.47239327430725, "training_acc": 52.0, "val_loss": 17.187632620334625, "val_acc": 56.0}
{"epoch": 3, "training_loss": 69.41056895256042, "training_acc": 52.0, "val_loss": 17.183609306812286, "val_acc": 56.0}
{"epoch": 4, "training_loss": 69.30645275115967, "training_acc": 52.0, "val_loss": 17.215892672538757, "val_acc": 56.0}
{"epoch": 5, "training_loss": 69.33644008636475, "training_acc": 52.0, "val_loss": 17.18437373638153, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.29384899139404, "training_acc": 52.0, "val_loss": 17.20922440290451, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.33557057380676, "training_acc": 52.0, "val_loss": 17.24981963634491, "val_acc": 56.0}
{"epoch": 8, "training_loss": 69.48767495155334, "training_acc": 52.0, "val_loss": 17.223884165287018, "val_acc": 56.0}
{"epoch": 9, "training_loss": 69.42865562438965, "training_acc": 52.0, "val_loss": 17.34471619129181, "val_acc": 56.0}
{"epoch": 10, "training_loss": 69.43944764137268, "training_acc": 48.0, "val_loss": 17.4694687128067, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.43637943267822, "training_acc": 48.0, "val_loss": 17.245356738567352, "val_acc": 56.0}
{"epoch": 12, "training_loss": 69.64668083190918, "training_acc": 52.0, "val_loss": 17.160452902317047, "val_acc": 56.0}
{"epoch": 13, "training_loss": 69.40655374526978, "training_acc": 52.0, "val_loss": 17.159171402454376, "val_acc": 56.0}
{"epoch": 14, "training_loss": 69.42405605316162, "training_acc": 52.0, "val_loss": 17.150598764419556, "val_acc": 56.0}
{"epoch": 15, "training_loss": 69.50292730331421, "training_acc": 52.0, "val_loss": 17.150455713272095, "val_acc": 56.0}
{"epoch": 16, "training_loss": 69.74052000045776, "training_acc": 52.0, "val_loss": 17.148660123348236, "val_acc": 56.0}
{"epoch": 17, "training_loss": 69.48788714408875, "training_acc": 52.0, "val_loss": 17.182978987693787, "val_acc": 56.0}
{"epoch": 18, "training_loss": 69.16241455078125, "training_acc": 52.0, "val_loss": 17.356395721435547, "val_acc": 56.0}
{"epoch": 19, "training_loss": 69.50688171386719, "training_acc": 48.0, "val_loss": 17.482253909111023, "val_acc": 56.0}
{"epoch": 20, "training_loss": 69.44814395904541, "training_acc": 48.0, "val_loss": 17.323946952819824, "val_acc": 56.0}
{"epoch": 21, "training_loss": 69.2459979057312, "training_acc": 53.0, "val_loss": 17.239457368850708, "val_acc": 56.0}
{"epoch": 22, "training_loss": 69.36005902290344, "training_acc": 52.0, "val_loss": 17.218364775180817, "val_acc": 56.0}
{"epoch": 23, "training_loss": 69.24016046524048, "training_acc": 52.0, "val_loss": 17.243903875350952, "val_acc": 56.0}
{"epoch": 24, "training_loss": 69.46429204940796, "training_acc": 52.0, "val_loss": 17.18214601278305, "val_acc": 56.0}
{"epoch": 25, "training_loss": 69.2685694694519, "training_acc": 52.0, "val_loss": 17.224273085594177, "val_acc": 56.0}
{"epoch": 26, "training_loss": 69.31079721450806, "training_acc": 52.0, "val_loss": 17.23759174346924, "val_acc": 56.0}
{"epoch": 27, "training_loss": 69.22657752037048, "training_acc": 52.0, "val_loss": 17.273418605327606, "val_acc": 56.0}
{"epoch": 28, "training_loss": 69.29815888404846, "training_acc": 52.0, "val_loss": 17.262114584445953, "val_acc": 56.0}
{"epoch": 29, "training_loss": 69.24984407424927, "training_acc": 52.0, "val_loss": 17.163509130477905, "val_acc": 56.0}
{"epoch": 30, "training_loss": 69.85199880599976, "training_acc": 52.0, "val_loss": 17.171791195869446, "val_acc": 56.0}
{"epoch": 31, "training_loss": 70.0739209651947, "training_acc": 52.0, "val_loss": 17.20428764820099, "val_acc": 56.0}
{"epoch": 32, "training_loss": 70.63721799850464, "training_acc": 52.0, "val_loss": 17.275363206863403, "val_acc": 56.0}
{"epoch": 33, "training_loss": 70.76784634590149, "training_acc": 52.0, "val_loss": 17.174947261810303, "val_acc": 56.0}
{"epoch": 34, "training_loss": 69.51555466651917, "training_acc": 52.0, "val_loss": 17.172496020793915, "val_acc": 56.0}
{"epoch": 35, "training_loss": 69.33300638198853, "training_acc": 52.0, "val_loss": 17.27420836687088, "val_acc": 56.0}
