"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 71.21490097045898, "training_acc": 47.0, "val_loss": 17.43876039981842, "val_acc": 52.0}
{"epoch": 1, "training_loss": 70.06392121315002, "training_acc": 47.0, "val_loss": 17.40163117647171, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.59436750411987, "training_acc": 45.0, "val_loss": 17.30845272541046, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.2033257484436, "training_acc": 53.0, "val_loss": 17.308154702186584, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.27918577194214, "training_acc": 53.0, "val_loss": 17.30848252773285, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.02879476547241, "training_acc": 53.0, "val_loss": 17.338815331459045, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.22677755355835, "training_acc": 53.0, "val_loss": 17.332185804843903, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.22133159637451, "training_acc": 53.0, "val_loss": 17.30937957763672, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.18184804916382, "training_acc": 53.0, "val_loss": 17.308999598026276, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.2742247581482, "training_acc": 53.0, "val_loss": 17.309662699699402, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.17293214797974, "training_acc": 53.0, "val_loss": 17.34578311443329, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.16368818283081, "training_acc": 53.0, "val_loss": 17.427046597003937, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.59444332122803, "training_acc": 53.0, "val_loss": 17.467723786830902, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.61548662185669, "training_acc": 53.0, "val_loss": 17.35648810863495, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.22086238861084, "training_acc": 53.0, "val_loss": 17.32262372970581, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.24299740791321, "training_acc": 53.0, "val_loss": 17.31031835079193, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.28757858276367, "training_acc": 53.0, "val_loss": 17.34670251607895, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.34518933296204, "training_acc": 53.0, "val_loss": 17.362292110919952, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.37267804145813, "training_acc": 53.0, "val_loss": 17.318187654018402, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.13382267951965, "training_acc": 53.0, "val_loss": 17.324981093406677, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.3589780330658, "training_acc": 53.0, "val_loss": 17.317672073841095, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.68269562721252, "training_acc": 53.0, "val_loss": 17.314089834690094, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.1916651725769, "training_acc": 53.0, "val_loss": 17.308712005615234, "val_acc": 52.0}
