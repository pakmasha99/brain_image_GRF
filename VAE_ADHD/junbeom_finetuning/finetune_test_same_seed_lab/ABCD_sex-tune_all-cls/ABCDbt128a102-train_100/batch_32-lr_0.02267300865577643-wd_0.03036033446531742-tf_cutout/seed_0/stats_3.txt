"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 473.3436470031738, "training_acc": 61.0, "val_loss": 1.1411417453063086e+26, "val_acc": 48.0}
{"epoch": 1, "training_loss": 3.793490784630381e+26, "training_acc": 47.0, "val_loss": 6.225113915587765e+25, "val_acc": 48.0}
{"epoch": 2, "training_loss": 2.124771124068929e+26, "training_acc": 47.0, "val_loss": 3.64223215544036e+25, "val_acc": 48.0}
{"epoch": 3, "training_loss": 1.2790024696879491e+26, "training_acc": 47.0, "val_loss": 2.088313382604176e+25, "val_acc": 48.0}
{"epoch": 4, "training_loss": 7.2270881345746065e+25, "training_acc": 47.0, "val_loss": 1.07480990173383e+25, "val_acc": 48.0}
{"epoch": 5, "training_loss": 3.1937187442110062e+25, "training_acc": 47.0, "val_loss": 4.4428537245166345e+24, "val_acc": 48.0}
{"epoch": 6, "training_loss": 6.536999894779341e+24, "training_acc": 44.0, "val_loss": 8.241092194258125e+18, "val_acc": 52.0}
{"epoch": 7, "training_loss": 1.4829858902649707e+21, "training_acc": 47.0, "val_loss": 2.4269547647690342e+19, "val_acc": 52.0}
{"epoch": 8, "training_loss": 1.5059605843003153e+20, "training_acc": 51.0, "val_loss": 2.9260156193734656e+18, "val_acc": 52.0}
{"epoch": 9, "training_loss": 4.70918844129896e+18, "training_acc": 53.0, "val_loss": 6.38011620458496e+16, "val_acc": 48.0}
{"epoch": 10, "training_loss": 1.527570779477115e+17, "training_acc": 47.0, "val_loss": 2644090499891200.0, "val_acc": 52.0}
{"epoch": 11, "training_loss": 7085929420292096.0, "training_acc": 57.0, "val_loss": 452735637913600.0, "val_acc": 48.0}
{"epoch": 12, "training_loss": 648296623112192.0, "training_acc": 55.0, "val_loss": 44090766131200.0, "val_acc": 48.0}
{"epoch": 13, "training_loss": 221167919562752.0, "training_acc": 41.0, "val_loss": 4968454553600.0, "val_acc": 48.0}
{"epoch": 14, "training_loss": 28871465893888.0, "training_acc": 47.0, "val_loss": 2599963033600.0, "val_acc": 52.0}
{"epoch": 15, "training_loss": 5570585821184.0, "training_acc": 54.0, "val_loss": 1054107750400.0, "val_acc": 48.0}
{"epoch": 16, "training_loss": 6257935728640.0, "training_acc": 51.0, "val_loss": 988055859200.0, "val_acc": 52.0}
{"epoch": 17, "training_loss": 2636745662464.0, "training_acc": 53.0, "val_loss": 185370368000.0, "val_acc": 52.0}
{"epoch": 18, "training_loss": 441472407040.0, "training_acc": 53.0, "val_loss": 20981620800.0, "val_acc": 48.0}
{"epoch": 19, "training_loss": 49926622720.0, "training_acc": 47.0, "val_loss": 2585684400.0, "val_acc": 48.0}
{"epoch": 20, "training_loss": 135422767616.0, "training_acc": 45.0, "val_loss": 7733130400.0, "val_acc": 48.0}
{"epoch": 21, "training_loss": 61248855552.0, "training_acc": 45.0, "val_loss": 17765672000.0, "val_acc": 48.0}
{"epoch": 22, "training_loss": 71203594944.0, "training_acc": 47.0, "val_loss": 28509241600.0, "val_acc": 52.0}
{"epoch": 23, "training_loss": 43772959488.0, "training_acc": 53.0, "val_loss": 1413337100.0, "val_acc": 48.0}
{"epoch": 24, "training_loss": 8619672846.0, "training_acc": 51.0, "val_loss": 638288500.0, "val_acc": 48.0}
{"epoch": 25, "training_loss": 1571017672.0, "training_acc": 57.0, "val_loss": 539162500.0, "val_acc": 52.0}
{"epoch": 26, "training_loss": 1582942196.0, "training_acc": 57.0, "val_loss": 62836825.0, "val_acc": 48.0}
{"epoch": 27, "training_loss": 3295061032.0, "training_acc": 52.0, "val_loss": 713028200.0, "val_acc": 48.0}
{"epoch": 28, "training_loss": 2440461040.0, "training_acc": 49.0, "val_loss": 537162300.0, "val_acc": 52.0}
{"epoch": 29, "training_loss": 989233584.0, "training_acc": 51.0, "val_loss": 231796500.0, "val_acc": 52.0}
{"epoch": 30, "training_loss": 1427487297.0, "training_acc": 45.0, "val_loss": 673966950.0, "val_acc": 52.0}
{"epoch": 31, "training_loss": 2654090112.0, "training_acc": 53.0, "val_loss": 138688350.0, "val_acc": 52.0}
{"epoch": 32, "training_loss": 461606357.5, "training_acc": 45.0, "val_loss": 50739037.5, "val_acc": 52.0}
{"epoch": 33, "training_loss": 143026742.0, "training_acc": 48.0, "val_loss": 54934043.75, "val_acc": 52.0}
{"epoch": 34, "training_loss": 252604343.0, "training_acc": 51.0, "val_loss": 93983100.0, "val_acc": 52.0}
{"epoch": 35, "training_loss": 239793985.0, "training_acc": 47.0, "val_loss": 13513918.75, "val_acc": 48.0}
{"epoch": 36, "training_loss": 36336257.078125, "training_acc": 55.0, "val_loss": 42704728.125, "val_acc": 52.0}
{"epoch": 37, "training_loss": 151750934.6171875, "training_acc": 53.0, "val_loss": 9342198.4375, "val_acc": 52.0}
{"epoch": 38, "training_loss": 58853890.75, "training_acc": 47.0, "val_loss": 15865967.1875, "val_acc": 48.0}
{"epoch": 39, "training_loss": 71812993.0, "training_acc": 47.0, "val_loss": 19288250.0, "val_acc": 52.0}
{"epoch": 40, "training_loss": 43582808.25, "training_acc": 45.0, "val_loss": 13203537.5, "val_acc": 52.0}
{"epoch": 41, "training_loss": 46433222.21875, "training_acc": 50.0, "val_loss": 6187327.34375, "val_acc": 48.0}
{"epoch": 42, "training_loss": 23445967.6640625, "training_acc": 60.0, "val_loss": 1505542.48046875, "val_acc": 56.0}
{"epoch": 43, "training_loss": 27951891.875, "training_acc": 49.0, "val_loss": 14508484.375, "val_acc": 52.0}
{"epoch": 44, "training_loss": 35643942.5, "training_acc": 42.0, "val_loss": 5975300.0, "val_acc": 52.0}
{"epoch": 45, "training_loss": 28361029.71875, "training_acc": 53.0, "val_loss": 7695817.1875, "val_acc": 52.0}
{"epoch": 46, "training_loss": 23161351.0, "training_acc": 51.0, "val_loss": 11555309.375, "val_acc": 52.0}
{"epoch": 47, "training_loss": 28137032.25, "training_acc": 59.0, "val_loss": 307413.4521484375, "val_acc": 64.0}
{"epoch": 48, "training_loss": 17502364.375, "training_acc": 48.0, "val_loss": 2829379.4921875, "val_acc": 52.0}
{"epoch": 49, "training_loss": 9186303.59375, "training_acc": 56.0, "val_loss": 4775854.296875, "val_acc": 52.0}
{"epoch": 50, "training_loss": 18981058.0, "training_acc": 38.0, "val_loss": 4993496.875, "val_acc": 52.0}
{"epoch": 51, "training_loss": 14555645.03125, "training_acc": 53.0, "val_loss": 11974082.03125, "val_acc": 52.0}
{"epoch": 52, "training_loss": 74517369.0, "training_acc": 53.0, "val_loss": 4503933.59375, "val_acc": 52.0}
{"epoch": 53, "training_loss": 37137716.125, "training_acc": 57.0, "val_loss": 8551510.9375, "val_acc": 52.0}
{"epoch": 54, "training_loss": 48663143.375, "training_acc": 53.0, "val_loss": 12640427.34375, "val_acc": 52.0}
{"epoch": 55, "training_loss": 33771521.75, "training_acc": 52.0, "val_loss": 13802964.0625, "val_acc": 48.0}
{"epoch": 56, "training_loss": 35066254.375, "training_acc": 47.0, "val_loss": 13375853.125, "val_acc": 52.0}
{"epoch": 57, "training_loss": 56923518.75, "training_acc": 53.0, "val_loss": 11050061.71875, "val_acc": 52.0}
{"epoch": 58, "training_loss": 25008096.8125, "training_acc": 55.0, "val_loss": 12496914.0625, "val_acc": 48.0}
{"epoch": 59, "training_loss": 35408862.625, "training_acc": 47.0, "val_loss": 3244177.9296875, "val_acc": 52.0}
{"epoch": 60, "training_loss": 12564998.1953125, "training_acc": 53.0, "val_loss": 2855247.65625, "val_acc": 48.0}
{"epoch": 61, "training_loss": 13185815.3125, "training_acc": 45.0, "val_loss": 1442018.5546875, "val_acc": 52.0}
{"epoch": 62, "training_loss": 3721621.578125, "training_acc": 50.0, "val_loss": 712659.130859375, "val_acc": 48.0}
{"epoch": 63, "training_loss": 3220816.625, "training_acc": 63.0, "val_loss": 945474.90234375, "val_acc": 52.0}
{"epoch": 64, "training_loss": 3270633.5, "training_acc": 57.0, "val_loss": 1647180.6640625, "val_acc": 48.0}
{"epoch": 65, "training_loss": 4081877.875, "training_acc": 57.0, "val_loss": 4050667.578125, "val_acc": 52.0}
{"epoch": 66, "training_loss": 16831465.75, "training_acc": 53.0, "val_loss": 1951545.3125, "val_acc": 52.0}
