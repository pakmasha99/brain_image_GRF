"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 1713.498104095459, "training_acc": 48.0, "val_loss": 7.817182358116762e+18, "val_acc": 52.0}
{"epoch": 1, "training_loss": 9.829602066481455e+18, "training_acc": 61.0, "val_loss": 1468494300.0, "val_acc": 48.0}
{"epoch": 2, "training_loss": 2874906294.0, "training_acc": 47.0, "val_loss": 7637225.0, "val_acc": 48.0}
{"epoch": 3, "training_loss": 305911845.140625, "training_acc": 43.0, "val_loss": 12921243.75, "val_acc": 48.0}
{"epoch": 4, "training_loss": 20039285.40625, "training_acc": 55.0, "val_loss": 1760060.15625, "val_acc": 52.0}
{"epoch": 5, "training_loss": 7059072.46875, "training_acc": 57.0, "val_loss": 3818422.65625, "val_acc": 48.0}
{"epoch": 6, "training_loss": 6937261.527878761, "training_acc": 47.0, "val_loss": 12870.643615722656, "val_acc": 48.0}
{"epoch": 7, "training_loss": 6984461.263320923, "training_acc": 47.0, "val_loss": 1399110.9375, "val_acc": 48.0}
{"epoch": 8, "training_loss": 2209557.175254822, "training_acc": 47.0, "val_loss": 17.444144189357758, "val_acc": 52.0}
{"epoch": 9, "training_loss": 68.37501803040504, "training_acc": 53.0, "val_loss": 179760.7177734375, "val_acc": 48.0}
{"epoch": 10, "training_loss": 2469229.8904953003, "training_acc": 39.0, "val_loss": 45854.37316894531, "val_acc": 48.0}
{"epoch": 11, "training_loss": 213225.15474909544, "training_acc": 57.0, "val_loss": 30.688193440437317, "val_acc": 52.0}
{"epoch": 12, "training_loss": 149.28675508499146, "training_acc": 53.0, "val_loss": 31783.3984375, "val_acc": 52.0}
{"epoch": 13, "training_loss": 12967772979.6875, "training_acc": 53.0, "val_loss": 6630266.40625, "val_acc": 48.0}
{"epoch": 14, "training_loss": 63295068.78125, "training_acc": 51.0, "val_loss": 288589.9658203125, "val_acc": 48.0}
{"epoch": 15, "training_loss": 33758655.0625, "training_acc": 59.0, "val_loss": 1491184.1796875, "val_acc": 48.0}
{"epoch": 16, "training_loss": 5171025.53125, "training_acc": 49.0, "val_loss": 34979.632568359375, "val_acc": 48.0}
{"epoch": 17, "training_loss": 2082118.23828125, "training_acc": 41.0, "val_loss": 364544.677734375, "val_acc": 52.0}
{"epoch": 18, "training_loss": 723859.4287109375, "training_acc": 55.0, "val_loss": 2778537.890625, "val_acc": 52.0}
{"epoch": 19, "training_loss": 3607514.243408203, "training_acc": 49.0, "val_loss": 18714.566040039062, "val_acc": 48.0}
{"epoch": 20, "training_loss": 54243.535736083984, "training_acc": 47.0, "val_loss": 83.31184983253479, "val_acc": 52.0}
{"epoch": 21, "training_loss": 7536.6097412109375, "training_acc": 53.0, "val_loss": 27.17442512512207, "val_acc": 48.0}
{"epoch": 22, "training_loss": 84.78363680839539, "training_acc": 49.0, "val_loss": 22.88830727338791, "val_acc": 52.0}
{"epoch": 23, "training_loss": 95.64536476135254, "training_acc": 53.0, "val_loss": 19.30679678916931, "val_acc": 52.0}
{"epoch": 24, "training_loss": 75.04604244232178, "training_acc": 51.0, "val_loss": 19.860616326332092, "val_acc": 48.0}
{"epoch": 25, "training_loss": 71.11142539978027, "training_acc": 51.0, "val_loss": 5377.901077270508, "val_acc": 52.0}
{"epoch": 26, "training_loss": 27353.819447040558, "training_acc": 49.0, "val_loss": 31.833580136299133, "val_acc": 48.0}
{"epoch": 27, "training_loss": 150.88716208934784, "training_acc": 47.0, "val_loss": 35.55855453014374, "val_acc": 48.0}
{"epoch": 28, "training_loss": 108.99964380264282, "training_acc": 45.0, "val_loss": 17.308692634105682, "val_acc": 52.0}
{"epoch": 29, "training_loss": 10311.823470115662, "training_acc": 50.0, "val_loss": 32.682302594184875, "val_acc": 52.0}
{"epoch": 30, "training_loss": 127.63534927368164, "training_acc": 53.0, "val_loss": 18.944992125034332, "val_acc": 52.0}
{"epoch": 31, "training_loss": 89.18413066864014, "training_acc": 43.0, "val_loss": 23.063939809799194, "val_acc": 48.0}
{"epoch": 32, "training_loss": 79.62779927253723, "training_acc": 47.0, "val_loss": 17.943908274173737, "val_acc": 52.0}
{"epoch": 33, "training_loss": 72.90866112709045, "training_acc": 53.0, "val_loss": 17.888158559799194, "val_acc": 52.0}
{"epoch": 34, "training_loss": 70.52277088165283, "training_acc": 53.0, "val_loss": 17.547045648097992, "val_acc": 52.0}
{"epoch": 35, "training_loss": 70.7460458278656, "training_acc": 47.0, "val_loss": 17.529506981372833, "val_acc": 52.0}
{"epoch": 36, "training_loss": 70.0735216140747, "training_acc": 47.0, "val_loss": 17.321181297302246, "val_acc": 52.0}
{"epoch": 37, "training_loss": 70.5385422706604, "training_acc": 53.0, "val_loss": 18.251517415046692, "val_acc": 52.0}
{"epoch": 38, "training_loss": 73.63845944404602, "training_acc": 53.0, "val_loss": 17.664173245429993, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.9679365158081, "training_acc": 51.0, "val_loss": 18.062274158000946, "val_acc": 52.0}
{"epoch": 40, "training_loss": 75.18953323364258, "training_acc": 47.0, "val_loss": 18.192489445209503, "val_acc": 52.0}
{"epoch": 41, "training_loss": 70.03504300117493, "training_acc": 49.0, "val_loss": 17.48490035533905, "val_acc": 52.0}
{"epoch": 42, "training_loss": 73.25356340408325, "training_acc": 53.0, "val_loss": 19.330863654613495, "val_acc": 52.0}
{"epoch": 43, "training_loss": 76.68879127502441, "training_acc": 53.0, "val_loss": 18.315494060516357, "val_acc": 52.0}
{"epoch": 44, "training_loss": 70.97722959518433, "training_acc": 53.0, "val_loss": 17.337332665920258, "val_acc": 52.0}
{"epoch": 45, "training_loss": 69.67853236198425, "training_acc": 47.0, "val_loss": 17.567306756973267, "val_acc": 52.0}
{"epoch": 46, "training_loss": 70.36437654495239, "training_acc": 47.0, "val_loss": 17.39177703857422, "val_acc": 52.0}
{"epoch": 47, "training_loss": 69.54639720916748, "training_acc": 47.0, "val_loss": 17.317432165145874, "val_acc": 52.0}
