"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.28007578849792, "training_acc": 53.0, "val_loss": 17.225150763988495, "val_acc": 56.0}
{"epoch": 1, "training_loss": 69.24561405181885, "training_acc": 52.0, "val_loss": 17.235131561756134, "val_acc": 56.0}
{"epoch": 2, "training_loss": 69.4773576259613, "training_acc": 52.0, "val_loss": 17.205743491649628, "val_acc": 56.0}
{"epoch": 3, "training_loss": 69.38418960571289, "training_acc": 52.0, "val_loss": 17.17708557844162, "val_acc": 56.0}
{"epoch": 4, "training_loss": 69.30331206321716, "training_acc": 52.0, "val_loss": 17.206771671772003, "val_acc": 56.0}
{"epoch": 5, "training_loss": 69.33332300186157, "training_acc": 52.0, "val_loss": 17.178964614868164, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.29514455795288, "training_acc": 52.0, "val_loss": 17.212863266468048, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.32964777946472, "training_acc": 52.0, "val_loss": 17.26004034280777, "val_acc": 56.0}
{"epoch": 8, "training_loss": 69.40898251533508, "training_acc": 52.0, "val_loss": 17.236000299453735, "val_acc": 56.0}
{"epoch": 9, "training_loss": 69.36978912353516, "training_acc": 52.0, "val_loss": 17.330363392829895, "val_acc": 56.0}
{"epoch": 10, "training_loss": 69.40420770645142, "training_acc": 47.0, "val_loss": 17.46843159198761, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.47038626670837, "training_acc": 48.0, "val_loss": 17.291472852230072, "val_acc": 56.0}
{"epoch": 12, "training_loss": 69.58719992637634, "training_acc": 52.0, "val_loss": 17.168357968330383, "val_acc": 56.0}
{"epoch": 13, "training_loss": 69.31668090820312, "training_acc": 52.0, "val_loss": 17.155654728412628, "val_acc": 56.0}
{"epoch": 14, "training_loss": 69.44852232933044, "training_acc": 52.0, "val_loss": 17.14857518672943, "val_acc": 56.0}
{"epoch": 15, "training_loss": 69.54445815086365, "training_acc": 52.0, "val_loss": 17.14860498905182, "val_acc": 56.0}
{"epoch": 16, "training_loss": 69.74038648605347, "training_acc": 52.0, "val_loss": 17.15126931667328, "val_acc": 56.0}
{"epoch": 17, "training_loss": 69.5731782913208, "training_acc": 52.0, "val_loss": 17.16538965702057, "val_acc": 56.0}
{"epoch": 18, "training_loss": 69.1866557598114, "training_acc": 52.0, "val_loss": 17.291343212127686, "val_acc": 56.0}
{"epoch": 19, "training_loss": 69.37327790260315, "training_acc": 48.0, "val_loss": 17.4205020070076, "val_acc": 56.0}
{"epoch": 20, "training_loss": 69.4233341217041, "training_acc": 48.0, "val_loss": 17.341460287570953, "val_acc": 56.0}
{"epoch": 21, "training_loss": 69.28880834579468, "training_acc": 52.0, "val_loss": 17.275118827819824, "val_acc": 56.0}
{"epoch": 22, "training_loss": 69.3549256324768, "training_acc": 52.0, "val_loss": 17.24458634853363, "val_acc": 56.0}
{"epoch": 23, "training_loss": 69.2468934059143, "training_acc": 52.0, "val_loss": 17.260272800922394, "val_acc": 56.0}
{"epoch": 24, "training_loss": 69.44607830047607, "training_acc": 52.0, "val_loss": 17.18771904706955, "val_acc": 56.0}
{"epoch": 25, "training_loss": 69.26712536811829, "training_acc": 52.0, "val_loss": 17.215104401111603, "val_acc": 56.0}
{"epoch": 26, "training_loss": 69.29083728790283, "training_acc": 52.0, "val_loss": 17.225933074951172, "val_acc": 56.0}
{"epoch": 27, "training_loss": 69.22319674491882, "training_acc": 52.0, "val_loss": 17.257624864578247, "val_acc": 56.0}
{"epoch": 28, "training_loss": 69.28136777877808, "training_acc": 52.0, "val_loss": 17.261920869350433, "val_acc": 56.0}
{"epoch": 29, "training_loss": 69.23926711082458, "training_acc": 52.0, "val_loss": 17.183279991149902, "val_acc": 56.0}
{"epoch": 30, "training_loss": 69.61730575561523, "training_acc": 52.0, "val_loss": 17.150425910949707, "val_acc": 56.0}
{"epoch": 31, "training_loss": 69.68910074234009, "training_acc": 52.0, "val_loss": 17.176711559295654, "val_acc": 56.0}
{"epoch": 32, "training_loss": 70.31320118904114, "training_acc": 52.0, "val_loss": 17.240892350673676, "val_acc": 56.0}
{"epoch": 33, "training_loss": 70.60901498794556, "training_acc": 52.0, "val_loss": 17.19658225774765, "val_acc": 56.0}
