"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.27854919433594, "training_acc": 51.0, "val_loss": 17.217186093330383, "val_acc": 56.0}
{"epoch": 1, "training_loss": 69.19992399215698, "training_acc": 52.0, "val_loss": 17.26038008928299, "val_acc": 56.0}
{"epoch": 2, "training_loss": 69.44170355796814, "training_acc": 52.0, "val_loss": 17.18401163816452, "val_acc": 56.0}
{"epoch": 3, "training_loss": 69.36807870864868, "training_acc": 52.0, "val_loss": 17.188432812690735, "val_acc": 56.0}
{"epoch": 4, "training_loss": 69.29095077514648, "training_acc": 52.0, "val_loss": 17.21513867378235, "val_acc": 56.0}
{"epoch": 5, "training_loss": 69.31717205047607, "training_acc": 52.0, "val_loss": 17.187510430812836, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.28203582763672, "training_acc": 52.0, "val_loss": 17.211586236953735, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.32116723060608, "training_acc": 52.0, "val_loss": 17.24650114774704, "val_acc": 56.0}
{"epoch": 8, "training_loss": 69.40972638130188, "training_acc": 52.0, "val_loss": 17.22627729177475, "val_acc": 56.0}
{"epoch": 9, "training_loss": 69.36208486557007, "training_acc": 52.0, "val_loss": 17.31368601322174, "val_acc": 56.0}
{"epoch": 10, "training_loss": 69.36735439300537, "training_acc": 52.0, "val_loss": 17.439040541648865, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.43364095687866, "training_acc": 48.0, "val_loss": 17.28363037109375, "val_acc": 56.0}
{"epoch": 12, "training_loss": 69.54712438583374, "training_acc": 52.0, "val_loss": 17.17335879802704, "val_acc": 56.0}
{"epoch": 13, "training_loss": 69.30177164077759, "training_acc": 52.0, "val_loss": 17.160019278526306, "val_acc": 56.0}
{"epoch": 14, "training_loss": 69.40430402755737, "training_acc": 52.0, "val_loss": 17.148984968662262, "val_acc": 56.0}
{"epoch": 15, "training_loss": 69.4982647895813, "training_acc": 52.0, "val_loss": 17.149512469768524, "val_acc": 56.0}
{"epoch": 16, "training_loss": 69.70180892944336, "training_acc": 52.0, "val_loss": 17.14935302734375, "val_acc": 56.0}
{"epoch": 17, "training_loss": 69.53948879241943, "training_acc": 52.0, "val_loss": 17.167234420776367, "val_acc": 56.0}
{"epoch": 18, "training_loss": 69.18944263458252, "training_acc": 52.0, "val_loss": 17.2865629196167, "val_acc": 56.0}
{"epoch": 19, "training_loss": 69.3586745262146, "training_acc": 48.0, "val_loss": 17.405374348163605, "val_acc": 56.0}
{"epoch": 20, "training_loss": 69.3973822593689, "training_acc": 48.0, "val_loss": 17.3428937792778, "val_acc": 56.0}
{"epoch": 21, "training_loss": 69.30432724952698, "training_acc": 53.0, "val_loss": 17.28796660900116, "val_acc": 56.0}
{"epoch": 22, "training_loss": 69.3495979309082, "training_acc": 52.0, "val_loss": 17.254433035850525, "val_acc": 56.0}
{"epoch": 23, "training_loss": 69.25576400756836, "training_acc": 52.0, "val_loss": 17.26209968328476, "val_acc": 56.0}
{"epoch": 24, "training_loss": 69.43064260482788, "training_acc": 52.0, "val_loss": 17.19070076942444, "val_acc": 56.0}
{"epoch": 25, "training_loss": 69.26474380493164, "training_acc": 52.0, "val_loss": 17.212998867034912, "val_acc": 56.0}
{"epoch": 26, "training_loss": 69.28693509101868, "training_acc": 52.0, "val_loss": 17.222267389297485, "val_acc": 56.0}
{"epoch": 27, "training_loss": 69.22524690628052, "training_acc": 52.0, "val_loss": 17.249006032943726, "val_acc": 56.0}
{"epoch": 28, "training_loss": 69.26454496383667, "training_acc": 52.0, "val_loss": 17.254681885242462, "val_acc": 56.0}
{"epoch": 29, "training_loss": 69.23838424682617, "training_acc": 52.0, "val_loss": 17.186498641967773, "val_acc": 56.0}
{"epoch": 30, "training_loss": 69.5687563419342, "training_acc": 52.0, "val_loss": 17.14981496334076, "val_acc": 56.0}
{"epoch": 31, "training_loss": 69.61423110961914, "training_acc": 52.0, "val_loss": 17.166154086589813, "val_acc": 56.0}
{"epoch": 32, "training_loss": 70.18055295944214, "training_acc": 52.0, "val_loss": 17.221984267234802, "val_acc": 56.0}
{"epoch": 33, "training_loss": 70.46963310241699, "training_acc": 52.0, "val_loss": 17.18784272670746, "val_acc": 56.0}
