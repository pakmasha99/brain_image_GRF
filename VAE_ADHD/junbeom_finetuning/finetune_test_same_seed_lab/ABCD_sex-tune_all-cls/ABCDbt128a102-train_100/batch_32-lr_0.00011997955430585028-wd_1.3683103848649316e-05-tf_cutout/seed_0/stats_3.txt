"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.19618034362793, "training_acc": 54.0, "val_loss": 17.342212796211243, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.2737979888916, "training_acc": 53.0, "val_loss": 17.34336018562317, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.24125146865845, "training_acc": 53.0, "val_loss": 17.341335117816925, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.28498148918152, "training_acc": 53.0, "val_loss": 17.363306879997253, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.32647442817688, "training_acc": 53.0, "val_loss": 17.413529753684998, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.41649580001831, "training_acc": 53.0, "val_loss": 17.45760291814804, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.59967875480652, "training_acc": 53.0, "val_loss": 17.498204112052917, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.61806011199951, "training_acc": 53.0, "val_loss": 17.492635548114777, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.63101148605347, "training_acc": 53.0, "val_loss": 17.41098016500473, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.36814165115356, "training_acc": 53.0, "val_loss": 17.33875274658203, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.18371510505676, "training_acc": 53.0, "val_loss": 17.30925291776657, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.19481372833252, "training_acc": 53.0, "val_loss": 17.313465476036072, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.23526954650879, "training_acc": 53.0, "val_loss": 17.311392724514008, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.25021624565125, "training_acc": 53.0, "val_loss": 17.30996072292328, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.31447649002075, "training_acc": 53.0, "val_loss": 17.31167584657669, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.34203290939331, "training_acc": 53.0, "val_loss": 17.3308327794075, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.19535756111145, "training_acc": 53.0, "val_loss": 17.32516437768936, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.11031866073608, "training_acc": 53.0, "val_loss": 17.309848964214325, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.21882390975952, "training_acc": 53.0, "val_loss": 17.324455082416534, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.30994510650635, "training_acc": 52.0, "val_loss": 17.338170111179352, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.37933206558228, "training_acc": 47.0, "val_loss": 17.346791923046112, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.43180084228516, "training_acc": 47.0, "val_loss": 17.327409982681274, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.34538292884827, "training_acc": 53.0, "val_loss": 17.311443388462067, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.17815804481506, "training_acc": 53.0, "val_loss": 17.32712835073471, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.22853517532349, "training_acc": 53.0, "val_loss": 17.345981299877167, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.18967962265015, "training_acc": 53.0, "val_loss": 17.37731546163559, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.26996946334839, "training_acc": 53.0, "val_loss": 17.43365377187729, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.49239087104797, "training_acc": 53.0, "val_loss": 17.425867915153503, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.32445669174194, "training_acc": 53.0, "val_loss": 17.371006309986115, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.32137727737427, "training_acc": 53.0, "val_loss": 17.375236749649048, "val_acc": 52.0}
