"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 72.19130611419678, "training_acc": 46.0, "val_loss": 17.65422374010086, "val_acc": 52.0}
{"epoch": 1, "training_loss": 70.39209365844727, "training_acc": 46.0, "val_loss": 17.533281445503235, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.80267667770386, "training_acc": 54.0, "val_loss": 17.40788072347641, "val_acc": 52.0}
{"epoch": 3, "training_loss": 68.89417266845703, "training_acc": 53.0, "val_loss": 17.350736260414124, "val_acc": 52.0}
{"epoch": 4, "training_loss": 70.95794677734375, "training_acc": 39.0, "val_loss": 17.459779977798462, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.10883331298828, "training_acc": 53.0, "val_loss": 17.462828755378723, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.19725155830383, "training_acc": 53.0, "val_loss": 17.41768568754196, "val_acc": 52.0}
{"epoch": 7, "training_loss": 72.1691210269928, "training_acc": 47.0, "val_loss": 17.367984354496002, "val_acc": 52.0}
{"epoch": 8, "training_loss": 68.51104235649109, "training_acc": 53.0, "val_loss": 17.6640123128891, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.99145412445068, "training_acc": 53.0, "val_loss": 17.364221811294556, "val_acc": 52.0}
{"epoch": 10, "training_loss": 72.15458631515503, "training_acc": 47.0, "val_loss": 18.038980662822723, "val_acc": 52.0}
{"epoch": 11, "training_loss": 71.62801456451416, "training_acc": 47.0, "val_loss": 17.32342094182968, "val_acc": 52.0}
{"epoch": 12, "training_loss": 70.13201427459717, "training_acc": 53.0, "val_loss": 17.925181984901428, "val_acc": 52.0}
{"epoch": 13, "training_loss": 72.22753572463989, "training_acc": 53.0, "val_loss": 17.37181693315506, "val_acc": 52.0}
{"epoch": 14, "training_loss": 70.90756320953369, "training_acc": 47.0, "val_loss": 17.339150607585907, "val_acc": 52.0}
{"epoch": 15, "training_loss": 68.77196598052979, "training_acc": 63.0, "val_loss": 17.49032586812973, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.35381269454956, "training_acc": 53.0, "val_loss": 17.31870472431183, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.86152672767639, "training_acc": 41.0, "val_loss": 17.324919998645782, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.12231397628784, "training_acc": 55.0, "val_loss": 17.33042150735855, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.53353834152222, "training_acc": 53.0, "val_loss": 17.315155267715454, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.17947149276733, "training_acc": 53.0, "val_loss": 17.340315878391266, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.70897150039673, "training_acc": 53.0, "val_loss": 17.312972247600555, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.26686239242554, "training_acc": 53.0, "val_loss": 17.32115149497986, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.27866315841675, "training_acc": 51.0, "val_loss": 17.368637025356293, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.65469312667847, "training_acc": 47.0, "val_loss": 17.36799329519272, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.60593938827515, "training_acc": 41.0, "val_loss": 17.31511652469635, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.19716000556946, "training_acc": 53.0, "val_loss": 17.33078360557556, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.09218311309814, "training_acc": 53.0, "val_loss": 17.398051917552948, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.31166934967041, "training_acc": 53.0, "val_loss": 17.426328361034393, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.37812376022339, "training_acc": 53.0, "val_loss": 17.469801008701324, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.58026194572449, "training_acc": 53.0, "val_loss": 17.505382001399994, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.50417470932007, "training_acc": 53.0, "val_loss": 17.376311123371124, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.19372510910034, "training_acc": 53.0, "val_loss": 17.341242730617523, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.39143776893616, "training_acc": 52.0, "val_loss": 17.389385402202606, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.61116099357605, "training_acc": 47.0, "val_loss": 17.344167828559875, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.14294290542603, "training_acc": 53.0, "val_loss": 17.318660020828247, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.12156462669373, "training_acc": 53.0, "val_loss": 17.319825291633606, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.18625164031982, "training_acc": 53.0, "val_loss": 17.31814593076706, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.20292258262634, "training_acc": 53.0, "val_loss": 17.349551618099213, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.32813215255737, "training_acc": 53.0, "val_loss": 17.345736920833588, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.24112749099731, "training_acc": 53.0, "val_loss": 17.310383915901184, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.24278807640076, "training_acc": 53.0, "val_loss": 17.314495146274567, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.18874001502991, "training_acc": 53.0, "val_loss": 17.309823632240295, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.26414561271667, "training_acc": 53.0, "val_loss": 17.37339049577713, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.26989555358887, "training_acc": 53.0, "val_loss": 17.335253953933716, "val_acc": 52.0}
{"epoch": 45, "training_loss": 69.18744707107544, "training_acc": 53.0, "val_loss": 17.33333170413971, "val_acc": 52.0}
{"epoch": 46, "training_loss": 69.14942789077759, "training_acc": 53.0, "val_loss": 17.330172657966614, "val_acc": 52.0}
{"epoch": 47, "training_loss": 69.1447639465332, "training_acc": 53.0, "val_loss": 17.326416075229645, "val_acc": 52.0}
{"epoch": 48, "training_loss": 69.23954582214355, "training_acc": 53.0, "val_loss": 17.33642667531967, "val_acc": 52.0}
{"epoch": 49, "training_loss": 69.33790254592896, "training_acc": 53.0, "val_loss": 17.31639802455902, "val_acc": 52.0}
{"epoch": 50, "training_loss": 69.16903758049011, "training_acc": 53.0, "val_loss": 17.31853485107422, "val_acc": 52.0}
{"epoch": 51, "training_loss": 69.17786192893982, "training_acc": 53.0, "val_loss": 17.320193350315094, "val_acc": 52.0}
{"epoch": 52, "training_loss": 69.18210554122925, "training_acc": 53.0, "val_loss": 17.3215389251709, "val_acc": 52.0}
{"epoch": 53, "training_loss": 69.19697117805481, "training_acc": 53.0, "val_loss": 17.321525514125824, "val_acc": 52.0}
{"epoch": 54, "training_loss": 69.13657569885254, "training_acc": 53.0, "val_loss": 17.328673601150513, "val_acc": 52.0}
{"epoch": 55, "training_loss": 69.13626909255981, "training_acc": 53.0, "val_loss": 17.327775061130524, "val_acc": 52.0}
{"epoch": 56, "training_loss": 69.26651573181152, "training_acc": 53.0, "val_loss": 17.34106093645096, "val_acc": 52.0}
{"epoch": 57, "training_loss": 69.13521814346313, "training_acc": 53.0, "val_loss": 17.323072254657745, "val_acc": 52.0}
{"epoch": 58, "training_loss": 69.17871379852295, "training_acc": 58.0, "val_loss": 17.371390759944916, "val_acc": 52.0}
{"epoch": 59, "training_loss": 69.69503664970398, "training_acc": 47.0, "val_loss": 17.381592094898224, "val_acc": 52.0}
{"epoch": 60, "training_loss": 69.50339913368225, "training_acc": 46.0, "val_loss": 17.314964532852173, "val_acc": 52.0}
{"epoch": 61, "training_loss": 69.21888184547424, "training_acc": 53.0, "val_loss": 17.31397807598114, "val_acc": 52.0}
