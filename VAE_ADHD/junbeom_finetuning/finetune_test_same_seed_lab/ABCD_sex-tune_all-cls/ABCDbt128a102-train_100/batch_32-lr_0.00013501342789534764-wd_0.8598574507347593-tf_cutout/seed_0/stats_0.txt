"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 70.83094239234924, "training_acc": 52.0, "val_loss": 18.309955298900604, "val_acc": 56.0}
{"epoch": 1, "training_loss": 75.22368144989014, "training_acc": 52.0, "val_loss": 17.203837633132935, "val_acc": 56.0}
{"epoch": 2, "training_loss": 74.41614246368408, "training_acc": 52.0, "val_loss": 17.164522409439087, "val_acc": 56.0}
{"epoch": 3, "training_loss": 69.25511693954468, "training_acc": 52.0, "val_loss": 18.26002448797226, "val_acc": 56.0}
{"epoch": 4, "training_loss": 72.86074113845825, "training_acc": 48.0, "val_loss": 17.227432131767273, "val_acc": 56.0}
{"epoch": 5, "training_loss": 71.02470445632935, "training_acc": 52.0, "val_loss": 17.56581813097, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.9592490196228, "training_acc": 47.0, "val_loss": 17.296989262104034, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.17073154449463, "training_acc": 52.0, "val_loss": 17.220871150493622, "val_acc": 56.0}
{"epoch": 8, "training_loss": 70.40984082221985, "training_acc": 52.0, "val_loss": 17.50863939523697, "val_acc": 56.0}
{"epoch": 9, "training_loss": 70.82943344116211, "training_acc": 43.0, "val_loss": 18.129603564739227, "val_acc": 56.0}
{"epoch": 10, "training_loss": 71.1992449760437, "training_acc": 48.0, "val_loss": 17.876532673835754, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.48560857772827, "training_acc": 48.0, "val_loss": 17.16809868812561, "val_acc": 56.0}
{"epoch": 12, "training_loss": 72.45165491104126, "training_acc": 52.0, "val_loss": 17.54506230354309, "val_acc": 56.0}
{"epoch": 13, "training_loss": 71.76758098602295, "training_acc": 52.0, "val_loss": 17.181575298309326, "val_acc": 56.0}
{"epoch": 14, "training_loss": 69.41288208961487, "training_acc": 52.0, "val_loss": 17.255523800849915, "val_acc": 56.0}
{"epoch": 15, "training_loss": 69.4741895198822, "training_acc": 47.0, "val_loss": 17.241761088371277, "val_acc": 56.0}
{"epoch": 16, "training_loss": 70.01349902153015, "training_acc": 52.0, "val_loss": 17.201268672943115, "val_acc": 56.0}
{"epoch": 17, "training_loss": 69.672123670578, "training_acc": 52.0, "val_loss": 17.245475947856903, "val_acc": 56.0}
{"epoch": 18, "training_loss": 69.10472679138184, "training_acc": 50.0, "val_loss": 17.670460045337677, "val_acc": 56.0}
{"epoch": 19, "training_loss": 70.35924172401428, "training_acc": 48.0, "val_loss": 17.820636928081512, "val_acc": 56.0}
{"epoch": 20, "training_loss": 69.7569043636322, "training_acc": 48.0, "val_loss": 17.240601778030396, "val_acc": 56.0}
{"epoch": 21, "training_loss": 69.18321371078491, "training_acc": 52.0, "val_loss": 17.164352536201477, "val_acc": 56.0}
{"epoch": 22, "training_loss": 69.60256624221802, "training_acc": 52.0, "val_loss": 17.200247943401337, "val_acc": 56.0}
{"epoch": 23, "training_loss": 69.16796207427979, "training_acc": 49.0, "val_loss": 17.337827384471893, "val_acc": 56.0}
{"epoch": 24, "training_loss": 69.4734878540039, "training_acc": 52.0, "val_loss": 17.23444163799286, "val_acc": 56.0}
{"epoch": 25, "training_loss": 69.22320413589478, "training_acc": 52.0, "val_loss": 17.283371090888977, "val_acc": 56.0}
{"epoch": 26, "training_loss": 69.24695873260498, "training_acc": 52.0, "val_loss": 17.31526404619217, "val_acc": 56.0}
{"epoch": 27, "training_loss": 69.18736934661865, "training_acc": 52.0, "val_loss": 17.358119785785675, "val_acc": 56.0}
{"epoch": 28, "training_loss": 69.27366852760315, "training_acc": 54.0, "val_loss": 17.3387348651886, "val_acc": 56.0}
{"epoch": 29, "training_loss": 69.18537998199463, "training_acc": 52.0, "val_loss": 17.22332388162613, "val_acc": 56.0}
{"epoch": 30, "training_loss": 69.89868760108948, "training_acc": 52.0, "val_loss": 17.24543869495392, "val_acc": 56.0}
{"epoch": 31, "training_loss": 70.35500574111938, "training_acc": 52.0, "val_loss": 17.31204241514206, "val_acc": 56.0}
{"epoch": 32, "training_loss": 71.22433376312256, "training_acc": 52.0, "val_loss": 17.40138977766037, "val_acc": 56.0}
{"epoch": 33, "training_loss": 71.32227659225464, "training_acc": 52.0, "val_loss": 17.259599268436432, "val_acc": 56.0}
{"epoch": 34, "training_loss": 69.78653025627136, "training_acc": 52.0, "val_loss": 17.192909121513367, "val_acc": 56.0}
{"epoch": 35, "training_loss": 69.3519389629364, "training_acc": 50.0, "val_loss": 17.39516854286194, "val_acc": 56.0}
{"epoch": 36, "training_loss": 69.37632322311401, "training_acc": 48.0, "val_loss": 17.380523681640625, "val_acc": 56.0}
{"epoch": 37, "training_loss": 69.20274496078491, "training_acc": 52.0, "val_loss": 17.26038008928299, "val_acc": 56.0}
{"epoch": 38, "training_loss": 69.20589995384216, "training_acc": 52.0, "val_loss": 17.2515869140625, "val_acc": 56.0}
{"epoch": 39, "training_loss": 69.16386270523071, "training_acc": 52.0, "val_loss": 17.33868420124054, "val_acc": 56.0}
{"epoch": 40, "training_loss": 69.52563953399658, "training_acc": 43.0, "val_loss": 17.47070848941803, "val_acc": 56.0}
