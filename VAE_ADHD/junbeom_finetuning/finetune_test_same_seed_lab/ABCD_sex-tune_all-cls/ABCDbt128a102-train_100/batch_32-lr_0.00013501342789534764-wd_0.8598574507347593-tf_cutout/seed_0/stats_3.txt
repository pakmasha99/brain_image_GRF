"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 74.76958990097046, "training_acc": 41.0, "val_loss": 17.679813504219055, "val_acc": 52.0}
{"epoch": 1, "training_loss": 75.54934430122375, "training_acc": 42.0, "val_loss": 17.73611605167389, "val_acc": 44.0}
{"epoch": 2, "training_loss": 71.02813291549683, "training_acc": 47.0, "val_loss": 17.86171644926071, "val_acc": 44.0}
{"epoch": 3, "training_loss": 70.30266857147217, "training_acc": 57.0, "val_loss": 18.164141476154327, "val_acc": 52.0}
{"epoch": 4, "training_loss": 73.04866862297058, "training_acc": 53.0, "val_loss": 17.42793768644333, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.63889050483704, "training_acc": 55.0, "val_loss": 17.39828884601593, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.2847580909729, "training_acc": 52.0, "val_loss": 17.626196146011353, "val_acc": 52.0}
{"epoch": 7, "training_loss": 71.16047596931458, "training_acc": 53.0, "val_loss": 17.24987030029297, "val_acc": 52.0}
{"epoch": 8, "training_loss": 70.78934574127197, "training_acc": 53.0, "val_loss": 17.365826666355133, "val_acc": 52.0}
{"epoch": 9, "training_loss": 71.37328052520752, "training_acc": 45.0, "val_loss": 17.42331087589264, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.93123817443848, "training_acc": 48.0, "val_loss": 17.372100055217743, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.60009264945984, "training_acc": 45.0, "val_loss": 17.390352487564087, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.39246082305908, "training_acc": 53.0, "val_loss": 17.392250895500183, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.72915458679199, "training_acc": 53.0, "val_loss": 17.638148367404938, "val_acc": 52.0}
{"epoch": 14, "training_loss": 70.09997987747192, "training_acc": 53.0, "val_loss": 17.615224421024323, "val_acc": 52.0}
{"epoch": 15, "training_loss": 70.39498972892761, "training_acc": 53.0, "val_loss": 17.385224997997284, "val_acc": 52.0}
{"epoch": 16, "training_loss": 68.81096839904785, "training_acc": 52.0, "val_loss": 17.431090772151947, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.92225432395935, "training_acc": 47.0, "val_loss": 17.345881462097168, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.66309070587158, "training_acc": 45.0, "val_loss": 17.37831085920334, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.54413962364197, "training_acc": 44.0, "val_loss": 17.332884669303894, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.44610834121704, "training_acc": 45.0, "val_loss": 17.403963208198547, "val_acc": 52.0}
{"epoch": 21, "training_loss": 70.64004778862, "training_acc": 47.0, "val_loss": 17.570172250270844, "val_acc": 52.0}
{"epoch": 22, "training_loss": 70.04237866401672, "training_acc": 47.0, "val_loss": 17.329907417297363, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.0406436920166, "training_acc": 53.0, "val_loss": 17.341621220111847, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.16701316833496, "training_acc": 53.0, "val_loss": 17.317180335521698, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.24966192245483, "training_acc": 53.0, "val_loss": 17.312294244766235, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.22011303901672, "training_acc": 49.0, "val_loss": 17.37220287322998, "val_acc": 52.0}
