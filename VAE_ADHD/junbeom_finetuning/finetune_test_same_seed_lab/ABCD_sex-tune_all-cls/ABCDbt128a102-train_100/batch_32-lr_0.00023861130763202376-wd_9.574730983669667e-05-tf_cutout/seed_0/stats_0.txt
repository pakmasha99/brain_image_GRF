"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.24603700637817, "training_acc": 48.0, "val_loss": 17.232541739940643, "val_acc": 56.0}
{"epoch": 1, "training_loss": 69.35622787475586, "training_acc": 52.0, "val_loss": 17.234618961811066, "val_acc": 56.0}
{"epoch": 2, "training_loss": 69.67810416221619, "training_acc": 42.0, "val_loss": 17.182447016239166, "val_acc": 56.0}
{"epoch": 3, "training_loss": 69.45924544334412, "training_acc": 52.0, "val_loss": 17.180299758911133, "val_acc": 56.0}
{"epoch": 4, "training_loss": 69.32160472869873, "training_acc": 52.0, "val_loss": 17.202799022197723, "val_acc": 56.0}
{"epoch": 5, "training_loss": 69.38767099380493, "training_acc": 52.0, "val_loss": 17.200946807861328, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.31812381744385, "training_acc": 52.0, "val_loss": 17.232848703861237, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.35836029052734, "training_acc": 52.0, "val_loss": 17.236845195293427, "val_acc": 56.0}
{"epoch": 8, "training_loss": 69.55647587776184, "training_acc": 52.0, "val_loss": 17.25628226995468, "val_acc": 56.0}
{"epoch": 9, "training_loss": 69.57437133789062, "training_acc": 52.0, "val_loss": 17.372268438339233, "val_acc": 56.0}
{"epoch": 10, "training_loss": 69.4489197731018, "training_acc": 48.0, "val_loss": 17.404380440711975, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.37870860099792, "training_acc": 52.0, "val_loss": 17.21278578042984, "val_acc": 56.0}
{"epoch": 12, "training_loss": 69.8372573852539, "training_acc": 52.0, "val_loss": 17.148147523403168, "val_acc": 56.0}
{"epoch": 13, "training_loss": 69.61795091629028, "training_acc": 52.0, "val_loss": 17.148718237876892, "val_acc": 56.0}
{"epoch": 14, "training_loss": 69.56155967712402, "training_acc": 52.0, "val_loss": 17.14816987514496, "val_acc": 56.0}
{"epoch": 15, "training_loss": 69.59467601776123, "training_acc": 52.0, "val_loss": 17.151246964931488, "val_acc": 56.0}
{"epoch": 16, "training_loss": 69.77267956733704, "training_acc": 52.0, "val_loss": 17.149245738983154, "val_acc": 56.0}
{"epoch": 17, "training_loss": 69.46500825881958, "training_acc": 52.0, "val_loss": 17.21329391002655, "val_acc": 56.0}
{"epoch": 18, "training_loss": 69.14668989181519, "training_acc": 54.0, "val_loss": 17.48318374156952, "val_acc": 56.0}
{"epoch": 19, "training_loss": 69.82907676696777, "training_acc": 48.0, "val_loss": 17.611104249954224, "val_acc": 56.0}
{"epoch": 20, "training_loss": 69.62574100494385, "training_acc": 48.0, "val_loss": 17.340613901615143, "val_acc": 56.0}
{"epoch": 21, "training_loss": 69.25156664848328, "training_acc": 52.0, "val_loss": 17.23606586456299, "val_acc": 56.0}
{"epoch": 22, "training_loss": 69.39854025840759, "training_acc": 52.0, "val_loss": 17.212706804275513, "val_acc": 56.0}
{"epoch": 23, "training_loss": 69.23713898658752, "training_acc": 52.0, "val_loss": 17.23540425300598, "val_acc": 56.0}
{"epoch": 24, "training_loss": 69.54260778427124, "training_acc": 52.0, "val_loss": 17.172202467918396, "val_acc": 56.0}
{"epoch": 25, "training_loss": 69.27937412261963, "training_acc": 52.0, "val_loss": 17.235496640205383, "val_acc": 56.0}
{"epoch": 26, "training_loss": 69.33794736862183, "training_acc": 52.0, "val_loss": 17.254658043384552, "val_acc": 56.0}
{"epoch": 27, "training_loss": 69.23227882385254, "training_acc": 52.0, "val_loss": 17.29794591665268, "val_acc": 56.0}
{"epoch": 28, "training_loss": 69.33070635795593, "training_acc": 52.0, "val_loss": 17.260871827602386, "val_acc": 56.0}
{"epoch": 29, "training_loss": 69.26292061805725, "training_acc": 52.0, "val_loss": 17.150744795799255, "val_acc": 56.0}
{"epoch": 30, "training_loss": 70.13543558120728, "training_acc": 52.0, "val_loss": 17.211242020130157, "val_acc": 56.0}
{"epoch": 31, "training_loss": 70.47456645965576, "training_acc": 52.0, "val_loss": 17.235416173934937, "val_acc": 56.0}
