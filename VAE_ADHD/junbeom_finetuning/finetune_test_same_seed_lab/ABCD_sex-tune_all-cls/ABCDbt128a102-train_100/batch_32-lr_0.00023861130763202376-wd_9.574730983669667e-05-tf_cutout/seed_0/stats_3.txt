"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 70.14465475082397, "training_acc": 37.0, "val_loss": 17.34059900045395, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.43309783935547, "training_acc": 45.0, "val_loss": 17.309175431728363, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.37482500076294, "training_acc": 53.0, "val_loss": 17.32661724090576, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.18309497833252, "training_acc": 53.0, "val_loss": 17.312049865722656, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.23626351356506, "training_acc": 53.0, "val_loss": 17.324773967266083, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.1305365562439, "training_acc": 53.0, "val_loss": 17.39879548549652, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.36571025848389, "training_acc": 53.0, "val_loss": 17.53239333629608, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.85419654846191, "training_acc": 53.0, "val_loss": 17.43093430995941, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.2306604385376, "training_acc": 53.0, "val_loss": 17.334800958633423, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.25529527664185, "training_acc": 53.0, "val_loss": 17.335987091064453, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.09919118881226, "training_acc": 53.0, "val_loss": 17.308974266052246, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.15220618247986, "training_acc": 53.0, "val_loss": 17.31303781270981, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.26432800292969, "training_acc": 53.0, "val_loss": 17.33688712120056, "val_acc": 52.0}
{"epoch": 13, "training_loss": 70.29199814796448, "training_acc": 47.0, "val_loss": 17.39969551563263, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.33819389343262, "training_acc": 45.0, "val_loss": 17.313429713249207, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.20842456817627, "training_acc": 53.0, "val_loss": 17.312097549438477, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.49813866615295, "training_acc": 53.0, "val_loss": 17.36314594745636, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.27277612686157, "training_acc": 53.0, "val_loss": 17.351852357387543, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.28298282623291, "training_acc": 53.0, "val_loss": 17.326483130455017, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.16631484031677, "training_acc": 53.0, "val_loss": 17.339639365673065, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.17708444595337, "training_acc": 53.0, "val_loss": 17.328180372714996, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.21779346466064, "training_acc": 53.0, "val_loss": 17.33158826828003, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.34620666503906, "training_acc": 53.0, "val_loss": 17.43551641702652, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.89528846740723, "training_acc": 53.0, "val_loss": 17.408940196037292, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.44181752204895, "training_acc": 53.0, "val_loss": 17.311035096645355, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.2609658241272, "training_acc": 51.0, "val_loss": 17.392516136169434, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.82814335823059, "training_acc": 47.0, "val_loss": 17.3810675740242, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.50326108932495, "training_acc": 47.0, "val_loss": 17.312079668045044, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.1418776512146, "training_acc": 53.0, "val_loss": 17.376112937927246, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.26387929916382, "training_acc": 53.0, "val_loss": 17.501430213451385, "val_acc": 52.0}
