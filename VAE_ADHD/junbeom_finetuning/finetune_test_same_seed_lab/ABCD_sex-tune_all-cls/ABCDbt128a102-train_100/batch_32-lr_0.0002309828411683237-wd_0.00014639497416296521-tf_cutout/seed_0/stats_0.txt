"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.2793562412262, "training_acc": 49.0, "val_loss": 17.20433533191681, "val_acc": 56.0}
{"epoch": 1, "training_loss": 69.28716611862183, "training_acc": 52.0, "val_loss": 17.243370413780212, "val_acc": 56.0}
{"epoch": 2, "training_loss": 69.6975736618042, "training_acc": 42.0, "val_loss": 17.208001017570496, "val_acc": 56.0}
{"epoch": 3, "training_loss": 69.42432260513306, "training_acc": 52.0, "val_loss": 17.19965934753418, "val_acc": 56.0}
{"epoch": 4, "training_loss": 69.30076599121094, "training_acc": 52.0, "val_loss": 17.212508618831635, "val_acc": 56.0}
{"epoch": 5, "training_loss": 69.37924408912659, "training_acc": 52.0, "val_loss": 17.173972725868225, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.31737470626831, "training_acc": 52.0, "val_loss": 17.219798266887665, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.35876297950745, "training_acc": 52.0, "val_loss": 17.262539267539978, "val_acc": 56.0}
{"epoch": 8, "training_loss": 69.53594994544983, "training_acc": 52.0, "val_loss": 17.221495509147644, "val_acc": 56.0}
{"epoch": 9, "training_loss": 69.44488382339478, "training_acc": 52.0, "val_loss": 17.361779510974884, "val_acc": 56.0}
{"epoch": 10, "training_loss": 69.52127456665039, "training_acc": 48.0, "val_loss": 17.563167214393616, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.5672664642334, "training_acc": 48.0, "val_loss": 17.26149320602417, "val_acc": 56.0}
{"epoch": 12, "training_loss": 69.78367590904236, "training_acc": 52.0, "val_loss": 17.149367928504944, "val_acc": 56.0}
{"epoch": 13, "training_loss": 69.53968334197998, "training_acc": 52.0, "val_loss": 17.148447036743164, "val_acc": 56.0}
{"epoch": 14, "training_loss": 69.62558269500732, "training_acc": 52.0, "val_loss": 17.149342596530914, "val_acc": 56.0}
{"epoch": 15, "training_loss": 69.67751359939575, "training_acc": 52.0, "val_loss": 17.148147523403168, "val_acc": 56.0}
{"epoch": 16, "training_loss": 69.84157752990723, "training_acc": 52.0, "val_loss": 17.151767015457153, "val_acc": 56.0}
{"epoch": 17, "training_loss": 69.52072358131409, "training_acc": 52.0, "val_loss": 17.194482684135437, "val_acc": 56.0}
{"epoch": 18, "training_loss": 69.14846801757812, "training_acc": 52.0, "val_loss": 17.43086129426956, "val_acc": 56.0}
{"epoch": 19, "training_loss": 69.68977403640747, "training_acc": 48.0, "val_loss": 17.555974423885345, "val_acc": 56.0}
{"epoch": 20, "training_loss": 69.54652285575867, "training_acc": 48.0, "val_loss": 17.33304113149643, "val_acc": 56.0}
{"epoch": 21, "training_loss": 69.24685740470886, "training_acc": 52.0, "val_loss": 17.238077521324158, "val_acc": 56.0}
{"epoch": 22, "training_loss": 69.37289881706238, "training_acc": 52.0, "val_loss": 17.22209006547928, "val_acc": 56.0}
{"epoch": 23, "training_loss": 69.23892545700073, "training_acc": 52.0, "val_loss": 17.247775197029114, "val_acc": 56.0}
{"epoch": 24, "training_loss": 69.55391836166382, "training_acc": 52.0, "val_loss": 17.17264950275421, "val_acc": 56.0}
{"epoch": 25, "training_loss": 69.28720569610596, "training_acc": 52.0, "val_loss": 17.225348949432373, "val_acc": 56.0}
{"epoch": 26, "training_loss": 69.37905621528625, "training_acc": 52.0, "val_loss": 17.24611669778824, "val_acc": 56.0}
{"epoch": 27, "training_loss": 69.22305917739868, "training_acc": 52.0, "val_loss": 17.30107069015503, "val_acc": 56.0}
{"epoch": 28, "training_loss": 69.3434374332428, "training_acc": 52.0, "val_loss": 17.256487905979156, "val_acc": 56.0}
{"epoch": 29, "training_loss": 69.279052734375, "training_acc": 52.0, "val_loss": 17.149299383163452, "val_acc": 56.0}
{"epoch": 30, "training_loss": 70.17855167388916, "training_acc": 52.0, "val_loss": 17.207571864128113, "val_acc": 56.0}
{"epoch": 31, "training_loss": 70.44940519332886, "training_acc": 52.0, "val_loss": 17.220984399318695, "val_acc": 56.0}
{"epoch": 32, "training_loss": 70.8520917892456, "training_acc": 52.0, "val_loss": 17.30419099330902, "val_acc": 56.0}
{"epoch": 33, "training_loss": 70.96803951263428, "training_acc": 52.0, "val_loss": 17.184258997440338, "val_acc": 56.0}
{"epoch": 34, "training_loss": 69.53280639648438, "training_acc": 52.0, "val_loss": 17.202317714691162, "val_acc": 56.0}
