"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 73.01401686668396, "training_acc": 45.0, "val_loss": 17.436698079109192, "val_acc": 56.0}
{"epoch": 1, "training_loss": 68.33331727981567, "training_acc": 58.0, "val_loss": 17.91183650493622, "val_acc": 52.0}
{"epoch": 2, "training_loss": 75.66214609146118, "training_acc": 48.0, "val_loss": 19.868101179599762, "val_acc": 52.0}
{"epoch": 3, "training_loss": 78.03954744338989, "training_acc": 53.0, "val_loss": 17.581135034561157, "val_acc": 52.0}
{"epoch": 4, "training_loss": 70.13437080383301, "training_acc": 49.0, "val_loss": 17.849911749362946, "val_acc": 52.0}
{"epoch": 5, "training_loss": 72.99465703964233, "training_acc": 47.0, "val_loss": 17.949187755584717, "val_acc": 52.0}
{"epoch": 6, "training_loss": 71.46715116500854, "training_acc": 47.0, "val_loss": 17.313973605632782, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.3354640007019, "training_acc": 48.0, "val_loss": 17.431624233722687, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.47334909439087, "training_acc": 53.0, "val_loss": 17.365306615829468, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.16946578025818, "training_acc": 53.0, "val_loss": 17.44855046272278, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.53741192817688, "training_acc": 53.0, "val_loss": 17.705173790454865, "val_acc": 52.0}
{"epoch": 11, "training_loss": 70.43435382843018, "training_acc": 53.0, "val_loss": 17.637228965759277, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.72370982170105, "training_acc": 53.0, "val_loss": 17.307527363300323, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.17285013198853, "training_acc": 56.0, "val_loss": 17.380908131599426, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.63495779037476, "training_acc": 48.0, "val_loss": 17.303678393363953, "val_acc": 52.0}
{"epoch": 15, "training_loss": 68.90212941169739, "training_acc": 53.0, "val_loss": 17.385874688625336, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.3491940498352, "training_acc": 53.0, "val_loss": 17.33352243900299, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.13196659088135, "training_acc": 53.0, "val_loss": 17.28983223438263, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.31994295120239, "training_acc": 51.0, "val_loss": 17.325694859027863, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.3734769821167, "training_acc": 50.0, "val_loss": 17.297768592834473, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.18564414978027, "training_acc": 52.0, "val_loss": 17.29365885257721, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.04499101638794, "training_acc": 53.0, "val_loss": 17.315417528152466, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.31907486915588, "training_acc": 51.0, "val_loss": 17.34924614429474, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.28998899459839, "training_acc": 48.0, "val_loss": 17.308636009693146, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.08517265319824, "training_acc": 53.0, "val_loss": 17.356307804584503, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.1692726612091, "training_acc": 53.0, "val_loss": 17.356058955192566, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.12647199630737, "training_acc": 53.0, "val_loss": 17.335647344589233, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.08789491653442, "training_acc": 53.0, "val_loss": 17.32463538646698, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.03146004676819, "training_acc": 53.0, "val_loss": 17.332172393798828, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.05470561981201, "training_acc": 53.0, "val_loss": 17.32071489095688, "val_acc": 52.0}
{"epoch": 30, "training_loss": 68.9830527305603, "training_acc": 53.0, "val_loss": 17.32606142759323, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.0224175453186, "training_acc": 53.0, "val_loss": 17.297373712062836, "val_acc": 52.0}
{"epoch": 32, "training_loss": 68.8727912902832, "training_acc": 53.0, "val_loss": 17.294898629188538, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.59887409210205, "training_acc": 44.0, "val_loss": 17.34895259141922, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.78219103813171, "training_acc": 44.0, "val_loss": 17.37450361251831, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.8817458152771, "training_acc": 53.0, "val_loss": 17.420729994773865, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.78742671012878, "training_acc": 52.0, "val_loss": 17.290565371513367, "val_acc": 52.0}
