"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 71.50084638595581, "training_acc": 47.0, "val_loss": 17.324742674827576, "val_acc": 52.0}
{"epoch": 1, "training_loss": 74.37778353691101, "training_acc": 43.0, "val_loss": 17.55618304014206, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.09935259819031, "training_acc": 53.0, "val_loss": 17.719104886054993, "val_acc": 52.0}
{"epoch": 3, "training_loss": 70.80627131462097, "training_acc": 53.0, "val_loss": 17.313677072525024, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.89052486419678, "training_acc": 47.0, "val_loss": 17.318521440029144, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.91897869110107, "training_acc": 53.0, "val_loss": 17.36871749162674, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.45728874206543, "training_acc": 53.0, "val_loss": 17.330928146839142, "val_acc": 52.0}
{"epoch": 7, "training_loss": 70.52391934394836, "training_acc": 47.0, "val_loss": 17.567256093025208, "val_acc": 52.0}
{"epoch": 8, "training_loss": 70.51714372634888, "training_acc": 41.0, "val_loss": 17.33155846595764, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.20106625556946, "training_acc": 53.0, "val_loss": 17.335574328899384, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.37478971481323, "training_acc": 53.0, "val_loss": 17.428958415985107, "val_acc": 52.0}
{"epoch": 11, "training_loss": 70.03158283233643, "training_acc": 53.0, "val_loss": 17.832285165786743, "val_acc": 52.0}
{"epoch": 12, "training_loss": 70.30708193778992, "training_acc": 53.0, "val_loss": 17.336948215961456, "val_acc": 52.0}
{"epoch": 13, "training_loss": 70.33244943618774, "training_acc": 37.0, "val_loss": 17.414481937885284, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.83017253875732, "training_acc": 47.0, "val_loss": 17.316272854804993, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.57656002044678, "training_acc": 53.0, "val_loss": 17.37215220928192, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.93654584884644, "training_acc": 53.0, "val_loss": 17.633040249347687, "val_acc": 52.0}
{"epoch": 17, "training_loss": 70.10207033157349, "training_acc": 53.0, "val_loss": 17.445583641529083, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.33114337921143, "training_acc": 53.0, "val_loss": 17.322663962841034, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.25375509262085, "training_acc": 53.0, "val_loss": 17.310550808906555, "val_acc": 52.0}
{"epoch": 20, "training_loss": 70.23189973831177, "training_acc": 39.0, "val_loss": 17.35919862985611, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.01217579841614, "training_acc": 55.0, "val_loss": 17.376132309436798, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.83455657958984, "training_acc": 53.0, "val_loss": 17.388761043548584, "val_acc": 52.0}
{"epoch": 23, "training_loss": 70.04104661941528, "training_acc": 51.0, "val_loss": 17.347389459609985, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.47379970550537, "training_acc": 47.0, "val_loss": 17.308813333511353, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.4127128124237, "training_acc": 53.0, "val_loss": 17.30976402759552, "val_acc": 52.0}
{"epoch": 26, "training_loss": 70.04667329788208, "training_acc": 45.0, "val_loss": 17.768631875514984, "val_acc": 52.0}
{"epoch": 27, "training_loss": 71.66794061660767, "training_acc": 47.0, "val_loss": 17.47801899909973, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.85649228096008, "training_acc": 53.0, "val_loss": 17.85033345222473, "val_acc": 52.0}
{"epoch": 29, "training_loss": 73.41159915924072, "training_acc": 53.0, "val_loss": 18.18150281906128, "val_acc": 52.0}
{"epoch": 30, "training_loss": 70.67567753791809, "training_acc": 53.0, "val_loss": 17.32238680124283, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.87901020050049, "training_acc": 55.0, "val_loss": 17.395927011966705, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.99517059326172, "training_acc": 47.0, "val_loss": 17.514297366142273, "val_acc": 52.0}
{"epoch": 33, "training_loss": 70.10870313644409, "training_acc": 47.0, "val_loss": 17.3101007938385, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.06608080863953, "training_acc": 53.0, "val_loss": 17.466190457344055, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.84567785263062, "training_acc": 53.0, "val_loss": 17.33592599630356, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.2164568901062, "training_acc": 53.0, "val_loss": 17.32475608587265, "val_acc": 52.0}
{"epoch": 37, "training_loss": 70.2105188369751, "training_acc": 47.0, "val_loss": 17.374785244464874, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.68102359771729, "training_acc": 45.0, "val_loss": 17.35266000032425, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.49421072006226, "training_acc": 47.0, "val_loss": 17.37748682498932, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.5907187461853, "training_acc": 47.0, "val_loss": 17.326970398426056, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.61011576652527, "training_acc": 53.0, "val_loss": 17.337937653064728, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.27187657356262, "training_acc": 53.0, "val_loss": 17.383724451065063, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.36121439933777, "training_acc": 53.0, "val_loss": 17.40947514772415, "val_acc": 52.0}
