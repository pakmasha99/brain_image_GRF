"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 70.41477727890015, "training_acc": 51.0, "val_loss": 18.143530189990997, "val_acc": 44.0}
{"epoch": 1, "training_loss": 70.66053652763367, "training_acc": 46.0, "val_loss": 17.78995841741562, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.78471660614014, "training_acc": 53.0, "val_loss": 19.463492929935455, "val_acc": 52.0}
{"epoch": 3, "training_loss": 76.61189460754395, "training_acc": 53.0, "val_loss": 17.484313249588013, "val_acc": 52.0}
{"epoch": 4, "training_loss": 71.94579029083252, "training_acc": 47.0, "val_loss": 17.62184053659439, "val_acc": 52.0}
{"epoch": 5, "training_loss": 70.59863328933716, "training_acc": 47.0, "val_loss": 17.343254387378693, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.31918954849243, "training_acc": 51.0, "val_loss": 17.447957396507263, "val_acc": 52.0}
{"epoch": 7, "training_loss": 72.27405595779419, "training_acc": 47.0, "val_loss": 17.773650586605072, "val_acc": 52.0}
{"epoch": 8, "training_loss": 71.05066108703613, "training_acc": 47.0, "val_loss": 17.40783005952835, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.02676320075989, "training_acc": 56.0, "val_loss": 17.377860844135284, "val_acc": 52.0}
{"epoch": 10, "training_loss": 70.43579196929932, "training_acc": 53.0, "val_loss": 17.655695974826813, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.62659955024719, "training_acc": 53.0, "val_loss": 17.317111790180206, "val_acc": 52.0}
{"epoch": 12, "training_loss": 71.13248205184937, "training_acc": 41.0, "val_loss": 17.6096111536026, "val_acc": 52.0}
{"epoch": 13, "training_loss": 70.12329435348511, "training_acc": 47.0, "val_loss": 17.341232299804688, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.1730649471283, "training_acc": 59.0, "val_loss": 17.312030494213104, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.35048055648804, "training_acc": 53.0, "val_loss": 17.330682277679443, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.3028678894043, "training_acc": 53.0, "val_loss": 17.31874644756317, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.1604814529419, "training_acc": 53.0, "val_loss": 17.323531210422516, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.10483479499817, "training_acc": 53.0, "val_loss": 17.326360940933228, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.30171251296997, "training_acc": 45.0, "val_loss": 17.32259690761566, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.04226970672607, "training_acc": 53.0, "val_loss": 17.3176571726799, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.06929445266724, "training_acc": 53.0, "val_loss": 17.317630350589752, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.08555316925049, "training_acc": 53.0, "val_loss": 17.327548563480377, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.13503837585449, "training_acc": 53.0, "val_loss": 17.34784096479416, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.11002278327942, "training_acc": 53.0, "val_loss": 17.33124703168869, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.12260007858276, "training_acc": 53.0, "val_loss": 17.346160113811493, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.15459442138672, "training_acc": 53.0, "val_loss": 17.377673089504242, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.28491044044495, "training_acc": 53.0, "val_loss": 17.349810898303986, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.0282666683197, "training_acc": 53.0, "val_loss": 17.424842715263367, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.42938375473022, "training_acc": 53.0, "val_loss": 17.43798404932022, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.38873887062073, "training_acc": 53.0, "val_loss": 17.35963076353073, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.33823037147522, "training_acc": 53.0, "val_loss": 17.322689294815063, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.33584380149841, "training_acc": 48.0, "val_loss": 17.340688407421112, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.23552536964417, "training_acc": 53.0, "val_loss": 17.31095016002655, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.08687233924866, "training_acc": 53.0, "val_loss": 17.3098623752594, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.08427739143372, "training_acc": 53.0, "val_loss": 17.30288863182068, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.28752565383911, "training_acc": 54.0, "val_loss": 17.30020046234131, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.27852201461792, "training_acc": 52.0, "val_loss": 17.290836572647095, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.13172602653503, "training_acc": 53.0, "val_loss": 17.30731874704361, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.34390306472778, "training_acc": 53.0, "val_loss": 17.36205220222473, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.3630313873291, "training_acc": 53.0, "val_loss": 17.346641421318054, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.26293683052063, "training_acc": 53.0, "val_loss": 17.340388894081116, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.24298191070557, "training_acc": 53.0, "val_loss": 17.3183873295784, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.13446307182312, "training_acc": 59.0, "val_loss": 17.407719790935516, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.44603633880615, "training_acc": 49.0, "val_loss": 17.39402413368225, "val_acc": 52.0}
{"epoch": 45, "training_loss": 71.21616697311401, "training_acc": 53.0, "val_loss": 17.502661049365997, "val_acc": 52.0}
{"epoch": 46, "training_loss": 68.87555646896362, "training_acc": 56.0, "val_loss": 17.496532201766968, "val_acc": 52.0}
{"epoch": 47, "training_loss": 70.05442762374878, "training_acc": 48.0, "val_loss": 17.298905551433563, "val_acc": 52.0}
{"epoch": 48, "training_loss": 69.11103510856628, "training_acc": 53.0, "val_loss": 17.31688529253006, "val_acc": 52.0}
{"epoch": 49, "training_loss": 69.1769073009491, "training_acc": 53.0, "val_loss": 17.374850809574127, "val_acc": 52.0}
{"epoch": 50, "training_loss": 69.43864607810974, "training_acc": 53.0, "val_loss": 17.393310368061066, "val_acc": 52.0}
{"epoch": 51, "training_loss": 69.15709400177002, "training_acc": 53.0, "val_loss": 17.32761412858963, "val_acc": 52.0}
{"epoch": 52, "training_loss": 69.16431903839111, "training_acc": 53.0, "val_loss": 17.324909567832947, "val_acc": 52.0}
{"epoch": 53, "training_loss": 69.17055797576904, "training_acc": 53.0, "val_loss": 17.33449548482895, "val_acc": 52.0}
{"epoch": 54, "training_loss": 69.07156085968018, "training_acc": 53.0, "val_loss": 17.320866882801056, "val_acc": 52.0}
{"epoch": 55, "training_loss": 69.13971471786499, "training_acc": 53.0, "val_loss": 17.319689691066742, "val_acc": 52.0}
{"epoch": 56, "training_loss": 69.16895890235901, "training_acc": 53.0, "val_loss": 17.37002581357956, "val_acc": 52.0}
