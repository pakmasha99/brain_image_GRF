"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 67.60643529891968, "training_acc": 62.0, "val_loss": 17.098160088062286, "val_acc": 48.0}
{"epoch": 1, "training_loss": 68.55955982208252, "training_acc": 57.0, "val_loss": 18.70780885219574, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.33122229576111, "training_acc": 49.0, "val_loss": 17.649565637111664, "val_acc": 52.0}
{"epoch": 3, "training_loss": 65.74348044395447, "training_acc": 64.0, "val_loss": 18.36424469947815, "val_acc": 48.0}
{"epoch": 4, "training_loss": 70.53458380699158, "training_acc": 48.0, "val_loss": 17.91631430387497, "val_acc": 52.0}
{"epoch": 5, "training_loss": 66.31374406814575, "training_acc": 58.0, "val_loss": 17.600424587726593, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.78677487373352, "training_acc": 52.0, "val_loss": 17.8546741604805, "val_acc": 52.0}
{"epoch": 7, "training_loss": 71.0290138721466, "training_acc": 54.0, "val_loss": 17.790381610393524, "val_acc": 52.0}
{"epoch": 8, "training_loss": 68.24455237388611, "training_acc": 54.0, "val_loss": 17.824843525886536, "val_acc": 52.0}
{"epoch": 9, "training_loss": 67.84722638130188, "training_acc": 55.0, "val_loss": 17.379584908485413, "val_acc": 56.0}
{"epoch": 10, "training_loss": 71.11489796638489, "training_acc": 51.0, "val_loss": 17.37731844186783, "val_acc": 60.0}
{"epoch": 11, "training_loss": 72.33944892883301, "training_acc": 48.0, "val_loss": 17.030668258666992, "val_acc": 56.0}
{"epoch": 12, "training_loss": 69.44456148147583, "training_acc": 44.0, "val_loss": 16.965630650520325, "val_acc": 52.0}
{"epoch": 13, "training_loss": 68.53955125808716, "training_acc": 59.0, "val_loss": 17.42261052131653, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.1355619430542, "training_acc": 55.0, "val_loss": 17.9243803024292, "val_acc": 52.0}
{"epoch": 15, "training_loss": 68.5795726776123, "training_acc": 54.0, "val_loss": 18.01704615354538, "val_acc": 48.0}
{"epoch": 16, "training_loss": 68.39780926704407, "training_acc": 54.0, "val_loss": 18.03349256515503, "val_acc": 48.0}
{"epoch": 17, "training_loss": 68.6715898513794, "training_acc": 59.0, "val_loss": 18.1479275226593, "val_acc": 48.0}
{"epoch": 18, "training_loss": 69.53030514717102, "training_acc": 57.0, "val_loss": 18.213878571987152, "val_acc": 48.0}
{"epoch": 19, "training_loss": 68.82963180541992, "training_acc": 57.0, "val_loss": 18.271762132644653, "val_acc": 48.0}
{"epoch": 20, "training_loss": 68.24883031845093, "training_acc": 59.0, "val_loss": 18.35263818502426, "val_acc": 52.0}
{"epoch": 21, "training_loss": 67.71689820289612, "training_acc": 58.0, "val_loss": 18.214470148086548, "val_acc": 52.0}
{"epoch": 22, "training_loss": 67.81991481781006, "training_acc": 55.0, "val_loss": 17.958807945251465, "val_acc": 52.0}
{"epoch": 23, "training_loss": 67.72039413452148, "training_acc": 57.0, "val_loss": 18.140774965286255, "val_acc": 52.0}
{"epoch": 24, "training_loss": 67.28316283226013, "training_acc": 56.0, "val_loss": 17.629174888134003, "val_acc": 52.0}
{"epoch": 25, "training_loss": 65.32621884346008, "training_acc": 62.0, "val_loss": 18.120543658733368, "val_acc": 52.0}
{"epoch": 26, "training_loss": 67.21725296974182, "training_acc": 61.0, "val_loss": 17.825911939144135, "val_acc": 52.0}
{"epoch": 27, "training_loss": 67.22233963012695, "training_acc": 59.0, "val_loss": 17.175915837287903, "val_acc": 52.0}
{"epoch": 28, "training_loss": 65.51897120475769, "training_acc": 62.0, "val_loss": 16.12197905778885, "val_acc": 56.0}
{"epoch": 29, "training_loss": 66.18164873123169, "training_acc": 56.0, "val_loss": 16.298647224903107, "val_acc": 56.0}
{"epoch": 30, "training_loss": 67.0201473236084, "training_acc": 60.0, "val_loss": 16.34453535079956, "val_acc": 56.0}
{"epoch": 31, "training_loss": 67.30267643928528, "training_acc": 56.0, "val_loss": 17.55884438753128, "val_acc": 48.0}
{"epoch": 32, "training_loss": 67.32845783233643, "training_acc": 59.0, "val_loss": 16.808734834194183, "val_acc": 56.0}
{"epoch": 33, "training_loss": 67.91488289833069, "training_acc": 60.0, "val_loss": 17.82478243112564, "val_acc": 48.0}
{"epoch": 34, "training_loss": 68.17457151412964, "training_acc": 59.0, "val_loss": 17.846980690956116, "val_acc": 48.0}
{"epoch": 35, "training_loss": 68.14520144462585, "training_acc": 59.0, "val_loss": 17.944733798503876, "val_acc": 52.0}
{"epoch": 36, "training_loss": 68.28908252716064, "training_acc": 60.0, "val_loss": 17.619727551937103, "val_acc": 52.0}
{"epoch": 37, "training_loss": 72.25424861907959, "training_acc": 45.0, "val_loss": 17.487607896327972, "val_acc": 52.0}
{"epoch": 38, "training_loss": 72.43113422393799, "training_acc": 42.0, "val_loss": 17.20733940601349, "val_acc": 52.0}
{"epoch": 39, "training_loss": 70.34787893295288, "training_acc": 47.0, "val_loss": 17.119918763637543, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.17935395240784, "training_acc": 53.0, "val_loss": 17.240792512893677, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.6407151222229, "training_acc": 54.0, "val_loss": 17.667855322360992, "val_acc": 52.0}
{"epoch": 42, "training_loss": 71.29024600982666, "training_acc": 53.0, "val_loss": 17.70213693380356, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.91528797149658, "training_acc": 53.0, "val_loss": 17.17720776796341, "val_acc": 52.0}
{"epoch": 44, "training_loss": 70.2069730758667, "training_acc": 49.0, "val_loss": 17.586280405521393, "val_acc": 60.0}
{"epoch": 45, "training_loss": 71.70806741714478, "training_acc": 49.0, "val_loss": 17.244243621826172, "val_acc": 52.0}
{"epoch": 46, "training_loss": 69.85113668441772, "training_acc": 48.0, "val_loss": 17.174556851387024, "val_acc": 52.0}
{"epoch": 47, "training_loss": 69.10899925231934, "training_acc": 57.0, "val_loss": 17.371077835559845, "val_acc": 52.0}
