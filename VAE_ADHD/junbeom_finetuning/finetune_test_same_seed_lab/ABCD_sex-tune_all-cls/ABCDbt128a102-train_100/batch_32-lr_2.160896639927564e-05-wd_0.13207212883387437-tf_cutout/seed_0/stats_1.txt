"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 70.53250169754028, "training_acc": 55.0, "val_loss": 17.38440692424774, "val_acc": 52.0}
{"epoch": 1, "training_loss": 68.4396493434906, "training_acc": 58.0, "val_loss": 17.264263331890106, "val_acc": 52.0}
{"epoch": 2, "training_loss": 66.67054510116577, "training_acc": 59.0, "val_loss": 17.12746024131775, "val_acc": 56.0}
{"epoch": 3, "training_loss": 68.71277904510498, "training_acc": 62.0, "val_loss": 18.01631897687912, "val_acc": 52.0}
{"epoch": 4, "training_loss": 70.04811334609985, "training_acc": 52.0, "val_loss": 17.37430840730667, "val_acc": 52.0}
{"epoch": 5, "training_loss": 68.77842712402344, "training_acc": 53.0, "val_loss": 17.26089119911194, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.12828826904297, "training_acc": 52.0, "val_loss": 17.428874969482422, "val_acc": 52.0}
{"epoch": 7, "training_loss": 68.61192989349365, "training_acc": 55.0, "val_loss": 17.123687267303467, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.19993162155151, "training_acc": 51.0, "val_loss": 17.14470684528351, "val_acc": 52.0}
{"epoch": 9, "training_loss": 68.61566257476807, "training_acc": 56.0, "val_loss": 17.113448679447174, "val_acc": 52.0}
{"epoch": 10, "training_loss": 67.52190589904785, "training_acc": 55.0, "val_loss": 17.106875777244568, "val_acc": 52.0}
{"epoch": 11, "training_loss": 67.99676561355591, "training_acc": 56.0, "val_loss": 17.150837182998657, "val_acc": 52.0}
{"epoch": 12, "training_loss": 68.13346815109253, "training_acc": 54.0, "val_loss": 17.33507663011551, "val_acc": 52.0}
{"epoch": 13, "training_loss": 67.3288369178772, "training_acc": 57.0, "val_loss": 17.356152832508087, "val_acc": 52.0}
{"epoch": 14, "training_loss": 67.42023491859436, "training_acc": 58.0, "val_loss": 16.857610642910004, "val_acc": 52.0}
{"epoch": 15, "training_loss": 66.27849054336548, "training_acc": 62.0, "val_loss": 16.96118712425232, "val_acc": 56.0}
{"epoch": 16, "training_loss": 66.04613828659058, "training_acc": 62.0, "val_loss": 17.29971319437027, "val_acc": 56.0}
{"epoch": 17, "training_loss": 67.28344488143921, "training_acc": 60.0, "val_loss": 17.05358475446701, "val_acc": 52.0}
{"epoch": 18, "training_loss": 68.53786444664001, "training_acc": 57.0, "val_loss": 16.98823720216751, "val_acc": 52.0}
{"epoch": 19, "training_loss": 67.65370869636536, "training_acc": 55.0, "val_loss": 17.707687616348267, "val_acc": 52.0}
{"epoch": 20, "training_loss": 68.89492201805115, "training_acc": 56.0, "val_loss": 17.67183095216751, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.98145365715027, "training_acc": 53.0, "val_loss": 17.859728634357452, "val_acc": 48.0}
{"epoch": 22, "training_loss": 66.98938417434692, "training_acc": 58.0, "val_loss": 18.164488673210144, "val_acc": 52.0}
{"epoch": 23, "training_loss": 67.46638989448547, "training_acc": 59.0, "val_loss": 16.621960699558258, "val_acc": 52.0}
{"epoch": 24, "training_loss": 66.8388159275055, "training_acc": 56.0, "val_loss": 16.232439875602722, "val_acc": 56.0}
{"epoch": 25, "training_loss": 69.42525863647461, "training_acc": 54.0, "val_loss": 17.125388979911804, "val_acc": 52.0}
{"epoch": 26, "training_loss": 66.4912588596344, "training_acc": 59.0, "val_loss": 17.05455482006073, "val_acc": 56.0}
{"epoch": 27, "training_loss": 66.30586552619934, "training_acc": 62.0, "val_loss": 17.48722940683365, "val_acc": 56.0}
{"epoch": 28, "training_loss": 67.65479230880737, "training_acc": 52.0, "val_loss": 18.078716099262238, "val_acc": 60.0}
{"epoch": 29, "training_loss": 66.32949209213257, "training_acc": 60.0, "val_loss": 17.590007185935974, "val_acc": 52.0}
{"epoch": 30, "training_loss": 67.52772283554077, "training_acc": 59.0, "val_loss": 17.513370513916016, "val_acc": 56.0}
{"epoch": 31, "training_loss": 65.79287624359131, "training_acc": 65.0, "val_loss": 18.04015338420868, "val_acc": 56.0}
{"epoch": 32, "training_loss": 65.51530623435974, "training_acc": 58.0, "val_loss": 17.60668307542801, "val_acc": 56.0}
{"epoch": 33, "training_loss": 63.957934856414795, "training_acc": 64.0, "val_loss": 17.14469939470291, "val_acc": 52.0}
{"epoch": 34, "training_loss": 62.33322525024414, "training_acc": 70.0, "val_loss": 16.982384026050568, "val_acc": 56.0}
{"epoch": 35, "training_loss": 63.3730845451355, "training_acc": 62.0, "val_loss": 17.237231135368347, "val_acc": 60.0}
{"epoch": 36, "training_loss": 68.39200353622437, "training_acc": 59.0, "val_loss": 17.163893580436707, "val_acc": 56.0}
{"epoch": 37, "training_loss": 61.93492150306702, "training_acc": 65.0, "val_loss": 17.949676513671875, "val_acc": 56.0}
{"epoch": 38, "training_loss": 64.58145570755005, "training_acc": 67.0, "val_loss": 19.598288834095, "val_acc": 48.0}
{"epoch": 39, "training_loss": 66.86264371871948, "training_acc": 57.0, "val_loss": 17.87385195493698, "val_acc": 52.0}
{"epoch": 40, "training_loss": 61.405057430267334, "training_acc": 70.0, "val_loss": 17.89853870868683, "val_acc": 56.0}
{"epoch": 41, "training_loss": 66.00305271148682, "training_acc": 58.0, "val_loss": 18.32716315984726, "val_acc": 52.0}
{"epoch": 42, "training_loss": 67.87564325332642, "training_acc": 57.0, "val_loss": 17.063313722610474, "val_acc": 56.0}
{"epoch": 43, "training_loss": 64.28209352493286, "training_acc": 64.0, "val_loss": 16.6960671544075, "val_acc": 56.0}
