"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 71.70863747596741, "training_acc": 48.0, "val_loss": 18.77186894416809, "val_acc": 52.0}
{"epoch": 1, "training_loss": 74.56305360794067, "training_acc": 43.0, "val_loss": 18.67806762456894, "val_acc": 48.0}
{"epoch": 2, "training_loss": 73.79416179656982, "training_acc": 47.0, "val_loss": 18.749061226844788, "val_acc": 52.0}
{"epoch": 3, "training_loss": 73.23621129989624, "training_acc": 46.0, "val_loss": 18.700480461120605, "val_acc": 52.0}
{"epoch": 4, "training_loss": 73.22670269012451, "training_acc": 44.0, "val_loss": 18.619391322135925, "val_acc": 52.0}
{"epoch": 5, "training_loss": 72.56411790847778, "training_acc": 47.0, "val_loss": 18.54296326637268, "val_acc": 56.0}
{"epoch": 6, "training_loss": 71.82743883132935, "training_acc": 47.0, "val_loss": 18.52371245622635, "val_acc": 56.0}
{"epoch": 7, "training_loss": 71.0282335281372, "training_acc": 51.0, "val_loss": 18.46262663602829, "val_acc": 60.0}
{"epoch": 8, "training_loss": 70.71987724304199, "training_acc": 50.0, "val_loss": 18.416740000247955, "val_acc": 56.0}
{"epoch": 9, "training_loss": 71.0686571598053, "training_acc": 51.0, "val_loss": 18.38987171649933, "val_acc": 56.0}
{"epoch": 10, "training_loss": 70.12366580963135, "training_acc": 51.0, "val_loss": 18.335311114788055, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.50326776504517, "training_acc": 51.0, "val_loss": 18.269453942775726, "val_acc": 56.0}
{"epoch": 12, "training_loss": 68.67126536369324, "training_acc": 57.0, "val_loss": 18.214240670204163, "val_acc": 56.0}
{"epoch": 13, "training_loss": 69.4920220375061, "training_acc": 53.0, "val_loss": 18.164154887199402, "val_acc": 56.0}
{"epoch": 14, "training_loss": 69.37249398231506, "training_acc": 48.0, "val_loss": 18.12014728784561, "val_acc": 56.0}
{"epoch": 15, "training_loss": 68.87102222442627, "training_acc": 51.0, "val_loss": 18.114985525608063, "val_acc": 56.0}
{"epoch": 16, "training_loss": 68.53913044929504, "training_acc": 48.0, "val_loss": 18.12260001897812, "val_acc": 56.0}
{"epoch": 17, "training_loss": 68.59899139404297, "training_acc": 51.0, "val_loss": 18.130725622177124, "val_acc": 56.0}
{"epoch": 18, "training_loss": 68.34365272521973, "training_acc": 51.0, "val_loss": 18.112334609031677, "val_acc": 56.0}
{"epoch": 19, "training_loss": 68.41546249389648, "training_acc": 53.0, "val_loss": 18.1052029132843, "val_acc": 56.0}
{"epoch": 20, "training_loss": 68.06563329696655, "training_acc": 53.0, "val_loss": 18.084852397441864, "val_acc": 56.0}
{"epoch": 21, "training_loss": 67.9966549873352, "training_acc": 54.0, "val_loss": 18.048758804798126, "val_acc": 56.0}
{"epoch": 22, "training_loss": 68.42805910110474, "training_acc": 53.0, "val_loss": 18.013791739940643, "val_acc": 56.0}
{"epoch": 23, "training_loss": 67.40167355537415, "training_acc": 56.0, "val_loss": 17.994697391986847, "val_acc": 56.0}
{"epoch": 24, "training_loss": 67.86299753189087, "training_acc": 56.0, "val_loss": 17.977377772331238, "val_acc": 56.0}
{"epoch": 25, "training_loss": 67.01927995681763, "training_acc": 59.0, "val_loss": 17.96320676803589, "val_acc": 56.0}
{"epoch": 26, "training_loss": 66.91603660583496, "training_acc": 59.0, "val_loss": 17.958399653434753, "val_acc": 56.0}
{"epoch": 27, "training_loss": 67.0571870803833, "training_acc": 58.0, "val_loss": 17.948704957962036, "val_acc": 56.0}
{"epoch": 28, "training_loss": 66.3345730304718, "training_acc": 59.0, "val_loss": 17.826899886131287, "val_acc": 56.0}
{"epoch": 29, "training_loss": 66.38308095932007, "training_acc": 60.0, "val_loss": 17.72225946187973, "val_acc": 56.0}
{"epoch": 30, "training_loss": 66.84609031677246, "training_acc": 58.0, "val_loss": 17.477717995643616, "val_acc": 56.0}
{"epoch": 31, "training_loss": 66.19687032699585, "training_acc": 61.0, "val_loss": 17.345578968524933, "val_acc": 56.0}
{"epoch": 32, "training_loss": 66.6432876586914, "training_acc": 58.0, "val_loss": 17.317336797714233, "val_acc": 56.0}
{"epoch": 33, "training_loss": 66.75411319732666, "training_acc": 55.0, "val_loss": 17.320401966571808, "val_acc": 56.0}
{"epoch": 34, "training_loss": 66.38747453689575, "training_acc": 59.0, "val_loss": 17.45186150074005, "val_acc": 56.0}
{"epoch": 35, "training_loss": 65.91006708145142, "training_acc": 61.0, "val_loss": 17.69789457321167, "val_acc": 56.0}
{"epoch": 36, "training_loss": 65.65212631225586, "training_acc": 60.0, "val_loss": 17.843440175056458, "val_acc": 56.0}
{"epoch": 37, "training_loss": 66.1694769859314, "training_acc": 58.0, "val_loss": 17.975886166095734, "val_acc": 56.0}
{"epoch": 38, "training_loss": 65.79154539108276, "training_acc": 57.0, "val_loss": 18.068677186965942, "val_acc": 56.0}
{"epoch": 39, "training_loss": 65.41539120674133, "training_acc": 56.0, "val_loss": 18.128541111946106, "val_acc": 56.0}
{"epoch": 40, "training_loss": 65.34026670455933, "training_acc": 59.0, "val_loss": 18.161046504974365, "val_acc": 56.0}
{"epoch": 41, "training_loss": 65.22888398170471, "training_acc": 61.0, "val_loss": 18.201862275600433, "val_acc": 56.0}
{"epoch": 42, "training_loss": 65.57457113265991, "training_acc": 63.0, "val_loss": 18.2342529296875, "val_acc": 56.0}
{"epoch": 43, "training_loss": 65.46346020698547, "training_acc": 61.0, "val_loss": 18.24067085981369, "val_acc": 56.0}
{"epoch": 44, "training_loss": 65.35615921020508, "training_acc": 58.0, "val_loss": 18.19620281457901, "val_acc": 56.0}
{"epoch": 45, "training_loss": 65.1047043800354, "training_acc": 60.0, "val_loss": 18.11036467552185, "val_acc": 56.0}
{"epoch": 46, "training_loss": 64.57359170913696, "training_acc": 63.0, "val_loss": 18.08471977710724, "val_acc": 56.0}
{"epoch": 47, "training_loss": 64.8426628112793, "training_acc": 62.0, "val_loss": 18.09919625520706, "val_acc": 56.0}
{"epoch": 48, "training_loss": 64.79100012779236, "training_acc": 59.0, "val_loss": 18.156760931015015, "val_acc": 56.0}
{"epoch": 49, "training_loss": 64.37870669364929, "training_acc": 60.0, "val_loss": 18.221652507781982, "val_acc": 56.0}
{"epoch": 50, "training_loss": 64.52567052841187, "training_acc": 60.0, "val_loss": 18.27370375394821, "val_acc": 52.0}
{"epoch": 51, "training_loss": 64.78690767288208, "training_acc": 64.0, "val_loss": 18.388114869594574, "val_acc": 52.0}
