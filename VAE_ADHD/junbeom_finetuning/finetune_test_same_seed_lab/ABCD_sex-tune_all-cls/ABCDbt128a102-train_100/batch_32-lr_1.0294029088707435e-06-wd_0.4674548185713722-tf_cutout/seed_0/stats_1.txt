"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 73.97359490394592, "training_acc": 43.0, "val_loss": 18.164418637752533, "val_acc": 52.0}
{"epoch": 1, "training_loss": 72.10025572776794, "training_acc": 47.0, "val_loss": 18.139152228832245, "val_acc": 56.0}
{"epoch": 2, "training_loss": 71.01233077049255, "training_acc": 51.0, "val_loss": 18.15900057554245, "val_acc": 52.0}
{"epoch": 3, "training_loss": 70.54751086235046, "training_acc": 52.0, "val_loss": 18.205295503139496, "val_acc": 52.0}
{"epoch": 4, "training_loss": 70.42976975440979, "training_acc": 51.0, "val_loss": 18.246863782405853, "val_acc": 52.0}
{"epoch": 5, "training_loss": 70.1019332408905, "training_acc": 53.0, "val_loss": 18.26559603214264, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.34396266937256, "training_acc": 57.0, "val_loss": 18.275843560695648, "val_acc": 52.0}
{"epoch": 7, "training_loss": 68.99403119087219, "training_acc": 58.0, "val_loss": 18.27116310596466, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.59868907928467, "training_acc": 57.0, "val_loss": 18.246091902256012, "val_acc": 52.0}
{"epoch": 9, "training_loss": 68.83423519134521, "training_acc": 58.0, "val_loss": 18.23877841234207, "val_acc": 52.0}
{"epoch": 10, "training_loss": 68.78664994239807, "training_acc": 59.0, "val_loss": 18.20014864206314, "val_acc": 52.0}
{"epoch": 11, "training_loss": 68.31175136566162, "training_acc": 58.0, "val_loss": 18.20344477891922, "val_acc": 52.0}
{"epoch": 12, "training_loss": 68.4780125617981, "training_acc": 58.0, "val_loss": 18.190185725688934, "val_acc": 52.0}
{"epoch": 13, "training_loss": 68.4852032661438, "training_acc": 57.0, "val_loss": 18.208667635917664, "val_acc": 52.0}
{"epoch": 14, "training_loss": 68.26698446273804, "training_acc": 58.0, "val_loss": 18.224193155765533, "val_acc": 52.0}
{"epoch": 15, "training_loss": 67.84794092178345, "training_acc": 60.0, "val_loss": 18.247905373573303, "val_acc": 52.0}
{"epoch": 16, "training_loss": 67.71433782577515, "training_acc": 56.0, "val_loss": 18.258434534072876, "val_acc": 52.0}
{"epoch": 17, "training_loss": 67.55305337905884, "training_acc": 56.0, "val_loss": 18.2705357670784, "val_acc": 52.0}
{"epoch": 18, "training_loss": 67.19444513320923, "training_acc": 57.0, "val_loss": 18.21921169757843, "val_acc": 52.0}
{"epoch": 19, "training_loss": 67.13787508010864, "training_acc": 55.0, "val_loss": 18.116171658039093, "val_acc": 52.0}
{"epoch": 20, "training_loss": 67.27599668502808, "training_acc": 53.0, "val_loss": 18.00132691860199, "val_acc": 52.0}
{"epoch": 21, "training_loss": 66.8890609741211, "training_acc": 53.0, "val_loss": 18.008269369602203, "val_acc": 52.0}
{"epoch": 22, "training_loss": 66.5272102355957, "training_acc": 57.0, "val_loss": 18.00418496131897, "val_acc": 52.0}
{"epoch": 23, "training_loss": 66.61491918563843, "training_acc": 56.0, "val_loss": 18.046753108501434, "val_acc": 52.0}
{"epoch": 24, "training_loss": 66.52867388725281, "training_acc": 55.0, "val_loss": 18.07919293642044, "val_acc": 52.0}
{"epoch": 25, "training_loss": 66.51339936256409, "training_acc": 56.0, "val_loss": 18.153640627861023, "val_acc": 52.0}
{"epoch": 26, "training_loss": 66.45367622375488, "training_acc": 57.0, "val_loss": 18.187315762043, "val_acc": 52.0}
{"epoch": 27, "training_loss": 66.29285621643066, "training_acc": 59.0, "val_loss": 18.214938044548035, "val_acc": 52.0}
{"epoch": 28, "training_loss": 66.39091730117798, "training_acc": 58.0, "val_loss": 18.25690269470215, "val_acc": 52.0}
{"epoch": 29, "training_loss": 66.2742862701416, "training_acc": 58.0, "val_loss": 18.30371469259262, "val_acc": 52.0}
{"epoch": 30, "training_loss": 66.36044263839722, "training_acc": 59.0, "val_loss": 18.287719786167145, "val_acc": 52.0}
{"epoch": 31, "training_loss": 65.91705274581909, "training_acc": 58.0, "val_loss": 18.256692588329315, "val_acc": 52.0}
{"epoch": 32, "training_loss": 66.20372867584229, "training_acc": 59.0, "val_loss": 18.226821720600128, "val_acc": 52.0}
{"epoch": 33, "training_loss": 65.71878981590271, "training_acc": 60.0, "val_loss": 18.219807744026184, "val_acc": 52.0}
{"epoch": 34, "training_loss": 65.71985721588135, "training_acc": 56.0, "val_loss": 18.202219903469086, "val_acc": 52.0}
{"epoch": 35, "training_loss": 65.37271118164062, "training_acc": 60.0, "val_loss": 18.18881332874298, "val_acc": 52.0}
{"epoch": 36, "training_loss": 65.53229141235352, "training_acc": 61.0, "val_loss": 18.098348379135132, "val_acc": 52.0}
{"epoch": 37, "training_loss": 65.1031174659729, "training_acc": 59.0, "val_loss": 18.028685450553894, "val_acc": 52.0}
{"epoch": 38, "training_loss": 65.84117460250854, "training_acc": 58.0, "val_loss": 17.986202239990234, "val_acc": 52.0}
{"epoch": 39, "training_loss": 65.40024709701538, "training_acc": 62.0, "val_loss": 17.964808642864227, "val_acc": 52.0}
{"epoch": 40, "training_loss": 65.22685289382935, "training_acc": 63.0, "val_loss": 18.084539473056793, "val_acc": 52.0}
{"epoch": 41, "training_loss": 65.09377861022949, "training_acc": 62.0, "val_loss": 18.249981105327606, "val_acc": 52.0}
{"epoch": 42, "training_loss": 64.8749349117279, "training_acc": 63.0, "val_loss": 18.312576413154602, "val_acc": 52.0}
{"epoch": 43, "training_loss": 64.6019287109375, "training_acc": 63.0, "val_loss": 18.289300799369812, "val_acc": 52.0}
{"epoch": 44, "training_loss": 64.22187209129333, "training_acc": 64.0, "val_loss": 18.227286636829376, "val_acc": 52.0}
{"epoch": 45, "training_loss": 64.97521162033081, "training_acc": 63.0, "val_loss": 18.18438321352005, "val_acc": 52.0}
{"epoch": 46, "training_loss": 64.9985876083374, "training_acc": 63.0, "val_loss": 18.243810534477234, "val_acc": 52.0}
{"epoch": 47, "training_loss": 64.34229278564453, "training_acc": 66.0, "val_loss": 18.21066588163376, "val_acc": 52.0}
{"epoch": 48, "training_loss": 63.73248791694641, "training_acc": 66.0, "val_loss": 18.12424659729004, "val_acc": 52.0}
{"epoch": 49, "training_loss": 63.93765902519226, "training_acc": 66.0, "val_loss": 18.08265894651413, "val_acc": 52.0}
{"epoch": 50, "training_loss": 64.17108917236328, "training_acc": 66.0, "val_loss": 18.057699501514435, "val_acc": 52.0}
{"epoch": 51, "training_loss": 64.00334596633911, "training_acc": 64.0, "val_loss": 18.147651851177216, "val_acc": 52.0}
{"epoch": 52, "training_loss": 64.088463306427, "training_acc": 68.0, "val_loss": 18.36504638195038, "val_acc": 52.0}
{"epoch": 53, "training_loss": 63.86311340332031, "training_acc": 67.0, "val_loss": 18.459497392177582, "val_acc": 48.0}
{"epoch": 54, "training_loss": 63.85545825958252, "training_acc": 68.0, "val_loss": 18.509933352470398, "val_acc": 48.0}
{"epoch": 55, "training_loss": 63.61359405517578, "training_acc": 67.0, "val_loss": 18.539151549339294, "val_acc": 48.0}
{"epoch": 56, "training_loss": 63.36619520187378, "training_acc": 70.0, "val_loss": 18.601761758327484, "val_acc": 52.0}
{"epoch": 57, "training_loss": 63.60290765762329, "training_acc": 69.0, "val_loss": 18.566635251045227, "val_acc": 52.0}
{"epoch": 58, "training_loss": 63.73133850097656, "training_acc": 66.0, "val_loss": 18.440064787864685, "val_acc": 52.0}
