"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 70.37625646591187, "training_acc": 53.0, "val_loss": 17.389144003391266, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.45602774620056, "training_acc": 53.0, "val_loss": 17.335088551044464, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.87596821784973, "training_acc": 45.0, "val_loss": 17.408135533332825, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.50806069374084, "training_acc": 53.0, "val_loss": 17.538142204284668, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.72186517715454, "training_acc": 53.0, "val_loss": 17.395468056201935, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.18940615653992, "training_acc": 53.0, "val_loss": 17.311586439609528, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.65851593017578, "training_acc": 49.0, "val_loss": 17.390725016593933, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.5221471786499, "training_acc": 49.0, "val_loss": 17.313876748085022, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.0093994140625, "training_acc": 53.0, "val_loss": 17.38024652004242, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.32225227355957, "training_acc": 53.0, "val_loss": 17.337743937969208, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.24795198440552, "training_acc": 53.0, "val_loss": 17.333291471004486, "val_acc": 52.0}
{"epoch": 11, "training_loss": 70.32992458343506, "training_acc": 47.0, "val_loss": 17.446723580360413, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.7812352180481, "training_acc": 45.0, "val_loss": 17.309600114822388, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.09411239624023, "training_acc": 53.0, "val_loss": 17.367444932460785, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.57361102104187, "training_acc": 53.0, "val_loss": 17.46908575296402, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.50464105606079, "training_acc": 53.0, "val_loss": 17.39085018634796, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.37825679779053, "training_acc": 53.0, "val_loss": 17.354173958301544, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.2271523475647, "training_acc": 53.0, "val_loss": 17.308780550956726, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.39739394187927, "training_acc": 47.0, "val_loss": 17.49398559331894, "val_acc": 52.0}
{"epoch": 19, "training_loss": 70.46579051017761, "training_acc": 47.0, "val_loss": 17.426389455795288, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.66294384002686, "training_acc": 49.0, "val_loss": 17.40729808807373, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.76735067367554, "training_acc": 53.0, "val_loss": 17.48477965593338, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.440589427948, "training_acc": 53.0, "val_loss": 17.338265478610992, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.4444272518158, "training_acc": 53.0, "val_loss": 17.318332195281982, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.29335260391235, "training_acc": 53.0, "val_loss": 17.310114204883575, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.30481123924255, "training_acc": 53.0, "val_loss": 17.358313500881195, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.20862770080566, "training_acc": 53.0, "val_loss": 17.312751710414886, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.15488076210022, "training_acc": 53.0, "val_loss": 17.342139780521393, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.31859016418457, "training_acc": 53.0, "val_loss": 17.37024635076523, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.13986253738403, "training_acc": 53.0, "val_loss": 17.310218513011932, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.24399185180664, "training_acc": 49.0, "val_loss": 17.309173941612244, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.287513256073, "training_acc": 53.0, "val_loss": 17.396751046180725, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.33363199234009, "training_acc": 53.0, "val_loss": 17.380477488040924, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.56929588317871, "training_acc": 53.0, "val_loss": 17.665952444076538, "val_acc": 52.0}
{"epoch": 34, "training_loss": 70.65111255645752, "training_acc": 53.0, "val_loss": 17.91670173406601, "val_acc": 52.0}
{"epoch": 35, "training_loss": 70.92259693145752, "training_acc": 53.0, "val_loss": 17.617903649806976, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.8024730682373, "training_acc": 53.0, "val_loss": 17.395305633544922, "val_acc": 52.0}
