"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 70.68451166152954, "training_acc": 53.0, "val_loss": 17.34279841184616, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.36755847930908, "training_acc": 53.0, "val_loss": 17.365428805351257, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.31534910202026, "training_acc": 53.0, "val_loss": 17.31005609035492, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.33399724960327, "training_acc": 49.0, "val_loss": 17.314517498016357, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.35184025764465, "training_acc": 53.0, "val_loss": 17.34403222799301, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.10526323318481, "training_acc": 53.0, "val_loss": 17.309600114822388, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.18391180038452, "training_acc": 53.0, "val_loss": 17.31143742799759, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.27815008163452, "training_acc": 53.0, "val_loss": 17.34386533498764, "val_acc": 52.0}
{"epoch": 8, "training_loss": 70.91298460960388, "training_acc": 47.0, "val_loss": 17.44956523180008, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.3592586517334, "training_acc": 45.0, "val_loss": 17.30884313583374, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.1756944656372, "training_acc": 53.0, "val_loss": 17.34154522418976, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.81941986083984, "training_acc": 53.0, "val_loss": 17.4556702375412, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.44605660438538, "training_acc": 53.0, "val_loss": 17.35464483499527, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.33216214179993, "training_acc": 53.0, "val_loss": 17.310382425785065, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.1360182762146, "training_acc": 53.0, "val_loss": 17.323383688926697, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.14362239837646, "training_acc": 53.0, "val_loss": 17.319221794605255, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.21752429008484, "training_acc": 53.0, "val_loss": 17.343924939632416, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.53944826126099, "training_acc": 53.0, "val_loss": 17.549197375774384, "val_acc": 52.0}
{"epoch": 18, "training_loss": 70.35858154296875, "training_acc": 53.0, "val_loss": 17.420662939548492, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.55671191215515, "training_acc": 53.0, "val_loss": 17.35938787460327, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.76500272750854, "training_acc": 47.0, "val_loss": 17.54724234342575, "val_acc": 52.0}
{"epoch": 21, "training_loss": 70.45153141021729, "training_acc": 47.0, "val_loss": 17.377199232578278, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.32256507873535, "training_acc": 49.0, "val_loss": 17.32049584388733, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.28633141517639, "training_acc": 53.0, "val_loss": 17.54944920539856, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.92428064346313, "training_acc": 53.0, "val_loss": 17.69922971725464, "val_acc": 52.0}
{"epoch": 25, "training_loss": 70.69102001190186, "training_acc": 53.0, "val_loss": 17.474283277988434, "val_acc": 52.0}
{"epoch": 26, "training_loss": 68.99858808517456, "training_acc": 53.0, "val_loss": 17.314767837524414, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.31833672523499, "training_acc": 53.0, "val_loss": 17.326682806015015, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.80742764472961, "training_acc": 45.0, "val_loss": 17.38964021205902, "val_acc": 52.0}
