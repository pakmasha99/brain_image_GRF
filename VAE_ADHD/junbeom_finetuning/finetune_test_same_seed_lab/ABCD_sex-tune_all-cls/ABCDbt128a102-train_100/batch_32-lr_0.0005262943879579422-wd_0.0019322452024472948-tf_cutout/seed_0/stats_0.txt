"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.48526453971863, "training_acc": 49.0, "val_loss": 17.16218888759613, "val_acc": 56.0}
{"epoch": 1, "training_loss": 69.45839643478394, "training_acc": 52.0, "val_loss": 17.30639636516571, "val_acc": 56.0}
{"epoch": 2, "training_loss": 70.12254977226257, "training_acc": 44.0, "val_loss": 17.17134118080139, "val_acc": 56.0}
{"epoch": 3, "training_loss": 69.63884210586548, "training_acc": 52.0, "val_loss": 17.19275563955307, "val_acc": 56.0}
{"epoch": 4, "training_loss": 69.34435844421387, "training_acc": 50.0, "val_loss": 17.197667062282562, "val_acc": 56.0}
{"epoch": 5, "training_loss": 69.52533721923828, "training_acc": 52.0, "val_loss": 17.204494774341583, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.36453485488892, "training_acc": 52.0, "val_loss": 17.243902385234833, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.49711537361145, "training_acc": 48.0, "val_loss": 17.223195731639862, "val_acc": 56.0}
{"epoch": 8, "training_loss": 69.99274015426636, "training_acc": 52.0, "val_loss": 17.232760787010193, "val_acc": 56.0}
{"epoch": 9, "training_loss": 69.79106092453003, "training_acc": 38.0, "val_loss": 17.564895749092102, "val_acc": 56.0}
{"epoch": 10, "training_loss": 70.13118767738342, "training_acc": 48.0, "val_loss": 17.671070992946625, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.54053592681885, "training_acc": 52.0, "val_loss": 17.15746968984604, "val_acc": 56.0}
{"epoch": 12, "training_loss": 70.81081867218018, "training_acc": 52.0, "val_loss": 17.234469950199127, "val_acc": 56.0}
{"epoch": 13, "training_loss": 70.48589825630188, "training_acc": 52.0, "val_loss": 17.163489758968353, "val_acc": 56.0}
{"epoch": 14, "training_loss": 69.92867994308472, "training_acc": 52.0, "val_loss": 17.148563265800476, "val_acc": 56.0}
{"epoch": 15, "training_loss": 69.64456796646118, "training_acc": 52.0, "val_loss": 17.175088822841644, "val_acc": 56.0}
{"epoch": 16, "training_loss": 69.82121515274048, "training_acc": 52.0, "val_loss": 17.149099707603455, "val_acc": 56.0}
{"epoch": 17, "training_loss": 69.38889122009277, "training_acc": 52.0, "val_loss": 17.326024174690247, "val_acc": 56.0}
{"epoch": 18, "training_loss": 69.36583995819092, "training_acc": 54.0, "val_loss": 17.824287712574005, "val_acc": 56.0}
{"epoch": 19, "training_loss": 70.77767968177795, "training_acc": 48.0, "val_loss": 17.791633307933807, "val_acc": 56.0}
{"epoch": 20, "training_loss": 69.73246955871582, "training_acc": 48.0, "val_loss": 17.233358323574066, "val_acc": 56.0}
{"epoch": 21, "training_loss": 69.18628358840942, "training_acc": 52.0, "val_loss": 17.15133637189865, "val_acc": 56.0}
{"epoch": 22, "training_loss": 69.66321158409119, "training_acc": 52.0, "val_loss": 17.166849970817566, "val_acc": 56.0}
{"epoch": 23, "training_loss": 69.2427306175232, "training_acc": 48.0, "val_loss": 17.263202369213104, "val_acc": 56.0}
{"epoch": 24, "training_loss": 69.65873098373413, "training_acc": 52.0, "val_loss": 17.171309888362885, "val_acc": 56.0}
{"epoch": 25, "training_loss": 69.28363466262817, "training_acc": 52.0, "val_loss": 17.28196293115616, "val_acc": 56.0}
{"epoch": 26, "training_loss": 69.38664817810059, "training_acc": 52.0, "val_loss": 17.31184422969818, "val_acc": 56.0}
{"epoch": 27, "training_loss": 69.29477262496948, "training_acc": 52.0, "val_loss": 17.339590191841125, "val_acc": 56.0}
{"epoch": 28, "training_loss": 69.37854623794556, "training_acc": 44.0, "val_loss": 17.23911613225937, "val_acc": 56.0}
{"epoch": 29, "training_loss": 69.31051206588745, "training_acc": 52.0, "val_loss": 17.155402898788452, "val_acc": 56.0}
{"epoch": 30, "training_loss": 71.00199770927429, "training_acc": 52.0, "val_loss": 17.402485013008118, "val_acc": 56.0}
{"epoch": 31, "training_loss": 71.80011022090912, "training_acc": 52.0, "val_loss": 17.373353242874146, "val_acc": 56.0}
{"epoch": 32, "training_loss": 71.82556533813477, "training_acc": 52.0, "val_loss": 17.360764741897583, "val_acc": 56.0}
{"epoch": 33, "training_loss": 71.1447684764862, "training_acc": 52.0, "val_loss": 17.149458825588226, "val_acc": 56.0}
