"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 70.1662802696228, "training_acc": 55.0, "val_loss": 18.298818171024323, "val_acc": 52.0}
{"epoch": 1, "training_loss": 70.67434811592102, "training_acc": 59.0, "val_loss": 18.434947729110718, "val_acc": 52.0}
{"epoch": 2, "training_loss": 70.33672189712524, "training_acc": 58.0, "val_loss": 18.57750415802002, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.54624509811401, "training_acc": 59.0, "val_loss": 18.63666921854019, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.54795098304749, "training_acc": 58.0, "val_loss": 18.64951252937317, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.66146230697632, "training_acc": 56.0, "val_loss": 18.575328588485718, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.65990447998047, "training_acc": 57.0, "val_loss": 18.56820583343506, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.54396533966064, "training_acc": 55.0, "val_loss": 18.51586103439331, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.47081100940704, "training_acc": 59.0, "val_loss": 18.48146766424179, "val_acc": 52.0}
{"epoch": 9, "training_loss": 68.75503754615784, "training_acc": 59.0, "val_loss": 18.452072143554688, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.03884840011597, "training_acc": 60.0, "val_loss": 18.488147854804993, "val_acc": 52.0}
{"epoch": 11, "training_loss": 68.84096169471741, "training_acc": 60.0, "val_loss": 18.451638519763947, "val_acc": 52.0}
{"epoch": 12, "training_loss": 68.7825882434845, "training_acc": 58.0, "val_loss": 18.34169328212738, "val_acc": 52.0}
{"epoch": 13, "training_loss": 68.73585319519043, "training_acc": 59.0, "val_loss": 18.243111670017242, "val_acc": 52.0}
{"epoch": 14, "training_loss": 68.85535454750061, "training_acc": 58.0, "val_loss": 18.19254606962204, "val_acc": 52.0}
{"epoch": 15, "training_loss": 68.59815764427185, "training_acc": 61.0, "val_loss": 18.203745782375336, "val_acc": 52.0}
{"epoch": 16, "training_loss": 68.40666055679321, "training_acc": 59.0, "val_loss": 18.245884776115417, "val_acc": 52.0}
{"epoch": 17, "training_loss": 68.3943600654602, "training_acc": 60.0, "val_loss": 18.298742175102234, "val_acc": 52.0}
{"epoch": 18, "training_loss": 67.85384774208069, "training_acc": 63.0, "val_loss": 18.32490861415863, "val_acc": 52.0}
{"epoch": 19, "training_loss": 67.72970390319824, "training_acc": 62.0, "val_loss": 18.34987998008728, "val_acc": 52.0}
{"epoch": 20, "training_loss": 67.54994106292725, "training_acc": 62.0, "val_loss": 18.37776154279709, "val_acc": 52.0}
{"epoch": 21, "training_loss": 67.5115852355957, "training_acc": 63.0, "val_loss": 18.416060507297516, "val_acc": 52.0}
{"epoch": 22, "training_loss": 67.3962287902832, "training_acc": 61.0, "val_loss": 18.493056297302246, "val_acc": 52.0}
{"epoch": 23, "training_loss": 67.4055495262146, "training_acc": 60.0, "val_loss": 18.526138365268707, "val_acc": 52.0}
{"epoch": 24, "training_loss": 67.13942670822144, "training_acc": 61.0, "val_loss": 18.57527494430542, "val_acc": 52.0}
{"epoch": 25, "training_loss": 66.97765707969666, "training_acc": 63.0, "val_loss": 18.597623705863953, "val_acc": 52.0}
{"epoch": 26, "training_loss": 66.84591960906982, "training_acc": 64.0, "val_loss": 18.5928076505661, "val_acc": 52.0}
{"epoch": 27, "training_loss": 66.94630837440491, "training_acc": 62.0, "val_loss": 18.58174055814743, "val_acc": 52.0}
{"epoch": 28, "training_loss": 66.57399439811707, "training_acc": 63.0, "val_loss": 18.590296804904938, "val_acc": 52.0}
{"epoch": 29, "training_loss": 66.28289437294006, "training_acc": 65.0, "val_loss": 18.5811385512352, "val_acc": 52.0}
{"epoch": 30, "training_loss": 66.44855046272278, "training_acc": 66.0, "val_loss": 18.576234579086304, "val_acc": 52.0}
{"epoch": 31, "training_loss": 66.11786603927612, "training_acc": 66.0, "val_loss": 18.575194478034973, "val_acc": 52.0}
{"epoch": 32, "training_loss": 66.43081474304199, "training_acc": 63.0, "val_loss": 18.583112955093384, "val_acc": 52.0}
{"epoch": 33, "training_loss": 65.9068295955658, "training_acc": 64.0, "val_loss": 18.60050857067108, "val_acc": 52.0}
