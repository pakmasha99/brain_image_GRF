"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 71.4410011768341, "training_acc": 50.0, "val_loss": 18.684488534927368, "val_acc": 52.0}
{"epoch": 1, "training_loss": 75.11229491233826, "training_acc": 49.0, "val_loss": 18.517817556858063, "val_acc": 52.0}
{"epoch": 2, "training_loss": 73.54704737663269, "training_acc": 49.0, "val_loss": 18.26011687517166, "val_acc": 52.0}
{"epoch": 3, "training_loss": 72.15066170692444, "training_acc": 55.0, "val_loss": 18.04520934820175, "val_acc": 52.0}
{"epoch": 4, "training_loss": 72.0999710559845, "training_acc": 55.0, "val_loss": 17.957839369773865, "val_acc": 52.0}
{"epoch": 5, "training_loss": 71.30523109436035, "training_acc": 51.0, "val_loss": 17.963610589504242, "val_acc": 52.0}
{"epoch": 6, "training_loss": 71.06437230110168, "training_acc": 53.0, "val_loss": 17.971038818359375, "val_acc": 52.0}
{"epoch": 7, "training_loss": 71.140296459198, "training_acc": 51.0, "val_loss": 17.947347462177277, "val_acc": 52.0}
{"epoch": 8, "training_loss": 70.9387001991272, "training_acc": 51.0, "val_loss": 17.945273220539093, "val_acc": 52.0}
{"epoch": 9, "training_loss": 71.36518883705139, "training_acc": 52.0, "val_loss": 17.955155670642853, "val_acc": 52.0}
{"epoch": 10, "training_loss": 70.11568236351013, "training_acc": 53.0, "val_loss": 18.066848814487457, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.28804230690002, "training_acc": 54.0, "val_loss": 18.141500651836395, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.42316484451294, "training_acc": 54.0, "val_loss": 18.137842416763306, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.05128717422485, "training_acc": 52.0, "val_loss": 18.089526891708374, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.50336217880249, "training_acc": 51.0, "val_loss": 18.044376373291016, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.551842212677, "training_acc": 50.0, "val_loss": 17.994782328605652, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.11846113204956, "training_acc": 55.0, "val_loss": 17.958906292915344, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.20835542678833, "training_acc": 55.0, "val_loss": 17.89787858724594, "val_acc": 52.0}
{"epoch": 18, "training_loss": 68.62257385253906, "training_acc": 54.0, "val_loss": 17.883658409118652, "val_acc": 52.0}
{"epoch": 19, "training_loss": 68.66883635520935, "training_acc": 54.0, "val_loss": 17.87247210741043, "val_acc": 52.0}
{"epoch": 20, "training_loss": 68.15503644943237, "training_acc": 55.0, "val_loss": 17.88828670978546, "val_acc": 52.0}
{"epoch": 21, "training_loss": 68.28996133804321, "training_acc": 53.0, "val_loss": 17.89841800928116, "val_acc": 52.0}
{"epoch": 22, "training_loss": 68.04687738418579, "training_acc": 52.0, "val_loss": 17.873375117778778, "val_acc": 52.0}
{"epoch": 23, "training_loss": 68.24915957450867, "training_acc": 53.0, "val_loss": 17.791444063186646, "val_acc": 52.0}
{"epoch": 24, "training_loss": 67.96312642097473, "training_acc": 55.0, "val_loss": 17.731299996376038, "val_acc": 52.0}
{"epoch": 25, "training_loss": 67.08238697052002, "training_acc": 57.0, "val_loss": 17.715880274772644, "val_acc": 52.0}
{"epoch": 26, "training_loss": 68.00062608718872, "training_acc": 56.0, "val_loss": 17.70901083946228, "val_acc": 52.0}
{"epoch": 27, "training_loss": 67.8761248588562, "training_acc": 56.0, "val_loss": 17.70680695772171, "val_acc": 52.0}
{"epoch": 28, "training_loss": 67.97560548782349, "training_acc": 53.0, "val_loss": 17.767591774463654, "val_acc": 52.0}
{"epoch": 29, "training_loss": 67.34996938705444, "training_acc": 58.0, "val_loss": 17.81650334596634, "val_acc": 52.0}
{"epoch": 30, "training_loss": 67.45133304595947, "training_acc": 56.0, "val_loss": 17.861896753311157, "val_acc": 52.0}
{"epoch": 31, "training_loss": 66.87996745109558, "training_acc": 62.0, "val_loss": 17.88974404335022, "val_acc": 52.0}
{"epoch": 32, "training_loss": 66.72663712501526, "training_acc": 61.0, "val_loss": 17.909780144691467, "val_acc": 52.0}
{"epoch": 33, "training_loss": 66.6271162033081, "training_acc": 61.0, "val_loss": 17.94564723968506, "val_acc": 52.0}
{"epoch": 34, "training_loss": 66.51990723609924, "training_acc": 59.0, "val_loss": 17.9062157869339, "val_acc": 52.0}
{"epoch": 35, "training_loss": 66.69924306869507, "training_acc": 59.0, "val_loss": 17.80855506658554, "val_acc": 52.0}
{"epoch": 36, "training_loss": 66.71467208862305, "training_acc": 60.0, "val_loss": 17.698542773723602, "val_acc": 52.0}
{"epoch": 37, "training_loss": 66.32239627838135, "training_acc": 59.0, "val_loss": 17.579834163188934, "val_acc": 52.0}
{"epoch": 38, "training_loss": 66.42110443115234, "training_acc": 62.0, "val_loss": 17.541927099227905, "val_acc": 52.0}
{"epoch": 39, "training_loss": 66.10739135742188, "training_acc": 62.0, "val_loss": 17.620818316936493, "val_acc": 52.0}
{"epoch": 40, "training_loss": 65.87379145622253, "training_acc": 63.0, "val_loss": 17.70736277103424, "val_acc": 52.0}
{"epoch": 41, "training_loss": 65.86745309829712, "training_acc": 63.0, "val_loss": 17.75730401277542, "val_acc": 52.0}
{"epoch": 42, "training_loss": 65.60637950897217, "training_acc": 63.0, "val_loss": 17.79959201812744, "val_acc": 52.0}
{"epoch": 43, "training_loss": 66.01319742202759, "training_acc": 65.0, "val_loss": 17.82698780298233, "val_acc": 52.0}
{"epoch": 44, "training_loss": 65.32149600982666, "training_acc": 64.0, "val_loss": 17.85290241241455, "val_acc": 52.0}
{"epoch": 45, "training_loss": 65.4237871170044, "training_acc": 60.0, "val_loss": 17.935119569301605, "val_acc": 52.0}
{"epoch": 46, "training_loss": 65.73769903182983, "training_acc": 61.0, "val_loss": 17.926666140556335, "val_acc": 52.0}
{"epoch": 47, "training_loss": 65.54749274253845, "training_acc": 62.0, "val_loss": 17.854836583137512, "val_acc": 52.0}
{"epoch": 48, "training_loss": 65.40561938285828, "training_acc": 62.0, "val_loss": 17.83869117498398, "val_acc": 52.0}
{"epoch": 49, "training_loss": 65.22732019424438, "training_acc": 61.0, "val_loss": 17.829564213752747, "val_acc": 52.0}
{"epoch": 50, "training_loss": 65.05072021484375, "training_acc": 62.0, "val_loss": 17.82740354537964, "val_acc": 52.0}
{"epoch": 51, "training_loss": 65.09996843338013, "training_acc": 64.0, "val_loss": 17.77728646993637, "val_acc": 52.0}
{"epoch": 52, "training_loss": 64.65173935890198, "training_acc": 62.0, "val_loss": 17.7159383893013, "val_acc": 52.0}
{"epoch": 53, "training_loss": 65.1667902469635, "training_acc": 63.0, "val_loss": 17.700469493865967, "val_acc": 52.0}
{"epoch": 54, "training_loss": 64.85959339141846, "training_acc": 65.0, "val_loss": 17.671668529510498, "val_acc": 52.0}
{"epoch": 55, "training_loss": 64.6402575969696, "training_acc": 64.0, "val_loss": 17.676736414432526, "val_acc": 52.0}
{"epoch": 56, "training_loss": 64.2679283618927, "training_acc": 64.0, "val_loss": 17.70869940519333, "val_acc": 52.0}
{"epoch": 57, "training_loss": 64.53615355491638, "training_acc": 62.0, "val_loss": 17.740802466869354, "val_acc": 52.0}
