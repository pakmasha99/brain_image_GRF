"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 71.7055344581604, "training_acc": 48.0, "val_loss": 18.768249452114105, "val_acc": 52.0}
{"epoch": 1, "training_loss": 74.55556750297546, "training_acc": 43.0, "val_loss": 18.652699887752533, "val_acc": 48.0}
{"epoch": 2, "training_loss": 73.69384908676147, "training_acc": 47.0, "val_loss": 18.703410029411316, "val_acc": 52.0}
{"epoch": 3, "training_loss": 73.03317356109619, "training_acc": 45.0, "val_loss": 18.63265037536621, "val_acc": 52.0}
{"epoch": 4, "training_loss": 72.94834184646606, "training_acc": 44.0, "val_loss": 18.53160858154297, "val_acc": 56.0}
{"epoch": 5, "training_loss": 72.17091751098633, "training_acc": 50.0, "val_loss": 18.465058505535126, "val_acc": 56.0}
{"epoch": 6, "training_loss": 71.34464025497437, "training_acc": 50.0, "val_loss": 18.4413880109787, "val_acc": 56.0}
{"epoch": 7, "training_loss": 70.30547904968262, "training_acc": 52.0, "val_loss": 18.391749262809753, "val_acc": 56.0}
{"epoch": 8, "training_loss": 69.86911082267761, "training_acc": 51.0, "val_loss": 18.34869533777237, "val_acc": 56.0}
{"epoch": 9, "training_loss": 70.24345922470093, "training_acc": 50.0, "val_loss": 18.270251154899597, "val_acc": 56.0}
{"epoch": 10, "training_loss": 69.57327032089233, "training_acc": 50.0, "val_loss": 18.15674602985382, "val_acc": 56.0}
{"epoch": 11, "training_loss": 68.83499145507812, "training_acc": 52.0, "val_loss": 18.019935488700867, "val_acc": 56.0}
{"epoch": 12, "training_loss": 68.31682467460632, "training_acc": 53.0, "val_loss": 17.93869286775589, "val_acc": 56.0}
{"epoch": 13, "training_loss": 68.53536891937256, "training_acc": 53.0, "val_loss": 17.93266236782074, "val_acc": 56.0}
{"epoch": 14, "training_loss": 68.68476939201355, "training_acc": 53.0, "val_loss": 17.941230535507202, "val_acc": 56.0}
{"epoch": 15, "training_loss": 68.446457862854, "training_acc": 53.0, "val_loss": 17.928245663642883, "val_acc": 56.0}
{"epoch": 16, "training_loss": 67.75168585777283, "training_acc": 52.0, "val_loss": 17.83747375011444, "val_acc": 56.0}
{"epoch": 17, "training_loss": 68.20933151245117, "training_acc": 56.0, "val_loss": 17.858459055423737, "val_acc": 56.0}
{"epoch": 18, "training_loss": 67.70718193054199, "training_acc": 55.0, "val_loss": 17.776410281658173, "val_acc": 56.0}
{"epoch": 19, "training_loss": 67.41391229629517, "training_acc": 57.0, "val_loss": 17.778457701206207, "val_acc": 56.0}
{"epoch": 20, "training_loss": 67.29276704788208, "training_acc": 57.0, "val_loss": 17.787963151931763, "val_acc": 56.0}
{"epoch": 21, "training_loss": 67.15051078796387, "training_acc": 60.0, "val_loss": 17.72020310163498, "val_acc": 56.0}
{"epoch": 22, "training_loss": 67.38546943664551, "training_acc": 57.0, "val_loss": 17.714861035346985, "val_acc": 56.0}
{"epoch": 23, "training_loss": 66.6759102344513, "training_acc": 56.0, "val_loss": 17.73560792207718, "val_acc": 56.0}
{"epoch": 24, "training_loss": 67.25966167449951, "training_acc": 59.0, "val_loss": 17.630797624588013, "val_acc": 56.0}
{"epoch": 25, "training_loss": 66.77911853790283, "training_acc": 61.0, "val_loss": 17.666953802108765, "val_acc": 56.0}
{"epoch": 26, "training_loss": 66.35055184364319, "training_acc": 59.0, "val_loss": 17.70040988922119, "val_acc": 56.0}
{"epoch": 27, "training_loss": 66.43623971939087, "training_acc": 60.0, "val_loss": 17.62697398662567, "val_acc": 56.0}
{"epoch": 28, "training_loss": 65.91351890563965, "training_acc": 61.0, "val_loss": 17.435531318187714, "val_acc": 56.0}
{"epoch": 29, "training_loss": 65.66700530052185, "training_acc": 64.0, "val_loss": 17.43999570608139, "val_acc": 56.0}
{"epoch": 30, "training_loss": 66.2810926437378, "training_acc": 62.0, "val_loss": 17.368339002132416, "val_acc": 56.0}
{"epoch": 31, "training_loss": 65.48970699310303, "training_acc": 61.0, "val_loss": 17.409373819828033, "val_acc": 56.0}
{"epoch": 32, "training_loss": 65.85484743118286, "training_acc": 62.0, "val_loss": 17.500591278076172, "val_acc": 56.0}
{"epoch": 33, "training_loss": 65.92272758483887, "training_acc": 60.0, "val_loss": 17.56674498319626, "val_acc": 56.0}
{"epoch": 34, "training_loss": 65.6128499507904, "training_acc": 60.0, "val_loss": 17.719461023807526, "val_acc": 56.0}
{"epoch": 35, "training_loss": 65.07070446014404, "training_acc": 64.0, "val_loss": 17.97952651977539, "val_acc": 56.0}
{"epoch": 36, "training_loss": 64.88713002204895, "training_acc": 62.0, "val_loss": 18.09312403202057, "val_acc": 56.0}
{"epoch": 37, "training_loss": 65.62720370292664, "training_acc": 58.0, "val_loss": 18.190233409404755, "val_acc": 56.0}
{"epoch": 38, "training_loss": 65.18084621429443, "training_acc": 59.0, "val_loss": 18.23507249355316, "val_acc": 56.0}
{"epoch": 39, "training_loss": 64.8863034248352, "training_acc": 58.0, "val_loss": 18.25166642665863, "val_acc": 56.0}
{"epoch": 40, "training_loss": 64.87224006652832, "training_acc": 64.0, "val_loss": 18.239395320415497, "val_acc": 56.0}
{"epoch": 41, "training_loss": 64.49579334259033, "training_acc": 62.0, "val_loss": 18.24599653482437, "val_acc": 56.0}
{"epoch": 42, "training_loss": 64.81122922897339, "training_acc": 62.0, "val_loss": 18.28951984643936, "val_acc": 56.0}
{"epoch": 43, "training_loss": 64.81609988212585, "training_acc": 61.0, "val_loss": 18.28937530517578, "val_acc": 56.0}
{"epoch": 44, "training_loss": 64.64854836463928, "training_acc": 59.0, "val_loss": 18.201102316379547, "val_acc": 56.0}
{"epoch": 45, "training_loss": 64.35595560073853, "training_acc": 63.0, "val_loss": 18.07647794485092, "val_acc": 56.0}
{"epoch": 46, "training_loss": 63.77505898475647, "training_acc": 64.0, "val_loss": 18.051905930042267, "val_acc": 56.0}
{"epoch": 47, "training_loss": 64.07237434387207, "training_acc": 62.0, "val_loss": 18.04993450641632, "val_acc": 56.0}
{"epoch": 48, "training_loss": 64.12546586990356, "training_acc": 62.0, "val_loss": 18.17961484193802, "val_acc": 52.0}
{"epoch": 49, "training_loss": 63.57026815414429, "training_acc": 61.0, "val_loss": 18.34794133901596, "val_acc": 52.0}
