"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 69.00017404556274, "training_acc": 53.0, "val_loss": 18.661338090896606, "val_acc": 48.0}
{"epoch": 1, "training_loss": 73.89548754692078, "training_acc": 46.0, "val_loss": 19.575797021389008, "val_acc": 52.0}
{"epoch": 2, "training_loss": 75.52253675460815, "training_acc": 54.0, "val_loss": 17.297248542308807, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.7711410522461, "training_acc": 40.0, "val_loss": 17.50316321849823, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.47315883636475, "training_acc": 48.0, "val_loss": 17.422881722450256, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.1708517074585, "training_acc": 51.0, "val_loss": 17.48044341802597, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.3456916809082, "training_acc": 53.0, "val_loss": 17.43280589580536, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.47244000434875, "training_acc": 53.0, "val_loss": 17.720893025398254, "val_acc": 52.0}
{"epoch": 8, "training_loss": 70.06163311004639, "training_acc": 53.0, "val_loss": 17.645904421806335, "val_acc": 52.0}
{"epoch": 9, "training_loss": 70.02695655822754, "training_acc": 53.0, "val_loss": 17.6301509141922, "val_acc": 52.0}
{"epoch": 10, "training_loss": 70.04146671295166, "training_acc": 53.0, "val_loss": 17.917953431606293, "val_acc": 52.0}
{"epoch": 11, "training_loss": 70.606853723526, "training_acc": 53.0, "val_loss": 17.550474405288696, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.60143184661865, "training_acc": 53.0, "val_loss": 17.33095645904541, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.16969513893127, "training_acc": 49.0, "val_loss": 17.429006099700928, "val_acc": 52.0}
{"epoch": 14, "training_loss": 70.05447626113892, "training_acc": 48.0, "val_loss": 17.55874454975128, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.82012414932251, "training_acc": 47.0, "val_loss": 17.423400282859802, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.32254910469055, "training_acc": 53.0, "val_loss": 17.37431436777115, "val_acc": 52.0}
{"epoch": 17, "training_loss": 70.61421585083008, "training_acc": 46.0, "val_loss": 17.34950542449951, "val_acc": 52.0}
{"epoch": 18, "training_loss": 70.63302111625671, "training_acc": 53.0, "val_loss": 17.42722988128662, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.69757747650146, "training_acc": 53.0, "val_loss": 17.258991301059723, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.24170351028442, "training_acc": 53.0, "val_loss": 17.35503375530243, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.3299560546875, "training_acc": 51.0, "val_loss": 17.437419295310974, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.76917171478271, "training_acc": 47.0, "val_loss": 17.43079274892807, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.74136734008789, "training_acc": 47.0, "val_loss": 17.3992782831192, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.32562446594238, "training_acc": 50.0, "val_loss": 17.383937537670135, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.39965891838074, "training_acc": 53.0, "val_loss": 17.563121020793915, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.45479726791382, "training_acc": 53.0, "val_loss": 17.438916862010956, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.02137231826782, "training_acc": 53.0, "val_loss": 17.3679381608963, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.12655019760132, "training_acc": 53.0, "val_loss": 17.388439178466797, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.02631831169128, "training_acc": 53.0, "val_loss": 17.551247775554657, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.79245162010193, "training_acc": 53.0, "val_loss": 17.575041949748993, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.36431694030762, "training_acc": 53.0, "val_loss": 17.380687594413757, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.1813542842865, "training_acc": 53.0, "val_loss": 17.359210550785065, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.0251612663269, "training_acc": 53.0, "val_loss": 17.316485941410065, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.1573793888092, "training_acc": 53.0, "val_loss": 17.33400672674179, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.32012271881104, "training_acc": 45.0, "val_loss": 17.37259477376938, "val_acc": 52.0}
{"epoch": 36, "training_loss": 70.36291694641113, "training_acc": 49.0, "val_loss": 17.45772957801819, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.40808391571045, "training_acc": 46.0, "val_loss": 17.342716455459595, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.14801454544067, "training_acc": 52.0, "val_loss": 17.34648048877716, "val_acc": 52.0}
