"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 71.00252532958984, "training_acc": 52.0, "val_loss": 17.672982811927795, "val_acc": 64.0}
{"epoch": 1, "training_loss": 72.76440811157227, "training_acc": 42.0, "val_loss": 17.064012587070465, "val_acc": 56.0}
{"epoch": 2, "training_loss": 71.9470853805542, "training_acc": 41.0, "val_loss": 19.938924908638, "val_acc": 56.0}
{"epoch": 3, "training_loss": 72.781014919281, "training_acc": 54.0, "val_loss": 23.776452243328094, "val_acc": 44.0}
{"epoch": 4, "training_loss": 85.0037944316864, "training_acc": 48.0, "val_loss": 17.139635980129242, "val_acc": 56.0}
{"epoch": 5, "training_loss": 71.20553350448608, "training_acc": 52.0, "val_loss": 17.747384309768677, "val_acc": 56.0}
{"epoch": 6, "training_loss": 70.50537490844727, "training_acc": 52.0, "val_loss": 17.19573587179184, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.60051345825195, "training_acc": 53.0, "val_loss": 17.01766699552536, "val_acc": 56.0}
{"epoch": 8, "training_loss": 70.27321147918701, "training_acc": 51.0, "val_loss": 17.152392864227295, "val_acc": 56.0}
{"epoch": 9, "training_loss": 69.78457450866699, "training_acc": 42.0, "val_loss": 17.776404321193695, "val_acc": 56.0}
{"epoch": 10, "training_loss": 70.76969909667969, "training_acc": 48.0, "val_loss": 18.16416084766388, "val_acc": 56.0}
{"epoch": 11, "training_loss": 70.22936725616455, "training_acc": 48.0, "val_loss": 17.30792373418808, "val_acc": 56.0}
{"epoch": 12, "training_loss": 70.23821830749512, "training_acc": 52.0, "val_loss": 17.259925603866577, "val_acc": 56.0}
{"epoch": 13, "training_loss": 70.42534685134888, "training_acc": 52.0, "val_loss": 17.241397500038147, "val_acc": 56.0}
{"epoch": 14, "training_loss": 70.07162046432495, "training_acc": 52.0, "val_loss": 17.250046133995056, "val_acc": 56.0}
{"epoch": 15, "training_loss": 69.63427305221558, "training_acc": 52.0, "val_loss": 17.319507896900177, "val_acc": 56.0}
{"epoch": 16, "training_loss": 69.44671630859375, "training_acc": 52.0, "val_loss": 17.301422357559204, "val_acc": 56.0}
{"epoch": 17, "training_loss": 69.18687701225281, "training_acc": 52.0, "val_loss": 17.41006076335907, "val_acc": 56.0}
{"epoch": 18, "training_loss": 69.02417826652527, "training_acc": 53.0, "val_loss": 17.71594136953354, "val_acc": 56.0}
{"epoch": 19, "training_loss": 70.03554391860962, "training_acc": 48.0, "val_loss": 17.833369970321655, "val_acc": 56.0}
{"epoch": 20, "training_loss": 69.88505935668945, "training_acc": 48.0, "val_loss": 17.461390793323517, "val_acc": 56.0}
{"epoch": 21, "training_loss": 69.18107271194458, "training_acc": 55.0, "val_loss": 17.305101454257965, "val_acc": 56.0}
{"epoch": 22, "training_loss": 69.38001751899719, "training_acc": 52.0, "val_loss": 17.281967401504517, "val_acc": 56.0}
{"epoch": 23, "training_loss": 69.10919642448425, "training_acc": 52.0, "val_loss": 17.371074855327606, "val_acc": 56.0}
{"epoch": 24, "training_loss": 69.3955910205841, "training_acc": 52.0, "val_loss": 17.382970452308655, "val_acc": 56.0}
{"epoch": 25, "training_loss": 69.14313769340515, "training_acc": 52.0, "val_loss": 17.404600977897644, "val_acc": 56.0}
{"epoch": 26, "training_loss": 68.99699211120605, "training_acc": 53.0, "val_loss": 17.479777336120605, "val_acc": 56.0}
