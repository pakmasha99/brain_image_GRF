"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 70.38870739936829, "training_acc": 51.0, "val_loss": 17.954818904399872, "val_acc": 56.0}
{"epoch": 1, "training_loss": 69.06097507476807, "training_acc": 54.0, "val_loss": 18.758030235767365, "val_acc": 56.0}
{"epoch": 2, "training_loss": 71.48000359535217, "training_acc": 51.0, "val_loss": 17.639102041721344, "val_acc": 52.0}
{"epoch": 3, "training_loss": 74.22354412078857, "training_acc": 52.0, "val_loss": 17.205531895160675, "val_acc": 52.0}
{"epoch": 4, "training_loss": 70.12148857116699, "training_acc": 50.0, "val_loss": 17.15659648180008, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.88469314575195, "training_acc": 50.0, "val_loss": 17.55855679512024, "val_acc": 52.0}
{"epoch": 6, "training_loss": 70.40466809272766, "training_acc": 54.0, "val_loss": 17.414267361164093, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.49298477172852, "training_acc": 54.0, "val_loss": 17.347678542137146, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.41193675994873, "training_acc": 52.0, "val_loss": 17.55536049604416, "val_acc": 52.0}
{"epoch": 9, "training_loss": 71.12273240089417, "training_acc": 53.0, "val_loss": 17.83546358346939, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.80515122413635, "training_acc": 53.0, "val_loss": 17.309269309043884, "val_acc": 52.0}
{"epoch": 11, "training_loss": 70.19579076766968, "training_acc": 46.0, "val_loss": 17.46309995651245, "val_acc": 52.0}
{"epoch": 12, "training_loss": 70.02082133293152, "training_acc": 48.0, "val_loss": 17.46375262737274, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.46181106567383, "training_acc": 53.0, "val_loss": 17.42488294839859, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.43960976600647, "training_acc": 53.0, "val_loss": 17.482204735279083, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.34401392936707, "training_acc": 53.0, "val_loss": 17.48422533273697, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.21392011642456, "training_acc": 53.0, "val_loss": 17.477507889270782, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.09742665290833, "training_acc": 53.0, "val_loss": 17.459718883037567, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.57084965705872, "training_acc": 46.0, "val_loss": 17.38697737455368, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.20818519592285, "training_acc": 55.0, "val_loss": 17.339226603507996, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.28183197975159, "training_acc": 53.0, "val_loss": 17.32754409313202, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.36979126930237, "training_acc": 53.0, "val_loss": 17.320193350315094, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.14685344696045, "training_acc": 53.0, "val_loss": 17.35222637653351, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.12877345085144, "training_acc": 53.0, "val_loss": 17.363500595092773, "val_acc": 52.0}
