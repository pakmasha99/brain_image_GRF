"main_optuna.py --pretrained_path /scratch/connectome/dyhan316/VAE_ADHD/barlowtwins/pretrain_results/ABCDbt128a102.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option BT_org --batch_size 32 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab --eval_mode False"
{"epoch": 0, "training_loss": 71.07887601852417, "training_acc": 49.0, "val_loss": 18.073292076587677, "val_acc": 52.0}
{"epoch": 1, "training_loss": 70.97491407394409, "training_acc": 45.0, "val_loss": 17.736464738845825, "val_acc": 52.0}
{"epoch": 2, "training_loss": 70.73387360572815, "training_acc": 47.0, "val_loss": 17.61845499277115, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.9084939956665, "training_acc": 53.0, "val_loss": 17.8601011633873, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.24181985855103, "training_acc": 52.0, "val_loss": 17.456425726413727, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.97895431518555, "training_acc": 48.0, "val_loss": 17.468760907649994, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.75088000297546, "training_acc": 47.0, "val_loss": 17.437930405139923, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.60997200012207, "training_acc": 49.0, "val_loss": 17.305032908916473, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.25291180610657, "training_acc": 55.0, "val_loss": 17.263153195381165, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.5055103302002, "training_acc": 49.0, "val_loss": 17.286647856235504, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.09523463249207, "training_acc": 55.0, "val_loss": 17.33516901731491, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.19145584106445, "training_acc": 53.0, "val_loss": 17.31589138507843, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.2198555469513, "training_acc": 53.0, "val_loss": 17.47061461210251, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.41200542449951, "training_acc": 53.0, "val_loss": 17.5836443901062, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.85895252227783, "training_acc": 53.0, "val_loss": 17.653335630893707, "val_acc": 52.0}
{"epoch": 15, "training_loss": 70.138258934021, "training_acc": 53.0, "val_loss": 17.775659263134003, "val_acc": 52.0}
{"epoch": 16, "training_loss": 70.31299638748169, "training_acc": 53.0, "val_loss": 17.533138394355774, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.54894590377808, "training_acc": 53.0, "val_loss": 17.335213720798492, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.14268493652344, "training_acc": 53.0, "val_loss": 17.335818707942963, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.5575475692749, "training_acc": 46.0, "val_loss": 17.41166263818741, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.75055003166199, "training_acc": 47.0, "val_loss": 17.33575165271759, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.4003233909607, "training_acc": 54.0, "val_loss": 17.330898344516754, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.31366753578186, "training_acc": 49.0, "val_loss": 17.33248680830002, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.42665815353394, "training_acc": 53.0, "val_loss": 17.357870936393738, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.39187335968018, "training_acc": 53.0, "val_loss": 17.292115092277527, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.04340648651123, "training_acc": 53.0, "val_loss": 17.287930846214294, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.31863832473755, "training_acc": 48.0, "val_loss": 17.351296544075012, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.47219109535217, "training_acc": 47.0, "val_loss": 17.380118370056152, "val_acc": 52.0}
