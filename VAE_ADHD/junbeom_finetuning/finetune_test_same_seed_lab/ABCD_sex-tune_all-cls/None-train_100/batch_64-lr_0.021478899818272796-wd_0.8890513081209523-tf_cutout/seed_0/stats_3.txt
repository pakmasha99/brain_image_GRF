"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 1885.2618713378906, "training_acc": 53.0, "val_loss": 3.162405723082588e+19, "val_acc": 48.0}
{"epoch": 1, "training_loss": 9.601656813370268e+19, "training_acc": 37.0, "val_loss": 234068275.0, "val_acc": 52.0}
{"epoch": 2, "training_loss": 678350049.5, "training_acc": 53.0, "val_loss": 152250650.0, "val_acc": 48.0}
{"epoch": 3, "training_loss": 484886144.5, "training_acc": 45.0, "val_loss": 181930775.0, "val_acc": 52.0}
{"epoch": 4, "training_loss": 434902098.5, "training_acc": 53.0, "val_loss": 11386979.6875, "val_acc": 48.0}
{"epoch": 5, "training_loss": 40035677.25, "training_acc": 51.0, "val_loss": 1608836.42578125, "val_acc": 48.0}
{"epoch": 6, "training_loss": 5887536.28125, "training_acc": 45.0, "val_loss": 1656759.1796875, "val_acc": 48.0}
{"epoch": 7, "training_loss": 5590945.40625, "training_acc": 49.0, "val_loss": 262870.849609375, "val_acc": 48.0}
{"epoch": 8, "training_loss": 1292782.7578125, "training_acc": 41.0, "val_loss": 40193.389892578125, "val_acc": 52.0}
{"epoch": 9, "training_loss": 350076.685546875, "training_acc": 53.0, "val_loss": 25509.983825683594, "val_acc": 48.0}
{"epoch": 10, "training_loss": 157949.7626953125, "training_acc": 45.0, "val_loss": 18183.441162109375, "val_acc": 52.0}
{"epoch": 11, "training_loss": 48390.40640258789, "training_acc": 53.0, "val_loss": 2735.673713684082, "val_acc": 48.0}
{"epoch": 12, "training_loss": 10138.962982177734, "training_acc": 47.0, "val_loss": 862.2157096862793, "val_acc": 48.0}
{"epoch": 13, "training_loss": 2275.9023513793945, "training_acc": 57.0, "val_loss": 792.7490234375, "val_acc": 48.0}
{"epoch": 14, "training_loss": 3593.405227661133, "training_acc": 51.0, "val_loss": 41.04366600513458, "val_acc": 52.0}
{"epoch": 15, "training_loss": 432.68069076538086, "training_acc": 53.0, "val_loss": 118.87890100479126, "val_acc": 48.0}
{"epoch": 16, "training_loss": 390.2450656890869, "training_acc": 47.0, "val_loss": 32.84943997859955, "val_acc": 48.0}
{"epoch": 17, "training_loss": 107.7227611541748, "training_acc": 51.0, "val_loss": 22.09840714931488, "val_acc": 52.0}
{"epoch": 18, "training_loss": 80.86943697929382, "training_acc": 53.0, "val_loss": 17.446666955947876, "val_acc": 52.0}
{"epoch": 19, "training_loss": 71.46750235557556, "training_acc": 47.0, "val_loss": 18.317869305610657, "val_acc": 52.0}
{"epoch": 20, "training_loss": 73.598557472229, "training_acc": 47.0, "val_loss": 17.39286780357361, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.37728333473206, "training_acc": 51.0, "val_loss": 17.3499658703804, "val_acc": 52.0}
{"epoch": 22, "training_loss": 71.53064751625061, "training_acc": 53.0, "val_loss": 17.391231656074524, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.27074241638184, "training_acc": 53.0, "val_loss": 17.316535115242004, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.39242148399353, "training_acc": 49.0, "val_loss": 17.31792390346527, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.23887825012207, "training_acc": 53.0, "val_loss": 17.339256405830383, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.95584845542908, "training_acc": 43.0, "val_loss": 17.31235235929489, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.25600123405457, "training_acc": 53.0, "val_loss": 17.32385754585266, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.27268052101135, "training_acc": 53.0, "val_loss": 17.32184588909149, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.20606517791748, "training_acc": 53.0, "val_loss": 17.30944663286209, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.19891023635864, "training_acc": 53.0, "val_loss": 17.309297621250153, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.27044749259949, "training_acc": 53.0, "val_loss": 17.309726774692535, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.14298462867737, "training_acc": 53.0, "val_loss": 17.312656342983246, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.10832357406616, "training_acc": 53.0, "val_loss": 17.335650324821472, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.2481300830841, "training_acc": 53.0, "val_loss": 17.34669804573059, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.27448749542236, "training_acc": 53.0, "val_loss": 17.32485145330429, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.19928812980652, "training_acc": 53.0, "val_loss": 17.31676757335663, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.13427376747131, "training_acc": 53.0, "val_loss": 17.314405739307404, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.1285080909729, "training_acc": 53.0, "val_loss": 17.31143742799759, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.12875199317932, "training_acc": 53.0, "val_loss": 17.309197783470154, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.1543960571289, "training_acc": 53.0, "val_loss": 17.308683693408966, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.16289973258972, "training_acc": 53.0, "val_loss": 17.308959364891052, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.18094182014465, "training_acc": 53.0, "val_loss": 17.309299111366272, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.18290424346924, "training_acc": 53.0, "val_loss": 17.3093244433403, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.17089819908142, "training_acc": 53.0, "val_loss": 17.30904132127762, "val_acc": 52.0}
{"epoch": 45, "training_loss": 69.18773221969604, "training_acc": 53.0, "val_loss": 17.30889081954956, "val_acc": 52.0}
{"epoch": 46, "training_loss": 69.1754059791565, "training_acc": 53.0, "val_loss": 17.309117317199707, "val_acc": 52.0}
{"epoch": 47, "training_loss": 69.17407155036926, "training_acc": 53.0, "val_loss": 17.30915606021881, "val_acc": 52.0}
{"epoch": 48, "training_loss": 69.17233395576477, "training_acc": 53.0, "val_loss": 17.309020459651947, "val_acc": 52.0}
{"epoch": 49, "training_loss": 69.16684150695801, "training_acc": 53.0, "val_loss": 17.30903536081314, "val_acc": 52.0}
{"epoch": 50, "training_loss": 69.16948747634888, "training_acc": 53.0, "val_loss": 17.309103906154633, "val_acc": 52.0}
{"epoch": 51, "training_loss": 69.17435884475708, "training_acc": 53.0, "val_loss": 17.309372127056122, "val_acc": 52.0}
{"epoch": 52, "training_loss": 69.17855525016785, "training_acc": 53.0, "val_loss": 17.309457063674927, "val_acc": 52.0}
{"epoch": 53, "training_loss": 69.17445397377014, "training_acc": 53.0, "val_loss": 17.309284210205078, "val_acc": 52.0}
{"epoch": 54, "training_loss": 69.17232060432434, "training_acc": 53.0, "val_loss": 17.309202253818512, "val_acc": 52.0}
{"epoch": 55, "training_loss": 69.16849517822266, "training_acc": 53.0, "val_loss": 17.30899065732956, "val_acc": 52.0}
{"epoch": 56, "training_loss": 69.16411352157593, "training_acc": 53.0, "val_loss": 17.3087939620018, "val_acc": 52.0}
{"epoch": 57, "training_loss": 69.18026041984558, "training_acc": 53.0, "val_loss": 17.308686673641205, "val_acc": 52.0}
{"epoch": 58, "training_loss": 69.15673494338989, "training_acc": 53.0, "val_loss": 17.3087015748024, "val_acc": 52.0}
{"epoch": 59, "training_loss": 69.18778848648071, "training_acc": 53.0, "val_loss": 17.30877012014389, "val_acc": 52.0}
