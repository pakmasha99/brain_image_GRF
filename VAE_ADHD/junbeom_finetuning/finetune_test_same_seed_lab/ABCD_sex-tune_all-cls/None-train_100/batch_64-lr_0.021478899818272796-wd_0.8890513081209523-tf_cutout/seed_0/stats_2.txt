"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 1166.3852310180664, "training_acc": 52.0, "val_loss": 2.402315258946388e+20, "val_acc": 48.0}
{"epoch": 1, "training_loss": 6.372246554820858e+20, "training_acc": 47.0, "val_loss": 8515765600.0, "val_acc": 52.0}
{"epoch": 2, "training_loss": 18767445544.0, "training_acc": 53.0, "val_loss": 899454300.0, "val_acc": 48.0}
{"epoch": 3, "training_loss": 2399515808.0, "training_acc": 57.0, "val_loss": 35396318.75, "val_acc": 52.0}
{"epoch": 4, "training_loss": 108578561.5, "training_acc": 45.0, "val_loss": 3131402.9296875, "val_acc": 52.0}
{"epoch": 5, "training_loss": 6889543.00390625, "training_acc": 59.0, "val_loss": 3235271.484375, "val_acc": 52.0}
{"epoch": 6, "training_loss": 8737561.734375, "training_acc": 53.0, "val_loss": 385755.76171875, "val_acc": 48.0}
{"epoch": 7, "training_loss": 1256435.068359375, "training_acc": 49.0, "val_loss": 240993.994140625, "val_acc": 48.0}
{"epoch": 8, "training_loss": 707571.435546875, "training_acc": 47.0, "val_loss": 135108.154296875, "val_acc": 52.0}
{"epoch": 9, "training_loss": 430583.21875, "training_acc": 53.0, "val_loss": 16243.522644042969, "val_acc": 48.0}
{"epoch": 10, "training_loss": 41294.28234863281, "training_acc": 55.0, "val_loss": 2287.1028900146484, "val_acc": 48.0}
{"epoch": 11, "training_loss": 5603.653202056885, "training_acc": 47.0, "val_loss": 2249.443244934082, "val_acc": 52.0}
{"epoch": 12, "training_loss": 7559.336883544922, "training_acc": 53.0, "val_loss": 469.59543228149414, "val_acc": 52.0}
{"epoch": 13, "training_loss": 1113.5318517684937, "training_acc": 53.0, "val_loss": 267.3137664794922, "val_acc": 48.0}
{"epoch": 14, "training_loss": 1027.0529136657715, "training_acc": 47.0, "val_loss": 166.17552042007446, "val_acc": 48.0}
{"epoch": 15, "training_loss": 548.6470384597778, "training_acc": 47.0, "val_loss": 21.740588545799255, "val_acc": 48.0}
{"epoch": 16, "training_loss": 130.60962104797363, "training_acc": 49.0, "val_loss": 25.250431895256042, "val_acc": 52.0}
{"epoch": 17, "training_loss": 87.6619725227356, "training_acc": 53.0, "val_loss": 17.74934083223343, "val_acc": 52.0}
{"epoch": 18, "training_loss": 71.58982181549072, "training_acc": 47.0, "val_loss": 17.44074523448944, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.14266538619995, "training_acc": 51.0, "val_loss": 17.57725030183792, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.49315476417542, "training_acc": 53.0, "val_loss": 17.526650428771973, "val_acc": 52.0}
{"epoch": 21, "training_loss": 70.05220437049866, "training_acc": 47.0, "val_loss": 17.311617732048035, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.44219064712524, "training_acc": 53.0, "val_loss": 17.319190502166748, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.15236306190491, "training_acc": 53.0, "val_loss": 17.32822060585022, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.18663740158081, "training_acc": 53.0, "val_loss": 17.31504052877426, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.25711750984192, "training_acc": 53.0, "val_loss": 17.30906218290329, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.41736388206482, "training_acc": 53.0, "val_loss": 17.31480062007904, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.19265007972717, "training_acc": 53.0, "val_loss": 17.309708893299103, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.1519091129303, "training_acc": 53.0, "val_loss": 17.311368882656097, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.13009595870972, "training_acc": 53.0, "val_loss": 17.32247918844223, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.28033256530762, "training_acc": 53.0, "val_loss": 17.33820289373398, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.20218205451965, "training_acc": 53.0, "val_loss": 17.31160879135132, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.1381471157074, "training_acc": 53.0, "val_loss": 17.30845421552658, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.25647044181824, "training_acc": 53.0, "val_loss": 17.308659851551056, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.14203953742981, "training_acc": 53.0, "val_loss": 17.31102019548416, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.15422081947327, "training_acc": 53.0, "val_loss": 17.31579154729843, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.13904333114624, "training_acc": 53.0, "val_loss": 17.315134406089783, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.17399549484253, "training_acc": 53.0, "val_loss": 17.311736941337585, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.15726041793823, "training_acc": 53.0, "val_loss": 17.310748994350433, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.15354585647583, "training_acc": 53.0, "val_loss": 17.310790717601776, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.13300728797913, "training_acc": 53.0, "val_loss": 17.30957329273224, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.18438076972961, "training_acc": 53.0, "val_loss": 17.308734357357025, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.16498184204102, "training_acc": 53.0, "val_loss": 17.308667302131653, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.15429019927979, "training_acc": 53.0, "val_loss": 17.30867028236389, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.15743064880371, "training_acc": 53.0, "val_loss": 17.30867177248001, "val_acc": 52.0}
{"epoch": 45, "training_loss": 69.1555004119873, "training_acc": 53.0, "val_loss": 17.308712005615234, "val_acc": 52.0}
{"epoch": 46, "training_loss": 69.15940833091736, "training_acc": 53.0, "val_loss": 17.308765649795532, "val_acc": 52.0}
{"epoch": 47, "training_loss": 69.16115975379944, "training_acc": 53.0, "val_loss": 17.308925092220306, "val_acc": 52.0}
{"epoch": 48, "training_loss": 69.16714191436768, "training_acc": 53.0, "val_loss": 17.309021949768066, "val_acc": 52.0}
{"epoch": 49, "training_loss": 69.18721652030945, "training_acc": 53.0, "val_loss": 17.309042811393738, "val_acc": 52.0}
{"epoch": 50, "training_loss": 69.16345286369324, "training_acc": 53.0, "val_loss": 17.308713495731354, "val_acc": 52.0}
{"epoch": 51, "training_loss": 69.16292834281921, "training_acc": 53.0, "val_loss": 17.308782041072845, "val_acc": 52.0}
