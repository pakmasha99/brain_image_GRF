"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 483.7338180541992, "training_acc": 53.0, "val_loss": 7.31970551839685e+25, "val_acc": 48.0}
{"epoch": 1, "training_loss": 2.1253259109778096e+26, "training_acc": 47.0, "val_loss": 7.669135473140386e+22, "val_acc": 48.0}
{"epoch": 2, "training_loss": 2.1892015831129906e+23, "training_acc": 47.0, "val_loss": 1320297676800.0, "val_acc": 48.0}
{"epoch": 3, "training_loss": 4355563061248.0, "training_acc": 47.0, "val_loss": 64878828800.0, "val_acc": 48.0}
{"epoch": 4, "training_loss": 182402859776.0, "training_acc": 53.0, "val_loss": 1590473900.0, "val_acc": 52.0}
{"epoch": 5, "training_loss": 29745856512.0, "training_acc": 55.0, "val_loss": 290318775.0, "val_acc": 48.0}
{"epoch": 6, "training_loss": 725813273.5, "training_acc": 53.0, "val_loss": 9985915.625, "val_acc": 48.0}
{"epoch": 7, "training_loss": 28730823.25, "training_acc": 55.0, "val_loss": 2187603.125, "val_acc": 52.0}
{"epoch": 8, "training_loss": 6206391.234375, "training_acc": 53.0, "val_loss": 75230.31005859375, "val_acc": 48.0}
{"epoch": 9, "training_loss": 212871.47778320312, "training_acc": 47.0, "val_loss": 105682.18994140625, "val_acc": 52.0}
{"epoch": 10, "training_loss": 290662.79681396484, "training_acc": 53.0, "val_loss": 6270.86296081543, "val_acc": 48.0}
{"epoch": 11, "training_loss": 18346.47589111328, "training_acc": 47.0, "val_loss": 1390.4279708862305, "val_acc": 52.0}
{"epoch": 12, "training_loss": 5224.257965087891, "training_acc": 53.0, "val_loss": 460.19129753112793, "val_acc": 52.0}
{"epoch": 13, "training_loss": 1885.6514892578125, "training_acc": 45.0, "val_loss": 246.59183025360107, "val_acc": 48.0}
{"epoch": 14, "training_loss": 680.903618812561, "training_acc": 47.0, "val_loss": 96.46974205970764, "val_acc": 52.0}
{"epoch": 15, "training_loss": 304.4554147720337, "training_acc": 53.0, "val_loss": 18.124675750732422, "val_acc": 44.0}
{"epoch": 16, "training_loss": 1330.722267150879, "training_acc": 48.0, "val_loss": 21.595536172389984, "val_acc": 52.0}
{"epoch": 17, "training_loss": 292.2434329986572, "training_acc": 39.0, "val_loss": 50.06248950958252, "val_acc": 48.0}
{"epoch": 18, "training_loss": 158.9111795425415, "training_acc": 47.0, "val_loss": 17.330539226531982, "val_acc": 52.0}
{"epoch": 19, "training_loss": 73.63400220870972, "training_acc": 39.0, "val_loss": 18.84181797504425, "val_acc": 52.0}
{"epoch": 20, "training_loss": 73.68394875526428, "training_acc": 53.0, "val_loss": 17.59096533060074, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.64627385139465, "training_acc": 53.0, "val_loss": 17.38489419221878, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.53622126579285, "training_acc": 47.0, "val_loss": 17.31521189212799, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.39892196655273, "training_acc": 53.0, "val_loss": 17.328372597694397, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.31820273399353, "training_acc": 53.0, "val_loss": 17.322997748851776, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.42273688316345, "training_acc": 53.0, "val_loss": 17.31143295764923, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.21869277954102, "training_acc": 53.0, "val_loss": 17.31022298336029, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.20637345314026, "training_acc": 53.0, "val_loss": 17.310580611228943, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.26825547218323, "training_acc": 53.0, "val_loss": 17.31555461883545, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.13500642776489, "training_acc": 53.0, "val_loss": 17.310313880443573, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.16778659820557, "training_acc": 53.0, "val_loss": 17.30932891368866, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.15217137336731, "training_acc": 53.0, "val_loss": 17.31080561876297, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.17731881141663, "training_acc": 53.0, "val_loss": 17.31717437505722, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.13326215744019, "training_acc": 53.0, "val_loss": 17.32124239206314, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.13708448410034, "training_acc": 53.0, "val_loss": 17.32771098613739, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.15138363838196, "training_acc": 53.0, "val_loss": 17.332324385643005, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.16360831260681, "training_acc": 53.0, "val_loss": 17.33357459306717, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.19552946090698, "training_acc": 53.0, "val_loss": 17.329804599285126, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.15771222114563, "training_acc": 53.0, "val_loss": 17.328473925590515, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.15246057510376, "training_acc": 53.0, "val_loss": 17.325523495674133, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.16998386383057, "training_acc": 53.0, "val_loss": 17.32095330953598, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.14018273353577, "training_acc": 53.0, "val_loss": 17.318448424339294, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.14142036437988, "training_acc": 53.0, "val_loss": 17.315775156021118, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.13485169410706, "training_acc": 53.0, "val_loss": 17.31368452310562, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.14474582672119, "training_acc": 53.0, "val_loss": 17.311912775039673, "val_acc": 52.0}
{"epoch": 45, "training_loss": 69.13265299797058, "training_acc": 53.0, "val_loss": 17.310939729213715, "val_acc": 52.0}
{"epoch": 46, "training_loss": 69.1413037776947, "training_acc": 53.0, "val_loss": 17.309890687465668, "val_acc": 52.0}
{"epoch": 47, "training_loss": 69.13796544075012, "training_acc": 53.0, "val_loss": 17.309337854385376, "val_acc": 52.0}
{"epoch": 48, "training_loss": 69.14087843894958, "training_acc": 53.0, "val_loss": 17.30894297361374, "val_acc": 52.0}
{"epoch": 49, "training_loss": 69.14703106880188, "training_acc": 53.0, "val_loss": 17.308726906776428, "val_acc": 52.0}
{"epoch": 50, "training_loss": 69.14965844154358, "training_acc": 53.0, "val_loss": 17.30867326259613, "val_acc": 52.0}
{"epoch": 51, "training_loss": 69.15483069419861, "training_acc": 53.0, "val_loss": 17.308706045150757, "val_acc": 52.0}
{"epoch": 52, "training_loss": 69.16037678718567, "training_acc": 53.0, "val_loss": 17.30877012014389, "val_acc": 52.0}
{"epoch": 53, "training_loss": 69.1616997718811, "training_acc": 53.0, "val_loss": 17.308780550956726, "val_acc": 52.0}
{"epoch": 54, "training_loss": 69.16078209877014, "training_acc": 53.0, "val_loss": 17.30872243642807, "val_acc": 52.0}
{"epoch": 55, "training_loss": 69.15817666053772, "training_acc": 53.0, "val_loss": 17.30867475271225, "val_acc": 52.0}
{"epoch": 56, "training_loss": 69.15311551094055, "training_acc": 53.0, "val_loss": 17.308852076530457, "val_acc": 52.0}
{"epoch": 57, "training_loss": 69.17482781410217, "training_acc": 53.0, "val_loss": 17.309238016605377, "val_acc": 52.0}
{"epoch": 58, "training_loss": 69.14537262916565, "training_acc": 53.0, "val_loss": 17.308932542800903, "val_acc": 52.0}
{"epoch": 59, "training_loss": 69.17212057113647, "training_acc": 53.0, "val_loss": 17.30867475271225, "val_acc": 52.0}
{"epoch": 60, "training_loss": 69.15482687950134, "training_acc": 53.0, "val_loss": 17.308679223060608, "val_acc": 52.0}
{"epoch": 61, "training_loss": 69.15266919136047, "training_acc": 53.0, "val_loss": 17.308741807937622, "val_acc": 52.0}
{"epoch": 62, "training_loss": 69.16782402992249, "training_acc": 53.0, "val_loss": 17.308881878852844, "val_acc": 52.0}
{"epoch": 63, "training_loss": 69.14068746566772, "training_acc": 53.0, "val_loss": 17.31017976999283, "val_acc": 52.0}
{"epoch": 64, "training_loss": 69.13832545280457, "training_acc": 53.0, "val_loss": 17.313052713871002, "val_acc": 52.0}
{"epoch": 65, "training_loss": 69.1608316898346, "training_acc": 53.0, "val_loss": 17.315714061260223, "val_acc": 52.0}
{"epoch": 66, "training_loss": 69.1349561214447, "training_acc": 53.0, "val_loss": 17.3141747713089, "val_acc": 52.0}
{"epoch": 67, "training_loss": 69.13945508003235, "training_acc": 53.0, "val_loss": 17.312632501125336, "val_acc": 52.0}
{"epoch": 68, "training_loss": 69.15674138069153, "training_acc": 53.0, "val_loss": 17.31169819831848, "val_acc": 52.0}
{"epoch": 69, "training_loss": 69.15941715240479, "training_acc": 53.0, "val_loss": 17.31240749359131, "val_acc": 52.0}
