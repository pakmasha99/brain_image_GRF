"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 2030.9013214111328, "training_acc": 45.0, "val_loss": 1885797785600.0, "val_acc": 52.0}
{"epoch": 1, "training_loss": 5262769753104.0, "training_acc": 43.0, "val_loss": 877153600.0, "val_acc": 48.0}
{"epoch": 2, "training_loss": 2671099349.0, "training_acc": 41.0, "val_loss": 1255937.01171875, "val_acc": 48.0}
{"epoch": 3, "training_loss": 11384697.5625, "training_acc": 47.0, "val_loss": 4810413.28125, "val_acc": 48.0}
{"epoch": 4, "training_loss": 76603118.0, "training_acc": 49.0, "val_loss": 57683450.0, "val_acc": 48.0}
{"epoch": 5, "training_loss": 262860674.0, "training_acc": 47.0, "val_loss": 561084400.0, "val_acc": 52.0}
{"epoch": 6, "training_loss": 1330769123.8834229, "training_acc": 54.0, "val_loss": 10762214.0625, "val_acc": 48.0}
{"epoch": 7, "training_loss": 64158932.5, "training_acc": 47.0, "val_loss": 14829632.8125, "val_acc": 48.0}
{"epoch": 8, "training_loss": 74852753.25, "training_acc": 45.0, "val_loss": 817126150.0, "val_acc": 48.0}
{"epoch": 9, "training_loss": 2312805664.5, "training_acc": 43.0, "val_loss": 25659375.0, "val_acc": 52.0}
{"epoch": 10, "training_loss": 3832461568864.0, "training_acc": 52.0, "val_loss": 903611.5234375, "val_acc": 48.0}
{"epoch": 11, "training_loss": 16592774.0, "training_acc": 51.0, "val_loss": 85236668.75, "val_acc": 52.0}
{"epoch": 12, "training_loss": 255027255.0, "training_acc": 53.0, "val_loss": 55025581.25, "val_acc": 52.0}
{"epoch": 13, "training_loss": 463504932.0, "training_acc": 53.0, "val_loss": 139833912.5, "val_acc": 52.0}
{"epoch": 14, "training_loss": 497173902.0, "training_acc": 53.0, "val_loss": 49057446.875, "val_acc": 52.0}
{"epoch": 15, "training_loss": 7704582272.0, "training_acc": 51.0, "val_loss": 1708515.625, "val_acc": 48.0}
{"epoch": 16, "training_loss": 128081622.0, "training_acc": 47.0, "val_loss": 42892946.875, "val_acc": 52.0}
{"epoch": 17, "training_loss": 163421705.0, "training_acc": 53.0, "val_loss": 2412832.421875, "val_acc": 52.0}
{"epoch": 18, "training_loss": 7640250.4921875, "training_acc": 53.0, "val_loss": 302081.298828125, "val_acc": 52.0}
{"epoch": 19, "training_loss": 903019.3681640625, "training_acc": 45.0, "val_loss": 247817.7490234375, "val_acc": 52.0}
{"epoch": 20, "training_loss": 717680.884765625, "training_acc": 53.0, "val_loss": 21284.939575195312, "val_acc": 48.0}
{"epoch": 21, "training_loss": 3120116.140625, "training_acc": 40.0, "val_loss": 272525.3662109375, "val_acc": 52.0}
{"epoch": 22, "training_loss": 1300910.2265625, "training_acc": 55.0, "val_loss": 284366.455078125, "val_acc": 48.0}
{"epoch": 23, "training_loss": 772177.97265625, "training_acc": 55.0, "val_loss": 11258.379364013672, "val_acc": 48.0}
{"epoch": 24, "training_loss": 375096.18359375, "training_acc": 53.0, "val_loss": 165764.4775390625, "val_acc": 52.0}
{"epoch": 25, "training_loss": 556740.015625, "training_acc": 51.0, "val_loss": 57195.562744140625, "val_acc": 48.0}
{"epoch": 26, "training_loss": 196225.55615234375, "training_acc": 53.0, "val_loss": 476700.537109375, "val_acc": 48.0}
{"epoch": 27, "training_loss": 1585951.25390625, "training_acc": 45.0, "val_loss": 115236.07177734375, "val_acc": 52.0}
{"epoch": 28, "training_loss": 428914.12890625, "training_acc": 61.0, "val_loss": 112268.73779296875, "val_acc": 48.0}
{"epoch": 29, "training_loss": 311633.8125, "training_acc": 51.0, "val_loss": 33918.10607910156, "val_acc": 48.0}
{"epoch": 30, "training_loss": 122709.9345703125, "training_acc": 49.0, "val_loss": 297803.125, "val_acc": 52.0}
{"epoch": 31, "training_loss": 698213.9625244141, "training_acc": 53.0, "val_loss": 21684.877014160156, "val_acc": 48.0}
{"epoch": 32, "training_loss": 66274.83801269531, "training_acc": 51.0, "val_loss": 45765.49987792969, "val_acc": 48.0}
{"epoch": 33, "training_loss": 126106.60607910156, "training_acc": 47.0, "val_loss": 125619.32373046875, "val_acc": 52.0}
{"epoch": 34, "training_loss": 6935188.875, "training_acc": 53.0, "val_loss": 20351.695251464844, "val_acc": 52.0}
{"epoch": 35, "training_loss": 1561713.703125, "training_acc": 51.0, "val_loss": 358569.1162109375, "val_acc": 48.0}
{"epoch": 36, "training_loss": 1766108.8984375, "training_acc": 47.0, "val_loss": 23772.976684570312, "val_acc": 48.0}
{"epoch": 37, "training_loss": 959104.15625, "training_acc": 45.0, "val_loss": 74404.43725585938, "val_acc": 48.0}
{"epoch": 38, "training_loss": 624065.9140625, "training_acc": 55.0, "val_loss": 14132.289123535156, "val_acc": 52.0}
{"epoch": 39, "training_loss": 1191096.083984375, "training_acc": 53.0, "val_loss": 457154.00390625, "val_acc": 48.0}
{"epoch": 40, "training_loss": 1368899.625, "training_acc": 47.0, "val_loss": 237182.4951171875, "val_acc": 52.0}
{"epoch": 41, "training_loss": 791693.12890625, "training_acc": 53.0, "val_loss": 191963.8671875, "val_acc": 48.0}
{"epoch": 42, "training_loss": 494065.8466796875, "training_acc": 48.0, "val_loss": 60356.817626953125, "val_acc": 48.0}
