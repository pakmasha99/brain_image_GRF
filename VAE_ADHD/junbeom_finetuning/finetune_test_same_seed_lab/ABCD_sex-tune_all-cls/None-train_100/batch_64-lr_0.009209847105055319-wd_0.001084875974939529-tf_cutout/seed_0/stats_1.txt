"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 519.5666427612305, "training_acc": 53.0, "val_loss": 1288226400.0, "val_acc": 48.0}
{"epoch": 1, "training_loss": 2923501971.3168945, "training_acc": 57.0, "val_loss": 73273.48022460938, "val_acc": 48.0}
{"epoch": 2, "training_loss": 4735546.96875, "training_acc": 47.0, "val_loss": 74892.07763671875, "val_acc": 52.0}
{"epoch": 3, "training_loss": 209674.34729003906, "training_acc": 53.0, "val_loss": 1471.343994140625, "val_acc": 48.0}
{"epoch": 4, "training_loss": 9399.83251953125, "training_acc": 51.0, "val_loss": 650.6494998931885, "val_acc": 48.0}
{"epoch": 5, "training_loss": 2523.00675201416, "training_acc": 53.0, "val_loss": 3424.688720703125, "val_acc": 48.0}
{"epoch": 6, "training_loss": 9453.22946357727, "training_acc": 45.0, "val_loss": 2004.9449920654297, "val_acc": 48.0}
{"epoch": 7, "training_loss": 13724.969848632812, "training_acc": 45.0, "val_loss": 1168.626594543457, "val_acc": 52.0}
{"epoch": 8, "training_loss": 3020.40665435791, "training_acc": 53.0, "val_loss": 283.5545063018799, "val_acc": 52.0}
{"epoch": 9, "training_loss": 2029.6664581298828, "training_acc": 41.0, "val_loss": 59775.47607421875, "val_acc": 52.0}
{"epoch": 10, "training_loss": 165473.6814880371, "training_acc": 53.0, "val_loss": 3855.010986328125, "val_acc": 48.0}
{"epoch": 11, "training_loss": 12870.71810913086, "training_acc": 47.0, "val_loss": 275.5418062210083, "val_acc": 48.0}
{"epoch": 12, "training_loss": 3345.6478576660156, "training_acc": 59.0, "val_loss": 1703.3414840698242, "val_acc": 52.0}
{"epoch": 13, "training_loss": 5182.191953659058, "training_acc": 53.0, "val_loss": 63.78113627433777, "val_acc": 52.0}
{"epoch": 14, "training_loss": 589.3622627258301, "training_acc": 53.0, "val_loss": 124.94844198226929, "val_acc": 48.0}
{"epoch": 15, "training_loss": 380.32634258270264, "training_acc": 45.0, "val_loss": 26.557284593582153, "val_acc": 52.0}
{"epoch": 16, "training_loss": 621.1731147766113, "training_acc": 49.0, "val_loss": 55.12840747833252, "val_acc": 52.0}
{"epoch": 17, "training_loss": 258.72988986968994, "training_acc": 53.0, "val_loss": 35.07237434387207, "val_acc": 52.0}
{"epoch": 18, "training_loss": 135.45965051651, "training_acc": 49.0, "val_loss": 18.47102642059326, "val_acc": 52.0}
{"epoch": 19, "training_loss": 78.137535572052, "training_acc": 53.0, "val_loss": 20.373302698135376, "val_acc": 48.0}
{"epoch": 20, "training_loss": 81.96075892448425, "training_acc": 47.0, "val_loss": 29.529055953025818, "val_acc": 52.0}
{"epoch": 21, "training_loss": 109.8553192615509, "training_acc": 49.0, "val_loss": 17.442889511585236, "val_acc": 52.0}
{"epoch": 22, "training_loss": 128.21187686920166, "training_acc": 45.0, "val_loss": 25.62740445137024, "val_acc": 48.0}
{"epoch": 23, "training_loss": 105.43216037750244, "training_acc": 47.0, "val_loss": 17.345857620239258, "val_acc": 52.0}
{"epoch": 24, "training_loss": 85.64347887039185, "training_acc": 49.0, "val_loss": 20.541997253894806, "val_acc": 52.0}
{"epoch": 25, "training_loss": 71.68977856636047, "training_acc": 55.0, "val_loss": 22.65196591615677, "val_acc": 48.0}
{"epoch": 26, "training_loss": 86.56069350242615, "training_acc": 47.0, "val_loss": 17.47504025697708, "val_acc": 52.0}
{"epoch": 27, "training_loss": 71.9555356502533, "training_acc": 47.0, "val_loss": 18.04453283548355, "val_acc": 52.0}
{"epoch": 28, "training_loss": 70.81390762329102, "training_acc": 53.0, "val_loss": 18.095384538173676, "val_acc": 52.0}
{"epoch": 29, "training_loss": 68.82614016532898, "training_acc": 53.0, "val_loss": 20.683684945106506, "val_acc": 52.0}
{"epoch": 30, "training_loss": 80.97391486167908, "training_acc": 53.0, "val_loss": 17.712898552417755, "val_acc": 52.0}
{"epoch": 31, "training_loss": 74.82207036018372, "training_acc": 43.0, "val_loss": 17.806293070316315, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.56337809562683, "training_acc": 48.0, "val_loss": 17.887035012245178, "val_acc": 52.0}
{"epoch": 33, "training_loss": 72.08726048469543, "training_acc": 53.0, "val_loss": 18.367625772953033, "val_acc": 52.0}
{"epoch": 34, "training_loss": 71.9876606464386, "training_acc": 53.0, "val_loss": 17.53838062286377, "val_acc": 52.0}
{"epoch": 35, "training_loss": 70.24357914924622, "training_acc": 45.0, "val_loss": 17.72826313972473, "val_acc": 52.0}
{"epoch": 36, "training_loss": 70.89748859405518, "training_acc": 48.0, "val_loss": 17.4228698015213, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.9988362789154, "training_acc": 53.0, "val_loss": 17.523522675037384, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.2485044002533, "training_acc": 53.0, "val_loss": 17.451079189777374, "val_acc": 52.0}
{"epoch": 39, "training_loss": 70.09871411323547, "training_acc": 42.0, "val_loss": 17.61658787727356, "val_acc": 52.0}
{"epoch": 40, "training_loss": 72.30995869636536, "training_acc": 35.0, "val_loss": 17.375297844409943, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.48163914680481, "training_acc": 53.0, "val_loss": 17.370645701885223, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.22806644439697, "training_acc": 52.0, "val_loss": 17.375968396663666, "val_acc": 52.0}
