"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 171.99957084655762, "training_acc": 47.0, "val_loss": 4196.14143371582, "val_acc": 52.0}
{"epoch": 1, "training_loss": 10046.055540084839, "training_acc": 55.0, "val_loss": 25.349456071853638, "val_acc": 48.0}
{"epoch": 2, "training_loss": 135.91468238830566, "training_acc": 43.0, "val_loss": 17.32241064310074, "val_acc": 52.0}
{"epoch": 3, "training_loss": 70.58628916740417, "training_acc": 53.0, "val_loss": 17.70535111427307, "val_acc": 52.0}
{"epoch": 4, "training_loss": 70.05865025520325, "training_acc": 51.0, "val_loss": 18.055814504623413, "val_acc": 52.0}
{"epoch": 5, "training_loss": 71.09034705162048, "training_acc": 53.0, "val_loss": 17.399856448173523, "val_acc": 52.0}
{"epoch": 6, "training_loss": 73.55750799179077, "training_acc": 47.0, "val_loss": 17.35677272081375, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.35723423957825, "training_acc": 53.0, "val_loss": 17.717157304286957, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.90592312812805, "training_acc": 53.0, "val_loss": 17.42970496416092, "val_acc": 52.0}
{"epoch": 9, "training_loss": 72.95585250854492, "training_acc": 47.0, "val_loss": 17.314158380031586, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.2552444934845, "training_acc": 53.0, "val_loss": 17.42495745420456, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.41926550865173, "training_acc": 53.0, "val_loss": 17.380274832248688, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.27103018760681, "training_acc": 53.0, "val_loss": 17.31446087360382, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.25496792793274, "training_acc": 53.0, "val_loss": 17.32983887195587, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.32937097549438, "training_acc": 45.0, "val_loss": 17.31496751308441, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.25394868850708, "training_acc": 53.0, "val_loss": 17.323990166187286, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.16943836212158, "training_acc": 53.0, "val_loss": 17.337913811206818, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.21236395835876, "training_acc": 53.0, "val_loss": 17.345009744167328, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.22723698616028, "training_acc": 53.0, "val_loss": 17.340785264968872, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.15924835205078, "training_acc": 53.0, "val_loss": 17.31557548046112, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.13905048370361, "training_acc": 53.0, "val_loss": 17.309054732322693, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.17866158485413, "training_acc": 53.0, "val_loss": 17.3124760389328, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.22872161865234, "training_acc": 53.0, "val_loss": 17.31063425540924, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.18394088745117, "training_acc": 53.0, "val_loss": 17.30855703353882, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.16106748580933, "training_acc": 53.0, "val_loss": 17.31416881084442, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.14701056480408, "training_acc": 53.0, "val_loss": 17.334464192390442, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.1429214477539, "training_acc": 53.0, "val_loss": 17.393939197063446, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.43231773376465, "training_acc": 53.0, "val_loss": 17.40492582321167, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.45928311347961, "training_acc": 53.0, "val_loss": 17.347808182239532, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.17807483673096, "training_acc": 53.0, "val_loss": 17.330776154994965, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.22527289390564, "training_acc": 53.0, "val_loss": 17.31954962015152, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.15539503097534, "training_acc": 53.0, "val_loss": 17.319011688232422, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.13031005859375, "training_acc": 53.0, "val_loss": 17.323309183120728, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.18413662910461, "training_acc": 53.0, "val_loss": 17.323756217956543, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.19609022140503, "training_acc": 53.0, "val_loss": 17.329640686511993, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.17124152183533, "training_acc": 53.0, "val_loss": 17.319247126579285, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.16939187049866, "training_acc": 53.0, "val_loss": 17.315076291561127, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.12712073326111, "training_acc": 53.0, "val_loss": 17.315326631069183, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.16339039802551, "training_acc": 53.0, "val_loss": 17.315027117729187, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.14172673225403, "training_acc": 53.0, "val_loss": 17.31809377670288, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.13004660606384, "training_acc": 53.0, "val_loss": 17.317894101142883, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.14356279373169, "training_acc": 53.0, "val_loss": 17.316360771656036, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.1393530368805, "training_acc": 53.0, "val_loss": 17.316719889640808, "val_acc": 52.0}
