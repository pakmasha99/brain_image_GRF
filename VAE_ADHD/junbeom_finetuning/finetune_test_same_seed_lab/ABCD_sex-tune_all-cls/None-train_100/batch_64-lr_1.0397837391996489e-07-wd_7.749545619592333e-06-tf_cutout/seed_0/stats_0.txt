"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.65592622756958, "training_acc": 52.0, "val_loss": 17.18817502260208, "val_acc": 56.0}
{"epoch": 1, "training_loss": 69.78073525428772, "training_acc": 52.0, "val_loss": 17.18716025352478, "val_acc": 56.0}
{"epoch": 2, "training_loss": 69.79907584190369, "training_acc": 52.0, "val_loss": 17.186078429222107, "val_acc": 56.0}
{"epoch": 3, "training_loss": 69.76676440238953, "training_acc": 52.0, "val_loss": 17.185039818286896, "val_acc": 56.0}
{"epoch": 4, "training_loss": 69.69750165939331, "training_acc": 52.0, "val_loss": 17.183944582939148, "val_acc": 56.0}
{"epoch": 5, "training_loss": 69.71045398712158, "training_acc": 52.0, "val_loss": 17.182914912700653, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.73082685470581, "training_acc": 52.0, "val_loss": 17.181724309921265, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.72581219673157, "training_acc": 52.0, "val_loss": 17.180800437927246, "val_acc": 56.0}
{"epoch": 8, "training_loss": 69.69292950630188, "training_acc": 52.0, "val_loss": 17.17994213104248, "val_acc": 56.0}
{"epoch": 9, "training_loss": 69.71483707427979, "training_acc": 52.0, "val_loss": 17.179158329963684, "val_acc": 56.0}
{"epoch": 10, "training_loss": 69.63093614578247, "training_acc": 52.0, "val_loss": 17.17842072248459, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.68831205368042, "training_acc": 52.0, "val_loss": 17.17756986618042, "val_acc": 56.0}
{"epoch": 12, "training_loss": 69.66801023483276, "training_acc": 52.0, "val_loss": 17.176780104637146, "val_acc": 56.0}
{"epoch": 13, "training_loss": 69.59980869293213, "training_acc": 52.0, "val_loss": 17.176136374473572, "val_acc": 56.0}
{"epoch": 14, "training_loss": 69.62153720855713, "training_acc": 52.0, "val_loss": 17.175443470478058, "val_acc": 56.0}
{"epoch": 15, "training_loss": 69.59850788116455, "training_acc": 52.0, "val_loss": 17.17495173215866, "val_acc": 56.0}
{"epoch": 16, "training_loss": 69.62546277046204, "training_acc": 52.0, "val_loss": 17.17434823513031, "val_acc": 56.0}
{"epoch": 17, "training_loss": 69.61705660820007, "training_acc": 52.0, "val_loss": 17.173902690410614, "val_acc": 56.0}
{"epoch": 18, "training_loss": 69.55468249320984, "training_acc": 52.0, "val_loss": 17.17342585325241, "val_acc": 56.0}
{"epoch": 19, "training_loss": 69.57634902000427, "training_acc": 52.0, "val_loss": 17.173045873641968, "val_acc": 56.0}
{"epoch": 20, "training_loss": 69.53612637519836, "training_acc": 52.0, "val_loss": 17.172658443450928, "val_acc": 56.0}
{"epoch": 21, "training_loss": 69.52471923828125, "training_acc": 52.0, "val_loss": 17.172257602214813, "val_acc": 56.0}
{"epoch": 22, "training_loss": 69.55976057052612, "training_acc": 52.0, "val_loss": 17.172059416770935, "val_acc": 56.0}
{"epoch": 23, "training_loss": 69.53281855583191, "training_acc": 52.0, "val_loss": 17.1717569231987, "val_acc": 56.0}
{"epoch": 24, "training_loss": 69.52084350585938, "training_acc": 52.0, "val_loss": 17.171519994735718, "val_acc": 56.0}
{"epoch": 25, "training_loss": 69.50987267494202, "training_acc": 52.0, "val_loss": 17.171500623226166, "val_acc": 56.0}
{"epoch": 26, "training_loss": 69.5054669380188, "training_acc": 52.0, "val_loss": 17.17122197151184, "val_acc": 56.0}
{"epoch": 27, "training_loss": 69.51713132858276, "training_acc": 52.0, "val_loss": 17.17107743024826, "val_acc": 56.0}
{"epoch": 28, "training_loss": 69.42386889457703, "training_acc": 52.0, "val_loss": 17.170925438404083, "val_acc": 56.0}
{"epoch": 29, "training_loss": 69.50552487373352, "training_acc": 52.0, "val_loss": 17.170943319797516, "val_acc": 56.0}
{"epoch": 30, "training_loss": 69.45978212356567, "training_acc": 52.0, "val_loss": 17.171001434326172, "val_acc": 56.0}
{"epoch": 31, "training_loss": 69.49911975860596, "training_acc": 52.0, "val_loss": 17.170852422714233, "val_acc": 56.0}
{"epoch": 32, "training_loss": 69.45881390571594, "training_acc": 52.0, "val_loss": 17.170993983745575, "val_acc": 56.0}
{"epoch": 33, "training_loss": 69.40647959709167, "training_acc": 52.0, "val_loss": 17.171011865139008, "val_acc": 56.0}
{"epoch": 34, "training_loss": 69.39246368408203, "training_acc": 52.0, "val_loss": 17.171041667461395, "val_acc": 56.0}
{"epoch": 35, "training_loss": 69.46869277954102, "training_acc": 52.0, "val_loss": 17.171253263950348, "val_acc": 56.0}
{"epoch": 36, "training_loss": 69.41595554351807, "training_acc": 52.0, "val_loss": 17.171260714530945, "val_acc": 56.0}
{"epoch": 37, "training_loss": 69.4677357673645, "training_acc": 52.0, "val_loss": 17.171581089496613, "val_acc": 56.0}
{"epoch": 38, "training_loss": 69.36550998687744, "training_acc": 52.0, "val_loss": 17.171648144721985, "val_acc": 56.0}
{"epoch": 39, "training_loss": 69.39074063301086, "training_acc": 52.0, "val_loss": 17.171740531921387, "val_acc": 56.0}
{"epoch": 40, "training_loss": 69.37756443023682, "training_acc": 52.0, "val_loss": 17.172063887119293, "val_acc": 56.0}
{"epoch": 41, "training_loss": 69.41754245758057, "training_acc": 52.0, "val_loss": 17.172135412693024, "val_acc": 56.0}
{"epoch": 42, "training_loss": 69.417964220047, "training_acc": 52.0, "val_loss": 17.172381281852722, "val_acc": 56.0}
{"epoch": 43, "training_loss": 69.38636684417725, "training_acc": 52.0, "val_loss": 17.172658443450928, "val_acc": 56.0}
{"epoch": 44, "training_loss": 69.40185976028442, "training_acc": 52.0, "val_loss": 17.17275083065033, "val_acc": 56.0}
{"epoch": 45, "training_loss": 69.3774151802063, "training_acc": 52.0, "val_loss": 17.173168063163757, "val_acc": 56.0}
{"epoch": 46, "training_loss": 69.38846492767334, "training_acc": 52.0, "val_loss": 17.173433303833008, "val_acc": 56.0}
{"epoch": 47, "training_loss": 69.35218000411987, "training_acc": 52.0, "val_loss": 17.173708975315094, "val_acc": 56.0}
{"epoch": 48, "training_loss": 69.37324833869934, "training_acc": 52.0, "val_loss": 17.173926532268524, "val_acc": 56.0}
{"epoch": 49, "training_loss": 69.36985468864441, "training_acc": 52.0, "val_loss": 17.174306511878967, "val_acc": 56.0}
{"epoch": 50, "training_loss": 69.39458227157593, "training_acc": 52.0, "val_loss": 17.174701392650604, "val_acc": 56.0}
