"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 172.0365800857544, "training_acc": 55.0, "val_loss": 1.273117806780901e+21, "val_acc": 52.0}
{"epoch": 1, "training_loss": 3.553616795897466e+21, "training_acc": 53.0, "val_loss": 109524000.0, "val_acc": 52.0}
{"epoch": 2, "training_loss": 873022000.0, "training_acc": 55.0, "val_loss": 65931831.25, "val_acc": 48.0}
{"epoch": 3, "training_loss": 327898404.0, "training_acc": 47.0, "val_loss": 702192.87109375, "val_acc": 52.0}
{"epoch": 4, "training_loss": 341050140.0, "training_acc": 51.0, "val_loss": 2017316.796875, "val_acc": 48.0}
{"epoch": 5, "training_loss": 8653599.59375, "training_acc": 47.0, "val_loss": 267186.767578125, "val_acc": 48.0}
{"epoch": 6, "training_loss": 1987256.65625, "training_acc": 43.0, "val_loss": 668091.50390625, "val_acc": 48.0}
{"epoch": 7, "training_loss": 2407121.734375, "training_acc": 55.0, "val_loss": 667627.9296875, "val_acc": 52.0}
{"epoch": 8, "training_loss": 3073975.0625, "training_acc": 53.0, "val_loss": 3359353.90625, "val_acc": 52.0}
{"epoch": 9, "training_loss": 102674998.0, "training_acc": 57.0, "val_loss": 7057665.625, "val_acc": 48.0}
{"epoch": 10, "training_loss": 28067882.3125, "training_acc": 47.0, "val_loss": 2793241.6015625, "val_acc": 48.0}
{"epoch": 11, "training_loss": 8767560.875, "training_acc": 47.0, "val_loss": 557104.6875, "val_acc": 48.0}
{"epoch": 12, "training_loss": 1625212.49609375, "training_acc": 51.0, "val_loss": 576458.447265625, "val_acc": 48.0}
{"epoch": 13, "training_loss": 2000719.265625, "training_acc": 43.0, "val_loss": 13640.982055664062, "val_acc": 48.0}
{"epoch": 14, "training_loss": 2206789.703125, "training_acc": 54.0, "val_loss": 420197.75390625, "val_acc": 52.0}
{"epoch": 15, "training_loss": 1357373.25390625, "training_acc": 55.0, "val_loss": 18447.35870361328, "val_acc": 48.0}
{"epoch": 16, "training_loss": 7657523.62109375, "training_acc": 48.0, "val_loss": 262948.3642578125, "val_acc": 48.0}
{"epoch": 17, "training_loss": 1107385.32421875, "training_acc": 47.0, "val_loss": 54461.63330078125, "val_acc": 48.0}
{"epoch": 18, "training_loss": 353814.28125, "training_acc": 53.0, "val_loss": 1039741.50390625, "val_acc": 48.0}
{"epoch": 19, "training_loss": 3097551.7734375, "training_acc": 45.0, "val_loss": 665013.8671875, "val_acc": 48.0}
{"epoch": 20, "training_loss": 2467772.734375, "training_acc": 53.0, "val_loss": 2026628.515625, "val_acc": 48.0}
{"epoch": 21, "training_loss": 6036961.546875, "training_acc": 41.0, "val_loss": 116828.41796875, "val_acc": 48.0}
{"epoch": 22, "training_loss": 390164.947265625, "training_acc": 47.0, "val_loss": 584743.65234375, "val_acc": 48.0}
{"epoch": 23, "training_loss": 1685138.3466796875, "training_acc": 47.0, "val_loss": 140174.10888671875, "val_acc": 52.0}
{"epoch": 24, "training_loss": 14234337.4375, "training_acc": 43.0, "val_loss": 203094.66552734375, "val_acc": 52.0}
{"epoch": 25, "training_loss": 9556722.6875, "training_acc": 55.0, "val_loss": 547757.2265625, "val_acc": 52.0}
{"epoch": 26, "training_loss": 1722935.76171875, "training_acc": 53.0, "val_loss": 1730344.53125, "val_acc": 48.0}
{"epoch": 27, "training_loss": 7761065.15625, "training_acc": 43.0, "val_loss": 184908.85009765625, "val_acc": 48.0}
{"epoch": 28, "training_loss": 6260636.40625, "training_acc": 47.0, "val_loss": 712726.416015625, "val_acc": 52.0}
{"epoch": 29, "training_loss": 4280734.21875, "training_acc": 55.0, "val_loss": 175866.1865234375, "val_acc": 48.0}
{"epoch": 30, "training_loss": 2190811.078125, "training_acc": 55.0, "val_loss": 873884.765625, "val_acc": 52.0}
{"epoch": 31, "training_loss": 2683606.3515625, "training_acc": 53.0, "val_loss": 298264.208984375, "val_acc": 48.0}
{"epoch": 32, "training_loss": 975505.931640625, "training_acc": 47.0, "val_loss": 581188.37890625, "val_acc": 48.0}
