"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.1451575756073, "training_acc": 56.0, "val_loss": 17.324844002723694, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.31702375411987, "training_acc": 53.0, "val_loss": 17.31393337249756, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.18372464179993, "training_acc": 53.0, "val_loss": 17.31686294078827, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.60273480415344, "training_acc": 50.0, "val_loss": 17.341220378875732, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.17549896240234, "training_acc": 53.0, "val_loss": 17.334482073783875, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.34320783615112, "training_acc": 53.0, "val_loss": 17.318637669086456, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.11157298088074, "training_acc": 53.0, "val_loss": 17.33558624982834, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.82863545417786, "training_acc": 53.0, "val_loss": 17.31889247894287, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.2847638130188, "training_acc": 53.0, "val_loss": 17.31330305337906, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.23444032669067, "training_acc": 53.0, "val_loss": 17.314743995666504, "val_acc": 52.0}
{"epoch": 10, "training_loss": 70.03407979011536, "training_acc": 53.0, "val_loss": 17.324447631835938, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.0652346611023, "training_acc": 53.0, "val_loss": 17.31504797935486, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.29154229164124, "training_acc": 51.0, "val_loss": 17.320246994495392, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.22424125671387, "training_acc": 53.0, "val_loss": 17.313268780708313, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.2590057849884, "training_acc": 53.0, "val_loss": 17.333461344242096, "val_acc": 52.0}
{"epoch": 15, "training_loss": 70.016761302948, "training_acc": 53.0, "val_loss": 17.37317442893982, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.38497304916382, "training_acc": 53.0, "val_loss": 17.310844361782074, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.23389077186584, "training_acc": 53.0, "val_loss": 17.30951964855194, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.25019598007202, "training_acc": 53.0, "val_loss": 17.32027530670166, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.26504111289978, "training_acc": 53.0, "val_loss": 17.31063276529312, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.17661046981812, "training_acc": 53.0, "val_loss": 17.31107085943222, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.14328384399414, "training_acc": 53.0, "val_loss": 17.320892214775085, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.13053488731384, "training_acc": 53.0, "val_loss": 17.34650880098343, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.20206308364868, "training_acc": 53.0, "val_loss": 17.34655648469925, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.33988523483276, "training_acc": 53.0, "val_loss": 17.321525514125824, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.16574025154114, "training_acc": 53.0, "val_loss": 17.323192954063416, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.63280487060547, "training_acc": 53.0, "val_loss": 17.330458760261536, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.39292049407959, "training_acc": 53.0, "val_loss": 17.30927973985672, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.19117093086243, "training_acc": 53.0, "val_loss": 17.310184240341187, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.169926404953, "training_acc": 53.0, "val_loss": 17.310458421707153, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.17429804801941, "training_acc": 53.0, "val_loss": 17.309856414794922, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.22795557975769, "training_acc": 53.0, "val_loss": 17.314712703227997, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.1288652420044, "training_acc": 53.0, "val_loss": 17.315076291561127, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.26502132415771, "training_acc": 53.0, "val_loss": 17.315441370010376, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.11498355865479, "training_acc": 53.0, "val_loss": 17.310810089111328, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.1754105091095, "training_acc": 53.0, "val_loss": 17.315857112407684, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.22958731651306, "training_acc": 53.0, "val_loss": 17.318469285964966, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.2491307258606, "training_acc": 53.0, "val_loss": 17.310696840286255, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.23454451560974, "training_acc": 53.0, "val_loss": 17.311732470989227, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.10119366645813, "training_acc": 53.0, "val_loss": 17.343531548976898, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.2513267993927, "training_acc": 53.0, "val_loss": 17.374803125858307, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.26474094390869, "training_acc": 53.0, "val_loss": 17.350482940673828, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.21078777313232, "training_acc": 53.0, "val_loss": 17.32483059167862, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.17715454101562, "training_acc": 53.0, "val_loss": 17.31262356042862, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.13683199882507, "training_acc": 53.0, "val_loss": 17.31075495481491, "val_acc": 52.0}
{"epoch": 45, "training_loss": 69.24288272857666, "training_acc": 53.0, "val_loss": 17.31419563293457, "val_acc": 52.0}
{"epoch": 46, "training_loss": 69.26424407958984, "training_acc": 53.0, "val_loss": 17.322202026844025, "val_acc": 52.0}
