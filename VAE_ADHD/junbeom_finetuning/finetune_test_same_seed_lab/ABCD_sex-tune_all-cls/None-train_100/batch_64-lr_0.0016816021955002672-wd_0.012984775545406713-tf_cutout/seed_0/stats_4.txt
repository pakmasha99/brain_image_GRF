"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 151.0828037261963, "training_acc": 45.0, "val_loss": 92.2425389289856, "val_acc": 52.0}
{"epoch": 1, "training_loss": 488.42504119873047, "training_acc": 55.0, "val_loss": 26.212626695632935, "val_acc": 48.0}
{"epoch": 2, "training_loss": 97.05123996734619, "training_acc": 47.0, "val_loss": 17.31022745370865, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.31988501548767, "training_acc": 53.0, "val_loss": 17.429694533348083, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.3991711139679, "training_acc": 53.0, "val_loss": 17.346152663230896, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.27443671226501, "training_acc": 53.0, "val_loss": 19.91492658853531, "val_acc": 52.0}
{"epoch": 6, "training_loss": 79.20594334602356, "training_acc": 43.0, "val_loss": 17.40332692861557, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.65903210639954, "training_acc": 47.0, "val_loss": 17.35508292913437, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.48962116241455, "training_acc": 45.0, "val_loss": 17.317958176136017, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.29553365707397, "training_acc": 53.0, "val_loss": 17.308448255062103, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.1783287525177, "training_acc": 53.0, "val_loss": 17.306846380233765, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.16112184524536, "training_acc": 53.0, "val_loss": 17.30838418006897, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.22184443473816, "training_acc": 53.0, "val_loss": 17.31955260038376, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.14408087730408, "training_acc": 53.0, "val_loss": 17.31780171394348, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.32638597488403, "training_acc": 53.0, "val_loss": 17.31191575527191, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.2311520576477, "training_acc": 53.0, "val_loss": 17.319823801517487, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.13816285133362, "training_acc": 53.0, "val_loss": 17.315207421779633, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.23608827590942, "training_acc": 53.0, "val_loss": 17.31083244085312, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.13940048217773, "training_acc": 53.0, "val_loss": 17.313742637634277, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.14126491546631, "training_acc": 53.0, "val_loss": 17.316074669361115, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.15846991539001, "training_acc": 53.0, "val_loss": 17.317098379135132, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.14972114562988, "training_acc": 53.0, "val_loss": 17.312969267368317, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.16952848434448, "training_acc": 53.0, "val_loss": 17.311303317546844, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.14327263832092, "training_acc": 53.0, "val_loss": 17.3135906457901, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.14196038246155, "training_acc": 53.0, "val_loss": 17.314019799232483, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.13491368293762, "training_acc": 53.0, "val_loss": 17.31269806623459, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.13457894325256, "training_acc": 53.0, "val_loss": 17.31160283088684, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.13944554328918, "training_acc": 53.0, "val_loss": 17.31090098619461, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.13366460800171, "training_acc": 53.0, "val_loss": 17.31143295764923, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.13451743125916, "training_acc": 53.0, "val_loss": 17.3130601644516, "val_acc": 52.0}
