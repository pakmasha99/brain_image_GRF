"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 528.3458938598633, "training_acc": 47.0, "val_loss": 1.1861882404559074e+22, "val_acc": 52.0}
{"epoch": 1, "training_loss": 2.9233369584420155e+22, "training_acc": 53.0, "val_loss": 18926179.6875, "val_acc": 48.0}
{"epoch": 2, "training_loss": 64083796.0, "training_acc": 47.0, "val_loss": 802161.5234375, "val_acc": 52.0}
{"epoch": 3, "training_loss": 23869160.25, "training_acc": 45.0, "val_loss": 179100.62255859375, "val_acc": 48.0}
{"epoch": 4, "training_loss": 3857370.53125, "training_acc": 43.0, "val_loss": 641960.83984375, "val_acc": 52.0}
{"epoch": 5, "training_loss": 2277979.015625, "training_acc": 53.0, "val_loss": 1197641.015625, "val_acc": 48.0}
{"epoch": 6, "training_loss": 40365522.25, "training_acc": 43.0, "val_loss": 5033542.96875, "val_acc": 48.0}
{"epoch": 7, "training_loss": 15331109.15625, "training_acc": 57.0, "val_loss": 717947.0703125, "val_acc": 48.0}
{"epoch": 8, "training_loss": 2448627.9140625, "training_acc": 47.0, "val_loss": 525601.806640625, "val_acc": 52.0}
{"epoch": 9, "training_loss": 1342366.28515625, "training_acc": 53.0, "val_loss": 57110293.75, "val_acc": 48.0}
{"epoch": 10, "training_loss": 146445692.15234375, "training_acc": 49.0, "val_loss": 342658.935546875, "val_acc": 52.0}
{"epoch": 11, "training_loss": 2329987.4375, "training_acc": 53.0, "val_loss": 376035950.0, "val_acc": 48.0}
{"epoch": 12, "training_loss": 1009299610.5, "training_acc": 51.0, "val_loss": 7528653.125, "val_acc": 48.0}
{"epoch": 13, "training_loss": 38571013.5, "training_acc": 51.0, "val_loss": 323758.1787109375, "val_acc": 52.0}
{"epoch": 14, "training_loss": 1044787.85546875, "training_acc": 59.0, "val_loss": 325003.22265625, "val_acc": 52.0}
{"epoch": 15, "training_loss": 1246120.89453125, "training_acc": 49.0, "val_loss": 864103.80859375, "val_acc": 52.0}
{"epoch": 16, "training_loss": 2395918.75390625, "training_acc": 53.0, "val_loss": 708948.486328125, "val_acc": 48.0}
{"epoch": 17, "training_loss": 2165577.56640625, "training_acc": 47.0, "val_loss": 1154108.0078125, "val_acc": 52.0}
{"epoch": 18, "training_loss": 3682221.5078125, "training_acc": 53.0, "val_loss": 1477795.99609375, "val_acc": 48.0}
{"epoch": 19, "training_loss": 4574208.9140625, "training_acc": 47.0, "val_loss": 427721.875, "val_acc": 52.0}
{"epoch": 20, "training_loss": 1185143.58203125, "training_acc": 53.0, "val_loss": 117897.0947265625, "val_acc": 48.0}
{"epoch": 21, "training_loss": 761952.77734375, "training_acc": 49.0, "val_loss": 51497.369384765625, "val_acc": 52.0}
{"epoch": 22, "training_loss": 359910.142578125, "training_acc": 47.0, "val_loss": 138197.900390625, "val_acc": 48.0}
{"epoch": 23, "training_loss": 485214.69921875, "training_acc": 47.0, "val_loss": 4743.9544677734375, "val_acc": 52.0}
{"epoch": 24, "training_loss": 103635.96484375, "training_acc": 53.0, "val_loss": 31008118.75, "val_acc": 48.0}
{"epoch": 25, "training_loss": 169569814.0, "training_acc": 53.0, "val_loss": 192316.85791015625, "val_acc": 52.0}
{"epoch": 26, "training_loss": 897957.90234375, "training_acc": 53.0, "val_loss": 120567.3095703125, "val_acc": 48.0}
{"epoch": 27, "training_loss": 11944984.875, "training_acc": 55.0, "val_loss": 3521312.890625, "val_acc": 52.0}
{"epoch": 28, "training_loss": 10758824.21875, "training_acc": 53.0, "val_loss": 604470.60546875, "val_acc": 52.0}
{"epoch": 29, "training_loss": 2472908.390625, "training_acc": 51.0, "val_loss": 9846771.875, "val_acc": 52.0}
{"epoch": 30, "training_loss": 31103144.96875, "training_acc": 41.0, "val_loss": 1575673.6328125, "val_acc": 48.0}
{"epoch": 31, "training_loss": 5004610.25, "training_acc": 47.0, "val_loss": 1814995.3125, "val_acc": 52.0}
{"epoch": 32, "training_loss": 5682845.265625, "training_acc": 53.0, "val_loss": 380275.5615234375, "val_acc": 48.0}
{"epoch": 33, "training_loss": 1444198.1484375, "training_acc": 47.0, "val_loss": 6796.0357666015625, "val_acc": 48.0}
{"epoch": 34, "training_loss": 571241.765625, "training_acc": 50.0, "val_loss": 339668.6767578125, "val_acc": 52.0}
{"epoch": 35, "training_loss": 1033000.71875, "training_acc": 51.0, "val_loss": 160151.6845703125, "val_acc": 52.0}
{"epoch": 36, "training_loss": 456863.2958984375, "training_acc": 53.0, "val_loss": 379730.46875, "val_acc": 48.0}
{"epoch": 37, "training_loss": 1442922.55078125, "training_acc": 47.0, "val_loss": 81457.01293945312, "val_acc": 48.0}
{"epoch": 38, "training_loss": 645161.62109375, "training_acc": 47.0, "val_loss": 348036.376953125, "val_acc": 52.0}
{"epoch": 39, "training_loss": 1198526.005859375, "training_acc": 53.0, "val_loss": 44156.28662109375, "val_acc": 52.0}
{"epoch": 40, "training_loss": 399112.23828125, "training_acc": 55.0, "val_loss": 282114.74609375, "val_acc": 48.0}
{"epoch": 41, "training_loss": 1010463.775390625, "training_acc": 47.0, "val_loss": 48581.96716308594, "val_acc": 48.0}
{"epoch": 42, "training_loss": 249491.2919921875, "training_acc": 49.0, "val_loss": 102258.154296875, "val_acc": 52.0}
