"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 436.36220932006836, "training_acc": 57.0, "val_loss": 3.58058969849068e+23, "val_acc": 48.0}
{"epoch": 1, "training_loss": 9.974121649292635e+23, "training_acc": 47.0, "val_loss": 107246912000.0, "val_acc": 48.0}
{"epoch": 2, "training_loss": 409989594365952.0, "training_acc": 53.0, "val_loss": 1.16006462685184e+16, "val_acc": 48.0}
{"epoch": 3, "training_loss": 1.5137040080683663e+18, "training_acc": 45.0, "val_loss": 27661703577600.0, "val_acc": 52.0}
{"epoch": 4, "training_loss": 70939398800384.0, "training_acc": 53.0, "val_loss": 77601094400.0, "val_acc": 48.0}
{"epoch": 5, "training_loss": 229532652544.0, "training_acc": 47.0, "val_loss": 68001849600.0, "val_acc": 52.0}
{"epoch": 6, "training_loss": 168186348736.0, "training_acc": 53.0, "val_loss": 73056678400.0, "val_acc": 48.0}
{"epoch": 7, "training_loss": 233227683328.0, "training_acc": 47.0, "val_loss": 5659456000.0, "val_acc": 48.0}
{"epoch": 8, "training_loss": 15557172564.0, "training_acc": 47.0, "val_loss": 13172673600.0, "val_acc": 52.0}
{"epoch": 9, "training_loss": 42853593152.0, "training_acc": 53.0, "val_loss": 68941132800.0, "val_acc": 48.0}
{"epoch": 10, "training_loss": 162516143336.0, "training_acc": 47.0, "val_loss": 98878950.0, "val_acc": 52.0}
{"epoch": 11, "training_loss": 419296412.0, "training_acc": 53.0, "val_loss": 152323562.5, "val_acc": 48.0}
{"epoch": 12, "training_loss": 478621887.0, "training_acc": 57.0, "val_loss": 96022431.25, "val_acc": 52.0}
{"epoch": 13, "training_loss": 420401924.0, "training_acc": 53.0, "val_loss": 309016750.0, "val_acc": 52.0}
{"epoch": 14, "training_loss": 778463492.0625, "training_acc": 53.0, "val_loss": 68254831.25, "val_acc": 48.0}
{"epoch": 15, "training_loss": 287121282.0, "training_acc": 47.0, "val_loss": 12690232.03125, "val_acc": 52.0}
{"epoch": 16, "training_loss": 40761258.0, "training_acc": 57.0, "val_loss": 43222515.625, "val_acc": 52.0}
{"epoch": 17, "training_loss": 137329382.0, "training_acc": 53.0, "val_loss": 17501550.0, "val_acc": 48.0}
{"epoch": 18, "training_loss": 63884849.875, "training_acc": 47.0, "val_loss": 6011964.84375, "val_acc": 52.0}
{"epoch": 19, "training_loss": 18573787.90625, "training_acc": 53.0, "val_loss": 8373022.65625, "val_acc": 52.0}
{"epoch": 20, "training_loss": 28748482.0625, "training_acc": 53.0, "val_loss": 11275315.625, "val_acc": 48.0}
{"epoch": 21, "training_loss": 34388639.0625, "training_acc": 47.0, "val_loss": 10911773.4375, "val_acc": 52.0}
{"epoch": 22, "training_loss": 40434920.25, "training_acc": 53.0, "val_loss": 2235790.0390625, "val_acc": 52.0}
{"epoch": 23, "training_loss": 20494157.25, "training_acc": 45.0, "val_loss": 202764225.0, "val_acc": 52.0}
{"epoch": 24, "training_loss": 539073199.5, "training_acc": 49.0, "val_loss": 237853200.0, "val_acc": 48.0}
{"epoch": 25, "training_loss": 679827187.5, "training_acc": 47.0, "val_loss": 59925193.75, "val_acc": 52.0}
{"epoch": 26, "training_loss": 209118329.5, "training_acc": 53.0, "val_loss": 46010631.25, "val_acc": 48.0}
{"epoch": 27, "training_loss": 156760498.0, "training_acc": 47.0, "val_loss": 15228256.25, "val_acc": 52.0}
{"epoch": 28, "training_loss": 46147340.875, "training_acc": 53.0, "val_loss": 49318478.125, "val_acc": 48.0}
{"epoch": 29, "training_loss": 136807891.625, "training_acc": 47.0, "val_loss": 19760987.5, "val_acc": 52.0}
{"epoch": 30, "training_loss": 74634287.5, "training_acc": 53.0, "val_loss": 4560455.859375, "val_acc": 52.0}
{"epoch": 31, "training_loss": 2503189857.0, "training_acc": 53.0, "val_loss": 3087340000.0, "val_acc": 48.0}
{"epoch": 32, "training_loss": 153397436416.0, "training_acc": 51.0, "val_loss": 2022346800.0, "val_acc": 48.0}
{"epoch": 33, "training_loss": 10778366912.0, "training_acc": 47.0, "val_loss": 1315175900.0, "val_acc": 48.0}
{"epoch": 34, "training_loss": 3878185984.0, "training_acc": 47.0, "val_loss": 332510275.0, "val_acc": 48.0}
{"epoch": 35, "training_loss": 1322882800.0, "training_acc": 47.0, "val_loss": 203772925.0, "val_acc": 48.0}
{"epoch": 36, "training_loss": 688802554.0, "training_acc": 47.0, "val_loss": 3456907.421875, "val_acc": 48.0}
{"epoch": 37, "training_loss": 2313579176.0, "training_acc": 46.0, "val_loss": 371055800.0, "val_acc": 52.0}
{"epoch": 38, "training_loss": 1931357976.0, "training_acc": 53.0, "val_loss": 1159064800.0, "val_acc": 48.0}
{"epoch": 39, "training_loss": 2991985620.0, "training_acc": 51.0, "val_loss": 83039200.0, "val_acc": 52.0}
{"epoch": 40, "training_loss": 331894398.0, "training_acc": 53.0, "val_loss": 58007781.25, "val_acc": 52.0}
{"epoch": 41, "training_loss": 219483584.5, "training_acc": 47.0, "val_loss": 3454705.078125, "val_acc": 48.0}
