"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 75.18117427825928, "training_acc": 43.0, "val_loss": 60.867661237716675, "val_acc": 52.0}
{"epoch": 1, "training_loss": 169.62749314308167, "training_acc": 55.0, "val_loss": 17.36827939748764, "val_acc": 52.0}
{"epoch": 2, "training_loss": 70.78289341926575, "training_acc": 47.0, "val_loss": 19.04706507921219, "val_acc": 52.0}
{"epoch": 3, "training_loss": 74.31483936309814, "training_acc": 53.0, "val_loss": 17.315155267715454, "val_acc": 52.0}
{"epoch": 4, "training_loss": 71.11520314216614, "training_acc": 41.0, "val_loss": 17.317453026771545, "val_acc": 52.0}
{"epoch": 5, "training_loss": 68.94552636146545, "training_acc": 53.0, "val_loss": 17.383897304534912, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.30873417854309, "training_acc": 53.0, "val_loss": 17.433996498584747, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.64156079292297, "training_acc": 53.0, "val_loss": 17.354953289031982, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.1899299621582, "training_acc": 53.0, "val_loss": 17.319542169570923, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.28003001213074, "training_acc": 53.0, "val_loss": 17.335620522499084, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.47873044013977, "training_acc": 50.0, "val_loss": 17.33347475528717, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.18966341018677, "training_acc": 51.0, "val_loss": 17.31581687927246, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.14270091056824, "training_acc": 53.0, "val_loss": 17.344562709331512, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.19991064071655, "training_acc": 53.0, "val_loss": 17.352838814258575, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.46124005317688, "training_acc": 53.0, "val_loss": 17.338749766349792, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.25653958320618, "training_acc": 53.0, "val_loss": 17.349012196063995, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.2224633693695, "training_acc": 53.0, "val_loss": 17.335987091064453, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.13719177246094, "training_acc": 53.0, "val_loss": 17.3170804977417, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.19341731071472, "training_acc": 53.0, "val_loss": 17.310646176338196, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.13696646690369, "training_acc": 53.0, "val_loss": 17.310824990272522, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.16328144073486, "training_acc": 53.0, "val_loss": 17.311617732048035, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.16827321052551, "training_acc": 53.0, "val_loss": 17.310813069343567, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.15132069587708, "training_acc": 53.0, "val_loss": 17.311526834964752, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.15463590621948, "training_acc": 53.0, "val_loss": 17.316246032714844, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.14749312400818, "training_acc": 53.0, "val_loss": 17.32065975666046, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.1875593662262, "training_acc": 53.0, "val_loss": 17.321357131004333, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.1310703754425, "training_acc": 53.0, "val_loss": 17.328958213329315, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.14636635780334, "training_acc": 53.0, "val_loss": 17.334575951099396, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.1711528301239, "training_acc": 53.0, "val_loss": 17.33921319246292, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.18860340118408, "training_acc": 53.0, "val_loss": 17.334845662117004, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.1768569946289, "training_acc": 53.0, "val_loss": 17.32340008020401, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.1154944896698, "training_acc": 53.0, "val_loss": 17.311596870422363, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.13455295562744, "training_acc": 53.0, "val_loss": 17.309178411960602, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.15700459480286, "training_acc": 53.0, "val_loss": 17.31046289205551, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.17550921440125, "training_acc": 53.0, "val_loss": 17.311081290245056, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.18609690666199, "training_acc": 53.0, "val_loss": 17.31061339378357, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.16696381568909, "training_acc": 53.0, "val_loss": 17.309409379959106, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.14297080039978, "training_acc": 53.0, "val_loss": 17.310771346092224, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.11979794502258, "training_acc": 53.0, "val_loss": 17.315642535686493, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.25422310829163, "training_acc": 53.0, "val_loss": 17.32492446899414, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.1524543762207, "training_acc": 53.0, "val_loss": 17.32182651758194, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.18380355834961, "training_acc": 53.0, "val_loss": 17.315620183944702, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.16205310821533, "training_acc": 53.0, "val_loss": 17.315223813056946, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.13985419273376, "training_acc": 53.0, "val_loss": 17.311914265155792, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.18002271652222, "training_acc": 53.0, "val_loss": 17.310959100723267, "val_acc": 52.0}
{"epoch": 45, "training_loss": 69.13458943367004, "training_acc": 53.0, "val_loss": 17.311981320381165, "val_acc": 52.0}
{"epoch": 46, "training_loss": 69.12876057624817, "training_acc": 53.0, "val_loss": 17.314469814300537, "val_acc": 52.0}
{"epoch": 47, "training_loss": 69.13117980957031, "training_acc": 53.0, "val_loss": 17.31908768415451, "val_acc": 52.0}
{"epoch": 48, "training_loss": 69.13411402702332, "training_acc": 53.0, "val_loss": 17.323210835456848, "val_acc": 52.0}
{"epoch": 49, "training_loss": 69.13691973686218, "training_acc": 53.0, "val_loss": 17.325463891029358, "val_acc": 52.0}
{"epoch": 50, "training_loss": 69.14397883415222, "training_acc": 53.0, "val_loss": 17.327195405960083, "val_acc": 52.0}
{"epoch": 51, "training_loss": 69.15309619903564, "training_acc": 53.0, "val_loss": 17.326313257217407, "val_acc": 52.0}
