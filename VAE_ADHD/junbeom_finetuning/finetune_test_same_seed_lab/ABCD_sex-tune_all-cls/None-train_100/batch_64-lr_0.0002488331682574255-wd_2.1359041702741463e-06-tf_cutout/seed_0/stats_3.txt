"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 78.03278398513794, "training_acc": 44.0, "val_loss": 17.354944348335266, "val_acc": 52.0}
{"epoch": 1, "training_loss": 101.22582387924194, "training_acc": 47.0, "val_loss": 17.980407178401947, "val_acc": 52.0}
{"epoch": 2, "training_loss": 71.47638845443726, "training_acc": 49.0, "val_loss": 17.533092200756073, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.9912383556366, "training_acc": 47.0, "val_loss": 17.319709062576294, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.00055623054504, "training_acc": 53.0, "val_loss": 17.383502423763275, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.44387030601501, "training_acc": 53.0, "val_loss": 17.407192289829254, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.27051949501038, "training_acc": 53.0, "val_loss": 17.318828403949738, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.09970831871033, "training_acc": 53.0, "val_loss": 17.340360581874847, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.9740309715271, "training_acc": 50.0, "val_loss": 17.33657419681549, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.05665946006775, "training_acc": 57.0, "val_loss": 17.32979118824005, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.08310413360596, "training_acc": 53.0, "val_loss": 17.44459867477417, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.51623368263245, "training_acc": 53.0, "val_loss": 17.448820173740387, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.52435636520386, "training_acc": 53.0, "val_loss": 17.373953759670258, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.2869884967804, "training_acc": 53.0, "val_loss": 17.326422035694122, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.21932435035706, "training_acc": 53.0, "val_loss": 17.313072085380554, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.1598551273346, "training_acc": 53.0, "val_loss": 17.313793301582336, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.18859553337097, "training_acc": 53.0, "val_loss": 17.319804430007935, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.15818786621094, "training_acc": 53.0, "val_loss": 17.31456369161606, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.13277053833008, "training_acc": 53.0, "val_loss": 17.312432825565338, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.22435736656189, "training_acc": 53.0, "val_loss": 17.312751710414886, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.33282494544983, "training_acc": 53.0, "val_loss": 17.327161133289337, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.16801309585571, "training_acc": 53.0, "val_loss": 17.31889694929123, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.14144468307495, "training_acc": 53.0, "val_loss": 17.317765951156616, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.22097325325012, "training_acc": 53.0, "val_loss": 17.31591522693634, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.10875630378723, "training_acc": 53.0, "val_loss": 17.310702800750732, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.10305738449097, "training_acc": 53.0, "val_loss": 17.31409728527069, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.16569519042969, "training_acc": 53.0, "val_loss": 17.32179820537567, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.23198509216309, "training_acc": 53.0, "val_loss": 17.32608526945114, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.26493120193481, "training_acc": 53.0, "val_loss": 17.327257990837097, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.23017978668213, "training_acc": 53.0, "val_loss": 17.315711081027985, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.12889814376831, "training_acc": 53.0, "val_loss": 17.312732338905334, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.19503831863403, "training_acc": 53.0, "val_loss": 17.320820689201355, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.12608051300049, "training_acc": 53.0, "val_loss": 17.325159907341003, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.13533973693848, "training_acc": 53.0, "val_loss": 17.325328290462494, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.1388168334961, "training_acc": 53.0, "val_loss": 17.322970926761627, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.12255787849426, "training_acc": 53.0, "val_loss": 17.317192256450653, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.26055645942688, "training_acc": 53.0, "val_loss": 17.313283681869507, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.1388590335846, "training_acc": 53.0, "val_loss": 17.31574833393097, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.13686680793762, "training_acc": 53.0, "val_loss": 17.322078347206116, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.23915433883667, "training_acc": 53.0, "val_loss": 17.32526868581772, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.11185479164124, "training_acc": 53.0, "val_loss": 17.31749176979065, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.18766665458679, "training_acc": 53.0, "val_loss": 17.31201857328415, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.14525055885315, "training_acc": 53.0, "val_loss": 17.31235831975937, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.12386631965637, "training_acc": 53.0, "val_loss": 17.31393039226532, "val_acc": 52.0}
