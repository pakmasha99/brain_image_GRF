"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 114.57615661621094, "training_acc": 45.0, "val_loss": 96.14722728729248, "val_acc": 52.0}
{"epoch": 1, "training_loss": 538.0795612335205, "training_acc": 45.0, "val_loss": 18.603679537773132, "val_acc": 48.0}
{"epoch": 2, "training_loss": 71.41915440559387, "training_acc": 53.0, "val_loss": 18.55623722076416, "val_acc": 52.0}
{"epoch": 3, "training_loss": 72.54397487640381, "training_acc": 53.0, "val_loss": 17.34141856431961, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.68060398101807, "training_acc": 47.0, "val_loss": 17.350895702838898, "val_acc": 52.0}
{"epoch": 5, "training_loss": 70.19586777687073, "training_acc": 45.0, "val_loss": 17.362315952777863, "val_acc": 52.0}
{"epoch": 6, "training_loss": 70.16479778289795, "training_acc": 44.0, "val_loss": 17.31482446193695, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.37735104560852, "training_acc": 53.0, "val_loss": 17.31882095336914, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.05315494537354, "training_acc": 53.0, "val_loss": 17.403823137283325, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.3992109298706, "training_acc": 53.0, "val_loss": 17.403891682624817, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.38800001144409, "training_acc": 53.0, "val_loss": 17.356786131858826, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.25191974639893, "training_acc": 53.0, "val_loss": 17.324599623680115, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.20527696609497, "training_acc": 53.0, "val_loss": 17.313285171985626, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.15891695022583, "training_acc": 53.0, "val_loss": 17.313170433044434, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.19087266921997, "training_acc": 53.0, "val_loss": 17.318163812160492, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.14852523803711, "training_acc": 53.0, "val_loss": 17.312970757484436, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.1415913105011, "training_acc": 53.0, "val_loss": 17.311806976795197, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.22267937660217, "training_acc": 53.0, "val_loss": 17.312945425510406, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.31770086288452, "training_acc": 53.0, "val_loss": 17.32388585805893, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.18239331245422, "training_acc": 53.0, "val_loss": 17.316508293151855, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.13686847686768, "training_acc": 53.0, "val_loss": 17.315377295017242, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.2052993774414, "training_acc": 53.0, "val_loss": 17.3147976398468, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.12376046180725, "training_acc": 53.0, "val_loss": 17.310339212417603, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.12877702713013, "training_acc": 53.0, "val_loss": 17.309783399105072, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.1401093006134, "training_acc": 53.0, "val_loss": 17.311422526836395, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.17285513877869, "training_acc": 53.0, "val_loss": 17.31351613998413, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.20834374427795, "training_acc": 53.0, "val_loss": 17.31618493795395, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.21228766441345, "training_acc": 53.0, "val_loss": 17.314639687538147, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.19117426872253, "training_acc": 53.0, "val_loss": 17.311443388462067, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.19984936714172, "training_acc": 53.0, "val_loss": 17.31070727109909, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.14072465896606, "training_acc": 53.0, "val_loss": 17.31218248605728, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.13558602333069, "training_acc": 53.0, "val_loss": 17.31455624103546, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.1428108215332, "training_acc": 53.0, "val_loss": 17.31724739074707, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.13863205909729, "training_acc": 53.0, "val_loss": 17.317435145378113, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.23097944259644, "training_acc": 53.0, "val_loss": 17.314323782920837, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.13093185424805, "training_acc": 53.0, "val_loss": 17.318303883075714, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.15563488006592, "training_acc": 53.0, "val_loss": 17.32608824968338, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.24023532867432, "training_acc": 53.0, "val_loss": 17.328576743602753, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.13546085357666, "training_acc": 53.0, "val_loss": 17.319829761981964, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.19236493110657, "training_acc": 53.0, "val_loss": 17.31293797492981, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.14864039421082, "training_acc": 53.0, "val_loss": 17.311634123325348, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.13631534576416, "training_acc": 53.0, "val_loss": 17.31198877096176, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.14130020141602, "training_acc": 53.0, "val_loss": 17.312432825565338, "val_acc": 52.0}
