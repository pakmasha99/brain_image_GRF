"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 1123.8251419067383, "training_acc": 50.0, "val_loss": 5.399850854121472e+17, "val_acc": 56.0}
{"epoch": 1, "training_loss": 1.4324298673585452e+18, "training_acc": 52.0, "val_loss": 423607850.0, "val_acc": 56.0}
{"epoch": 2, "training_loss": 11189359488.0, "training_acc": 54.0, "val_loss": 772234850.0, "val_acc": 44.0}
{"epoch": 3, "training_loss": 2084303006.0, "training_acc": 48.0, "val_loss": 763179500.0, "val_acc": 56.0}
{"epoch": 4, "training_loss": 2592285560.0, "training_acc": 50.0, "val_loss": 350001228800.0, "val_acc": 44.0}
{"epoch": 5, "training_loss": 897195546112.0, "training_acc": 48.0, "val_loss": 64037248000.0, "val_acc": 44.0}
{"epoch": 6, "training_loss": 156508097152.0, "training_acc": 50.0, "val_loss": 23799816000.0, "val_acc": 56.0}
{"epoch": 7, "training_loss": 89287140864.0, "training_acc": 52.0, "val_loss": 139361075200.0, "val_acc": 44.0}
{"epoch": 8, "training_loss": 363689072101.0, "training_acc": 42.0, "val_loss": 9987971481600.0, "val_acc": 44.0}
{"epoch": 9, "training_loss": 23967473379840.0, "training_acc": 48.0, "val_loss": 18096496000.0, "val_acc": 56.0}
{"epoch": 10, "training_loss": 52240750272.0, "training_acc": 50.0, "val_loss": 117645273600.0, "val_acc": 56.0}
{"epoch": 11, "training_loss": 423420052480.0, "training_acc": 52.0, "val_loss": 182178508800.0, "val_acc": 44.0}
{"epoch": 12, "training_loss": 461926168448.0, "training_acc": 48.0, "val_loss": 1293710600.0, "val_acc": 56.0}
{"epoch": 13, "training_loss": 5163647600.0, "training_acc": 52.0, "val_loss": 1155968000.0, "val_acc": 44.0}
{"epoch": 14, "training_loss": 38503114496.0, "training_acc": 52.0, "val_loss": 268671950.0, "val_acc": 56.0}
{"epoch": 15, "training_loss": 12497490752.0, "training_acc": 46.0, "val_loss": 40497180800.0, "val_acc": 56.0}
{"epoch": 16, "training_loss": 1474118696960.0, "training_acc": 52.0, "val_loss": 31785846400.0, "val_acc": 44.0}
{"epoch": 17, "training_loss": 80936843264.0, "training_acc": 50.0, "val_loss": 102369900800.0, "val_acc": 44.0}
{"epoch": 18, "training_loss": 254622343424.0, "training_acc": 50.0, "val_loss": 292578227200.0, "val_acc": 44.0}
{"epoch": 19, "training_loss": 781907976192.0, "training_acc": 48.0, "val_loss": 328467609600.0, "val_acc": 44.0}
{"epoch": 20, "training_loss": 962799290368.0, "training_acc": 52.0, "val_loss": 601811507200.0, "val_acc": 44.0}
{"epoch": 21, "training_loss": 2027888205824.0, "training_acc": 48.0, "val_loss": 647160524800.0, "val_acc": 56.0}
{"epoch": 22, "training_loss": 2181432733696.0, "training_acc": 52.0, "val_loss": 29577811200.0, "val_acc": 56.0}
{"epoch": 23, "training_loss": 86608861056.0, "training_acc": 52.0, "val_loss": 218499675.0, "val_acc": 44.0}
{"epoch": 24, "training_loss": 573849616.5, "training_acc": 47.0, "val_loss": 5429071600.0, "val_acc": 44.0}
{"epoch": 25, "training_loss": 17757914688.0, "training_acc": 48.0, "val_loss": 1282957800.0, "val_acc": 56.0}
{"epoch": 26, "training_loss": 28363648512.0, "training_acc": 57.0, "val_loss": 1071778611200.0, "val_acc": 56.0}
{"epoch": 27, "training_loss": 2813658279928.0, "training_acc": 52.0, "val_loss": 5746858800.0, "val_acc": 44.0}
{"epoch": 28, "training_loss": 46739890944.0, "training_acc": 48.0, "val_loss": 12396940800.0, "val_acc": 44.0}
{"epoch": 29, "training_loss": 29236158416.0, "training_acc": 48.0, "val_loss": 5935294400.0, "val_acc": 44.0}
{"epoch": 30, "training_loss": 19157963360.0, "training_acc": 48.0, "val_loss": 7405168800.0, "val_acc": 44.0}
{"epoch": 31, "training_loss": 22874372672.0, "training_acc": 48.0, "val_loss": 1533724600.0, "val_acc": 44.0}
{"epoch": 32, "training_loss": 4871881088.0, "training_acc": 48.0, "val_loss": 345964400.0, "val_acc": 44.0}
{"epoch": 33, "training_loss": 892662425.0, "training_acc": 48.0, "val_loss": 20487151.5625, "val_acc": 44.0}
{"epoch": 34, "training_loss": 161048904.0, "training_acc": 46.0, "val_loss": 45721559.375, "val_acc": 56.0}
{"epoch": 35, "training_loss": 132756280.125, "training_acc": 52.0, "val_loss": 63666681.25, "val_acc": 44.0}
{"epoch": 36, "training_loss": 186164302.0, "training_acc": 48.0, "val_loss": 208354812.5, "val_acc": 44.0}
{"epoch": 37, "training_loss": 1105019328704.0, "training_acc": 58.0, "val_loss": 10498047180800.0, "val_acc": 44.0}
{"epoch": 38, "training_loss": 20825406681336.0, "training_acc": 58.0, "val_loss": 1505343795200.0, "val_acc": 56.0}
{"epoch": 39, "training_loss": 5313869529088.0, "training_acc": 52.0, "val_loss": 4479875891200.0, "val_acc": 44.0}
{"epoch": 40, "training_loss": 10692147968512.0, "training_acc": 48.0, "val_loss": 6842909600.0, "val_acc": 44.0}
{"epoch": 41, "training_loss": 29022583168.0, "training_acc": 48.0, "val_loss": 90533932800.0, "val_acc": 44.0}
{"epoch": 42, "training_loss": 257615492608.0, "training_acc": 48.0, "val_loss": 5000330000.0, "val_acc": 44.0}
{"epoch": 43, "training_loss": 28595761792.0, "training_acc": 48.0, "val_loss": 4415366000.0, "val_acc": 44.0}
{"epoch": 44, "training_loss": 910383757312.0, "training_acc": 52.0, "val_loss": 27740854400.0, "val_acc": 56.0}
{"epoch": 45, "training_loss": 75620944832.0, "training_acc": 52.0, "val_loss": 5226949600.0, "val_acc": 44.0}
{"epoch": 46, "training_loss": 14734936928.0, "training_acc": 48.0, "val_loss": 5699856400.0, "val_acc": 44.0}
{"epoch": 47, "training_loss": 14196437008.0, "training_acc": 48.0, "val_loss": 35303529600.0, "val_acc": 44.0}
{"epoch": 48, "training_loss": 85038176496.0, "training_acc": 48.0, "val_loss": 685392200.0, "val_acc": 56.0}
{"epoch": 49, "training_loss": 1826836973.0, "training_acc": 56.0, "val_loss": 11824126.5625, "val_acc": 48.0}
{"epoch": 50, "training_loss": 1291250904.0, "training_acc": 49.0, "val_loss": 71935718.75, "val_acc": 56.0}
{"epoch": 51, "training_loss": 962713464.0, "training_acc": 52.0, "val_loss": 200349687.5, "val_acc": 44.0}
{"epoch": 52, "training_loss": 881371144.0, "training_acc": 48.0, "val_loss": 103362637.5, "val_acc": 44.0}
{"epoch": 53, "training_loss": 282329592.0, "training_acc": 48.0, "val_loss": 16849079.6875, "val_acc": 56.0}
{"epoch": 54, "training_loss": 71070293.25, "training_acc": 52.0, "val_loss": 42915631.25, "val_acc": 44.0}
{"epoch": 55, "training_loss": 115699555.78125, "training_acc": 52.0, "val_loss": 139042962.5, "val_acc": 56.0}
{"epoch": 56, "training_loss": 668838893.0, "training_acc": 53.0, "val_loss": 161365787.5, "val_acc": 44.0}
{"epoch": 57, "training_loss": 2372408352.0, "training_acc": 52.0, "val_loss": 1090151100.0, "val_acc": 56.0}
{"epoch": 58, "training_loss": 7449079808.0, "training_acc": 52.0, "val_loss": 1597348800.0, "val_acc": 44.0}
{"epoch": 59, "training_loss": 5475172160.0, "training_acc": 48.0, "val_loss": 847027300.0, "val_acc": 44.0}
{"epoch": 60, "training_loss": 2493192052.0, "training_acc": 48.0, "val_loss": 344910575.0, "val_acc": 44.0}
{"epoch": 61, "training_loss": 1105097848.0, "training_acc": 48.0, "val_loss": 60762725.0, "val_acc": 44.0}
{"epoch": 62, "training_loss": 208268231.5, "training_acc": 50.0, "val_loss": 62041175.0, "val_acc": 56.0}
{"epoch": 63, "training_loss": 240815430.5, "training_acc": 52.0, "val_loss": 14301853.125, "val_acc": 56.0}
{"epoch": 64, "training_loss": 1278778048.0, "training_acc": 50.0, "val_loss": 617701750.0, "val_acc": 44.0}
{"epoch": 65, "training_loss": 8346220864.0, "training_acc": 52.0, "val_loss": 1262547600.0, "val_acc": 56.0}
{"epoch": 66, "training_loss": 3677941693.0, "training_acc": 48.0, "val_loss": 75991856.25, "val_acc": 56.0}
{"epoch": 67, "training_loss": 682950344.0, "training_acc": 58.0, "val_loss": 83186781.25, "val_acc": 44.0}
{"epoch": 68, "training_loss": 345342997.0, "training_acc": 42.0, "val_loss": 85804568.75, "val_acc": 44.0}
