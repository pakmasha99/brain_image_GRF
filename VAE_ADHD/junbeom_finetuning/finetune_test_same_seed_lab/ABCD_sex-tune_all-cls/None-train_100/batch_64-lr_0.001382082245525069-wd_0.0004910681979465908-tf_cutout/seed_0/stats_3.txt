"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 128.76537704467773, "training_acc": 45.0, "val_loss": 208.27043056488037, "val_acc": 52.0}
{"epoch": 1, "training_loss": 1015.106689453125, "training_acc": 45.0, "val_loss": 20.49429416656494, "val_acc": 48.0}
{"epoch": 2, "training_loss": 75.66738319396973, "training_acc": 53.0, "val_loss": 17.828845977783203, "val_acc": 52.0}
{"epoch": 3, "training_loss": 71.32290530204773, "training_acc": 53.0, "val_loss": 17.33628958463669, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.98948788642883, "training_acc": 49.0, "val_loss": 17.34376847743988, "val_acc": 52.0}
{"epoch": 5, "training_loss": 71.38018798828125, "training_acc": 53.0, "val_loss": 17.34730899333954, "val_acc": 52.0}
{"epoch": 6, "training_loss": 70.39983034133911, "training_acc": 43.0, "val_loss": 17.323113977909088, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.32966709136963, "training_acc": 53.0, "val_loss": 17.340056598186493, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.11111426353455, "training_acc": 53.0, "val_loss": 17.499886453151703, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.71598720550537, "training_acc": 53.0, "val_loss": 17.381343245506287, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.32412314414978, "training_acc": 53.0, "val_loss": 17.31124073266983, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.23017120361328, "training_acc": 53.0, "val_loss": 17.31201857328415, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.16393971443176, "training_acc": 53.0, "val_loss": 17.322559654712677, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.10650777816772, "training_acc": 53.0, "val_loss": 17.371678352355957, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.43184804916382, "training_acc": 53.0, "val_loss": 17.37366020679474, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.29360008239746, "training_acc": 53.0, "val_loss": 17.318029701709747, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.16359734535217, "training_acc": 53.0, "val_loss": 17.309299111366272, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.27967047691345, "training_acc": 53.0, "val_loss": 17.308995127677917, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.30250263214111, "training_acc": 53.0, "val_loss": 17.31189340353012, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.15967798233032, "training_acc": 53.0, "val_loss": 17.310084402561188, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.13803553581238, "training_acc": 53.0, "val_loss": 17.3113152384758, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.24392700195312, "training_acc": 53.0, "val_loss": 17.312540113925934, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.12739968299866, "training_acc": 53.0, "val_loss": 17.309100925922394, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.13807606697083, "training_acc": 53.0, "val_loss": 17.30860471725464, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.14460372924805, "training_acc": 53.0, "val_loss": 17.309822142124176, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.17623448371887, "training_acc": 53.0, "val_loss": 17.312052845954895, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.21241450309753, "training_acc": 53.0, "val_loss": 17.314960062503815, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.21591544151306, "training_acc": 53.0, "val_loss": 17.312869429588318, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.19165873527527, "training_acc": 53.0, "val_loss": 17.309585213661194, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.20067310333252, "training_acc": 53.0, "val_loss": 17.309175431728363, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.14145255088806, "training_acc": 53.0, "val_loss": 17.311210930347443, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.13414907455444, "training_acc": 53.0, "val_loss": 17.313891649246216, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.14061665534973, "training_acc": 53.0, "val_loss": 17.316535115242004, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.14070987701416, "training_acc": 53.0, "val_loss": 17.316946387290955, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.20394849777222, "training_acc": 53.0, "val_loss": 17.31487661600113, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.13304305076599, "training_acc": 53.0, "val_loss": 17.318324744701385, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.15079236030579, "training_acc": 53.0, "val_loss": 17.324885725975037, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.22960996627808, "training_acc": 53.0, "val_loss": 17.327380180358887, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.1420488357544, "training_acc": 53.0, "val_loss": 17.319422960281372, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.18369388580322, "training_acc": 53.0, "val_loss": 17.312607169151306, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.15067291259766, "training_acc": 53.0, "val_loss": 17.310935258865356, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.13489127159119, "training_acc": 53.0, "val_loss": 17.31090545654297, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.14011740684509, "training_acc": 53.0, "val_loss": 17.31100231409073, "val_acc": 52.0}
