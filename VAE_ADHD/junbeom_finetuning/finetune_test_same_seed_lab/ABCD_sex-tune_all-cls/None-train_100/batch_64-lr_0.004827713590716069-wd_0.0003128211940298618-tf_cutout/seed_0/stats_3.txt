"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 393.94514083862305, "training_acc": 43.0, "val_loss": 562012900.0, "val_acc": 52.0}
{"epoch": 1, "training_loss": 1354337976.1754217, "training_acc": 53.0, "val_loss": 2278.9377212524414, "val_acc": 52.0}
{"epoch": 2, "training_loss": 62368.7392578125, "training_acc": 61.0, "val_loss": 24977.67333984375, "val_acc": 48.0}
{"epoch": 3, "training_loss": 72047.91421508789, "training_acc": 47.0, "val_loss": 395.77178955078125, "val_acc": 52.0}
{"epoch": 4, "training_loss": 3136.3106689453125, "training_acc": 45.0, "val_loss": 1411.0767364501953, "val_acc": 52.0}
{"epoch": 5, "training_loss": 4323.060157775879, "training_acc": 53.0, "val_loss": 51.429688930511475, "val_acc": 48.0}
{"epoch": 6, "training_loss": 465.1326560974121, "training_acc": 49.0, "val_loss": 21.725404262542725, "val_acc": 48.0}
{"epoch": 7, "training_loss": 81.08725380897522, "training_acc": 47.0, "val_loss": 96.14970088005066, "val_acc": 52.0}
{"epoch": 8, "training_loss": 269.5861783027649, "training_acc": 55.0, "val_loss": 39.8618221282959, "val_acc": 52.0}
{"epoch": 9, "training_loss": 111.04421067237854, "training_acc": 61.0, "val_loss": 17.899057269096375, "val_acc": 52.0}
{"epoch": 10, "training_loss": 90.1433892250061, "training_acc": 51.0, "val_loss": 24.31311309337616, "val_acc": 48.0}
{"epoch": 11, "training_loss": 87.56557035446167, "training_acc": 48.0, "val_loss": 18.48442107439041, "val_acc": 52.0}
{"epoch": 12, "training_loss": 71.07628345489502, "training_acc": 55.0, "val_loss": 17.519359290599823, "val_acc": 52.0}
{"epoch": 13, "training_loss": 73.18140006065369, "training_acc": 51.0, "val_loss": 18.993645906448364, "val_acc": 48.0}
{"epoch": 14, "training_loss": 73.74387335777283, "training_acc": 47.0, "val_loss": 19.714240729808807, "val_acc": 52.0}
{"epoch": 15, "training_loss": 77.66720700263977, "training_acc": 47.0, "val_loss": 18.211838603019714, "val_acc": 52.0}
{"epoch": 16, "training_loss": 71.02203345298767, "training_acc": 53.0, "val_loss": 17.950892448425293, "val_acc": 52.0}
{"epoch": 17, "training_loss": 72.03914737701416, "training_acc": 47.0, "val_loss": 17.334967851638794, "val_acc": 52.0}
{"epoch": 18, "training_loss": 72.12594890594482, "training_acc": 43.0, "val_loss": 17.759625613689423, "val_acc": 52.0}
{"epoch": 19, "training_loss": 74.54737043380737, "training_acc": 47.0, "val_loss": 18.49559098482132, "val_acc": 52.0}
{"epoch": 20, "training_loss": 71.78790330886841, "training_acc": 53.0, "val_loss": 38.7664258480072, "val_acc": 48.0}
{"epoch": 21, "training_loss": 142.99267053604126, "training_acc": 49.0, "val_loss": 26.43495798110962, "val_acc": 52.0}
{"epoch": 22, "training_loss": 106.17742538452148, "training_acc": 45.0, "val_loss": 18.76191645860672, "val_acc": 48.0}
{"epoch": 23, "training_loss": 75.64216375350952, "training_acc": 49.0, "val_loss": 20.156830549240112, "val_acc": 52.0}
{"epoch": 24, "training_loss": 77.41608166694641, "training_acc": 53.0, "val_loss": 17.92529672384262, "val_acc": 52.0}
{"epoch": 25, "training_loss": 71.8887255191803, "training_acc": 47.0, "val_loss": 17.310306429862976, "val_acc": 52.0}
{"epoch": 26, "training_loss": 74.259845495224, "training_acc": 53.0, "val_loss": 17.424723505973816, "val_acc": 52.0}
{"epoch": 27, "training_loss": 70.4959602355957, "training_acc": 49.0, "val_loss": 17.86864995956421, "val_acc": 52.0}
{"epoch": 28, "training_loss": 71.00287771224976, "training_acc": 47.0, "val_loss": 17.41030514240265, "val_acc": 52.0}
{"epoch": 29, "training_loss": 75.14918518066406, "training_acc": 53.0, "val_loss": 17.85300225019455, "val_acc": 52.0}
{"epoch": 30, "training_loss": 71.44978332519531, "training_acc": 47.0, "val_loss": 17.41402894258499, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.34755945205688, "training_acc": 51.0, "val_loss": 17.633751034736633, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.87502264976501, "training_acc": 53.0, "val_loss": 17.316868901252747, "val_acc": 52.0}
{"epoch": 33, "training_loss": 70.86220932006836, "training_acc": 43.0, "val_loss": 17.323359847068787, "val_acc": 52.0}
{"epoch": 34, "training_loss": 71.91758632659912, "training_acc": 53.0, "val_loss": 17.528516054153442, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.84134984016418, "training_acc": 53.0, "val_loss": 17.317692935466766, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.26364827156067, "training_acc": 53.0, "val_loss": 17.37942397594452, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.79289484024048, "training_acc": 47.0, "val_loss": 17.39545464515686, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.59092020988464, "training_acc": 47.0, "val_loss": 17.32502281665802, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.22059798240662, "training_acc": 53.0, "val_loss": 17.40555912256241, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.40483021736145, "training_acc": 53.0, "val_loss": 17.37426072359085, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.23135447502136, "training_acc": 53.0, "val_loss": 17.329658567905426, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.1273033618927, "training_acc": 53.0, "val_loss": 17.31356382369995, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.40509533882141, "training_acc": 53.0, "val_loss": 17.314021289348602, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.11128306388855, "training_acc": 53.0, "val_loss": 17.352759838104248, "val_acc": 52.0}
