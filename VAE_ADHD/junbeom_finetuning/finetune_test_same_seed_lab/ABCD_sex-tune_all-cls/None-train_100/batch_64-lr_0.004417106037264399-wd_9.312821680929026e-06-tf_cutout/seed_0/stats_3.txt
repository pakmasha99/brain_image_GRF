"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 114.13959693908691, "training_acc": 52.0, "val_loss": 2186431.4453125, "val_acc": 48.0}
{"epoch": 1, "training_loss": 5131806.866580963, "training_acc": 47.0, "val_loss": 390.8292770385742, "val_acc": 52.0}
{"epoch": 2, "training_loss": 1185.2459506988525, "training_acc": 53.0, "val_loss": 89.536452293396, "val_acc": 48.0}
{"epoch": 3, "training_loss": 401.7594041824341, "training_acc": 39.0, "val_loss": 35.57204306125641, "val_acc": 52.0}
{"epoch": 4, "training_loss": 109.80754780769348, "training_acc": 53.0, "val_loss": 25.05568563938141, "val_acc": 48.0}
{"epoch": 5, "training_loss": 91.76178288459778, "training_acc": 55.0, "val_loss": 17.584235966205597, "val_acc": 52.0}
{"epoch": 6, "training_loss": 73.0865626335144, "training_acc": 47.0, "val_loss": 18.368050456047058, "val_acc": 52.0}
{"epoch": 7, "training_loss": 71.53424119949341, "training_acc": 51.0, "val_loss": 17.31019765138626, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.12698554992676, "training_acc": 53.0, "val_loss": 20.560669898986816, "val_acc": 52.0}
{"epoch": 9, "training_loss": 74.19258832931519, "training_acc": 55.0, "val_loss": 18.611298501491547, "val_acc": 48.0}
{"epoch": 10, "training_loss": 78.77156376838684, "training_acc": 39.0, "val_loss": 18.18571388721466, "val_acc": 52.0}
{"epoch": 11, "training_loss": 73.74077582359314, "training_acc": 51.0, "val_loss": 17.54114329814911, "val_acc": 52.0}
{"epoch": 12, "training_loss": 70.77663159370422, "training_acc": 49.0, "val_loss": 28.45534086227417, "val_acc": 48.0}
{"epoch": 13, "training_loss": 106.99888157844543, "training_acc": 45.0, "val_loss": 18.531528115272522, "val_acc": 52.0}
{"epoch": 14, "training_loss": 73.0145993232727, "training_acc": 53.0, "val_loss": 18.028344213962555, "val_acc": 52.0}
{"epoch": 15, "training_loss": 71.25912928581238, "training_acc": 53.0, "val_loss": 18.109774589538574, "val_acc": 52.0}
{"epoch": 16, "training_loss": 71.84422564506531, "training_acc": 53.0, "val_loss": 17.771586775779724, "val_acc": 52.0}
{"epoch": 17, "training_loss": 70.05143332481384, "training_acc": 53.0, "val_loss": 17.345409095287323, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.47198820114136, "training_acc": 47.0, "val_loss": 17.30979084968567, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.61440396308899, "training_acc": 53.0, "val_loss": 17.326845228672028, "val_acc": 52.0}
{"epoch": 20, "training_loss": 70.19899201393127, "training_acc": 47.0, "val_loss": 17.315302789211273, "val_acc": 52.0}
{"epoch": 21, "training_loss": 70.11944508552551, "training_acc": 53.0, "val_loss": 17.451541125774384, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.36575436592102, "training_acc": 53.0, "val_loss": 17.3115536570549, "val_acc": 52.0}
{"epoch": 23, "training_loss": 70.8693540096283, "training_acc": 45.0, "val_loss": 17.32410043478012, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.28322219848633, "training_acc": 53.0, "val_loss": 17.31375902891159, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.49792385101318, "training_acc": 47.0, "val_loss": 17.470665276050568, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.48256349563599, "training_acc": 53.0, "val_loss": 17.322425544261932, "val_acc": 52.0}
{"epoch": 27, "training_loss": 76.10583162307739, "training_acc": 53.0, "val_loss": 17.308680713176727, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.2634346485138, "training_acc": 53.0, "val_loss": 17.30954349040985, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.38402652740479, "training_acc": 48.0, "val_loss": 18.156974017620087, "val_acc": 52.0}
{"epoch": 30, "training_loss": 73.17662572860718, "training_acc": 53.0, "val_loss": 18.754877150058746, "val_acc": 48.0}
{"epoch": 31, "training_loss": 74.70776200294495, "training_acc": 47.0, "val_loss": 17.365561425685883, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.24975967407227, "training_acc": 51.0, "val_loss": 17.319121956825256, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.34194493293762, "training_acc": 51.0, "val_loss": 17.9852157831192, "val_acc": 52.0}
{"epoch": 34, "training_loss": 73.50694584846497, "training_acc": 43.0, "val_loss": 17.31792241334915, "val_acc": 52.0}
{"epoch": 35, "training_loss": 75.54620456695557, "training_acc": 53.0, "val_loss": 17.32761412858963, "val_acc": 52.0}
{"epoch": 36, "training_loss": 70.43120527267456, "training_acc": 46.0, "val_loss": 17.669425904750824, "val_acc": 52.0}
{"epoch": 37, "training_loss": 71.06054067611694, "training_acc": 47.0, "val_loss": 17.547722160816193, "val_acc": 52.0}
{"epoch": 38, "training_loss": 70.0602879524231, "training_acc": 47.0, "val_loss": 17.321237921714783, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.82282328605652, "training_acc": 53.0, "val_loss": 17.523638904094696, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.50319337844849, "training_acc": 53.0, "val_loss": 17.315764725208282, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.58053135871887, "training_acc": 47.0, "val_loss": 17.326751351356506, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.13740968704224, "training_acc": 53.0, "val_loss": 17.4740731716156, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.37717461585999, "training_acc": 53.0, "val_loss": 17.32943058013916, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.96999740600586, "training_acc": 47.0, "val_loss": 17.312079668045044, "val_acc": 52.0}
{"epoch": 45, "training_loss": 68.9731593132019, "training_acc": 53.0, "val_loss": 17.497001588344574, "val_acc": 52.0}
{"epoch": 46, "training_loss": 72.28794813156128, "training_acc": 53.0, "val_loss": 17.415030300617218, "val_acc": 52.0}
