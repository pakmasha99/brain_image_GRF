"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 1701.9735946655273, "training_acc": 45.0, "val_loss": 4.945606308213555e+18, "val_acc": 52.0}
{"epoch": 1, "training_loss": 1.2968049156319238e+19, "training_acc": 47.0, "val_loss": 7733525.78125, "val_acc": 48.0}
{"epoch": 2, "training_loss": 81759294094.0, "training_acc": 59.0, "val_loss": 2789290400.0, "val_acc": 48.0}
{"epoch": 3, "training_loss": 99022524416.0, "training_acc": 47.0, "val_loss": 49179971200.0, "val_acc": 48.0}
{"epoch": 4, "training_loss": 130010851072.0, "training_acc": 53.0, "val_loss": 1015519800.0, "val_acc": 52.0}
{"epoch": 5, "training_loss": 6375569184.0, "training_acc": 53.0, "val_loss": 191666412.5, "val_acc": 48.0}
{"epoch": 6, "training_loss": 1095042224.0, "training_acc": 49.0, "val_loss": 7478188000.0, "val_acc": 48.0}
{"epoch": 7, "training_loss": 21296571626.3125, "training_acc": 47.0, "val_loss": 402657200.0, "val_acc": 52.0}
{"epoch": 8, "training_loss": 1464040132.0, "training_acc": 53.0, "val_loss": 233170800.0, "val_acc": 48.0}
{"epoch": 9, "training_loss": 6972735168.0, "training_acc": 53.0, "val_loss": 296747575.0, "val_acc": 52.0}
{"epoch": 10, "training_loss": 984334402.0, "training_acc": 51.0, "val_loss": 13156164000.0, "val_acc": 52.0}
{"epoch": 11, "training_loss": 32924954536.0, "training_acc": 53.0, "val_loss": 135388412.5, "val_acc": 48.0}
{"epoch": 12, "training_loss": 894836112.0, "training_acc": 47.0, "val_loss": 205561462.5, "val_acc": 48.0}
{"epoch": 13, "training_loss": 715728787.0, "training_acc": 47.0, "val_loss": 121042612.5, "val_acc": 52.0}
{"epoch": 14, "training_loss": 64383631552.0, "training_acc": 59.0, "val_loss": 13286990.625, "val_acc": 52.0}
{"epoch": 15, "training_loss": 238590110.0, "training_acc": 53.0, "val_loss": 141929.0283203125, "val_acc": 52.0}
{"epoch": 16, "training_loss": 36786517.0, "training_acc": 49.0, "val_loss": 17929553.125, "val_acc": 48.0}
{"epoch": 17, "training_loss": 56857312.5, "training_acc": 47.0, "val_loss": 86060000.0, "val_acc": 48.0}
{"epoch": 18, "training_loss": 235647266.0, "training_acc": 51.0, "val_loss": 87227025.0, "val_acc": 48.0}
{"epoch": 19, "training_loss": 447776066.0, "training_acc": 51.0, "val_loss": 70018550.0, "val_acc": 52.0}
{"epoch": 20, "training_loss": 195568915.125, "training_acc": 53.0, "val_loss": 722392.1875, "val_acc": 48.0}
{"epoch": 21, "training_loss": 2570892.1796875, "training_acc": 47.0, "val_loss": 1638260.9375, "val_acc": 48.0}
{"epoch": 22, "training_loss": 5260009.0390625, "training_acc": 47.0, "val_loss": 908721.19140625, "val_acc": 52.0}
{"epoch": 23, "training_loss": 3201819.48828125, "training_acc": 53.0, "val_loss": 496977.490234375, "val_acc": 48.0}
{"epoch": 24, "training_loss": 1250797.6000976562, "training_acc": 53.0, "val_loss": 211707.2509765625, "val_acc": 48.0}
{"epoch": 25, "training_loss": 1264550.7265625, "training_acc": 53.0, "val_loss": 314583.6181640625, "val_acc": 52.0}
{"epoch": 26, "training_loss": 1532191.328125, "training_acc": 53.0, "val_loss": 272730.2734375, "val_acc": 48.0}
{"epoch": 27, "training_loss": 2312044.984375, "training_acc": 43.0, "val_loss": 539714.990234375, "val_acc": 52.0}
{"epoch": 28, "training_loss": 1176417.7490234375, "training_acc": 59.0, "val_loss": 313777.490234375, "val_acc": 48.0}
{"epoch": 29, "training_loss": 1545668.5, "training_acc": 47.0, "val_loss": 289680.517578125, "val_acc": 52.0}
{"epoch": 30, "training_loss": 852286.34765625, "training_acc": 52.0, "val_loss": 50084.09118652344, "val_acc": 48.0}
{"epoch": 31, "training_loss": 501982.9609375, "training_acc": 51.0, "val_loss": 197677.67333984375, "val_acc": 52.0}
{"epoch": 32, "training_loss": 705570.052734375, "training_acc": 51.0, "val_loss": 155391.27197265625, "val_acc": 48.0}
{"epoch": 33, "training_loss": 516354.70703125, "training_acc": 51.0, "val_loss": 46739.80712890625, "val_acc": 52.0}
{"epoch": 34, "training_loss": 372919.921875, "training_acc": 47.0, "val_loss": 124456.31103515625, "val_acc": 48.0}
{"epoch": 35, "training_loss": 363352.6083984375, "training_acc": 51.0, "val_loss": 57034.442138671875, "val_acc": 52.0}
{"epoch": 36, "training_loss": 243384.39453125, "training_acc": 47.0, "val_loss": 12887.498474121094, "val_acc": 52.0}
{"epoch": 37, "training_loss": 62781.722900390625, "training_acc": 59.0, "val_loss": 28924.664306640625, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69959.5400390625, "training_acc": 55.0, "val_loss": 60453.155517578125, "val_acc": 48.0}
{"epoch": 39, "training_loss": 195974.8271484375, "training_acc": 47.0, "val_loss": 76113.04321289062, "val_acc": 52.0}
{"epoch": 40, "training_loss": 333482.25, "training_acc": 53.0, "val_loss": 14910.786437988281, "val_acc": 52.0}
{"epoch": 41, "training_loss": 651411.4921875, "training_acc": 48.0, "val_loss": 85255.9326171875, "val_acc": 48.0}
{"epoch": 42, "training_loss": 260180.69775390625, "training_acc": 47.0, "val_loss": 73874.24926757812, "val_acc": 52.0}
{"epoch": 43, "training_loss": 293498.927734375, "training_acc": 53.0, "val_loss": 43626.104736328125, "val_acc": 52.0}
{"epoch": 44, "training_loss": 191874.591796875, "training_acc": 45.0, "val_loss": 56399.9755859375, "val_acc": 48.0}
{"epoch": 45, "training_loss": 152337.380859375, "training_acc": 46.0, "val_loss": 79311.21215820312, "val_acc": 52.0}
{"epoch": 46, "training_loss": 323332.4091796875, "training_acc": 53.0, "val_loss": 64698.480224609375, "val_acc": 52.0}
{"epoch": 47, "training_loss": 175416.099609375, "training_acc": 51.0, "val_loss": 65567.37060546875, "val_acc": 48.0}
{"epoch": 48, "training_loss": 286263.34375, "training_acc": 47.0, "val_loss": 60027.484130859375, "val_acc": 48.0}
{"epoch": 49, "training_loss": 167711.3204345703, "training_acc": 48.0, "val_loss": 70159.94262695312, "val_acc": 52.0}
{"epoch": 50, "training_loss": 285937.361328125, "training_acc": 53.0, "val_loss": 297697650.0, "val_acc": 48.0}
{"epoch": 51, "training_loss": 802822679.75, "training_acc": 47.0, "val_loss": 357583950.0, "val_acc": 52.0}
{"epoch": 52, "training_loss": 1063412541.0, "training_acc": 53.0, "val_loss": 1362096435200.0, "val_acc": 52.0}
{"epoch": 53, "training_loss": 3146420297297.375, "training_acc": 53.0, "val_loss": 997738000.0, "val_acc": 52.0}
{"epoch": 54, "training_loss": 6384099264.0, "training_acc": 61.0, "val_loss": 1443026400.0, "val_acc": 48.0}
{"epoch": 55, "training_loss": 69923722240.0, "training_acc": 47.0, "val_loss": 91019302400.0, "val_acc": 52.0}
