"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 1880.5552597045898, "training_acc": 53.0, "val_loss": 3.5925898211727853e+23, "val_acc": 48.0}
{"epoch": 1, "training_loss": 8.744496870846485e+23, "training_acc": 47.0, "val_loss": 157435297792000.0, "val_acc": 52.0}
{"epoch": 2, "training_loss": 4.024011559901594e+16, "training_acc": 53.0, "val_loss": 384179057459200.0, "val_acc": 52.0}
{"epoch": 3, "training_loss": 1066269703667712.0, "training_acc": 45.0, "val_loss": 1171327488000.0, "val_acc": 48.0}
{"epoch": 4, "training_loss": 3237066334208.0, "training_acc": 47.0, "val_loss": 56715436800.0, "val_acc": 48.0}
{"epoch": 5, "training_loss": 148117724880.0, "training_acc": 49.0, "val_loss": 2509642000.0, "val_acc": 52.0}
{"epoch": 6, "training_loss": 3695406190058496.0, "training_acc": 42.0, "val_loss": 46896231219200.0, "val_acc": 52.0}
{"epoch": 7, "training_loss": 377320974057472.0, "training_acc": 53.0, "val_loss": 991633408000.0, "val_acc": 52.0}
{"epoch": 8, "training_loss": 27284150157312.0, "training_acc": 53.0, "val_loss": 15690778214400.0, "val_acc": 48.0}
{"epoch": 9, "training_loss": 300550574833664.0, "training_acc": 47.0, "val_loss": 560730828800.0, "val_acc": 48.0}
{"epoch": 10, "training_loss": 19392539525120.0, "training_acc": 47.0, "val_loss": 447747430400.0, "val_acc": 48.0}
{"epoch": 11, "training_loss": 2373473566720.0, "training_acc": 47.0, "val_loss": 9581472153600.0, "val_acc": 52.0}
{"epoch": 12, "training_loss": 32708166090752.0, "training_acc": 53.0, "val_loss": 242157081600.0, "val_acc": 52.0}
{"epoch": 13, "training_loss": 738440814592.0, "training_acc": 53.0, "val_loss": 16594945600.0, "val_acc": 48.0}
{"epoch": 14, "training_loss": 125667853312.0, "training_acc": 47.0, "val_loss": 148516505600.0, "val_acc": 48.0}
{"epoch": 15, "training_loss": 458850612224.0, "training_acc": 53.0, "val_loss": 196789196800.0, "val_acc": 48.0}
{"epoch": 16, "training_loss": 712804558848.0, "training_acc": 45.0, "val_loss": 131007296000.0, "val_acc": 48.0}
{"epoch": 17, "training_loss": 751014297600.0, "training_acc": 43.0, "val_loss": 77924787200.0, "val_acc": 52.0}
{"epoch": 18, "training_loss": 200321468224.0, "training_acc": 53.0, "val_loss": 345248102400.0, "val_acc": 48.0}
{"epoch": 19, "training_loss": 796393025792.0, "training_acc": 47.0, "val_loss": 12986214400.0, "val_acc": 52.0}
{"epoch": 20, "training_loss": 60974345728.0, "training_acc": 53.0, "val_loss": 17755585600.0, "val_acc": 52.0}
{"epoch": 21, "training_loss": 55816262144.0, "training_acc": 53.0, "val_loss": 6597831600.0, "val_acc": 52.0}
{"epoch": 22, "training_loss": 25469418112.0, "training_acc": 53.0, "val_loss": 3577699600.0, "val_acc": 52.0}
{"epoch": 23, "training_loss": 25622253824.0, "training_acc": 53.0, "val_loss": 4556092000.0, "val_acc": 48.0}
{"epoch": 24, "training_loss": 13709854720.0, "training_acc": 53.0, "val_loss": 1686589200.0, "val_acc": 52.0}
{"epoch": 25, "training_loss": 7369363008.0, "training_acc": 41.0, "val_loss": 6000349200.0, "val_acc": 52.0}
{"epoch": 26, "training_loss": 17105598864.0, "training_acc": 53.0, "val_loss": 400062275.0, "val_acc": 52.0}
{"epoch": 27, "training_loss": 1538430036.0, "training_acc": 47.0, "val_loss": 92309107200.0, "val_acc": 52.0}
{"epoch": 28, "training_loss": 3610722140160.0, "training_acc": 53.0, "val_loss": 4722965200.0, "val_acc": 52.0}
{"epoch": 29, "training_loss": 36414140672.0, "training_acc": 53.0, "val_loss": 15901704000.0, "val_acc": 52.0}
{"epoch": 30, "training_loss": 59506664192.0, "training_acc": 53.0, "val_loss": 8905498400.0, "val_acc": 52.0}
{"epoch": 31, "training_loss": 33629387008.0, "training_acc": 53.0, "val_loss": 6479730400.0, "val_acc": 52.0}
{"epoch": 32, "training_loss": 24822789504.0, "training_acc": 53.0, "val_loss": 1815662000.0, "val_acc": 52.0}
{"epoch": 33, "training_loss": 5372122024.0, "training_acc": 53.0, "val_loss": 861923800.0, "val_acc": 48.0}
{"epoch": 34, "training_loss": 10892451904.0, "training_acc": 47.0, "val_loss": 3327394800.0, "val_acc": 52.0}
{"epoch": 35, "training_loss": 12374782912.0, "training_acc": 51.0, "val_loss": 2615456800.0, "val_acc": 48.0}
{"epoch": 36, "training_loss": 6609182730.0, "training_acc": 53.0, "val_loss": 744028350.0, "val_acc": 48.0}
{"epoch": 37, "training_loss": 2479067636.0, "training_acc": 49.0, "val_loss": 83244256.25, "val_acc": 48.0}
{"epoch": 38, "training_loss": 814507960.0, "training_acc": 51.0, "val_loss": 169138187.5, "val_acc": 52.0}
{"epoch": 39, "training_loss": 697321264.0, "training_acc": 59.0, "val_loss": 204426600.0, "val_acc": 48.0}
{"epoch": 40, "training_loss": 679590529.0, "training_acc": 49.0, "val_loss": 110862287.5, "val_acc": 52.0}
{"epoch": 41, "training_loss": 349710497.5, "training_acc": 50.0, "val_loss": 88781918.75, "val_acc": 52.0}
{"epoch": 42, "training_loss": 1137274168.0, "training_acc": 53.0, "val_loss": 76362547200.0, "val_acc": 52.0}
{"epoch": 43, "training_loss": 244642961408.0, "training_acc": 51.0, "val_loss": 34283225600.0, "val_acc": 52.0}
{"epoch": 44, "training_loss": 711508680704.0, "training_acc": 45.0, "val_loss": 1587810099200.0, "val_acc": 52.0}
{"epoch": 45, "training_loss": 5047650680832.0, "training_acc": 53.0, "val_loss": 3649767014400.0, "val_acc": 52.0}
{"epoch": 46, "training_loss": 9247584718848.0, "training_acc": 51.0, "val_loss": 1020919700.0, "val_acc": 48.0}
{"epoch": 47, "training_loss": 7331809920.0, "training_acc": 53.0, "val_loss": 2400242200.0, "val_acc": 52.0}
{"epoch": 48, "training_loss": 32232472832.0, "training_acc": 49.0, "val_loss": 77869555200.0, "val_acc": 48.0}
{"epoch": 49, "training_loss": 28571920613376.0, "training_acc": 47.0, "val_loss": 189818496000.0, "val_acc": 52.0}
{"epoch": 50, "training_loss": 704601040896.0, "training_acc": 53.0, "val_loss": 3886368768000.0, "val_acc": 48.0}
{"epoch": 51, "training_loss": 12974125187072.0, "training_acc": 45.0, "val_loss": 569220249600.0, "val_acc": 52.0}
{"epoch": 52, "training_loss": 1743490002944.0, "training_acc": 53.0, "val_loss": 319728307200.0, "val_acc": 52.0}
{"epoch": 53, "training_loss": 786349412736.0, "training_acc": 53.0, "val_loss": 10894812000.0, "val_acc": 48.0}
{"epoch": 54, "training_loss": 1366877605115904.0, "training_acc": 51.0, "val_loss": 160864256000.0, "val_acc": 48.0}
{"epoch": 55, "training_loss": 2574067728384.0, "training_acc": 47.0, "val_loss": 49199192473600.0, "val_acc": 48.0}
{"epoch": 56, "training_loss": 112163087515648.0, "training_acc": 57.0, "val_loss": 107045748736000.0, "val_acc": 48.0}
