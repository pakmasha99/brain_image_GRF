"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.42716526985168, "training_acc": 45.0, "val_loss": 18.552474677562714, "val_acc": 52.0}
{"epoch": 1, "training_loss": 175.99358081817627, "training_acc": 49.0, "val_loss": 17.493434250354767, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.73375296592712, "training_acc": 51.0, "val_loss": 18.05635839700699, "val_acc": 52.0}
{"epoch": 3, "training_loss": 71.102135181427, "training_acc": 53.0, "val_loss": 17.329299449920654, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.20411038398743, "training_acc": 53.0, "val_loss": 17.319568991661072, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.21248936653137, "training_acc": 53.0, "val_loss": 17.38792359828949, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.74200701713562, "training_acc": 53.0, "val_loss": 17.409110069274902, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.75367879867554, "training_acc": 47.0, "val_loss": 17.440204322338104, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.6939651966095, "training_acc": 47.0, "val_loss": 17.30848252773285, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.21666049957275, "training_acc": 53.0, "val_loss": 17.40800440311432, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.5512125492096, "training_acc": 53.0, "val_loss": 17.30942726135254, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.47620129585266, "training_acc": 53.0, "val_loss": 17.305833101272583, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.14845585823059, "training_acc": 53.0, "val_loss": 17.33928620815277, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.35242915153503, "training_acc": 53.0, "val_loss": 17.335456609725952, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.12963223457336, "training_acc": 53.0, "val_loss": 17.305947840213776, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.16032385826111, "training_acc": 53.0, "val_loss": 17.315423488616943, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.2938585281372, "training_acc": 53.0, "val_loss": 17.31562167406082, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.30878233909607, "training_acc": 53.0, "val_loss": 17.30724722146988, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.14214015007019, "training_acc": 53.0, "val_loss": 17.314431071281433, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.31698274612427, "training_acc": 53.0, "val_loss": 17.31618046760559, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.36585235595703, "training_acc": 53.0, "val_loss": 17.340537905693054, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.17951393127441, "training_acc": 53.0, "val_loss": 17.322225868701935, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.29742550849915, "training_acc": 53.0, "val_loss": 17.312762141227722, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.13639783859253, "training_acc": 53.0, "val_loss": 17.31533408164978, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.12767720222473, "training_acc": 53.0, "val_loss": 17.317581176757812, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.14561581611633, "training_acc": 53.0, "val_loss": 17.31768399477005, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.14419484138489, "training_acc": 53.0, "val_loss": 17.313210666179657, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.17916798591614, "training_acc": 53.0, "val_loss": 17.31206625699997, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.1466646194458, "training_acc": 53.0, "val_loss": 17.31596142053604, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.1371738910675, "training_acc": 53.0, "val_loss": 17.316627502441406, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.13438415527344, "training_acc": 53.0, "val_loss": 17.31385737657547, "val_acc": 52.0}
