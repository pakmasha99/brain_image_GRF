"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 75.63808560371399, "training_acc": 51.0, "val_loss": 17.90292114019394, "val_acc": 52.0}
{"epoch": 1, "training_loss": 90.71551609039307, "training_acc": 59.0, "val_loss": 17.435340583324432, "val_acc": 52.0}
{"epoch": 2, "training_loss": 68.58611917495728, "training_acc": 57.0, "val_loss": 18.36235821247101, "val_acc": 52.0}
{"epoch": 3, "training_loss": 71.50932621955872, "training_acc": 53.0, "val_loss": 17.326368391513824, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.14648103713989, "training_acc": 53.0, "val_loss": 17.325186729431152, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.95316314697266, "training_acc": 53.0, "val_loss": 17.32259839773178, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.40811419487, "training_acc": 49.0, "val_loss": 17.36532896757126, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.39209747314453, "training_acc": 48.0, "val_loss": 17.319202423095703, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.28801608085632, "training_acc": 53.0, "val_loss": 17.392121255397797, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.34621953964233, "training_acc": 53.0, "val_loss": 17.34471023082733, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.20191311836243, "training_acc": 53.0, "val_loss": 17.333538830280304, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.17163109779358, "training_acc": 53.0, "val_loss": 17.34061986207962, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.13704538345337, "training_acc": 53.0, "val_loss": 17.3258975148201, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.11822891235352, "training_acc": 53.0, "val_loss": 17.323054373264313, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.18753957748413, "training_acc": 53.0, "val_loss": 17.32332706451416, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.18646597862244, "training_acc": 53.0, "val_loss": 17.322097718715668, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.12825417518616, "training_acc": 53.0, "val_loss": 17.349955439567566, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.31374049186707, "training_acc": 53.0, "val_loss": 17.357921600341797, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.38286256790161, "training_acc": 53.0, "val_loss": 17.383454740047455, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.23633670806885, "training_acc": 53.0, "val_loss": 17.46293604373932, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.52690553665161, "training_acc": 53.0, "val_loss": 17.406585812568665, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.45780682563782, "training_acc": 53.0, "val_loss": 17.32850968837738, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.1030170917511, "training_acc": 53.0, "val_loss": 17.318159341812134, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.21809816360474, "training_acc": 53.0, "val_loss": 17.31584072113037, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.16708421707153, "training_acc": 53.0, "val_loss": 17.314253747463226, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.13228416442871, "training_acc": 53.0, "val_loss": 17.31673777103424, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.11648654937744, "training_acc": 53.0, "val_loss": 17.325416207313538, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.20768690109253, "training_acc": 53.0, "val_loss": 17.345260083675385, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.15825772285461, "training_acc": 53.0, "val_loss": 17.341163754463196, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.17531561851501, "training_acc": 53.0, "val_loss": 17.333121597766876, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.14211440086365, "training_acc": 53.0, "val_loss": 17.32986867427826, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.18306875228882, "training_acc": 53.0, "val_loss": 17.32361614704132, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.13644337654114, "training_acc": 53.0, "val_loss": 17.326076328754425, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.12110495567322, "training_acc": 53.0, "val_loss": 17.32325702905655, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.13133978843689, "training_acc": 53.0, "val_loss": 17.319975793361664, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.12476801872253, "training_acc": 53.0, "val_loss": 17.31909215450287, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.1233491897583, "training_acc": 53.0, "val_loss": 17.317314445972443, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.10241341590881, "training_acc": 53.0, "val_loss": 17.314685881137848, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.16173267364502, "training_acc": 53.0, "val_loss": 17.31446385383606, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.13429951667786, "training_acc": 53.0, "val_loss": 17.315341532230377, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.11335587501526, "training_acc": 53.0, "val_loss": 17.317311465740204, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.1132652759552, "training_acc": 53.0, "val_loss": 17.320486903190613, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.11239957809448, "training_acc": 53.0, "val_loss": 17.323951423168182, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.12919425964355, "training_acc": 53.0, "val_loss": 17.323824763298035, "val_acc": 52.0}
