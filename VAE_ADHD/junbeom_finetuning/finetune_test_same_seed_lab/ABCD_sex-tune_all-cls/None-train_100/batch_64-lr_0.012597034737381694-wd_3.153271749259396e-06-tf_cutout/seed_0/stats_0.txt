"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 735.9145812988281, "training_acc": 50.0, "val_loss": 7605518336000.0, "val_acc": 56.0}
{"epoch": 1, "training_loss": 20297136178772.734, "training_acc": 52.0, "val_loss": 5822950.78125, "val_acc": 44.0}
{"epoch": 2, "training_loss": 25399137.375, "training_acc": 46.0, "val_loss": 2766241.6015625, "val_acc": 44.0}
{"epoch": 3, "training_loss": 7365154.75, "training_acc": 48.0, "val_loss": 1650526.171875, "val_acc": 44.0}
{"epoch": 4, "training_loss": 3903198.203125, "training_acc": 48.0, "val_loss": 5481.974411010742, "val_acc": 56.0}
{"epoch": 5, "training_loss": 268622.466796875, "training_acc": 56.0, "val_loss": 1047.4879264831543, "val_acc": 56.0}
{"epoch": 6, "training_loss": 872968.7763671875, "training_acc": 50.0, "val_loss": 30904.791259765625, "val_acc": 44.0}
{"epoch": 7, "training_loss": 1217695.1015625, "training_acc": 52.0, "val_loss": 19152.30255126953, "val_acc": 56.0}
{"epoch": 8, "training_loss": 181707.8173828125, "training_acc": 58.0, "val_loss": 71066.9677734375, "val_acc": 44.0}
{"epoch": 9, "training_loss": 222232.54443359375, "training_acc": 48.0, "val_loss": 95531.45141601562, "val_acc": 44.0}
{"epoch": 10, "training_loss": 329548.4091796875, "training_acc": 50.0, "val_loss": 2455062.890625, "val_acc": 44.0}
{"epoch": 11, "training_loss": 6238609.02734375, "training_acc": 48.0, "val_loss": 47529.75158691406, "val_acc": 44.0}
{"epoch": 12, "training_loss": 212995.861328125, "training_acc": 46.0, "val_loss": 162892.98095703125, "val_acc": 44.0}
{"epoch": 13, "training_loss": 454090.4033203125, "training_acc": 48.0, "val_loss": 146582.09228515625, "val_acc": 56.0}
{"epoch": 14, "training_loss": 463979.02783203125, "training_acc": 52.0, "val_loss": 388317.0654296875, "val_acc": 44.0}
{"epoch": 15, "training_loss": 902538.42578125, "training_acc": 54.0, "val_loss": 54471.954345703125, "val_acc": 56.0}
{"epoch": 16, "training_loss": 182639.85180664062, "training_acc": 52.0, "val_loss": 30391.464233398438, "val_acc": 44.0}
{"epoch": 17, "training_loss": 101628.88403320312, "training_acc": 48.0, "val_loss": 7863.001251220703, "val_acc": 56.0}
{"epoch": 18, "training_loss": 39247.78332519531, "training_acc": 50.0, "val_loss": 3436.441421508789, "val_acc": 56.0}
{"epoch": 19, "training_loss": 12404.297271728516, "training_acc": 52.0, "val_loss": 288.9073133468628, "val_acc": 44.0}
{"epoch": 20, "training_loss": 943.9667148590088, "training_acc": 48.0, "val_loss": 1109.7612380981445, "val_acc": 56.0}
{"epoch": 21, "training_loss": 4550.276901245117, "training_acc": 52.0, "val_loss": 53.293782472610474, "val_acc": 44.0}
{"epoch": 22, "training_loss": 431.0214424133301, "training_acc": 51.0, "val_loss": 263.3754253387451, "val_acc": 56.0}
{"epoch": 23, "training_loss": 742.3749241828918, "training_acc": 51.0, "val_loss": 845.3098297119141, "val_acc": 44.0}
{"epoch": 24, "training_loss": 2845.0429458618164, "training_acc": 48.0, "val_loss": 75.28942823410034, "val_acc": 56.0}
{"epoch": 25, "training_loss": 360.8080654144287, "training_acc": 52.0, "val_loss": 304.9344778060913, "val_acc": 44.0}
{"epoch": 26, "training_loss": 919.9107689857483, "training_acc": 48.0, "val_loss": 304.4736862182617, "val_acc": 56.0}
{"epoch": 27, "training_loss": 1371.9231567382812, "training_acc": 52.0, "val_loss": 17.181777954101562, "val_acc": 60.0}
{"epoch": 28, "training_loss": 745.3672637939453, "training_acc": 46.0, "val_loss": 392.06855297088623, "val_acc": 44.0}
{"epoch": 29, "training_loss": 947.4336857795715, "training_acc": 51.0, "val_loss": 171.94494009017944, "val_acc": 56.0}
{"epoch": 30, "training_loss": 617.6885566711426, "training_acc": 52.0, "val_loss": 338.90063762664795, "val_acc": 44.0}
{"epoch": 31, "training_loss": 1453.5373916625977, "training_acc": 48.0, "val_loss": 174.5747208595276, "val_acc": 44.0}
{"epoch": 32, "training_loss": 860.9240837097168, "training_acc": 48.0, "val_loss": 408.37836265563965, "val_acc": 56.0}
{"epoch": 33, "training_loss": 1685.5031661987305, "training_acc": 52.0, "val_loss": 141.8916940689087, "val_acc": 56.0}
{"epoch": 34, "training_loss": 676.4251594543457, "training_acc": 54.0, "val_loss": 421.3531494140625, "val_acc": 44.0}
{"epoch": 35, "training_loss": 1445.183250427246, "training_acc": 48.0, "val_loss": 68.89786124229431, "val_acc": 44.0}
{"epoch": 36, "training_loss": 540.4264335632324, "training_acc": 52.0, "val_loss": 424.7224807739258, "val_acc": 56.0}
{"epoch": 37, "training_loss": 1829.1915130615234, "training_acc": 52.0, "val_loss": 319.585919380188, "val_acc": 56.0}
{"epoch": 38, "training_loss": 1112.2462282180786, "training_acc": 52.0, "val_loss": 228.5912036895752, "val_acc": 44.0}
{"epoch": 39, "training_loss": 984.6702117919922, "training_acc": 48.0, "val_loss": 355.31139373779297, "val_acc": 44.0}
{"epoch": 40, "training_loss": 1088.5931491851807, "training_acc": 48.0, "val_loss": 55.26483654975891, "val_acc": 56.0}
{"epoch": 41, "training_loss": 389.1920528411865, "training_acc": 52.0, "val_loss": 131.190025806427, "val_acc": 56.0}
{"epoch": 42, "training_loss": 400.70160007476807, "training_acc": 52.0, "val_loss": 188.48828077316284, "val_acc": 44.0}
{"epoch": 43, "training_loss": 738.7987632751465, "training_acc": 48.0, "val_loss": 207.16521739959717, "val_acc": 44.0}
{"epoch": 44, "training_loss": 548.1780347824097, "training_acc": 48.0, "val_loss": 146.39137983322144, "val_acc": 56.0}
{"epoch": 45, "training_loss": 751.094482421875, "training_acc": 52.0, "val_loss": 116.52514934539795, "val_acc": 56.0}
{"epoch": 46, "training_loss": 635.6937637329102, "training_acc": 52.0, "val_loss": 40.20660221576691, "val_acc": 44.0}
