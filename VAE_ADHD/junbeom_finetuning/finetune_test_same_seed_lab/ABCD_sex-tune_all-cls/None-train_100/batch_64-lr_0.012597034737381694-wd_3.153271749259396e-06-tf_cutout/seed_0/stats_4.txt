"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 94.15500259399414, "training_acc": 51.0, "val_loss": 5.72863854631977e+18, "val_acc": 52.0}
{"epoch": 1, "training_loss": 1.5513447161919717e+19, "training_acc": 53.0, "val_loss": 127420.751953125, "val_acc": 52.0}
{"epoch": 2, "training_loss": 1210670.4609375, "training_acc": 55.0, "val_loss": 431277.001953125, "val_acc": 48.0}
{"epoch": 3, "training_loss": 1082415.6247558594, "training_acc": 53.0, "val_loss": 284662.3291015625, "val_acc": 48.0}
{"epoch": 4, "training_loss": 748613.9060058594, "training_acc": 47.0, "val_loss": 49246.875, "val_acc": 48.0}
{"epoch": 5, "training_loss": 171544.36962890625, "training_acc": 51.0, "val_loss": 106069.64111328125, "val_acc": 48.0}
{"epoch": 6, "training_loss": 291933.6428222656, "training_acc": 49.0, "val_loss": 11559.407806396484, "val_acc": 48.0}
{"epoch": 7, "training_loss": 30522.3603515625, "training_acc": 49.0, "val_loss": 101535.69946289062, "val_acc": 48.0}
{"epoch": 8, "training_loss": 358230.0654296875, "training_acc": 47.0, "val_loss": 35209.735107421875, "val_acc": 52.0}
{"epoch": 9, "training_loss": 1035039.1640625, "training_acc": 53.0, "val_loss": 1850849.4140625, "val_acc": 52.0}
{"epoch": 10, "training_loss": 5192457.58984375, "training_acc": 53.0, "val_loss": 8362.191009521484, "val_acc": 52.0}
{"epoch": 11, "training_loss": 203548.525390625, "training_acc": 51.0, "val_loss": 36942.2607421875, "val_acc": 48.0}
{"epoch": 12, "training_loss": 146816.8037109375, "training_acc": 47.0, "val_loss": 23740.858459472656, "val_acc": 52.0}
{"epoch": 13, "training_loss": 6807650.50390625, "training_acc": 51.0, "val_loss": 75965.52124023438, "val_acc": 52.0}
{"epoch": 14, "training_loss": 277663.03125, "training_acc": 53.0, "val_loss": 100750.48217773438, "val_acc": 48.0}
{"epoch": 15, "training_loss": 8598808.875, "training_acc": 45.0, "val_loss": 863486.5234375, "val_acc": 52.0}
{"epoch": 16, "training_loss": 2961163.7421875, "training_acc": 49.0, "val_loss": 547981.494140625, "val_acc": 48.0}
{"epoch": 17, "training_loss": 1625453.140625, "training_acc": 47.0, "val_loss": 23132.80792236328, "val_acc": 48.0}
{"epoch": 18, "training_loss": 81715.78369140625, "training_acc": 47.0, "val_loss": 4123.649215698242, "val_acc": 52.0}
{"epoch": 19, "training_loss": 54639.953125, "training_acc": 51.0, "val_loss": 17919.667053222656, "val_acc": 48.0}
{"epoch": 20, "training_loss": 48034.21474456787, "training_acc": 52.0, "val_loss": 244247.3388671875, "val_acc": 48.0}
{"epoch": 21, "training_loss": 686549.955078125, "training_acc": 49.0, "val_loss": 35106.396484375, "val_acc": 52.0}
{"epoch": 22, "training_loss": 91900.94189453125, "training_acc": 49.0, "val_loss": 16133.729553222656, "val_acc": 52.0}
{"epoch": 23, "training_loss": 42870.06771850586, "training_acc": 53.0, "val_loss": 39621.42028808594, "val_acc": 48.0}
{"epoch": 24, "training_loss": 142215.98583984375, "training_acc": 47.0, "val_loss": 4322.837448120117, "val_acc": 48.0}
{"epoch": 25, "training_loss": 39610.4248046875, "training_acc": 49.0, "val_loss": 21027.08282470703, "val_acc": 52.0}
{"epoch": 26, "training_loss": 60519.19128417969, "training_acc": 53.0, "val_loss": 28771.859741210938, "val_acc": 48.0}
{"epoch": 27, "training_loss": 104574.849609375, "training_acc": 47.0, "val_loss": 6285.898208618164, "val_acc": 48.0}
{"epoch": 28, "training_loss": 31752.650390625, "training_acc": 45.0, "val_loss": 4756.031036376953, "val_acc": 52.0}
{"epoch": 29, "training_loss": 40140.50927734375, "training_acc": 45.0, "val_loss": 4614.837265014648, "val_acc": 48.0}
{"epoch": 30, "training_loss": 30064.569702148438, "training_acc": 51.0, "val_loss": 13515.611267089844, "val_acc": 52.0}
{"epoch": 31, "training_loss": 39757.494201660156, "training_acc": 49.0, "val_loss": 4577.761459350586, "val_acc": 52.0}
{"epoch": 32, "training_loss": 25612.765747070312, "training_acc": 41.0, "val_loss": 97.78779149055481, "val_acc": 48.0}
{"epoch": 33, "training_loss": 11693.988525390625, "training_acc": 54.0, "val_loss": 9316.605377197266, "val_acc": 52.0}
{"epoch": 34, "training_loss": 29157.85076904297, "training_acc": 53.0, "val_loss": 2644.3309783935547, "val_acc": 48.0}
{"epoch": 35, "training_loss": 8331.104522705078, "training_acc": 47.0, "val_loss": 4883.123016357422, "val_acc": 52.0}
{"epoch": 36, "training_loss": 55570.6240234375, "training_acc": 51.0, "val_loss": 2179.375648498535, "val_acc": 52.0}
{"epoch": 37, "training_loss": 9193.800689697266, "training_acc": 53.0, "val_loss": 3131.894302368164, "val_acc": 48.0}
{"epoch": 38, "training_loss": 7822.740795135498, "training_acc": 54.0, "val_loss": 1510.082721710205, "val_acc": 48.0}
{"epoch": 39, "training_loss": 4642.334568023682, "training_acc": 44.0, "val_loss": 1082.7579498291016, "val_acc": 48.0}
{"epoch": 40, "training_loss": 2944.090196609497, "training_acc": 51.0, "val_loss": 322.89135456085205, "val_acc": 48.0}
{"epoch": 41, "training_loss": 2572.8789825439453, "training_acc": 51.0, "val_loss": 998.2970237731934, "val_acc": 52.0}
{"epoch": 42, "training_loss": 3939.663101196289, "training_acc": 49.0, "val_loss": 783.2321643829346, "val_acc": 48.0}
{"epoch": 43, "training_loss": 2611.0701904296875, "training_acc": 53.0, "val_loss": 194.60906982421875, "val_acc": 52.0}
{"epoch": 44, "training_loss": 10777.983459472656, "training_acc": 48.0, "val_loss": 1241.028881072998, "val_acc": 48.0}
{"epoch": 45, "training_loss": 4704.93586730957, "training_acc": 47.0, "val_loss": 641.9589519500732, "val_acc": 52.0}
{"epoch": 46, "training_loss": 3605.0044555664062, "training_acc": 43.0, "val_loss": 851.6474723815918, "val_acc": 48.0}
{"epoch": 47, "training_loss": 2536.9658164978027, "training_acc": 55.0, "val_loss": 664.861011505127, "val_acc": 52.0}
{"epoch": 48, "training_loss": 2036.0489501953125, "training_acc": 51.0, "val_loss": 128.76112461090088, "val_acc": 52.0}
{"epoch": 49, "training_loss": 1258.8211975097656, "training_acc": 46.0, "val_loss": 176.13143920898438, "val_acc": 52.0}
{"epoch": 50, "training_loss": 765.8211040496826, "training_acc": 51.0, "val_loss": 589.5797729492188, "val_acc": 52.0}
{"epoch": 51, "training_loss": 1867.5098762512207, "training_acc": 53.0, "val_loss": 955.467700958252, "val_acc": 48.0}
