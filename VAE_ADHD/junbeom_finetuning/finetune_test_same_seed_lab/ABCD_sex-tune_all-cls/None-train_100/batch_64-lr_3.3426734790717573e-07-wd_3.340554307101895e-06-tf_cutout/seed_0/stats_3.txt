"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 70.65615463256836, "training_acc": 47.0, "val_loss": 17.328768968582153, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.36083626747131, "training_acc": 48.0, "val_loss": 17.325782775878906, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.31246876716614, "training_acc": 52.0, "val_loss": 17.319461703300476, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.22447657585144, "training_acc": 51.0, "val_loss": 17.313475906848907, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.33256483078003, "training_acc": 48.0, "val_loss": 17.30731576681137, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.30110287666321, "training_acc": 52.0, "val_loss": 17.302948236465454, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.2386314868927, "training_acc": 53.0, "val_loss": 17.300425469875336, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.25936818122864, "training_acc": 53.0, "val_loss": 17.29806512594223, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.20753359794617, "training_acc": 53.0, "val_loss": 17.29547828435898, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.23852968215942, "training_acc": 53.0, "val_loss": 17.294423282146454, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.19278311729431, "training_acc": 53.0, "val_loss": 17.293402552604675, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.27094841003418, "training_acc": 53.0, "val_loss": 17.292460799217224, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.21252942085266, "training_acc": 53.0, "val_loss": 17.291511595249176, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.17218041419983, "training_acc": 53.0, "val_loss": 17.290203273296356, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.22529578208923, "training_acc": 53.0, "val_loss": 17.28990077972412, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.18149256706238, "training_acc": 53.0, "val_loss": 17.289182543754578, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.23178887367249, "training_acc": 53.0, "val_loss": 17.289000749588013, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.17867708206177, "training_acc": 53.0, "val_loss": 17.288851737976074, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.18061661720276, "training_acc": 53.0, "val_loss": 17.288589477539062, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.16783118247986, "training_acc": 53.0, "val_loss": 17.28871762752533, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.14396977424622, "training_acc": 53.0, "val_loss": 17.2884464263916, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.16510391235352, "training_acc": 53.0, "val_loss": 17.288190126419067, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.18700075149536, "training_acc": 53.0, "val_loss": 17.28847026824951, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.17279839515686, "training_acc": 53.0, "val_loss": 17.288441956043243, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.15707039833069, "training_acc": 53.0, "val_loss": 17.288261651992798, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.16185331344604, "training_acc": 53.0, "val_loss": 17.288531363010406, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.1356406211853, "training_acc": 53.0, "val_loss": 17.288650572299957, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.23113369941711, "training_acc": 53.0, "val_loss": 17.288951575756073, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.14284110069275, "training_acc": 53.0, "val_loss": 17.288967967033386, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.17393231391907, "training_acc": 53.0, "val_loss": 17.289163172245026, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.18909883499146, "training_acc": 53.0, "val_loss": 17.289267480373383, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.12766623497009, "training_acc": 53.0, "val_loss": 17.289386689662933, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.20766735076904, "training_acc": 53.0, "val_loss": 17.289476096630096, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.1824414730072, "training_acc": 53.0, "val_loss": 17.28951632976532, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.16234135627747, "training_acc": 53.0, "val_loss": 17.289720475673676, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.09482145309448, "training_acc": 53.0, "val_loss": 17.289742827415466, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.15005111694336, "training_acc": 53.0, "val_loss": 17.289750277996063, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.14796185493469, "training_acc": 53.0, "val_loss": 17.289999127388, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.08724403381348, "training_acc": 53.0, "val_loss": 17.29031652212143, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.11165928840637, "training_acc": 53.0, "val_loss": 17.290252447128296, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.15854001045227, "training_acc": 53.0, "val_loss": 17.290429770946503, "val_acc": 52.0}
