"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 706.4608917236328, "training_acc": 53.0, "val_loss": 6.949693285819799e+24, "val_acc": 48.0}
{"epoch": 1, "training_loss": 2.381096732246693e+25, "training_acc": 47.0, "val_loss": 1.962414489409097e+24, "val_acc": 48.0}
{"epoch": 2, "training_loss": 5.919563208679544e+24, "training_acc": 47.0, "val_loss": 1.138742343392536e+23, "val_acc": 48.0}
{"epoch": 3, "training_loss": 3.053387275491065e+23, "training_acc": 47.0, "val_loss": 8.135590617599181e+18, "val_acc": 48.0}
{"epoch": 4, "training_loss": 2.0734833928376523e+20, "training_acc": 47.0, "val_loss": 5.463252001703854e+19, "val_acc": 52.0}
{"epoch": 5, "training_loss": 1.3033773305679577e+20, "training_acc": 55.0, "val_loss": 7.05328723263488e+16, "val_acc": 48.0}
{"epoch": 6, "training_loss": 2.6668945676802458e+17, "training_acc": 47.0, "val_loss": 7.84155801550848e+16, "val_acc": 52.0}
{"epoch": 7, "training_loss": 2.36416456554709e+17, "training_acc": 51.0, "val_loss": 173149349478400.0, "val_acc": 52.0}
{"epoch": 8, "training_loss": 880252867837952.0, "training_acc": 53.0, "val_loss": 7.97283369091072e+17, "val_acc": 48.0}
{"epoch": 9, "training_loss": 1.9974286727593329e+18, "training_acc": 51.0, "val_loss": 3656311950540800.0, "val_acc": 52.0}
{"epoch": 10, "training_loss": 1.387262793220096e+16, "training_acc": 53.0, "val_loss": 1.13555680526336e+16, "val_acc": 48.0}
{"epoch": 11, "training_loss": 2.9755360889798656e+16, "training_acc": 53.0, "val_loss": 804298371891200.0, "val_acc": 48.0}
{"epoch": 12, "training_loss": 2160288604880896.0, "training_acc": 48.0, "val_loss": 1857787684454400.0, "val_acc": 48.0}
{"epoch": 13, "training_loss": 5880849299931136.0, "training_acc": 49.0, "val_loss": 560261968691200.0, "val_acc": 52.0}
{"epoch": 14, "training_loss": 1586481499996160.0, "training_acc": 53.0, "val_loss": 380245639168000.0, "val_acc": 48.0}
{"epoch": 15, "training_loss": 1112281469419520.0, "training_acc": 47.0, "val_loss": 9275130675200.0, "val_acc": 52.0}
{"epoch": 16, "training_loss": 47338483482624.0, "training_acc": 53.0, "val_loss": 20218601472000.0, "val_acc": 52.0}
{"epoch": 17, "training_loss": 61240144363520.0, "training_acc": 53.0, "val_loss": 58573946880000.0, "val_acc": 48.0}
{"epoch": 18, "training_loss": 161135536373760.0, "training_acc": 47.0, "val_loss": 10809661849600.0, "val_acc": 48.0}
{"epoch": 19, "training_loss": 197065317548032.0, "training_acc": 51.0, "val_loss": 67735735500800.0, "val_acc": 52.0}
{"epoch": 20, "training_loss": 195786718838784.0, "training_acc": 53.0, "val_loss": 7392381337600.0, "val_acc": 52.0}
{"epoch": 21, "training_loss": 21547354636288.0, "training_acc": 47.0, "val_loss": 1221756211200.0, "val_acc": 52.0}
{"epoch": 22, "training_loss": 20703295045632.0, "training_acc": 47.0, "val_loss": 1011365068800.0, "val_acc": 52.0}
{"epoch": 23, "training_loss": 2606410508288.0, "training_acc": 54.0, "val_loss": 33764386406400.0, "val_acc": 48.0}
{"epoch": 24, "training_loss": 87728789815296.0, "training_acc": 51.0, "val_loss": 3150402764800.0, "val_acc": 52.0}
{"epoch": 25, "training_loss": 14451767574528.0, "training_acc": 53.0, "val_loss": 3442940313600.0, "val_acc": 48.0}
{"epoch": 26, "training_loss": 15667211862016.0, "training_acc": 47.0, "val_loss": 16459100979200.0, "val_acc": 52.0}
{"epoch": 27, "training_loss": 49300671168512.0, "training_acc": 53.0, "val_loss": 1589096755200.0, "val_acc": 52.0}
{"epoch": 28, "training_loss": 6395731132416.0, "training_acc": 53.0, "val_loss": 15127881318400.0, "val_acc": 48.0}
{"epoch": 29, "training_loss": 54250957045760.0, "training_acc": 47.0, "val_loss": 63885731430400.0, "val_acc": 48.0}
{"epoch": 30, "training_loss": 232927244845056.0, "training_acc": 47.0, "val_loss": 514931503923200.0, "val_acc": 52.0}
{"epoch": 31, "training_loss": 1237023169118208.0, "training_acc": 53.0, "val_loss": 3373895884800.0, "val_acc": 48.0}
{"epoch": 32, "training_loss": 13656656445440.0, "training_acc": 47.0, "val_loss": 3631207628800.0, "val_acc": 52.0}
{"epoch": 33, "training_loss": 13544264695808.0, "training_acc": 53.0, "val_loss": 16520185446400.0, "val_acc": 48.0}
{"epoch": 34, "training_loss": 67637884223488.0, "training_acc": 47.0, "val_loss": 1460113715200.0, "val_acc": 48.0}
{"epoch": 35, "training_loss": 14789350391808.0, "training_acc": 51.0, "val_loss": 8712093696000.0, "val_acc": 52.0}
{"epoch": 36, "training_loss": 27723616288768.0, "training_acc": 53.0, "val_loss": 5489790156800.0, "val_acc": 48.0}
{"epoch": 37, "training_loss": 16566190276608.0, "training_acc": 47.0, "val_loss": 1139504640000.0, "val_acc": 52.0}
{"epoch": 38, "training_loss": 4140763774976.0, "training_acc": 53.0, "val_loss": 75548616294400.0, "val_acc": 48.0}
{"epoch": 39, "training_loss": 197463908204544.0, "training_acc": 49.0, "val_loss": 1128659660800.0, "val_acc": 52.0}
{"epoch": 40, "training_loss": 3358000300032.0, "training_acc": 51.0, "val_loss": 674099046400.0, "val_acc": 52.0}
{"epoch": 41, "training_loss": 3476022116352.0, "training_acc": 53.0, "val_loss": 1326183014400.0, "val_acc": 52.0}
{"epoch": 42, "training_loss": 3865222479872.0, "training_acc": 53.0, "val_loss": 3167858073600.0, "val_acc": 48.0}
{"epoch": 43, "training_loss": 10880337674240.0, "training_acc": 47.0, "val_loss": 1328666828800.0, "val_acc": 52.0}
{"epoch": 44, "training_loss": 4831024791552.0, "training_acc": 53.0, "val_loss": 106460838400.0, "val_acc": 48.0}
{"epoch": 45, "training_loss": 1633033043968.0, "training_acc": 47.0, "val_loss": 353640345600.0, "val_acc": 52.0}
{"epoch": 46, "training_loss": 2031999959040.0, "training_acc": 61.0, "val_loss": 1162695372800.0, "val_acc": 48.0}
{"epoch": 47, "training_loss": 4195877257216.0, "training_acc": 45.0, "val_loss": 362888166400.0, "val_acc": 52.0}
{"epoch": 48, "training_loss": 1645458833408.0, "training_acc": 45.0, "val_loss": 161623910400.0, "val_acc": 48.0}
{"epoch": 49, "training_loss": 937094479872.0, "training_acc": 53.0, "val_loss": 473995673600.0, "val_acc": 52.0}
{"epoch": 50, "training_loss": 1373125926912.0, "training_acc": 53.0, "val_loss": 443006310400.0, "val_acc": 48.0}
{"epoch": 51, "training_loss": 1743009718272.0, "training_acc": 47.0, "val_loss": 2217578291200.0, "val_acc": 52.0}
{"epoch": 52, "training_loss": 5975591788544.0, "training_acc": 53.0, "val_loss": 268213580800.0, "val_acc": 48.0}
{"epoch": 53, "training_loss": 798445346816.0, "training_acc": 44.0, "val_loss": 70925587200.0, "val_acc": 48.0}
{"epoch": 54, "training_loss": 321357117440.0, "training_acc": 51.0, "val_loss": 263371008000.0, "val_acc": 48.0}
{"epoch": 55, "training_loss": 816903084032.0, "training_acc": 49.0, "val_loss": 365445504000.0, "val_acc": 48.0}
{"epoch": 56, "training_loss": 1239430502400.0, "training_acc": 45.0, "val_loss": 42090355200.0, "val_acc": 52.0}
{"epoch": 57, "training_loss": 754887958528.0, "training_acc": 50.0, "val_loss": 271793715200.0, "val_acc": 48.0}
{"epoch": 58, "training_loss": 767113193472.0, "training_acc": 53.0, "val_loss": 58485958400.0, "val_acc": 52.0}
{"epoch": 59, "training_loss": 615424524288.0, "training_acc": 47.0, "val_loss": 86488236800.0, "val_acc": 52.0}
{"epoch": 60, "training_loss": 292274959360.0, "training_acc": 53.0, "val_loss": 322539417600.0, "val_acc": 48.0}
{"epoch": 61, "training_loss": 1065220352000.0, "training_acc": 47.0, "val_loss": 36689376000.0, "val_acc": 52.0}
{"epoch": 62, "training_loss": 167062656000.0, "training_acc": 54.0, "val_loss": 25169361600.0, "val_acc": 48.0}
{"epoch": 63, "training_loss": 100097259520.0, "training_acc": 45.0, "val_loss": 40093718400.0, "val_acc": 52.0}
{"epoch": 64, "training_loss": 163403733504.0, "training_acc": 53.0, "val_loss": 5196590800.0, "val_acc": 44.0}
{"epoch": 65, "training_loss": 92068801024.0, "training_acc": 50.0, "val_loss": 46491728000.0, "val_acc": 48.0}
{"epoch": 66, "training_loss": 119973040000.0, "training_acc": 48.0, "val_loss": 58997196800.0, "val_acc": 52.0}
{"epoch": 67, "training_loss": 263462925312.0, "training_acc": 53.0, "val_loss": 77945209600.0, "val_acc": 52.0}
{"epoch": 68, "training_loss": 267806212608.0, "training_acc": 53.0, "val_loss": 9393299200.0, "val_acc": 48.0}
{"epoch": 69, "training_loss": 48440861696.0, "training_acc": 47.0, "val_loss": 847417300.0, "val_acc": 60.0}
{"epoch": 70, "training_loss": 53595554816.0, "training_acc": 48.0, "val_loss": 10020552800.0, "val_acc": 52.0}
{"epoch": 71, "training_loss": 59373405696.0, "training_acc": 48.0, "val_loss": 24137449600.0, "val_acc": 48.0}
{"epoch": 72, "training_loss": 61767548928.0, "training_acc": 49.0, "val_loss": 26490488000.0, "val_acc": 52.0}
{"epoch": 73, "training_loss": 113647059456.0, "training_acc": 53.0, "val_loss": 2601801600.0, "val_acc": 52.0}
{"epoch": 74, "training_loss": 53468029440.0, "training_acc": 53.0, "val_loss": 43895324800.0, "val_acc": 48.0}
{"epoch": 75, "training_loss": 151209994752.0, "training_acc": 47.0, "val_loss": 12763772800.0, "val_acc": 52.0}
{"epoch": 76, "training_loss": 65689679104.0, "training_acc": 53.0, "val_loss": 14707276800.0, "val_acc": 52.0}
{"epoch": 77, "training_loss": 77311303168.0, "training_acc": 43.0, "val_loss": 16818518400.0, "val_acc": 48.0}
{"epoch": 78, "training_loss": 72411491072.0, "training_acc": 37.0, "val_loss": 7315327200.0, "val_acc": 52.0}
{"epoch": 79, "training_loss": 39879115264.0, "training_acc": 46.0, "val_loss": 7892414400.0, "val_acc": 48.0}
{"epoch": 80, "training_loss": 46949710336.0, "training_acc": 41.0, "val_loss": 13948518400.0, "val_acc": 52.0}
{"epoch": 81, "training_loss": 39608518784.0, "training_acc": 48.0, "val_loss": 8751544000.0, "val_acc": 48.0}
{"epoch": 82, "training_loss": 22620561536.0, "training_acc": 45.0, "val_loss": 4055646000.0, "val_acc": 52.0}
{"epoch": 83, "training_loss": 19953334080.0, "training_acc": 53.0, "val_loss": 878178300.0, "val_acc": 60.0}
{"epoch": 84, "training_loss": 10270115904.0, "training_acc": 47.0, "val_loss": 10484616800.0, "val_acc": 52.0}
{"epoch": 85, "training_loss": 38248534080.0, "training_acc": 53.0, "val_loss": 10302056000.0, "val_acc": 48.0}
{"epoch": 86, "training_loss": 36880080768.0, "training_acc": 48.0, "val_loss": 5934237600.0, "val_acc": 52.0}
{"epoch": 87, "training_loss": 25509550400.0, "training_acc": 51.0, "val_loss": 5874344000.0, "val_acc": 48.0}
{"epoch": 88, "training_loss": 18050025472.0, "training_acc": 51.0, "val_loss": 10822727200.0, "val_acc": 52.0}
