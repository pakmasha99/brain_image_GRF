"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.26970052719116, "training_acc": 53.0, "val_loss": 17.31283664703369, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.3246796131134, "training_acc": 53.0, "val_loss": 17.311760783195496, "val_acc": 52.0}
{"epoch": 2, "training_loss": 70.03309369087219, "training_acc": 45.0, "val_loss": 17.35769957304001, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.23091650009155, "training_acc": 51.0, "val_loss": 17.35691726207733, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.31945514678955, "training_acc": 53.0, "val_loss": 17.47131645679474, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.5864245891571, "training_acc": 53.0, "val_loss": 17.368724942207336, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.83082914352417, "training_acc": 53.0, "val_loss": 17.307859659194946, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.17590188980103, "training_acc": 53.0, "val_loss": 17.308399081230164, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.22087526321411, "training_acc": 53.0, "val_loss": 17.304164171218872, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.02671456336975, "training_acc": 53.0, "val_loss": 17.326976358890533, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.10864567756653, "training_acc": 53.0, "val_loss": 17.3786461353302, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.51552557945251, "training_acc": 53.0, "val_loss": 17.40788072347641, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.40556716918945, "training_acc": 53.0, "val_loss": 17.35118478536606, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.16885304450989, "training_acc": 53.0, "val_loss": 17.314313352108, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.22283673286438, "training_acc": 53.0, "val_loss": 17.312318086624146, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.29150199890137, "training_acc": 53.0, "val_loss": 17.32516586780548, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.2260172367096, "training_acc": 53.0, "val_loss": 17.321883141994476, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.19161438941956, "training_acc": 53.0, "val_loss": 17.312133312225342, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.21129941940308, "training_acc": 53.0, "val_loss": 17.308208346366882, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.24971342086792, "training_acc": 53.0, "val_loss": 17.32620894908905, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.23879361152649, "training_acc": 53.0, "val_loss": 17.344169318675995, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.1192672252655, "training_acc": 53.0, "val_loss": 17.333820462226868, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.17399144172668, "training_acc": 53.0, "val_loss": 17.320027947425842, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.05506443977356, "training_acc": 53.0, "val_loss": 17.316068708896637, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.06040620803833, "training_acc": 53.0, "val_loss": 17.314621806144714, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.05589389801025, "training_acc": 53.0, "val_loss": 17.315831780433655, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.10552525520325, "training_acc": 53.0, "val_loss": 17.316806316375732, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.04708480834961, "training_acc": 53.0, "val_loss": 17.31995791196823, "val_acc": 52.0}
