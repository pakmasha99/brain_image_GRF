"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 183.8403615951538, "training_acc": 53.0, "val_loss": 65999267430400.0, "val_acc": 48.0}
{"epoch": 1, "training_loss": 169258587830824.16, "training_acc": 47.0, "val_loss": 3479.9747467041016, "val_acc": 52.0}
{"epoch": 2, "training_loss": 604114.400390625, "training_acc": 49.0, "val_loss": 844845.8984375, "val_acc": 52.0}
{"epoch": 3, "training_loss": 2462679.4375, "training_acc": 53.0, "val_loss": 17966.925048828125, "val_acc": 52.0}
{"epoch": 4, "training_loss": 130035.1220703125, "training_acc": 57.0, "val_loss": 36345.27893066406, "val_acc": 52.0}
{"epoch": 5, "training_loss": 126795.26293945312, "training_acc": 47.0, "val_loss": 367011.865234375, "val_acc": 52.0}
{"epoch": 6, "training_loss": 951596.7751464844, "training_acc": 53.0, "val_loss": 46787.451171875, "val_acc": 48.0}
{"epoch": 7, "training_loss": 153480.546875, "training_acc": 47.0, "val_loss": 58616.61376953125, "val_acc": 52.0}
{"epoch": 8, "training_loss": 173659.61450195312, "training_acc": 53.0, "val_loss": 12599.170684814453, "val_acc": 48.0}
{"epoch": 9, "training_loss": 30631.895420074463, "training_acc": 54.0, "val_loss": 2196.4942932128906, "val_acc": 48.0}
{"epoch": 10, "training_loss": 41260.9267578125, "training_acc": 47.0, "val_loss": 4130.235290527344, "val_acc": 52.0}
{"epoch": 11, "training_loss": 94996.5498046875, "training_acc": 43.0, "val_loss": 5935.099411010742, "val_acc": 48.0}
{"epoch": 12, "training_loss": 14265.187801361084, "training_acc": 55.0, "val_loss": 831.3051223754883, "val_acc": 52.0}
{"epoch": 13, "training_loss": 7399.380859375, "training_acc": 51.0, "val_loss": 918.5750961303711, "val_acc": 48.0}
{"epoch": 14, "training_loss": 29812.446533203125, "training_acc": 55.0, "val_loss": 3265.460968017578, "val_acc": 52.0}
{"epoch": 15, "training_loss": 10125.203109741211, "training_acc": 51.0, "val_loss": 1819.2209243774414, "val_acc": 52.0}
{"epoch": 16, "training_loss": 10867.247924804688, "training_acc": 53.0, "val_loss": 1155.8356285095215, "val_acc": 48.0}
{"epoch": 17, "training_loss": 13359.40185546875, "training_acc": 49.0, "val_loss": 4663.949203491211, "val_acc": 52.0}
{"epoch": 18, "training_loss": 11794.209838867188, "training_acc": 53.0, "val_loss": 7625.077819824219, "val_acc": 48.0}
{"epoch": 19, "training_loss": 29368.09552001953, "training_acc": 47.0, "val_loss": 2682.374382019043, "val_acc": 48.0}
{"epoch": 20, "training_loss": 7240.858348846436, "training_acc": 51.0, "val_loss": 398.55620861053467, "val_acc": 52.0}
{"epoch": 21, "training_loss": 2152.1522369384766, "training_acc": 49.0, "val_loss": 106.4495325088501, "val_acc": 52.0}
{"epoch": 22, "training_loss": 398.8597354888916, "training_acc": 53.0, "val_loss": 732.0842266082764, "val_acc": 52.0}
{"epoch": 23, "training_loss": 2306.3755798339844, "training_acc": 53.0, "val_loss": 261.91422939300537, "val_acc": 48.0}
{"epoch": 24, "training_loss": 1015.4291954040527, "training_acc": 47.0, "val_loss": 123.39024543762207, "val_acc": 52.0}
{"epoch": 25, "training_loss": 451.77070236206055, "training_acc": 53.0, "val_loss": 151.798677444458, "val_acc": 48.0}
{"epoch": 26, "training_loss": 503.41911029815674, "training_acc": 47.0, "val_loss": 223.50537776947021, "val_acc": 52.0}
{"epoch": 27, "training_loss": 875.3195533752441, "training_acc": 53.0, "val_loss": 134.25382375717163, "val_acc": 52.0}
{"epoch": 28, "training_loss": 399.0747957229614, "training_acc": 55.0, "val_loss": 48.71490001678467, "val_acc": 48.0}
{"epoch": 29, "training_loss": 488.14299392700195, "training_acc": 47.0, "val_loss": 163.8210415840149, "val_acc": 52.0}
{"epoch": 30, "training_loss": 586.8855667114258, "training_acc": 51.0, "val_loss": 85.06715297698975, "val_acc": 48.0}
{"epoch": 31, "training_loss": 335.4153251647949, "training_acc": 47.0, "val_loss": 61.74933314323425, "val_acc": 52.0}
{"epoch": 32, "training_loss": 158.8363618850708, "training_acc": 57.0, "val_loss": 43.86155903339386, "val_acc": 48.0}
{"epoch": 33, "training_loss": 122.1168782711029, "training_acc": 50.0, "val_loss": 69.87251043319702, "val_acc": 52.0}
{"epoch": 34, "training_loss": 218.28879928588867, "training_acc": 53.0, "val_loss": 65.71399569511414, "val_acc": 48.0}
{"epoch": 35, "training_loss": 256.80451822280884, "training_acc": 47.0, "val_loss": 26.81598663330078, "val_acc": 52.0}
{"epoch": 36, "training_loss": 135.12273788452148, "training_acc": 53.0, "val_loss": 26.27756893634796, "val_acc": 48.0}
{"epoch": 37, "training_loss": 113.30267715454102, "training_acc": 49.0, "val_loss": 31.437131762504578, "val_acc": 52.0}
{"epoch": 38, "training_loss": 116.10707998275757, "training_acc": 53.0, "val_loss": 26.363161206245422, "val_acc": 48.0}
{"epoch": 39, "training_loss": 106.40287852287292, "training_acc": 45.0, "val_loss": 20.499396324157715, "val_acc": 52.0}
{"epoch": 40, "training_loss": 85.31418991088867, "training_acc": 51.0, "val_loss": 17.75061786174774, "val_acc": 52.0}
{"epoch": 41, "training_loss": 74.41179943084717, "training_acc": 51.0, "val_loss": 18.726584315299988, "val_acc": 44.0}
{"epoch": 42, "training_loss": 76.27313184738159, "training_acc": 50.0, "val_loss": 18.49316954612732, "val_acc": 52.0}
{"epoch": 43, "training_loss": 70.00950241088867, "training_acc": 54.0, "val_loss": 19.198502600193024, "val_acc": 48.0}
{"epoch": 44, "training_loss": 76.62479829788208, "training_acc": 51.0, "val_loss": 22.596126794815063, "val_acc": 52.0}
{"epoch": 45, "training_loss": 79.69268083572388, "training_acc": 53.0, "val_loss": 23.770423233509064, "val_acc": 48.0}
{"epoch": 46, "training_loss": 88.22298264503479, "training_acc": 44.0, "val_loss": 27.601036429405212, "val_acc": 52.0}
{"epoch": 47, "training_loss": 104.16223239898682, "training_acc": 53.0, "val_loss": 30.49238622188568, "val_acc": 48.0}
{"epoch": 48, "training_loss": 115.5344705581665, "training_acc": 47.0, "val_loss": 20.194056630134583, "val_acc": 52.0}
{"epoch": 49, "training_loss": 96.12596559524536, "training_acc": 53.0, "val_loss": 17.633366584777832, "val_acc": 52.0}
{"epoch": 50, "training_loss": 104.97319793701172, "training_acc": 46.0, "val_loss": 17.661067843437195, "val_acc": 52.0}
{"epoch": 51, "training_loss": 93.04752540588379, "training_acc": 49.0, "val_loss": 20.824874937534332, "val_acc": 52.0}
{"epoch": 52, "training_loss": 85.55818128585815, "training_acc": 54.0, "val_loss": 25.577786564826965, "val_acc": 48.0}
{"epoch": 53, "training_loss": 101.66740655899048, "training_acc": 43.0, "val_loss": 20.57066261768341, "val_acc": 48.0}
{"epoch": 54, "training_loss": 120.63167572021484, "training_acc": 49.0, "val_loss": 27.336329221725464, "val_acc": 52.0}
{"epoch": 55, "training_loss": 100.69348955154419, "training_acc": 55.0, "val_loss": 36.9480162858963, "val_acc": 48.0}
{"epoch": 56, "training_loss": 116.85998106002808, "training_acc": 51.0, "val_loss": 35.975632071495056, "val_acc": 52.0}
{"epoch": 57, "training_loss": 147.44961214065552, "training_acc": 53.0, "val_loss": 18.967440724372864, "val_acc": 52.0}
{"epoch": 58, "training_loss": 92.91109466552734, "training_acc": 50.0, "val_loss": 34.81836915016174, "val_acc": 48.0}
{"epoch": 59, "training_loss": 122.3964991569519, "training_acc": 49.0, "val_loss": 27.64989733695984, "val_acc": 52.0}
{"epoch": 60, "training_loss": 98.8805603981018, "training_acc": 53.0, "val_loss": 18.84768307209015, "val_acc": 44.0}
{"epoch": 61, "training_loss": 96.52046298980713, "training_acc": 41.0, "val_loss": 17.873242497444153, "val_acc": 52.0}
{"epoch": 62, "training_loss": 72.01245522499084, "training_acc": 59.0, "val_loss": 26.18986964225769, "val_acc": 52.0}
{"epoch": 63, "training_loss": 96.36415791511536, "training_acc": 49.0, "val_loss": 20.865726470947266, "val_acc": 48.0}
{"epoch": 64, "training_loss": 87.22436714172363, "training_acc": 47.0, "val_loss": 19.64050680398941, "val_acc": 52.0}
{"epoch": 65, "training_loss": 91.04569435119629, "training_acc": 53.0, "val_loss": 19.640910625457764, "val_acc": 52.0}
{"epoch": 66, "training_loss": 90.70782470703125, "training_acc": 43.0, "val_loss": 23.281849920749664, "val_acc": 48.0}
{"epoch": 67, "training_loss": 88.62944459915161, "training_acc": 42.0, "val_loss": 22.07675129175186, "val_acc": 52.0}
{"epoch": 68, "training_loss": 83.08325338363647, "training_acc": 53.0, "val_loss": 18.69610697031021, "val_acc": 36.0}
