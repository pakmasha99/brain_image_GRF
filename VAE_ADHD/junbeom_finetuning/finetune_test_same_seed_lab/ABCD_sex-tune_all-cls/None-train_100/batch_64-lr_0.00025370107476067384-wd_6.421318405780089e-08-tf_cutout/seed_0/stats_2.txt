"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.56151175498962, "training_acc": 51.0, "val_loss": 17.332200706005096, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.30016040802002, "training_acc": 53.0, "val_loss": 17.37000197172165, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.80282211303711, "training_acc": 53.0, "val_loss": 17.38070249557495, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.42873001098633, "training_acc": 53.0, "val_loss": 17.310751974582672, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.25696802139282, "training_acc": 53.0, "val_loss": 17.311181128025055, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.16411995887756, "training_acc": 53.0, "val_loss": 17.354369163513184, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.24367451667786, "training_acc": 53.0, "val_loss": 17.343135178089142, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.55534100532532, "training_acc": 53.0, "val_loss": 17.31453239917755, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.22271633148193, "training_acc": 53.0, "val_loss": 17.329244315624237, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.14239454269409, "training_acc": 53.0, "val_loss": 17.310497164726257, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.18034386634827, "training_acc": 53.0, "val_loss": 17.30959266424179, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.19755554199219, "training_acc": 53.0, "val_loss": 17.31964200735092, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.20245933532715, "training_acc": 53.0, "val_loss": 17.30963885784149, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.11802077293396, "training_acc": 53.0, "val_loss": 17.317427694797516, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.37219214439392, "training_acc": 47.0, "val_loss": 17.31082946062088, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.2747163772583, "training_acc": 53.0, "val_loss": 17.33730286359787, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.17962431907654, "training_acc": 53.0, "val_loss": 17.341075837612152, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.16947102546692, "training_acc": 53.0, "val_loss": 17.314788699150085, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.1180956363678, "training_acc": 53.0, "val_loss": 17.308296263217926, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.30594635009766, "training_acc": 53.0, "val_loss": 17.309844493865967, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.23431777954102, "training_acc": 53.0, "val_loss": 17.321759462356567, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.14837384223938, "training_acc": 53.0, "val_loss": 17.32846349477768, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.16639065742493, "training_acc": 53.0, "val_loss": 17.318493127822876, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.14853477478027, "training_acc": 53.0, "val_loss": 17.31453537940979, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.12075543403625, "training_acc": 53.0, "val_loss": 17.307963967323303, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.14536333084106, "training_acc": 53.0, "val_loss": 17.3094242811203, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.49238920211792, "training_acc": 53.0, "val_loss": 17.308346927165985, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.15208792686462, "training_acc": 53.0, "val_loss": 17.313215136528015, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.22523045539856, "training_acc": 53.0, "val_loss": 17.317669093608856, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.30769801139832, "training_acc": 53.0, "val_loss": 17.308998107910156, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.23709058761597, "training_acc": 53.0, "val_loss": 17.357909679412842, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.53802180290222, "training_acc": 53.0, "val_loss": 17.352625727653503, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.62654232978821, "training_acc": 53.0, "val_loss": 17.308782041072845, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.240318775177, "training_acc": 53.0, "val_loss": 17.310035228729248, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.50613784790039, "training_acc": 53.0, "val_loss": 17.33255535364151, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.15566945075989, "training_acc": 53.0, "val_loss": 17.312273383140564, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.1431782245636, "training_acc": 53.0, "val_loss": 17.312121391296387, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.88737082481384, "training_acc": 37.0, "val_loss": 17.31356531381607, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.1227171421051, "training_acc": 53.0, "val_loss": 17.342200875282288, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.13263821601868, "training_acc": 53.0, "val_loss": 17.41025745868683, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.41768765449524, "training_acc": 53.0, "val_loss": 17.436525225639343, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.54282021522522, "training_acc": 53.0, "val_loss": 17.420418560504913, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.45399188995361, "training_acc": 53.0, "val_loss": 17.385776340961456, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.21721386909485, "training_acc": 53.0, "val_loss": 17.315292358398438, "val_acc": 52.0}
