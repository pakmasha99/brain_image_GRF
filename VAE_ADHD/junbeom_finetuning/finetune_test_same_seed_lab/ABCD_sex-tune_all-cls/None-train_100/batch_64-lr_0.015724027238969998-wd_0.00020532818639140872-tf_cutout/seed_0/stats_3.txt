"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 1302.3339881896973, "training_acc": 44.0, "val_loss": 2.228534378496e+16, "val_acc": 52.0}
{"epoch": 1, "training_loss": 4.954733973228955e+16, "training_acc": 57.0, "val_loss": 133108625.0, "val_acc": 48.0}
{"epoch": 2, "training_loss": 708815500.0, "training_acc": 47.0, "val_loss": 1457434.47265625, "val_acc": 52.0}
{"epoch": 3, "training_loss": 1614150370.75, "training_acc": 59.0, "val_loss": 45253312.5, "val_acc": 52.0}
{"epoch": 4, "training_loss": 159139726.5, "training_acc": 53.0, "val_loss": 2069641.2109375, "val_acc": 52.0}
{"epoch": 5, "training_loss": 170406025.0, "training_acc": 61.0, "val_loss": 654081050.0, "val_acc": 52.0}
{"epoch": 6, "training_loss": 1452778990.25, "training_acc": 57.0, "val_loss": 793015.52734375, "val_acc": 52.0}
{"epoch": 7, "training_loss": 2900225.40625, "training_acc": 53.0, "val_loss": 536363.232421875, "val_acc": 52.0}
{"epoch": 8, "training_loss": 836428223.125, "training_acc": 51.0, "val_loss": 265654600.0, "val_acc": 52.0}
{"epoch": 9, "training_loss": 802445352.5, "training_acc": 53.0, "val_loss": 30340568.75, "val_acc": 52.0}
{"epoch": 10, "training_loss": 112325132.5, "training_acc": 53.0, "val_loss": 10921453.90625, "val_acc": 52.0}
{"epoch": 11, "training_loss": 36844805.75, "training_acc": 53.0, "val_loss": 5259241.796875, "val_acc": 52.0}
{"epoch": 12, "training_loss": 19718168.5, "training_acc": 53.0, "val_loss": 3757789.84375, "val_acc": 48.0}
{"epoch": 13, "training_loss": 97464243.0, "training_acc": 45.0, "val_loss": 4625061.71875, "val_acc": 48.0}
{"epoch": 14, "training_loss": 23398807.25, "training_acc": 47.0, "val_loss": 2785719.921875, "val_acc": 48.0}
{"epoch": 15, "training_loss": 10695098.9375, "training_acc": 55.0, "val_loss": 3798456.25, "val_acc": 48.0}
{"epoch": 16, "training_loss": 15934434.8125, "training_acc": 51.0, "val_loss": 2405401.3671875, "val_acc": 52.0}
{"epoch": 17, "training_loss": 6030745.75, "training_acc": 53.0, "val_loss": 71862612.5, "val_acc": 48.0}
{"epoch": 18, "training_loss": 170605033.40625, "training_acc": 55.0, "val_loss": 8113896.875, "val_acc": 52.0}
{"epoch": 19, "training_loss": 22743060.48828125, "training_acc": 44.0, "val_loss": 5395142.1875, "val_acc": 52.0}
{"epoch": 20, "training_loss": 22357301.8125, "training_acc": 53.0, "val_loss": 1390797.75390625, "val_acc": 52.0}
{"epoch": 21, "training_loss": 6065992.21875, "training_acc": 55.0, "val_loss": 1426926.7578125, "val_acc": 48.0}
{"epoch": 22, "training_loss": 4270107.890625, "training_acc": 47.0, "val_loss": 452822.65625, "val_acc": 52.0}
{"epoch": 23, "training_loss": 14874412.0, "training_acc": 50.0, "val_loss": 31556643200.0, "val_acc": 52.0}
{"epoch": 24, "training_loss": 80986531464.5, "training_acc": 49.0, "val_loss": 211610325.0, "val_acc": 52.0}
{"epoch": 25, "training_loss": 619797561856.0, "training_acc": 45.0, "val_loss": 301986625.0, "val_acc": 52.0}
{"epoch": 26, "training_loss": 3708793600.0, "training_acc": 43.0, "val_loss": 39480774400.0, "val_acc": 52.0}
{"epoch": 27, "training_loss": 110352418816.0, "training_acc": 53.0, "val_loss": 1844492400.0, "val_acc": 52.0}
{"epoch": 28, "training_loss": 23643066112.0, "training_acc": 53.0, "val_loss": 476619350.0, "val_acc": 48.0}
{"epoch": 29, "training_loss": 127699498240.0, "training_acc": 55.0, "val_loss": 7969522400.0, "val_acc": 48.0}
{"epoch": 30, "training_loss": 25900306176.0, "training_acc": 47.0, "val_loss": 207485462.5, "val_acc": 48.0}
{"epoch": 31, "training_loss": 1637017696.0, "training_acc": 45.0, "val_loss": 388299725.0, "val_acc": 52.0}
{"epoch": 32, "training_loss": 1004447551.0, "training_acc": 53.0, "val_loss": 424647800.0, "val_acc": 48.0}
{"epoch": 33, "training_loss": 1353953414.0, "training_acc": 47.0, "val_loss": 5623896000.0, "val_acc": 52.0}
{"epoch": 34, "training_loss": 14574690235.5, "training_acc": 47.0, "val_loss": 52395718.75, "val_acc": 52.0}
{"epoch": 35, "training_loss": 160351048.25, "training_acc": 53.0, "val_loss": 29550400.0, "val_acc": 48.0}
{"epoch": 36, "training_loss": 90361514.625, "training_acc": 47.0, "val_loss": 11806189.84375, "val_acc": 48.0}
{"epoch": 37, "training_loss": 28787962.44140625, "training_acc": 52.0, "val_loss": 556870.5078125, "val_acc": 52.0}
{"epoch": 38, "training_loss": 20989779.5, "training_acc": 47.0, "val_loss": 6081817.96875, "val_acc": 48.0}
{"epoch": 39, "training_loss": 19882348.0625, "training_acc": 51.0, "val_loss": 7712014.0625, "val_acc": 48.0}
{"epoch": 40, "training_loss": 35029432.25, "training_acc": 41.0, "val_loss": 4559580.078125, "val_acc": 52.0}
{"epoch": 41, "training_loss": 14860201.96875, "training_acc": 53.0, "val_loss": 452163.623046875, "val_acc": 52.0}
{"epoch": 42, "training_loss": 8821896.625, "training_acc": 37.0, "val_loss": 338133.7158203125, "val_acc": 52.0}
{"epoch": 43, "training_loss": 4193503.0, "training_acc": 41.0, "val_loss": 2101122.265625, "val_acc": 48.0}
{"epoch": 44, "training_loss": 30187883.75, "training_acc": 51.0, "val_loss": 3206809.375, "val_acc": 52.0}
{"epoch": 45, "training_loss": 9503151.5078125, "training_acc": 53.0, "val_loss": 2431372.8515625, "val_acc": 48.0}
{"epoch": 46, "training_loss": 10207276.1875, "training_acc": 47.0, "val_loss": 1638067.48046875, "val_acc": 48.0}
{"epoch": 47, "training_loss": 4435312.037109375, "training_acc": 47.0, "val_loss": 1145515.72265625, "val_acc": 52.0}
{"epoch": 48, "training_loss": 4507483.65625, "training_acc": 53.0, "val_loss": 509152.197265625, "val_acc": 48.0}
{"epoch": 49, "training_loss": 1619820.3046875, "training_acc": 46.0, "val_loss": 229757.080078125, "val_acc": 48.0}
{"epoch": 50, "training_loss": 926345.86328125, "training_acc": 53.0, "val_loss": 283378.173828125, "val_acc": 52.0}
{"epoch": 51, "training_loss": 1000416.8046875, "training_acc": 51.0, "val_loss": 69520.62377929688, "val_acc": 52.0}
{"epoch": 52, "training_loss": 259771.9189453125, "training_acc": 59.0, "val_loss": 21652.05841064453, "val_acc": 52.0}
{"epoch": 53, "training_loss": 284146.7734375, "training_acc": 46.0, "val_loss": 49740.36560058594, "val_acc": 48.0}
{"epoch": 54, "training_loss": 377045.572265625, "training_acc": 51.0, "val_loss": 63555.84716796875, "val_acc": 52.0}
{"epoch": 55, "training_loss": 604835.6171875, "training_acc": 48.0, "val_loss": 188359.912109375, "val_acc": 48.0}
{"epoch": 56, "training_loss": 611912.9765625, "training_acc": 50.0, "val_loss": 122397.4609375, "val_acc": 52.0}
{"epoch": 57, "training_loss": 1282407.9921875, "training_acc": 54.0, "val_loss": 552519.482421875, "val_acc": 52.0}
{"epoch": 58, "training_loss": 1930780.5078125, "training_acc": 53.0, "val_loss": 69943.3349609375, "val_acc": 48.0}
{"epoch": 59, "training_loss": 556032.14453125, "training_acc": 51.0, "val_loss": 108658.3740234375, "val_acc": 52.0}
{"epoch": 60, "training_loss": 1006781.234375, "training_acc": 39.0, "val_loss": 193898.6328125, "val_acc": 52.0}
{"epoch": 61, "training_loss": 752071.0942382812, "training_acc": 53.0, "val_loss": 34012.359619140625, "val_acc": 48.0}
{"epoch": 62, "training_loss": 228315.1494140625, "training_acc": 44.0, "val_loss": 8044.097137451172, "val_acc": 44.0}
{"epoch": 63, "training_loss": 136868.166015625, "training_acc": 47.0, "val_loss": 80965.4052734375, "val_acc": 52.0}
{"epoch": 64, "training_loss": 273746.1552734375, "training_acc": 52.0, "val_loss": 129955.1513671875, "val_acc": 48.0}
{"epoch": 65, "training_loss": 431343.328125, "training_acc": 44.0, "val_loss": 311449.9267578125, "val_acc": 52.0}
{"epoch": 66, "training_loss": 1022732.49609375, "training_acc": 53.0, "val_loss": 24017.718505859375, "val_acc": 48.0}
{"epoch": 67, "training_loss": 171311.736328125, "training_acc": 49.0, "val_loss": 83193.38989257812, "val_acc": 52.0}
{"epoch": 68, "training_loss": 295898.7861328125, "training_acc": 54.0, "val_loss": 52329.437255859375, "val_acc": 48.0}
{"epoch": 69, "training_loss": 230398.41015625, "training_acc": 47.0, "val_loss": 86032.26318359375, "val_acc": 52.0}
{"epoch": 70, "training_loss": 369080.533203125, "training_acc": 53.0, "val_loss": 17464.398193359375, "val_acc": 52.0}
{"epoch": 71, "training_loss": 223029.5302734375, "training_acc": 48.0, "val_loss": 128299.52392578125, "val_acc": 48.0}
{"epoch": 72, "training_loss": 320935.32873535156, "training_acc": 52.0, "val_loss": 135499.853515625, "val_acc": 52.0}
{"epoch": 73, "training_loss": 530923.013671875, "training_acc": 53.0, "val_loss": 46203.82385253906, "val_acc": 52.0}
{"epoch": 74, "training_loss": 386129.48828125, "training_acc": 46.0, "val_loss": 199749.0478515625, "val_acc": 48.0}
{"epoch": 75, "training_loss": 684061.9677734375, "training_acc": 47.0, "val_loss": 50076.483154296875, "val_acc": 52.0}
{"epoch": 76, "training_loss": 249462.9970703125, "training_acc": 53.0, "val_loss": 64688.946533203125, "val_acc": 52.0}
{"epoch": 77, "training_loss": 198292.3349609375, "training_acc": 55.0, "val_loss": 51832.91015625, "val_acc": 48.0}
{"epoch": 78, "training_loss": 146103.173828125, "training_acc": 51.0, "val_loss": 52291.30859375, "val_acc": 52.0}
{"epoch": 79, "training_loss": 160401.35229492188, "training_acc": 55.0, "val_loss": 39810.00671386719, "val_acc": 48.0}
{"epoch": 80, "training_loss": 132400.16528320312, "training_acc": 42.0, "val_loss": 47796.97265625, "val_acc": 52.0}
{"epoch": 81, "training_loss": 172490.861328125, "training_acc": 54.0, "val_loss": 34130.06896972656, "val_acc": 48.0}
