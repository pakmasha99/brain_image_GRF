"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 1757.4592819213867, "training_acc": 50.0, "val_loss": 9.75916641042295e+23, "val_acc": 56.0}
{"epoch": 1, "training_loss": 2.593677661956244e+24, "training_acc": 52.0, "val_loss": 418276206182400.0, "val_acc": 56.0}
{"epoch": 2, "training_loss": 6.398287659579474e+17, "training_acc": 54.0, "val_loss": 6.141469299572736e+17, "val_acc": 44.0}
{"epoch": 3, "training_loss": 3.3889720855345234e+18, "training_acc": 46.0, "val_loss": 4.11340359335936e+16, "val_acc": 56.0}
{"epoch": 4, "training_loss": 1.1557207593399091e+17, "training_acc": 52.0, "val_loss": 1030953841459200.0, "val_acc": 44.0}
{"epoch": 5, "training_loss": 1.9780989693722624e+16, "training_acc": 44.0, "val_loss": 362552177459200.0, "val_acc": 56.0}
{"epoch": 6, "training_loss": 2882379161010176.0, "training_acc": 52.0, "val_loss": 99390337843200.0, "val_acc": 56.0}
{"epoch": 7, "training_loss": 298642899730432.0, "training_acc": 48.0, "val_loss": 9843870105600.0, "val_acc": 44.0}
{"epoch": 8, "training_loss": 477607128727552.0, "training_acc": 42.0, "val_loss": 102293962752000.0, "val_acc": 44.0}
{"epoch": 9, "training_loss": 603374206582784.0, "training_acc": 48.0, "val_loss": 119730588876800.0, "val_acc": 44.0}
{"epoch": 10, "training_loss": 321315622354944.0, "training_acc": 48.0, "val_loss": 20303054438400.0, "val_acc": 44.0}
{"epoch": 11, "training_loss": 149462819274752.0, "training_acc": 48.0, "val_loss": 9510839091200.0, "val_acc": 56.0}
{"epoch": 12, "training_loss": 3233560519507968.0, "training_acc": 52.0, "val_loss": 3583152803020800.0, "val_acc": 44.0}
{"epoch": 13, "training_loss": 8124218156777472.0, "training_acc": 48.0, "val_loss": 14843705753600.0, "val_acc": 44.0}
{"epoch": 14, "training_loss": 63465573318656.0, "training_acc": 48.0, "val_loss": 18835624755200.0, "val_acc": 44.0}
{"epoch": 15, "training_loss": 75270586368000.0, "training_acc": 54.0, "val_loss": 156944079257600.0, "val_acc": 44.0}
{"epoch": 16, "training_loss": 442841864601600.0, "training_acc": 48.0, "val_loss": 8921832652800.0, "val_acc": 56.0}
{"epoch": 17, "training_loss": 31719013351424.0, "training_acc": 52.0, "val_loss": 49304131993600.0, "val_acc": 44.0}
{"epoch": 18, "training_loss": 123945447391232.0, "training_acc": 50.0, "val_loss": 2483157401600.0, "val_acc": 56.0}
{"epoch": 19, "training_loss": 14004433321984.0, "training_acc": 52.0, "val_loss": 6738833408000.0, "val_acc": 44.0}
{"epoch": 20, "training_loss": 88241828003840.0, "training_acc": 52.0, "val_loss": 1718559744000.0, "val_acc": 44.0}
{"epoch": 21, "training_loss": 7142680788992.0, "training_acc": 48.0, "val_loss": 92551443200.0, "val_acc": 56.0}
{"epoch": 22, "training_loss": 3273498034176.0, "training_acc": 51.0, "val_loss": 775600629350400.0, "val_acc": 56.0}
{"epoch": 23, "training_loss": 2050886039896064.0, "training_acc": 52.0, "val_loss": 393012340326400.0, "val_acc": 44.0}
{"epoch": 24, "training_loss": 1696475037827072.0, "training_acc": 48.0, "val_loss": 1872072854732800.0, "val_acc": 56.0}
{"epoch": 25, "training_loss": 5411070232494080.0, "training_acc": 52.0, "val_loss": 176548754227200.0, "val_acc": 44.0}
{"epoch": 26, "training_loss": 537143561158656.0, "training_acc": 48.0, "val_loss": 28299568742400.0, "val_acc": 56.0}
{"epoch": 27, "training_loss": 88774208126976.0, "training_acc": 52.0, "val_loss": 23102282137600.0, "val_acc": 44.0}
{"epoch": 28, "training_loss": 60361045311488.0, "training_acc": 48.0, "val_loss": 5127256064000.0, "val_acc": 56.0}
{"epoch": 29, "training_loss": 17847403216896.0, "training_acc": 52.0, "val_loss": 2599152025600.0, "val_acc": 44.0}
{"epoch": 30, "training_loss": 12271923429376.0, "training_acc": 40.0, "val_loss": 15561991782400.0, "val_acc": 56.0}
{"epoch": 31, "training_loss": 56599546822656.0, "training_acc": 52.0, "val_loss": 3239201996800.0, "val_acc": 44.0}
{"epoch": 32, "training_loss": 1921055140085760.0, "training_acc": 49.0, "val_loss": 454933505638400.0, "val_acc": 44.0}
{"epoch": 33, "training_loss": 1256288669401088.0, "training_acc": 48.0, "val_loss": 14946028748800.0, "val_acc": 56.0}
{"epoch": 34, "training_loss": 87119042707456.0, "training_acc": 52.0, "val_loss": 32319350374400.0, "val_acc": 56.0}
{"epoch": 35, "training_loss": 122425828179968.0, "training_acc": 52.0, "val_loss": 16185370214400.0, "val_acc": 56.0}
{"epoch": 36, "training_loss": 51555312402432.0, "training_acc": 52.0, "val_loss": 1075092582400.0, "val_acc": 44.0}
{"epoch": 37, "training_loss": 4687688630272.0, "training_acc": 59.0, "val_loss": 14770497126400.0, "val_acc": 44.0}
{"epoch": 38, "training_loss": 34436514217984.0, "training_acc": 48.0, "val_loss": 31032678809600.0, "val_acc": 56.0}
{"epoch": 39, "training_loss": 93250247852032.0, "training_acc": 56.0, "val_loss": 28796180889600.0, "val_acc": 44.0}
{"epoch": 40, "training_loss": 2850043560198144.0, "training_acc": 50.0, "val_loss": 518149426380800.0, "val_acc": 44.0}
