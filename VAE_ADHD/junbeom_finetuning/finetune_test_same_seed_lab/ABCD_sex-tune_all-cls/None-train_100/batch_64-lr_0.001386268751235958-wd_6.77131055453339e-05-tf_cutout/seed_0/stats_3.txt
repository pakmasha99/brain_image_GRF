"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 128.97743034362793, "training_acc": 45.0, "val_loss": 211.42253875732422, "val_acc": 52.0}
{"epoch": 1, "training_loss": 1008.7504425048828, "training_acc": 45.0, "val_loss": 20.618104934692383, "val_acc": 48.0}
{"epoch": 2, "training_loss": 75.96728253364563, "training_acc": 53.0, "val_loss": 17.660190165042877, "val_acc": 52.0}
{"epoch": 3, "training_loss": 70.77920460700989, "training_acc": 53.0, "val_loss": 17.313252389431, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.87007093429565, "training_acc": 51.0, "val_loss": 17.377200722694397, "val_acc": 52.0}
{"epoch": 5, "training_loss": 72.12082004547119, "training_acc": 53.0, "val_loss": 17.31194704771042, "val_acc": 52.0}
{"epoch": 6, "training_loss": 70.93916034698486, "training_acc": 43.0, "val_loss": 17.3974871635437, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.18207931518555, "training_acc": 47.0, "val_loss": 17.327791452407837, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.05406427383423, "training_acc": 53.0, "val_loss": 17.473629117012024, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.6089289188385, "training_acc": 53.0, "val_loss": 17.42471605539322, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.42098999023438, "training_acc": 53.0, "val_loss": 17.326746881008148, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.22072172164917, "training_acc": 53.0, "val_loss": 17.31114089488983, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.25858068466187, "training_acc": 53.0, "val_loss": 17.309866845607758, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.09797239303589, "training_acc": 53.0, "val_loss": 17.334195971488953, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.33302426338196, "training_acc": 53.0, "val_loss": 17.367541790008545, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.26512217521667, "training_acc": 53.0, "val_loss": 17.333079874515533, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.17787623405457, "training_acc": 53.0, "val_loss": 17.316195368766785, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.26409864425659, "training_acc": 53.0, "val_loss": 17.311184108257294, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.2434012889862, "training_acc": 53.0, "val_loss": 17.315803468227386, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.16474604606628, "training_acc": 53.0, "val_loss": 17.312175035476685, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.13535737991333, "training_acc": 53.0, "val_loss": 17.312614619731903, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.22389769554138, "training_acc": 53.0, "val_loss": 17.31320172548294, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.12718224525452, "training_acc": 53.0, "val_loss": 17.309749126434326, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.13735151290894, "training_acc": 53.0, "val_loss": 17.309580743312836, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.14806151390076, "training_acc": 53.0, "val_loss": 17.311766743659973, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.18713045120239, "training_acc": 53.0, "val_loss": 17.31451153755188, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.22477793693542, "training_acc": 53.0, "val_loss": 17.31705814599991, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.22625303268433, "training_acc": 53.0, "val_loss": 17.313630878925323, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.18846344947815, "training_acc": 53.0, "val_loss": 17.309726774692535, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.2029230594635, "training_acc": 53.0, "val_loss": 17.310254275798798, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.13780355453491, "training_acc": 53.0, "val_loss": 17.312835156917572, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.13330030441284, "training_acc": 53.0, "val_loss": 17.316047847270966, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.14137291908264, "training_acc": 53.0, "val_loss": 17.31850802898407, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.14031672477722, "training_acc": 53.0, "val_loss": 17.31792241334915, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.22543835639954, "training_acc": 53.0, "val_loss": 17.314669489860535, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.13520216941833, "training_acc": 53.0, "val_loss": 17.317642271518707, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.14927768707275, "training_acc": 53.0, "val_loss": 17.32383370399475, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.22454619407654, "training_acc": 53.0, "val_loss": 17.32614040374756, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.14045929908752, "training_acc": 53.0, "val_loss": 17.318931221961975, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.18102741241455, "training_acc": 53.0, "val_loss": 17.312467098236084, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.14978170394897, "training_acc": 53.0, "val_loss": 17.310932278633118, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.13638043403625, "training_acc": 53.0, "val_loss": 17.311187088489532, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.13908648490906, "training_acc": 53.0, "val_loss": 17.311622202396393, "val_acc": 52.0}
