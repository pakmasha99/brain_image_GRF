"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 630.3629531860352, "training_acc": 53.0, "val_loss": 7.84090303299584e+17, "val_acc": 48.0}
{"epoch": 1, "training_loss": 2.1324160782173386e+18, "training_acc": 47.0, "val_loss": 8772110400.0, "val_acc": 52.0}
{"epoch": 2, "training_loss": 24536483872.0, "training_acc": 53.0, "val_loss": 8706538400.0, "val_acc": 48.0}
{"epoch": 3, "training_loss": 23888496144.0, "training_acc": 47.0, "val_loss": 388303700.0, "val_acc": 52.0}
{"epoch": 4, "training_loss": 1366262408.0, "training_acc": 53.0, "val_loss": 8965675.78125, "val_acc": 52.0}
{"epoch": 5, "training_loss": 49763224.25, "training_acc": 53.0, "val_loss": 8242875.78125, "val_acc": 48.0}
{"epoch": 6, "training_loss": 1304762884.0, "training_acc": 47.0, "val_loss": 10028717.96875, "val_acc": 52.0}
{"epoch": 7, "training_loss": 46517419.0, "training_acc": 53.0, "val_loss": 85119212.5, "val_acc": 48.0}
{"epoch": 8, "training_loss": 228968952.5, "training_acc": 51.0, "val_loss": 7305324.21875, "val_acc": 52.0}
{"epoch": 9, "training_loss": 19157689.37109375, "training_acc": 49.0, "val_loss": 332961275.0, "val_acc": 52.0}
{"epoch": 10, "training_loss": 858260420.375, "training_acc": 53.0, "val_loss": 382966225.0, "val_acc": 52.0}
{"epoch": 11, "training_loss": 932466278.5, "training_acc": 51.0, "val_loss": 10914875.0, "val_acc": 48.0}
{"epoch": 12, "training_loss": 71298499.0, "training_acc": 47.0, "val_loss": 28927268.75, "val_acc": 52.0}
{"epoch": 13, "training_loss": 92422805.25, "training_acc": 57.0, "val_loss": 14726781.25, "val_acc": 48.0}
{"epoch": 14, "training_loss": 61347391.5, "training_acc": 45.0, "val_loss": 95058368.75, "val_acc": 48.0}
{"epoch": 15, "training_loss": 1373305672.0, "training_acc": 55.0, "val_loss": 172860975.0, "val_acc": 48.0}
{"epoch": 16, "training_loss": 486235351.5, "training_acc": 47.0, "val_loss": 11298604.6875, "val_acc": 48.0}
{"epoch": 17, "training_loss": 35530854.8125, "training_acc": 47.0, "val_loss": 35195512.5, "val_acc": 52.0}
{"epoch": 18, "training_loss": 12347151272.0, "training_acc": 41.0, "val_loss": 55606906.25, "val_acc": 52.0}
{"epoch": 19, "training_loss": 702112996.0, "training_acc": 53.0, "val_loss": 46891456.25, "val_acc": 52.0}
{"epoch": 20, "training_loss": 120710227.8125, "training_acc": 51.0, "val_loss": 44861031.25, "val_acc": 52.0}
{"epoch": 21, "training_loss": 221247557.0, "training_acc": 55.0, "val_loss": 228077375.0, "val_acc": 52.0}
{"epoch": 22, "training_loss": 693084459.0, "training_acc": 53.0, "val_loss": 10982221.875, "val_acc": 48.0}
{"epoch": 23, "training_loss": 148420121.0, "training_acc": 51.0, "val_loss": 103330887.5, "val_acc": 48.0}
{"epoch": 24, "training_loss": 284808774.5, "training_acc": 47.0, "val_loss": 11842691.40625, "val_acc": 48.0}
{"epoch": 25, "training_loss": 37968724.15625, "training_acc": 47.0, "val_loss": 1178372.4609375, "val_acc": 48.0}
{"epoch": 26, "training_loss": 3662117.2109375, "training_acc": 47.0, "val_loss": 515019.53125, "val_acc": 52.0}
{"epoch": 27, "training_loss": 7242020.6875, "training_acc": 53.0, "val_loss": 91618268.75, "val_acc": 48.0}
{"epoch": 28, "training_loss": 248550260.5, "training_acc": 52.0, "val_loss": 1159690.4296875, "val_acc": 52.0}
{"epoch": 29, "training_loss": 47725498.0, "training_acc": 47.0, "val_loss": 16782517.1875, "val_acc": 52.0}
{"epoch": 30, "training_loss": 59043100.5, "training_acc": 53.0, "val_loss": 3067264.84375, "val_acc": 52.0}
{"epoch": 31, "training_loss": 9609725.546875, "training_acc": 53.0, "val_loss": 906548.33984375, "val_acc": 52.0}
{"epoch": 32, "training_loss": 15454127.75, "training_acc": 43.0, "val_loss": 2715660.15625, "val_acc": 52.0}
{"epoch": 33, "training_loss": 10948178.0625, "training_acc": 53.0, "val_loss": 9546292.96875, "val_acc": 48.0}
{"epoch": 34, "training_loss": 25510592.484375, "training_acc": 49.0, "val_loss": 961647.4609375, "val_acc": 52.0}
{"epoch": 35, "training_loss": 3557030.5625, "training_acc": 53.0, "val_loss": 463310.107421875, "val_acc": 48.0}
{"epoch": 36, "training_loss": 62823996.75, "training_acc": 48.0, "val_loss": 245866.6748046875, "val_acc": 52.0}
{"epoch": 37, "training_loss": 1375851.265625, "training_acc": 53.0, "val_loss": 940417.08984375, "val_acc": 52.0}
{"epoch": 38, "training_loss": 3369884.578125, "training_acc": 51.0, "val_loss": 5212739.0625, "val_acc": 52.0}
{"epoch": 39, "training_loss": 12699571.087890625, "training_acc": 53.0, "val_loss": 8275978.90625, "val_acc": 48.0}
{"epoch": 40, "training_loss": 24108283.546875, "training_acc": 47.0, "val_loss": 3534030.078125, "val_acc": 52.0}
{"epoch": 41, "training_loss": 11563911.78125, "training_acc": 53.0, "val_loss": 800078.61328125, "val_acc": 52.0}
{"epoch": 42, "training_loss": 2388738.76953125, "training_acc": 49.0, "val_loss": 780279.931640625, "val_acc": 48.0}
{"epoch": 43, "training_loss": 116407822.375, "training_acc": 46.0, "val_loss": 13230092.1875, "val_acc": 52.0}
{"epoch": 44, "training_loss": 47651752.5, "training_acc": 53.0, "val_loss": 7156412.5, "val_acc": 48.0}
{"epoch": 45, "training_loss": 54187033.25, "training_acc": 49.0, "val_loss": 348464025.0, "val_acc": 48.0}
{"epoch": 46, "training_loss": 951470105.75, "training_acc": 47.0, "val_loss": 26145096.875, "val_acc": 52.0}
{"epoch": 47, "training_loss": 72797857.125, "training_acc": 53.0, "val_loss": 31569728.125, "val_acc": 48.0}
{"epoch": 48, "training_loss": 97746269.0, "training_acc": 47.0, "val_loss": 22922346.875, "val_acc": 52.0}
{"epoch": 49, "training_loss": 71971552.375, "training_acc": 53.0, "val_loss": 5326805.859375, "val_acc": 52.0}
{"epoch": 50, "training_loss": 15761119.53125, "training_acc": 53.0, "val_loss": 994139.35546875, "val_acc": 48.0}
{"epoch": 51, "training_loss": 3965145.78125, "training_acc": 47.0, "val_loss": 381149.0966796875, "val_acc": 48.0}
{"epoch": 52, "training_loss": 1338026.25390625, "training_acc": 47.0, "val_loss": 74156.57348632812, "val_acc": 52.0}
{"epoch": 53, "training_loss": 368306.53125, "training_acc": 57.0, "val_loss": 1909.952163696289, "val_acc": 52.0}
{"epoch": 54, "training_loss": 1620076.91015625, "training_acc": 58.0, "val_loss": 18661.656188964844, "val_acc": 48.0}
{"epoch": 55, "training_loss": 418018.8671875, "training_acc": 50.0, "val_loss": 159785.63232421875, "val_acc": 48.0}
{"epoch": 56, "training_loss": 956567.80078125, "training_acc": 43.0, "val_loss": 318775.146484375, "val_acc": 48.0}
{"epoch": 57, "training_loss": 1295184.59765625, "training_acc": 43.0, "val_loss": 195537.8662109375, "val_acc": 48.0}
{"epoch": 58, "training_loss": 883435.73828125, "training_acc": 51.0, "val_loss": 63452.1728515625, "val_acc": 48.0}
{"epoch": 59, "training_loss": 768129.3515625, "training_acc": 51.0, "val_loss": 146027.25830078125, "val_acc": 52.0}
{"epoch": 60, "training_loss": 1120951.375, "training_acc": 57.0, "val_loss": 643688.623046875, "val_acc": 48.0}
{"epoch": 61, "training_loss": 1885576.76953125, "training_acc": 53.0, "val_loss": 301740.9423828125, "val_acc": 52.0}
{"epoch": 62, "training_loss": 1157551.140625, "training_acc": 55.0, "val_loss": 172647.27783203125, "val_acc": 48.0}
{"epoch": 63, "training_loss": 1182195.28125, "training_acc": 49.0, "val_loss": 603547.36328125, "val_acc": 52.0}
{"epoch": 64, "training_loss": 1871020.34765625, "training_acc": 53.0, "val_loss": 153624.08447265625, "val_acc": 48.0}
{"epoch": 65, "training_loss": 455511.587890625, "training_acc": 47.0, "val_loss": 886061.328125, "val_acc": 52.0}
{"epoch": 66, "training_loss": 3094103.6640625, "training_acc": 53.0, "val_loss": 34488.91296386719, "val_acc": 52.0}
{"epoch": 67, "training_loss": 974541.9296875, "training_acc": 47.0, "val_loss": 562375.244140625, "val_acc": 48.0}
{"epoch": 68, "training_loss": 1985781.984375, "training_acc": 47.0, "val_loss": 67508.80126953125, "val_acc": 48.0}
{"epoch": 69, "training_loss": 595768.109375, "training_acc": 47.0, "val_loss": 340602.5634765625, "val_acc": 52.0}
{"epoch": 70, "training_loss": 1095575.310546875, "training_acc": 53.0, "val_loss": 58701.849365234375, "val_acc": 48.0}
{"epoch": 71, "training_loss": 236841.412109375, "training_acc": 47.0, "val_loss": 165837.68310546875, "val_acc": 52.0}
{"epoch": 72, "training_loss": 684979.828125, "training_acc": 53.0, "val_loss": 51256.53076171875, "val_acc": 52.0}
