"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 1061.865821838379, "training_acc": 50.0, "val_loss": 8.10902710386688e+16, "val_acc": 56.0}
{"epoch": 1, "training_loss": 2.1538446664599533e+17, "training_acc": 52.0, "val_loss": 194074512.5, "val_acc": 56.0}
{"epoch": 2, "training_loss": 3928336448.0, "training_acc": 54.0, "val_loss": 225844150.0, "val_acc": 44.0}
{"epoch": 3, "training_loss": 702923763.0, "training_acc": 48.0, "val_loss": 42025650.0, "val_acc": 44.0}
{"epoch": 4, "training_loss": 219007898.0, "training_acc": 50.0, "val_loss": 1339545800.0, "val_acc": 44.0}
{"epoch": 5, "training_loss": 3685629896.0, "training_acc": 48.0, "val_loss": 1262539468800.0, "val_acc": 56.0}
{"epoch": 6, "training_loss": 3481508476412.0, "training_acc": 52.0, "val_loss": 18113410.9375, "val_acc": 44.0}
{"epoch": 7, "training_loss": 385534628.0, "training_acc": 48.0, "val_loss": 213778375.0, "val_acc": 44.0}
{"epoch": 8, "training_loss": 1059921308.0, "training_acc": 42.0, "val_loss": 1979157000.0, "val_acc": 44.0}
{"epoch": 9, "training_loss": 5781972968.0, "training_acc": 48.0, "val_loss": 300664700.0, "val_acc": 44.0}
{"epoch": 10, "training_loss": 856037470.0, "training_acc": 48.0, "val_loss": 32136812.5, "val_acc": 44.0}
{"epoch": 11, "training_loss": 104341948.5, "training_acc": 48.0, "val_loss": 188213600.0, "val_acc": 56.0}
{"epoch": 12, "training_loss": 512368513.0, "training_acc": 54.0, "val_loss": 14333467.1875, "val_acc": 44.0}
{"epoch": 13, "training_loss": 60405506.75, "training_acc": 52.0, "val_loss": 25023188800.0, "val_acc": 44.0}
{"epoch": 14, "training_loss": 56774535314.0, "training_acc": 52.0, "val_loss": 2493209800.0, "val_acc": 44.0}
{"epoch": 15, "training_loss": 5683792114.0, "training_acc": 54.0, "val_loss": 317833925.0, "val_acc": 56.0}
{"epoch": 16, "training_loss": 1071050890.0, "training_acc": 52.0, "val_loss": 18354929600.0, "val_acc": 44.0}
{"epoch": 17, "training_loss": 49629982528.0, "training_acc": 48.0, "val_loss": 363881250.0, "val_acc": 56.0}
{"epoch": 18, "training_loss": 1353614912.0, "training_acc": 52.0, "val_loss": 46776912.5, "val_acc": 56.0}
{"epoch": 19, "training_loss": 156090527.25, "training_acc": 52.0, "val_loss": 9076573.4375, "val_acc": 56.0}
{"epoch": 20, "training_loss": 32334413.0, "training_acc": 52.0, "val_loss": 3135021.484375, "val_acc": 56.0}
{"epoch": 21, "training_loss": 28546284.25, "training_acc": 52.0, "val_loss": 609191.9921875, "val_acc": 56.0}
{"epoch": 22, "training_loss": 72456591.75, "training_acc": 52.0, "val_loss": 7742323916800.0, "val_acc": 44.0}
{"epoch": 23, "training_loss": 19085173475024.0, "training_acc": 46.0, "val_loss": 305502650.0, "val_acc": 56.0}
{"epoch": 24, "training_loss": 1383622324.0, "training_acc": 52.0, "val_loss": 70363512.5, "val_acc": 56.0}
{"epoch": 25, "training_loss": 1675168416.0, "training_acc": 48.0, "val_loss": 2351375600.0, "val_acc": 56.0}
{"epoch": 26, "training_loss": 7963471936.0, "training_acc": 52.0, "val_loss": 1465159900.0, "val_acc": 44.0}
{"epoch": 27, "training_loss": 11526213440.0, "training_acc": 48.0, "val_loss": 2794397200.0, "val_acc": 44.0}
{"epoch": 28, "training_loss": 7977766144.0, "training_acc": 48.0, "val_loss": 3677821200.0, "val_acc": 44.0}
{"epoch": 29, "training_loss": 8580970308.0, "training_acc": 48.0, "val_loss": 1196193800.0, "val_acc": 56.0}
{"epoch": 30, "training_loss": 5897571616768.0, "training_acc": 59.0, "val_loss": 423564650.0, "val_acc": 56.0}
{"epoch": 31, "training_loss": 17013798400.0, "training_acc": 52.0, "val_loss": 1378991206400.0, "val_acc": 44.0}
{"epoch": 32, "training_loss": 11232415645696.0, "training_acc": 48.0, "val_loss": 191368550.0, "val_acc": 56.0}
{"epoch": 33, "training_loss": 1205255360.0, "training_acc": 52.0, "val_loss": 5345357600.0, "val_acc": 56.0}
{"epoch": 34, "training_loss": 16895791584.0, "training_acc": 54.0, "val_loss": 161863303168000.0, "val_acc": 44.0}
{"epoch": 35, "training_loss": 376381371260928.0, "training_acc": 50.0, "val_loss": 918757376000.0, "val_acc": 56.0}
{"epoch": 36, "training_loss": 13610899210240.0, "training_acc": 52.0, "val_loss": 2279540200.0, "val_acc": 56.0}
{"epoch": 37, "training_loss": 1722040229388288.0, "training_acc": 43.0, "val_loss": 18049456537600.0, "val_acc": 56.0}
{"epoch": 38, "training_loss": 72845819248640.0, "training_acc": 52.0, "val_loss": 4887517593600.0, "val_acc": 56.0}
{"epoch": 39, "training_loss": 13767406206976.0, "training_acc": 52.0, "val_loss": 452438169600.0, "val_acc": 56.0}
{"epoch": 40, "training_loss": 1536230950912.0, "training_acc": 52.0, "val_loss": 1681525760000.0, "val_acc": 44.0}
