"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 770.2032089233398, "training_acc": 41.0, "val_loss": 5.17997483524096e+17, "val_acc": 48.0}
{"epoch": 1, "training_loss": 1.213216248372935e+18, "training_acc": 55.0, "val_loss": 202603.7109375, "val_acc": 48.0}
{"epoch": 2, "training_loss": 126888086.375, "training_acc": 47.0, "val_loss": 188048250.0, "val_acc": 52.0}
{"epoch": 3, "training_loss": 528870536.5, "training_acc": 53.0, "val_loss": 4074448.4375, "val_acc": 48.0}
{"epoch": 4, "training_loss": 10616321.4296875, "training_acc": 47.0, "val_loss": 1920560.9375, "val_acc": 52.0}
{"epoch": 5, "training_loss": 6277320.0625, "training_acc": 49.0, "val_loss": 453951.123046875, "val_acc": 48.0}
{"epoch": 6, "training_loss": 1475819.412109375, "training_acc": 47.0, "val_loss": 38953.240966796875, "val_acc": 52.0}
{"epoch": 7, "training_loss": 334084.791015625, "training_acc": 53.0, "val_loss": 14823.991394042969, "val_acc": 48.0}
{"epoch": 8, "training_loss": 61603.76025390625, "training_acc": 61.0, "val_loss": 20514.768981933594, "val_acc": 52.0}
{"epoch": 9, "training_loss": 255012.775390625, "training_acc": 45.0, "val_loss": 65660.19897460938, "val_acc": 52.0}
{"epoch": 10, "training_loss": 495725.0078125, "training_acc": 49.0, "val_loss": 17491.107177734375, "val_acc": 52.0}
{"epoch": 11, "training_loss": 272681.00390625, "training_acc": 43.0, "val_loss": 10392.061614990234, "val_acc": 48.0}
{"epoch": 12, "training_loss": 59225.046142578125, "training_acc": 57.0, "val_loss": 28320.318603515625, "val_acc": 48.0}
{"epoch": 13, "training_loss": 260070.328125, "training_acc": 51.0, "val_loss": 37309.136962890625, "val_acc": 52.0}
{"epoch": 14, "training_loss": 105304.14736938477, "training_acc": 53.0, "val_loss": 43873.00720214844, "val_acc": 48.0}
{"epoch": 15, "training_loss": 121813.67596435547, "training_acc": 47.0, "val_loss": 132540.41748046875, "val_acc": 48.0}
{"epoch": 16, "training_loss": 351410.4768066406, "training_acc": 47.0, "val_loss": 3469.938278198242, "val_acc": 52.0}
{"epoch": 17, "training_loss": 11014.6806640625, "training_acc": 53.0, "val_loss": 6299.917984008789, "val_acc": 48.0}
{"epoch": 18, "training_loss": 17863.623889923096, "training_acc": 46.0, "val_loss": 13340.4541015625, "val_acc": 52.0}
{"epoch": 19, "training_loss": 42892.80847167969, "training_acc": 53.0, "val_loss": 2262.67032623291, "val_acc": 48.0}
{"epoch": 20, "training_loss": 160443.19091796875, "training_acc": 49.0, "val_loss": 1067.7714347839355, "val_acc": 52.0}
{"epoch": 21, "training_loss": 8656.2255859375, "training_acc": 47.0, "val_loss": 2115.5961990356445, "val_acc": 52.0}
{"epoch": 22, "training_loss": 7550.2197265625, "training_acc": 49.0, "val_loss": 130.4787278175354, "val_acc": 48.0}
{"epoch": 23, "training_loss": 4339.825347900391, "training_acc": 47.0, "val_loss": 5139.487075805664, "val_acc": 48.0}
{"epoch": 24, "training_loss": 13882.68920326233, "training_acc": 47.0, "val_loss": 3774.656295776367, "val_acc": 52.0}
{"epoch": 25, "training_loss": 14216.495208740234, "training_acc": 53.0, "val_loss": 2719.4183349609375, "val_acc": 52.0}
{"epoch": 26, "training_loss": 7173.4216232299805, "training_acc": 57.0, "val_loss": 176.90999507904053, "val_acc": 52.0}
{"epoch": 27, "training_loss": 2313.138473510742, "training_acc": 48.0, "val_loss": 250.7392168045044, "val_acc": 52.0}
{"epoch": 28, "training_loss": 941.8906173706055, "training_acc": 55.0, "val_loss": 419.92902755737305, "val_acc": 52.0}
{"epoch": 29, "training_loss": 1507.1752471923828, "training_acc": 45.0, "val_loss": 548.771333694458, "val_acc": 52.0}
{"epoch": 30, "training_loss": 1840.5634956359863, "training_acc": 53.0, "val_loss": 247.70898818969727, "val_acc": 48.0}
{"epoch": 31, "training_loss": 881.7746906280518, "training_acc": 47.0, "val_loss": 434.78074073791504, "val_acc": 52.0}
{"epoch": 32, "training_loss": 1564.2869491577148, "training_acc": 53.0, "val_loss": 189.00645971298218, "val_acc": 48.0}
{"epoch": 33, "training_loss": 734.0327835083008, "training_acc": 47.0, "val_loss": 314.32788372039795, "val_acc": 52.0}
{"epoch": 34, "training_loss": 1208.284595489502, "training_acc": 53.0, "val_loss": 80.68272471427917, "val_acc": 48.0}
{"epoch": 35, "training_loss": 386.12592697143555, "training_acc": 47.0, "val_loss": 275.242280960083, "val_acc": 52.0}
{"epoch": 36, "training_loss": 1158.8874702453613, "training_acc": 53.0, "val_loss": 111.26376390457153, "val_acc": 52.0}
{"epoch": 37, "training_loss": 764.4693336486816, "training_acc": 55.0, "val_loss": 511.9679927825928, "val_acc": 48.0}
{"epoch": 38, "training_loss": 1670.7553329467773, "training_acc": 47.0, "val_loss": 190.25276899337769, "val_acc": 52.0}
{"epoch": 39, "training_loss": 915.266902923584, "training_acc": 53.0, "val_loss": 219.02806758880615, "val_acc": 52.0}
{"epoch": 40, "training_loss": 777.038236618042, "training_acc": 45.0, "val_loss": 125.84986686706543, "val_acc": 48.0}
{"epoch": 41, "training_loss": 488.8909320831299, "training_acc": 43.0, "val_loss": 49.05516803264618, "val_acc": 52.0}
{"epoch": 42, "training_loss": 479.7315444946289, "training_acc": 47.0, "val_loss": 151.66443586349487, "val_acc": 48.0}
{"epoch": 43, "training_loss": 614.2013263702393, "training_acc": 45.0, "val_loss": 151.87172889709473, "val_acc": 52.0}
{"epoch": 44, "training_loss": 412.50753903388977, "training_acc": 48.0, "val_loss": 100.78884363174438, "val_acc": 48.0}
{"epoch": 45, "training_loss": 291.1889908313751, "training_acc": 47.0, "val_loss": 93.44779849052429, "val_acc": 52.0}
{"epoch": 46, "training_loss": 257.0711965560913, "training_acc": 53.0, "val_loss": 179.54431772232056, "val_acc": 48.0}
{"epoch": 47, "training_loss": 598.8019552230835, "training_acc": 47.0, "val_loss": 163.75170946121216, "val_acc": 52.0}
{"epoch": 48, "training_loss": 622.024377822876, "training_acc": 53.0, "val_loss": 41.67904555797577, "val_acc": 48.0}
{"epoch": 49, "training_loss": 180.82809829711914, "training_acc": 47.0, "val_loss": 138.15176486968994, "val_acc": 52.0}
{"epoch": 50, "training_loss": 518.3227024078369, "training_acc": 53.0, "val_loss": 17.70331859588623, "val_acc": 48.0}
{"epoch": 51, "training_loss": 135.34040355682373, "training_acc": 51.0, "val_loss": 17.629139125347137, "val_acc": 52.0}
{"epoch": 52, "training_loss": 124.01214790344238, "training_acc": 55.0, "val_loss": 31.723758578300476, "val_acc": 52.0}
{"epoch": 53, "training_loss": 157.78792190551758, "training_acc": 60.0, "val_loss": 62.21230626106262, "val_acc": 48.0}
{"epoch": 54, "training_loss": 266.47913360595703, "training_acc": 51.0, "val_loss": 91.70107245445251, "val_acc": 52.0}
{"epoch": 55, "training_loss": 247.8669090270996, "training_acc": 53.0, "val_loss": 49.65432584285736, "val_acc": 48.0}
{"epoch": 56, "training_loss": 222.32098484039307, "training_acc": 43.0, "val_loss": 20.152823626995087, "val_acc": 52.0}
{"epoch": 57, "training_loss": 97.19050455093384, "training_acc": 58.0, "val_loss": 17.682024836540222, "val_acc": 48.0}
{"epoch": 58, "training_loss": 189.15670108795166, "training_acc": 37.0, "val_loss": 18.04909110069275, "val_acc": 48.0}
{"epoch": 59, "training_loss": 115.78191566467285, "training_acc": 50.0, "val_loss": 26.10262930393219, "val_acc": 52.0}
{"epoch": 60, "training_loss": 97.86714887619019, "training_acc": 52.0, "val_loss": 47.201499342918396, "val_acc": 48.0}
{"epoch": 61, "training_loss": 165.33159613609314, "training_acc": 44.0, "val_loss": 48.39423894882202, "val_acc": 52.0}
{"epoch": 62, "training_loss": 144.01087379455566, "training_acc": 53.0, "val_loss": 75.29131770133972, "val_acc": 48.0}
{"epoch": 63, "training_loss": 297.77188777923584, "training_acc": 47.0, "val_loss": 54.97971773147583, "val_acc": 52.0}
{"epoch": 64, "training_loss": 233.9444456100464, "training_acc": 53.0, "val_loss": 20.211230218410492, "val_acc": 52.0}
{"epoch": 65, "training_loss": 150.08056735992432, "training_acc": 52.0, "val_loss": 61.32938861846924, "val_acc": 48.0}
{"epoch": 66, "training_loss": 233.84512567520142, "training_acc": 47.0, "val_loss": 52.18479633331299, "val_acc": 52.0}
{"epoch": 67, "training_loss": 163.2588505744934, "training_acc": 51.0, "val_loss": 27.446871995925903, "val_acc": 48.0}
{"epoch": 68, "training_loss": 125.3258113861084, "training_acc": 51.0, "val_loss": 28.464120626449585, "val_acc": 52.0}
{"epoch": 69, "training_loss": 125.94806098937988, "training_acc": 51.0, "val_loss": 26.43437683582306, "val_acc": 52.0}
{"epoch": 70, "training_loss": 117.78562211990356, "training_acc": 53.0, "val_loss": 18.54463219642639, "val_acc": 52.0}
