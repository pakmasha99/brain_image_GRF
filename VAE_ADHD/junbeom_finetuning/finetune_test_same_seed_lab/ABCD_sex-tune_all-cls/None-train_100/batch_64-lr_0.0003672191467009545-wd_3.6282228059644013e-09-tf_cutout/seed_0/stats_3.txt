"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.27111482620239, "training_acc": 50.0, "val_loss": 17.347773909568787, "val_acc": 52.0}
{"epoch": 1, "training_loss": 70.07548093795776, "training_acc": 53.0, "val_loss": 17.316533625125885, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.3550820350647, "training_acc": 53.0, "val_loss": 17.360320687294006, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.79406809806824, "training_acc": 43.0, "val_loss": 17.367957532405853, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.51954913139343, "training_acc": 47.0, "val_loss": 17.309285700321198, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.09933018684387, "training_acc": 53.0, "val_loss": 17.35716462135315, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.24901676177979, "training_acc": 53.0, "val_loss": 17.328868806362152, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.28013253211975, "training_acc": 53.0, "val_loss": 17.310839891433716, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.17747330665588, "training_acc": 53.0, "val_loss": 17.319616675376892, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.22951054573059, "training_acc": 53.0, "val_loss": 17.31063276529312, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.21848344802856, "training_acc": 53.0, "val_loss": 17.325469851493835, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.1570692062378, "training_acc": 53.0, "val_loss": 17.31038987636566, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.91724348068237, "training_acc": 41.0, "val_loss": 17.310328781604767, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.03398656845093, "training_acc": 53.0, "val_loss": 17.369821667671204, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.43217468261719, "training_acc": 53.0, "val_loss": 17.430584132671356, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.60422992706299, "training_acc": 53.0, "val_loss": 17.33063906431198, "val_acc": 52.0}
{"epoch": 16, "training_loss": 68.98655128479004, "training_acc": 53.0, "val_loss": 17.339886724948883, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.76536345481873, "training_acc": 47.0, "val_loss": 17.36331582069397, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.35973477363586, "training_acc": 47.0, "val_loss": 17.309212684631348, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.16450881958008, "training_acc": 53.0, "val_loss": 17.371174693107605, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.27661490440369, "training_acc": 53.0, "val_loss": 17.377331852912903, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.35483980178833, "training_acc": 53.0, "val_loss": 17.349788546562195, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.22920966148376, "training_acc": 53.0, "val_loss": 17.30876863002777, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.30068516731262, "training_acc": 49.0, "val_loss": 17.352135479450226, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.44021677970886, "training_acc": 47.0, "val_loss": 17.332056164741516, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.4137454032898, "training_acc": 45.0, "val_loss": 17.30847954750061, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.13313245773315, "training_acc": 53.0, "val_loss": 17.317138612270355, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.12484765052795, "training_acc": 53.0, "val_loss": 17.337915301322937, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.16858673095703, "training_acc": 53.0, "val_loss": 17.359694838523865, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.23803281784058, "training_acc": 53.0, "val_loss": 17.361153662204742, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.21171832084656, "training_acc": 53.0, "val_loss": 17.325568199157715, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.09905648231506, "training_acc": 53.0, "val_loss": 17.30894446372986, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.43155550956726, "training_acc": 45.0, "val_loss": 17.323169112205505, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.29013514518738, "training_acc": 53.0, "val_loss": 17.30877459049225, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.15420436859131, "training_acc": 53.0, "val_loss": 17.316368222236633, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.16079258918762, "training_acc": 53.0, "val_loss": 17.330962419509888, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.17925190925598, "training_acc": 53.0, "val_loss": 17.361225187778473, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.26258945465088, "training_acc": 53.0, "val_loss": 17.348094284534454, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.19518685340881, "training_acc": 53.0, "val_loss": 17.332500219345093, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.32334637641907, "training_acc": 53.0, "val_loss": 17.314352095127106, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.21875309944153, "training_acc": 53.0, "val_loss": 17.31993407011032, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.35360932350159, "training_acc": 47.0, "val_loss": 17.322909832000732, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.76368236541748, "training_acc": 53.0, "val_loss": 17.308665812015533, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.16343760490417, "training_acc": 53.0, "val_loss": 17.30904132127762, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.19565320014954, "training_acc": 53.0, "val_loss": 17.310601472854614, "val_acc": 52.0}
