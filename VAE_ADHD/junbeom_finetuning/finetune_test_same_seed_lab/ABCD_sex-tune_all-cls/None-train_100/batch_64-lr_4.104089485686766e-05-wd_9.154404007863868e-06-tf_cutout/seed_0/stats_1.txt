"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.30928778648376, "training_acc": 53.0, "val_loss": 17.313598096370697, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.35426640510559, "training_acc": 53.0, "val_loss": 17.32816994190216, "val_acc": 52.0}
{"epoch": 2, "training_loss": 70.48320508003235, "training_acc": 45.0, "val_loss": 17.37792193889618, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.23337483406067, "training_acc": 51.0, "val_loss": 17.445217072963715, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.6776192188263, "training_acc": 53.0, "val_loss": 17.53503829240799, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.6668598651886, "training_acc": 53.0, "val_loss": 17.32834279537201, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.89501214027405, "training_acc": 53.0, "val_loss": 17.312903702259064, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.24678158760071, "training_acc": 51.0, "val_loss": 17.31603443622589, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.22018265724182, "training_acc": 53.0, "val_loss": 17.301970720291138, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.03978943824768, "training_acc": 53.0, "val_loss": 17.322000861167908, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.11060786247253, "training_acc": 53.0, "val_loss": 17.38058030605316, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.54782485961914, "training_acc": 53.0, "val_loss": 17.413504421710968, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.42577195167542, "training_acc": 53.0, "val_loss": 17.354610562324524, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.18729901313782, "training_acc": 53.0, "val_loss": 17.31593906879425, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.23661851882935, "training_acc": 53.0, "val_loss": 17.308177053928375, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.26388311386108, "training_acc": 53.0, "val_loss": 17.318159341812134, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.21734189987183, "training_acc": 53.0, "val_loss": 17.316386103630066, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.2036600112915, "training_acc": 53.0, "val_loss": 17.31051653623581, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.24196481704712, "training_acc": 53.0, "val_loss": 17.30736941099167, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.23953628540039, "training_acc": 53.0, "val_loss": 17.317867279052734, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.22451376914978, "training_acc": 53.0, "val_loss": 17.33105033636093, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.10113716125488, "training_acc": 53.0, "val_loss": 17.32509583234787, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.16316223144531, "training_acc": 53.0, "val_loss": 17.315000295639038, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.06859159469604, "training_acc": 53.0, "val_loss": 17.312124371528625, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.07367539405823, "training_acc": 53.0, "val_loss": 17.31068789958954, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.07009601593018, "training_acc": 53.0, "val_loss": 17.31230318546295, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.09836983680725, "training_acc": 53.0, "val_loss": 17.314530909061432, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.06333208084106, "training_acc": 53.0, "val_loss": 17.31906682252884, "val_acc": 52.0}
