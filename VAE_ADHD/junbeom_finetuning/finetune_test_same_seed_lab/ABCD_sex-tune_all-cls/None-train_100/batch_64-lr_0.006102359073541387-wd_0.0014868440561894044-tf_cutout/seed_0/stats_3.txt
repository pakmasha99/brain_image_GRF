"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 570.5802574157715, "training_acc": 38.0, "val_loss": 355449400.0, "val_acc": 52.0}
{"epoch": 1, "training_loss": 883464238.5230408, "training_acc": 51.0, "val_loss": 52504.0283203125, "val_acc": 48.0}
{"epoch": 2, "training_loss": 140278.3436279297, "training_acc": 55.0, "val_loss": 45272.198486328125, "val_acc": 48.0}
{"epoch": 3, "training_loss": 145620.9541015625, "training_acc": 47.0, "val_loss": 1976.9100189208984, "val_acc": 48.0}
{"epoch": 4, "training_loss": 5548.638023376465, "training_acc": 47.0, "val_loss": 91.10240936279297, "val_acc": 52.0}
{"epoch": 5, "training_loss": 2090.659713745117, "training_acc": 51.0, "val_loss": 91.55807495117188, "val_acc": 48.0}
{"epoch": 6, "training_loss": 1844.4491577148438, "training_acc": 39.0, "val_loss": 295.48370838165283, "val_acc": 52.0}
{"epoch": 7, "training_loss": 1180.7485809326172, "training_acc": 49.0, "val_loss": 80.07611036300659, "val_acc": 48.0}
{"epoch": 8, "training_loss": 387.9277992248535, "training_acc": 53.0, "val_loss": 42.781028151512146, "val_acc": 48.0}
{"epoch": 9, "training_loss": 358.98348808288574, "training_acc": 37.0, "val_loss": 34.87110137939453, "val_acc": 52.0}
{"epoch": 10, "training_loss": 131.91202449798584, "training_acc": 55.0, "val_loss": 33.03905427455902, "val_acc": 48.0}
{"epoch": 11, "training_loss": 122.26025724411011, "training_acc": 51.0, "val_loss": 19.215914607048035, "val_acc": 52.0}
{"epoch": 12, "training_loss": 77.18479871749878, "training_acc": 51.0, "val_loss": 19.265972077846527, "val_acc": 52.0}
{"epoch": 13, "training_loss": 389.91223526000977, "training_acc": 43.0, "val_loss": 102.29653120040894, "val_acc": 52.0}
{"epoch": 14, "training_loss": 280.9160985946655, "training_acc": 53.0, "val_loss": 17.31378138065338, "val_acc": 52.0}
{"epoch": 15, "training_loss": 87.45875930786133, "training_acc": 47.0, "val_loss": 22.557757794857025, "val_acc": 52.0}
{"epoch": 16, "training_loss": 88.48704099655151, "training_acc": 53.0, "val_loss": 18.687215447425842, "val_acc": 52.0}
{"epoch": 17, "training_loss": 73.87943029403687, "training_acc": 49.0, "val_loss": 17.909659445285797, "val_acc": 52.0}
{"epoch": 18, "training_loss": 71.57015919685364, "training_acc": 47.0, "val_loss": 17.451034486293793, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.32676982879639, "training_acc": 53.0, "val_loss": 17.333529889583588, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.10746836662292, "training_acc": 58.0, "val_loss": 20.87794989347458, "val_acc": 52.0}
{"epoch": 21, "training_loss": 79.09848713874817, "training_acc": 49.0, "val_loss": 18.125489354133606, "val_acc": 52.0}
{"epoch": 22, "training_loss": 72.18774819374084, "training_acc": 47.0, "val_loss": 17.312394082546234, "val_acc": 52.0}
{"epoch": 23, "training_loss": 68.65237474441528, "training_acc": 53.0, "val_loss": 18.1364968419075, "val_acc": 52.0}
{"epoch": 24, "training_loss": 71.61817860603333, "training_acc": 53.0, "val_loss": 17.424307763576508, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.35979175567627, "training_acc": 53.0, "val_loss": 17.447540163993835, "val_acc": 52.0}
{"epoch": 26, "training_loss": 71.99996399879456, "training_acc": 47.0, "val_loss": 17.717984318733215, "val_acc": 52.0}
{"epoch": 27, "training_loss": 71.05445790290833, "training_acc": 45.0, "val_loss": 17.349979281425476, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.36572217941284, "training_acc": 53.0, "val_loss": 17.411136627197266, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.76369524002075, "training_acc": 49.0, "val_loss": 17.312610149383545, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.63888430595398, "training_acc": 53.0, "val_loss": 17.371097207069397, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.18966317176819, "training_acc": 53.0, "val_loss": 17.314361035823822, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.26807975769043, "training_acc": 53.0, "val_loss": 17.313510179519653, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.25574135780334, "training_acc": 53.0, "val_loss": 17.354559898376465, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.66560292243958, "training_acc": 45.0, "val_loss": 17.322848737239838, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.2869758605957, "training_acc": 52.0, "val_loss": 17.320366203784943, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.32931900024414, "training_acc": 53.0, "val_loss": 17.32289046049118, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.32537817955017, "training_acc": 53.0, "val_loss": 17.335213720798492, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.12842059135437, "training_acc": 53.0, "val_loss": 17.39770919084549, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.35008955001831, "training_acc": 53.0, "val_loss": 17.376472055912018, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.30056762695312, "training_acc": 53.0, "val_loss": 17.312610149383545, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.18871426582336, "training_acc": 53.0, "val_loss": 17.31189638376236, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.38799810409546, "training_acc": 53.0, "val_loss": 17.31378585100174, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.10740733146667, "training_acc": 53.0, "val_loss": 17.311301827430725, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.29899287223816, "training_acc": 53.0, "val_loss": 17.311792075634003, "val_acc": 52.0}
{"epoch": 45, "training_loss": 69.2358148097992, "training_acc": 53.0, "val_loss": 17.321787774562836, "val_acc": 52.0}
{"epoch": 46, "training_loss": 69.60867762565613, "training_acc": 53.0, "val_loss": 17.311641573905945, "val_acc": 52.0}
{"epoch": 47, "training_loss": 69.21060228347778, "training_acc": 53.0, "val_loss": 17.315639555454254, "val_acc": 52.0}
{"epoch": 48, "training_loss": 69.25814127922058, "training_acc": 53.0, "val_loss": 17.328090965747833, "val_acc": 52.0}
{"epoch": 49, "training_loss": 69.2579095363617, "training_acc": 53.0, "val_loss": 17.311197519302368, "val_acc": 52.0}
{"epoch": 50, "training_loss": 69.16156339645386, "training_acc": 53.0, "val_loss": 17.324480414390564, "val_acc": 52.0}
{"epoch": 51, "training_loss": 69.25431370735168, "training_acc": 53.0, "val_loss": 17.39453822374344, "val_acc": 52.0}
{"epoch": 52, "training_loss": 69.38691830635071, "training_acc": 53.0, "val_loss": 17.385224997997284, "val_acc": 52.0}
{"epoch": 53, "training_loss": 69.2971363067627, "training_acc": 53.0, "val_loss": 17.320413887500763, "val_acc": 52.0}
{"epoch": 54, "training_loss": 69.10242438316345, "training_acc": 53.0, "val_loss": 17.312464118003845, "val_acc": 52.0}
{"epoch": 55, "training_loss": 69.43846249580383, "training_acc": 45.0, "val_loss": 17.329835891723633, "val_acc": 52.0}
{"epoch": 56, "training_loss": 69.30868291854858, "training_acc": 48.0, "val_loss": 17.311835289001465, "val_acc": 52.0}
{"epoch": 57, "training_loss": 69.10484623908997, "training_acc": 53.0, "val_loss": 17.33640283346176, "val_acc": 52.0}
{"epoch": 58, "training_loss": 69.33022809028625, "training_acc": 53.0, "val_loss": 17.394904792308807, "val_acc": 52.0}
{"epoch": 59, "training_loss": 69.36667370796204, "training_acc": 53.0, "val_loss": 17.37200915813446, "val_acc": 52.0}
{"epoch": 60, "training_loss": 69.32099175453186, "training_acc": 53.0, "val_loss": 17.349496483802795, "val_acc": 52.0}
{"epoch": 61, "training_loss": 69.20220232009888, "training_acc": 53.0, "val_loss": 17.336581647396088, "val_acc": 52.0}
{"epoch": 62, "training_loss": 69.1343605518341, "training_acc": 53.0, "val_loss": 17.316310107707977, "val_acc": 52.0}
{"epoch": 63, "training_loss": 69.09929299354553, "training_acc": 53.0, "val_loss": 17.312298715114594, "val_acc": 52.0}
{"epoch": 64, "training_loss": 69.2243926525116, "training_acc": 53.0, "val_loss": 17.32287108898163, "val_acc": 52.0}
{"epoch": 65, "training_loss": 69.26737356185913, "training_acc": 53.0, "val_loss": 17.31610894203186, "val_acc": 52.0}
{"epoch": 66, "training_loss": 69.15822625160217, "training_acc": 53.0, "val_loss": 17.311453819274902, "val_acc": 52.0}
{"epoch": 67, "training_loss": 69.04061079025269, "training_acc": 53.0, "val_loss": 17.342999577522278, "val_acc": 52.0}
{"epoch": 68, "training_loss": 69.21359968185425, "training_acc": 53.0, "val_loss": 17.415280640125275, "val_acc": 52.0}
