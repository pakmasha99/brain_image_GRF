"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 229.39622688293457, "training_acc": 53.0, "val_loss": 1.3721119135629312e+18, "val_acc": 48.0}
{"epoch": 1, "training_loss": 3.520084002424985e+18, "training_acc": 47.0, "val_loss": 121842.02880859375, "val_acc": 52.0}
{"epoch": 2, "training_loss": 12953442.9375, "training_acc": 49.0, "val_loss": 1074331.4453125, "val_acc": 52.0}
{"epoch": 3, "training_loss": 3559152.0, "training_acc": 53.0, "val_loss": 482601.07421875, "val_acc": 52.0}
{"epoch": 4, "training_loss": 2150097.8515625, "training_acc": 57.0, "val_loss": 1125912.5, "val_acc": 52.0}
{"epoch": 5, "training_loss": 3135331.861328125, "training_acc": 53.0, "val_loss": 20508.924865722656, "val_acc": 48.0}
{"epoch": 6, "training_loss": 1107631.359375, "training_acc": 51.0, "val_loss": 372582.5439453125, "val_acc": 52.0}
{"epoch": 7, "training_loss": 1059181.8125, "training_acc": 49.0, "val_loss": 26057.894897460938, "val_acc": 52.0}
{"epoch": 8, "training_loss": 407449.93359375, "training_acc": 47.0, "val_loss": 46514.66979980469, "val_acc": 48.0}
{"epoch": 9, "training_loss": 1104204.1875, "training_acc": 53.0, "val_loss": 10193.826293945312, "val_acc": 48.0}
{"epoch": 10, "training_loss": 107051.8837890625, "training_acc": 47.0, "val_loss": 153016.07666015625, "val_acc": 48.0}
{"epoch": 11, "training_loss": 394997.07373046875, "training_acc": 47.0, "val_loss": 8715.101623535156, "val_acc": 52.0}
{"epoch": 12, "training_loss": 39712.17492675781, "training_acc": 53.0, "val_loss": 10964.006805419922, "val_acc": 52.0}
{"epoch": 13, "training_loss": 30067.86264038086, "training_acc": 51.0, "val_loss": 6446.8841552734375, "val_acc": 52.0}
{"epoch": 14, "training_loss": 18668.76644897461, "training_acc": 45.0, "val_loss": 13195.155334472656, "val_acc": 52.0}
{"epoch": 15, "training_loss": 37948.16491699219, "training_acc": 53.0, "val_loss": 7311.56005859375, "val_acc": 48.0}
{"epoch": 16, "training_loss": 19531.105228424072, "training_acc": 44.0, "val_loss": 15744.23828125, "val_acc": 52.0}
{"epoch": 17, "training_loss": 40409.62579345703, "training_acc": 53.0, "val_loss": 5585.259628295898, "val_acc": 48.0}
{"epoch": 18, "training_loss": 20350.434814453125, "training_acc": 47.0, "val_loss": 1722.9877471923828, "val_acc": 52.0}
{"epoch": 19, "training_loss": 6905.575653076172, "training_acc": 53.0, "val_loss": 4243.626022338867, "val_acc": 52.0}
{"epoch": 20, "training_loss": 14125.551300048828, "training_acc": 53.0, "val_loss": 9377.87094116211, "val_acc": 48.0}
{"epoch": 21, "training_loss": 26475.09942626953, "training_acc": 47.0, "val_loss": 1147.725772857666, "val_acc": 52.0}
{"epoch": 22, "training_loss": 4156.18489074707, "training_acc": 53.0, "val_loss": 2478.2026290893555, "val_acc": 52.0}
{"epoch": 23, "training_loss": 8522.340362548828, "training_acc": 53.0, "val_loss": 2012.364387512207, "val_acc": 48.0}
{"epoch": 24, "training_loss": 7154.412170410156, "training_acc": 47.0, "val_loss": 1905.9822082519531, "val_acc": 52.0}
{"epoch": 25, "training_loss": 7774.468017578125, "training_acc": 53.0, "val_loss": 330.03125190734863, "val_acc": 52.0}
{"epoch": 26, "training_loss": 6209.636779785156, "training_acc": 47.0, "val_loss": 3615.2462005615234, "val_acc": 48.0}
{"epoch": 27, "training_loss": 11812.826690673828, "training_acc": 47.0, "val_loss": 30.32928705215454, "val_acc": 52.0}
{"epoch": 28, "training_loss": 8777.267272949219, "training_acc": 55.0, "val_loss": 4722.574234008789, "val_acc": 52.0}
{"epoch": 29, "training_loss": 15223.774475097656, "training_acc": 53.0, "val_loss": 1267.1185493469238, "val_acc": 48.0}
{"epoch": 30, "training_loss": 3962.44766998291, "training_acc": 47.0, "val_loss": 3657.720184326172, "val_acc": 52.0}
{"epoch": 31, "training_loss": 11052.294342041016, "training_acc": 53.0, "val_loss": 1429.2888641357422, "val_acc": 48.0}
{"epoch": 32, "training_loss": 4929.684768676758, "training_acc": 43.0, "val_loss": 1316.8907165527344, "val_acc": 48.0}
{"epoch": 33, "training_loss": 3104.137320995331, "training_acc": 56.0, "val_loss": 483.50000381469727, "val_acc": 48.0}
{"epoch": 34, "training_loss": 2562.1581268310547, "training_acc": 51.0, "val_loss": 904.8291206359863, "val_acc": 52.0}
{"epoch": 35, "training_loss": 2064.23406457901, "training_acc": 58.0, "val_loss": 295.27456760406494, "val_acc": 48.0}
{"epoch": 36, "training_loss": 1526.9676666259766, "training_acc": 43.0, "val_loss": 175.80856084823608, "val_acc": 52.0}
{"epoch": 37, "training_loss": 2480.0604400634766, "training_acc": 49.0, "val_loss": 941.4664268493652, "val_acc": 48.0}
{"epoch": 38, "training_loss": 2532.143310546875, "training_acc": 51.0, "val_loss": 211.27567291259766, "val_acc": 52.0}
{"epoch": 39, "training_loss": 615.6334552764893, "training_acc": 57.0, "val_loss": 40.31084477901459, "val_acc": 52.0}
{"epoch": 40, "training_loss": 459.2378578186035, "training_acc": 46.0, "val_loss": 71.6623842716217, "val_acc": 52.0}
{"epoch": 41, "training_loss": 207.5646734237671, "training_acc": 55.0, "val_loss": 174.9315619468689, "val_acc": 52.0}
{"epoch": 42, "training_loss": 462.9191517829895, "training_acc": 48.0, "val_loss": 248.65307807922363, "val_acc": 48.0}
{"epoch": 43, "training_loss": 727.6746888160706, "training_acc": 47.0, "val_loss": 397.45171070098877, "val_acc": 52.0}
{"epoch": 44, "training_loss": 1577.0206413269043, "training_acc": 53.0, "val_loss": 203.12631130218506, "val_acc": 52.0}
{"epoch": 45, "training_loss": 801.6786422729492, "training_acc": 55.0, "val_loss": 382.1566104888916, "val_acc": 48.0}
{"epoch": 46, "training_loss": 1242.5542106628418, "training_acc": 47.0, "val_loss": 250.651216506958, "val_acc": 52.0}
