"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 583.8268699645996, "training_acc": 54.0, "val_loss": 2.405528504369152e+18, "val_acc": 48.0}
{"epoch": 1, "training_loss": 6.92245154119867e+18, "training_acc": 41.0, "val_loss": 1050085.05859375, "val_acc": 48.0}
{"epoch": 2, "training_loss": 9209878.5, "training_acc": 47.0, "val_loss": 1558670.703125, "val_acc": 52.0}
{"epoch": 3, "training_loss": 4676954.3173828125, "training_acc": 53.0, "val_loss": 190607.4462890625, "val_acc": 48.0}
{"epoch": 4, "training_loss": 13342581.46875, "training_acc": 55.0, "val_loss": 328676.85546875, "val_acc": 52.0}
{"epoch": 5, "training_loss": 1146430.390625, "training_acc": 49.0, "val_loss": 161618.90869140625, "val_acc": 52.0}
{"epoch": 6, "training_loss": 10915814.78125, "training_acc": 53.0, "val_loss": 21247392.1875, "val_acc": 48.0}
{"epoch": 7, "training_loss": 69637824.75, "training_acc": 57.0, "val_loss": 3190935.15625, "val_acc": 48.0}
{"epoch": 8, "training_loss": 20081099.125, "training_acc": 51.0, "val_loss": 545299.31640625, "val_acc": 48.0}
{"epoch": 9, "training_loss": 1597555.46484375, "training_acc": 47.0, "val_loss": 16576265.625, "val_acc": 52.0}
{"epoch": 10, "training_loss": 48805039.0, "training_acc": 53.0, "val_loss": 252193.8232421875, "val_acc": 52.0}
{"epoch": 11, "training_loss": 8208474.5625, "training_acc": 47.0, "val_loss": 565381.34765625, "val_acc": 48.0}
{"epoch": 12, "training_loss": 4070442.03125, "training_acc": 49.0, "val_loss": 938195.3125, "val_acc": 52.0}
{"epoch": 13, "training_loss": 2612171.3125, "training_acc": 53.0, "val_loss": 119354.1015625, "val_acc": 48.0}
{"epoch": 14, "training_loss": 590338.73828125, "training_acc": 37.0, "val_loss": 235675.9033203125, "val_acc": 48.0}
{"epoch": 15, "training_loss": 715363.3876953125, "training_acc": 47.0, "val_loss": 49282.16247558594, "val_acc": 52.0}
{"epoch": 16, "training_loss": 258069.216796875, "training_acc": 47.0, "val_loss": 304684.6435546875, "val_acc": 52.0}
{"epoch": 17, "training_loss": 807313.6401367188, "training_acc": 49.0, "val_loss": 3900.088882446289, "val_acc": 52.0}
{"epoch": 18, "training_loss": 21609.225952148438, "training_acc": 53.0, "val_loss": 6929.170989990234, "val_acc": 52.0}
{"epoch": 19, "training_loss": 30582.401977539062, "training_acc": 53.0, "val_loss": 5900.903701782227, "val_acc": 52.0}
{"epoch": 20, "training_loss": 20654.450134277344, "training_acc": 47.0, "val_loss": 14334.736633300781, "val_acc": 52.0}
{"epoch": 21, "training_loss": 38827.95526123047, "training_acc": 53.0, "val_loss": 153.3815622329712, "val_acc": 48.0}
{"epoch": 22, "training_loss": 5248.095367431641, "training_acc": 43.0, "val_loss": 227.37689018249512, "val_acc": 52.0}
{"epoch": 23, "training_loss": 5244.688690185547, "training_acc": 51.0, "val_loss": 15122.164916992188, "val_acc": 52.0}
{"epoch": 24, "training_loss": 42051.248931884766, "training_acc": 45.0, "val_loss": 1868.0435180664062, "val_acc": 52.0}
{"epoch": 25, "training_loss": 5077.904876708984, "training_acc": 50.0, "val_loss": 1530.930233001709, "val_acc": 52.0}
{"epoch": 26, "training_loss": 4926.242294311523, "training_acc": 53.0, "val_loss": 1945.0677871704102, "val_acc": 48.0}
{"epoch": 27, "training_loss": 7331.261489868164, "training_acc": 47.0, "val_loss": 74.96269345283508, "val_acc": 52.0}
{"epoch": 28, "training_loss": 1555.0467376708984, "training_acc": 51.0, "val_loss": 1516.787338256836, "val_acc": 52.0}
{"epoch": 29, "training_loss": 4160.2546310424805, "training_acc": 54.0, "val_loss": 1026.8848419189453, "val_acc": 52.0}
{"epoch": 30, "training_loss": 3270.125907897949, "training_acc": 51.0, "val_loss": 1038.5380744934082, "val_acc": 48.0}
{"epoch": 31, "training_loss": 3083.396120071411, "training_acc": 45.0, "val_loss": 1038.41552734375, "val_acc": 52.0}
{"epoch": 32, "training_loss": 3995.1907501220703, "training_acc": 53.0, "val_loss": 663.7608051300049, "val_acc": 52.0}
{"epoch": 33, "training_loss": 2001.7832126617432, "training_acc": 48.0, "val_loss": 257.63823986053467, "val_acc": 48.0}
{"epoch": 34, "training_loss": 776.8749008178711, "training_acc": 47.0, "val_loss": 26.23516023159027, "val_acc": 52.0}
{"epoch": 35, "training_loss": 310.145676612854, "training_acc": 46.0, "val_loss": 184.05394554138184, "val_acc": 52.0}
{"epoch": 36, "training_loss": 660.008617401123, "training_acc": 49.0, "val_loss": 355.7196855545044, "val_acc": 48.0}
{"epoch": 37, "training_loss": 1321.928539276123, "training_acc": 47.0, "val_loss": 173.70918989181519, "val_acc": 52.0}
{"epoch": 38, "training_loss": 944.9874458312988, "training_acc": 52.0, "val_loss": 34.082841873168945, "val_acc": 60.0}
{"epoch": 39, "training_loss": 455.0904998779297, "training_acc": 59.0, "val_loss": 26.69423222541809, "val_acc": 48.0}
{"epoch": 40, "training_loss": 495.96435928344727, "training_acc": 40.0, "val_loss": 51.811009645462036, "val_acc": 52.0}
{"epoch": 41, "training_loss": 552.0615310668945, "training_acc": 51.0, "val_loss": 204.74956035614014, "val_acc": 48.0}
{"epoch": 42, "training_loss": 753.6314029693604, "training_acc": 46.0, "val_loss": 190.9005045890808, "val_acc": 52.0}
{"epoch": 43, "training_loss": 622.916750907898, "training_acc": 49.0, "val_loss": 129.48899269104004, "val_acc": 48.0}
{"epoch": 44, "training_loss": 389.78857135772705, "training_acc": 51.0, "val_loss": 111.03284358978271, "val_acc": 52.0}
{"epoch": 45, "training_loss": 452.89567279815674, "training_acc": 45.0, "val_loss": 24.943087995052338, "val_acc": 52.0}
{"epoch": 46, "training_loss": 299.6024799346924, "training_acc": 48.0, "val_loss": 74.80217814445496, "val_acc": 48.0}
{"epoch": 47, "training_loss": 314.5742597579956, "training_acc": 49.0, "val_loss": 150.4773497581482, "val_acc": 52.0}
{"epoch": 48, "training_loss": 651.486213684082, "training_acc": 53.0, "val_loss": 41.061437129974365, "val_acc": 48.0}
{"epoch": 49, "training_loss": 335.4401741027832, "training_acc": 48.0, "val_loss": 17.823272943496704, "val_acc": 52.0}
{"epoch": 50, "training_loss": 238.69847106933594, "training_acc": 56.0, "val_loss": 53.87505888938904, "val_acc": 52.0}
{"epoch": 51, "training_loss": 291.0788116455078, "training_acc": 55.0, "val_loss": 99.55418109893799, "val_acc": 48.0}
{"epoch": 52, "training_loss": 421.0059652328491, "training_acc": 40.0, "val_loss": 52.6070237159729, "val_acc": 52.0}
{"epoch": 53, "training_loss": 212.44607400894165, "training_acc": 62.0, "val_loss": 123.14002513885498, "val_acc": 48.0}
{"epoch": 54, "training_loss": 324.53411865234375, "training_acc": 52.0, "val_loss": 120.77479362487793, "val_acc": 52.0}
{"epoch": 55, "training_loss": 441.25570583343506, "training_acc": 53.0, "val_loss": 87.68231272697449, "val_acc": 48.0}
{"epoch": 56, "training_loss": 317.0775480270386, "training_acc": 50.0, "val_loss": 22.220002114772797, "val_acc": 64.0}
{"epoch": 57, "training_loss": 355.4259490966797, "training_acc": 50.0, "val_loss": 108.14259052276611, "val_acc": 52.0}
{"epoch": 58, "training_loss": 429.4613285064697, "training_acc": 46.0, "val_loss": 96.72052264213562, "val_acc": 48.0}
{"epoch": 59, "training_loss": 275.88979387283325, "training_acc": 49.0, "val_loss": 81.74941539764404, "val_acc": 52.0}
{"epoch": 60, "training_loss": 269.2244801521301, "training_acc": 57.0, "val_loss": 117.2802209854126, "val_acc": 48.0}
{"epoch": 61, "training_loss": 425.813325881958, "training_acc": 48.0, "val_loss": 48.472678661346436, "val_acc": 52.0}
{"epoch": 62, "training_loss": 241.32788848876953, "training_acc": 54.0, "val_loss": 26.28103196620941, "val_acc": 52.0}
{"epoch": 63, "training_loss": 216.30139350891113, "training_acc": 55.0, "val_loss": 19.35744881629944, "val_acc": 44.0}
{"epoch": 64, "training_loss": 211.25939655303955, "training_acc": 55.0, "val_loss": 45.47120928764343, "val_acc": 52.0}
{"epoch": 65, "training_loss": 275.20347690582275, "training_acc": 52.0, "val_loss": 79.46780323982239, "val_acc": 48.0}
{"epoch": 66, "training_loss": 319.4713592529297, "training_acc": 48.0, "val_loss": 72.57068753242493, "val_acc": 52.0}
{"epoch": 67, "training_loss": 290.65532207489014, "training_acc": 47.0, "val_loss": 53.90534996986389, "val_acc": 48.0}
{"epoch": 68, "training_loss": 271.2785348892212, "training_acc": 45.0, "val_loss": 39.99490141868591, "val_acc": 52.0}
