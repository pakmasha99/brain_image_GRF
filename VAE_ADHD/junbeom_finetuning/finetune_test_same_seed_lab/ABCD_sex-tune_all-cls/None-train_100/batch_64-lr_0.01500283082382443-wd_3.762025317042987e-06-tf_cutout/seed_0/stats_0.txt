"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 918.4884719848633, "training_acc": 50.0, "val_loss": 3145438593024000.0, "val_acc": 56.0}
{"epoch": 1, "training_loss": 8359329209422407.0, "training_acc": 52.0, "val_loss": 2147421.6796875, "val_acc": 56.0}
{"epoch": 2, "training_loss": 2358227407.5, "training_acc": 54.0, "val_loss": 11890659200.0, "val_acc": 44.0}
{"epoch": 3, "training_loss": 30271016736.0, "training_acc": 48.0, "val_loss": 4240734.375, "val_acc": 56.0}
{"epoch": 4, "training_loss": 229078190.0, "training_acc": 50.0, "val_loss": 332535925.0, "val_acc": 56.0}
{"epoch": 5, "training_loss": 834998552.8125, "training_acc": 56.0, "val_loss": 22961406.25, "val_acc": 56.0}
{"epoch": 6, "training_loss": 122249234.0, "training_acc": 50.0, "val_loss": 11645885.9375, "val_acc": 44.0}
{"epoch": 7, "training_loss": 27058232.3984375, "training_acc": 48.0, "val_loss": 5136606.25, "val_acc": 56.0}
{"epoch": 8, "training_loss": 16385702.0625, "training_acc": 52.0, "val_loss": 141346.96044921875, "val_acc": 56.0}
{"epoch": 9, "training_loss": 2983877.1875, "training_acc": 52.0, "val_loss": 2190106.640625, "val_acc": 44.0}
{"epoch": 10, "training_loss": 5774757.5859375, "training_acc": 48.0, "val_loss": 45303.155517578125, "val_acc": 44.0}
{"epoch": 11, "training_loss": 712310.81640625, "training_acc": 48.0, "val_loss": 18038.409423828125, "val_acc": 44.0}
{"epoch": 12, "training_loss": 4915341.359375, "training_acc": 47.0, "val_loss": 30991.510009765625, "val_acc": 44.0}
{"epoch": 13, "training_loss": 646567.66015625, "training_acc": 48.0, "val_loss": 108042.4560546875, "val_acc": 44.0}
{"epoch": 14, "training_loss": 2130818.0625, "training_acc": 52.0, "val_loss": 321958.935546875, "val_acc": 56.0}
{"epoch": 15, "training_loss": 1439577.5546875, "training_acc": 46.0, "val_loss": 346418.4326171875, "val_acc": 44.0}
{"epoch": 16, "training_loss": 1272023.15625, "training_acc": 48.0, "val_loss": 48619.56481933594, "val_acc": 56.0}
{"epoch": 17, "training_loss": 179513.74072265625, "training_acc": 50.0, "val_loss": 104948.10791015625, "val_acc": 56.0}
{"epoch": 18, "training_loss": 365799.6806640625, "training_acc": 52.0, "val_loss": 66131.39038085938, "val_acc": 44.0}
{"epoch": 19, "training_loss": 215944.5, "training_acc": 48.0, "val_loss": 3659.379196166992, "val_acc": 56.0}
{"epoch": 20, "training_loss": 13566.352478027344, "training_acc": 50.0, "val_loss": 11798.822021484375, "val_acc": 56.0}
{"epoch": 21, "training_loss": 53142.577880859375, "training_acc": 46.0, "val_loss": 9776.095581054688, "val_acc": 56.0}
{"epoch": 22, "training_loss": 25479.06884765625, "training_acc": 57.0, "val_loss": 30541.937255859375, "val_acc": 44.0}
{"epoch": 23, "training_loss": 94502.94506835938, "training_acc": 48.0, "val_loss": 15021.025085449219, "val_acc": 56.0}
{"epoch": 24, "training_loss": 67487.71069335938, "training_acc": 52.0, "val_loss": 4749.307632446289, "val_acc": 44.0}
{"epoch": 25, "training_loss": 11076.777626037598, "training_acc": 50.0, "val_loss": 5978.458786010742, "val_acc": 44.0}
{"epoch": 26, "training_loss": 46543.946533203125, "training_acc": 38.0, "val_loss": 18626.85089111328, "val_acc": 44.0}
{"epoch": 27, "training_loss": 73532.89770507812, "training_acc": 48.0, "val_loss": 10579.31137084961, "val_acc": 56.0}
{"epoch": 28, "training_loss": 49508.03515625, "training_acc": 46.0, "val_loss": 12673.62289428711, "val_acc": 44.0}
{"epoch": 29, "training_loss": 30175.461135864258, "training_acc": 52.0, "val_loss": 186.18181943893433, "val_acc": 60.0}
{"epoch": 30, "training_loss": 6982.266845703125, "training_acc": 53.0, "val_loss": 805.6769371032715, "val_acc": 56.0}
{"epoch": 31, "training_loss": 13775.794799804688, "training_acc": 43.0, "val_loss": 1772.1429824829102, "val_acc": 56.0}
{"epoch": 32, "training_loss": 7177.764724731445, "training_acc": 52.0, "val_loss": 7302.986907958984, "val_acc": 56.0}
{"epoch": 33, "training_loss": 2508542.06640625, "training_acc": 48.0, "val_loss": 26554903.125, "val_acc": 56.0}
{"epoch": 34, "training_loss": 1224043184.0, "training_acc": 55.0, "val_loss": 119193925.0, "val_acc": 44.0}
{"epoch": 35, "training_loss": 274268212.734375, "training_acc": 50.0, "val_loss": 17850431.25, "val_acc": 56.0}
{"epoch": 36, "training_loss": 71052597.5, "training_acc": 52.0, "val_loss": 15782428.125, "val_acc": 56.0}
{"epoch": 37, "training_loss": 76735432.25, "training_acc": 42.0, "val_loss": 16035496.875, "val_acc": 44.0}
{"epoch": 38, "training_loss": 37618736.625, "training_acc": 48.0, "val_loss": 775137.79296875, "val_acc": 44.0}
{"epoch": 39, "training_loss": 11051197.3125, "training_acc": 44.0, "val_loss": 769199.169921875, "val_acc": 44.0}
{"epoch": 40, "training_loss": 2511715.6484375, "training_acc": 50.0, "val_loss": 3958051.953125, "val_acc": 44.0}
{"epoch": 41, "training_loss": 10844824.328125, "training_acc": 48.0, "val_loss": 586435.7421875, "val_acc": 44.0}
{"epoch": 42, "training_loss": 1616792.94921875, "training_acc": 48.0, "val_loss": 290202.734375, "val_acc": 56.0}
{"epoch": 43, "training_loss": 728904.4282226562, "training_acc": 60.0, "val_loss": 268969.43359375, "val_acc": 56.0}
{"epoch": 44, "training_loss": 814990.7221679688, "training_acc": 52.0, "val_loss": 658450.341796875, "val_acc": 44.0}
{"epoch": 45, "training_loss": 1939722.89453125, "training_acc": 48.0, "val_loss": 63404.852294921875, "val_acc": 44.0}
{"epoch": 46, "training_loss": 232690.6103515625, "training_acc": 48.0, "val_loss": 68108.49609375, "val_acc": 44.0}
{"epoch": 47, "training_loss": 193197.8037109375, "training_acc": 48.0, "val_loss": 6621.037292480469, "val_acc": 56.0}
{"epoch": 48, "training_loss": 36189.607177734375, "training_acc": 52.0, "val_loss": 6770.682525634766, "val_acc": 56.0}
