"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 159.80062580108643, "training_acc": 53.0, "val_loss": 1799.713134765625, "val_acc": 48.0}
{"epoch": 1, "training_loss": 4843.22678565979, "training_acc": 49.0, "val_loss": 27.84615457057953, "val_acc": 52.0}
{"epoch": 2, "training_loss": 90.65307307243347, "training_acc": 53.0, "val_loss": 18.315662443637848, "val_acc": 56.0}
{"epoch": 3, "training_loss": 73.06446146965027, "training_acc": 47.0, "val_loss": 17.33204275369644, "val_acc": 52.0}
{"epoch": 4, "training_loss": 68.8132975101471, "training_acc": 53.0, "val_loss": 19.357341527938843, "val_acc": 52.0}
{"epoch": 5, "training_loss": 75.41157627105713, "training_acc": 53.0, "val_loss": 17.416387796401978, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.80039286613464, "training_acc": 47.0, "val_loss": 17.425532639026642, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.74243712425232, "training_acc": 47.0, "val_loss": 17.319342494010925, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.41272163391113, "training_acc": 53.0, "val_loss": 17.316146194934845, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.3193211555481, "training_acc": 53.0, "val_loss": 17.320239543914795, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.27376127243042, "training_acc": 53.0, "val_loss": 17.364999651908875, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.21067714691162, "training_acc": 53.0, "val_loss": 17.31322407722473, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.77027606964111, "training_acc": 45.0, "val_loss": 17.30947196483612, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.1492075920105, "training_acc": 53.0, "val_loss": 17.32320785522461, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.1690354347229, "training_acc": 53.0, "val_loss": 17.33236461877823, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.17069149017334, "training_acc": 53.0, "val_loss": 17.34626144170761, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.61240124702454, "training_acc": 53.0, "val_loss": 17.323796451091766, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.21661424636841, "training_acc": 53.0, "val_loss": 17.309708893299103, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.23989963531494, "training_acc": 53.0, "val_loss": 17.31209307909012, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.47719931602478, "training_acc": 53.0, "val_loss": 17.30874627828598, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.15063428878784, "training_acc": 53.0, "val_loss": 17.30991154909134, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.19936680793762, "training_acc": 53.0, "val_loss": 17.311622202396393, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.19430541992188, "training_acc": 53.0, "val_loss": 17.308419942855835, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.20098114013672, "training_acc": 53.0, "val_loss": 17.309072613716125, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.69603443145752, "training_acc": 53.0, "val_loss": 17.322547733783722, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.20661664009094, "training_acc": 53.0, "val_loss": 17.31056123971939, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.16809964179993, "training_acc": 53.0, "val_loss": 17.30969548225403, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.15379810333252, "training_acc": 53.0, "val_loss": 17.30850785970688, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.15367889404297, "training_acc": 53.0, "val_loss": 17.308570444583893, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.15388917922974, "training_acc": 53.0, "val_loss": 17.308714985847473, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.15892100334167, "training_acc": 53.0, "val_loss": 17.308922111988068, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.13723039627075, "training_acc": 53.0, "val_loss": 17.311404645442963, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.13679218292236, "training_acc": 53.0, "val_loss": 17.317405343055725, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.14054656028748, "training_acc": 53.0, "val_loss": 17.322763800621033, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.13430333137512, "training_acc": 53.0, "val_loss": 17.333507537841797, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.4178102016449, "training_acc": 53.0, "val_loss": 17.343203723430634, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.25914168357849, "training_acc": 53.0, "val_loss": 17.321953177452087, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.13385844230652, "training_acc": 53.0, "val_loss": 17.31567680835724, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.12469387054443, "training_acc": 53.0, "val_loss": 17.310824990272522, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.14953351020813, "training_acc": 53.0, "val_loss": 17.30908900499344, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.14795112609863, "training_acc": 53.0, "val_loss": 17.308956384658813, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.1509153842926, "training_acc": 53.0, "val_loss": 17.308983206748962, "val_acc": 52.0}
