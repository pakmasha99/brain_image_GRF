"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 180.90617084503174, "training_acc": 48.0, "val_loss": 2360.664939880371, "val_acc": 52.0}
{"epoch": 1, "training_loss": 5994.146910667419, "training_acc": 51.0, "val_loss": 26.90461575984955, "val_acc": 48.0}
{"epoch": 2, "training_loss": 112.46979475021362, "training_acc": 43.0, "val_loss": 18.36645156145096, "val_acc": 48.0}
{"epoch": 3, "training_loss": 72.97162818908691, "training_acc": 47.0, "val_loss": 18.54834109544754, "val_acc": 52.0}
{"epoch": 4, "training_loss": 72.36131763458252, "training_acc": 53.0, "val_loss": 17.490805685520172, "val_acc": 52.0}
{"epoch": 5, "training_loss": 70.29362916946411, "training_acc": 47.0, "val_loss": 17.33647733926773, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.84554290771484, "training_acc": 47.0, "val_loss": 17.354537546634674, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.18584108352661, "training_acc": 53.0, "val_loss": 17.464454472064972, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.5208899974823, "training_acc": 53.0, "val_loss": 17.312590777873993, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.05605244636536, "training_acc": 53.0, "val_loss": 17.361491918563843, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.46571445465088, "training_acc": 47.0, "val_loss": 17.31293797492981, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.18265891075134, "training_acc": 53.0, "val_loss": 17.3420712351799, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.21250557899475, "training_acc": 53.0, "val_loss": 17.3440620303154, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.17982029914856, "training_acc": 53.0, "val_loss": 17.336265742778778, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.71564841270447, "training_acc": 53.0, "val_loss": 17.31100082397461, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.43573331832886, "training_acc": 53.0, "val_loss": 17.32376664876938, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.11481022834778, "training_acc": 53.0, "val_loss": 17.394228279590607, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.45424795150757, "training_acc": 53.0, "val_loss": 17.364484071731567, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.36939072608948, "training_acc": 53.0, "val_loss": 17.334087193012238, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.17918610572815, "training_acc": 53.0, "val_loss": 17.31916517019272, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.53452610969543, "training_acc": 53.0, "val_loss": 17.310111224651337, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.09725522994995, "training_acc": 53.0, "val_loss": 17.33003258705139, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.14661407470703, "training_acc": 53.0, "val_loss": 17.34502613544464, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.2305793762207, "training_acc": 53.0, "val_loss": 17.34067052602768, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.24374842643738, "training_acc": 53.0, "val_loss": 17.343023419380188, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.18748068809509, "training_acc": 53.0, "val_loss": 17.319515347480774, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.19030690193176, "training_acc": 53.0, "val_loss": 17.312343418598175, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.12529563903809, "training_acc": 53.0, "val_loss": 17.313244938850403, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.18107390403748, "training_acc": 53.0, "val_loss": 17.313705384731293, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.16495633125305, "training_acc": 53.0, "val_loss": 17.32138991355896, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.13674521446228, "training_acc": 53.0, "val_loss": 17.321887612342834, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.14639782905579, "training_acc": 53.0, "val_loss": 17.3191100358963, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.13400340080261, "training_acc": 53.0, "val_loss": 17.317943274974823, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.13459324836731, "training_acc": 53.0, "val_loss": 17.313964664936066, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.12872529029846, "training_acc": 53.0, "val_loss": 17.30983704328537, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.18948292732239, "training_acc": 53.0, "val_loss": 17.309032380580902, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.16740775108337, "training_acc": 53.0, "val_loss": 17.30911284685135, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.14872980117798, "training_acc": 53.0, "val_loss": 17.309676110744476, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.14085364341736, "training_acc": 53.0, "val_loss": 17.311111092567444, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.14593887329102, "training_acc": 53.0, "val_loss": 17.313238978385925, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.14453911781311, "training_acc": 53.0, "val_loss": 17.313693463802338, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.14927625656128, "training_acc": 53.0, "val_loss": 17.317241430282593, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.13460183143616, "training_acc": 53.0, "val_loss": 17.317329347133636, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.14514327049255, "training_acc": 53.0, "val_loss": 17.316211760044098, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.136563539505, "training_acc": 53.0, "val_loss": 17.31712371110916, "val_acc": 52.0}
{"epoch": 45, "training_loss": 69.13479709625244, "training_acc": 53.0, "val_loss": 17.314885556697845, "val_acc": 52.0}
{"epoch": 46, "training_loss": 69.13264155387878, "training_acc": 53.0, "val_loss": 17.311839759349823, "val_acc": 52.0}
{"epoch": 47, "training_loss": 69.16789555549622, "training_acc": 53.0, "val_loss": 17.31063276529312, "val_acc": 52.0}
{"epoch": 48, "training_loss": 69.1171543598175, "training_acc": 53.0, "val_loss": 17.30889528989792, "val_acc": 52.0}
{"epoch": 49, "training_loss": 69.15693211555481, "training_acc": 53.0, "val_loss": 17.30959713459015, "val_acc": 52.0}
{"epoch": 50, "training_loss": 69.20415925979614, "training_acc": 53.0, "val_loss": 17.31013059616089, "val_acc": 52.0}
{"epoch": 51, "training_loss": 69.18065762519836, "training_acc": 53.0, "val_loss": 17.309056222438812, "val_acc": 52.0}
{"epoch": 52, "training_loss": 69.2134575843811, "training_acc": 53.0, "val_loss": 17.30918437242508, "val_acc": 52.0}
{"epoch": 53, "training_loss": 69.18681263923645, "training_acc": 53.0, "val_loss": 17.309100925922394, "val_acc": 52.0}
{"epoch": 54, "training_loss": 69.13508820533752, "training_acc": 53.0, "val_loss": 17.311133444309235, "val_acc": 52.0}
{"epoch": 55, "training_loss": 69.2509195804596, "training_acc": 53.0, "val_loss": 17.317768931388855, "val_acc": 52.0}
{"epoch": 56, "training_loss": 69.15013074874878, "training_acc": 53.0, "val_loss": 17.316852509975433, "val_acc": 52.0}
{"epoch": 57, "training_loss": 69.13423871994019, "training_acc": 53.0, "val_loss": 17.313729226589203, "val_acc": 52.0}
{"epoch": 58, "training_loss": 69.28040361404419, "training_acc": 53.0, "val_loss": 17.311587929725647, "val_acc": 52.0}
{"epoch": 59, "training_loss": 69.13205075263977, "training_acc": 53.0, "val_loss": 17.316623032093048, "val_acc": 52.0}
{"epoch": 60, "training_loss": 69.12871932983398, "training_acc": 53.0, "val_loss": 17.323490977287292, "val_acc": 52.0}
{"epoch": 61, "training_loss": 69.1455307006836, "training_acc": 53.0, "val_loss": 17.33546406030655, "val_acc": 52.0}
{"epoch": 62, "training_loss": 69.15686678886414, "training_acc": 53.0, "val_loss": 17.354941368103027, "val_acc": 52.0}
{"epoch": 63, "training_loss": 69.31132531166077, "training_acc": 53.0, "val_loss": 17.375503480434418, "val_acc": 52.0}
{"epoch": 64, "training_loss": 69.25034523010254, "training_acc": 53.0, "val_loss": 17.348113656044006, "val_acc": 52.0}
{"epoch": 65, "training_loss": 69.19982576370239, "training_acc": 53.0, "val_loss": 17.323671281337738, "val_acc": 52.0}
{"epoch": 66, "training_loss": 69.142493724823, "training_acc": 53.0, "val_loss": 17.314141988754272, "val_acc": 52.0}
{"epoch": 67, "training_loss": 69.12819194793701, "training_acc": 53.0, "val_loss": 17.31036603450775, "val_acc": 52.0}
