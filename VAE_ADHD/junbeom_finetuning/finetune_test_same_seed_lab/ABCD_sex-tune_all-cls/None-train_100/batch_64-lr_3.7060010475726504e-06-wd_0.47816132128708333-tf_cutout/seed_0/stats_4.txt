"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 72.55051302909851, "training_acc": 47.0, "val_loss": 19.267834722995758, "val_acc": 48.0}
{"epoch": 1, "training_loss": 77.08870434761047, "training_acc": 47.0, "val_loss": 18.76840591430664, "val_acc": 48.0}
{"epoch": 2, "training_loss": 74.86058783531189, "training_acc": 47.0, "val_loss": 18.278850615024567, "val_acc": 40.0}
{"epoch": 3, "training_loss": 73.2473521232605, "training_acc": 47.0, "val_loss": 17.8868368268013, "val_acc": 52.0}
{"epoch": 4, "training_loss": 71.35980939865112, "training_acc": 47.0, "val_loss": 17.615610361099243, "val_acc": 52.0}
{"epoch": 5, "training_loss": 70.29084324836731, "training_acc": 47.0, "val_loss": 17.43689328432083, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.57709074020386, "training_acc": 47.0, "val_loss": 17.34200417995453, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.16864681243896, "training_acc": 53.0, "val_loss": 17.311054468154907, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.28220295906067, "training_acc": 53.0, "val_loss": 17.3286035656929, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.14364695549011, "training_acc": 53.0, "val_loss": 17.366424202919006, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.25508999824524, "training_acc": 53.0, "val_loss": 17.405587434768677, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.38446044921875, "training_acc": 53.0, "val_loss": 17.43503212928772, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.49515271186829, "training_acc": 53.0, "val_loss": 17.453400790691376, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.53161287307739, "training_acc": 53.0, "val_loss": 17.451198399066925, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.46326565742493, "training_acc": 53.0, "val_loss": 17.43525117635727, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.4427752494812, "training_acc": 53.0, "val_loss": 17.40703582763672, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.4111077785492, "training_acc": 53.0, "val_loss": 17.378617823123932, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.30158996582031, "training_acc": 53.0, "val_loss": 17.3577681183815, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.1621835231781, "training_acc": 53.0, "val_loss": 17.341770231723785, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.24884748458862, "training_acc": 53.0, "val_loss": 17.327818274497986, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.11709237098694, "training_acc": 53.0, "val_loss": 17.32126623392105, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.11599802970886, "training_acc": 53.0, "val_loss": 17.31521189212799, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.10364937782288, "training_acc": 53.0, "val_loss": 17.312289774417877, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.11915683746338, "training_acc": 53.0, "val_loss": 17.310811579227448, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.15184807777405, "training_acc": 53.0, "val_loss": 17.311030626296997, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.12629628181458, "training_acc": 53.0, "val_loss": 17.311546206474304, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.14687252044678, "training_acc": 53.0, "val_loss": 17.31151193380356, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.15888786315918, "training_acc": 53.0, "val_loss": 17.312082648277283, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.15758085250854, "training_acc": 53.0, "val_loss": 17.311950027942657, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.08915424346924, "training_acc": 53.0, "val_loss": 17.312011122703552, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.07007098197937, "training_acc": 53.0, "val_loss": 17.312167584896088, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.12574338912964, "training_acc": 53.0, "val_loss": 17.313803732395172, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.09622693061829, "training_acc": 53.0, "val_loss": 17.316360771656036, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.08955526351929, "training_acc": 53.0, "val_loss": 17.320716381072998, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.07596397399902, "training_acc": 53.0, "val_loss": 17.323875427246094, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.1097023487091, "training_acc": 53.0, "val_loss": 17.326003313064575, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.0965039730072, "training_acc": 53.0, "val_loss": 17.327621579170227, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.1086037158966, "training_acc": 53.0, "val_loss": 17.32957661151886, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.09255337715149, "training_acc": 53.0, "val_loss": 17.330752313137054, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.11942601203918, "training_acc": 53.0, "val_loss": 17.330202460289, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.12167072296143, "training_acc": 53.0, "val_loss": 17.329296469688416, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.14360404014587, "training_acc": 53.0, "val_loss": 17.327480018138885, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.13092350959778, "training_acc": 53.0, "val_loss": 17.325112223625183, "val_acc": 52.0}
