"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 569.8087844848633, "training_acc": 50.0, "val_loss": 71785036800.0, "val_acc": 56.0}
{"epoch": 1, "training_loss": 191958633455.79102, "training_acc": 52.0, "val_loss": 3416825.78125, "val_acc": 44.0}
{"epoch": 2, "training_loss": 9100252.8828125, "training_acc": 46.0, "val_loss": 434944050.0, "val_acc": 44.0}
{"epoch": 3, "training_loss": 1076665383.96875, "training_acc": 48.0, "val_loss": 634753.857421875, "val_acc": 56.0}
{"epoch": 4, "training_loss": 3484147.3125, "training_acc": 50.0, "val_loss": 1248358.984375, "val_acc": 56.0}
{"epoch": 5, "training_loss": 3177384.7333984375, "training_acc": 52.0, "val_loss": 6966221.09375, "val_acc": 44.0}
{"epoch": 6, "training_loss": 16489251.6953125, "training_acc": 48.0, "val_loss": 187335.9375, "val_acc": 56.0}
{"epoch": 7, "training_loss": 539887.7993164062, "training_acc": 52.0, "val_loss": 211023.2177734375, "val_acc": 44.0}
{"epoch": 8, "training_loss": 604536.2680664062, "training_acc": 48.0, "val_loss": 3484.3631744384766, "val_acc": 56.0}
{"epoch": 9, "training_loss": 20427.126831054688, "training_acc": 52.0, "val_loss": 1729.0321350097656, "val_acc": 56.0}
{"epoch": 10, "training_loss": 7595.111358642578, "training_acc": 50.0, "val_loss": 2215.746307373047, "val_acc": 56.0}
{"epoch": 11, "training_loss": 6444.995662689209, "training_acc": 52.0, "val_loss": 13253.1494140625, "val_acc": 44.0}
{"epoch": 12, "training_loss": 39374.580627441406, "training_acc": 48.0, "val_loss": 4725.560760498047, "val_acc": 56.0}
{"epoch": 13, "training_loss": 14662.81526184082, "training_acc": 52.0, "val_loss": 554.9177169799805, "val_acc": 56.0}
{"epoch": 14, "training_loss": 1652.5576601028442, "training_acc": 47.0, "val_loss": 181.7076325416565, "val_acc": 56.0}
{"epoch": 15, "training_loss": 1309.8153686523438, "training_acc": 46.0, "val_loss": 699.9037265777588, "val_acc": 56.0}
{"epoch": 16, "training_loss": 2512.3851928710938, "training_acc": 52.0, "val_loss": 162.5876784324646, "val_acc": 56.0}
{"epoch": 17, "training_loss": 1297.7277069091797, "training_acc": 50.0, "val_loss": 21.494144201278687, "val_acc": 56.0}
{"epoch": 18, "training_loss": 534.1592903137207, "training_acc": 55.0, "val_loss": 88.3030652999878, "val_acc": 56.0}
{"epoch": 19, "training_loss": 1117.1483154296875, "training_acc": 52.0, "val_loss": 440.97113609313965, "val_acc": 44.0}
{"epoch": 20, "training_loss": 1177.0801525115967, "training_acc": 48.0, "val_loss": 293.99940967559814, "val_acc": 56.0}
{"epoch": 21, "training_loss": 1187.1668891906738, "training_acc": 52.0, "val_loss": 94.60732936859131, "val_acc": 56.0}
{"epoch": 22, "training_loss": 1340.0604095458984, "training_acc": 56.0, "val_loss": 107.98488855361938, "val_acc": 44.0}
{"epoch": 23, "training_loss": 541.7542057037354, "training_acc": 46.0, "val_loss": 87.33474612236023, "val_acc": 56.0}
{"epoch": 24, "training_loss": 278.97480630874634, "training_acc": 52.0, "val_loss": 21.833422780036926, "val_acc": 44.0}
{"epoch": 25, "training_loss": 99.24377155303955, "training_acc": 48.0, "val_loss": 23.257867991924286, "val_acc": 44.0}
{"epoch": 26, "training_loss": 200.97908973693848, "training_acc": 38.0, "val_loss": 23.846405744552612, "val_acc": 44.0}
{"epoch": 27, "training_loss": 88.02366924285889, "training_acc": 48.0, "val_loss": 23.782819509506226, "val_acc": 56.0}
{"epoch": 28, "training_loss": 94.56940007209778, "training_acc": 45.0, "val_loss": 18.65186244249344, "val_acc": 56.0}
{"epoch": 29, "training_loss": 71.74896669387817, "training_acc": 47.0, "val_loss": 17.972710728645325, "val_acc": 56.0}
{"epoch": 30, "training_loss": 122.24526023864746, "training_acc": 40.0, "val_loss": 17.206308245658875, "val_acc": 56.0}
{"epoch": 31, "training_loss": 73.54108548164368, "training_acc": 42.0, "val_loss": 17.185622453689575, "val_acc": 56.0}
{"epoch": 32, "training_loss": 73.03704380989075, "training_acc": 45.0, "val_loss": 19.336260855197906, "val_acc": 40.0}
{"epoch": 33, "training_loss": 71.43283581733704, "training_acc": 59.0, "val_loss": 19.76395696401596, "val_acc": 56.0}
{"epoch": 34, "training_loss": 78.4699718952179, "training_acc": 53.0, "val_loss": 22.31018841266632, "val_acc": 44.0}
{"epoch": 35, "training_loss": 81.3274314403534, "training_acc": 48.0, "val_loss": 19.980883598327637, "val_acc": 56.0}
{"epoch": 36, "training_loss": 77.72910761833191, "training_acc": 52.0, "val_loss": 19.594982266426086, "val_acc": 40.0}
{"epoch": 37, "training_loss": 76.79284405708313, "training_acc": 48.0, "val_loss": 18.81641000509262, "val_acc": 56.0}
{"epoch": 38, "training_loss": 77.05491065979004, "training_acc": 52.0, "val_loss": 17.73187965154648, "val_acc": 56.0}
{"epoch": 39, "training_loss": 69.49853682518005, "training_acc": 55.0, "val_loss": 19.47901099920273, "val_acc": 40.0}
{"epoch": 40, "training_loss": 77.75424194335938, "training_acc": 48.0, "val_loss": 17.490141093730927, "val_acc": 56.0}
{"epoch": 41, "training_loss": 70.9896171092987, "training_acc": 49.0, "val_loss": 17.90877729654312, "val_acc": 56.0}
{"epoch": 42, "training_loss": 74.75208950042725, "training_acc": 52.0, "val_loss": 17.09955632686615, "val_acc": 56.0}
{"epoch": 43, "training_loss": 67.95312023162842, "training_acc": 59.0, "val_loss": 18.808166682720184, "val_acc": 56.0}
{"epoch": 44, "training_loss": 75.38965702056885, "training_acc": 48.0, "val_loss": 18.046775460243225, "val_acc": 56.0}
{"epoch": 45, "training_loss": 71.00380325317383, "training_acc": 44.0, "val_loss": 17.210330069065094, "val_acc": 56.0}
{"epoch": 46, "training_loss": 71.55769968032837, "training_acc": 52.0, "val_loss": 17.37285703420639, "val_acc": 56.0}
{"epoch": 47, "training_loss": 71.6213767528534, "training_acc": 54.0, "val_loss": 17.20351129770279, "val_acc": 56.0}
{"epoch": 48, "training_loss": 69.19885158538818, "training_acc": 52.0, "val_loss": 17.671620845794678, "val_acc": 56.0}
{"epoch": 49, "training_loss": 70.15494227409363, "training_acc": 49.0, "val_loss": 17.5454780459404, "val_acc": 56.0}
{"epoch": 50, "training_loss": 70.58103203773499, "training_acc": 47.0, "val_loss": 17.245547473430634, "val_acc": 56.0}
{"epoch": 51, "training_loss": 69.7859845161438, "training_acc": 56.0, "val_loss": 17.10546910762787, "val_acc": 56.0}
{"epoch": 52, "training_loss": 69.39149475097656, "training_acc": 50.0, "val_loss": 17.12336540222168, "val_acc": 56.0}
{"epoch": 53, "training_loss": 68.8736503124237, "training_acc": 51.0, "val_loss": 17.215418815612793, "val_acc": 56.0}
{"epoch": 54, "training_loss": 70.29326748847961, "training_acc": 46.0, "val_loss": 17.304101586341858, "val_acc": 56.0}
{"epoch": 55, "training_loss": 70.71420288085938, "training_acc": 45.0, "val_loss": 17.18812584877014, "val_acc": 56.0}
{"epoch": 56, "training_loss": 69.44348168373108, "training_acc": 51.0, "val_loss": 17.189380526542664, "val_acc": 56.0}
{"epoch": 57, "training_loss": 69.40155553817749, "training_acc": 51.0, "val_loss": 17.23746806383133, "val_acc": 56.0}
{"epoch": 58, "training_loss": 68.97219514846802, "training_acc": 54.0, "val_loss": 17.185871303081512, "val_acc": 56.0}
{"epoch": 59, "training_loss": 68.74856042861938, "training_acc": 55.0, "val_loss": 17.18657910823822, "val_acc": 56.0}
{"epoch": 60, "training_loss": 69.67183184623718, "training_acc": 49.0, "val_loss": 17.160190641880035, "val_acc": 56.0}
{"epoch": 61, "training_loss": 68.92910432815552, "training_acc": 52.0, "val_loss": 17.229890823364258, "val_acc": 56.0}
