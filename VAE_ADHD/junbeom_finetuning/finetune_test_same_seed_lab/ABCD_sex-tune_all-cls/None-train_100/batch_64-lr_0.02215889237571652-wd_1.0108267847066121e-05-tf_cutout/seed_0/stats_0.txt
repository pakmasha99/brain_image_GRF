"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 1553.8371658325195, "training_acc": 50.0, "val_loss": 3.970950807035807e+22, "val_acc": 56.0}
{"epoch": 1, "training_loss": 1.0559229758325419e+23, "training_acc": 52.0, "val_loss": 237560089600.0, "val_acc": 56.0}
{"epoch": 2, "training_loss": 2037535080448.0, "training_acc": 54.0, "val_loss": 4538743200.0, "val_acc": 44.0}
{"epoch": 3, "training_loss": 73507181056.0, "training_acc": 46.0, "val_loss": 1618356400.0, "val_acc": 56.0}
{"epoch": 4, "training_loss": 1839832572928.0, "training_acc": 50.0, "val_loss": 3663413600.0, "val_acc": 56.0}
{"epoch": 5, "training_loss": 14318451648.0, "training_acc": 52.0, "val_loss": 20199374400.0, "val_acc": 56.0}
{"epoch": 6, "training_loss": 62145819200.0, "training_acc": 52.0, "val_loss": 542248450.0, "val_acc": 56.0}
{"epoch": 7, "training_loss": 596853227392.0, "training_acc": 48.0, "val_loss": 346323507200.0, "val_acc": 56.0}
{"epoch": 8, "training_loss": 1147543621632.0, "training_acc": 58.0, "val_loss": 584977100800.0, "val_acc": 56.0}
{"epoch": 9, "training_loss": 1579193238400.0, "training_acc": 52.0, "val_loss": 2157886400.0, "val_acc": 44.0}
{"epoch": 10, "training_loss": 14895515264.0, "training_acc": 48.0, "val_loss": 396785638400.0, "val_acc": 56.0}
{"epoch": 11, "training_loss": 1210210023424.0, "training_acc": 52.0, "val_loss": 180577484800.0, "val_acc": 44.0}
{"epoch": 12, "training_loss": 473322626560.0, "training_acc": 48.0, "val_loss": 40466796800.0, "val_acc": 44.0}
{"epoch": 13, "training_loss": 93403199712.0, "training_acc": 48.0, "val_loss": 1788026400.0, "val_acc": 56.0}
{"epoch": 14, "training_loss": 5796094780.0, "training_acc": 52.0, "val_loss": 13874084800.0, "val_acc": 44.0}
{"epoch": 15, "training_loss": 31608774864.0, "training_acc": 48.0, "val_loss": 523511040000.0, "val_acc": 56.0}
{"epoch": 16, "training_loss": 1410545546272.0, "training_acc": 52.0, "val_loss": 2887658400.0, "val_acc": 44.0}
{"epoch": 17, "training_loss": 896693564928.0, "training_acc": 48.0, "val_loss": 23428602265600.0, "val_acc": 44.0}
{"epoch": 18, "training_loss": 54656223236096.0, "training_acc": 50.0, "val_loss": 281980850.0, "val_acc": 44.0}
{"epoch": 19, "training_loss": 670163095808.0, "training_acc": 48.0, "val_loss": 3219456000.0, "val_acc": 56.0}
{"epoch": 20, "training_loss": 187945907200.0, "training_acc": 52.0, "val_loss": 127438873600.0, "val_acc": 44.0}
{"epoch": 21, "training_loss": 281289122816.0, "training_acc": 54.0, "val_loss": 56089862400.0, "val_acc": 56.0}
{"epoch": 22, "training_loss": 342297667584.0, "training_acc": 56.0, "val_loss": 598192793600.0, "val_acc": 44.0}
{"epoch": 23, "training_loss": 1528949373952.0, "training_acc": 48.0, "val_loss": 1278055116800.0, "val_acc": 56.0}
{"epoch": 24, "training_loss": 3351397347072.0, "training_acc": 52.0, "val_loss": 5546160400.0, "val_acc": 44.0}
{"epoch": 25, "training_loss": 18428366144.0, "training_acc": 48.0, "val_loss": 9688379200.0, "val_acc": 56.0}
{"epoch": 26, "training_loss": 86297068544.0, "training_acc": 62.0, "val_loss": 13043520000.0, "val_acc": 44.0}
{"epoch": 27, "training_loss": 62742686976.0, "training_acc": 48.0, "val_loss": 1738147600.0, "val_acc": 44.0}
{"epoch": 28, "training_loss": 6887419904.0, "training_acc": 48.0, "val_loss": 77589632000.0, "val_acc": 56.0}
{"epoch": 29, "training_loss": 380285782016.0, "training_acc": 48.0, "val_loss": 119043194880000.0, "val_acc": 56.0}
{"epoch": 30, "training_loss": 276642396836864.0, "training_acc": 60.0, "val_loss": 48758896000.0, "val_acc": 44.0}
{"epoch": 31, "training_loss": 681387708416.0, "training_acc": 48.0, "val_loss": 80280000000.0, "val_acc": 44.0}
{"epoch": 32, "training_loss": 3017610182656.0, "training_acc": 48.0, "val_loss": 131193152000.0, "val_acc": 56.0}
{"epoch": 33, "training_loss": 1262956126208.0, "training_acc": 52.0, "val_loss": 1216811110400.0, "val_acc": 44.0}
{"epoch": 34, "training_loss": 2989663613136.0, "training_acc": 46.0, "val_loss": 9879608800.0, "val_acc": 56.0}
{"epoch": 35, "training_loss": 55750849536.0, "training_acc": 52.0, "val_loss": 43697641600.0, "val_acc": 56.0}
{"epoch": 36, "training_loss": 1824164806656.0, "training_acc": 52.0, "val_loss": 2346243072000.0, "val_acc": 56.0}
{"epoch": 37, "training_loss": 38702178893824.0, "training_acc": 42.0, "val_loss": 183885068800.0, "val_acc": 56.0}
