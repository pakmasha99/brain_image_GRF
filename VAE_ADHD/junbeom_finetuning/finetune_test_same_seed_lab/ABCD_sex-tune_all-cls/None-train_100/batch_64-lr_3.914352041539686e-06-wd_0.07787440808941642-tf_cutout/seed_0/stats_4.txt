"main_optuna_fix.py --pretrained_path None --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 72.53941941261292, "training_acc": 47.0, "val_loss": 19.258491694927216, "val_acc": 48.0}
{"epoch": 1, "training_loss": 77.03917193412781, "training_acc": 47.0, "val_loss": 18.73592585325241, "val_acc": 48.0}
{"epoch": 2, "training_loss": 74.71253895759583, "training_acc": 47.0, "val_loss": 18.230052292346954, "val_acc": 52.0}
{"epoch": 3, "training_loss": 73.05191946029663, "training_acc": 47.0, "val_loss": 17.83430129289627, "val_acc": 52.0}
{"epoch": 4, "training_loss": 71.1365237236023, "training_acc": 47.0, "val_loss": 17.57006049156189, "val_acc": 52.0}
{"epoch": 5, "training_loss": 70.09724640846252, "training_acc": 47.0, "val_loss": 17.404620349407196, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.43704319000244, "training_acc": 47.0, "val_loss": 17.326152324676514, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.09024357795715, "training_acc": 53.0, "val_loss": 17.313125729560852, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.27773022651672, "training_acc": 53.0, "val_loss": 17.346133291721344, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.19146275520325, "training_acc": 53.0, "val_loss": 17.39213764667511, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.3346312046051, "training_acc": 53.0, "val_loss": 17.43267923593521, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.47338724136353, "training_acc": 53.0, "val_loss": 17.45656579732895, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.55542421340942, "training_acc": 53.0, "val_loss": 17.467200756072998, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.56751561164856, "training_acc": 53.0, "val_loss": 17.45511144399643, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.46417236328125, "training_acc": 53.0, "val_loss": 17.431552708148956, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.42606282234192, "training_acc": 53.0, "val_loss": 17.398789525032043, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.39098191261292, "training_acc": 53.0, "val_loss": 17.368018627166748, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.2733051776886, "training_acc": 53.0, "val_loss": 17.347529530525208, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.13422393798828, "training_acc": 53.0, "val_loss": 17.332729697227478, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.23065662384033, "training_acc": 53.0, "val_loss": 17.320963740348816, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.10926938056946, "training_acc": 53.0, "val_loss": 17.316465079784393, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.11149716377258, "training_acc": 53.0, "val_loss": 17.31252670288086, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.10566830635071, "training_acc": 53.0, "val_loss": 17.310871183872223, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.12561774253845, "training_acc": 53.0, "val_loss": 17.310574650764465, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.15488052368164, "training_acc": 53.0, "val_loss": 17.31126755475998, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.13119196891785, "training_acc": 53.0, "val_loss": 17.311416566371918, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.15905332565308, "training_acc": 53.0, "val_loss": 17.311111092567444, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.15483927726746, "training_acc": 53.0, "val_loss": 17.311370372772217, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.15576124191284, "training_acc": 53.0, "val_loss": 17.311294376850128, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.08339929580688, "training_acc": 53.0, "val_loss": 17.311418056488037, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.06387758255005, "training_acc": 53.0, "val_loss": 17.312124371528625, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.12222027778625, "training_acc": 53.0, "val_loss": 17.31434017419815, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.09349918365479, "training_acc": 53.0, "val_loss": 17.317821085453033, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.08941268920898, "training_acc": 53.0, "val_loss": 17.32269674539566, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.07621097564697, "training_acc": 53.0, "val_loss": 17.326079308986664, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.11130452156067, "training_acc": 53.0, "val_loss": 17.32819676399231, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.1011323928833, "training_acc": 53.0, "val_loss": 17.32947528362274, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.1105465888977, "training_acc": 53.0, "val_loss": 17.330966889858246, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.09074068069458, "training_acc": 53.0, "val_loss": 17.331692576408386, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.12255215644836, "training_acc": 53.0, "val_loss": 17.330726981163025, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.12453770637512, "training_acc": 53.0, "val_loss": 17.32948273420334, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.14068579673767, "training_acc": 53.0, "val_loss": 17.326942086219788, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.12507939338684, "training_acc": 53.0, "val_loss": 17.324155569076538, "val_acc": 52.0}
