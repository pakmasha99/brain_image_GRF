"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.17356991767883, "training_acc": 53.0, "val_loss": 17.319580912590027, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.15156483650208, "training_acc": 53.0, "val_loss": 17.31780916452408, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.21726155281067, "training_acc": 53.0, "val_loss": 17.3426553606987, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.22977185249329, "training_acc": 53.0, "val_loss": 17.333434522151947, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.14942264556885, "training_acc": 53.0, "val_loss": 17.331725358963013, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.67784881591797, "training_acc": 53.0, "val_loss": 17.317070066928864, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.50885033607483, "training_acc": 53.0, "val_loss": 17.35939383506775, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.21885943412781, "training_acc": 53.0, "val_loss": 17.39984303712845, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.35206246376038, "training_acc": 53.0, "val_loss": 17.33601689338684, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.39901566505432, "training_acc": 53.0, "val_loss": 17.316333949565887, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.17054033279419, "training_acc": 53.0, "val_loss": 17.319190502166748, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.27921652793884, "training_acc": 53.0, "val_loss": 17.316697537899017, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.12732315063477, "training_acc": 53.0, "val_loss": 17.332400381565094, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.1648519039154, "training_acc": 53.0, "val_loss": 17.344732582569122, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.27819442749023, "training_acc": 53.0, "val_loss": 17.33771115541458, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.25716376304626, "training_acc": 53.0, "val_loss": 17.341728508472443, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.19744277000427, "training_acc": 53.0, "val_loss": 17.315830290317535, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.20598721504211, "training_acc": 53.0, "val_loss": 17.31036752462387, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.13987731933594, "training_acc": 53.0, "val_loss": 17.31470078229904, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.19194483757019, "training_acc": 53.0, "val_loss": 17.318597435951233, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.17884302139282, "training_acc": 53.0, "val_loss": 17.337659001350403, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.17432117462158, "training_acc": 53.0, "val_loss": 17.33115315437317, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.16159892082214, "training_acc": 53.0, "val_loss": 17.31792837381363, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.13533902168274, "training_acc": 53.0, "val_loss": 17.31305420398712, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.14653205871582, "training_acc": 53.0, "val_loss": 17.30918139219284, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.12974143028259, "training_acc": 53.0, "val_loss": 17.310164868831635, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.24733853340149, "training_acc": 53.0, "val_loss": 17.311377823352814, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.20457077026367, "training_acc": 53.0, "val_loss": 17.31044352054596, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.12156891822815, "training_acc": 53.0, "val_loss": 17.319634556770325, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.14305830001831, "training_acc": 53.0, "val_loss": 17.326731979846954, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.17281079292297, "training_acc": 53.0, "val_loss": 17.32415407896042, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.19588923454285, "training_acc": 53.0, "val_loss": 17.313064634799957, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.13696694374084, "training_acc": 53.0, "val_loss": 17.313405871391296, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.12487721443176, "training_acc": 53.0, "val_loss": 17.3110693693161, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.144211769104, "training_acc": 53.0, "val_loss": 17.31055974960327, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.14632892608643, "training_acc": 53.0, "val_loss": 17.314107716083527, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.12636637687683, "training_acc": 53.0, "val_loss": 17.313465476036072, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.13746380805969, "training_acc": 53.0, "val_loss": 17.31015294790268, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.27087330818176, "training_acc": 53.0, "val_loss": 17.309583723545074, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.10455775260925, "training_acc": 53.0, "val_loss": 17.311152815818787, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.19933104515076, "training_acc": 53.0, "val_loss": 17.319686710834503, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.32430338859558, "training_acc": 53.0, "val_loss": 17.313751578330994, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.2375099658966, "training_acc": 53.0, "val_loss": 17.314019799232483, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.3560745716095, "training_acc": 53.0, "val_loss": 17.333216965198517, "val_acc": 52.0}
