"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 70.13447523117065, "training_acc": 47.0, "val_loss": 17.35435426235199, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.52410125732422, "training_acc": 47.0, "val_loss": 17.338964343070984, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.22413182258606, "training_acc": 51.0, "val_loss": 17.338742315769196, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.17232298851013, "training_acc": 53.0, "val_loss": 17.3458069562912, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.37077903747559, "training_acc": 53.0, "val_loss": 17.333535850048065, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.28819584846497, "training_acc": 53.0, "val_loss": 17.352832853794098, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.13594079017639, "training_acc": 53.0, "val_loss": 17.313052713871002, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.72847032546997, "training_acc": 44.0, "val_loss": 17.309488356113434, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.09596371650696, "training_acc": 53.0, "val_loss": 17.341382801532745, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.1889500617981, "training_acc": 53.0, "val_loss": 17.347069084644318, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.22599506378174, "training_acc": 53.0, "val_loss": 17.33444780111313, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.19625449180603, "training_acc": 53.0, "val_loss": 17.309272289276123, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.31865859031677, "training_acc": 53.0, "val_loss": 17.31235682964325, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.19948196411133, "training_acc": 53.0, "val_loss": 17.328670620918274, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.3011326789856, "training_acc": 47.0, "val_loss": 17.309890687465668, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.18083882331848, "training_acc": 53.0, "val_loss": 17.315152287483215, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.13040518760681, "training_acc": 53.0, "val_loss": 17.320850491523743, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.15896344184875, "training_acc": 53.0, "val_loss": 17.317090928554535, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.20070576667786, "training_acc": 53.0, "val_loss": 17.324714362621307, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.13568472862244, "training_acc": 53.0, "val_loss": 17.341677844524384, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.22285771369934, "training_acc": 53.0, "val_loss": 17.336909472942352, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.49044036865234, "training_acc": 53.0, "val_loss": 17.334258556365967, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.06891894340515, "training_acc": 53.0, "val_loss": 17.313332855701447, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.50279450416565, "training_acc": 45.0, "val_loss": 17.328259348869324, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.57319331169128, "training_acc": 53.0, "val_loss": 17.310050129890442, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.15982031822205, "training_acc": 53.0, "val_loss": 17.310722172260284, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.22846221923828, "training_acc": 53.0, "val_loss": 17.315711081027985, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.07846856117249, "training_acc": 53.0, "val_loss": 17.350898683071136, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.20272874832153, "training_acc": 53.0, "val_loss": 17.38138496875763, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.30052208900452, "training_acc": 53.0, "val_loss": 17.384818196296692, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.27410554885864, "training_acc": 53.0, "val_loss": 17.3379585146904, "val_acc": 52.0}
