"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.39187955856323, "training_acc": 52.0, "val_loss": 17.274856567382812, "val_acc": 56.0}
{"epoch": 1, "training_loss": 69.21102213859558, "training_acc": 52.0, "val_loss": 17.279061675071716, "val_acc": 56.0}
{"epoch": 2, "training_loss": 69.18499946594238, "training_acc": 52.0, "val_loss": 17.277808487415314, "val_acc": 56.0}
{"epoch": 3, "training_loss": 69.161691904068, "training_acc": 52.0, "val_loss": 17.276617884635925, "val_acc": 56.0}
{"epoch": 4, "training_loss": 69.27801394462585, "training_acc": 52.0, "val_loss": 17.277877032756805, "val_acc": 56.0}
{"epoch": 5, "training_loss": 69.22438335418701, "training_acc": 52.0, "val_loss": 17.282862961292267, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.11170697212219, "training_acc": 52.0, "val_loss": 17.28423684835434, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.24173736572266, "training_acc": 52.0, "val_loss": 17.279189825057983, "val_acc": 56.0}
{"epoch": 8, "training_loss": 69.11096811294556, "training_acc": 52.0, "val_loss": 17.283591628074646, "val_acc": 56.0}
{"epoch": 9, "training_loss": 69.15604519844055, "training_acc": 52.0, "val_loss": 17.28382110595703, "val_acc": 56.0}
{"epoch": 10, "training_loss": 69.10703682899475, "training_acc": 52.0, "val_loss": 17.28368103504181, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.07091236114502, "training_acc": 52.0, "val_loss": 17.278502881526947, "val_acc": 56.0}
{"epoch": 12, "training_loss": 68.94330191612244, "training_acc": 52.0, "val_loss": 17.276062071323395, "val_acc": 56.0}
{"epoch": 13, "training_loss": 68.98827290534973, "training_acc": 52.0, "val_loss": 17.2736719250679, "val_acc": 56.0}
{"epoch": 14, "training_loss": 69.03074955940247, "training_acc": 52.0, "val_loss": 17.272958159446716, "val_acc": 56.0}
{"epoch": 15, "training_loss": 69.07226157188416, "training_acc": 52.0, "val_loss": 17.266811430454254, "val_acc": 56.0}
{"epoch": 16, "training_loss": 68.96844434738159, "training_acc": 52.0, "val_loss": 17.260350286960602, "val_acc": 56.0}
{"epoch": 17, "training_loss": 68.9289162158966, "training_acc": 52.0, "val_loss": 17.252328991889954, "val_acc": 56.0}
{"epoch": 18, "training_loss": 68.82150769233704, "training_acc": 52.0, "val_loss": 17.241567373275757, "val_acc": 56.0}
{"epoch": 19, "training_loss": 68.84612798690796, "training_acc": 52.0, "val_loss": 17.232222855091095, "val_acc": 56.0}
{"epoch": 20, "training_loss": 68.89994525909424, "training_acc": 52.0, "val_loss": 17.2245055437088, "val_acc": 56.0}
{"epoch": 21, "training_loss": 68.82227683067322, "training_acc": 52.0, "val_loss": 17.221568524837494, "val_acc": 56.0}
{"epoch": 22, "training_loss": 68.89699244499207, "training_acc": 52.0, "val_loss": 17.219191789627075, "val_acc": 56.0}
{"epoch": 23, "training_loss": 68.94049406051636, "training_acc": 52.0, "val_loss": 17.22090095281601, "val_acc": 56.0}
{"epoch": 24, "training_loss": 68.94370007514954, "training_acc": 52.0, "val_loss": 17.23177433013916, "val_acc": 56.0}
{"epoch": 25, "training_loss": 68.93066501617432, "training_acc": 52.0, "val_loss": 17.23763942718506, "val_acc": 56.0}
{"epoch": 26, "training_loss": 68.53870129585266, "training_acc": 52.0, "val_loss": 17.235514521598816, "val_acc": 56.0}
{"epoch": 27, "training_loss": 68.92219185829163, "training_acc": 52.0, "val_loss": 17.237839102745056, "val_acc": 56.0}
{"epoch": 28, "training_loss": 68.83727192878723, "training_acc": 52.0, "val_loss": 17.241141200065613, "val_acc": 56.0}
{"epoch": 29, "training_loss": 68.81870460510254, "training_acc": 52.0, "val_loss": 17.247433960437775, "val_acc": 56.0}
{"epoch": 30, "training_loss": 68.81415629386902, "training_acc": 52.0, "val_loss": 17.246855795383453, "val_acc": 56.0}
{"epoch": 31, "training_loss": 68.81144499778748, "training_acc": 52.0, "val_loss": 17.24606305360794, "val_acc": 56.0}
{"epoch": 32, "training_loss": 68.76963019371033, "training_acc": 52.0, "val_loss": 17.242753505706787, "val_acc": 56.0}
{"epoch": 33, "training_loss": 68.53114342689514, "training_acc": 52.0, "val_loss": 17.24705845117569, "val_acc": 56.0}
{"epoch": 34, "training_loss": 68.79346108436584, "training_acc": 52.0, "val_loss": 17.24778711795807, "val_acc": 56.0}
{"epoch": 35, "training_loss": 68.70237731933594, "training_acc": 52.0, "val_loss": 17.25488305091858, "val_acc": 56.0}
{"epoch": 36, "training_loss": 68.57397031784058, "training_acc": 52.0, "val_loss": 17.26146638393402, "val_acc": 56.0}
{"epoch": 37, "training_loss": 68.57923221588135, "training_acc": 52.0, "val_loss": 17.261144518852234, "val_acc": 56.0}
{"epoch": 38, "training_loss": 68.6660463809967, "training_acc": 52.0, "val_loss": 17.253081500530243, "val_acc": 56.0}
{"epoch": 39, "training_loss": 68.77185583114624, "training_acc": 52.0, "val_loss": 17.24257618188858, "val_acc": 56.0}
{"epoch": 40, "training_loss": 68.70256447792053, "training_acc": 52.0, "val_loss": 17.230169475078583, "val_acc": 56.0}
{"epoch": 41, "training_loss": 68.68824315071106, "training_acc": 52.0, "val_loss": 17.223693430423737, "val_acc": 56.0}
