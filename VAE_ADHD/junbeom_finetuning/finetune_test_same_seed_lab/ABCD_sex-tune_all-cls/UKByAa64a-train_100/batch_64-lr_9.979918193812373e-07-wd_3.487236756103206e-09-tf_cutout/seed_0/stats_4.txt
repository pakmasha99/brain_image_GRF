"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.22022247314453, "training_acc": 53.0, "val_loss": 17.348864674568176, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.27138257026672, "training_acc": 53.0, "val_loss": 17.345553636550903, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.10159611701965, "training_acc": 53.0, "val_loss": 17.346487939357758, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.15448117256165, "training_acc": 53.0, "val_loss": 17.344729602336884, "val_acc": 52.0}
{"epoch": 4, "training_loss": 68.97368788719177, "training_acc": 53.0, "val_loss": 17.34701097011566, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.11521506309509, "training_acc": 53.0, "val_loss": 17.348657548427582, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.057452917099, "training_acc": 53.0, "val_loss": 17.354151606559753, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.0787422657013, "training_acc": 53.0, "val_loss": 17.358799278736115, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.22141313552856, "training_acc": 53.0, "val_loss": 17.36053228378296, "val_acc": 52.0}
{"epoch": 9, "training_loss": 68.99703907966614, "training_acc": 53.0, "val_loss": 17.361272871494293, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.11793637275696, "training_acc": 53.0, "val_loss": 17.361842095851898, "val_acc": 52.0}
{"epoch": 11, "training_loss": 68.99425435066223, "training_acc": 53.0, "val_loss": 17.35687404870987, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.03459763526917, "training_acc": 53.0, "val_loss": 17.348098754882812, "val_acc": 52.0}
{"epoch": 13, "training_loss": 68.98172903060913, "training_acc": 53.0, "val_loss": 17.342573404312134, "val_acc": 52.0}
{"epoch": 14, "training_loss": 68.9319851398468, "training_acc": 53.0, "val_loss": 17.33832359313965, "val_acc": 52.0}
{"epoch": 15, "training_loss": 68.88451337814331, "training_acc": 53.0, "val_loss": 17.334476113319397, "val_acc": 52.0}
{"epoch": 16, "training_loss": 68.91449666023254, "training_acc": 53.0, "val_loss": 17.33330935239792, "val_acc": 52.0}
{"epoch": 17, "training_loss": 68.83609008789062, "training_acc": 53.0, "val_loss": 17.333674430847168, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.08827137947083, "training_acc": 53.0, "val_loss": 17.33267605304718, "val_acc": 52.0}
{"epoch": 19, "training_loss": 68.88237404823303, "training_acc": 53.0, "val_loss": 17.33393967151642, "val_acc": 52.0}
{"epoch": 20, "training_loss": 68.80111241340637, "training_acc": 53.0, "val_loss": 17.335395514965057, "val_acc": 52.0}
{"epoch": 21, "training_loss": 68.74517464637756, "training_acc": 53.0, "val_loss": 17.33546555042267, "val_acc": 52.0}
{"epoch": 22, "training_loss": 68.74998188018799, "training_acc": 53.0, "val_loss": 17.334342002868652, "val_acc": 52.0}
{"epoch": 23, "training_loss": 68.83690905570984, "training_acc": 53.0, "val_loss": 17.327773571014404, "val_acc": 52.0}
{"epoch": 24, "training_loss": 68.87916731834412, "training_acc": 53.0, "val_loss": 17.319437861442566, "val_acc": 52.0}
{"epoch": 25, "training_loss": 68.88252234458923, "training_acc": 53.0, "val_loss": 17.316432297229767, "val_acc": 52.0}
{"epoch": 26, "training_loss": 68.67302632331848, "training_acc": 53.0, "val_loss": 17.315402626991272, "val_acc": 52.0}
{"epoch": 27, "training_loss": 68.6819236278534, "training_acc": 53.0, "val_loss": 17.314407229423523, "val_acc": 52.0}
{"epoch": 28, "training_loss": 68.94609832763672, "training_acc": 53.0, "val_loss": 17.317676544189453, "val_acc": 52.0}
{"epoch": 29, "training_loss": 68.78363490104675, "training_acc": 53.0, "val_loss": 17.321541905403137, "val_acc": 52.0}
{"epoch": 30, "training_loss": 68.73025631904602, "training_acc": 53.0, "val_loss": 17.331288754940033, "val_acc": 52.0}
{"epoch": 31, "training_loss": 68.60494565963745, "training_acc": 53.0, "val_loss": 17.341330647468567, "val_acc": 52.0}
{"epoch": 32, "training_loss": 68.7802963256836, "training_acc": 53.0, "val_loss": 17.343974113464355, "val_acc": 52.0}
{"epoch": 33, "training_loss": 68.55446600914001, "training_acc": 53.0, "val_loss": 17.342230677604675, "val_acc": 52.0}
{"epoch": 34, "training_loss": 68.5571403503418, "training_acc": 53.0, "val_loss": 17.337800562381744, "val_acc": 52.0}
{"epoch": 35, "training_loss": 68.57819032669067, "training_acc": 53.0, "val_loss": 17.333507537841797, "val_acc": 52.0}
{"epoch": 36, "training_loss": 68.67511630058289, "training_acc": 53.0, "val_loss": 17.329291999340057, "val_acc": 52.0}
{"epoch": 37, "training_loss": 68.47295880317688, "training_acc": 53.0, "val_loss": 17.322130501270294, "val_acc": 52.0}
{"epoch": 38, "training_loss": 68.52325820922852, "training_acc": 53.0, "val_loss": 17.319999635219574, "val_acc": 52.0}
{"epoch": 39, "training_loss": 68.6855251789093, "training_acc": 53.0, "val_loss": 17.316967248916626, "val_acc": 52.0}
{"epoch": 40, "training_loss": 68.80291366577148, "training_acc": 53.0, "val_loss": 17.32316166162491, "val_acc": 52.0}
{"epoch": 41, "training_loss": 68.56380009651184, "training_acc": 53.0, "val_loss": 17.329539358615875, "val_acc": 52.0}
{"epoch": 42, "training_loss": 68.46385955810547, "training_acc": 53.0, "val_loss": 17.333032190799713, "val_acc": 52.0}
{"epoch": 43, "training_loss": 68.36823797225952, "training_acc": 53.0, "val_loss": 17.32954978942871, "val_acc": 52.0}
{"epoch": 44, "training_loss": 68.685302734375, "training_acc": 53.0, "val_loss": 17.324575781822205, "val_acc": 52.0}
{"epoch": 45, "training_loss": 68.4482274055481, "training_acc": 53.0, "val_loss": 17.321528494358063, "val_acc": 52.0}
{"epoch": 46, "training_loss": 68.42180824279785, "training_acc": 53.0, "val_loss": 17.325682938098907, "val_acc": 52.0}
