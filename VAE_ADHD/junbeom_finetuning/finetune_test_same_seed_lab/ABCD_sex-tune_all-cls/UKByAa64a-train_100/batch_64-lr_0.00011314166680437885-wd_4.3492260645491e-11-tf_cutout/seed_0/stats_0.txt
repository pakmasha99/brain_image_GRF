"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.3414990901947, "training_acc": 53.0, "val_loss": 17.284215986728668, "val_acc": 56.0}
{"epoch": 1, "training_loss": 69.16971373558044, "training_acc": 52.0, "val_loss": 17.29278564453125, "val_acc": 56.0}
{"epoch": 2, "training_loss": 69.34615445137024, "training_acc": 52.0, "val_loss": 17.258933186531067, "val_acc": 56.0}
{"epoch": 3, "training_loss": 69.29384899139404, "training_acc": 52.0, "val_loss": 17.302757501602173, "val_acc": 56.0}
{"epoch": 4, "training_loss": 69.2019190788269, "training_acc": 49.0, "val_loss": 17.283278703689575, "val_acc": 56.0}
{"epoch": 5, "training_loss": 69.52895855903625, "training_acc": 52.0, "val_loss": 17.24778264760971, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.26769351959229, "training_acc": 52.0, "val_loss": 17.31598675251007, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.26006245613098, "training_acc": 57.0, "val_loss": 17.2477126121521, "val_acc": 56.0}
{"epoch": 8, "training_loss": 69.6126720905304, "training_acc": 52.0, "val_loss": 17.20840036869049, "val_acc": 56.0}
{"epoch": 9, "training_loss": 69.17793893814087, "training_acc": 52.0, "val_loss": 17.293715476989746, "val_acc": 56.0}
{"epoch": 10, "training_loss": 69.23088383674622, "training_acc": 54.0, "val_loss": 17.313678562641144, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.29961705207825, "training_acc": 52.0, "val_loss": 17.247210443019867, "val_acc": 56.0}
{"epoch": 12, "training_loss": 69.26617550849915, "training_acc": 52.0, "val_loss": 17.234843969345093, "val_acc": 56.0}
{"epoch": 13, "training_loss": 69.23199653625488, "training_acc": 52.0, "val_loss": 17.275823652744293, "val_acc": 56.0}
{"epoch": 14, "training_loss": 69.20755314826965, "training_acc": 52.0, "val_loss": 17.228572070598602, "val_acc": 56.0}
{"epoch": 15, "training_loss": 69.11351776123047, "training_acc": 52.0, "val_loss": 17.19110608100891, "val_acc": 56.0}
{"epoch": 16, "training_loss": 69.17953562736511, "training_acc": 52.0, "val_loss": 17.181991040706635, "val_acc": 56.0}
{"epoch": 17, "training_loss": 69.31758999824524, "training_acc": 52.0, "val_loss": 17.251382768154144, "val_acc": 56.0}
{"epoch": 18, "training_loss": 69.17683935165405, "training_acc": 52.0, "val_loss": 17.31339544057846, "val_acc": 56.0}
{"epoch": 19, "training_loss": 69.21132755279541, "training_acc": 51.0, "val_loss": 17.302779853343964, "val_acc": 56.0}
{"epoch": 20, "training_loss": 69.21979641914368, "training_acc": 51.0, "val_loss": 17.28226989507675, "val_acc": 56.0}
{"epoch": 21, "training_loss": 69.08658456802368, "training_acc": 52.0, "val_loss": 17.20993220806122, "val_acc": 56.0}
{"epoch": 22, "training_loss": 69.71354508399963, "training_acc": 52.0, "val_loss": 17.17536747455597, "val_acc": 56.0}
{"epoch": 23, "training_loss": 69.34870648384094, "training_acc": 52.0, "val_loss": 17.2438845038414, "val_acc": 56.0}
{"epoch": 24, "training_loss": 69.11862516403198, "training_acc": 54.0, "val_loss": 17.397227883338928, "val_acc": 56.0}
{"epoch": 25, "training_loss": 69.46913146972656, "training_acc": 48.0, "val_loss": 17.44702458381653, "val_acc": 56.0}
{"epoch": 26, "training_loss": 69.84936833381653, "training_acc": 38.0, "val_loss": 17.31167584657669, "val_acc": 56.0}
{"epoch": 27, "training_loss": 69.27389240264893, "training_acc": 52.0, "val_loss": 17.321227490901947, "val_acc": 56.0}
{"epoch": 28, "training_loss": 69.2299702167511, "training_acc": 57.0, "val_loss": 17.299784719944, "val_acc": 56.0}
{"epoch": 29, "training_loss": 69.14940619468689, "training_acc": 52.0, "val_loss": 17.21758544445038, "val_acc": 56.0}
{"epoch": 30, "training_loss": 69.58693027496338, "training_acc": 52.0, "val_loss": 17.180150747299194, "val_acc": 56.0}
{"epoch": 31, "training_loss": 69.52730011940002, "training_acc": 52.0, "val_loss": 17.23378896713257, "val_acc": 56.0}
{"epoch": 32, "training_loss": 69.22019672393799, "training_acc": 52.0, "val_loss": 17.232154309749603, "val_acc": 56.0}
{"epoch": 33, "training_loss": 69.08668041229248, "training_acc": 52.0, "val_loss": 17.26531684398651, "val_acc": 56.0}
{"epoch": 34, "training_loss": 69.12939763069153, "training_acc": 52.0, "val_loss": 17.28501468896866, "val_acc": 56.0}
{"epoch": 35, "training_loss": 69.21596264839172, "training_acc": 52.0, "val_loss": 17.328472435474396, "val_acc": 56.0}
{"epoch": 36, "training_loss": 68.95949673652649, "training_acc": 55.0, "val_loss": 17.29694902896881, "val_acc": 56.0}
{"epoch": 37, "training_loss": 69.02837443351746, "training_acc": 53.0, "val_loss": 17.22566932439804, "val_acc": 56.0}
{"epoch": 38, "training_loss": 68.929682970047, "training_acc": 52.0, "val_loss": 17.22329705953598, "val_acc": 56.0}
{"epoch": 39, "training_loss": 69.0778603553772, "training_acc": 52.0, "val_loss": 17.336982488632202, "val_acc": 56.0}
{"epoch": 40, "training_loss": 68.70328545570374, "training_acc": 51.0, "val_loss": 17.416030168533325, "val_acc": 56.0}
{"epoch": 41, "training_loss": 69.25775575637817, "training_acc": 49.0, "val_loss": 17.39245355129242, "val_acc": 56.0}
