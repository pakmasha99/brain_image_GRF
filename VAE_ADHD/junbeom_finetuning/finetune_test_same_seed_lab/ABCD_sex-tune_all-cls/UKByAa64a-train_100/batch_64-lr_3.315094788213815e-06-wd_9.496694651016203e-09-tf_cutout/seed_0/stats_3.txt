"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.67279362678528, "training_acc": 53.0, "val_loss": 17.512615025043488, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.48361563682556, "training_acc": 53.0, "val_loss": 17.49284863471985, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.62026500701904, "training_acc": 53.0, "val_loss": 17.480966448783875, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.48345708847046, "training_acc": 53.0, "val_loss": 17.469841241836548, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.40597534179688, "training_acc": 53.0, "val_loss": 17.47221350669861, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.32599520683289, "training_acc": 53.0, "val_loss": 17.475001513957977, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.2447350025177, "training_acc": 53.0, "val_loss": 17.47787445783615, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.20312309265137, "training_acc": 53.0, "val_loss": 17.475029826164246, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.16759967803955, "training_acc": 53.0, "val_loss": 17.4662247300148, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.09645056724548, "training_acc": 53.0, "val_loss": 17.455431818962097, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.01386380195618, "training_acc": 53.0, "val_loss": 17.456255853176117, "val_acc": 52.0}
{"epoch": 11, "training_loss": 68.81056666374207, "training_acc": 53.0, "val_loss": 17.457038164138794, "val_acc": 52.0}
{"epoch": 12, "training_loss": 68.86277437210083, "training_acc": 53.0, "val_loss": 17.453140020370483, "val_acc": 52.0}
{"epoch": 13, "training_loss": 68.76054501533508, "training_acc": 53.0, "val_loss": 17.463377118110657, "val_acc": 52.0}
{"epoch": 14, "training_loss": 68.77996802330017, "training_acc": 53.0, "val_loss": 17.443343997001648, "val_acc": 52.0}
{"epoch": 15, "training_loss": 68.61382961273193, "training_acc": 53.0, "val_loss": 17.397063970565796, "val_acc": 52.0}
{"epoch": 16, "training_loss": 68.79968929290771, "training_acc": 53.0, "val_loss": 17.41432100534439, "val_acc": 52.0}
{"epoch": 17, "training_loss": 68.41693830490112, "training_acc": 53.0, "val_loss": 17.433148622512817, "val_acc": 52.0}
{"epoch": 18, "training_loss": 68.39395833015442, "training_acc": 53.0, "val_loss": 17.43992567062378, "val_acc": 52.0}
{"epoch": 19, "training_loss": 68.4545521736145, "training_acc": 53.0, "val_loss": 17.42057055234909, "val_acc": 52.0}
{"epoch": 20, "training_loss": 68.29449796676636, "training_acc": 53.0, "val_loss": 17.366038262844086, "val_acc": 52.0}
{"epoch": 21, "training_loss": 68.42309069633484, "training_acc": 53.0, "val_loss": 17.358218133449554, "val_acc": 52.0}
{"epoch": 22, "training_loss": 68.53783917427063, "training_acc": 53.0, "val_loss": 17.4303337931633, "val_acc": 52.0}
{"epoch": 23, "training_loss": 68.27622175216675, "training_acc": 53.0, "val_loss": 17.435158789157867, "val_acc": 52.0}
{"epoch": 24, "training_loss": 68.59343767166138, "training_acc": 53.0, "val_loss": 17.44152009487152, "val_acc": 52.0}
{"epoch": 25, "training_loss": 68.36830830574036, "training_acc": 53.0, "val_loss": 17.42994338274002, "val_acc": 52.0}
{"epoch": 26, "training_loss": 68.0166826248169, "training_acc": 53.0, "val_loss": 17.386150360107422, "val_acc": 52.0}
{"epoch": 27, "training_loss": 67.69707560539246, "training_acc": 53.0, "val_loss": 17.34398454427719, "val_acc": 52.0}
{"epoch": 28, "training_loss": 68.24243783950806, "training_acc": 53.0, "val_loss": 17.395013570785522, "val_acc": 52.0}
{"epoch": 29, "training_loss": 67.95811462402344, "training_acc": 53.0, "val_loss": 17.456477880477905, "val_acc": 52.0}
{"epoch": 30, "training_loss": 67.95933866500854, "training_acc": 53.0, "val_loss": 17.472834885120392, "val_acc": 52.0}
{"epoch": 31, "training_loss": 67.66512823104858, "training_acc": 53.0, "val_loss": 17.45705157518387, "val_acc": 52.0}
{"epoch": 32, "training_loss": 67.55907440185547, "training_acc": 54.0, "val_loss": 17.467059195041656, "val_acc": 52.0}
{"epoch": 33, "training_loss": 67.49756240844727, "training_acc": 53.0, "val_loss": 17.506064474582672, "val_acc": 52.0}
{"epoch": 34, "training_loss": 67.89751768112183, "training_acc": 53.0, "val_loss": 17.492488026618958, "val_acc": 52.0}
{"epoch": 35, "training_loss": 67.27389764785767, "training_acc": 54.0, "val_loss": 17.48650372028351, "val_acc": 52.0}
{"epoch": 36, "training_loss": 67.15560102462769, "training_acc": 53.0, "val_loss": 17.5082266330719, "val_acc": 52.0}
{"epoch": 37, "training_loss": 67.24699330329895, "training_acc": 53.0, "val_loss": 17.553284764289856, "val_acc": 52.0}
{"epoch": 38, "training_loss": 67.48303890228271, "training_acc": 53.0, "val_loss": 17.544275522232056, "val_acc": 52.0}
{"epoch": 39, "training_loss": 67.4686963558197, "training_acc": 55.0, "val_loss": 17.423903942108154, "val_acc": 52.0}
{"epoch": 40, "training_loss": 67.32799196243286, "training_acc": 60.0, "val_loss": 17.431166768074036, "val_acc": 52.0}
{"epoch": 41, "training_loss": 66.57805943489075, "training_acc": 61.0, "val_loss": 17.596793174743652, "val_acc": 52.0}
{"epoch": 42, "training_loss": 67.58682870864868, "training_acc": 53.0, "val_loss": 17.591865360736847, "val_acc": 52.0}
{"epoch": 43, "training_loss": 66.83840084075928, "training_acc": 53.0, "val_loss": 17.496605217456818, "val_acc": 52.0}
{"epoch": 44, "training_loss": 66.6065526008606, "training_acc": 65.0, "val_loss": 17.44726002216339, "val_acc": 52.0}
{"epoch": 45, "training_loss": 66.34489750862122, "training_acc": 64.0, "val_loss": 17.617295682430267, "val_acc": 52.0}
{"epoch": 46, "training_loss": 66.10584235191345, "training_acc": 58.0, "val_loss": 17.664405703544617, "val_acc": 52.0}
