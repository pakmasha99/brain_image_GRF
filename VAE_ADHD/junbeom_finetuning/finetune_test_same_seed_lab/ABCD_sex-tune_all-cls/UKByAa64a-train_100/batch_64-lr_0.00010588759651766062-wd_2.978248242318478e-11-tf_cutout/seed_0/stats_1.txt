"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 70.13555932044983, "training_acc": 47.0, "val_loss": 17.42672622203827, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.47886681556702, "training_acc": 47.0, "val_loss": 17.35699772834778, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.25706100463867, "training_acc": 50.0, "val_loss": 17.350758612155914, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.23315072059631, "training_acc": 53.0, "val_loss": 17.385657131671906, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.82355070114136, "training_acc": 53.0, "val_loss": 17.360305786132812, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.08022379875183, "training_acc": 53.0, "val_loss": 17.31155812740326, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.46552610397339, "training_acc": 44.0, "val_loss": 17.312392592430115, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.53940606117249, "training_acc": 53.0, "val_loss": 17.314237356185913, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.13603663444519, "training_acc": 53.0, "val_loss": 17.31238067150116, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.27088117599487, "training_acc": 53.0, "val_loss": 17.316950857639313, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.0988039970398, "training_acc": 53.0, "val_loss": 17.353960871696472, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.20800805091858, "training_acc": 53.0, "val_loss": 17.37687587738037, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.26592516899109, "training_acc": 53.0, "val_loss": 17.37283021211624, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.18308520317078, "training_acc": 53.0, "val_loss": 17.325280606746674, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.19108414649963, "training_acc": 53.0, "val_loss": 17.312316596508026, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.13939142227173, "training_acc": 53.0, "val_loss": 17.312024533748627, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.16978311538696, "training_acc": 53.0, "val_loss": 17.314784228801727, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.19923067092896, "training_acc": 53.0, "val_loss": 17.330367863178253, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.17930698394775, "training_acc": 53.0, "val_loss": 17.365045845508575, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.33664417266846, "training_acc": 53.0, "val_loss": 17.349253594875336, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.26112842559814, "training_acc": 53.0, "val_loss": 17.344409227371216, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.26309299468994, "training_acc": 53.0, "val_loss": 17.314007878303528, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.21110606193542, "training_acc": 53.0, "val_loss": 17.31763482093811, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.2480583190918, "training_acc": 53.0, "val_loss": 17.315921187400818, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.50474524497986, "training_acc": 39.0, "val_loss": 17.310845851898193, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.16290330886841, "training_acc": 53.0, "val_loss": 17.33386218547821, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.14917325973511, "training_acc": 53.0, "val_loss": 17.362716794013977, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.26772618293762, "training_acc": 53.0, "val_loss": 17.362843453884125, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.3528528213501, "training_acc": 53.0, "val_loss": 17.35604852437973, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.18822407722473, "training_acc": 53.0, "val_loss": 17.31530725955963, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.11241245269775, "training_acc": 53.0, "val_loss": 17.3095241189003, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.43500232696533, "training_acc": 53.0, "val_loss": 17.317773401737213, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.20261788368225, "training_acc": 53.0, "val_loss": 17.30927973985672, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.18701219558716, "training_acc": 53.0, "val_loss": 17.326056957244873, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.14564275741577, "training_acc": 53.0, "val_loss": 17.338740825653076, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.23145031929016, "training_acc": 53.0, "val_loss": 17.334556579589844, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.1524806022644, "training_acc": 53.0, "val_loss": 17.338337004184723, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.16421723365784, "training_acc": 53.0, "val_loss": 17.333945631980896, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.26862812042236, "training_acc": 53.0, "val_loss": 17.32654869556427, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.17637634277344, "training_acc": 53.0, "val_loss": 17.309294641017914, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.15108919143677, "training_acc": 53.0, "val_loss": 17.30968803167343, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.18569540977478, "training_acc": 53.0, "val_loss": 17.30949431657791, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.18000984191895, "training_acc": 53.0, "val_loss": 17.310301959514618, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.1382532119751, "training_acc": 53.0, "val_loss": 17.313796281814575, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.13084435462952, "training_acc": 53.0, "val_loss": 17.32543110847473, "val_acc": 52.0}
{"epoch": 45, "training_loss": 69.14484238624573, "training_acc": 53.0, "val_loss": 17.332366108894348, "val_acc": 52.0}
{"epoch": 46, "training_loss": 69.18537855148315, "training_acc": 53.0, "val_loss": 17.328262329101562, "val_acc": 52.0}
{"epoch": 47, "training_loss": 69.1703999042511, "training_acc": 53.0, "val_loss": 17.328238487243652, "val_acc": 52.0}
{"epoch": 48, "training_loss": 69.36730146408081, "training_acc": 53.0, "val_loss": 17.315372824668884, "val_acc": 52.0}
{"epoch": 49, "training_loss": 69.12953925132751, "training_acc": 53.0, "val_loss": 17.32136905193329, "val_acc": 52.0}
{"epoch": 50, "training_loss": 69.1371853351593, "training_acc": 53.0, "val_loss": 17.33240634202957, "val_acc": 52.0}
{"epoch": 51, "training_loss": 69.13621640205383, "training_acc": 53.0, "val_loss": 17.350974678993225, "val_acc": 52.0}
