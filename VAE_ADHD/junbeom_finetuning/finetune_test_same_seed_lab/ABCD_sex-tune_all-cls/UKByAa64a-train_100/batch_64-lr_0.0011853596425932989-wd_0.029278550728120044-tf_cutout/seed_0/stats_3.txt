"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.05834341049194, "training_acc": 54.0, "val_loss": 17.342884838581085, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.72036409378052, "training_acc": 47.0, "val_loss": 17.523352801799774, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.38652777671814, "training_acc": 53.0, "val_loss": 17.338387668132782, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.31063389778137, "training_acc": 49.0, "val_loss": 17.338502407073975, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.57564663887024, "training_acc": 53.0, "val_loss": 17.323924601078033, "val_acc": 52.0}
{"epoch": 5, "training_loss": 71.09996056556702, "training_acc": 43.0, "val_loss": 17.315375804901123, "val_acc": 52.0}
{"epoch": 6, "training_loss": 68.8889672756195, "training_acc": 53.0, "val_loss": 17.624886333942413, "val_acc": 52.0}
{"epoch": 7, "training_loss": 70.13490986824036, "training_acc": 53.0, "val_loss": 17.759746313095093, "val_acc": 52.0}
{"epoch": 8, "training_loss": 70.50780129432678, "training_acc": 53.0, "val_loss": 17.395830154418945, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.38637280464172, "training_acc": 53.0, "val_loss": 17.309926450252533, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.2554726600647, "training_acc": 53.0, "val_loss": 17.30932742357254, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.16132235527039, "training_acc": 53.0, "val_loss": 17.33480989933014, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.149001121521, "training_acc": 53.0, "val_loss": 17.397227883338928, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.50116300582886, "training_acc": 53.0, "val_loss": 17.341579496860504, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.27865290641785, "training_acc": 53.0, "val_loss": 17.315112054347992, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.27290678024292, "training_acc": 53.0, "val_loss": 17.31056421995163, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.32213258743286, "training_acc": 53.0, "val_loss": 17.32870489358902, "val_acc": 52.0}
{"epoch": 17, "training_loss": 70.02173829078674, "training_acc": 53.0, "val_loss": 17.432032525539398, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.54927062988281, "training_acc": 53.0, "val_loss": 17.318598926067352, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.13616609573364, "training_acc": 53.0, "val_loss": 17.309443652629852, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.49719548225403, "training_acc": 53.0, "val_loss": 17.314617335796356, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.21607851982117, "training_acc": 53.0, "val_loss": 17.34989285469055, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.55266308784485, "training_acc": 43.0, "val_loss": 17.323623597621918, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.35171437263489, "training_acc": 53.0, "val_loss": 17.310747504234314, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.37850618362427, "training_acc": 53.0, "val_loss": 17.308753728866577, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.18967270851135, "training_acc": 53.0, "val_loss": 17.308732867240906, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.13961482048035, "training_acc": 53.0, "val_loss": 17.319361865520477, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.13454747200012, "training_acc": 53.0, "val_loss": 17.33826994895935, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.33302021026611, "training_acc": 53.0, "val_loss": 17.333994805812836, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.14984464645386, "training_acc": 53.0, "val_loss": 17.309218645095825, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.17938256263733, "training_acc": 53.0, "val_loss": 17.312675714492798, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.22852396965027, "training_acc": 53.0, "val_loss": 17.30882376432419, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.17915797233582, "training_acc": 53.0, "val_loss": 17.311392724514008, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.60853576660156, "training_acc": 53.0, "val_loss": 17.312949895858765, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.03575611114502, "training_acc": 53.0, "val_loss": 17.407044768333435, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.56508660316467, "training_acc": 53.0, "val_loss": 17.47472584247589, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.70989966392517, "training_acc": 53.0, "val_loss": 17.32732206583023, "val_acc": 52.0}
{"epoch": 37, "training_loss": 68.93830418586731, "training_acc": 53.0, "val_loss": 17.40947514772415, "val_acc": 52.0}
{"epoch": 38, "training_loss": 70.35788798332214, "training_acc": 47.0, "val_loss": 17.485995590686798, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.78269648551941, "training_acc": 47.0, "val_loss": 17.308956384658813, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.22447085380554, "training_acc": 53.0, "val_loss": 17.526571452617645, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.76122450828552, "training_acc": 53.0, "val_loss": 17.535334825515747, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.71134305000305, "training_acc": 53.0, "val_loss": 17.364375293254852, "val_acc": 52.0}
{"epoch": 43, "training_loss": 68.98200702667236, "training_acc": 53.0, "val_loss": 17.333407700061798, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.71234083175659, "training_acc": 47.0, "val_loss": 17.509467899799347, "val_acc": 52.0}
