"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.7086853981018, "training_acc": 47.0, "val_loss": 17.311738431453705, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.05375003814697, "training_acc": 53.0, "val_loss": 17.581593990325928, "val_acc": 52.0}
{"epoch": 2, "training_loss": 70.08195567131042, "training_acc": 53.0, "val_loss": 17.518138885498047, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.68178510665894, "training_acc": 53.0, "val_loss": 17.327971756458282, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.22363591194153, "training_acc": 53.0, "val_loss": 17.32882261276245, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.2868070602417, "training_acc": 53.0, "val_loss": 17.34614074230194, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.24955320358276, "training_acc": 53.0, "val_loss": 17.374907433986664, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.47373294830322, "training_acc": 53.0, "val_loss": 17.338915169239044, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.2791051864624, "training_acc": 53.0, "val_loss": 17.31182634830475, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.20617699623108, "training_acc": 53.0, "val_loss": 17.309917509555817, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.28817296028137, "training_acc": 53.0, "val_loss": 17.339181900024414, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.84419369697571, "training_acc": 53.0, "val_loss": 17.40003526210785, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.44632983207703, "training_acc": 53.0, "val_loss": 17.31826215982437, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.13565993309021, "training_acc": 53.0, "val_loss": 17.308779060840607, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.44758820533752, "training_acc": 53.0, "val_loss": 17.308658361434937, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.14200186729431, "training_acc": 53.0, "val_loss": 17.326006293296814, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.36927700042725, "training_acc": 53.0, "val_loss": 17.322900891304016, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.31376361846924, "training_acc": 53.0, "val_loss": 17.316314578056335, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.34641170501709, "training_acc": 53.0, "val_loss": 17.310702800750732, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.20963978767395, "training_acc": 53.0, "val_loss": 17.309115827083588, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.14482498168945, "training_acc": 53.0, "val_loss": 17.31664687395096, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.12131810188293, "training_acc": 53.0, "val_loss": 17.345114052295685, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.31030774116516, "training_acc": 53.0, "val_loss": 17.34761744737625, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.17626214027405, "training_acc": 53.0, "val_loss": 17.311374843120575, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.14103960990906, "training_acc": 53.0, "val_loss": 17.31029450893402, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.18855381011963, "training_acc": 53.0, "val_loss": 17.310020327568054, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.19279217720032, "training_acc": 53.0, "val_loss": 17.308712005615234, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.42632007598877, "training_acc": 53.0, "val_loss": 17.309539020061493, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.04651188850403, "training_acc": 53.0, "val_loss": 17.356473207473755, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.347163438797, "training_acc": 53.0, "val_loss": 17.42342710494995, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.64148211479187, "training_acc": 53.0, "val_loss": 17.376451194286346, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.11504077911377, "training_acc": 53.0, "val_loss": 17.308616638183594, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.4927887916565, "training_acc": 45.0, "val_loss": 17.338602244853973, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.40134882926941, "training_acc": 47.0, "val_loss": 17.323467135429382, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.251788854599, "training_acc": 53.0, "val_loss": 17.310111224651337, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.09293103218079, "training_acc": 53.0, "val_loss": 17.33611673116684, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.31994318962097, "training_acc": 53.0, "val_loss": 17.37193465232849, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.44704747200012, "training_acc": 53.0, "val_loss": 17.34178066253662, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.23140668869019, "training_acc": 53.0, "val_loss": 17.30903536081314, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.18307137489319, "training_acc": 53.0, "val_loss": 17.31928437948227, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.27413702011108, "training_acc": 53.0, "val_loss": 17.318052053451538, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.25156736373901, "training_acc": 53.0, "val_loss": 17.312338948249817, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.16798877716064, "training_acc": 53.0, "val_loss": 17.31104999780655, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.08694195747375, "training_acc": 53.0, "val_loss": 17.3390194773674, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.2010087966919, "training_acc": 53.0, "val_loss": 17.373748123645782, "val_acc": 52.0}
{"epoch": 45, "training_loss": 69.29179692268372, "training_acc": 53.0, "val_loss": 17.365756630897522, "val_acc": 52.0}
{"epoch": 46, "training_loss": 69.21518778800964, "training_acc": 53.0, "val_loss": 17.327730357646942, "val_acc": 52.0}
{"epoch": 47, "training_loss": 69.33608365058899, "training_acc": 53.0, "val_loss": 17.30867177248001, "val_acc": 52.0}
{"epoch": 48, "training_loss": 69.1547200679779, "training_acc": 53.0, "val_loss": 17.30906069278717, "val_acc": 52.0}
{"epoch": 49, "training_loss": 69.1690514087677, "training_acc": 53.0, "val_loss": 17.30910986661911, "val_acc": 52.0}
{"epoch": 50, "training_loss": 69.22057867050171, "training_acc": 53.0, "val_loss": 17.30867475271225, "val_acc": 52.0}
