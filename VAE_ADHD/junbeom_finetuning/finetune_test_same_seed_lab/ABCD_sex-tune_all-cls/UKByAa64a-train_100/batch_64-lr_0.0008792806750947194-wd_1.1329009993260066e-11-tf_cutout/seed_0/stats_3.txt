"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.39920830726624, "training_acc": 52.0, "val_loss": 17.381589114665985, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.96723127365112, "training_acc": 49.0, "val_loss": 17.330802977085114, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.3168797492981, "training_acc": 53.0, "val_loss": 17.380061745643616, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.54052877426147, "training_acc": 53.0, "val_loss": 17.367805540561676, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.36241221427917, "training_acc": 53.0, "val_loss": 17.37125962972641, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.52535247802734, "training_acc": 53.0, "val_loss": 17.320556938648224, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.34300327301025, "training_acc": 49.0, "val_loss": 17.335017025470734, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.31019568443298, "training_acc": 51.0, "val_loss": 17.310959100723267, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.19173836708069, "training_acc": 53.0, "val_loss": 17.384181916713715, "val_acc": 52.0}
{"epoch": 9, "training_loss": 70.24928760528564, "training_acc": 53.0, "val_loss": 17.4892857670784, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.74679112434387, "training_acc": 53.0, "val_loss": 17.309756577014923, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.16330742835999, "training_acc": 53.0, "val_loss": 17.326711118221283, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.61596417427063, "training_acc": 53.0, "val_loss": 17.31957197189331, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.25534343719482, "training_acc": 53.0, "val_loss": 17.331580817699432, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.46308207511902, "training_acc": 43.0, "val_loss": 17.312568426132202, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.25978326797485, "training_acc": 53.0, "val_loss": 17.3092320561409, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.29739260673523, "training_acc": 53.0, "val_loss": 17.308741807937622, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.20272374153137, "training_acc": 53.0, "val_loss": 17.309901118278503, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.1656506061554, "training_acc": 53.0, "val_loss": 17.311835289001465, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.11514592170715, "training_acc": 53.0, "val_loss": 17.337489128112793, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.32458782196045, "training_acc": 53.0, "val_loss": 17.354026436805725, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.19062781333923, "training_acc": 53.0, "val_loss": 17.312918603420258, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.13895034790039, "training_acc": 53.0, "val_loss": 17.31083244085312, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.19134616851807, "training_acc": 53.0, "val_loss": 17.312148213386536, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.2155544757843, "training_acc": 53.0, "val_loss": 17.308704555034637, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.44913816452026, "training_acc": 53.0, "val_loss": 17.312023043632507, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.023357629776, "training_acc": 53.0, "val_loss": 17.394617199897766, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.5145411491394, "training_acc": 53.0, "val_loss": 17.484603822231293, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.78428769111633, "training_acc": 53.0, "val_loss": 17.36571490764618, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.00235509872437, "training_acc": 53.0, "val_loss": 17.337924242019653, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.9886405467987, "training_acc": 47.0, "val_loss": 17.455974221229553, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.8509681224823, "training_acc": 47.0, "val_loss": 17.33393371105194, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.2820291519165, "training_acc": 49.0, "val_loss": 17.339004576206207, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.11699223518372, "training_acc": 53.0, "val_loss": 17.458346486091614, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.72039484977722, "training_acc": 53.0, "val_loss": 17.470210790634155, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.52279782295227, "training_acc": 53.0, "val_loss": 17.33008772134781, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.30712056159973, "training_acc": 53.0, "val_loss": 17.362602055072784, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.56370091438293, "training_acc": 47.0, "val_loss": 17.398306727409363, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.71378684043884, "training_acc": 47.0, "val_loss": 17.32337474822998, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.2111701965332, "training_acc": 53.0, "val_loss": 17.312897741794586, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.07255101203918, "training_acc": 53.0, "val_loss": 17.368796467781067, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.23999547958374, "training_acc": 53.0, "val_loss": 17.42122173309326, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.41973328590393, "training_acc": 53.0, "val_loss": 17.395155131816864, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.26183843612671, "training_acc": 53.0, "val_loss": 17.32010245323181, "val_acc": 52.0}
