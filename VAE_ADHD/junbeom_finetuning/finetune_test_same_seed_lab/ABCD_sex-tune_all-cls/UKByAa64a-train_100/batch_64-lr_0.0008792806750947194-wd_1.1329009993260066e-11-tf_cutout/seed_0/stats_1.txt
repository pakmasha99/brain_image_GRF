"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 70.00489091873169, "training_acc": 52.0, "val_loss": 17.388612031936646, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.06925964355469, "training_acc": 53.0, "val_loss": 17.330418527126312, "val_acc": 52.0}
{"epoch": 2, "training_loss": 70.66516900062561, "training_acc": 47.0, "val_loss": 17.372386157512665, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.27275490760803, "training_acc": 53.0, "val_loss": 17.41175353527069, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.30782508850098, "training_acc": 53.0, "val_loss": 17.309080064296722, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.38637971878052, "training_acc": 53.0, "val_loss": 17.311665415763855, "val_acc": 52.0}
{"epoch": 6, "training_loss": 70.56591606140137, "training_acc": 41.0, "val_loss": 17.309552431106567, "val_acc": 52.0}
{"epoch": 7, "training_loss": 68.94082951545715, "training_acc": 53.0, "val_loss": 17.554256319999695, "val_acc": 52.0}
{"epoch": 8, "training_loss": 70.1508457660675, "training_acc": 53.0, "val_loss": 17.534999549388885, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.87431001663208, "training_acc": 53.0, "val_loss": 17.433424293994904, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.5239326953888, "training_acc": 53.0, "val_loss": 17.335471510887146, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.24555945396423, "training_acc": 53.0, "val_loss": 17.308814823627472, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.17777347564697, "training_acc": 53.0, "val_loss": 17.33076423406601, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.59992289543152, "training_acc": 41.0, "val_loss": 17.31669455766678, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.37076568603516, "training_acc": 47.0, "val_loss": 17.31518656015396, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.15108561515808, "training_acc": 53.0, "val_loss": 17.32463240623474, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.18955278396606, "training_acc": 53.0, "val_loss": 17.395396530628204, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.32863712310791, "training_acc": 53.0, "val_loss": 17.362019419670105, "val_acc": 52.0}
{"epoch": 18, "training_loss": 70.12296223640442, "training_acc": 53.0, "val_loss": 17.318789660930634, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.41516089439392, "training_acc": 53.0, "val_loss": 17.351849377155304, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.31892132759094, "training_acc": 53.0, "val_loss": 17.31775254011154, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.07939600944519, "training_acc": 53.0, "val_loss": 17.32722818851471, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.50264406204224, "training_acc": 47.0, "val_loss": 17.349687218666077, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.43100810050964, "training_acc": 47.0, "val_loss": 17.30894446372986, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.12144184112549, "training_acc": 53.0, "val_loss": 17.329837381839752, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.14489722251892, "training_acc": 53.0, "val_loss": 17.373088002204895, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.32406163215637, "training_acc": 53.0, "val_loss": 17.37457662820816, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.30557584762573, "training_acc": 53.0, "val_loss": 17.359566688537598, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.17396855354309, "training_acc": 53.0, "val_loss": 17.31194257736206, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.74151659011841, "training_acc": 53.0, "val_loss": 17.31511652469635, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.176917552948, "training_acc": 53.0, "val_loss": 17.314353585243225, "val_acc": 52.0}
