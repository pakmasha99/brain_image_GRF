"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.39108347892761, "training_acc": 45.0, "val_loss": 17.303575575351715, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.36070418357849, "training_acc": 50.0, "val_loss": 17.282548546791077, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.17504549026489, "training_acc": 59.0, "val_loss": 17.277155816555023, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.19162797927856, "training_acc": 50.0, "val_loss": 17.320068180561066, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.11452555656433, "training_acc": 50.0, "val_loss": 17.346160113811493, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.01356220245361, "training_acc": 54.0, "val_loss": 17.362390458583832, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.15117645263672, "training_acc": 50.0, "val_loss": 17.342446744441986, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.07212805747986, "training_acc": 54.0, "val_loss": 17.339572310447693, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.09885001182556, "training_acc": 53.0, "val_loss": 17.32393354177475, "val_acc": 52.0}
{"epoch": 9, "training_loss": 68.89555406570435, "training_acc": 54.0, "val_loss": 17.315801978111267, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.00719952583313, "training_acc": 54.0, "val_loss": 17.33786165714264, "val_acc": 52.0}
{"epoch": 11, "training_loss": 68.88653993606567, "training_acc": 55.0, "val_loss": 17.33674257993698, "val_acc": 52.0}
{"epoch": 12, "training_loss": 68.84601712226868, "training_acc": 53.0, "val_loss": 17.3044815659523, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.1583251953125, "training_acc": 53.0, "val_loss": 17.258447408676147, "val_acc": 52.0}
{"epoch": 14, "training_loss": 68.78724694252014, "training_acc": 53.0, "val_loss": 17.284613847732544, "val_acc": 52.0}
{"epoch": 15, "training_loss": 68.63423919677734, "training_acc": 53.0, "val_loss": 17.24737286567688, "val_acc": 52.0}
{"epoch": 16, "training_loss": 68.72348403930664, "training_acc": 53.0, "val_loss": 17.267337441444397, "val_acc": 52.0}
{"epoch": 17, "training_loss": 68.5898368358612, "training_acc": 53.0, "val_loss": 17.299598455429077, "val_acc": 52.0}
{"epoch": 18, "training_loss": 68.82671332359314, "training_acc": 53.0, "val_loss": 17.352506518363953, "val_acc": 52.0}
{"epoch": 19, "training_loss": 68.7244565486908, "training_acc": 54.0, "val_loss": 17.368434369564056, "val_acc": 52.0}
{"epoch": 20, "training_loss": 68.48927474021912, "training_acc": 53.0, "val_loss": 17.346984148025513, "val_acc": 52.0}
{"epoch": 21, "training_loss": 68.57157444953918, "training_acc": 53.0, "val_loss": 17.305880784988403, "val_acc": 52.0}
{"epoch": 22, "training_loss": 68.57190299034119, "training_acc": 53.0, "val_loss": 17.315150797367096, "val_acc": 52.0}
{"epoch": 23, "training_loss": 68.43994641304016, "training_acc": 53.0, "val_loss": 17.314574122428894, "val_acc": 52.0}
{"epoch": 24, "training_loss": 68.3285641670227, "training_acc": 53.0, "val_loss": 17.317327857017517, "val_acc": 52.0}
{"epoch": 25, "training_loss": 68.49787068367004, "training_acc": 53.0, "val_loss": 17.315948009490967, "val_acc": 52.0}
{"epoch": 26, "training_loss": 68.39706492424011, "training_acc": 53.0, "val_loss": 17.335866391658783, "val_acc": 52.0}
{"epoch": 27, "training_loss": 68.5513699054718, "training_acc": 53.0, "val_loss": 17.369087040424347, "val_acc": 52.0}
{"epoch": 28, "training_loss": 68.49095797538757, "training_acc": 53.0, "val_loss": 17.36418455839157, "val_acc": 52.0}
{"epoch": 29, "training_loss": 68.22859978675842, "training_acc": 53.0, "val_loss": 17.32299029827118, "val_acc": 52.0}
{"epoch": 30, "training_loss": 67.95519423484802, "training_acc": 53.0, "val_loss": 17.306342720985413, "val_acc": 52.0}
{"epoch": 31, "training_loss": 68.0648205280304, "training_acc": 53.0, "val_loss": 17.298991978168488, "val_acc": 52.0}
{"epoch": 32, "training_loss": 67.8959469795227, "training_acc": 53.0, "val_loss": 17.30700433254242, "val_acc": 52.0}
{"epoch": 33, "training_loss": 67.86338782310486, "training_acc": 53.0, "val_loss": 17.31995791196823, "val_acc": 52.0}
{"epoch": 34, "training_loss": 67.69336724281311, "training_acc": 53.0, "val_loss": 17.32218712568283, "val_acc": 52.0}
