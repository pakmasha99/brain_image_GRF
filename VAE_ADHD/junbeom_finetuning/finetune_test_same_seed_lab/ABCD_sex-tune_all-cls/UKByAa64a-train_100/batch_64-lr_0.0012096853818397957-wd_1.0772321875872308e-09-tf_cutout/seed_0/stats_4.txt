"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 105.17389488220215, "training_acc": 45.0, "val_loss": 474.7166156768799, "val_acc": 52.0}
{"epoch": 1, "training_loss": 1074.5281779766083, "training_acc": 55.0, "val_loss": 36.81425750255585, "val_acc": 52.0}
{"epoch": 2, "training_loss": 615.0794792175293, "training_acc": 57.0, "val_loss": 22.5830078125, "val_acc": 48.0}
{"epoch": 3, "training_loss": 84.67980194091797, "training_acc": 47.0, "val_loss": 17.353540658950806, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.76953434944153, "training_acc": 49.0, "val_loss": 17.435935139656067, "val_acc": 52.0}
{"epoch": 5, "training_loss": 71.68036723136902, "training_acc": 47.0, "val_loss": 17.436321079730988, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.43607115745544, "training_acc": 53.0, "val_loss": 17.920878529548645, "val_acc": 52.0}
{"epoch": 7, "training_loss": 70.69900894165039, "training_acc": 53.0, "val_loss": 17.30480194091797, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.06002116203308, "training_acc": 55.0, "val_loss": 17.493128776550293, "val_acc": 52.0}
{"epoch": 9, "training_loss": 70.14643931388855, "training_acc": 47.0, "val_loss": 17.31107085943222, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.18554496765137, "training_acc": 53.0, "val_loss": 17.330443859100342, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.21716737747192, "training_acc": 53.0, "val_loss": 17.341086268424988, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.39738965034485, "training_acc": 53.0, "val_loss": 17.336104810237885, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.1673333644867, "training_acc": 53.0, "val_loss": 17.316608130931854, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.98399472236633, "training_acc": 41.0, "val_loss": 17.318488657474518, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.51726508140564, "training_acc": 53.0, "val_loss": 17.319034039974213, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.14501094818115, "training_acc": 53.0, "val_loss": 17.318905889987946, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.2978823184967, "training_acc": 53.0, "val_loss": 17.312873899936676, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.13976669311523, "training_acc": 53.0, "val_loss": 17.327146232128143, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.17014336585999, "training_acc": 53.0, "val_loss": 17.331431806087494, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.17830562591553, "training_acc": 53.0, "val_loss": 17.32340008020401, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.17544436454773, "training_acc": 53.0, "val_loss": 17.309585213661194, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.23068237304688, "training_acc": 53.0, "val_loss": 17.30813980102539, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.16669130325317, "training_acc": 53.0, "val_loss": 17.311641573905945, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.1549596786499, "training_acc": 53.0, "val_loss": 17.31443703174591, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.13507986068726, "training_acc": 53.0, "val_loss": 17.313875257968903, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.13535761833191, "training_acc": 53.0, "val_loss": 17.31276661157608, "val_acc": 52.0}
