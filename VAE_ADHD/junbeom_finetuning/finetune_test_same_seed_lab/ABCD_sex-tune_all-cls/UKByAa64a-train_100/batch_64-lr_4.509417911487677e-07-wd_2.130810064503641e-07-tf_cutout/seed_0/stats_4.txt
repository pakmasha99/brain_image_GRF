"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.2102484703064, "training_acc": 56.0, "val_loss": 17.296157777309418, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.19636178016663, "training_acc": 52.0, "val_loss": 17.285218834877014, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.16297268867493, "training_acc": 53.0, "val_loss": 17.28024184703827, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.29946637153625, "training_acc": 53.0, "val_loss": 17.279113829135895, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.23276233673096, "training_acc": 53.0, "val_loss": 17.276088893413544, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.20760321617126, "training_acc": 52.0, "val_loss": 17.27551370859146, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.24382019042969, "training_acc": 53.0, "val_loss": 17.275352776050568, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.197345495224, "training_acc": 53.0, "val_loss": 17.27502793073654, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.20019721984863, "training_acc": 53.0, "val_loss": 17.276926338672638, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.19800901412964, "training_acc": 53.0, "val_loss": 17.27665513753891, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.16015291213989, "training_acc": 53.0, "val_loss": 17.27646440267563, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.24304676055908, "training_acc": 53.0, "val_loss": 17.27716475725174, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.2336847782135, "training_acc": 53.0, "val_loss": 17.279791831970215, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.09385848045349, "training_acc": 53.0, "val_loss": 17.28067547082901, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.20306324958801, "training_acc": 53.0, "val_loss": 17.28074997663498, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.03781819343567, "training_acc": 53.0, "val_loss": 17.28178709745407, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.0718126296997, "training_acc": 53.0, "val_loss": 17.282965779304504, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.14460396766663, "training_acc": 53.0, "val_loss": 17.283454537391663, "val_acc": 52.0}
{"epoch": 18, "training_loss": 68.96371507644653, "training_acc": 53.0, "val_loss": 17.28123426437378, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.09145402908325, "training_acc": 53.0, "val_loss": 17.279331386089325, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.07506370544434, "training_acc": 53.0, "val_loss": 17.278467118740082, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.07839798927307, "training_acc": 53.0, "val_loss": 17.279544472694397, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.03985285758972, "training_acc": 53.0, "val_loss": 17.281238734722137, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.0223286151886, "training_acc": 53.0, "val_loss": 17.282283306121826, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.10264468193054, "training_acc": 53.0, "val_loss": 17.281395196914673, "val_acc": 52.0}
{"epoch": 25, "training_loss": 68.95737504959106, "training_acc": 53.0, "val_loss": 17.281383275985718, "val_acc": 52.0}
{"epoch": 26, "training_loss": 68.9668037891388, "training_acc": 53.0, "val_loss": 17.278863489627838, "val_acc": 52.0}
