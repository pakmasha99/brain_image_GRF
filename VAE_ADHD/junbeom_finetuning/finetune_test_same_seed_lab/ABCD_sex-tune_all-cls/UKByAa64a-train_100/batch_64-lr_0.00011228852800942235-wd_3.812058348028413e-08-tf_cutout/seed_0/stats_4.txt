"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 71.15686535835266, "training_acc": 47.0, "val_loss": 17.474353313446045, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.96149134635925, "training_acc": 47.0, "val_loss": 17.336158454418182, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.47528123855591, "training_acc": 43.0, "val_loss": 17.38261729478836, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.86712622642517, "training_acc": 47.0, "val_loss": 17.314326763153076, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.2162139415741, "training_acc": 53.0, "val_loss": 17.319628596305847, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.17315673828125, "training_acc": 53.0, "val_loss": 17.327222228050232, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.36903095245361, "training_acc": 53.0, "val_loss": 17.342999577522278, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.1412844657898, "training_acc": 53.0, "val_loss": 17.37828552722931, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.28204131126404, "training_acc": 53.0, "val_loss": 17.347800731658936, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.20827388763428, "training_acc": 53.0, "val_loss": 17.324741184711456, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.1208119392395, "training_acc": 53.0, "val_loss": 17.3089861869812, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.16268038749695, "training_acc": 53.0, "val_loss": 17.30867773294449, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.15069627761841, "training_acc": 53.0, "val_loss": 17.316560447216034, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.11978459358215, "training_acc": 53.0, "val_loss": 17.346426844596863, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.19498777389526, "training_acc": 53.0, "val_loss": 17.351724207401276, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.2195634841919, "training_acc": 53.0, "val_loss": 17.3243448138237, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.13441014289856, "training_acc": 53.0, "val_loss": 17.311835289001465, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.13256812095642, "training_acc": 53.0, "val_loss": 17.308881878852844, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.15690636634827, "training_acc": 53.0, "val_loss": 17.309723794460297, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.11857628822327, "training_acc": 53.0, "val_loss": 17.32058823108673, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.4989538192749, "training_acc": 53.0, "val_loss": 17.338290810585022, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.15626215934753, "training_acc": 53.0, "val_loss": 17.30899065732956, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.1541678905487, "training_acc": 53.0, "val_loss": 17.310690879821777, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.21308827400208, "training_acc": 53.0, "val_loss": 17.30937659740448, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.2572808265686, "training_acc": 53.0, "val_loss": 17.308717966079712, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.16420149803162, "training_acc": 53.0, "val_loss": 17.309801280498505, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.1982696056366, "training_acc": 53.0, "val_loss": 17.308305203914642, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.16004824638367, "training_acc": 53.0, "val_loss": 17.314735054969788, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.16466283798218, "training_acc": 53.0, "val_loss": 17.33778715133667, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.19875264167786, "training_acc": 53.0, "val_loss": 17.3833429813385, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.35048389434814, "training_acc": 53.0, "val_loss": 17.364101111888885, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.17680239677429, "training_acc": 53.0, "val_loss": 17.316004633903503, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.13937783241272, "training_acc": 53.0, "val_loss": 17.312291264533997, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.29379177093506, "training_acc": 53.0, "val_loss": 17.31579899787903, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.30995559692383, "training_acc": 53.0, "val_loss": 17.30915606021881, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.1436116695404, "training_acc": 53.0, "val_loss": 17.315077781677246, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.32755017280579, "training_acc": 53.0, "val_loss": 17.318490147590637, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.36156058311462, "training_acc": 53.0, "val_loss": 17.34965294599533, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.1760745048523, "training_acc": 53.0, "val_loss": 17.33105033636093, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.3453722000122, "training_acc": 53.0, "val_loss": 17.31541007757187, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.14003705978394, "training_acc": 53.0, "val_loss": 17.320960760116577, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.13105416297913, "training_acc": 53.0, "val_loss": 17.321640253067017, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.16619873046875, "training_acc": 53.0, "val_loss": 17.317956686019897, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.16202211380005, "training_acc": 53.0, "val_loss": 17.309217154979706, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.21928000450134, "training_acc": 53.0, "val_loss": 17.308685183525085, "val_acc": 52.0}
{"epoch": 45, "training_loss": 69.18235087394714, "training_acc": 53.0, "val_loss": 17.316970229148865, "val_acc": 52.0}
