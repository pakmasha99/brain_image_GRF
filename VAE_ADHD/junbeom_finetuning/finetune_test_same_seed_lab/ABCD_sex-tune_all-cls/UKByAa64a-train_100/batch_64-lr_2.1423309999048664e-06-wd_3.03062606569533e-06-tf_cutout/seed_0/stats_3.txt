"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.7004497051239, "training_acc": 53.0, "val_loss": 17.521552741527557, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.68272018432617, "training_acc": 53.0, "val_loss": 17.498941719532013, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.73936009407043, "training_acc": 53.0, "val_loss": 17.4906924366951, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.7677149772644, "training_acc": 53.0, "val_loss": 17.492148280143738, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.55266618728638, "training_acc": 53.0, "val_loss": 17.487017810344696, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.52244830131531, "training_acc": 53.0, "val_loss": 17.476005852222443, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.59119129180908, "training_acc": 53.0, "val_loss": 17.470356822013855, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.35794305801392, "training_acc": 53.0, "val_loss": 17.463260889053345, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.13895535469055, "training_acc": 53.0, "val_loss": 17.467734217643738, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.33136081695557, "training_acc": 53.0, "val_loss": 17.462556064128876, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.10801792144775, "training_acc": 53.0, "val_loss": 17.448502779006958, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.135671377182, "training_acc": 53.0, "val_loss": 17.413237690925598, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.14132785797119, "training_acc": 53.0, "val_loss": 17.396749556064606, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.09369921684265, "training_acc": 53.0, "val_loss": 17.425353825092316, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.03214931488037, "training_acc": 53.0, "val_loss": 17.442616820335388, "val_acc": 52.0}
{"epoch": 15, "training_loss": 68.82361197471619, "training_acc": 53.0, "val_loss": 17.447787523269653, "val_acc": 52.0}
{"epoch": 16, "training_loss": 68.84914350509644, "training_acc": 53.0, "val_loss": 17.446956038475037, "val_acc": 52.0}
{"epoch": 17, "training_loss": 68.76898241043091, "training_acc": 53.0, "val_loss": 17.426766455173492, "val_acc": 52.0}
{"epoch": 18, "training_loss": 68.51688146591187, "training_acc": 53.0, "val_loss": 17.39816963672638, "val_acc": 52.0}
{"epoch": 19, "training_loss": 68.84600043296814, "training_acc": 53.0, "val_loss": 17.379263043403625, "val_acc": 52.0}
{"epoch": 20, "training_loss": 68.63032460212708, "training_acc": 53.0, "val_loss": 17.36106127500534, "val_acc": 52.0}
{"epoch": 21, "training_loss": 68.79838871955872, "training_acc": 53.0, "val_loss": 17.376910150051117, "val_acc": 52.0}
{"epoch": 22, "training_loss": 68.61750149726868, "training_acc": 53.0, "val_loss": 17.40168333053589, "val_acc": 52.0}
{"epoch": 23, "training_loss": 68.45523405075073, "training_acc": 53.0, "val_loss": 17.442229390144348, "val_acc": 52.0}
{"epoch": 24, "training_loss": 68.36552429199219, "training_acc": 53.0, "val_loss": 17.458942532539368, "val_acc": 52.0}
{"epoch": 25, "training_loss": 68.26159024238586, "training_acc": 53.0, "val_loss": 17.443472146987915, "val_acc": 52.0}
{"epoch": 26, "training_loss": 68.55542087554932, "training_acc": 53.0, "val_loss": 17.393556237220764, "val_acc": 52.0}
{"epoch": 27, "training_loss": 68.53403162956238, "training_acc": 53.0, "val_loss": 17.34756976366043, "val_acc": 52.0}
{"epoch": 28, "training_loss": 68.81864070892334, "training_acc": 53.0, "val_loss": 17.384852468967438, "val_acc": 52.0}
{"epoch": 29, "training_loss": 68.55371117591858, "training_acc": 53.0, "val_loss": 17.479851841926575, "val_acc": 52.0}
{"epoch": 30, "training_loss": 68.56766247749329, "training_acc": 53.0, "val_loss": 17.505252361297607, "val_acc": 52.0}
{"epoch": 31, "training_loss": 68.51527571678162, "training_acc": 53.0, "val_loss": 17.50393807888031, "val_acc": 52.0}
{"epoch": 32, "training_loss": 68.3920361995697, "training_acc": 53.0, "val_loss": 17.490120232105255, "val_acc": 52.0}
{"epoch": 33, "training_loss": 68.59721684455872, "training_acc": 53.0, "val_loss": 17.43019074201584, "val_acc": 52.0}
{"epoch": 34, "training_loss": 67.87458181381226, "training_acc": 53.0, "val_loss": 17.389701306819916, "val_acc": 52.0}
{"epoch": 35, "training_loss": 68.23679375648499, "training_acc": 53.0, "val_loss": 17.359191179275513, "val_acc": 52.0}
{"epoch": 36, "training_loss": 68.166499376297, "training_acc": 53.0, "val_loss": 17.382284998893738, "val_acc": 52.0}
{"epoch": 37, "training_loss": 68.4576268196106, "training_acc": 53.0, "val_loss": 17.466606199741364, "val_acc": 52.0}
{"epoch": 38, "training_loss": 67.61533069610596, "training_acc": 53.0, "val_loss": 17.567314207553864, "val_acc": 52.0}
{"epoch": 39, "training_loss": 68.1478009223938, "training_acc": 53.0, "val_loss": 17.587383091449738, "val_acc": 52.0}
{"epoch": 40, "training_loss": 68.09494400024414, "training_acc": 53.0, "val_loss": 17.56007671356201, "val_acc": 52.0}
{"epoch": 41, "training_loss": 67.67066621780396, "training_acc": 53.0, "val_loss": 17.467057704925537, "val_acc": 52.0}
{"epoch": 42, "training_loss": 67.5789909362793, "training_acc": 53.0, "val_loss": 17.38501489162445, "val_acc": 52.0}
{"epoch": 43, "training_loss": 68.08342838287354, "training_acc": 53.0, "val_loss": 17.34904795885086, "val_acc": 52.0}
{"epoch": 44, "training_loss": 67.52547478675842, "training_acc": 53.0, "val_loss": 17.389510571956635, "val_acc": 52.0}
{"epoch": 45, "training_loss": 67.34805989265442, "training_acc": 53.0, "val_loss": 17.542295157909393, "val_acc": 52.0}
{"epoch": 46, "training_loss": 67.59378480911255, "training_acc": 53.0, "val_loss": 17.593924701213837, "val_acc": 52.0}
