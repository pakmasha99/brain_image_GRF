"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.89436769485474, "training_acc": 47.0, "val_loss": 17.424510419368744, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.92456555366516, "training_acc": 47.0, "val_loss": 17.424018681049347, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.75816869735718, "training_acc": 47.0, "val_loss": 17.421279847621918, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.76299691200256, "training_acc": 47.0, "val_loss": 17.418739199638367, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.76598811149597, "training_acc": 47.0, "val_loss": 17.417527735233307, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.6331717967987, "training_acc": 47.0, "val_loss": 17.417819797992706, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.74627375602722, "training_acc": 47.0, "val_loss": 17.419977486133575, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.63311433792114, "training_acc": 47.0, "val_loss": 17.423349618911743, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.54407382011414, "training_acc": 47.0, "val_loss": 17.426586151123047, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.53289985656738, "training_acc": 47.0, "val_loss": 17.42815375328064, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.48249530792236, "training_acc": 47.0, "val_loss": 17.425206303596497, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.51364588737488, "training_acc": 47.0, "val_loss": 17.42350161075592, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.53749632835388, "training_acc": 47.0, "val_loss": 17.41921603679657, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.56399464607239, "training_acc": 47.0, "val_loss": 17.41250306367874, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.60887598991394, "training_acc": 47.0, "val_loss": 17.40272492170334, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.29425764083862, "training_acc": 47.0, "val_loss": 17.396418750286102, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.35838294029236, "training_acc": 47.0, "val_loss": 17.38988310098648, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.71482229232788, "training_acc": 47.0, "val_loss": 17.383289337158203, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.15702390670776, "training_acc": 47.0, "val_loss": 17.377273738384247, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.56244993209839, "training_acc": 47.0, "val_loss": 17.37143248319626, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.24096941947937, "training_acc": 47.0, "val_loss": 17.370663583278656, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.2061538696289, "training_acc": 48.0, "val_loss": 17.372550070285797, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.09305500984192, "training_acc": 49.0, "val_loss": 17.381545901298523, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.16268229484558, "training_acc": 48.0, "val_loss": 17.390723526477814, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.41321420669556, "training_acc": 48.0, "val_loss": 17.396344244480133, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.14264678955078, "training_acc": 53.0, "val_loss": 17.39410310983658, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.38878393173218, "training_acc": 50.0, "val_loss": 17.38961786031723, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.26297616958618, "training_acc": 57.0, "val_loss": 17.39095449447632, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.12136507034302, "training_acc": 54.0, "val_loss": 17.385295033454895, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.01745986938477, "training_acc": 57.0, "val_loss": 17.376437783241272, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.12200164794922, "training_acc": 57.0, "val_loss": 17.363685369491577, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.04432344436646, "training_acc": 57.0, "val_loss": 17.355021834373474, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.24805688858032, "training_acc": 56.0, "val_loss": 17.350241541862488, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.14841365814209, "training_acc": 56.0, "val_loss": 17.34628975391388, "val_acc": 52.0}
{"epoch": 34, "training_loss": 68.91743421554565, "training_acc": 59.0, "val_loss": 17.342564463615417, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.01315379142761, "training_acc": 53.0, "val_loss": 17.34107881784439, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.14588189125061, "training_acc": 57.0, "val_loss": 17.3378124833107, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.1278703212738, "training_acc": 52.0, "val_loss": 17.332755029201508, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.09404873847961, "training_acc": 54.0, "val_loss": 17.32979714870453, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.05659294128418, "training_acc": 56.0, "val_loss": 17.329059541225433, "val_acc": 52.0}
{"epoch": 40, "training_loss": 68.90822243690491, "training_acc": 58.0, "val_loss": 17.334382236003876, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.057461977005, "training_acc": 55.0, "val_loss": 17.343829572200775, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.07817840576172, "training_acc": 53.0, "val_loss": 17.348094284534454, "val_acc": 52.0}
{"epoch": 43, "training_loss": 68.86415576934814, "training_acc": 61.0, "val_loss": 17.34577864408493, "val_acc": 52.0}
{"epoch": 44, "training_loss": 68.91190266609192, "training_acc": 56.0, "val_loss": 17.3330157995224, "val_acc": 52.0}
{"epoch": 45, "training_loss": 68.9574568271637, "training_acc": 55.0, "val_loss": 17.32190102338791, "val_acc": 52.0}
{"epoch": 46, "training_loss": 68.63440942764282, "training_acc": 62.0, "val_loss": 17.306765913963318, "val_acc": 52.0}
{"epoch": 47, "training_loss": 68.91347646713257, "training_acc": 60.0, "val_loss": 17.29816049337387, "val_acc": 52.0}
{"epoch": 48, "training_loss": 69.08489990234375, "training_acc": 56.0, "val_loss": 17.29968935251236, "val_acc": 52.0}
{"epoch": 49, "training_loss": 68.74720406532288, "training_acc": 60.0, "val_loss": 17.31094717979431, "val_acc": 52.0}
{"epoch": 50, "training_loss": 68.53815793991089, "training_acc": 66.0, "val_loss": 17.335273325443268, "val_acc": 52.0}
{"epoch": 51, "training_loss": 69.06937766075134, "training_acc": 53.0, "val_loss": 17.360606789588928, "val_acc": 52.0}
{"epoch": 52, "training_loss": 69.07188153266907, "training_acc": 58.0, "val_loss": 17.380955815315247, "val_acc": 52.0}
{"epoch": 53, "training_loss": 68.88698506355286, "training_acc": 58.0, "val_loss": 17.389845848083496, "val_acc": 52.0}
{"epoch": 54, "training_loss": 68.86159133911133, "training_acc": 57.0, "val_loss": 17.390727996826172, "val_acc": 52.0}
{"epoch": 55, "training_loss": 68.74477863311768, "training_acc": 60.0, "val_loss": 17.382507026195526, "val_acc": 52.0}
{"epoch": 56, "training_loss": 68.63215184211731, "training_acc": 59.0, "val_loss": 17.36472249031067, "val_acc": 52.0}
{"epoch": 57, "training_loss": 68.71200108528137, "training_acc": 58.0, "val_loss": 17.35145002603531, "val_acc": 52.0}
{"epoch": 58, "training_loss": 68.99205732345581, "training_acc": 56.0, "val_loss": 17.342835664749146, "val_acc": 52.0}
{"epoch": 59, "training_loss": 68.85946536064148, "training_acc": 58.0, "val_loss": 17.329631745815277, "val_acc": 52.0}
{"epoch": 60, "training_loss": 68.73525547981262, "training_acc": 57.0, "val_loss": 17.320097982883453, "val_acc": 52.0}
{"epoch": 61, "training_loss": 68.87820172309875, "training_acc": 54.0, "val_loss": 17.311912775039673, "val_acc": 52.0}
{"epoch": 62, "training_loss": 68.69449925422668, "training_acc": 56.0, "val_loss": 17.311803996562958, "val_acc": 52.0}
{"epoch": 63, "training_loss": 68.77178740501404, "training_acc": 55.0, "val_loss": 17.305748164653778, "val_acc": 52.0}
{"epoch": 64, "training_loss": 68.78040862083435, "training_acc": 53.0, "val_loss": 17.298422753810883, "val_acc": 52.0}
{"epoch": 65, "training_loss": 68.62568736076355, "training_acc": 61.0, "val_loss": 17.299160361289978, "val_acc": 52.0}
{"epoch": 66, "training_loss": 68.96392154693604, "training_acc": 53.0, "val_loss": 17.305129766464233, "val_acc": 52.0}
