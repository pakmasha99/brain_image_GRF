"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.26205325126648, "training_acc": 53.0, "val_loss": 17.310012876987457, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.07134556770325, "training_acc": 53.0, "val_loss": 17.311248183250427, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.20501327514648, "training_acc": 53.0, "val_loss": 17.31029599905014, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.0218653678894, "training_acc": 53.0, "val_loss": 17.310544848442078, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.04141759872437, "training_acc": 53.0, "val_loss": 17.308810353279114, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.25776982307434, "training_acc": 53.0, "val_loss": 17.308291792869568, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.152996301651, "training_acc": 53.0, "val_loss": 17.30974316596985, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.06407260894775, "training_acc": 53.0, "val_loss": 17.31317490339279, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.03004765510559, "training_acc": 53.0, "val_loss": 17.31112003326416, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.06193494796753, "training_acc": 53.0, "val_loss": 17.30942130088806, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.0375726222992, "training_acc": 53.0, "val_loss": 17.308440804481506, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.05756878852844, "training_acc": 53.0, "val_loss": 17.3093244433403, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.08914875984192, "training_acc": 53.0, "val_loss": 17.310722172260284, "val_acc": 52.0}
{"epoch": 13, "training_loss": 68.88138580322266, "training_acc": 53.0, "val_loss": 17.313489317893982, "val_acc": 52.0}
{"epoch": 14, "training_loss": 68.98752689361572, "training_acc": 53.0, "val_loss": 17.317308485507965, "val_acc": 52.0}
{"epoch": 15, "training_loss": 68.93012976646423, "training_acc": 53.0, "val_loss": 17.317819595336914, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.08996510505676, "training_acc": 53.0, "val_loss": 17.316873371601105, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.08344626426697, "training_acc": 53.0, "val_loss": 17.31656789779663, "val_acc": 52.0}
{"epoch": 18, "training_loss": 68.92313003540039, "training_acc": 53.0, "val_loss": 17.31564551591873, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.05060267448425, "training_acc": 53.0, "val_loss": 17.315518856048584, "val_acc": 52.0}
{"epoch": 20, "training_loss": 68.92254614830017, "training_acc": 53.0, "val_loss": 17.314539849758148, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.06520199775696, "training_acc": 53.0, "val_loss": 17.313964664936066, "val_acc": 52.0}
{"epoch": 22, "training_loss": 68.94612622261047, "training_acc": 53.0, "val_loss": 17.313620448112488, "val_acc": 52.0}
{"epoch": 23, "training_loss": 68.94435715675354, "training_acc": 53.0, "val_loss": 17.314574122428894, "val_acc": 52.0}
{"epoch": 24, "training_loss": 68.88335418701172, "training_acc": 53.0, "val_loss": 17.313899099826813, "val_acc": 52.0}
