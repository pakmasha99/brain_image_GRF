"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.7918860912323, "training_acc": 53.0, "val_loss": 17.550647258758545, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.89501667022705, "training_acc": 53.0, "val_loss": 17.498458921909332, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.51431155204773, "training_acc": 53.0, "val_loss": 17.3967108130455, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.37009382247925, "training_acc": 53.0, "val_loss": 17.40785390138626, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.0483672618866, "training_acc": 53.0, "val_loss": 17.419326305389404, "val_acc": 52.0}
{"epoch": 5, "training_loss": 68.91831588745117, "training_acc": 53.0, "val_loss": 17.42624044418335, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.09763717651367, "training_acc": 53.0, "val_loss": 17.301854491233826, "val_acc": 52.0}
{"epoch": 7, "training_loss": 68.82228207588196, "training_acc": 53.0, "val_loss": 17.291560769081116, "val_acc": 52.0}
{"epoch": 8, "training_loss": 68.7392349243164, "training_acc": 53.0, "val_loss": 17.303748428821564, "val_acc": 52.0}
{"epoch": 9, "training_loss": 68.78089618682861, "training_acc": 53.0, "val_loss": 17.33918786048889, "val_acc": 52.0}
{"epoch": 10, "training_loss": 68.8848774433136, "training_acc": 53.0, "val_loss": 17.37750619649887, "val_acc": 52.0}
{"epoch": 11, "training_loss": 68.6630368232727, "training_acc": 53.0, "val_loss": 17.344805598258972, "val_acc": 52.0}
{"epoch": 12, "training_loss": 68.5234603881836, "training_acc": 53.0, "val_loss": 17.321549355983734, "val_acc": 52.0}
{"epoch": 13, "training_loss": 68.59598922729492, "training_acc": 53.0, "val_loss": 17.39666908979416, "val_acc": 52.0}
{"epoch": 14, "training_loss": 68.36947631835938, "training_acc": 53.0, "val_loss": 17.437008023262024, "val_acc": 52.0}
{"epoch": 15, "training_loss": 68.3456084728241, "training_acc": 53.0, "val_loss": 17.453938722610474, "val_acc": 52.0}
{"epoch": 16, "training_loss": 68.00264739990234, "training_acc": 53.0, "val_loss": 17.449453473091125, "val_acc": 52.0}
{"epoch": 17, "training_loss": 68.30174946784973, "training_acc": 53.0, "val_loss": 17.51987934112549, "val_acc": 52.0}
{"epoch": 18, "training_loss": 67.55727696418762, "training_acc": 55.0, "val_loss": 17.604102194309235, "val_acc": 52.0}
{"epoch": 19, "training_loss": 67.58797097206116, "training_acc": 54.0, "val_loss": 17.475788295269012, "val_acc": 52.0}
{"epoch": 20, "training_loss": 68.01402592658997, "training_acc": 62.0, "val_loss": 17.5152987241745, "val_acc": 52.0}
{"epoch": 21, "training_loss": 67.84010529518127, "training_acc": 66.0, "val_loss": 17.571337521076202, "val_acc": 52.0}
{"epoch": 22, "training_loss": 67.22305226325989, "training_acc": 51.0, "val_loss": 17.689746618270874, "val_acc": 52.0}
{"epoch": 23, "training_loss": 68.07300400733948, "training_acc": 56.0, "val_loss": 17.566058039665222, "val_acc": 52.0}
{"epoch": 24, "training_loss": 66.92538523674011, "training_acc": 64.0, "val_loss": 17.626899480819702, "val_acc": 52.0}
{"epoch": 25, "training_loss": 66.90528106689453, "training_acc": 63.0, "val_loss": 17.781785130500793, "val_acc": 52.0}
{"epoch": 26, "training_loss": 66.65476703643799, "training_acc": 54.0, "val_loss": 17.49856173992157, "val_acc": 52.0}
