"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.74967360496521, "training_acc": 47.0, "val_loss": 17.326192557811737, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.46146702766418, "training_acc": 44.0, "val_loss": 17.30441302061081, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.25652503967285, "training_acc": 53.0, "val_loss": 17.44362562894821, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.24961996078491, "training_acc": 53.0, "val_loss": 17.369361221790314, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.41872477531433, "training_acc": 53.0, "val_loss": 17.3160582780838, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.1379861831665, "training_acc": 55.0, "val_loss": 17.291423678398132, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.10637307167053, "training_acc": 54.0, "val_loss": 17.28363186120987, "val_acc": 52.0}
{"epoch": 7, "training_loss": 68.95269393920898, "training_acc": 53.0, "val_loss": 17.29077845811844, "val_acc": 52.0}
{"epoch": 8, "training_loss": 68.862788438797, "training_acc": 53.0, "val_loss": 17.272178828716278, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.19394731521606, "training_acc": 52.0, "val_loss": 17.279687523841858, "val_acc": 52.0}
{"epoch": 10, "training_loss": 68.9464225769043, "training_acc": 53.0, "val_loss": 17.298033833503723, "val_acc": 52.0}
{"epoch": 11, "training_loss": 68.57014966011047, "training_acc": 53.0, "val_loss": 17.345820367336273, "val_acc": 52.0}
{"epoch": 12, "training_loss": 68.60060048103333, "training_acc": 53.0, "val_loss": 17.402032017707825, "val_acc": 52.0}
{"epoch": 13, "training_loss": 68.67706370353699, "training_acc": 53.0, "val_loss": 17.42769181728363, "val_acc": 52.0}
{"epoch": 14, "training_loss": 68.86013793945312, "training_acc": 53.0, "val_loss": 17.381170392036438, "val_acc": 52.0}
{"epoch": 15, "training_loss": 68.42601490020752, "training_acc": 53.0, "val_loss": 17.315194010734558, "val_acc": 52.0}
{"epoch": 16, "training_loss": 68.39463686943054, "training_acc": 55.0, "val_loss": 17.351490259170532, "val_acc": 52.0}
{"epoch": 17, "training_loss": 68.41985821723938, "training_acc": 70.0, "val_loss": 17.33887493610382, "val_acc": 52.0}
{"epoch": 18, "training_loss": 67.98290252685547, "training_acc": 66.0, "val_loss": 17.313973605632782, "val_acc": 52.0}
{"epoch": 19, "training_loss": 68.1063482761383, "training_acc": 54.0, "val_loss": 17.349599301815033, "val_acc": 52.0}
{"epoch": 20, "training_loss": 67.85761189460754, "training_acc": 53.0, "val_loss": 17.302751541137695, "val_acc": 52.0}
{"epoch": 21, "training_loss": 67.3340196609497, "training_acc": 56.0, "val_loss": 17.518208920955658, "val_acc": 52.0}
{"epoch": 22, "training_loss": 67.42376947402954, "training_acc": 57.0, "val_loss": 17.305953800678253, "val_acc": 52.0}
{"epoch": 23, "training_loss": 66.63247871398926, "training_acc": 53.0, "val_loss": 17.243659496307373, "val_acc": 52.0}
{"epoch": 24, "training_loss": 66.01810765266418, "training_acc": 69.0, "val_loss": 17.20777302980423, "val_acc": 52.0}
{"epoch": 25, "training_loss": 64.33232092857361, "training_acc": 62.0, "val_loss": 17.419426143169403, "val_acc": 52.0}
{"epoch": 26, "training_loss": 66.67785668373108, "training_acc": 61.0, "val_loss": 17.466731369495392, "val_acc": 52.0}
{"epoch": 27, "training_loss": 66.53602266311646, "training_acc": 54.0, "val_loss": 17.120251059532166, "val_acc": 52.0}
{"epoch": 28, "training_loss": 64.17772507667542, "training_acc": 66.0, "val_loss": 17.3867329955101, "val_acc": 52.0}
{"epoch": 29, "training_loss": 63.04494833946228, "training_acc": 68.0, "val_loss": 17.77721345424652, "val_acc": 52.0}
{"epoch": 30, "training_loss": 66.17664408683777, "training_acc": 54.0, "val_loss": 17.150145769119263, "val_acc": 52.0}
{"epoch": 31, "training_loss": 62.07657170295715, "training_acc": 75.0, "val_loss": 17.06078350543976, "val_acc": 52.0}
{"epoch": 32, "training_loss": 57.14098405838013, "training_acc": 87.0, "val_loss": 17.19065010547638, "val_acc": 56.0}
{"epoch": 33, "training_loss": 58.01132655143738, "training_acc": 73.0, "val_loss": 17.896251380443573, "val_acc": 52.0}
{"epoch": 34, "training_loss": 56.24124097824097, "training_acc": 76.0, "val_loss": 19.22677606344223, "val_acc": 52.0}
{"epoch": 35, "training_loss": 63.10907292366028, "training_acc": 62.0, "val_loss": 22.385260462760925, "val_acc": 48.0}
{"epoch": 36, "training_loss": 73.32149052619934, "training_acc": 50.0, "val_loss": 17.295552790164948, "val_acc": 52.0}
{"epoch": 37, "training_loss": 58.36725163459778, "training_acc": 70.0, "val_loss": 18.269145488739014, "val_acc": 52.0}
{"epoch": 38, "training_loss": 60.51051211357117, "training_acc": 68.0, "val_loss": 17.893774807453156, "val_acc": 56.0}
{"epoch": 39, "training_loss": 55.96550917625427, "training_acc": 73.0, "val_loss": 17.737269401550293, "val_acc": 56.0}
{"epoch": 40, "training_loss": 54.287556886672974, "training_acc": 70.0, "val_loss": 17.66670197248459, "val_acc": 52.0}
{"epoch": 41, "training_loss": 51.92334771156311, "training_acc": 78.0, "val_loss": 17.25376546382904, "val_acc": 52.0}
{"epoch": 42, "training_loss": 44.4375194311142, "training_acc": 89.0, "val_loss": 19.270344078540802, "val_acc": 56.0}
{"epoch": 43, "training_loss": 52.773372173309326, "training_acc": 74.0, "val_loss": 17.466087639331818, "val_acc": 56.0}
{"epoch": 44, "training_loss": 43.24343991279602, "training_acc": 88.0, "val_loss": 17.74531751871109, "val_acc": 52.0}
{"epoch": 45, "training_loss": 40.01895582675934, "training_acc": 86.0, "val_loss": 18.625304102897644, "val_acc": 56.0}
{"epoch": 46, "training_loss": 38.87386178970337, "training_acc": 87.0, "val_loss": 20.07184624671936, "val_acc": 60.0}
{"epoch": 47, "training_loss": 41.026912331581116, "training_acc": 82.0, "val_loss": 21.542087197303772, "val_acc": 56.0}
{"epoch": 48, "training_loss": 38.08235478401184, "training_acc": 85.0, "val_loss": 22.258204221725464, "val_acc": 56.0}
{"epoch": 49, "training_loss": 45.44895613193512, "training_acc": 74.0, "val_loss": 19.884926080703735, "val_acc": 56.0}
{"epoch": 50, "training_loss": 32.26385271549225, "training_acc": 90.0, "val_loss": 23.45958799123764, "val_acc": 56.0}
