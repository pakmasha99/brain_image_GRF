"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.16300129890442, "training_acc": 53.0, "val_loss": 17.339889705181122, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.22803044319153, "training_acc": 53.0, "val_loss": 17.35120564699173, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.30854511260986, "training_acc": 53.0, "val_loss": 17.35110878944397, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.25476408004761, "training_acc": 53.0, "val_loss": 17.337873578071594, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.30945992469788, "training_acc": 53.0, "val_loss": 17.349185049533844, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.25245833396912, "training_acc": 53.0, "val_loss": 17.36752837896347, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.43444800376892, "training_acc": 53.0, "val_loss": 17.32936203479767, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.25373888015747, "training_acc": 53.0, "val_loss": 17.309170961380005, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.17749738693237, "training_acc": 53.0, "val_loss": 17.314094305038452, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.33721160888672, "training_acc": 53.0, "val_loss": 17.333057522773743, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.71383333206177, "training_acc": 53.0, "val_loss": 17.365649342536926, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.36602711677551, "training_acc": 53.0, "val_loss": 17.314240336418152, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.13303542137146, "training_acc": 53.0, "val_loss": 17.311064898967743, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.48969960212708, "training_acc": 53.0, "val_loss": 17.30962097644806, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.1136531829834, "training_acc": 53.0, "val_loss": 17.319652438163757, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.33209991455078, "training_acc": 53.0, "val_loss": 17.319293320178986, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.28158569335938, "training_acc": 53.0, "val_loss": 17.318010330200195, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.35343503952026, "training_acc": 53.0, "val_loss": 17.312133312225342, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.22673010826111, "training_acc": 53.0, "val_loss": 17.31046587228775, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.1534776687622, "training_acc": 53.0, "val_loss": 17.315402626991272, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.11672282218933, "training_acc": 53.0, "val_loss": 17.34216958284378, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.29688215255737, "training_acc": 53.0, "val_loss": 17.348644137382507, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.18028354644775, "training_acc": 53.0, "val_loss": 17.313101887702942, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.13697910308838, "training_acc": 53.0, "val_loss": 17.309096455574036, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.17507576942444, "training_acc": 53.0, "val_loss": 17.309394478797913, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.17989826202393, "training_acc": 53.0, "val_loss": 17.30867326259613, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.43596410751343, "training_acc": 53.0, "val_loss": 17.30889081954956, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.057124376297, "training_acc": 53.0, "val_loss": 17.34488308429718, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.29957628250122, "training_acc": 53.0, "val_loss": 17.40201711654663, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.57007908821106, "training_acc": 53.0, "val_loss": 17.36966073513031, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.12228322029114, "training_acc": 53.0, "val_loss": 17.309780418872833, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.39285802841187, "training_acc": 53.0, "val_loss": 17.318817973136902, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.26258754730225, "training_acc": 53.0, "val_loss": 17.31109619140625, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.17876148223877, "training_acc": 53.0, "val_loss": 17.312385141849518, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.11936545372009, "training_acc": 53.0, "val_loss": 17.327505350112915, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.26540684700012, "training_acc": 53.0, "val_loss": 17.34946072101593, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.35713243484497, "training_acc": 53.0, "val_loss": 17.33047217130661, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.20284342765808, "training_acc": 53.0, "val_loss": 17.30867773294449, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.16886067390442, "training_acc": 53.0, "val_loss": 17.313659191131592, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.23068308830261, "training_acc": 53.0, "val_loss": 17.31296181678772, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.21782994270325, "training_acc": 53.0, "val_loss": 17.310823500156403, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.17012119293213, "training_acc": 53.0, "val_loss": 17.30959713459015, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.10521936416626, "training_acc": 53.0, "val_loss": 17.325206100940704, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.1627550125122, "training_acc": 53.0, "val_loss": 17.352935671806335, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.2392930984497, "training_acc": 53.0, "val_loss": 17.35612004995346, "val_acc": 52.0}
