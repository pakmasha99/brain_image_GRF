"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.39269208908081, "training_acc": 53.0, "val_loss": 17.387641966342926, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.47257328033447, "training_acc": 53.0, "val_loss": 17.374327778816223, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.32326197624207, "training_acc": 53.0, "val_loss": 17.34139770269394, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.17295956611633, "training_acc": 53.0, "val_loss": 17.33594536781311, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.06125950813293, "training_acc": 53.0, "val_loss": 17.32109636068344, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.1168520450592, "training_acc": 53.0, "val_loss": 17.3272505402565, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.16799592971802, "training_acc": 53.0, "val_loss": 17.33619123697281, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.0930061340332, "training_acc": 53.0, "val_loss": 17.340609431266785, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.34638619422913, "training_acc": 53.0, "val_loss": 17.31654554605484, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.06580209732056, "training_acc": 53.0, "val_loss": 17.31944978237152, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.18971681594849, "training_acc": 53.0, "val_loss": 17.31666475534439, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.05178022384644, "training_acc": 53.0, "val_loss": 17.31976419687271, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.15496921539307, "training_acc": 53.0, "val_loss": 17.326301336288452, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.03619623184204, "training_acc": 53.0, "val_loss": 17.33115315437317, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.00681042671204, "training_acc": 53.0, "val_loss": 17.32218563556671, "val_acc": 52.0}
{"epoch": 15, "training_loss": 68.68766713142395, "training_acc": 53.0, "val_loss": 17.335647344589233, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.0240740776062, "training_acc": 62.0, "val_loss": 17.332741618156433, "val_acc": 52.0}
{"epoch": 17, "training_loss": 68.96990537643433, "training_acc": 54.0, "val_loss": 17.3345148563385, "val_acc": 52.0}
{"epoch": 18, "training_loss": 68.72820806503296, "training_acc": 54.0, "val_loss": 17.34248697757721, "val_acc": 52.0}
{"epoch": 19, "training_loss": 68.06646156311035, "training_acc": 61.0, "val_loss": 17.359668016433716, "val_acc": 52.0}
{"epoch": 20, "training_loss": 68.66913294792175, "training_acc": 53.0, "val_loss": 17.373622953891754, "val_acc": 52.0}
{"epoch": 21, "training_loss": 68.39177465438843, "training_acc": 50.0, "val_loss": 17.401984333992004, "val_acc": 52.0}
{"epoch": 22, "training_loss": 67.1362578868866, "training_acc": 79.0, "val_loss": 17.43425279855728, "val_acc": 52.0}
{"epoch": 23, "training_loss": 67.4865312576294, "training_acc": 64.0, "val_loss": 17.5027534365654, "val_acc": 52.0}
{"epoch": 24, "training_loss": 66.94747972488403, "training_acc": 68.0, "val_loss": 17.514367401599884, "val_acc": 52.0}
{"epoch": 25, "training_loss": 65.63083600997925, "training_acc": 69.0, "val_loss": 17.488792538642883, "val_acc": 52.0}
{"epoch": 26, "training_loss": 66.0763692855835, "training_acc": 66.0, "val_loss": 17.676499485969543, "val_acc": 52.0}
{"epoch": 27, "training_loss": 64.2545051574707, "training_acc": 69.0, "val_loss": 17.381832003593445, "val_acc": 52.0}
