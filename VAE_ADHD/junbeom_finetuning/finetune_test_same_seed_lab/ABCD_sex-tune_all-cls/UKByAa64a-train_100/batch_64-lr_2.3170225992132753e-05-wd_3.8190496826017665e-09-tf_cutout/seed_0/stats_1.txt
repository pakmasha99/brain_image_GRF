"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.68108773231506, "training_acc": 47.0, "val_loss": 17.458701133728027, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.48103642463684, "training_acc": 47.0, "val_loss": 17.345738410949707, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.42778444290161, "training_acc": 49.0, "val_loss": 17.334231734275818, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.38808274269104, "training_acc": 47.0, "val_loss": 17.401118576526642, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.342609167099, "training_acc": 44.0, "val_loss": 17.38928109407425, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.23061108589172, "training_acc": 59.0, "val_loss": 17.34761744737625, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.12710618972778, "training_acc": 53.0, "val_loss": 17.320893704891205, "val_acc": 52.0}
{"epoch": 7, "training_loss": 68.98205518722534, "training_acc": 53.0, "val_loss": 17.333808541297913, "val_acc": 52.0}
{"epoch": 8, "training_loss": 68.93264508247375, "training_acc": 53.0, "val_loss": 17.335551977157593, "val_acc": 52.0}
{"epoch": 9, "training_loss": 68.95698928833008, "training_acc": 53.0, "val_loss": 17.332257330417633, "val_acc": 52.0}
{"epoch": 10, "training_loss": 68.82011127471924, "training_acc": 53.0, "val_loss": 17.348672449588776, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.06397700309753, "training_acc": 53.0, "val_loss": 17.36137419939041, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.05540537834167, "training_acc": 53.0, "val_loss": 17.35941469669342, "val_acc": 52.0}
{"epoch": 13, "training_loss": 68.88768696784973, "training_acc": 53.0, "val_loss": 17.319223284721375, "val_acc": 52.0}
{"epoch": 14, "training_loss": 68.72325134277344, "training_acc": 53.0, "val_loss": 17.308680713176727, "val_acc": 52.0}
{"epoch": 15, "training_loss": 68.70867085456848, "training_acc": 53.0, "val_loss": 17.328406870365143, "val_acc": 52.0}
{"epoch": 16, "training_loss": 68.693443775177, "training_acc": 53.0, "val_loss": 17.34551638364792, "val_acc": 52.0}
{"epoch": 17, "training_loss": 68.63124942779541, "training_acc": 53.0, "val_loss": 17.36581325531006, "val_acc": 52.0}
{"epoch": 18, "training_loss": 68.46519374847412, "training_acc": 53.0, "val_loss": 17.37769991159439, "val_acc": 52.0}
{"epoch": 19, "training_loss": 68.43167996406555, "training_acc": 53.0, "val_loss": 17.40635633468628, "val_acc": 52.0}
{"epoch": 20, "training_loss": 68.20810008049011, "training_acc": 53.0, "val_loss": 17.382869124412537, "val_acc": 52.0}
{"epoch": 21, "training_loss": 68.04817223548889, "training_acc": 53.0, "val_loss": 17.394760251045227, "val_acc": 52.0}
{"epoch": 22, "training_loss": 68.13983941078186, "training_acc": 53.0, "val_loss": 17.39782691001892, "val_acc": 52.0}
{"epoch": 23, "training_loss": 68.15615010261536, "training_acc": 53.0, "val_loss": 17.395129799842834, "val_acc": 52.0}
{"epoch": 24, "training_loss": 67.77523469924927, "training_acc": 56.0, "val_loss": 17.410145699977875, "val_acc": 52.0}
{"epoch": 25, "training_loss": 68.00859570503235, "training_acc": 54.0, "val_loss": 17.43689328432083, "val_acc": 52.0}
{"epoch": 26, "training_loss": 67.50679683685303, "training_acc": 66.0, "val_loss": 17.428787052631378, "val_acc": 52.0}
{"epoch": 27, "training_loss": 67.84763669967651, "training_acc": 68.0, "val_loss": 17.443998157978058, "val_acc": 52.0}
{"epoch": 28, "training_loss": 67.08717823028564, "training_acc": 57.0, "val_loss": 17.48286336660385, "val_acc": 52.0}
{"epoch": 29, "training_loss": 67.63692045211792, "training_acc": 56.0, "val_loss": 17.434722185134888, "val_acc": 52.0}
{"epoch": 30, "training_loss": 67.45161080360413, "training_acc": 59.0, "val_loss": 17.433974146842957, "val_acc": 52.0}
{"epoch": 31, "training_loss": 66.60549116134644, "training_acc": 68.0, "val_loss": 17.398425936698914, "val_acc": 52.0}
{"epoch": 32, "training_loss": 66.19840502738953, "training_acc": 72.0, "val_loss": 17.446060478687286, "val_acc": 52.0}
{"epoch": 33, "training_loss": 66.11851477622986, "training_acc": 59.0, "val_loss": 17.392268776893616, "val_acc": 52.0}
