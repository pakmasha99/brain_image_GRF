"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.14013648033142, "training_acc": 53.0, "val_loss": 17.336586117744446, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.18233609199524, "training_acc": 53.0, "val_loss": 17.36556738615036, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.16837644577026, "training_acc": 53.0, "val_loss": 17.369444668293, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.22903871536255, "training_acc": 53.0, "val_loss": 17.34628677368164, "val_acc": 52.0}
{"epoch": 4, "training_loss": 68.98897004127502, "training_acc": 53.0, "val_loss": 17.328418791294098, "val_acc": 52.0}
{"epoch": 5, "training_loss": 68.99900889396667, "training_acc": 53.0, "val_loss": 17.33214557170868, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.13252019882202, "training_acc": 53.0, "val_loss": 17.326746881008148, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.24765133857727, "training_acc": 53.0, "val_loss": 17.326010763645172, "val_acc": 52.0}
{"epoch": 8, "training_loss": 68.95960354804993, "training_acc": 53.0, "val_loss": 17.339442670345306, "val_acc": 52.0}
{"epoch": 9, "training_loss": 68.93503999710083, "training_acc": 53.0, "val_loss": 17.346076667308807, "val_acc": 52.0}
{"epoch": 10, "training_loss": 68.97710275650024, "training_acc": 53.0, "val_loss": 17.329399287700653, "val_acc": 52.0}
{"epoch": 11, "training_loss": 68.71538925170898, "training_acc": 53.0, "val_loss": 17.340166866779327, "val_acc": 52.0}
{"epoch": 12, "training_loss": 68.86908054351807, "training_acc": 53.0, "val_loss": 17.34946072101593, "val_acc": 52.0}
{"epoch": 13, "training_loss": 68.69983434677124, "training_acc": 53.0, "val_loss": 17.37644225358963, "val_acc": 52.0}
{"epoch": 14, "training_loss": 68.52507996559143, "training_acc": 53.0, "val_loss": 17.368462681770325, "val_acc": 52.0}
{"epoch": 15, "training_loss": 68.70091962814331, "training_acc": 53.0, "val_loss": 17.372499406337738, "val_acc": 52.0}
{"epoch": 16, "training_loss": 67.99960899353027, "training_acc": 53.0, "val_loss": 17.376592755317688, "val_acc": 52.0}
{"epoch": 17, "training_loss": 68.42941403388977, "training_acc": 52.0, "val_loss": 17.39102005958557, "val_acc": 52.0}
{"epoch": 18, "training_loss": 67.77694344520569, "training_acc": 55.0, "val_loss": 17.438599467277527, "val_acc": 52.0}
{"epoch": 19, "training_loss": 67.66805934906006, "training_acc": 56.0, "val_loss": 17.389610409736633, "val_acc": 52.0}
{"epoch": 20, "training_loss": 68.03569984436035, "training_acc": 61.0, "val_loss": 17.435988783836365, "val_acc": 52.0}
{"epoch": 21, "training_loss": 67.21693563461304, "training_acc": 58.0, "val_loss": 17.56787896156311, "val_acc": 52.0}
{"epoch": 22, "training_loss": 68.74059700965881, "training_acc": 47.0, "val_loss": 17.309755086898804, "val_acc": 52.0}
{"epoch": 23, "training_loss": 67.89177799224854, "training_acc": 67.0, "val_loss": 17.508475482463837, "val_acc": 52.0}
{"epoch": 24, "training_loss": 65.93136262893677, "training_acc": 57.0, "val_loss": 17.714296281337738, "val_acc": 52.0}
{"epoch": 25, "training_loss": 66.82347202301025, "training_acc": 53.0, "val_loss": 17.315879464149475, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.18494725227356, "training_acc": 49.0, "val_loss": 17.26115047931671, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.30133104324341, "training_acc": 52.0, "val_loss": 17.288528382778168, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.25890111923218, "training_acc": 53.0, "val_loss": 17.302775382995605, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.09006476402283, "training_acc": 53.0, "val_loss": 17.313352227211, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.0396876335144, "training_acc": 53.0, "val_loss": 17.321231961250305, "val_acc": 52.0}
{"epoch": 31, "training_loss": 68.75976610183716, "training_acc": 53.0, "val_loss": 17.324376106262207, "val_acc": 52.0}
{"epoch": 32, "training_loss": 68.86633372306824, "training_acc": 53.0, "val_loss": 17.333297431468964, "val_acc": 52.0}
{"epoch": 33, "training_loss": 68.70594024658203, "training_acc": 53.0, "val_loss": 17.34495460987091, "val_acc": 52.0}
{"epoch": 34, "training_loss": 68.68811058998108, "training_acc": 54.0, "val_loss": 17.34449714422226, "val_acc": 52.0}
{"epoch": 35, "training_loss": 68.78890562057495, "training_acc": 60.0, "val_loss": 17.339617013931274, "val_acc": 52.0}
{"epoch": 36, "training_loss": 68.29121732711792, "training_acc": 63.0, "val_loss": 17.37033873796463, "val_acc": 52.0}
{"epoch": 37, "training_loss": 68.70127582550049, "training_acc": 53.0, "val_loss": 17.37743765115738, "val_acc": 52.0}
{"epoch": 38, "training_loss": 68.1004707813263, "training_acc": 53.0, "val_loss": 17.334312200546265, "val_acc": 52.0}
{"epoch": 39, "training_loss": 67.86521530151367, "training_acc": 58.0, "val_loss": 17.343726754188538, "val_acc": 52.0}
{"epoch": 40, "training_loss": 67.60481572151184, "training_acc": 61.0, "val_loss": 17.420874536037445, "val_acc": 52.0}
{"epoch": 41, "training_loss": 67.593994140625, "training_acc": 54.0, "val_loss": 17.403899133205414, "val_acc": 52.0}
{"epoch": 42, "training_loss": 67.64915418624878, "training_acc": 64.0, "val_loss": 17.306895554065704, "val_acc": 52.0}
{"epoch": 43, "training_loss": 66.95077872276306, "training_acc": 66.0, "val_loss": 17.546451091766357, "val_acc": 52.0}
{"epoch": 44, "training_loss": 66.98258924484253, "training_acc": 59.0, "val_loss": 17.510072886943817, "val_acc": 52.0}
{"epoch": 45, "training_loss": 66.03611302375793, "training_acc": 61.0, "val_loss": 17.592811584472656, "val_acc": 52.0}
