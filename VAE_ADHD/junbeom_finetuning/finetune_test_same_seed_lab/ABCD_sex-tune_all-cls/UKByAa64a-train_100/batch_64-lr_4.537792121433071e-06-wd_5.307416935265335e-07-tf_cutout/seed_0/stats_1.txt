"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 70.05140805244446, "training_acc": 47.0, "val_loss": 17.48415380716324, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.73318290710449, "training_acc": 47.0, "val_loss": 17.43326485157013, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.79195475578308, "training_acc": 47.0, "val_loss": 17.406967282295227, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.53658199310303, "training_acc": 47.0, "val_loss": 17.346131801605225, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.50461888313293, "training_acc": 47.0, "val_loss": 17.287826538085938, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.57490015029907, "training_acc": 48.0, "val_loss": 17.27393865585327, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.15858459472656, "training_acc": 51.0, "val_loss": 17.28423535823822, "val_acc": 52.0}
{"epoch": 7, "training_loss": 68.86290383338928, "training_acc": 54.0, "val_loss": 17.255952954292297, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.0860104560852, "training_acc": 62.0, "val_loss": 17.229144275188446, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.01172924041748, "training_acc": 56.0, "val_loss": 17.24902242422104, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.11500144004822, "training_acc": 54.0, "val_loss": 17.279505729675293, "val_acc": 52.0}
{"epoch": 11, "training_loss": 68.94895696640015, "training_acc": 52.0, "val_loss": 17.242789268493652, "val_acc": 52.0}
{"epoch": 12, "training_loss": 68.66628193855286, "training_acc": 61.0, "val_loss": 17.29566901922226, "val_acc": 52.0}
{"epoch": 13, "training_loss": 68.89859294891357, "training_acc": 55.0, "val_loss": 17.325006425380707, "val_acc": 52.0}
{"epoch": 14, "training_loss": 68.75375866889954, "training_acc": 58.0, "val_loss": 17.32107102870941, "val_acc": 52.0}
{"epoch": 15, "training_loss": 68.59144639968872, "training_acc": 59.0, "val_loss": 17.301207780838013, "val_acc": 52.0}
{"epoch": 16, "training_loss": 68.83235168457031, "training_acc": 54.0, "val_loss": 17.319056391716003, "val_acc": 52.0}
{"epoch": 17, "training_loss": 68.39630150794983, "training_acc": 56.0, "val_loss": 17.31473058462143, "val_acc": 52.0}
{"epoch": 18, "training_loss": 68.53436064720154, "training_acc": 56.0, "val_loss": 17.266811430454254, "val_acc": 52.0}
{"epoch": 19, "training_loss": 68.10284447669983, "training_acc": 53.0, "val_loss": 17.201003432273865, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.02348470687866, "training_acc": 50.0, "val_loss": 17.183561623096466, "val_acc": 52.0}
{"epoch": 21, "training_loss": 68.39310503005981, "training_acc": 54.0, "val_loss": 17.287206649780273, "val_acc": 52.0}
{"epoch": 22, "training_loss": 68.09528851509094, "training_acc": 62.0, "val_loss": 17.4510195851326, "val_acc": 52.0}
{"epoch": 23, "training_loss": 68.88071417808533, "training_acc": 53.0, "val_loss": 17.47763752937317, "val_acc": 52.0}
{"epoch": 24, "training_loss": 68.70481157302856, "training_acc": 57.0, "val_loss": 17.47191548347473, "val_acc": 52.0}
{"epoch": 25, "training_loss": 68.68830800056458, "training_acc": 55.0, "val_loss": 17.3849955201149, "val_acc": 52.0}
{"epoch": 26, "training_loss": 68.13901209831238, "training_acc": 56.0, "val_loss": 17.31940358877182, "val_acc": 52.0}
{"epoch": 27, "training_loss": 68.3190848827362, "training_acc": 54.0, "val_loss": 17.278718948364258, "val_acc": 52.0}
{"epoch": 28, "training_loss": 67.8559238910675, "training_acc": 58.0, "val_loss": 17.302806675434113, "val_acc": 52.0}
{"epoch": 29, "training_loss": 67.98377990722656, "training_acc": 55.0, "val_loss": 17.343604564666748, "val_acc": 52.0}
{"epoch": 30, "training_loss": 68.32005715370178, "training_acc": 54.0, "val_loss": 17.306259274482727, "val_acc": 52.0}
{"epoch": 31, "training_loss": 68.20352101325989, "training_acc": 57.0, "val_loss": 17.195533215999603, "val_acc": 52.0}
{"epoch": 32, "training_loss": 68.25811052322388, "training_acc": 53.0, "val_loss": 17.1828031539917, "val_acc": 52.0}
{"epoch": 33, "training_loss": 67.7415075302124, "training_acc": 55.0, "val_loss": 17.23886877298355, "val_acc": 52.0}
{"epoch": 34, "training_loss": 68.05503559112549, "training_acc": 53.0, "val_loss": 17.38380789756775, "val_acc": 52.0}
{"epoch": 35, "training_loss": 67.81942772865295, "training_acc": 59.0, "val_loss": 17.481380701065063, "val_acc": 52.0}
{"epoch": 36, "training_loss": 68.04226231575012, "training_acc": 56.0, "val_loss": 17.467370629310608, "val_acc": 52.0}
{"epoch": 37, "training_loss": 67.43317985534668, "training_acc": 59.0, "val_loss": 17.309366166591644, "val_acc": 52.0}
{"epoch": 38, "training_loss": 67.70256781578064, "training_acc": 55.0, "val_loss": 17.278608679771423, "val_acc": 52.0}
{"epoch": 39, "training_loss": 67.8026123046875, "training_acc": 59.0, "val_loss": 17.367812991142273, "val_acc": 52.0}
{"epoch": 40, "training_loss": 67.74308371543884, "training_acc": 57.0, "val_loss": 17.402450740337372, "val_acc": 52.0}
{"epoch": 41, "training_loss": 67.52328324317932, "training_acc": 56.0, "val_loss": 17.368152737617493, "val_acc": 52.0}
{"epoch": 42, "training_loss": 67.31076455116272, "training_acc": 61.0, "val_loss": 17.264796793460846, "val_acc": 52.0}
{"epoch": 43, "training_loss": 67.67259931564331, "training_acc": 55.0, "val_loss": 17.309753596782684, "val_acc": 52.0}
{"epoch": 44, "training_loss": 67.26739859580994, "training_acc": 62.0, "val_loss": 17.37123131752014, "val_acc": 52.0}
{"epoch": 45, "training_loss": 67.56556344032288, "training_acc": 59.0, "val_loss": 17.225179076194763, "val_acc": 52.0}
{"epoch": 46, "training_loss": 66.94257473945618, "training_acc": 61.0, "val_loss": 17.251262068748474, "val_acc": 52.0}
{"epoch": 47, "training_loss": 66.87968587875366, "training_acc": 66.0, "val_loss": 17.404218018054962, "val_acc": 52.0}
{"epoch": 48, "training_loss": 66.77507710456848, "training_acc": 64.0, "val_loss": 17.291392385959625, "val_acc": 52.0}
{"epoch": 49, "training_loss": 67.39337086677551, "training_acc": 59.0, "val_loss": 17.493270337581635, "val_acc": 52.0}
{"epoch": 50, "training_loss": 67.17532086372375, "training_acc": 64.0, "val_loss": 17.35621988773346, "val_acc": 52.0}
{"epoch": 51, "training_loss": 66.93846106529236, "training_acc": 62.0, "val_loss": 17.32725352048874, "val_acc": 52.0}
