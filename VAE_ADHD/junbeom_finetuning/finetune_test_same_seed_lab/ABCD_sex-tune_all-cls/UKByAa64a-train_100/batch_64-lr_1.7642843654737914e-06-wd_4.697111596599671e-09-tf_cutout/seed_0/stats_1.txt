"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 71.80848407745361, "training_acc": 47.0, "val_loss": 17.889979481697083, "val_acc": 52.0}
{"epoch": 1, "training_loss": 71.85888004302979, "training_acc": 47.0, "val_loss": 17.83609539270401, "val_acc": 52.0}
{"epoch": 2, "training_loss": 71.64591145515442, "training_acc": 47.0, "val_loss": 17.790770530700684, "val_acc": 52.0}
{"epoch": 3, "training_loss": 71.43527030944824, "training_acc": 47.0, "val_loss": 17.755645513534546, "val_acc": 52.0}
{"epoch": 4, "training_loss": 71.22337770462036, "training_acc": 47.0, "val_loss": 17.72630661725998, "val_acc": 52.0}
{"epoch": 5, "training_loss": 71.18103408813477, "training_acc": 47.0, "val_loss": 17.707553505897522, "val_acc": 52.0}
{"epoch": 6, "training_loss": 70.98444390296936, "training_acc": 47.0, "val_loss": 17.65865981578827, "val_acc": 52.0}
{"epoch": 7, "training_loss": 70.87103271484375, "training_acc": 47.0, "val_loss": 17.603324353694916, "val_acc": 52.0}
{"epoch": 8, "training_loss": 70.53726315498352, "training_acc": 47.0, "val_loss": 17.55000203847885, "val_acc": 52.0}
{"epoch": 9, "training_loss": 70.425368309021, "training_acc": 47.0, "val_loss": 17.519599199295044, "val_acc": 52.0}
{"epoch": 10, "training_loss": 70.39353656768799, "training_acc": 47.0, "val_loss": 17.5050750374794, "val_acc": 52.0}
{"epoch": 11, "training_loss": 70.42528080940247, "training_acc": 47.0, "val_loss": 17.497798800468445, "val_acc": 52.0}
{"epoch": 12, "training_loss": 70.3775999546051, "training_acc": 47.0, "val_loss": 17.504379153251648, "val_acc": 52.0}
{"epoch": 13, "training_loss": 70.1111626625061, "training_acc": 47.0, "val_loss": 17.479021847248077, "val_acc": 52.0}
{"epoch": 14, "training_loss": 70.12485456466675, "training_acc": 47.0, "val_loss": 17.44084656238556, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.92327189445496, "training_acc": 47.0, "val_loss": 17.401231825351715, "val_acc": 52.0}
{"epoch": 16, "training_loss": 70.30679059028625, "training_acc": 47.0, "val_loss": 17.361542582511902, "val_acc": 52.0}
{"epoch": 17, "training_loss": 70.0139467716217, "training_acc": 47.0, "val_loss": 17.35956519842148, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.8943521976471, "training_acc": 47.0, "val_loss": 17.389674484729767, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.84476590156555, "training_acc": 47.0, "val_loss": 17.442332208156586, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.4245662689209, "training_acc": 47.0, "val_loss": 17.464175820350647, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.53135418891907, "training_acc": 47.0, "val_loss": 17.462879419326782, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.87987613677979, "training_acc": 47.0, "val_loss": 17.454223334789276, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.55062246322632, "training_acc": 47.0, "val_loss": 17.407970130443573, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.00998115539551, "training_acc": 48.0, "val_loss": 17.35580414533615, "val_acc": 52.0}
{"epoch": 25, "training_loss": 68.84735608100891, "training_acc": 47.0, "val_loss": 17.34597235918045, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.23847579956055, "training_acc": 53.0, "val_loss": 17.33359545469284, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.58891940116882, "training_acc": 52.0, "val_loss": 17.341364920139313, "val_acc": 52.0}
{"epoch": 28, "training_loss": 68.45814633369446, "training_acc": 53.0, "val_loss": 17.323949933052063, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.76044917106628, "training_acc": 52.0, "val_loss": 17.316865921020508, "val_acc": 52.0}
{"epoch": 30, "training_loss": 68.91451215744019, "training_acc": 58.0, "val_loss": 17.36292839050293, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.23368072509766, "training_acc": 54.0, "val_loss": 17.39719659090042, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.28011417388916, "training_acc": 55.0, "val_loss": 17.38777756690979, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.21746897697449, "training_acc": 60.0, "val_loss": 17.339877784252167, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.09892320632935, "training_acc": 58.0, "val_loss": 17.298708856105804, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.16888666152954, "training_acc": 59.0, "val_loss": 17.256446182727814, "val_acc": 52.0}
{"epoch": 36, "training_loss": 68.66636681556702, "training_acc": 59.0, "val_loss": 17.263270914554596, "val_acc": 52.0}
{"epoch": 37, "training_loss": 68.4982225894928, "training_acc": 56.0, "val_loss": 17.300422489643097, "val_acc": 52.0}
{"epoch": 38, "training_loss": 68.13528537750244, "training_acc": 63.0, "val_loss": 17.35881119966507, "val_acc": 52.0}
{"epoch": 39, "training_loss": 68.24575662612915, "training_acc": 58.0, "val_loss": 17.36578643321991, "val_acc": 52.0}
{"epoch": 40, "training_loss": 68.39991044998169, "training_acc": 62.0, "val_loss": 17.3433855175972, "val_acc": 52.0}
{"epoch": 41, "training_loss": 68.48669767379761, "training_acc": 61.0, "val_loss": 17.320969700813293, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.18923830986023, "training_acc": 56.0, "val_loss": 17.270483076572418, "val_acc": 52.0}
{"epoch": 43, "training_loss": 68.43198347091675, "training_acc": 63.0, "val_loss": 17.254379391670227, "val_acc": 52.0}
{"epoch": 44, "training_loss": 68.63349533081055, "training_acc": 61.0, "val_loss": 17.272046208381653, "val_acc": 52.0}
{"epoch": 45, "training_loss": 68.42178392410278, "training_acc": 59.0, "val_loss": 17.28813350200653, "val_acc": 52.0}
{"epoch": 46, "training_loss": 68.3790009021759, "training_acc": 62.0, "val_loss": 17.263171076774597, "val_acc": 52.0}
{"epoch": 47, "training_loss": 68.61822986602783, "training_acc": 53.0, "val_loss": 17.239978909492493, "val_acc": 52.0}
{"epoch": 48, "training_loss": 68.01079106330872, "training_acc": 61.0, "val_loss": 17.20920205116272, "val_acc": 52.0}
{"epoch": 49, "training_loss": 68.64844632148743, "training_acc": 53.0, "val_loss": 17.205283045768738, "val_acc": 52.0}
{"epoch": 50, "training_loss": 68.45752215385437, "training_acc": 57.0, "val_loss": 17.256072163581848, "val_acc": 52.0}
{"epoch": 51, "training_loss": 68.31679964065552, "training_acc": 60.0, "val_loss": 17.33306497335434, "val_acc": 52.0}
{"epoch": 52, "training_loss": 68.25553441047668, "training_acc": 61.0, "val_loss": 17.38651990890503, "val_acc": 52.0}
{"epoch": 53, "training_loss": 68.9734435081482, "training_acc": 55.0, "val_loss": 17.35742837190628, "val_acc": 52.0}
{"epoch": 54, "training_loss": 68.08903050422668, "training_acc": 65.0, "val_loss": 17.310012876987457, "val_acc": 52.0}
{"epoch": 55, "training_loss": 67.88890957832336, "training_acc": 59.0, "val_loss": 17.272955179214478, "val_acc": 52.0}
{"epoch": 56, "training_loss": 68.30836176872253, "training_acc": 62.0, "val_loss": 17.270782589912415, "val_acc": 52.0}
{"epoch": 57, "training_loss": 68.10198640823364, "training_acc": 66.0, "val_loss": 17.274750769138336, "val_acc": 52.0}
{"epoch": 58, "training_loss": 68.31554961204529, "training_acc": 65.0, "val_loss": 17.318545281887054, "val_acc": 52.0}
{"epoch": 59, "training_loss": 68.24300527572632, "training_acc": 60.0, "val_loss": 17.33640879392624, "val_acc": 52.0}
{"epoch": 60, "training_loss": 68.15606331825256, "training_acc": 61.0, "val_loss": 17.29559600353241, "val_acc": 52.0}
{"epoch": 61, "training_loss": 68.63206076622009, "training_acc": 56.0, "val_loss": 17.288462817668915, "val_acc": 52.0}
{"epoch": 62, "training_loss": 68.00336384773254, "training_acc": 62.0, "val_loss": 17.325647175312042, "val_acc": 52.0}
{"epoch": 63, "training_loss": 67.85481691360474, "training_acc": 69.0, "val_loss": 17.342311143875122, "val_acc": 52.0}
{"epoch": 64, "training_loss": 67.98209691047668, "training_acc": 67.0, "val_loss": 17.329737544059753, "val_acc": 52.0}
{"epoch": 65, "training_loss": 67.69101023674011, "training_acc": 67.0, "val_loss": 17.356742918491364, "val_acc": 52.0}
{"epoch": 66, "training_loss": 67.6524727344513, "training_acc": 62.0, "val_loss": 17.342399060726166, "val_acc": 52.0}
{"epoch": 67, "training_loss": 68.25682759284973, "training_acc": 57.0, "val_loss": 17.309673130512238, "val_acc": 52.0}
{"epoch": 68, "training_loss": 68.19655585289001, "training_acc": 54.0, "val_loss": 17.263606190681458, "val_acc": 52.0}
