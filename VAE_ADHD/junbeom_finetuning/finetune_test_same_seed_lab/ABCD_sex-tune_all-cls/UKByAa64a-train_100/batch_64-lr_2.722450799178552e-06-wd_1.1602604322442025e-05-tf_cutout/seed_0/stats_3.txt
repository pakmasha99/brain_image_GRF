"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.70394563674927, "training_acc": 53.0, "val_loss": 17.502734065055847, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.66132068634033, "training_acc": 53.0, "val_loss": 17.507247626781464, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.52790141105652, "training_acc": 53.0, "val_loss": 17.488206923007965, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.57703900337219, "training_acc": 53.0, "val_loss": 17.426501214504242, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.58050179481506, "training_acc": 53.0, "val_loss": 17.399947345256805, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.41009259223938, "training_acc": 53.0, "val_loss": 17.378050088882446, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.4980239868164, "training_acc": 53.0, "val_loss": 17.379920184612274, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.46447896957397, "training_acc": 53.0, "val_loss": 17.404107749462128, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.2590229511261, "training_acc": 53.0, "val_loss": 17.41214692592621, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.14000415802002, "training_acc": 53.0, "val_loss": 17.409880459308624, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.14787006378174, "training_acc": 53.0, "val_loss": 17.39664375782013, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.02426218986511, "training_acc": 53.0, "val_loss": 17.375899851322174, "val_acc": 52.0}
{"epoch": 12, "training_loss": 68.96264576911926, "training_acc": 53.0, "val_loss": 17.35612601041794, "val_acc": 52.0}
{"epoch": 13, "training_loss": 68.9116678237915, "training_acc": 53.0, "val_loss": 17.342479526996613, "val_acc": 52.0}
{"epoch": 14, "training_loss": 68.91308522224426, "training_acc": 53.0, "val_loss": 17.33013242483139, "val_acc": 52.0}
{"epoch": 15, "training_loss": 68.7860324382782, "training_acc": 53.0, "val_loss": 17.344270646572113, "val_acc": 52.0}
{"epoch": 16, "training_loss": 68.82707190513611, "training_acc": 53.0, "val_loss": 17.3575296998024, "val_acc": 52.0}
{"epoch": 17, "training_loss": 68.76910376548767, "training_acc": 53.0, "val_loss": 17.371253669261932, "val_acc": 52.0}
{"epoch": 18, "training_loss": 68.45547819137573, "training_acc": 53.0, "val_loss": 17.37966537475586, "val_acc": 52.0}
{"epoch": 19, "training_loss": 68.58000922203064, "training_acc": 53.0, "val_loss": 17.386268079280853, "val_acc": 52.0}
{"epoch": 20, "training_loss": 68.58526277542114, "training_acc": 53.0, "val_loss": 17.393308877944946, "val_acc": 52.0}
{"epoch": 21, "training_loss": 68.5074815750122, "training_acc": 53.0, "val_loss": 17.3730731010437, "val_acc": 52.0}
{"epoch": 22, "training_loss": 68.3136875629425, "training_acc": 53.0, "val_loss": 17.33032763004303, "val_acc": 52.0}
{"epoch": 23, "training_loss": 68.63641786575317, "training_acc": 53.0, "val_loss": 17.320214211940765, "val_acc": 52.0}
{"epoch": 24, "training_loss": 68.37960457801819, "training_acc": 53.0, "val_loss": 17.368800938129425, "val_acc": 52.0}
{"epoch": 25, "training_loss": 68.47684025764465, "training_acc": 53.0, "val_loss": 17.412765324115753, "val_acc": 52.0}
{"epoch": 26, "training_loss": 68.49037790298462, "training_acc": 53.0, "val_loss": 17.42483079433441, "val_acc": 52.0}
{"epoch": 27, "training_loss": 68.48419189453125, "training_acc": 53.0, "val_loss": 17.425866425037384, "val_acc": 52.0}
{"epoch": 28, "training_loss": 68.35067439079285, "training_acc": 53.0, "val_loss": 17.40100383758545, "val_acc": 52.0}
{"epoch": 29, "training_loss": 67.75296783447266, "training_acc": 53.0, "val_loss": 17.335504293441772, "val_acc": 52.0}
{"epoch": 30, "training_loss": 68.364830493927, "training_acc": 53.0, "val_loss": 17.314930260181427, "val_acc": 52.0}
{"epoch": 31, "training_loss": 68.53550791740417, "training_acc": 53.0, "val_loss": 17.35263168811798, "val_acc": 52.0}
{"epoch": 32, "training_loss": 68.32923531532288, "training_acc": 53.0, "val_loss": 17.42362678050995, "val_acc": 52.0}
{"epoch": 33, "training_loss": 68.13255310058594, "training_acc": 53.0, "val_loss": 17.47252196073532, "val_acc": 52.0}
{"epoch": 34, "training_loss": 68.19275045394897, "training_acc": 53.0, "val_loss": 17.487478256225586, "val_acc": 52.0}
{"epoch": 35, "training_loss": 68.68972086906433, "training_acc": 53.0, "val_loss": 17.46116429567337, "val_acc": 52.0}
{"epoch": 36, "training_loss": 68.15706872940063, "training_acc": 53.0, "val_loss": 17.394225299358368, "val_acc": 52.0}
{"epoch": 37, "training_loss": 67.87026715278625, "training_acc": 53.0, "val_loss": 17.334680259227753, "val_acc": 52.0}
{"epoch": 38, "training_loss": 67.72996926307678, "training_acc": 53.0, "val_loss": 17.341502010822296, "val_acc": 52.0}
{"epoch": 39, "training_loss": 67.76830244064331, "training_acc": 54.0, "val_loss": 17.38489270210266, "val_acc": 52.0}
{"epoch": 40, "training_loss": 67.61491084098816, "training_acc": 53.0, "val_loss": 17.446178197860718, "val_acc": 52.0}
{"epoch": 41, "training_loss": 67.61755871772766, "training_acc": 53.0, "val_loss": 17.435410618782043, "val_acc": 52.0}
{"epoch": 42, "training_loss": 67.37187218666077, "training_acc": 53.0, "val_loss": 17.393876612186432, "val_acc": 52.0}
{"epoch": 43, "training_loss": 67.16842222213745, "training_acc": 54.0, "val_loss": 17.37634688615799, "val_acc": 52.0}
{"epoch": 44, "training_loss": 67.37993049621582, "training_acc": 58.0, "val_loss": 17.372293770313263, "val_acc": 52.0}
{"epoch": 45, "training_loss": 67.89156317710876, "training_acc": 57.0, "val_loss": 17.428745329380035, "val_acc": 52.0}
{"epoch": 46, "training_loss": 67.18191528320312, "training_acc": 54.0, "val_loss": 17.425987124443054, "val_acc": 52.0}
{"epoch": 47, "training_loss": 67.68322014808655, "training_acc": 53.0, "val_loss": 17.44200438261032, "val_acc": 52.0}
{"epoch": 48, "training_loss": 67.88242602348328, "training_acc": 55.0, "val_loss": 17.50004142522812, "val_acc": 52.0}
{"epoch": 49, "training_loss": 67.57164025306702, "training_acc": 53.0, "val_loss": 17.479203641414642, "val_acc": 52.0}
