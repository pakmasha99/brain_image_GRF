"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.7741117477417, "training_acc": 47.0, "val_loss": 17.410945892333984, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.78628587722778, "training_acc": 47.0, "val_loss": 17.406730353832245, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.72208881378174, "training_acc": 47.0, "val_loss": 17.397935688495636, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.65301322937012, "training_acc": 47.0, "val_loss": 17.391961812973022, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.64156675338745, "training_acc": 47.0, "val_loss": 17.39020049571991, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.48114538192749, "training_acc": 47.0, "val_loss": 17.386074364185333, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.56105947494507, "training_acc": 47.0, "val_loss": 17.37368404865265, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.44837236404419, "training_acc": 47.0, "val_loss": 17.37053394317627, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.52236270904541, "training_acc": 47.0, "val_loss": 17.37224906682968, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.40170478820801, "training_acc": 47.0, "val_loss": 17.373326420783997, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.35960698127747, "training_acc": 47.0, "val_loss": 17.367643117904663, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.38641119003296, "training_acc": 47.0, "val_loss": 17.359544336795807, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.33371043205261, "training_acc": 48.0, "val_loss": 17.34926998615265, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.33527493476868, "training_acc": 53.0, "val_loss": 17.333610355854034, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.15565228462219, "training_acc": 51.0, "val_loss": 17.323441803455353, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.15425848960876, "training_acc": 57.0, "val_loss": 17.317397892475128, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.08503317832947, "training_acc": 55.0, "val_loss": 17.310819029808044, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.3387234210968, "training_acc": 55.0, "val_loss": 17.31061339378357, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.0426287651062, "training_acc": 58.0, "val_loss": 17.316167056560516, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.21833372116089, "training_acc": 53.0, "val_loss": 17.32465773820877, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.30567359924316, "training_acc": 53.0, "val_loss": 17.334038019180298, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.02389621734619, "training_acc": 62.0, "val_loss": 17.339137196540833, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.15677976608276, "training_acc": 51.0, "val_loss": 17.339688539505005, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.3392059803009, "training_acc": 52.0, "val_loss": 17.346397042274475, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.07427597045898, "training_acc": 59.0, "val_loss": 17.351587116718292, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.18208050727844, "training_acc": 51.0, "val_loss": 17.352236807346344, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.19365310668945, "training_acc": 58.0, "val_loss": 17.341719567775726, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.0272331237793, "training_acc": 61.0, "val_loss": 17.326705157756805, "val_acc": 52.0}
{"epoch": 28, "training_loss": 68.77931141853333, "training_acc": 66.0, "val_loss": 17.300143837928772, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.13929724693298, "training_acc": 51.0, "val_loss": 17.280210554599762, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.14384865760803, "training_acc": 53.0, "val_loss": 17.27043241262436, "val_acc": 52.0}
{"epoch": 31, "training_loss": 68.85621285438538, "training_acc": 56.0, "val_loss": 17.259204387664795, "val_acc": 52.0}
{"epoch": 32, "training_loss": 68.90709018707275, "training_acc": 58.0, "val_loss": 17.2536239027977, "val_acc": 52.0}
{"epoch": 33, "training_loss": 68.91815042495728, "training_acc": 61.0, "val_loss": 17.25856512784958, "val_acc": 52.0}
{"epoch": 34, "training_loss": 68.96211314201355, "training_acc": 55.0, "val_loss": 17.258460819721222, "val_acc": 52.0}
{"epoch": 35, "training_loss": 68.9551854133606, "training_acc": 54.0, "val_loss": 17.26853847503662, "val_acc": 52.0}
{"epoch": 36, "training_loss": 68.9133050441742, "training_acc": 52.0, "val_loss": 17.282220721244812, "val_acc": 52.0}
{"epoch": 37, "training_loss": 68.90403723716736, "training_acc": 55.0, "val_loss": 17.294830083847046, "val_acc": 52.0}
{"epoch": 38, "training_loss": 68.9202721118927, "training_acc": 53.0, "val_loss": 17.297321557998657, "val_acc": 52.0}
{"epoch": 39, "training_loss": 68.78861498832703, "training_acc": 56.0, "val_loss": 17.291276156902313, "val_acc": 52.0}
{"epoch": 40, "training_loss": 68.726083278656, "training_acc": 57.0, "val_loss": 17.289146780967712, "val_acc": 52.0}
{"epoch": 41, "training_loss": 68.72394180297852, "training_acc": 56.0, "val_loss": 17.290614545345306, "val_acc": 52.0}
{"epoch": 42, "training_loss": 68.57369494438171, "training_acc": 61.0, "val_loss": 17.286892235279083, "val_acc": 52.0}
{"epoch": 43, "training_loss": 68.85157608985901, "training_acc": 54.0, "val_loss": 17.280948162078857, "val_acc": 52.0}
{"epoch": 44, "training_loss": 68.87432050704956, "training_acc": 53.0, "val_loss": 17.270775139331818, "val_acc": 52.0}
{"epoch": 45, "training_loss": 68.57224011421204, "training_acc": 62.0, "val_loss": 17.28365421295166, "val_acc": 52.0}
{"epoch": 46, "training_loss": 68.9470682144165, "training_acc": 52.0, "val_loss": 17.30627566576004, "val_acc": 52.0}
{"epoch": 47, "training_loss": 68.73993873596191, "training_acc": 52.0, "val_loss": 17.30867475271225, "val_acc": 52.0}
{"epoch": 48, "training_loss": 68.68027091026306, "training_acc": 63.0, "val_loss": 17.30085015296936, "val_acc": 52.0}
{"epoch": 49, "training_loss": 68.52731370925903, "training_acc": 62.0, "val_loss": 17.286144196987152, "val_acc": 52.0}
{"epoch": 50, "training_loss": 68.75390577316284, "training_acc": 58.0, "val_loss": 17.278392612934113, "val_acc": 52.0}
{"epoch": 51, "training_loss": 68.71758317947388, "training_acc": 61.0, "val_loss": 17.275092005729675, "val_acc": 52.0}
