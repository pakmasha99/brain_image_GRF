"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.55092597007751, "training_acc": 53.0, "val_loss": 17.38712787628174, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.22573113441467, "training_acc": 53.0, "val_loss": 17.362448573112488, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.28160429000854, "training_acc": 53.0, "val_loss": 17.42331087589264, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.14833879470825, "training_acc": 53.0, "val_loss": 17.377135157585144, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.18367195129395, "training_acc": 53.0, "val_loss": 17.34284907579422, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.31799387931824, "training_acc": 53.0, "val_loss": 17.355908453464508, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.1798448562622, "training_acc": 53.0, "val_loss": 17.352859675884247, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.13781452178955, "training_acc": 53.0, "val_loss": 17.372828722000122, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.06865620613098, "training_acc": 53.0, "val_loss": 17.395640909671783, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.04383540153503, "training_acc": 53.0, "val_loss": 17.37084686756134, "val_acc": 52.0}
{"epoch": 10, "training_loss": 68.87039184570312, "training_acc": 53.0, "val_loss": 17.34902858734131, "val_acc": 52.0}
{"epoch": 11, "training_loss": 68.93327260017395, "training_acc": 53.0, "val_loss": 17.357897758483887, "val_acc": 52.0}
{"epoch": 12, "training_loss": 68.80502724647522, "training_acc": 53.0, "val_loss": 17.369699478149414, "val_acc": 52.0}
{"epoch": 13, "training_loss": 68.78856348991394, "training_acc": 53.0, "val_loss": 17.36898422241211, "val_acc": 52.0}
{"epoch": 14, "training_loss": 68.71736478805542, "training_acc": 53.0, "val_loss": 17.341823875904083, "val_acc": 52.0}
{"epoch": 15, "training_loss": 68.68625712394714, "training_acc": 53.0, "val_loss": 17.35028177499771, "val_acc": 52.0}
{"epoch": 16, "training_loss": 68.75203704833984, "training_acc": 53.0, "val_loss": 17.40744113922119, "val_acc": 52.0}
{"epoch": 17, "training_loss": 68.52730751037598, "training_acc": 53.0, "val_loss": 17.398585379123688, "val_acc": 52.0}
{"epoch": 18, "training_loss": 68.99072027206421, "training_acc": 53.0, "val_loss": 17.404358088970184, "val_acc": 52.0}
{"epoch": 19, "training_loss": 68.97907757759094, "training_acc": 53.0, "val_loss": 17.39371567964554, "val_acc": 52.0}
{"epoch": 20, "training_loss": 68.52364015579224, "training_acc": 53.0, "val_loss": 17.414504289627075, "val_acc": 52.0}
{"epoch": 21, "training_loss": 68.32993149757385, "training_acc": 53.0, "val_loss": 17.290355265140533, "val_acc": 52.0}
{"epoch": 22, "training_loss": 68.92500305175781, "training_acc": 53.0, "val_loss": 17.37026572227478, "val_acc": 52.0}
{"epoch": 23, "training_loss": 68.18449831008911, "training_acc": 53.0, "val_loss": 17.405883967876434, "val_acc": 52.0}
{"epoch": 24, "training_loss": 68.41354298591614, "training_acc": 53.0, "val_loss": 17.425401508808136, "val_acc": 52.0}
{"epoch": 25, "training_loss": 68.00367283821106, "training_acc": 53.0, "val_loss": 17.422832548618317, "val_acc": 52.0}
{"epoch": 26, "training_loss": 67.73069930076599, "training_acc": 53.0, "val_loss": 17.430606484413147, "val_acc": 52.0}
{"epoch": 27, "training_loss": 67.87950992584229, "training_acc": 61.0, "val_loss": 17.45423972606659, "val_acc": 52.0}
{"epoch": 28, "training_loss": 67.62984132766724, "training_acc": 65.0, "val_loss": 17.504218220710754, "val_acc": 52.0}
{"epoch": 29, "training_loss": 67.38908815383911, "training_acc": 64.0, "val_loss": 17.51524955034256, "val_acc": 52.0}
{"epoch": 30, "training_loss": 67.23008823394775, "training_acc": 71.0, "val_loss": 17.606207728385925, "val_acc": 52.0}
{"epoch": 31, "training_loss": 66.52407169342041, "training_acc": 65.0, "val_loss": 17.65083074569702, "val_acc": 52.0}
{"epoch": 32, "training_loss": 67.49453449249268, "training_acc": 59.0, "val_loss": 17.468690872192383, "val_acc": 52.0}
{"epoch": 33, "training_loss": 66.84877109527588, "training_acc": 75.0, "val_loss": 17.789101600646973, "val_acc": 52.0}
{"epoch": 34, "training_loss": 66.13649868965149, "training_acc": 60.0, "val_loss": 17.550700902938843, "val_acc": 52.0}
{"epoch": 35, "training_loss": 66.59723591804504, "training_acc": 70.0, "val_loss": 17.808732390403748, "val_acc": 52.0}
{"epoch": 36, "training_loss": 67.26103568077087, "training_acc": 61.0, "val_loss": 17.842118442058563, "val_acc": 52.0}
{"epoch": 37, "training_loss": 66.46366333961487, "training_acc": 61.0, "val_loss": 17.325343191623688, "val_acc": 52.0}
{"epoch": 38, "training_loss": 67.14557099342346, "training_acc": 66.0, "val_loss": 17.921443283557892, "val_acc": 52.0}
{"epoch": 39, "training_loss": 65.04281258583069, "training_acc": 69.0, "val_loss": 17.984406650066376, "val_acc": 52.0}
{"epoch": 40, "training_loss": 64.78219246864319, "training_acc": 67.0, "val_loss": 17.820918560028076, "val_acc": 52.0}
