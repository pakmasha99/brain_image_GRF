"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 70.01872301101685, "training_acc": 53.0, "val_loss": 17.410124838352203, "val_acc": 52.0}
{"epoch": 1, "training_loss": 68.89289045333862, "training_acc": 53.0, "val_loss": 17.967097461223602, "val_acc": 52.0}
{"epoch": 2, "training_loss": 72.01051688194275, "training_acc": 45.0, "val_loss": 17.37532913684845, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.94123888015747, "training_acc": 47.0, "val_loss": 17.35786348581314, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.79238152503967, "training_acc": 53.0, "val_loss": 17.40296185016632, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.27639389038086, "training_acc": 53.0, "val_loss": 17.308807373046875, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.11871385574341, "training_acc": 53.0, "val_loss": 17.34098345041275, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.40960621833801, "training_acc": 47.0, "val_loss": 17.31608361005783, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.1561770439148, "training_acc": 53.0, "val_loss": 17.33405888080597, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.33047604560852, "training_acc": 53.0, "val_loss": 17.39002913236618, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.3334002494812, "training_acc": 53.0, "val_loss": 17.464475333690643, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.77880024909973, "training_acc": 53.0, "val_loss": 17.38707721233368, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.82957434654236, "training_acc": 53.0, "val_loss": 17.310860753059387, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.20440316200256, "training_acc": 53.0, "val_loss": 17.318299412727356, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.24214601516724, "training_acc": 53.0, "val_loss": 17.311058938503265, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.3931028842926, "training_acc": 53.0, "val_loss": 17.31237918138504, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.1489634513855, "training_acc": 53.0, "val_loss": 17.31211692094803, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.13399481773376, "training_acc": 53.0, "val_loss": 17.3191100358963, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.17624735832214, "training_acc": 53.0, "val_loss": 17.326705157756805, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.21253037452698, "training_acc": 53.0, "val_loss": 17.320136725902557, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.17910504341125, "training_acc": 53.0, "val_loss": 17.323264479637146, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.1838641166687, "training_acc": 53.0, "val_loss": 17.331041395664215, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.18370509147644, "training_acc": 53.0, "val_loss": 17.315715551376343, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.13631749153137, "training_acc": 53.0, "val_loss": 17.310485243797302, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.19464135169983, "training_acc": 53.0, "val_loss": 17.313867807388306, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.23973417282104, "training_acc": 53.0, "val_loss": 17.308685183525085, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.15210771560669, "training_acc": 53.0, "val_loss": 17.3354372382164, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.31328201293945, "training_acc": 53.0, "val_loss": 17.361930012702942, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.42770099639893, "training_acc": 53.0, "val_loss": 17.41982400417328, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.43747067451477, "training_acc": 53.0, "val_loss": 17.517192661762238, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.76223087310791, "training_acc": 53.0, "val_loss": 17.435352504253387, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.68028688430786, "training_acc": 53.0, "val_loss": 17.31257438659668, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.10361075401306, "training_acc": 53.0, "val_loss": 17.31579899787903, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.45458579063416, "training_acc": 45.0, "val_loss": 17.328524589538574, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.25426125526428, "training_acc": 53.0, "val_loss": 17.308761179447174, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.09613966941833, "training_acc": 53.0, "val_loss": 17.353087663650513, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.20242047309875, "training_acc": 53.0, "val_loss": 17.406955361366272, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.5275993347168, "training_acc": 53.0, "val_loss": 17.429906129837036, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.42903137207031, "training_acc": 53.0, "val_loss": 17.33786016702652, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.29118752479553, "training_acc": 53.0, "val_loss": 17.308753728866577, "val_acc": 52.0}
{"epoch": 40, "training_loss": 69.15760397911072, "training_acc": 53.0, "val_loss": 17.311224341392517, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.29298090934753, "training_acc": 53.0, "val_loss": 17.30944663286209, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.21787881851196, "training_acc": 53.0, "val_loss": 17.32255667448044, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.15382885932922, "training_acc": 53.0, "val_loss": 17.34168529510498, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.18834733963013, "training_acc": 53.0, "val_loss": 17.335987091064453, "val_acc": 52.0}
