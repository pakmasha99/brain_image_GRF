"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.36414074897766, "training_acc": 48.0, "val_loss": 17.333097755908966, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.49331617355347, "training_acc": 40.0, "val_loss": 17.30535328388214, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.33575463294983, "training_acc": 52.0, "val_loss": 17.297066748142242, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.18447780609131, "training_acc": 52.0, "val_loss": 17.34803318977356, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.173006772995, "training_acc": 53.0, "val_loss": 17.32531487941742, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.04694867134094, "training_acc": 53.0, "val_loss": 17.325393855571747, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.03528714179993, "training_acc": 53.0, "val_loss": 17.312461137771606, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.1675980091095, "training_acc": 53.0, "val_loss": 17.284195125102997, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.08438062667847, "training_acc": 53.0, "val_loss": 17.278221249580383, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.01254343986511, "training_acc": 53.0, "val_loss": 17.35372245311737, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.09027910232544, "training_acc": 53.0, "val_loss": 17.35076904296875, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.06489062309265, "training_acc": 53.0, "val_loss": 17.35542267560959, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.248211145401, "training_acc": 53.0, "val_loss": 17.31370836496353, "val_acc": 52.0}
{"epoch": 13, "training_loss": 68.99410629272461, "training_acc": 53.0, "val_loss": 17.30768233537674, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.11768817901611, "training_acc": 53.0, "val_loss": 17.310252785682678, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.13657736778259, "training_acc": 53.0, "val_loss": 17.314746975898743, "val_acc": 52.0}
{"epoch": 16, "training_loss": 68.99313521385193, "training_acc": 53.0, "val_loss": 17.31165051460266, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.11451363563538, "training_acc": 53.0, "val_loss": 17.328228056430817, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.0047562122345, "training_acc": 53.0, "val_loss": 17.301180958747864, "val_acc": 52.0}
{"epoch": 19, "training_loss": 68.97942566871643, "training_acc": 53.0, "val_loss": 17.291541397571564, "val_acc": 52.0}
{"epoch": 20, "training_loss": 68.96927571296692, "training_acc": 53.0, "val_loss": 17.29714572429657, "val_acc": 52.0}
{"epoch": 21, "training_loss": 68.71526408195496, "training_acc": 53.0, "val_loss": 17.309169471263885, "val_acc": 52.0}
{"epoch": 22, "training_loss": 68.81161499023438, "training_acc": 53.0, "val_loss": 17.334705591201782, "val_acc": 52.0}
{"epoch": 23, "training_loss": 68.97735905647278, "training_acc": 53.0, "val_loss": 17.354579269886017, "val_acc": 52.0}
{"epoch": 24, "training_loss": 68.86238050460815, "training_acc": 53.0, "val_loss": 17.32785999774933, "val_acc": 52.0}
{"epoch": 25, "training_loss": 68.54573178291321, "training_acc": 53.0, "val_loss": 17.3112690448761, "val_acc": 52.0}
{"epoch": 26, "training_loss": 68.63896131515503, "training_acc": 53.0, "val_loss": 17.316919565200806, "val_acc": 52.0}
{"epoch": 27, "training_loss": 68.74855351448059, "training_acc": 53.0, "val_loss": 17.327815294265747, "val_acc": 52.0}
