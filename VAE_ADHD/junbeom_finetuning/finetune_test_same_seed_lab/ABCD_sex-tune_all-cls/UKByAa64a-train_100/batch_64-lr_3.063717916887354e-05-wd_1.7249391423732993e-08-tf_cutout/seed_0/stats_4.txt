"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.37334179878235, "training_acc": 53.0, "val_loss": 17.33623296022415, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.11648273468018, "training_acc": 53.0, "val_loss": 17.36263930797577, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.10990142822266, "training_acc": 53.0, "val_loss": 17.32662171125412, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.2940456867218, "training_acc": 53.0, "val_loss": 17.35352724790573, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.09115934371948, "training_acc": 53.0, "val_loss": 17.326751351356506, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.05061364173889, "training_acc": 53.0, "val_loss": 17.359288036823273, "val_acc": 52.0}
{"epoch": 6, "training_loss": 68.98070096969604, "training_acc": 53.0, "val_loss": 17.319419980049133, "val_acc": 52.0}
{"epoch": 7, "training_loss": 68.88499975204468, "training_acc": 53.0, "val_loss": 17.33012944459915, "val_acc": 52.0}
{"epoch": 8, "training_loss": 68.65609788894653, "training_acc": 53.0, "val_loss": 17.384836077690125, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.15577697753906, "training_acc": 53.0, "val_loss": 17.356041073799133, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.3559467792511, "training_acc": 53.0, "val_loss": 17.28925257921219, "val_acc": 52.0}
{"epoch": 11, "training_loss": 68.87981295585632, "training_acc": 53.0, "val_loss": 17.30816811323166, "val_acc": 52.0}
{"epoch": 12, "training_loss": 68.87478256225586, "training_acc": 53.0, "val_loss": 17.309075593948364, "val_acc": 52.0}
{"epoch": 13, "training_loss": 68.97425627708435, "training_acc": 53.0, "val_loss": 17.301541566848755, "val_acc": 52.0}
{"epoch": 14, "training_loss": 68.92164993286133, "training_acc": 53.0, "val_loss": 17.29515939950943, "val_acc": 52.0}
{"epoch": 15, "training_loss": 68.74820637702942, "training_acc": 53.0, "val_loss": 17.298075556755066, "val_acc": 52.0}
{"epoch": 16, "training_loss": 68.67677521705627, "training_acc": 53.0, "val_loss": 17.296819388866425, "val_acc": 52.0}
{"epoch": 17, "training_loss": 68.36733651161194, "training_acc": 53.0, "val_loss": 17.250382900238037, "val_acc": 52.0}
{"epoch": 18, "training_loss": 68.41581273078918, "training_acc": 54.0, "val_loss": 17.216403782367706, "val_acc": 52.0}
{"epoch": 19, "training_loss": 68.37165689468384, "training_acc": 56.0, "val_loss": 17.216797173023224, "val_acc": 52.0}
{"epoch": 20, "training_loss": 67.87385869026184, "training_acc": 59.0, "val_loss": 17.317788302898407, "val_acc": 52.0}
{"epoch": 21, "training_loss": 68.22162580490112, "training_acc": 54.0, "val_loss": 17.237944900989532, "val_acc": 52.0}
{"epoch": 22, "training_loss": 68.52163672447205, "training_acc": 69.0, "val_loss": 17.213545739650726, "val_acc": 52.0}
{"epoch": 23, "training_loss": 67.94539189338684, "training_acc": 67.0, "val_loss": 17.271433770656586, "val_acc": 52.0}
{"epoch": 24, "training_loss": 68.0127739906311, "training_acc": 53.0, "val_loss": 17.178653180599213, "val_acc": 52.0}
{"epoch": 25, "training_loss": 67.96937251091003, "training_acc": 66.0, "val_loss": 17.203079164028168, "val_acc": 52.0}
{"epoch": 26, "training_loss": 67.71993684768677, "training_acc": 66.0, "val_loss": 17.500178515911102, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.17503595352173, "training_acc": 53.0, "val_loss": 17.42991954088211, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.31272673606873, "training_acc": 53.0, "val_loss": 17.3310786485672, "val_acc": 52.0}
{"epoch": 29, "training_loss": 69.09107780456543, "training_acc": 53.0, "val_loss": 17.31475591659546, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.06169390678406, "training_acc": 53.0, "val_loss": 17.30733811855316, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.16859936714172, "training_acc": 53.0, "val_loss": 17.30484366416931, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.12197923660278, "training_acc": 53.0, "val_loss": 17.29884445667267, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.08935046195984, "training_acc": 53.0, "val_loss": 17.30320155620575, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.12690806388855, "training_acc": 53.0, "val_loss": 17.313086986541748, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.08128833770752, "training_acc": 53.0, "val_loss": 17.305663228034973, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.01842474937439, "training_acc": 53.0, "val_loss": 17.299281060695648, "val_acc": 52.0}
{"epoch": 37, "training_loss": 68.96754479408264, "training_acc": 53.0, "val_loss": 17.297950387001038, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.13416767120361, "training_acc": 53.0, "val_loss": 17.30026602745056, "val_acc": 52.0}
{"epoch": 39, "training_loss": 68.93410229682922, "training_acc": 53.0, "val_loss": 17.295436561107635, "val_acc": 52.0}
{"epoch": 40, "training_loss": 68.97864651679993, "training_acc": 53.0, "val_loss": 17.297951877117157, "val_acc": 52.0}
{"epoch": 41, "training_loss": 68.9514365196228, "training_acc": 53.0, "val_loss": 17.293967306613922, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.07407259941101, "training_acc": 53.0, "val_loss": 17.28970557451248, "val_acc": 52.0}
{"epoch": 43, "training_loss": 68.93237328529358, "training_acc": 53.0, "val_loss": 17.281383275985718, "val_acc": 52.0}
