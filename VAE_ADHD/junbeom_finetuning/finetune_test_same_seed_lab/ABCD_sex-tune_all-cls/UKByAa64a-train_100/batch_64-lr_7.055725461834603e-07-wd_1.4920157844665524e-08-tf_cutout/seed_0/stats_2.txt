"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.14026188850403, "training_acc": 53.0, "val_loss": 17.30653941631317, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.15728068351746, "training_acc": 53.0, "val_loss": 17.305707931518555, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.17869639396667, "training_acc": 53.0, "val_loss": 17.30147898197174, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.03998517990112, "training_acc": 53.0, "val_loss": 17.295818030834198, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.14372682571411, "training_acc": 53.0, "val_loss": 17.29196459054947, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.01777267456055, "training_acc": 53.0, "val_loss": 17.291586101055145, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.0755968093872, "training_acc": 53.0, "val_loss": 17.293281853199005, "val_acc": 52.0}
{"epoch": 7, "training_loss": 68.9900290966034, "training_acc": 53.0, "val_loss": 17.292357981204987, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.12762522697449, "training_acc": 53.0, "val_loss": 17.287884652614594, "val_acc": 52.0}
{"epoch": 9, "training_loss": 68.92240118980408, "training_acc": 53.0, "val_loss": 17.28435903787613, "val_acc": 52.0}
{"epoch": 10, "training_loss": 68.93031811714172, "training_acc": 53.0, "val_loss": 17.281699180603027, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.12775874137878, "training_acc": 53.0, "val_loss": 17.279838025569916, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.02451252937317, "training_acc": 53.0, "val_loss": 17.280128598213196, "val_acc": 52.0}
{"epoch": 13, "training_loss": 68.98639297485352, "training_acc": 53.0, "val_loss": 17.281334102153778, "val_acc": 52.0}
{"epoch": 14, "training_loss": 68.93007349967957, "training_acc": 53.0, "val_loss": 17.281416058540344, "val_acc": 52.0}
{"epoch": 15, "training_loss": 68.89442372322083, "training_acc": 53.0, "val_loss": 17.281833291053772, "val_acc": 52.0}
{"epoch": 16, "training_loss": 68.96480250358582, "training_acc": 53.0, "val_loss": 17.281994223594666, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.006587266922, "training_acc": 53.0, "val_loss": 17.283664643764496, "val_acc": 52.0}
{"epoch": 18, "training_loss": 68.91196417808533, "training_acc": 53.0, "val_loss": 17.285266518592834, "val_acc": 52.0}
{"epoch": 19, "training_loss": 68.86496353149414, "training_acc": 53.0, "val_loss": 17.287379503250122, "val_acc": 52.0}
{"epoch": 20, "training_loss": 68.91292643547058, "training_acc": 53.0, "val_loss": 17.288482189178467, "val_acc": 52.0}
{"epoch": 21, "training_loss": 68.96408152580261, "training_acc": 53.0, "val_loss": 17.286786437034607, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.02507638931274, "training_acc": 53.0, "val_loss": 17.28406548500061, "val_acc": 52.0}
{"epoch": 23, "training_loss": 68.89257621765137, "training_acc": 53.0, "val_loss": 17.283333837985992, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.01824569702148, "training_acc": 53.0, "val_loss": 17.284871637821198, "val_acc": 52.0}
{"epoch": 25, "training_loss": 68.71302342414856, "training_acc": 53.0, "val_loss": 17.28261560201645, "val_acc": 52.0}
{"epoch": 26, "training_loss": 68.78555727005005, "training_acc": 53.0, "val_loss": 17.28053390979767, "val_acc": 52.0}
{"epoch": 27, "training_loss": 68.84696292877197, "training_acc": 53.0, "val_loss": 17.279599606990814, "val_acc": 52.0}
{"epoch": 28, "training_loss": 68.95375561714172, "training_acc": 53.0, "val_loss": 17.281097173690796, "val_acc": 52.0}
{"epoch": 29, "training_loss": 68.87265372276306, "training_acc": 53.0, "val_loss": 17.280860245227814, "val_acc": 52.0}
{"epoch": 30, "training_loss": 68.79443693161011, "training_acc": 53.0, "val_loss": 17.2784686088562, "val_acc": 52.0}
{"epoch": 31, "training_loss": 68.80569863319397, "training_acc": 53.0, "val_loss": 17.27941930294037, "val_acc": 52.0}
{"epoch": 32, "training_loss": 68.85505294799805, "training_acc": 53.0, "val_loss": 17.280764877796173, "val_acc": 52.0}
{"epoch": 33, "training_loss": 68.76312208175659, "training_acc": 53.0, "val_loss": 17.280927300453186, "val_acc": 52.0}
{"epoch": 34, "training_loss": 68.78428649902344, "training_acc": 53.0, "val_loss": 17.2802597284317, "val_acc": 52.0}
{"epoch": 35, "training_loss": 68.74382448196411, "training_acc": 53.0, "val_loss": 17.279212176799774, "val_acc": 52.0}
{"epoch": 36, "training_loss": 68.80966973304749, "training_acc": 53.0, "val_loss": 17.280055582523346, "val_acc": 52.0}
{"epoch": 37, "training_loss": 68.63730716705322, "training_acc": 53.0, "val_loss": 17.281153798103333, "val_acc": 52.0}
{"epoch": 38, "training_loss": 68.68841552734375, "training_acc": 53.0, "val_loss": 17.281337082386017, "val_acc": 52.0}
{"epoch": 39, "training_loss": 68.62732338905334, "training_acc": 53.0, "val_loss": 17.282181978225708, "val_acc": 52.0}
{"epoch": 40, "training_loss": 68.73861575126648, "training_acc": 53.0, "val_loss": 17.284782230854034, "val_acc": 52.0}
{"epoch": 41, "training_loss": 68.6592664718628, "training_acc": 53.0, "val_loss": 17.28665679693222, "val_acc": 52.0}
{"epoch": 42, "training_loss": 68.6717119216919, "training_acc": 53.0, "val_loss": 17.287012934684753, "val_acc": 52.0}
{"epoch": 43, "training_loss": 68.81692576408386, "training_acc": 53.0, "val_loss": 17.28716492652893, "val_acc": 52.0}
{"epoch": 44, "training_loss": 68.68165707588196, "training_acc": 53.0, "val_loss": 17.28719472885132, "val_acc": 52.0}
{"epoch": 45, "training_loss": 68.5857846736908, "training_acc": 53.0, "val_loss": 17.286628484725952, "val_acc": 52.0}
{"epoch": 46, "training_loss": 68.58227968215942, "training_acc": 53.0, "val_loss": 17.286109924316406, "val_acc": 52.0}
{"epoch": 47, "training_loss": 68.68773245811462, "training_acc": 53.0, "val_loss": 17.284618318080902, "val_acc": 52.0}
{"epoch": 48, "training_loss": 68.66114544868469, "training_acc": 53.0, "val_loss": 17.28244423866272, "val_acc": 52.0}
{"epoch": 49, "training_loss": 68.60061430931091, "training_acc": 53.0, "val_loss": 17.28098839521408, "val_acc": 52.0}
