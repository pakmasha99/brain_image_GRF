"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.77107548713684, "training_acc": 47.0, "val_loss": 17.413221299648285, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.78789186477661, "training_acc": 47.0, "val_loss": 17.403046786785126, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.68347215652466, "training_acc": 47.0, "val_loss": 17.393963038921356, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.60476922988892, "training_acc": 47.0, "val_loss": 17.387928068637848, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.53574061393738, "training_acc": 47.0, "val_loss": 17.382556200027466, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.3821074962616, "training_acc": 47.0, "val_loss": 17.374473810195923, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.43187642097473, "training_acc": 47.0, "val_loss": 17.36343204975128, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.2669723033905, "training_acc": 48.0, "val_loss": 17.367710173130035, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.39507293701172, "training_acc": 50.0, "val_loss": 17.370250821113586, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.27424645423889, "training_acc": 54.0, "val_loss": 17.36598163843155, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.18044066429138, "training_acc": 55.0, "val_loss": 17.3589289188385, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.1749963760376, "training_acc": 58.0, "val_loss": 17.34398603439331, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.08371472358704, "training_acc": 62.0, "val_loss": 17.319388687610626, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.1299319267273, "training_acc": 52.0, "val_loss": 17.285697162151337, "val_acc": 52.0}
{"epoch": 14, "training_loss": 68.95166230201721, "training_acc": 60.0, "val_loss": 17.27890372276306, "val_acc": 52.0}
{"epoch": 15, "training_loss": 68.93399953842163, "training_acc": 58.0, "val_loss": 17.282676696777344, "val_acc": 52.0}
{"epoch": 16, "training_loss": 68.81082773208618, "training_acc": 63.0, "val_loss": 17.305251955986023, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.09939885139465, "training_acc": 52.0, "val_loss": 17.349454760551453, "val_acc": 52.0}
{"epoch": 18, "training_loss": 68.81066751480103, "training_acc": 60.0, "val_loss": 17.389845848083496, "val_acc": 52.0}
{"epoch": 19, "training_loss": 69.07949662208557, "training_acc": 53.0, "val_loss": 17.403480410575867, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.1060676574707, "training_acc": 51.0, "val_loss": 17.40405261516571, "val_acc": 52.0}
{"epoch": 21, "training_loss": 68.76336646080017, "training_acc": 57.0, "val_loss": 17.391790449619293, "val_acc": 52.0}
{"epoch": 22, "training_loss": 68.92769455909729, "training_acc": 52.0, "val_loss": 17.36813634634018, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.21470952033997, "training_acc": 46.0, "val_loss": 17.34188050031662, "val_acc": 52.0}
{"epoch": 24, "training_loss": 68.7792820930481, "training_acc": 55.0, "val_loss": 17.326244711875916, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.0007951259613, "training_acc": 48.0, "val_loss": 17.328962683677673, "val_acc": 52.0}
{"epoch": 26, "training_loss": 68.94101762771606, "training_acc": 52.0, "val_loss": 17.326323688030243, "val_acc": 52.0}
{"epoch": 27, "training_loss": 68.77664542198181, "training_acc": 60.0, "val_loss": 17.32340157032013, "val_acc": 52.0}
{"epoch": 28, "training_loss": 68.46559262275696, "training_acc": 62.0, "val_loss": 17.31378138065338, "val_acc": 52.0}
{"epoch": 29, "training_loss": 68.90120673179626, "training_acc": 50.0, "val_loss": 17.311252653598785, "val_acc": 52.0}
{"epoch": 30, "training_loss": 68.92540955543518, "training_acc": 56.0, "val_loss": 17.320214211940765, "val_acc": 52.0}
{"epoch": 31, "training_loss": 68.5761604309082, "training_acc": 57.0, "val_loss": 17.318493127822876, "val_acc": 52.0}
{"epoch": 32, "training_loss": 68.63583016395569, "training_acc": 55.0, "val_loss": 17.318016290664673, "val_acc": 52.0}
{"epoch": 33, "training_loss": 68.55934524536133, "training_acc": 51.0, "val_loss": 17.35002249479294, "val_acc": 52.0}
