"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.27613043785095, "training_acc": 53.0, "val_loss": 17.29385405778885, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.2198965549469, "training_acc": 53.0, "val_loss": 17.3151433467865, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.15273857116699, "training_acc": 53.0, "val_loss": 17.322494089603424, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.17576575279236, "training_acc": 53.0, "val_loss": 17.298099398612976, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.15513229370117, "training_acc": 53.0, "val_loss": 17.29569435119629, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.06143498420715, "training_acc": 53.0, "val_loss": 17.295899987220764, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.03463292121887, "training_acc": 53.0, "val_loss": 17.298465967178345, "val_acc": 52.0}
{"epoch": 7, "training_loss": 68.99317479133606, "training_acc": 53.0, "val_loss": 17.300933599472046, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.02446269989014, "training_acc": 53.0, "val_loss": 17.30688214302063, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.07635045051575, "training_acc": 53.0, "val_loss": 17.307591438293457, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.19432306289673, "training_acc": 53.0, "val_loss": 17.3003152012825, "val_acc": 52.0}
{"epoch": 11, "training_loss": 68.76794457435608, "training_acc": 53.0, "val_loss": 17.310437560081482, "val_acc": 52.0}
{"epoch": 12, "training_loss": 68.8854250907898, "training_acc": 53.0, "val_loss": 17.30814129114151, "val_acc": 52.0}
{"epoch": 13, "training_loss": 68.8323233127594, "training_acc": 53.0, "val_loss": 17.29274094104767, "val_acc": 52.0}
{"epoch": 14, "training_loss": 68.87614750862122, "training_acc": 53.0, "val_loss": 17.292290925979614, "val_acc": 52.0}
{"epoch": 15, "training_loss": 68.94552993774414, "training_acc": 53.0, "val_loss": 17.28212833404541, "val_acc": 52.0}
{"epoch": 16, "training_loss": 68.4801619052887, "training_acc": 53.0, "val_loss": 17.267557978630066, "val_acc": 52.0}
{"epoch": 17, "training_loss": 68.57079792022705, "training_acc": 53.0, "val_loss": 17.253299057483673, "val_acc": 52.0}
{"epoch": 18, "training_loss": 68.3641664981842, "training_acc": 53.0, "val_loss": 17.238521575927734, "val_acc": 52.0}
{"epoch": 19, "training_loss": 68.3431396484375, "training_acc": 53.0, "val_loss": 17.224635183811188, "val_acc": 52.0}
{"epoch": 20, "training_loss": 68.49783301353455, "training_acc": 54.0, "val_loss": 17.21421331167221, "val_acc": 52.0}
{"epoch": 21, "training_loss": 68.04754614830017, "training_acc": 54.0, "val_loss": 17.19915419816971, "val_acc": 52.0}
{"epoch": 22, "training_loss": 68.0889618396759, "training_acc": 57.0, "val_loss": 17.18709170818329, "val_acc": 52.0}
{"epoch": 23, "training_loss": 67.6388258934021, "training_acc": 65.0, "val_loss": 17.181983590126038, "val_acc": 52.0}
{"epoch": 24, "training_loss": 67.4433434009552, "training_acc": 56.0, "val_loss": 17.143285274505615, "val_acc": 52.0}
{"epoch": 25, "training_loss": 67.0775043964386, "training_acc": 61.0, "val_loss": 17.12445169687271, "val_acc": 52.0}
{"epoch": 26, "training_loss": 67.25209951400757, "training_acc": 68.0, "val_loss": 17.135703563690186, "val_acc": 52.0}
{"epoch": 27, "training_loss": 66.6998643875122, "training_acc": 56.0, "val_loss": 17.089396715164185, "val_acc": 52.0}
{"epoch": 28, "training_loss": 65.55054044723511, "training_acc": 65.0, "val_loss": 17.027240991592407, "val_acc": 52.0}
{"epoch": 29, "training_loss": 66.53257417678833, "training_acc": 63.0, "val_loss": 17.034874856472015, "val_acc": 52.0}
{"epoch": 30, "training_loss": 64.83940076828003, "training_acc": 79.0, "val_loss": 17.044763267040253, "val_acc": 52.0}
{"epoch": 31, "training_loss": 66.25621747970581, "training_acc": 57.0, "val_loss": 16.966979205608368, "val_acc": 52.0}
{"epoch": 32, "training_loss": 64.78706455230713, "training_acc": 73.0, "val_loss": 17.07877367734909, "val_acc": 52.0}
{"epoch": 33, "training_loss": 64.38047003746033, "training_acc": 81.0, "val_loss": 17.09817796945572, "val_acc": 52.0}
{"epoch": 34, "training_loss": 64.58137440681458, "training_acc": 61.0, "val_loss": 16.93202406167984, "val_acc": 52.0}
{"epoch": 35, "training_loss": 63.71827816963196, "training_acc": 84.0, "val_loss": 16.855241358280182, "val_acc": 52.0}
{"epoch": 36, "training_loss": 62.15297746658325, "training_acc": 84.0, "val_loss": 17.125126719474792, "val_acc": 52.0}
{"epoch": 37, "training_loss": 63.90814256668091, "training_acc": 59.0, "val_loss": 17.05392450094223, "val_acc": 52.0}
{"epoch": 38, "training_loss": 63.07650113105774, "training_acc": 76.0, "val_loss": 16.772837936878204, "val_acc": 52.0}
{"epoch": 39, "training_loss": 64.01353812217712, "training_acc": 72.0, "val_loss": 16.949976980686188, "val_acc": 52.0}
{"epoch": 40, "training_loss": 62.22043991088867, "training_acc": 68.0, "val_loss": 16.881610453128815, "val_acc": 52.0}
{"epoch": 41, "training_loss": 60.87749528884888, "training_acc": 87.0, "val_loss": 16.687403619289398, "val_acc": 52.0}
{"epoch": 42, "training_loss": 60.1580696105957, "training_acc": 77.0, "val_loss": 16.598527133464813, "val_acc": 52.0}
{"epoch": 43, "training_loss": 61.570011377334595, "training_acc": 75.0, "val_loss": 16.552989184856415, "val_acc": 52.0}
{"epoch": 44, "training_loss": 59.02489185333252, "training_acc": 80.0, "val_loss": 16.70920103788376, "val_acc": 52.0}
{"epoch": 45, "training_loss": 58.177364110946655, "training_acc": 82.0, "val_loss": 16.807931661605835, "val_acc": 52.0}
{"epoch": 46, "training_loss": 58.123539447784424, "training_acc": 85.0, "val_loss": 16.78996980190277, "val_acc": 52.0}
{"epoch": 47, "training_loss": 57.72846698760986, "training_acc": 78.0, "val_loss": 16.74688458442688, "val_acc": 52.0}
{"epoch": 48, "training_loss": 55.88398456573486, "training_acc": 86.0, "val_loss": 16.498389840126038, "val_acc": 56.0}
{"epoch": 49, "training_loss": 54.29993271827698, "training_acc": 96.0, "val_loss": 16.534098982810974, "val_acc": 56.0}
{"epoch": 50, "training_loss": 54.9311363697052, "training_acc": 83.0, "val_loss": 17.09810644388199, "val_acc": 56.0}
{"epoch": 51, "training_loss": 56.906752824783325, "training_acc": 83.0, "val_loss": 17.47673898935318, "val_acc": 52.0}
{"epoch": 52, "training_loss": 58.68830180168152, "training_acc": 69.0, "val_loss": 16.671748459339142, "val_acc": 56.0}
{"epoch": 53, "training_loss": 52.26750349998474, "training_acc": 89.0, "val_loss": 17.015403509140015, "val_acc": 64.0}
{"epoch": 54, "training_loss": 51.80914545059204, "training_acc": 91.0, "val_loss": 16.911369562149048, "val_acc": 56.0}
{"epoch": 55, "training_loss": 51.10235834121704, "training_acc": 92.0, "val_loss": 16.76766574382782, "val_acc": 52.0}
{"epoch": 56, "training_loss": 49.15419590473175, "training_acc": 95.0, "val_loss": 16.96248948574066, "val_acc": 56.0}
{"epoch": 57, "training_loss": 50.81506323814392, "training_acc": 85.0, "val_loss": 16.750355064868927, "val_acc": 52.0}
{"epoch": 58, "training_loss": 47.9739785194397, "training_acc": 92.0, "val_loss": 16.64597988128662, "val_acc": 60.0}
{"epoch": 59, "training_loss": 47.28614389896393, "training_acc": 93.0, "val_loss": 16.684047877788544, "val_acc": 56.0}
{"epoch": 60, "training_loss": 45.55261528491974, "training_acc": 97.0, "val_loss": 16.650791466236115, "val_acc": 56.0}
{"epoch": 61, "training_loss": 44.010576486587524, "training_acc": 97.0, "val_loss": 16.647660732269287, "val_acc": 56.0}
{"epoch": 62, "training_loss": 43.10672056674957, "training_acc": 99.0, "val_loss": 17.087605595588684, "val_acc": 64.0}
{"epoch": 63, "training_loss": 44.83151614665985, "training_acc": 97.0, "val_loss": 16.96729063987732, "val_acc": 56.0}
{"epoch": 64, "training_loss": 43.96286427974701, "training_acc": 94.0, "val_loss": 16.747769713401794, "val_acc": 56.0}
{"epoch": 65, "training_loss": 44.29658901691437, "training_acc": 93.0, "val_loss": 16.65603369474411, "val_acc": 56.0}
{"epoch": 66, "training_loss": 41.46927034854889, "training_acc": 98.0, "val_loss": 16.813945770263672, "val_acc": 56.0}
{"epoch": 67, "training_loss": 41.549047350883484, "training_acc": 96.0, "val_loss": 16.89891368150711, "val_acc": 56.0}
