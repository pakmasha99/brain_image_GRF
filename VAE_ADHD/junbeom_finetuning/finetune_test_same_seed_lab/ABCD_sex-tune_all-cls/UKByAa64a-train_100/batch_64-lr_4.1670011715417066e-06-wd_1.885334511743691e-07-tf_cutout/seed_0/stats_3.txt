"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.58091282844543, "training_acc": 53.0, "val_loss": 17.473872005939484, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.48442006111145, "training_acc": 53.0, "val_loss": 17.478494346141815, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.48998689651489, "training_acc": 53.0, "val_loss": 17.46007651090622, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.22585415840149, "training_acc": 53.0, "val_loss": 17.42384284734726, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.25551533699036, "training_acc": 53.0, "val_loss": 17.39964634180069, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.07353734970093, "training_acc": 53.0, "val_loss": 17.37280637025833, "val_acc": 52.0}
{"epoch": 6, "training_loss": 68.98240256309509, "training_acc": 53.0, "val_loss": 17.39100068807602, "val_acc": 52.0}
{"epoch": 7, "training_loss": 68.79987668991089, "training_acc": 53.0, "val_loss": 17.44813323020935, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.04756999015808, "training_acc": 53.0, "val_loss": 17.448844015598297, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.03314542770386, "training_acc": 53.0, "val_loss": 17.461983859539032, "val_acc": 52.0}
{"epoch": 10, "training_loss": 68.66678738594055, "training_acc": 53.0, "val_loss": 17.433054745197296, "val_acc": 52.0}
{"epoch": 11, "training_loss": 68.99273371696472, "training_acc": 53.0, "val_loss": 17.372488975524902, "val_acc": 52.0}
{"epoch": 12, "training_loss": 68.6060061454773, "training_acc": 53.0, "val_loss": 17.41632968187332, "val_acc": 52.0}
{"epoch": 13, "training_loss": 68.73097562789917, "training_acc": 53.0, "val_loss": 17.43313819169998, "val_acc": 52.0}
{"epoch": 14, "training_loss": 68.46858215332031, "training_acc": 53.0, "val_loss": 17.438769340515137, "val_acc": 52.0}
{"epoch": 15, "training_loss": 68.69790720939636, "training_acc": 53.0, "val_loss": 17.439132928848267, "val_acc": 52.0}
{"epoch": 16, "training_loss": 68.38792753219604, "training_acc": 53.0, "val_loss": 17.42403507232666, "val_acc": 52.0}
{"epoch": 17, "training_loss": 68.56417536735535, "training_acc": 53.0, "val_loss": 17.432445287704468, "val_acc": 52.0}
{"epoch": 18, "training_loss": 67.94591903686523, "training_acc": 53.0, "val_loss": 17.46489703655243, "val_acc": 52.0}
{"epoch": 19, "training_loss": 68.22707080841064, "training_acc": 53.0, "val_loss": 17.47540831565857, "val_acc": 52.0}
{"epoch": 20, "training_loss": 67.80592727661133, "training_acc": 53.0, "val_loss": 17.503860592842102, "val_acc": 52.0}
{"epoch": 21, "training_loss": 68.19125175476074, "training_acc": 53.0, "val_loss": 17.44915395975113, "val_acc": 52.0}
{"epoch": 22, "training_loss": 67.91629910469055, "training_acc": 53.0, "val_loss": 17.456889152526855, "val_acc": 52.0}
{"epoch": 23, "training_loss": 67.45486950874329, "training_acc": 55.0, "val_loss": 17.428453266620636, "val_acc": 52.0}
{"epoch": 24, "training_loss": 67.79974842071533, "training_acc": 53.0, "val_loss": 17.470522224903107, "val_acc": 52.0}
{"epoch": 25, "training_loss": 67.61975979804993, "training_acc": 55.0, "val_loss": 17.552655935287476, "val_acc": 52.0}
{"epoch": 26, "training_loss": 67.91998028755188, "training_acc": 53.0, "val_loss": 17.593254148960114, "val_acc": 52.0}
{"epoch": 27, "training_loss": 67.58175444602966, "training_acc": 53.0, "val_loss": 17.54968911409378, "val_acc": 52.0}
{"epoch": 28, "training_loss": 67.47689175605774, "training_acc": 56.0, "val_loss": 17.517317831516266, "val_acc": 52.0}
{"epoch": 29, "training_loss": 67.36763978004456, "training_acc": 57.0, "val_loss": 17.617951333522797, "val_acc": 52.0}
{"epoch": 30, "training_loss": 67.66809058189392, "training_acc": 53.0, "val_loss": 17.638076841831207, "val_acc": 52.0}
{"epoch": 31, "training_loss": 67.6190094947815, "training_acc": 55.0, "val_loss": 17.35062152147293, "val_acc": 52.0}
{"epoch": 32, "training_loss": 66.99064350128174, "training_acc": 64.0, "val_loss": 17.333409190177917, "val_acc": 52.0}
{"epoch": 33, "training_loss": 66.8303050994873, "training_acc": 67.0, "val_loss": 17.61915534734726, "val_acc": 52.0}
{"epoch": 34, "training_loss": 67.25791096687317, "training_acc": 56.0, "val_loss": 17.65020340681076, "val_acc": 52.0}
{"epoch": 35, "training_loss": 66.9022171497345, "training_acc": 57.0, "val_loss": 17.417508363723755, "val_acc": 52.0}
{"epoch": 36, "training_loss": 66.76278018951416, "training_acc": 65.0, "val_loss": 17.47412383556366, "val_acc": 52.0}
{"epoch": 37, "training_loss": 66.26428580284119, "training_acc": 61.0, "val_loss": 17.781411111354828, "val_acc": 52.0}
{"epoch": 38, "training_loss": 66.69669961929321, "training_acc": 53.0, "val_loss": 17.703022062778473, "val_acc": 52.0}
{"epoch": 39, "training_loss": 66.35745882987976, "training_acc": 59.0, "val_loss": 17.354369163513184, "val_acc": 52.0}
{"epoch": 40, "training_loss": 66.58012342453003, "training_acc": 69.0, "val_loss": 17.56817251443863, "val_acc": 52.0}
{"epoch": 41, "training_loss": 66.62269902229309, "training_acc": 62.0, "val_loss": 17.69166588783264, "val_acc": 52.0}
{"epoch": 42, "training_loss": 66.00698518753052, "training_acc": 68.0, "val_loss": 17.516036331653595, "val_acc": 52.0}
{"epoch": 43, "training_loss": 66.15061497688293, "training_acc": 66.0, "val_loss": 17.61002689599991, "val_acc": 52.0}
{"epoch": 44, "training_loss": 65.69067454338074, "training_acc": 64.0, "val_loss": 17.707152664661407, "val_acc": 52.0}
{"epoch": 45, "training_loss": 64.93518447875977, "training_acc": 67.0, "val_loss": 17.43854135274887, "val_acc": 52.0}
{"epoch": 46, "training_loss": 66.07561540603638, "training_acc": 67.0, "val_loss": 17.54266321659088, "val_acc": 52.0}
{"epoch": 47, "training_loss": 65.36511063575745, "training_acc": 69.0, "val_loss": 17.892509698867798, "val_acc": 52.0}
{"epoch": 48, "training_loss": 65.56983590126038, "training_acc": 57.0, "val_loss": 17.806440591812134, "val_acc": 52.0}
{"epoch": 49, "training_loss": 64.84022951126099, "training_acc": 70.0, "val_loss": 17.456112802028656, "val_acc": 52.0}
{"epoch": 50, "training_loss": 65.37621545791626, "training_acc": 77.0, "val_loss": 17.50507652759552, "val_acc": 52.0}
{"epoch": 51, "training_loss": 65.88970589637756, "training_acc": 68.0, "val_loss": 17.984482645988464, "val_acc": 52.0}
