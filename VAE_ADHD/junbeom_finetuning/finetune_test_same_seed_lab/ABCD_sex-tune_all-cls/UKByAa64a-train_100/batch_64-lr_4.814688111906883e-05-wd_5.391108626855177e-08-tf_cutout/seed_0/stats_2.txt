"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 68.84956669807434, "training_acc": 50.0, "val_loss": 17.354796826839447, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.36706113815308, "training_acc": 48.0, "val_loss": 17.336076498031616, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.85730838775635, "training_acc": 53.0, "val_loss": 17.34049618244171, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.74098348617554, "training_acc": 43.0, "val_loss": 17.319807410240173, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.09093475341797, "training_acc": 53.0, "val_loss": 17.355190217494965, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.13915586471558, "training_acc": 53.0, "val_loss": 17.33604371547699, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.1027443408966, "training_acc": 53.0, "val_loss": 17.320363223552704, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.38118314743042, "training_acc": 49.0, "val_loss": 17.365214228630066, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.12775230407715, "training_acc": 52.0, "val_loss": 17.291556298732758, "val_acc": 52.0}
{"epoch": 9, "training_loss": 68.98386526107788, "training_acc": 53.0, "val_loss": 17.373718321323395, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.27442264556885, "training_acc": 53.0, "val_loss": 17.323091626167297, "val_acc": 52.0}
{"epoch": 11, "training_loss": 68.92004489898682, "training_acc": 53.0, "val_loss": 17.2804594039917, "val_acc": 52.0}
{"epoch": 12, "training_loss": 68.70195436477661, "training_acc": 53.0, "val_loss": 17.266136407852173, "val_acc": 52.0}
{"epoch": 13, "training_loss": 68.52545237541199, "training_acc": 53.0, "val_loss": 17.286211252212524, "val_acc": 52.0}
{"epoch": 14, "training_loss": 68.73564887046814, "training_acc": 65.0, "val_loss": 17.26342886686325, "val_acc": 52.0}
{"epoch": 15, "training_loss": 68.3078191280365, "training_acc": 59.0, "val_loss": 17.282049357891083, "val_acc": 52.0}
{"epoch": 16, "training_loss": 68.4391553401947, "training_acc": 53.0, "val_loss": 17.231765389442444, "val_acc": 52.0}
{"epoch": 17, "training_loss": 67.94042897224426, "training_acc": 59.0, "val_loss": 17.228706181049347, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.449134349823, "training_acc": 57.0, "val_loss": 17.27995127439499, "val_acc": 52.0}
{"epoch": 19, "training_loss": 67.71052622795105, "training_acc": 53.0, "val_loss": 17.81446486711502, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.87758016586304, "training_acc": 53.0, "val_loss": 17.629386484622955, "val_acc": 52.0}
{"epoch": 21, "training_loss": 68.15355205535889, "training_acc": 53.0, "val_loss": 17.26292073726654, "val_acc": 52.0}
{"epoch": 22, "training_loss": 68.9081289768219, "training_acc": 66.0, "val_loss": 17.405840754508972, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.27956700325012, "training_acc": 48.0, "val_loss": 17.289617657661438, "val_acc": 52.0}
{"epoch": 24, "training_loss": 68.62613248825073, "training_acc": 76.0, "val_loss": 17.263133823871613, "val_acc": 52.0}
{"epoch": 25, "training_loss": 68.02469205856323, "training_acc": 53.0, "val_loss": 17.405833303928375, "val_acc": 52.0}
{"epoch": 26, "training_loss": 68.50547099113464, "training_acc": 53.0, "val_loss": 17.21940040588379, "val_acc": 52.0}
{"epoch": 27, "training_loss": 68.2295868396759, "training_acc": 58.0, "val_loss": 17.106305062770844, "val_acc": 52.0}
{"epoch": 28, "training_loss": 67.14701008796692, "training_acc": 55.0, "val_loss": 17.074117064476013, "val_acc": 52.0}
{"epoch": 29, "training_loss": 66.83572483062744, "training_acc": 54.0, "val_loss": 17.040227353572845, "val_acc": 52.0}
{"epoch": 30, "training_loss": 64.53018879890442, "training_acc": 81.0, "val_loss": 17.090164124965668, "val_acc": 52.0}
{"epoch": 31, "training_loss": 64.49320530891418, "training_acc": 66.0, "val_loss": 16.93859100341797, "val_acc": 56.0}
{"epoch": 32, "training_loss": 61.758243560791016, "training_acc": 72.0, "val_loss": 16.576842963695526, "val_acc": 56.0}
{"epoch": 33, "training_loss": 57.57606887817383, "training_acc": 84.0, "val_loss": 20.123353600502014, "val_acc": 52.0}
{"epoch": 34, "training_loss": 76.18509721755981, "training_acc": 50.0, "val_loss": 20.2492892742157, "val_acc": 52.0}
{"epoch": 35, "training_loss": 77.10630488395691, "training_acc": 53.0, "val_loss": 17.431342601776123, "val_acc": 52.0}
{"epoch": 36, "training_loss": 68.97641324996948, "training_acc": 53.0, "val_loss": 17.270401120185852, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.13006353378296, "training_acc": 57.0, "val_loss": 17.258071899414062, "val_acc": 52.0}
{"epoch": 38, "training_loss": 68.58063960075378, "training_acc": 53.0, "val_loss": 17.276886105537415, "val_acc": 52.0}
{"epoch": 39, "training_loss": 69.17128825187683, "training_acc": 57.0, "val_loss": 17.696233093738556, "val_acc": 52.0}
{"epoch": 40, "training_loss": 70.14377236366272, "training_acc": 53.0, "val_loss": 17.290112376213074, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.4485936164856, "training_acc": 60.0, "val_loss": 17.58706569671631, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.78635430335999, "training_acc": 53.0, "val_loss": 17.441919445991516, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.61545181274414, "training_acc": 53.0, "val_loss": 17.446410655975342, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.60200476646423, "training_acc": 53.0, "val_loss": 17.33344942331314, "val_acc": 52.0}
{"epoch": 45, "training_loss": 69.30470132827759, "training_acc": 51.0, "val_loss": 17.292648553848267, "val_acc": 52.0}
{"epoch": 46, "training_loss": 68.82647490501404, "training_acc": 55.0, "val_loss": 17.328795790672302, "val_acc": 52.0}
{"epoch": 47, "training_loss": 68.79694294929504, "training_acc": 53.0, "val_loss": 17.371413111686707, "val_acc": 52.0}
{"epoch": 48, "training_loss": 68.72900700569153, "training_acc": 53.0, "val_loss": 17.30712354183197, "val_acc": 52.0}
{"epoch": 49, "training_loss": 68.71738839149475, "training_acc": 53.0, "val_loss": 17.324015498161316, "val_acc": 52.0}
{"epoch": 50, "training_loss": 68.75640368461609, "training_acc": 59.0, "val_loss": 17.299634218215942, "val_acc": 52.0}
{"epoch": 51, "training_loss": 69.72459650039673, "training_acc": 59.0, "val_loss": 17.30867028236389, "val_acc": 52.0}
