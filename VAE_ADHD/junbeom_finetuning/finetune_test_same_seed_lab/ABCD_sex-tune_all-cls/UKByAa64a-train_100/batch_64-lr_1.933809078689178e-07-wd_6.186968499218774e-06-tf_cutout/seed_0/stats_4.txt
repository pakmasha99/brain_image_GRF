"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.39330673217773, "training_acc": 48.0, "val_loss": 17.320097982883453, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.43204617500305, "training_acc": 52.0, "val_loss": 17.319408059120178, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.31672978401184, "training_acc": 52.0, "val_loss": 17.318424582481384, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.17785000801086, "training_acc": 51.0, "val_loss": 17.317581176757812, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.10165333747864, "training_acc": 56.0, "val_loss": 17.31739342212677, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.11806225776672, "training_acc": 56.0, "val_loss": 17.316934466362, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.39272284507751, "training_acc": 49.0, "val_loss": 17.31690615415573, "val_acc": 52.0}
{"epoch": 7, "training_loss": 69.35714936256409, "training_acc": 51.0, "val_loss": 17.316633462905884, "val_acc": 52.0}
{"epoch": 8, "training_loss": 69.25974678993225, "training_acc": 57.0, "val_loss": 17.316539585590363, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.10897731781006, "training_acc": 52.0, "val_loss": 17.316463589668274, "val_acc": 52.0}
{"epoch": 10, "training_loss": 69.08682346343994, "training_acc": 54.0, "val_loss": 17.316140234470367, "val_acc": 52.0}
{"epoch": 11, "training_loss": 69.25148940086365, "training_acc": 54.0, "val_loss": 17.315994203090668, "val_acc": 52.0}
{"epoch": 12, "training_loss": 69.10816431045532, "training_acc": 53.0, "val_loss": 17.316000163555145, "val_acc": 52.0}
{"epoch": 13, "training_loss": 69.03242874145508, "training_acc": 54.0, "val_loss": 17.315909266471863, "val_acc": 52.0}
{"epoch": 14, "training_loss": 69.10451817512512, "training_acc": 55.0, "val_loss": 17.315693199634552, "val_acc": 52.0}
{"epoch": 15, "training_loss": 69.25760507583618, "training_acc": 52.0, "val_loss": 17.315489053726196, "val_acc": 52.0}
{"epoch": 16, "training_loss": 69.29127049446106, "training_acc": 52.0, "val_loss": 17.315247654914856, "val_acc": 52.0}
{"epoch": 17, "training_loss": 69.21279859542847, "training_acc": 52.0, "val_loss": 17.315253615379333, "val_acc": 52.0}
{"epoch": 18, "training_loss": 69.22729539871216, "training_acc": 53.0, "val_loss": 17.31516271829605, "val_acc": 52.0}
{"epoch": 19, "training_loss": 68.99841141700745, "training_acc": 53.0, "val_loss": 17.315159738063812, "val_acc": 52.0}
{"epoch": 20, "training_loss": 69.22007846832275, "training_acc": 50.0, "val_loss": 17.31507033109665, "val_acc": 52.0}
{"epoch": 21, "training_loss": 69.13538908958435, "training_acc": 53.0, "val_loss": 17.314906418323517, "val_acc": 52.0}
{"epoch": 22, "training_loss": 69.05674076080322, "training_acc": 53.0, "val_loss": 17.31487214565277, "val_acc": 52.0}
{"epoch": 23, "training_loss": 69.27677249908447, "training_acc": 54.0, "val_loss": 17.314794659614563, "val_acc": 52.0}
{"epoch": 24, "training_loss": 69.04362106323242, "training_acc": 53.0, "val_loss": 17.314478754997253, "val_acc": 52.0}
{"epoch": 25, "training_loss": 69.11809420585632, "training_acc": 54.0, "val_loss": 17.314189672470093, "val_acc": 52.0}
{"epoch": 26, "training_loss": 69.23498129844666, "training_acc": 50.0, "val_loss": 17.314055562019348, "val_acc": 52.0}
{"epoch": 27, "training_loss": 69.1090133190155, "training_acc": 53.0, "val_loss": 17.3140287399292, "val_acc": 52.0}
{"epoch": 28, "training_loss": 69.31129240989685, "training_acc": 53.0, "val_loss": 17.314310371875763, "val_acc": 52.0}
{"epoch": 29, "training_loss": 68.98392415046692, "training_acc": 54.0, "val_loss": 17.314285039901733, "val_acc": 52.0}
{"epoch": 30, "training_loss": 69.18907570838928, "training_acc": 53.0, "val_loss": 17.31477379798889, "val_acc": 52.0}
{"epoch": 31, "training_loss": 69.29152464866638, "training_acc": 52.0, "val_loss": 17.315033078193665, "val_acc": 52.0}
{"epoch": 32, "training_loss": 69.20740127563477, "training_acc": 52.0, "val_loss": 17.315292358398438, "val_acc": 52.0}
{"epoch": 33, "training_loss": 69.15676951408386, "training_acc": 53.0, "val_loss": 17.315465211868286, "val_acc": 52.0}
{"epoch": 34, "training_loss": 69.08512759208679, "training_acc": 52.0, "val_loss": 17.315571010112762, "val_acc": 52.0}
{"epoch": 35, "training_loss": 69.20899224281311, "training_acc": 53.0, "val_loss": 17.31541007757187, "val_acc": 52.0}
{"epoch": 36, "training_loss": 69.16312861442566, "training_acc": 53.0, "val_loss": 17.315438389778137, "val_acc": 52.0}
{"epoch": 37, "training_loss": 69.10979580879211, "training_acc": 53.0, "val_loss": 17.31553226709366, "val_acc": 52.0}
{"epoch": 38, "training_loss": 69.11269092559814, "training_acc": 53.0, "val_loss": 17.315512895584106, "val_acc": 52.0}
{"epoch": 39, "training_loss": 68.99076175689697, "training_acc": 54.0, "val_loss": 17.315658926963806, "val_acc": 52.0}
{"epoch": 40, "training_loss": 68.76997065544128, "training_acc": 53.0, "val_loss": 17.315547168254852, "val_acc": 52.0}
{"epoch": 41, "training_loss": 69.00030636787415, "training_acc": 53.0, "val_loss": 17.315512895584106, "val_acc": 52.0}
{"epoch": 42, "training_loss": 69.15286946296692, "training_acc": 53.0, "val_loss": 17.315462231636047, "val_acc": 52.0}
{"epoch": 43, "training_loss": 69.05767488479614, "training_acc": 53.0, "val_loss": 17.31555014848709, "val_acc": 52.0}
{"epoch": 44, "training_loss": 69.24117660522461, "training_acc": 53.0, "val_loss": 17.31579601764679, "val_acc": 52.0}
{"epoch": 45, "training_loss": 69.00223731994629, "training_acc": 53.0, "val_loss": 17.31586754322052, "val_acc": 52.0}
{"epoch": 46, "training_loss": 69.01915669441223, "training_acc": 53.0, "val_loss": 17.315886914730072, "val_acc": 52.0}
