"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.28762102127075, "training_acc": 53.0, "val_loss": 17.307208478450775, "val_acc": 56.0}
{"epoch": 1, "training_loss": 69.20497965812683, "training_acc": 52.0, "val_loss": 17.287059128284454, "val_acc": 56.0}
{"epoch": 2, "training_loss": 69.2055447101593, "training_acc": 52.0, "val_loss": 17.23897159099579, "val_acc": 56.0}
{"epoch": 3, "training_loss": 69.28939843177795, "training_acc": 52.0, "val_loss": 17.256812751293182, "val_acc": 56.0}
{"epoch": 4, "training_loss": 69.14972853660583, "training_acc": 52.0, "val_loss": 17.271289229393005, "val_acc": 56.0}
{"epoch": 5, "training_loss": 69.33142495155334, "training_acc": 52.0, "val_loss": 17.258411645889282, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.18148064613342, "training_acc": 51.0, "val_loss": 17.315369844436646, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.19285535812378, "training_acc": 56.0, "val_loss": 17.282044887542725, "val_acc": 56.0}
{"epoch": 8, "training_loss": 69.4733612537384, "training_acc": 52.0, "val_loss": 17.23261922597885, "val_acc": 56.0}
{"epoch": 9, "training_loss": 69.10269498825073, "training_acc": 52.0, "val_loss": 17.310301959514618, "val_acc": 56.0}
{"epoch": 10, "training_loss": 69.08068084716797, "training_acc": 53.0, "val_loss": 17.36140549182892, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.20166397094727, "training_acc": 52.0, "val_loss": 17.305633425712585, "val_acc": 56.0}
{"epoch": 12, "training_loss": 69.03981828689575, "training_acc": 52.0, "val_loss": 17.314523458480835, "val_acc": 56.0}
{"epoch": 13, "training_loss": 68.93390655517578, "training_acc": 54.0, "val_loss": 17.38358587026596, "val_acc": 56.0}
{"epoch": 14, "training_loss": 68.9299898147583, "training_acc": 53.0, "val_loss": 17.3210471868515, "val_acc": 56.0}
{"epoch": 15, "training_loss": 68.74922966957092, "training_acc": 52.0, "val_loss": 17.283950746059418, "val_acc": 56.0}
{"epoch": 16, "training_loss": 68.69386720657349, "training_acc": 52.0, "val_loss": 17.330077290534973, "val_acc": 56.0}
{"epoch": 17, "training_loss": 68.98011302947998, "training_acc": 53.0, "val_loss": 17.499980330467224, "val_acc": 56.0}
{"epoch": 18, "training_loss": 68.91829371452332, "training_acc": 53.0, "val_loss": 17.423664033412933, "val_acc": 56.0}
{"epoch": 19, "training_loss": 68.79828643798828, "training_acc": 57.0, "val_loss": 17.340493202209473, "val_acc": 56.0}
{"epoch": 20, "training_loss": 68.87733435630798, "training_acc": 53.0, "val_loss": 17.430749535560608, "val_acc": 56.0}
{"epoch": 21, "training_loss": 68.4856493473053, "training_acc": 58.0, "val_loss": 17.409925162792206, "val_acc": 56.0}
{"epoch": 22, "training_loss": 69.21885418891907, "training_acc": 53.0, "val_loss": 17.330892384052277, "val_acc": 56.0}
{"epoch": 23, "training_loss": 68.59678816795349, "training_acc": 54.0, "val_loss": 17.584559321403503, "val_acc": 56.0}
{"epoch": 24, "training_loss": 68.75424885749817, "training_acc": 56.0, "val_loss": 17.707648873329163, "val_acc": 56.0}
{"epoch": 25, "training_loss": 68.89580607414246, "training_acc": 55.0, "val_loss": 17.44302362203598, "val_acc": 56.0}
{"epoch": 26, "training_loss": 69.20517110824585, "training_acc": 57.0, "val_loss": 17.281147837638855, "val_acc": 56.0}
{"epoch": 27, "training_loss": 69.12801337242126, "training_acc": 52.0, "val_loss": 17.39923655986786, "val_acc": 56.0}
