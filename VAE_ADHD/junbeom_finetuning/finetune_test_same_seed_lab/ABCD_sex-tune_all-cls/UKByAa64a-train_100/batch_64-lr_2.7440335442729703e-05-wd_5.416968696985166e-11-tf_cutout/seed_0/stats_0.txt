"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.27427005767822, "training_acc": 52.0, "val_loss": 17.303302884101868, "val_acc": 56.0}
{"epoch": 1, "training_loss": 69.11016702651978, "training_acc": 52.0, "val_loss": 17.313767969608307, "val_acc": 56.0}
{"epoch": 2, "training_loss": 69.1514892578125, "training_acc": 52.0, "val_loss": 17.25778877735138, "val_acc": 56.0}
{"epoch": 3, "training_loss": 69.17889714241028, "training_acc": 52.0, "val_loss": 17.26898103952408, "val_acc": 56.0}
{"epoch": 4, "training_loss": 69.12793779373169, "training_acc": 52.0, "val_loss": 17.269372940063477, "val_acc": 56.0}
{"epoch": 5, "training_loss": 69.0878803730011, "training_acc": 52.0, "val_loss": 17.273910343647003, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.01217579841614, "training_acc": 52.0, "val_loss": 17.32947826385498, "val_acc": 56.0}
{"epoch": 7, "training_loss": 68.92240834236145, "training_acc": 59.0, "val_loss": 17.30121225118637, "val_acc": 56.0}
{"epoch": 8, "training_loss": 69.15295147895813, "training_acc": 51.0, "val_loss": 17.24969446659088, "val_acc": 56.0}
{"epoch": 9, "training_loss": 68.8681788444519, "training_acc": 52.0, "val_loss": 17.275996506214142, "val_acc": 56.0}
{"epoch": 10, "training_loss": 68.76264500617981, "training_acc": 55.0, "val_loss": 17.306286096572876, "val_acc": 56.0}
{"epoch": 11, "training_loss": 68.86639904975891, "training_acc": 56.0, "val_loss": 17.269474267959595, "val_acc": 56.0}
{"epoch": 12, "training_loss": 68.6802294254303, "training_acc": 53.0, "val_loss": 17.273032665252686, "val_acc": 56.0}
{"epoch": 13, "training_loss": 68.48371911048889, "training_acc": 56.0, "val_loss": 17.30063557624817, "val_acc": 56.0}
{"epoch": 14, "training_loss": 68.36875081062317, "training_acc": 53.0, "val_loss": 17.28271394968033, "val_acc": 56.0}
{"epoch": 15, "training_loss": 68.30027079582214, "training_acc": 55.0, "val_loss": 17.269083857536316, "val_acc": 56.0}
{"epoch": 16, "training_loss": 67.91765189170837, "training_acc": 53.0, "val_loss": 17.248249053955078, "val_acc": 56.0}
{"epoch": 17, "training_loss": 68.28324294090271, "training_acc": 54.0, "val_loss": 17.21450537443161, "val_acc": 56.0}
{"epoch": 18, "training_loss": 67.64312529563904, "training_acc": 59.0, "val_loss": 17.176446318626404, "val_acc": 56.0}
{"epoch": 19, "training_loss": 67.66766214370728, "training_acc": 52.0, "val_loss": 17.478808760643005, "val_acc": 56.0}
{"epoch": 20, "training_loss": 68.65302991867065, "training_acc": 57.0, "val_loss": 17.43912845849991, "val_acc": 56.0}
{"epoch": 21, "training_loss": 67.48703861236572, "training_acc": 67.0, "val_loss": 17.260032892227173, "val_acc": 56.0}
{"epoch": 22, "training_loss": 69.34865808486938, "training_acc": 52.0, "val_loss": 17.175503075122833, "val_acc": 56.0}
{"epoch": 23, "training_loss": 69.07883501052856, "training_acc": 52.0, "val_loss": 17.220081388950348, "val_acc": 56.0}
{"epoch": 24, "training_loss": 69.08031892776489, "training_acc": 52.0, "val_loss": 17.273612320423126, "val_acc": 56.0}
{"epoch": 25, "training_loss": 69.08926606178284, "training_acc": 52.0, "val_loss": 17.306137084960938, "val_acc": 56.0}
{"epoch": 26, "training_loss": 69.05530071258545, "training_acc": 56.0, "val_loss": 17.334987223148346, "val_acc": 56.0}
{"epoch": 27, "training_loss": 68.94117975234985, "training_acc": 58.0, "val_loss": 17.407400906085968, "val_acc": 56.0}
{"epoch": 28, "training_loss": 68.86452913284302, "training_acc": 53.0, "val_loss": 17.36021637916565, "val_acc": 56.0}
{"epoch": 29, "training_loss": 68.64247059822083, "training_acc": 66.0, "val_loss": 17.247295379638672, "val_acc": 56.0}
{"epoch": 30, "training_loss": 69.12597870826721, "training_acc": 52.0, "val_loss": 17.201092839241028, "val_acc": 56.0}
{"epoch": 31, "training_loss": 69.06152868270874, "training_acc": 52.0, "val_loss": 17.223237454891205, "val_acc": 56.0}
{"epoch": 32, "training_loss": 68.72844672203064, "training_acc": 52.0, "val_loss": 17.25260466337204, "val_acc": 56.0}
{"epoch": 33, "training_loss": 68.42608666419983, "training_acc": 56.0, "val_loss": 17.34790951013565, "val_acc": 56.0}
{"epoch": 34, "training_loss": 68.20446944236755, "training_acc": 66.0, "val_loss": 17.394591867923737, "val_acc": 56.0}
{"epoch": 35, "training_loss": 68.18692469596863, "training_acc": 65.0, "val_loss": 17.323242127895355, "val_acc": 56.0}
{"epoch": 36, "training_loss": 67.41278386116028, "training_acc": 63.0, "val_loss": 17.278657853603363, "val_acc": 56.0}
{"epoch": 37, "training_loss": 67.802325963974, "training_acc": 58.0, "val_loss": 17.261001467704773, "val_acc": 56.0}
{"epoch": 38, "training_loss": 67.34141802787781, "training_acc": 58.0, "val_loss": 17.246977984905243, "val_acc": 56.0}
{"epoch": 39, "training_loss": 66.75598907470703, "training_acc": 57.0, "val_loss": 17.43333339691162, "val_acc": 56.0}
{"epoch": 40, "training_loss": 67.4070074558258, "training_acc": 71.0, "val_loss": 17.350830137729645, "val_acc": 56.0}
{"epoch": 41, "training_loss": 66.5073013305664, "training_acc": 63.0, "val_loss": 17.25865751504898, "val_acc": 56.0}
