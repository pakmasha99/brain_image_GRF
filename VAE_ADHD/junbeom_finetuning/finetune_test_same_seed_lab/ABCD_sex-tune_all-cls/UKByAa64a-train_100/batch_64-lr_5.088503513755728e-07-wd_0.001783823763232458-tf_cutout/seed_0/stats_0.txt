"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.39285373687744, "training_acc": 52.0, "val_loss": 17.274563014507294, "val_acc": 56.0}
{"epoch": 1, "training_loss": 69.21064376831055, "training_acc": 52.0, "val_loss": 17.277593910694122, "val_acc": 56.0}
{"epoch": 2, "training_loss": 69.19450306892395, "training_acc": 52.0, "val_loss": 17.277534306049347, "val_acc": 56.0}
{"epoch": 3, "training_loss": 69.18289518356323, "training_acc": 52.0, "val_loss": 17.27508306503296, "val_acc": 56.0}
{"epoch": 4, "training_loss": 69.34264016151428, "training_acc": 52.0, "val_loss": 17.271658778190613, "val_acc": 56.0}
{"epoch": 5, "training_loss": 69.25385975837708, "training_acc": 52.0, "val_loss": 17.27052330970764, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.15342402458191, "training_acc": 52.0, "val_loss": 17.272309958934784, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.30975079536438, "training_acc": 52.0, "val_loss": 17.270497977733612, "val_acc": 56.0}
{"epoch": 8, "training_loss": 69.20131468772888, "training_acc": 52.0, "val_loss": 17.270274460315704, "val_acc": 56.0}
{"epoch": 9, "training_loss": 69.21727585792542, "training_acc": 52.0, "val_loss": 17.271587252616882, "val_acc": 56.0}
{"epoch": 10, "training_loss": 69.12057590484619, "training_acc": 52.0, "val_loss": 17.270247638225555, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.14138603210449, "training_acc": 52.0, "val_loss": 17.26771891117096, "val_acc": 56.0}
{"epoch": 12, "training_loss": 69.0060830116272, "training_acc": 52.0, "val_loss": 17.264865338802338, "val_acc": 56.0}
{"epoch": 13, "training_loss": 69.08270573616028, "training_acc": 52.0, "val_loss": 17.26243942975998, "val_acc": 56.0}
{"epoch": 14, "training_loss": 69.13832926750183, "training_acc": 52.0, "val_loss": 17.260649800300598, "val_acc": 56.0}
{"epoch": 15, "training_loss": 69.16405153274536, "training_acc": 52.0, "val_loss": 17.258615791797638, "val_acc": 56.0}
{"epoch": 16, "training_loss": 69.14328813552856, "training_acc": 52.0, "val_loss": 17.256568372249603, "val_acc": 56.0}
{"epoch": 17, "training_loss": 69.03476142883301, "training_acc": 52.0, "val_loss": 17.25548952817917, "val_acc": 56.0}
{"epoch": 18, "training_loss": 69.0115795135498, "training_acc": 52.0, "val_loss": 17.25342571735382, "val_acc": 56.0}
{"epoch": 19, "training_loss": 69.04434752464294, "training_acc": 52.0, "val_loss": 17.24986582994461, "val_acc": 56.0}
{"epoch": 20, "training_loss": 69.00294518470764, "training_acc": 52.0, "val_loss": 17.248085141181946, "val_acc": 56.0}
{"epoch": 21, "training_loss": 68.9960663318634, "training_acc": 52.0, "val_loss": 17.24674552679062, "val_acc": 56.0}
{"epoch": 22, "training_loss": 69.07210230827332, "training_acc": 52.0, "val_loss": 17.245449125766754, "val_acc": 56.0}
{"epoch": 23, "training_loss": 69.1127622127533, "training_acc": 52.0, "val_loss": 17.246213555336, "val_acc": 56.0}
{"epoch": 24, "training_loss": 69.07876229286194, "training_acc": 52.0, "val_loss": 17.247237265110016, "val_acc": 56.0}
{"epoch": 25, "training_loss": 69.02416968345642, "training_acc": 52.0, "val_loss": 17.24744886159897, "val_acc": 56.0}
{"epoch": 26, "training_loss": 68.8397490978241, "training_acc": 52.0, "val_loss": 17.246733605861664, "val_acc": 56.0}
{"epoch": 27, "training_loss": 69.01445841789246, "training_acc": 52.0, "val_loss": 17.24551171064377, "val_acc": 56.0}
{"epoch": 28, "training_loss": 69.02077436447144, "training_acc": 52.0, "val_loss": 17.24461019039154, "val_acc": 56.0}
{"epoch": 29, "training_loss": 68.94111180305481, "training_acc": 52.0, "val_loss": 17.243175208568573, "val_acc": 56.0}
{"epoch": 30, "training_loss": 68.94986915588379, "training_acc": 52.0, "val_loss": 17.241977155208588, "val_acc": 56.0}
{"epoch": 31, "training_loss": 68.95565056800842, "training_acc": 52.0, "val_loss": 17.242948710918427, "val_acc": 56.0}
{"epoch": 32, "training_loss": 68.9638147354126, "training_acc": 52.0, "val_loss": 17.24323183298111, "val_acc": 56.0}
{"epoch": 33, "training_loss": 68.74246406555176, "training_acc": 52.0, "val_loss": 17.24298596382141, "val_acc": 56.0}
{"epoch": 34, "training_loss": 69.02806067466736, "training_acc": 52.0, "val_loss": 17.24333018064499, "val_acc": 56.0}
{"epoch": 35, "training_loss": 68.92096710205078, "training_acc": 52.0, "val_loss": 17.246708273887634, "val_acc": 56.0}
{"epoch": 36, "training_loss": 68.82683539390564, "training_acc": 52.0, "val_loss": 17.250049114227295, "val_acc": 56.0}
{"epoch": 37, "training_loss": 68.85630345344543, "training_acc": 52.0, "val_loss": 17.251643538475037, "val_acc": 56.0}
{"epoch": 38, "training_loss": 68.85885310173035, "training_acc": 52.0, "val_loss": 17.253214120864868, "val_acc": 56.0}
{"epoch": 39, "training_loss": 69.00791501998901, "training_acc": 52.0, "val_loss": 17.253868281841278, "val_acc": 56.0}
{"epoch": 40, "training_loss": 68.92686796188354, "training_acc": 52.0, "val_loss": 17.255526781082153, "val_acc": 56.0}
{"epoch": 41, "training_loss": 68.92212414741516, "training_acc": 52.0, "val_loss": 17.258617281913757, "val_acc": 56.0}
{"epoch": 42, "training_loss": 68.87323760986328, "training_acc": 52.0, "val_loss": 17.2579824924469, "val_acc": 56.0}
{"epoch": 43, "training_loss": 68.86569499969482, "training_acc": 52.0, "val_loss": 17.255397140979767, "val_acc": 56.0}
{"epoch": 44, "training_loss": 68.8529622554779, "training_acc": 52.0, "val_loss": 17.253924906253815, "val_acc": 56.0}
{"epoch": 45, "training_loss": 68.91820025444031, "training_acc": 52.0, "val_loss": 17.25398600101471, "val_acc": 56.0}
{"epoch": 46, "training_loss": 68.89100694656372, "training_acc": 52.0, "val_loss": 17.254099249839783, "val_acc": 56.0}
{"epoch": 47, "training_loss": 68.89243412017822, "training_acc": 52.0, "val_loss": 17.252618074417114, "val_acc": 56.0}
{"epoch": 48, "training_loss": 68.89728856086731, "training_acc": 52.0, "val_loss": 17.25132316350937, "val_acc": 56.0}
{"epoch": 49, "training_loss": 68.85502243041992, "training_acc": 52.0, "val_loss": 17.249426245689392, "val_acc": 56.0}
