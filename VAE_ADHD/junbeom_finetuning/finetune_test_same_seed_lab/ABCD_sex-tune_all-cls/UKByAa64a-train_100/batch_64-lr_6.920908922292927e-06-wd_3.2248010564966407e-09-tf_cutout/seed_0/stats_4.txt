"main_optuna_fix.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 100 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_test_same_seed_lab --binary_class True --run_where lab"
{"epoch": 0, "training_loss": 69.18680953979492, "training_acc": 53.0, "val_loss": 17.38056391477585, "val_acc": 52.0}
{"epoch": 1, "training_loss": 69.17578864097595, "training_acc": 53.0, "val_loss": 17.355293035507202, "val_acc": 52.0}
{"epoch": 2, "training_loss": 69.21776175498962, "training_acc": 53.0, "val_loss": 17.339785397052765, "val_acc": 52.0}
{"epoch": 3, "training_loss": 69.12605333328247, "training_acc": 53.0, "val_loss": 17.30930358171463, "val_acc": 52.0}
{"epoch": 4, "training_loss": 69.1877110004425, "training_acc": 53.0, "val_loss": 17.28740483522415, "val_acc": 52.0}
{"epoch": 5, "training_loss": 69.12851357460022, "training_acc": 53.0, "val_loss": 17.320826649665833, "val_acc": 52.0}
{"epoch": 6, "training_loss": 69.02137398719788, "training_acc": 53.0, "val_loss": 17.309345304965973, "val_acc": 52.0}
{"epoch": 7, "training_loss": 68.75431060791016, "training_acc": 53.0, "val_loss": 17.3126220703125, "val_acc": 52.0}
{"epoch": 8, "training_loss": 68.86274075508118, "training_acc": 53.0, "val_loss": 17.306247353553772, "val_acc": 52.0}
{"epoch": 9, "training_loss": 69.03049397468567, "training_acc": 53.0, "val_loss": 17.277663946151733, "val_acc": 52.0}
{"epoch": 10, "training_loss": 68.70604038238525, "training_acc": 53.0, "val_loss": 17.260824143886566, "val_acc": 52.0}
{"epoch": 11, "training_loss": 68.85714054107666, "training_acc": 53.0, "val_loss": 17.250649631023407, "val_acc": 52.0}
{"epoch": 12, "training_loss": 68.64256238937378, "training_acc": 53.0, "val_loss": 17.262008786201477, "val_acc": 52.0}
{"epoch": 13, "training_loss": 68.67685627937317, "training_acc": 53.0, "val_loss": 17.25677400827408, "val_acc": 52.0}
{"epoch": 14, "training_loss": 68.56590986251831, "training_acc": 53.0, "val_loss": 17.25238710641861, "val_acc": 52.0}
{"epoch": 15, "training_loss": 68.48818302154541, "training_acc": 53.0, "val_loss": 17.256249487400055, "val_acc": 52.0}
{"epoch": 16, "training_loss": 68.71128511428833, "training_acc": 53.0, "val_loss": 17.244015634059906, "val_acc": 52.0}
{"epoch": 17, "training_loss": 68.59793162345886, "training_acc": 53.0, "val_loss": 17.259857058525085, "val_acc": 52.0}
{"epoch": 18, "training_loss": 68.49887013435364, "training_acc": 53.0, "val_loss": 17.246703803539276, "val_acc": 52.0}
{"epoch": 19, "training_loss": 68.81281900405884, "training_acc": 53.0, "val_loss": 17.25715398788452, "val_acc": 52.0}
{"epoch": 20, "training_loss": 68.36669540405273, "training_acc": 53.0, "val_loss": 17.23318248987198, "val_acc": 52.0}
{"epoch": 21, "training_loss": 68.49281740188599, "training_acc": 53.0, "val_loss": 17.23649799823761, "val_acc": 52.0}
{"epoch": 22, "training_loss": 68.40418601036072, "training_acc": 53.0, "val_loss": 17.242923378944397, "val_acc": 52.0}
{"epoch": 23, "training_loss": 68.54054403305054, "training_acc": 53.0, "val_loss": 17.258140444755554, "val_acc": 52.0}
{"epoch": 24, "training_loss": 68.40128660202026, "training_acc": 53.0, "val_loss": 17.269136011600494, "val_acc": 52.0}
{"epoch": 25, "training_loss": 68.28332114219666, "training_acc": 53.0, "val_loss": 17.260128259658813, "val_acc": 52.0}
{"epoch": 26, "training_loss": 67.94608354568481, "training_acc": 53.0, "val_loss": 17.216499149799347, "val_acc": 52.0}
{"epoch": 27, "training_loss": 68.17343878746033, "training_acc": 53.0, "val_loss": 17.2210231423378, "val_acc": 52.0}
{"epoch": 28, "training_loss": 68.08124876022339, "training_acc": 53.0, "val_loss": 17.187561094760895, "val_acc": 52.0}
{"epoch": 29, "training_loss": 67.63735294342041, "training_acc": 53.0, "val_loss": 17.243777215480804, "val_acc": 52.0}
{"epoch": 30, "training_loss": 68.19426774978638, "training_acc": 54.0, "val_loss": 17.295612394809723, "val_acc": 52.0}
{"epoch": 31, "training_loss": 68.17104077339172, "training_acc": 54.0, "val_loss": 17.237986624240875, "val_acc": 52.0}
{"epoch": 32, "training_loss": 67.99110078811646, "training_acc": 53.0, "val_loss": 17.20663756132126, "val_acc": 52.0}
{"epoch": 33, "training_loss": 67.8698992729187, "training_acc": 53.0, "val_loss": 17.150872945785522, "val_acc": 52.0}
{"epoch": 34, "training_loss": 67.51681518554688, "training_acc": 56.0, "val_loss": 17.250452935695648, "val_acc": 52.0}
{"epoch": 35, "training_loss": 67.89121866226196, "training_acc": 67.0, "val_loss": 17.25601553916931, "val_acc": 52.0}
{"epoch": 36, "training_loss": 67.8526337146759, "training_acc": 54.0, "val_loss": 17.239263653755188, "val_acc": 52.0}
{"epoch": 37, "training_loss": 67.69408369064331, "training_acc": 53.0, "val_loss": 17.25357621908188, "val_acc": 52.0}
{"epoch": 38, "training_loss": 67.27860021591187, "training_acc": 53.0, "val_loss": 17.210184037685394, "val_acc": 52.0}
{"epoch": 39, "training_loss": 67.37734866142273, "training_acc": 57.0, "val_loss": 17.21976399421692, "val_acc": 52.0}
{"epoch": 40, "training_loss": 67.06896448135376, "training_acc": 63.0, "val_loss": 17.17125028371811, "val_acc": 52.0}
{"epoch": 41, "training_loss": 67.7318856716156, "training_acc": 55.0, "val_loss": 17.158129811286926, "val_acc": 52.0}
{"epoch": 42, "training_loss": 67.39455151557922, "training_acc": 53.0, "val_loss": 17.18299835920334, "val_acc": 52.0}
