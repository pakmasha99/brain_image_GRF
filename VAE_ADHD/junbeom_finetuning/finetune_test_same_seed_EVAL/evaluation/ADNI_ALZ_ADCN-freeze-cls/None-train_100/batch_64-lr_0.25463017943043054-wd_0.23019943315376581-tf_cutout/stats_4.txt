"main_optuna_fix_2.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ADNI_ALZ_ADCN --input_option yAware --save_path finetune_test_same_seed_EVAL --binary_class True --batch_size 64 --eval_mode True --learning_rate 0.25463017943043054 --weight_decay 0.23019943315376581 --BN inst --foreach False"
{"epoch": 0, "training_loss": 1026.438331604004, "training_acc": 72.0, "val_loss": 503.7435054779053, "val_acc": 72.0}
{"epoch": 1, "training_loss": 1260.1829299926758, "training_acc": 72.0, "val_loss": 1372.7805137634277, "val_acc": 28.0}
{"epoch": 2, "training_loss": 5066.132110595703, "training_acc": 28.0, "val_loss": 214.69154357910156, "val_acc": 28.0}
{"epoch": 3, "training_loss": 1067.8363723754883, "training_acc": 44.0, "val_loss": 647.0367431640625, "val_acc": 72.0}
{"epoch": 4, "training_loss": 2806.1868743896484, "training_acc": 72.0, "val_loss": 847.7805137634277, "val_acc": 72.0}
{"epoch": 5, "training_loss": 3338.9322814941406, "training_acc": 72.0, "val_loss": 704.6725749969482, "val_acc": 72.0}
{"epoch": 6, "training_loss": 2629.683364868164, "training_acc": 72.0, "val_loss": 342.7909851074219, "val_acc": 72.0}
{"epoch": 7, "training_loss": 1096.3024082183838, "training_acc": 72.0, "val_loss": 396.6679096221924, "val_acc": 28.0}
{"epoch": 8, "training_loss": 1721.704818725586, "training_acc": 28.0, "val_loss": 203.49767208099365, "val_acc": 28.0}
{"epoch": 9, "training_loss": 806.7151775360107, "training_acc": 40.0, "val_loss": 286.58649921417236, "val_acc": 72.0}
{"epoch": 10, "training_loss": 1285.830795288086, "training_acc": 72.0, "val_loss": 338.98260593414307, "val_acc": 72.0}
{"epoch": 11, "training_loss": 1245.829792022705, "training_acc": 72.0, "val_loss": 155.32891750335693, "val_acc": 72.0}
{"epoch": 12, "training_loss": 425.1140480041504, "training_acc": 56.0, "val_loss": 220.98677158355713, "val_acc": 28.0}
{"epoch": 13, "training_loss": 648.8635859489441, "training_acc": 28.0, "val_loss": 149.62944984436035, "val_acc": 72.0}
{"epoch": 14, "training_loss": 713.7220916748047, "training_acc": 72.0, "val_loss": 269.04733180999756, "val_acc": 72.0}
{"epoch": 15, "training_loss": 1037.598720550537, "training_acc": 72.0, "val_loss": 166.20086431503296, "val_acc": 72.0}
{"epoch": 16, "training_loss": 507.6349549293518, "training_acc": 72.0, "val_loss": 273.5387325286865, "val_acc": 28.0}
{"epoch": 17, "training_loss": 1070.989631652832, "training_acc": 28.0, "val_loss": 32.54745006561279, "val_acc": 72.0}
{"epoch": 18, "training_loss": 236.12626457214355, "training_acc": 72.0, "val_loss": 102.88105010986328, "val_acc": 72.0}
{"epoch": 19, "training_loss": 342.50349140167236, "training_acc": 72.0, "val_loss": 94.02430653572083, "val_acc": 28.0}
{"epoch": 20, "training_loss": 264.35587644577026, "training_acc": 44.0, "val_loss": 30.034160614013672, "val_acc": 72.0}
{"epoch": 21, "training_loss": 87.84000372886658, "training_acc": 72.0, "val_loss": 51.185643672943115, "val_acc": 28.0}
{"epoch": 22, "training_loss": 225.1276731491089, "training_acc": 46.0, "val_loss": 111.06278896331787, "val_acc": 72.0}
{"epoch": 23, "training_loss": 414.4728422164917, "training_acc": 72.0, "val_loss": 22.88019508123398, "val_acc": 72.0}
{"epoch": 24, "training_loss": 381.7812919616699, "training_acc": 54.0, "val_loss": 91.93024039268494, "val_acc": 28.0}
{"epoch": 25, "training_loss": 532.402063369751, "training_acc": 36.0, "val_loss": 217.6990032196045, "val_acc": 72.0}
{"epoch": 26, "training_loss": 901.9293098449707, "training_acc": 72.0, "val_loss": 199.11494255065918, "val_acc": 72.0}
{"epoch": 27, "training_loss": 685.3595600128174, "training_acc": 72.0, "val_loss": 23.761527240276337, "val_acc": 28.0}
{"epoch": 28, "training_loss": 182.15523433685303, "training_acc": 28.0, "val_loss": 70.10209560394287, "val_acc": 72.0}
{"epoch": 29, "training_loss": 365.57147216796875, "training_acc": 72.0, "val_loss": 90.44996500015259, "val_acc": 72.0}
{"epoch": 30, "training_loss": 268.9768350124359, "training_acc": 72.0, "val_loss": 194.562566280365, "val_acc": 28.0}
{"epoch": 31, "training_loss": 587.8853693008423, "training_acc": 28.0, "val_loss": 130.10542392730713, "val_acc": 72.0}
{"epoch": 32, "training_loss": 642.424259185791, "training_acc": 72.0, "val_loss": 218.84570121765137, "val_acc": 72.0}
{"epoch": 33, "training_loss": 818.230770111084, "training_acc": 72.0, "val_loss": 91.13958477973938, "val_acc": 72.0}
{"epoch": 34, "training_loss": 347.5529680252075, "training_acc": 56.0, "val_loss": 25.33462941646576, "val_acc": 28.0}
{"epoch": 35, "training_loss": 233.17092323303223, "training_acc": 40.0, "val_loss": 125.69915056228638, "val_acc": 72.0}
{"epoch": 36, "training_loss": 466.59611320495605, "training_acc": 72.0, "val_loss": 27.48657464981079, "val_acc": 72.0}
{"epoch": 37, "training_loss": 331.15813064575195, "training_acc": 58.0, "val_loss": 90.9688413143158, "val_acc": 28.0}
{"epoch": 38, "training_loss": 546.5431480407715, "training_acc": 34.0, "val_loss": 205.62562942504883, "val_acc": 72.0}
{"epoch": 39, "training_loss": 827.6214752197266, "training_acc": 72.0, "val_loss": 173.52850437164307, "val_acc": 72.0}
{"epoch": 40, "training_loss": 585.5578298568726, "training_acc": 72.0, "val_loss": 70.6003725528717, "val_acc": 28.0}
{"epoch": 41, "training_loss": 208.65182065963745, "training_acc": 28.0, "val_loss": 72.70466685295105, "val_acc": 72.0}
{"epoch": 42, "training_loss": 293.2184615135193, "training_acc": 72.0, "val_loss": 48.030829429626465, "val_acc": 72.0}
