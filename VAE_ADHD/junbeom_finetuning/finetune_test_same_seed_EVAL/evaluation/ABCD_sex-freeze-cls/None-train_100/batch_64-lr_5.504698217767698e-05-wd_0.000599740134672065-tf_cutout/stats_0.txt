"main_optuna_fix_2.py --pretrained_path None --mode finetuning --train_num 100 --layer_control freeze --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --save_path finetune_test_same_seed_EVAL --binary_class True --batch_size 64 --eval_mode True --learning_rate 5.504698217767698e-05 --weight_decay 0.000599740134672065 --BN inst"
{"epoch": 0, "training_loss": 69.24061942100525, "training_acc": 52.0, "val_loss": 17.238150537014008, "val_acc": 56.0}
{"epoch": 1, "training_loss": 69.26479721069336, "training_acc": 52.0, "val_loss": 17.22390651702881, "val_acc": 56.0}
{"epoch": 2, "training_loss": 69.25646185874939, "training_acc": 52.0, "val_loss": 17.222346365451813, "val_acc": 56.0}
{"epoch": 3, "training_loss": 69.26688861846924, "training_acc": 52.0, "val_loss": 17.229758203029633, "val_acc": 56.0}
{"epoch": 4, "training_loss": 69.25478911399841, "training_acc": 52.0, "val_loss": 17.24199801683426, "val_acc": 56.0}
{"epoch": 5, "training_loss": 69.24209308624268, "training_acc": 52.0, "val_loss": 17.246532440185547, "val_acc": 56.0}
{"epoch": 6, "training_loss": 69.24085760116577, "training_acc": 52.0, "val_loss": 17.256280779838562, "val_acc": 56.0}
{"epoch": 7, "training_loss": 69.25525450706482, "training_acc": 52.0, "val_loss": 17.261241376399994, "val_acc": 56.0}
{"epoch": 8, "training_loss": 69.25658917427063, "training_acc": 52.0, "val_loss": 17.257876694202423, "val_acc": 56.0}
{"epoch": 9, "training_loss": 69.22990012168884, "training_acc": 52.0, "val_loss": 17.26130247116089, "val_acc": 56.0}
{"epoch": 10, "training_loss": 69.24890637397766, "training_acc": 52.0, "val_loss": 17.263391613960266, "val_acc": 56.0}
{"epoch": 11, "training_loss": 69.24167609214783, "training_acc": 52.0, "val_loss": 17.261981964111328, "val_acc": 56.0}
{"epoch": 12, "training_loss": 69.24746680259705, "training_acc": 52.0, "val_loss": 17.25974977016449, "val_acc": 56.0}
{"epoch": 13, "training_loss": 69.25189805030823, "training_acc": 52.0, "val_loss": 17.259536683559418, "val_acc": 56.0}
{"epoch": 14, "training_loss": 69.24663090705872, "training_acc": 52.0, "val_loss": 17.254824936389923, "val_acc": 56.0}
{"epoch": 15, "training_loss": 69.2381021976471, "training_acc": 52.0, "val_loss": 17.247454822063446, "val_acc": 56.0}
{"epoch": 16, "training_loss": 69.23626947402954, "training_acc": 52.0, "val_loss": 17.236328125, "val_acc": 56.0}
{"epoch": 17, "training_loss": 69.23632001876831, "training_acc": 52.0, "val_loss": 17.228394746780396, "val_acc": 56.0}
{"epoch": 18, "training_loss": 69.22023272514343, "training_acc": 52.0, "val_loss": 17.221689224243164, "val_acc": 56.0}
{"epoch": 19, "training_loss": 69.24177289009094, "training_acc": 52.0, "val_loss": 17.21630096435547, "val_acc": 56.0}
{"epoch": 20, "training_loss": 69.25096154212952, "training_acc": 52.0, "val_loss": 17.214156687259674, "val_acc": 56.0}
{"epoch": 21, "training_loss": 69.22168731689453, "training_acc": 52.0, "val_loss": 17.211662232875824, "val_acc": 56.0}
{"epoch": 22, "training_loss": 69.27047634124756, "training_acc": 52.0, "val_loss": 17.206983268260956, "val_acc": 56.0}
{"epoch": 23, "training_loss": 69.24638104438782, "training_acc": 52.0, "val_loss": 17.20835417509079, "val_acc": 56.0}
{"epoch": 24, "training_loss": 69.23222064971924, "training_acc": 52.0, "val_loss": 17.21300035715103, "val_acc": 56.0}
{"epoch": 25, "training_loss": 69.25546717643738, "training_acc": 52.0, "val_loss": 17.22065359354019, "val_acc": 56.0}
{"epoch": 26, "training_loss": 69.24269700050354, "training_acc": 52.0, "val_loss": 17.223885655403137, "val_acc": 56.0}
{"epoch": 27, "training_loss": 69.23704195022583, "training_acc": 52.0, "val_loss": 17.235098779201508, "val_acc": 56.0}
{"epoch": 28, "training_loss": 69.24389958381653, "training_acc": 52.0, "val_loss": 17.245735228061676, "val_acc": 56.0}
{"epoch": 29, "training_loss": 69.2491807937622, "training_acc": 52.0, "val_loss": 17.250418663024902, "val_acc": 56.0}
{"epoch": 30, "training_loss": 69.2504198551178, "training_acc": 52.0, "val_loss": 17.25035309791565, "val_acc": 56.0}
{"epoch": 31, "training_loss": 69.26765847206116, "training_acc": 52.0, "val_loss": 17.25669503211975, "val_acc": 56.0}
{"epoch": 32, "training_loss": 69.24251055717468, "training_acc": 52.0, "val_loss": 17.254191637039185, "val_acc": 56.0}
{"epoch": 33, "training_loss": 69.22524213790894, "training_acc": 52.0, "val_loss": 17.252051830291748, "val_acc": 56.0}
{"epoch": 34, "training_loss": 69.23991751670837, "training_acc": 52.0, "val_loss": 17.24725365638733, "val_acc": 56.0}
{"epoch": 35, "training_loss": 69.24305653572083, "training_acc": 52.0, "val_loss": 17.244897782802582, "val_acc": 56.0}
{"epoch": 36, "training_loss": 69.25854444503784, "training_acc": 52.0, "val_loss": 17.241941392421722, "val_acc": 56.0}
{"epoch": 37, "training_loss": 69.2447464466095, "training_acc": 52.0, "val_loss": 17.237842082977295, "val_acc": 56.0}
{"epoch": 38, "training_loss": 69.22680616378784, "training_acc": 52.0, "val_loss": 17.22949743270874, "val_acc": 56.0}
{"epoch": 39, "training_loss": 69.25824928283691, "training_acc": 52.0, "val_loss": 17.218098044395447, "val_acc": 56.0}
{"epoch": 40, "training_loss": 69.23971915245056, "training_acc": 52.0, "val_loss": 17.212747037410736, "val_acc": 56.0}
{"epoch": 41, "training_loss": 69.24707913398743, "training_acc": 52.0, "val_loss": 17.208896577358246, "val_acc": 56.0}
{"epoch": 42, "training_loss": 69.25575566291809, "training_acc": 52.0, "val_loss": 17.206180095672607, "val_acc": 56.0}
{"epoch": 43, "training_loss": 69.22644567489624, "training_acc": 52.0, "val_loss": 17.2072634100914, "val_acc": 56.0}
{"epoch": 44, "training_loss": 69.26428556442261, "training_acc": 52.0, "val_loss": 17.21397042274475, "val_acc": 56.0}
{"epoch": 45, "training_loss": 69.21704983711243, "training_acc": 52.0, "val_loss": 17.21883714199066, "val_acc": 56.0}
{"epoch": 46, "training_loss": 69.25908017158508, "training_acc": 52.0, "val_loss": 17.225299775600433, "val_acc": 56.0}
{"epoch": 47, "training_loss": 69.26253867149353, "training_acc": 52.0, "val_loss": 17.23197102546692, "val_acc": 56.0}
{"epoch": 48, "training_loss": 69.219895362854, "training_acc": 52.0, "val_loss": 17.23463535308838, "val_acc": 56.0}
{"epoch": 49, "training_loss": 69.25208353996277, "training_acc": 52.0, "val_loss": 17.237208783626556, "val_acc": 56.0}
{"epoch": 50, "training_loss": 69.23775792121887, "training_acc": 52.0, "val_loss": 17.24221706390381, "val_acc": 56.0}
{"epoch": 51, "training_loss": 69.2308247089386, "training_acc": 52.0, "val_loss": 17.247964441776276, "val_acc": 56.0}
{"epoch": 52, "training_loss": 69.24437832832336, "training_acc": 52.0, "val_loss": 17.25267916917801, "val_acc": 56.0}
{"epoch": 53, "training_loss": 69.2492516040802, "training_acc": 52.0, "val_loss": 17.257773876190186, "val_acc": 56.0}
{"epoch": 54, "training_loss": 69.24354219436646, "training_acc": 52.0, "val_loss": 17.26175993680954, "val_acc": 56.0}
{"epoch": 55, "training_loss": 69.21876454353333, "training_acc": 52.0, "val_loss": 17.261309921741486, "val_acc": 56.0}
{"epoch": 56, "training_loss": 69.24798917770386, "training_acc": 52.0, "val_loss": 17.263102531433105, "val_acc": 56.0}
{"epoch": 57, "training_loss": 69.26201581954956, "training_acc": 52.0, "val_loss": 17.265498638153076, "val_acc": 56.0}
{"epoch": 58, "training_loss": 69.24506306648254, "training_acc": 52.0, "val_loss": 17.26365238428116, "val_acc": 56.0}
{"epoch": 59, "training_loss": 69.2368848323822, "training_acc": 52.0, "val_loss": 17.2617107629776, "val_acc": 56.0}
{"epoch": 60, "training_loss": 69.24778008460999, "training_acc": 52.0, "val_loss": 17.255035042762756, "val_acc": 56.0}
{"epoch": 61, "training_loss": 69.23977303504944, "training_acc": 52.0, "val_loss": 17.252276837825775, "val_acc": 56.0}
