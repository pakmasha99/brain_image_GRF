"main_optuna_fix_3.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 80 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_ABCD_fix_3_early_loss --binary_class True --run_where lab --early_criteria loss"
{"epoch": 0, "training_loss": 284.3363342285156, "training_acc": 41.25, "val_loss": 6.002137768761754e+17, "val_acc": 50.0, "val_auroc": 0.68, "time": 10.49}
{"epoch": 1, "training_loss": 1.9597665020768522e+18, "training_acc": 43.75, "val_loss": 441414451200.0, "val_acc": 50.0, "val_auroc": 0.34, "time": 19.95}
{"epoch": 2, "training_loss": 2100652605440.0, "training_acc": 46.25, "val_loss": 14538445946880.0, "val_acc": 50.0, "val_auroc": 0.66, "time": 28.99}
{"epoch": 3, "training_loss": 41502760677888.0, "training_acc": 53.75, "val_loss": 85156121600.0, "val_acc": 50.0, "val_auroc": 0.35, "time": 38.67}
{"epoch": 4, "training_loss": 304168077312.0, "training_acc": 46.25, "val_loss": 3907815360.0, "val_acc": 50.0, "val_auroc": 0.31, "time": 47.77}
{"epoch": 5, "training_loss": 14771533568.0, "training_acc": 46.25, "val_loss": 2031548640.0, "val_acc": 50.0, "val_auroc": 0.68, "time": 57.14}
{"epoch": 6, "training_loss": 8303568384.0, "training_acc": 53.75, "val_loss": 37702156800.0, "val_acc": 50.0, "val_auroc": 0.39, "time": 66.41}
{"epoch": 7, "training_loss": 135390004864.0, "training_acc": 46.25, "val_loss": 678315760.0, "val_acc": 50.0, "val_auroc": 0.42, "time": 75.63}
{"epoch": 8, "training_loss": 7516204800.0, "training_acc": 46.25, "val_loss": 499333040.0, "val_acc": 50.0, "val_auroc": 0.32, "time": 86.66}
{"epoch": 9, "training_loss": 4144888832.0, "training_acc": 46.25, "val_loss": 3449148480.0, "val_acc": 50.0, "val_auroc": 0.31, "time": 95.99}
{"epoch": 10, "training_loss": 11805522592.0, "training_acc": 48.75, "val_loss": 933598240.0, "val_acc": 50.0, "val_auroc": 0.3, "time": 104.44}
{"epoch": 11, "training_loss": 3587603840.0, "training_acc": 51.25, "val_loss": 639786000.0, "val_acc": 50.0, "val_auroc": 0.68, "time": 113.32}
{"epoch": 12, "training_loss": 5240551424.0, "training_acc": 48.75, "val_loss": 68918640640.0, "val_acc": 50.0, "val_auroc": 0.66, "time": 122.52}
{"epoch": 13, "training_loss": 217413667968.0, "training_acc": 53.75, "val_loss": 18064632320.0, "val_acc": 50.0, "val_auroc": 0.35, "time": 131.15}
{"epoch": 14, "training_loss": 56099583744.0, "training_acc": 46.25, "val_loss": 618266880.0, "val_acc": 50.0, "val_auroc": 0.63, "time": 140.48}
{"epoch": 15, "training_loss": 2456391232.0, "training_acc": 48.75, "val_loss": 347634520.0, "val_acc": 50.0, "val_auroc": 0.66, "time": 150.08}
{"epoch": 16, "training_loss": 2701282944.0, "training_acc": 48.75, "val_loss": 40428487.5, "val_acc": 50.0, "val_auroc": 0.7, "time": 159.3}
{"epoch": 17, "training_loss": 202435824.0, "training_acc": 46.25, "val_loss": 14137438.75, "val_acc": 50.0, "val_auroc": 0.39, "time": 168.47}
{"epoch": 18, "training_loss": 9129330376.0, "training_acc": 50.0, "val_loss": 70776195.0, "val_acc": 50.0, "val_auroc": 0.66, "time": 177.76}
{"epoch": 19, "training_loss": 968328666528.0, "training_acc": 62.5, "val_loss": 1862306080.0, "val_acc": 50.0, "val_auroc": 0.24, "time": 187.32}
{"epoch": 20, "training_loss": 37975879168.0, "training_acc": 53.75, "val_loss": 4253894246400.0, "val_acc": 50.0, "val_auroc": 0.13, "time": 196.28}
{"epoch": 21, "training_loss": 12922870146688.0, "training_acc": 53.75, "val_loss": 2091641440.0, "val_acc": 50.0, "val_auroc": 0.34, "time": 205.35}
{"epoch": 22, "training_loss": 122397001216.0, "training_acc": 51.25, "val_loss": 915671859200.0, "val_acc": 50.0, "val_auroc": 0.32, "time": 214.36}
{"epoch": 23, "training_loss": 19398416400384.0, "training_acc": 46.25, "val_loss": 17726046720.0, "val_acc": 50.0, "val_auroc": 0.61, "time": 223.43}
{"epoch": 24, "training_loss": 61611252672.0, "training_acc": 46.25, "val_loss": 13163120640.0, "val_acc": 50.0, "val_auroc": 0.5, "time": 232.91}
{"epoch": 25, "training_loss": 599123517440.0, "training_acc": 46.25, "val_loss": 2583253975040.0, "val_acc": 50.0, "val_auroc": 0.68, "time": 241.85}
{"epoch": 26, "training_loss": 7678115532160.0, "training_acc": 51.25, "val_loss": 10824940800.0, "val_acc": 50.0, "val_auroc": 0.37, "time": 250.52}
{"epoch": 27, "training_loss": 43208304640.0, "training_acc": 46.25, "val_loss": 1898637280.0, "val_acc": 50.0, "val_auroc": 0.48, "time": 259.4}
{"epoch": 28, "training_loss": 7795666432.0, "training_acc": 43.75, "val_loss": 8523429120.0, "val_acc": 50.0, "val_auroc": 0.3, "time": 268.3}
{"epoch": 29, "training_loss": 30789470976.0, "training_acc": 48.75, "val_loss": 4085350080.0, "val_acc": 50.0, "val_auroc": 0.7, "time": 277.01}
{"epoch": 30, "training_loss": 13101974592.0, "training_acc": 53.75, "val_loss": 7546169600.0, "val_acc": 50.0, "val_auroc": 0.44, "time": 285.64}
{"epoch": 31, "training_loss": 41183223808.0, "training_acc": 53.75, "val_loss": 199290571980800.0, "val_acc": 50.0, "val_auroc": 0.69, "time": 294.26}
{"epoch": 32, "training_loss": 572713938516992.0, "training_acc": 53.75, "val_loss": 94572980.0, "val_acc": 50.0, "val_auroc": 0.34, "time": 303.18}
{"epoch": 33, "training_loss": 38033534560.0, "training_acc": 48.75, "val_loss": 91585566720.0, "val_acc": 50.0, "val_auroc": 0.3, "time": 311.93}
{"epoch": 34, "training_loss": 305429055488.0, "training_acc": 46.25, "val_loss": 1487691840.0, "val_acc": 50.0, "val_auroc": 0.55, "time": 320.7}
{"epoch": 35, "training_loss": 45901769728.0, "training_acc": 43.75, "val_loss": 80581073920.0, "val_acc": 50.0, "val_auroc": 0.36, "time": 329.6}
{"epoch": 36, "training_loss": 308878376960.0, "training_acc": 46.25, "val_loss": 5325916160.0, "val_acc": 50.0, "val_auroc": 0.72, "time": 338.56}
