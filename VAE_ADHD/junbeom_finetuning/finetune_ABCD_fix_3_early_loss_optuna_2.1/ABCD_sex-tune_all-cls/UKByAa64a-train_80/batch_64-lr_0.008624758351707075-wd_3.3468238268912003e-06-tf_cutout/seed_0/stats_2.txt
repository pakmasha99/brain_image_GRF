"main_optuna_fix_3.py --pretrained_path /scratch/connectome/study_group/VAE_ADHD/junbeom_weights/UKByAa64a.pth --mode finetuning --train_num 80 --layer_control tune_all --stratify strat --random_seed 0 --task ABCD_sex --input_option yAware --batch_size 64 --save_path finetune_ABCD_fix_3_early_loss --binary_class True --run_where lab --early_criteria loss"
{"epoch": 0, "training_loss": 303.9465522766113, "training_acc": 42.5, "val_loss": 66324413440.0, "val_acc": 55.0, "val_auroc": 0.364, "time": 12.02}
{"epoch": 1, "training_loss": 233975020974.0, "training_acc": 45.0, "val_loss": 21884225.0, "val_acc": 45.0, "val_auroc": 0.667, "time": 21.64}
{"epoch": 2, "training_loss": 78915525.0, "training_acc": 50.0, "val_loss": 57063.9208984375, "val_acc": 45.0, "val_auroc": 0.495, "time": 30.9}
{"epoch": 3, "training_loss": 273433.71875, "training_acc": 47.5, "val_loss": 84886.50390625, "val_acc": 55.0, "val_auroc": 0.414, "time": 40.28}
{"epoch": 4, "training_loss": 3105341.03125, "training_acc": 52.5, "val_loss": 32475.5322265625, "val_acc": 55.0, "val_auroc": 0.394, "time": 49.39}
{"epoch": 5, "training_loss": 196311.875, "training_acc": 52.5, "val_loss": 4883.349609375, "val_acc": 55.0, "val_auroc": 0.566, "time": 58.37}
{"epoch": 6, "training_loss": 970145.46875, "training_acc": 42.5, "val_loss": 63074.345703125, "val_acc": 55.0, "val_auroc": 0.495, "time": 67.06}
{"epoch": 7, "training_loss": 341890.5546875, "training_acc": 52.5, "val_loss": 46869.5654296875, "val_acc": 55.0, "val_auroc": 0.717, "time": 76.36}
{"epoch": 8, "training_loss": 152709.0771484375, "training_acc": 52.5, "val_loss": 6962.071533203125, "val_acc": 45.0, "val_auroc": 0.596, "time": 85.27}
{"epoch": 9, "training_loss": 26628.8984375, "training_acc": 47.5, "val_loss": 7629.8748779296875, "val_acc": 55.0, "val_auroc": 0.455, "time": 94.48}
{"epoch": 10, "training_loss": 27546.713989257812, "training_acc": 52.5, "val_loss": 7268.460693359375, "val_acc": 45.0, "val_auroc": 0.566, "time": 103.28}
{"epoch": 11, "training_loss": 26384.1767578125, "training_acc": 47.5, "val_loss": 23077.54638671875, "val_acc": 55.0, "val_auroc": 0.485, "time": 112.19}
{"epoch": 12, "training_loss": 82222.71484375, "training_acc": 52.5, "val_loss": 2657.93212890625, "val_acc": 55.0, "val_auroc": 0.424, "time": 121.32}
{"epoch": 13, "training_loss": 17991.2978515625, "training_acc": 52.5, "val_loss": 2144.494171142578, "val_acc": 45.0, "val_auroc": 0.636, "time": 130.6}
{"epoch": 14, "training_loss": 16479.65625, "training_acc": 47.5, "val_loss": 8492.073364257812, "val_acc": 45.0, "val_auroc": 0.434, "time": 139.21}
{"epoch": 15, "training_loss": 27141.927001953125, "training_acc": 55.0, "val_loss": 3921.3751220703125, "val_acc": 55.0, "val_auroc": 0.616, "time": 148.54}
{"epoch": 16, "training_loss": 110227.7119140625, "training_acc": 45.0, "val_loss": 4811.346740722656, "val_acc": 55.0, "val_auroc": 0.515, "time": 157.33}
{"epoch": 17, "training_loss": 16921.441162109375, "training_acc": 52.5, "val_loss": 1084.842529296875, "val_acc": 55.0, "val_auroc": 0.374, "time": 166.48}
{"epoch": 18, "training_loss": 3702.969940185547, "training_acc": 52.5, "val_loss": 1657.1299743652344, "val_acc": 55.0, "val_auroc": 0.434, "time": 175.21}
{"epoch": 19, "training_loss": 24226.91796875, "training_acc": 52.5, "val_loss": 61337.5048828125, "val_acc": 55.0, "val_auroc": 0.414, "time": 184.35}
{"epoch": 20, "training_loss": 205517.54943847656, "training_acc": 52.5, "val_loss": 4397.632751464844, "val_acc": 40.0, "val_auroc": 0.606, "time": 193.08}
{"epoch": 21, "training_loss": 88106782.0546875, "training_acc": 53.75, "val_loss": 115272.177734375, "val_acc": 45.0, "val_auroc": 0.626, "time": 202.18}
{"epoch": 22, "training_loss": 629647.6875, "training_acc": 47.5, "val_loss": 271219.31640625, "val_acc": 55.0, "val_auroc": 0.384, "time": 210.83}
{"epoch": 23, "training_loss": 1043737.234375, "training_acc": 50.0, "val_loss": 299732.55859375, "val_acc": 45.0, "val_auroc": 0.545, "time": 219.64}
{"epoch": 24, "training_loss": 1904850.75, "training_acc": 47.5, "val_loss": 3239903.4375, "val_acc": 55.0, "val_auroc": 0.657, "time": 228.25}
{"epoch": 25, "training_loss": 11543186.5625, "training_acc": 52.5, "val_loss": 9255.734252929688, "val_acc": 45.0, "val_auroc": 0.667, "time": 237.25}
{"epoch": 26, "training_loss": 42204.990234375, "training_acc": 47.5, "val_loss": 83378.349609375, "val_acc": 55.0, "val_auroc": 0.556, "time": 246.2}
{"epoch": 27, "training_loss": 289600.3525390625, "training_acc": 50.0, "val_loss": 25879.19921875, "val_acc": 55.0, "val_auroc": 0.485, "time": 255.15}
{"epoch": 28, "training_loss": 88171.4111328125, "training_acc": 52.5, "val_loss": 97232.001953125, "val_acc": 45.0, "val_auroc": 0.545, "time": 263.73}
{"epoch": 29, "training_loss": 312132.04296875, "training_acc": 47.5, "val_loss": 56522.626953125, "val_acc": 45.0, "val_auroc": 0.515, "time": 273.09}
{"epoch": 30, "training_loss": 186415.591796875, "training_acc": 50.0, "val_loss": 10261.94580078125, "val_acc": 55.0, "val_auroc": 0.545, "time": 282.2}
{"epoch": 31, "training_loss": 62076.21484375, "training_acc": 57.5, "val_loss": 21210.01708984375, "val_acc": 45.0, "val_auroc": 0.475, "time": 291.24}
{"epoch": 32, "training_loss": 68008.11279296875, "training_acc": 45.0, "val_loss": 5574.130859375, "val_acc": 45.0, "val_auroc": 0.505, "time": 300.01}
{"epoch": 33, "training_loss": 21443.3701171875, "training_acc": 47.5, "val_loss": 5822.3846435546875, "val_acc": 55.0, "val_auroc": 0.636, "time": 310.89}
{"epoch": 34, "training_loss": 20275.99151611328, "training_acc": 52.5, "val_loss": 33574.74365234375, "val_acc": 45.0, "val_auroc": 0.495, "time": 319.91}
{"epoch": 35, "training_loss": 108280.150390625, "training_acc": 47.5, "val_loss": 8679.508666992188, "val_acc": 55.0, "val_auroc": 0.485, "time": 328.63}
{"epoch": 36, "training_loss": 30219.980834960938, "training_acc": 52.5, "val_loss": 6632.0745849609375, "val_acc": 45.0, "val_auroc": 0.434, "time": 337.56}
