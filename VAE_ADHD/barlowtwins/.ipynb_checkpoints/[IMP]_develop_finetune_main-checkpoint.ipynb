{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6163c84c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103e7578",
   "metadata": {},
   "source": [
    "### 참고로 쌤이 run 할떄 쓰던 script\n",
    "```\n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH --job-name ADNI_eval  #job name을 다르게 하기 위해서\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --nodelist=node3 #used node4\n",
    "#SBATCH -t 10:00:00 # Time for running job #길게 10일넘게 잡음\n",
    "#SBATCH -o ./output/ADCN_yaware_epoch99_noCV.txt\n",
    "#SBATCH -e ./error/error_%J.error\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --mem-per-cpu=20000MB\n",
    "#SBATCH --gpus=2\n",
    "#SBATCH --cpus-per-task=8\n",
    "\n",
    "######################################################################################\n",
    "source /home/connectome/mieuxmin/.bashrc\n",
    "\n",
    "echo \"Used (y-Aware_Contrastive_MRI_epoch_99) for pretrained weights.\"\n",
    "#echo \"Used self.model='DenseNet', self.nb_epochs=100, self.tf='cutout', self.batch_size = 8\"\n",
    "echo \"self.input_size=(1,80,80,80)<- 얘는 그렇고, 바꾸기 전 버전임., self.lr=1e-4, self.weight_decay=5e-5, self.patience=20\"\n",
    "echo \"\"\n",
    "echo \"--train_num 100 & --layer_control tune_all\"\n",
    "python3 main.py --pretrained_path ./weights/y-Aware_Contrastive_MRI_epoch_99.pth --mode finetuning --train_num 100 --task_name AD/MCI --layer_control tune_all --stratify balan --random_seed 0\n",
    "echo \"--train_num 100 & --layer_control freeze\"\n",
    "python3 main.py --pretrained_path ./weights/y-Aware_Contrastive_MRI_epoch_99.pth --mode finetuning --train_num 100 --task_name AD/MCI --layer_control freeze --stratify balan --random_seed 0\n",
    "echo \"--train_num 300 & --layer_control tune_all\"\n",
    "python3 main.py --pretrained_path ./weights/y-Aware_Contrastive_MRI_epoch_99.pth --mode finetuning --train_num 300 --task_name AD/MCI --layer_control tune_all --stratify balan --random_seed 0\n",
    "echo \"--train_num 300 & --layer_control freeze\"\n",
    "python3 main.py --pretrained_path ./weights/y-Aware_Contrastive_MRI_epoch_99.pth --mode finetuning --train_num 300 --task_name AD/MCI --layer_control freeze --stratify balan --random_seed 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed69554",
   "metadata": {},
   "source": [
    "> our version : \n",
    "\n",
    "\n",
    "```bash\n",
    "\n",
    "weight_pth=xXXX\n",
    "layer_control=tune_all #freeze\n",
    "task=XXX\n",
    "\n",
    "python3 finetune.py --pretrained_path $weight_pth --mode finetuning --train_num,  ###적기 \n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8e87e9",
   "metadata": {},
   "source": [
    "### 밑 : the dataset.py that junbeom's code uses... let's try to modify ourse to fit this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4907d286",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmath import nan\n",
    "\n",
    "PRETRAINING = 0\n",
    "FINE_TUNING = 1\n",
    "\n",
    "class Config:\n",
    "\n",
    "    def __init__(self, mode):\n",
    "        assert mode in {PRETRAINING, FINE_TUNING}, \"Unknown mode: %i\"%mode\n",
    "\n",
    "        self.mode = mode\n",
    "\n",
    "        if self.mode == PRETRAINING:\n",
    "            self.batch_size = 8 # ADNI\n",
    "            self.nb_epochs_per_saving = 1\n",
    "            self.pin_mem = True\n",
    "            self.num_cpu_workers = 8\n",
    "            self.nb_epochs = 100 # ADNI #####\n",
    "            self.cuda = True\n",
    "            # Optimizer\n",
    "            self.lr = 1e-4\n",
    "            self.weight_decay = 5e-5\n",
    "            # Hyperparameters for our y-Aware InfoNCE Loss\n",
    "            self.temperature = 0.1\n",
    "            self.tf = 'cutout' # ADNI\n",
    "            self.model = 'DenseNet' # 'UNet'\n",
    "            ### ADNI\n",
    "            self.data = './adni_t1s_baseline' # ADNI\n",
    "            self.label = './csv/fsdat_baseline_CN.csv' # ADNI\n",
    "            self.valid_ratio = 0.25 # ADNI (valid set ratio compared to training set)\n",
    "            self.input_size = (1, 80, 80, 80) # ADNI #####\n",
    "            \n",
    "            self.label_name = ['PTAGE', 'PTGENDER'] # ADNI\n",
    "            self.label_type = ['cont', 'cat'] # ADNI\n",
    "            self.cat_similarity = [nan, 0] # similarity for mismatched categorical meta-data. set nan for continuous meta-data\n",
    "            self.alpha_list = [0.5, 0.5] # ADNI # sum = 1\n",
    "            self.sigma = [5, 5] # ADNI # depends on the meta-data at hand\n",
    "            \n",
    "            self.checkpoint_dir = './ckpts' # ADNI\n",
    "            self.patience = 20 # ADNI\n",
    "\n",
    "        elif self.mode == FINE_TUNING:\n",
    "            ## We assume a classification task here\n",
    "            self.batch_size = 8\n",
    "            self.nb_epochs_per_saving = 10\n",
    "            self.pin_mem = True\n",
    "            self.num_cpu_workers = 1\n",
    "            self.nb_epochs = 100 # ADNI #####\n",
    "            self.cuda = True\n",
    "            # Optimizer\n",
    "            self.lr = 1e-4\n",
    "            self.weight_decay = 5e-5\n",
    "            self.tf = 'cutout' # ADNI\n",
    "            self.model = 'DenseNet' # 'UNet'\n",
    "            ### ADNI\n",
    "            self.data = '/scratch/connectome/study_group/VAE_ADHD/data' #'./adni_t1s_baseline' # ADNI\n",
    "            self.label = '/scratch/connectome/dyhan316/VAE_ADHD/junbeom_finetuning/csv/fsdat_baseline.csv' # ADNI\n",
    "            self.valid_ratio = 0.25 # ADNI (valid set ratio compared to training set)\n",
    "            self.input_size = (1, 80, 80, 80) # ADNI\n",
    "\n",
    "            self.task_type = 'cls' # ADNI # 'cls' or 'reg' #####\n",
    "            self.label_name = 'Dx.new' # ADNI # `Dx.new` #####\n",
    "            self.num_classes = 2 # ADNI - AD vs CN or MCI vs CN or AD vs MCI or reg #####\n",
    "\n",
    "            #self.pretrained_path = './weights/BHByAa64c.pth' # ADNI #####\n",
    "            #self.layer_control = 'tune_all' # ADNI # 'freeze' or 'tune_diff' (whether to freeze pretrained layers or not) #####\n",
    "            self.patience = 20 # ADNI\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2bc84eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#some things to use \n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from augmentations import Transformer, Crop, Cutout, Noise, Normalize, Blur, Flip\n",
    "### ADNI\n",
    "import os\n",
    "import nibabel as nib\n",
    "from skimage.transform import resize\n",
    "\n",
    "class ADNI_Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, config, data_csv, data_type, *args, **kwargs): # ADNI\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        ### ADNI\n",
    "        self.config = config\n",
    "        self.data_type = data_type\n",
    "        self.transforms = Transformer()\n",
    "        self.transforms.register(Normalize(), probability=1.0)\n",
    "\n",
    "        if self.data_type == 'train' or self.config.mode == 0:\n",
    "            if self.config.tf == \"all_tf\":\n",
    "                self.transforms.register(Flip(), probability=0.5)\n",
    "                self.transforms.register(Blur(sigma=(0.1, 1)), probability=0.5)\n",
    "                self.transforms.register(Noise(sigma=(0.1, 1)), probability=0.5)\n",
    "                self.transforms.register(Cutout(patch_size=np.ceil(np.array(self.config.input_size)/4)), probability=0.5)\n",
    "                self.transforms.register(Crop(np.ceil(0.75*np.array(self.config.input_size)), \"random\", resize=True),\n",
    "                                        probability=0.5)\n",
    "\n",
    "            elif self.config.tf == \"cutout\":\n",
    "                self.transforms.register(Cutout(patch_size=np.ceil(np.array(self.config.input_size)/4)), probability=1)\n",
    "\n",
    "            elif self.config.tf == \"crop\":\n",
    "                self.transforms.register(Crop(np.ceil(0.75*np.array(self.config.input_size)), \"random\", resize=True),\n",
    "                                        probability=1)\n",
    "        \n",
    "        self.data_dir = self.config.data ###CHANGED### #'./adni_t1s_baseline'\n",
    "        self.data_csv = data_csv\n",
    "        self.files = [x for x in os.listdir(self.data_dir) if x[4:12] in list(self.data_csv['SubjectID'])]\n",
    "        ###\n",
    "        \n",
    "    def collate_fn(self, list_samples):\n",
    "        list_x = torch.stack([torch.as_tensor(x, dtype=torch.float) for (x, y) in list_samples], dim=0)\n",
    "        list_y = torch.stack([torch.as_tensor(y, dtype=torch.float) for (x, y) in list_samples], dim=0)\n",
    "\n",
    "        return (list_x, list_y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # For a single input x, samples (t, t') ~ T to generate (t(x), t'(x))\n",
    "        ### ADNI\n",
    "        if self.config.mode == 0: # Pre-training # consider multiple labels (list)\n",
    "            labels = []\n",
    "            for label_nm in self.config.label_name: # [\"PTAGE\", \"PTGENDER\"]\n",
    "                labels.append(float(self.data_csv[label_nm].values[idx]))\n",
    "            labels = tuple(labels)\n",
    "        else: # Fine-tuning\n",
    "            labels = self.data_csv[self.config.label_name].values[idx]\n",
    "        SubjectID = self.data_csv['SubjectID'].values[idx]\n",
    "        file_match = [file for file in self.files if SubjectID in file]\n",
    "        path = os.path.join(self.data_dir, file_match[0])\n",
    "        img = nib.load(os.path.join(path, 'brain_to_MNI_nonlin.nii.gz'))\n",
    "        img = np.swapaxes(img.get_data(),1,2)\n",
    "        img = np.flip(img,1)\n",
    "        img = np.flip(img,2)\n",
    "        img = resize(img, (self.config.input_size[1], self.config.input_size[2], self.config.input_size[3]), mode='constant')\n",
    "        img = torch.from_numpy(img).float().view(self.config.input_size[0], self.config.input_size[1], self.config.input_size[2], self.config.input_size[3])\n",
    "        img = img.numpy()\n",
    "        \n",
    "        np.random.seed()\n",
    "        if self.config.mode == 0: # Pre-training\n",
    "            x1 = self.transforms(img)\n",
    "            x2 = self.transforms(img)\n",
    "            x = np.stack((x1, x2), axis=0)\n",
    "        else: # Fine-tuning\n",
    "            x = self.transforms(img)\n",
    "        ###\n",
    "\n",
    "        return (x, labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0db953b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch/connectome/study_group/VAE_ADHD/data'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config(mode = FINE_TUNING)\n",
    "config.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0042d62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args(): \n",
    "    def __init__(self):\n",
    "        self.mode = \"finetuning\"\n",
    "        self.train_num = 100\n",
    "        self.task_name = \"MCI/CN\"\n",
    "        self.layer_control = \"tune_all\" #freeze\n",
    "        self.stratify = \"balan\"\n",
    "        self.rancom_seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db2e0d16",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m##THE THREE KEY THINGS THAT ARE USED!!! (the pandas directory, the name of the task, and the task itself)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#done so that even if there's like three task possibles for a given task (ex : Dx.new), we can still choose two out of those three to do classification\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m config\u001b[38;5;241m.\u001b[39mlabel, config\u001b[38;5;241m.\u001b[39mlabel_name, \u001b[43margs\u001b[49m\u001b[38;5;241m.\u001b[39mtask_name\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "##THE THREE KEY THINGS THAT ARE USED!!! (the pandas directory, the name of the task, and the task itself)\n",
    "#done so that even if there's like three task possibles for a given task (ex : Dx.new), we can still choose two out of those three to do classification\n",
    "config.label, config.label_name, args.task_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6fdedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_name)\n",
    "labels.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956e421e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "labels['Dx.new'] #config.label_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca35a256",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels['Dx.new']== \"CN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09613dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"./csv/BT_ABCD_ADHD_sex_edited.csv\"\n",
    "label_name = \"sex\"\n",
    "task_name = \"M/W\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02ee5ac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>subjectkey</th>\n",
       "      <th>sex</th>\n",
       "      <th>HCvsADHD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NDARINV0A4P0LWM</td>\n",
       "      <td>M</td>\n",
       "      <td>SecHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>NDARINV0CBFTKR7</td>\n",
       "      <td>M</td>\n",
       "      <td>SecHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>NDARINV0E4CT74P</td>\n",
       "      <td>M</td>\n",
       "      <td>HC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>NDARINV0H2AWWPU</td>\n",
       "      <td>M</td>\n",
       "      <td>SecHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>NDARINV0J1M2ETU</td>\n",
       "      <td>M</td>\n",
       "      <td>SecHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11281</th>\n",
       "      <td>11610</td>\n",
       "      <td>NDARINVZR16R6Y3</td>\n",
       "      <td>M</td>\n",
       "      <td>HC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11282</th>\n",
       "      <td>11611</td>\n",
       "      <td>NDARINVZRR4D9LW</td>\n",
       "      <td>M</td>\n",
       "      <td>SecHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11285</th>\n",
       "      <td>11614</td>\n",
       "      <td>NDARINVZW8G4W5A</td>\n",
       "      <td>M</td>\n",
       "      <td>HC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11287</th>\n",
       "      <td>11617</td>\n",
       "      <td>NDARINVZWP0XZ9A</td>\n",
       "      <td>M</td>\n",
       "      <td>SecHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11290</th>\n",
       "      <td>11620</td>\n",
       "      <td>NDARINVZYRTFYRP</td>\n",
       "      <td>M</td>\n",
       "      <td>ADHD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5934 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0       subjectkey sex HCvsADHD\n",
       "0               0  NDARINV0A4P0LWM   M    SecHC\n",
       "5               5  NDARINV0CBFTKR7   M    SecHC\n",
       "7               7  NDARINV0E4CT74P   M       HC\n",
       "10             10  NDARINV0H2AWWPU   M    SecHC\n",
       "12             12  NDARINV0J1M2ETU   M    SecHC\n",
       "...           ...              ...  ..      ...\n",
       "11281       11610  NDARINVZR16R6Y3   M       HC\n",
       "11282       11611  NDARINVZRR4D9LW   M    SecHC\n",
       "11285       11614  NDARINVZW8G4W5A   M       HC\n",
       "11287       11617  NDARINVZWP0XZ9A   M    SecHC\n",
       "11290       11620  NDARINVZYRTFYRP   M     ADHD\n",
       "\n",
       "[5934 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = pd.read_csv(label)\n",
    "task_include = task_name.split('/')\n",
    "\n",
    "labels[labels[label_name] == task_include[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6bca7059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ADHD', 'HC', 'SecHC'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(labels[\"HCvsADHD\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c20d291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>subjectkey</th>\n",
       "      <th>sex</th>\n",
       "      <th>HCvsADHD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NDARINV0A4P0LWM</td>\n",
       "      <td>M</td>\n",
       "      <td>SecHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NDARINV0A6WVRZY</td>\n",
       "      <td>W</td>\n",
       "      <td>SecHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>NDARINV0A86UD86</td>\n",
       "      <td>W</td>\n",
       "      <td>SecHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>NDARINV0AU5R8NA</td>\n",
       "      <td>W</td>\n",
       "      <td>SecHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>NDARINV0BEPJHU1</td>\n",
       "      <td>W</td>\n",
       "      <td>SecHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11287</th>\n",
       "      <td>11617</td>\n",
       "      <td>NDARINVZWP0XZ9A</td>\n",
       "      <td>M</td>\n",
       "      <td>SecHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11288</th>\n",
       "      <td>11618</td>\n",
       "      <td>NDARINVZWWDT1TG</td>\n",
       "      <td>W</td>\n",
       "      <td>SecHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11289</th>\n",
       "      <td>11619</td>\n",
       "      <td>NDARINVZY3TE53A</td>\n",
       "      <td>W</td>\n",
       "      <td>SecHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11290</th>\n",
       "      <td>11620</td>\n",
       "      <td>NDARINVZYRTFYRP</td>\n",
       "      <td>M</td>\n",
       "      <td>ADHD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11291</th>\n",
       "      <td>11621</td>\n",
       "      <td>NDARINVZZZ2ALR6</td>\n",
       "      <td>W</td>\n",
       "      <td>SecHC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11292 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0       subjectkey sex HCvsADHD\n",
       "0               0  NDARINV0A4P0LWM   M    SecHC\n",
       "1               1  NDARINV0A6WVRZY   W    SecHC\n",
       "2               2  NDARINV0A86UD86   W    SecHC\n",
       "3               3  NDARINV0AU5R8NA   W    SecHC\n",
       "4               4  NDARINV0BEPJHU1   W    SecHC\n",
       "...           ...              ...  ..      ...\n",
       "11287       11617  NDARINVZWP0XZ9A   M    SecHC\n",
       "11288       11618  NDARINVZWWDT1TG   W    SecHC\n",
       "11289       11619  NDARINVZY3TE53A   W    SecHC\n",
       "11290       11620  NDARINVZYRTFYRP   M     ADHD\n",
       "11291       11621  NDARINVZZZ2ALR6   W    SecHC\n",
       "\n",
       "[11292 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "41a0591e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         True\n",
      "1        False\n",
      "2        False\n",
      "3        False\n",
      "4        False\n",
      "         ...  \n",
      "11287     True\n",
      "11288    False\n",
      "11289    False\n",
      "11290     True\n",
      "11291    False\n",
      "Name: sex, Length: 11292, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "print(labels[label_name] == 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131d7bf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ceecae7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "495a69fa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: Fine-tuning for MCI/CN\n",
      "Task type: cls\n",
      "N = 100\n",
      "Policy: balan\n",
      "\n",
      "Train data info:\n",
      "CN     50\n",
      "MCI    50\n",
      "Name: Dx.new, dtype: int64\n",
      "Total: 100\n",
      "\n",
      "Valid data info:\n",
      "CN     13\n",
      "MCI    12\n",
      "Name: Dx.new, dtype: int64\n",
      "Total: 25\n",
      "\n",
      "Test data info:\n",
      "CN     675\n",
      "MCI    676\n",
      "Name: Dx.new, dtype: int64\n",
      "Total: 1351\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#FROM THE  main.py! \n",
    "args = Args() #brought in parser to here\n",
    "random_seed = 0 #set for things\n",
    "label_name = config.label_name # 'Dx.new'\n",
    "\n",
    "labels = pd.read_csv(config.label)\n",
    "\n",
    "print('Task: Fine-tuning for {0}'.format(args.task_name))\n",
    "print('Task type: {0}'.format(config.task_type))\n",
    "print('N = {0}'.format(args.train_num))\n",
    "\n",
    "if config.task_type == 'cls':\n",
    "    print('Policy: {0}'.format(args.stratify))\n",
    "    task_include = args.task_name.split('/')\n",
    "    #print(task_include)\n",
    "    assert len(task_include) == 2, 'Set two labels.'\n",
    "    assert config.num_classes == 2, 'Set config.num_classes == 2'\n",
    "    data_1 = labels[labels[label_name] == task_include[0]]\n",
    "    data_2 = labels[labels[label_name] == task_include[1]]\n",
    "    if args.stratify == 'strat':\n",
    "        ratio = len(data_1) / (len(data_1) + len(data_2))\n",
    "        len_1_train = round(args.train_num*ratio)\n",
    "        len_2_train = args.train_num - len_1_train\n",
    "        len_1_valid = round(int(args.train_num*config.valid_ratio)*ratio)\n",
    "        len_2_valid = int(args.train_num*config.valid_ratio) - len_1_valid\n",
    "        assert args.train_num*(1+config.valid_ratio) < (len(data_1) + len(data_2)), 'Not enough valid data. Set smaller --train_num or smaller config.valid_ratio in config.py.'\n",
    "        train1, valid1, test1 = np.split(data_1.sample(frac=1, random_state=random_seed), \n",
    "                                         [len_1_train, len_1_train + len_1_valid])\n",
    "        train2, valid2, test2 = np.split(data_2.sample(frac=1, random_state=random_seed), \n",
    "                                         [len_2_train, len_2_train + len_2_valid])\n",
    "        label_train = pd.concat([train1, train2]).sample(frac=1, random_state=random_seed)\n",
    "        label_valid = pd.concat([valid1, valid2]).sample(frac=1, random_state=random_seed)\n",
    "        label_test = pd.concat([test1, test2]).sample(frac=1, random_state=random_seed)\n",
    "        assert len(label_test) >= 100, 'Not enough test data. (Total: {0})'.format(len(label_test))\n",
    "    else: # args.stratify == 'balan'\n",
    "        if len(data_1) <= len(data_2):\n",
    "            limit = len(data_1)\n",
    "        else:\n",
    "            limit = len(data_2)\n",
    "        data_1 = data_1.sample(frac=1, random_state=random_seed)[:limit]\n",
    "        data_2 = data_2.sample(frac=1, random_state=random_seed)[:limit]\n",
    "        len_1_train = round(args.train_num*0.5)\n",
    "        len_2_train = args.train_num - len_1_train\n",
    "        len_1_valid = round(int(args.train_num*config.valid_ratio)*0.5)\n",
    "        len_2_valid = int(args.train_num*config.valid_ratio) - len_1_valid\n",
    "        assert args.train_num*(1+config.valid_ratio) < limit*2, 'Not enough data to make balanced set.'\n",
    "        train1, valid1, test1 = np.split(data_1.sample(frac=1, random_state=random_seed), \n",
    "                                         [len_1_train, len_1_train + len_1_valid])\n",
    "        train2, valid2, test2 = np.split(data_2.sample(frac=1, random_state=random_seed), \n",
    "                                         [len_2_train, len_2_train + len_2_valid])\n",
    "        label_train = pd.concat([train1, train2]).sample(frac=1, random_state=random_seed)\n",
    "        label_valid = pd.concat([valid1, valid2]).sample(frac=1, random_state=random_seed)\n",
    "        label_test = pd.concat([test1, test2]).sample(frac=1, random_state=random_seed)\n",
    "        assert len(label_test) >= 100, 'Not enough test data. (Total: {0})'.format(len(label_test))\n",
    "    print('\\nTrain data info:\\n{0}\\nTotal: {1}\\n'.format(label_train[label_name].value_counts().sort_index(), len(label_train)))\n",
    "    print('Valid data info:\\n{0}\\nTotal: {1}\\n'.format(label_valid[label_name].value_counts().sort_index(), len(label_valid)))\n",
    "    print('Test data info:\\n{0}\\nTotal: {1}\\n'.format(label_test[label_name].value_counts().sort_index(), len(label_test)))\n",
    "    label_train[label_name].replace({task_include[0]: 0, task_include[1]: 1}, inplace=True)\n",
    "    label_valid[label_name].replace({task_include[0]: 0, task_include[1]: 1}, inplace=True)\n",
    "    label_test[label_name].replace({task_include[0]: 0, task_include[1]: 1}, inplace=True)\n",
    "else: # config.task_type = 'reg'\n",
    "    task_include = args.task_name.split('/')\n",
    "    assert len(task_include) == 1, 'Set only one label.'\n",
    "    assert config.num_classes == 1, 'Set config.num_classes == 1'\n",
    "    labels = pd.read_csv(config.label)\n",
    "    labels = labels[(np.abs(stats.zscore(labels[label_name])) < 3)] # remove outliers w.r.t. z-score > 3\n",
    "    assert args.train_num*(1+config.valid_ratio) <= len(labels), 'Not enough valid data. Set smaller --train_num or smaller config.valid_ratio in config.py.'\n",
    "    label_train, label_valid, label_test = np.split(labels.sample(frac=1, random_state=random_seed), \n",
    "                                                    [args.train_num, int(args.train_num*(1+config.valid_ratio))])\n",
    "    \n",
    "    print('\\nTrain data info:\\nTotal: {0}\\n'.format(len(label_train)))\n",
    "    print('Valid data info:\\nTotal: {0}\\n'.format(len(label_valid)))\n",
    "    print('Test data info:\\nTotal: {0}\\n'.format(len(label_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5c1cae07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SubjectID</th>\n",
       "      <th>Dx.new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>002S0295</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002S0413</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002S0559</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002S0685</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>002S0729</td>\n",
       "      <td>MCI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1802</th>\n",
       "      <td>941S6570</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1803</th>\n",
       "      <td>941S6574</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804</th>\n",
       "      <td>941S6575</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1805</th>\n",
       "      <td>941S6580</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1806</th>\n",
       "      <td>941S6581</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1807 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     SubjectID Dx.new\n",
       "0     002S0295     CN\n",
       "1     002S0413     CN\n",
       "2     002S0559     CN\n",
       "3     002S0685     CN\n",
       "4     002S0729    MCI\n",
       "...        ...    ...\n",
       "1802  941S6570     CN\n",
       "1803  941S6574     CN\n",
       "1804  941S6575     CN\n",
       "1805  941S6580     CN\n",
       "1806  941S6581     CN\n",
       "\n",
       "[1807 rows x 2 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[['SubjectID', 'Dx.new']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fd0d4068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=1807, step=1)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaff667f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd750c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124cea15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99d7742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d5d0c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e243b92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221a64c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1646ca4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a20a57d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc967d1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae423501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5e9f0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81913c30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "845f4d4b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3075718483.py, line 157)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [3]\u001b[0;36m\u001b[0m\n\u001b[0;31m    elif config.mode == FINE_TUNING:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "### ADNI\n",
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1, 2, 3'\n",
    "import time\n",
    "import datetime\n",
    "###\n",
    "import numpy as np\n",
    "from dataset import ADNI_Dataset\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler\n",
    "from yAwareContrastiveLearning import yAwareCLModel\n",
    "from losses import GeneralizedSupervisedNTXenLoss\n",
    "from torch.nn import CrossEntropyLoss, MSELoss # ADNI\n",
    "from models.densenet import densenet121\n",
    "\n",
    "import argparse\n",
    "from config import Config, PRETRAINING, FINE_TUNING\n",
    "### ADNI\n",
    "import torch\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import roc_auc_score, mean_absolute_error, mean_squared_error, r2_score\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "###\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "nowDatetime = now.strftime('%Y-%m-%d %H:%M:%S') # ADNI\n",
    "print(\"[main.py started at {0}]\".format(nowDatetime))\n",
    "start_time = time.time() # ADNI\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--pretrained_path\", type=str, required=True, # ADNI\n",
    "                    help=\"Set the pretrained model path.\")   \n",
    "parser.add_argument(\"--mode\", type=str, choices=[\"pretraining\", \"finetuning\"], required=True,\n",
    "                    help=\"Set the training mode. Do not forget to configure config.py accordingly !\")\n",
    "parser.add_argument(\"--train_num\", type=int, required=True, # ADNI\n",
    "                    help=\"Set the number of training samples.\")                        \n",
    "parser.add_argument(\"--task_name\", type=str, required=False, # ADNI\n",
    "                    help=\"Set the name of the fine-tuning task. (e.g. AD/MCI)\")\n",
    "parser.add_argument(\"--layer_control\", type=str, choices=['tune_all', 'freeze', 'tune_diff'], required=False, # ADNI\n",
    "                    help=\"Set pretrained weight layer control option.\")\n",
    "parser.add_argument(\"--stratify\", type=str, choices=[\"strat\", \"balan\"], required=False, # ADNI\n",
    "                    help=\"Set training samples are stratified or balanced for fine-tuning task.\")\n",
    "parser.add_argument(\"--random_seed\", type=int, required=False, default=0, # ADNI\n",
    "                    help=\"Random seed for reproduction.\")\n",
    "args = parser.parse_args()\n",
    "mode = PRETRAINING if args.mode == \"pretraining\" else FINE_TUNING\n",
    "config = Config(mode)\n",
    "pretrained_path = args.pretrained_path\n",
    "print('Pretrained path:', pretrained_path)\n",
    "### ADNI\n",
    "label_name = config.label_name # 'Dx.new'\n",
    "# Control randomness for reproduction\n",
    "if args.random_seed != None:\n",
    "    random_seed = args.random_seed\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "#if config.mode == PRETRAINING:\n",
    "#    data = pd.read_csv(config.label)\n",
    "#    for i in range(len(label_name)): # [\"PTAGE\", \"PTGENDER\"]\n",
    "#        if config.label_type[i] != 'cont': # convert str object to numbers\n",
    "#            data[label_name[i]] = pd.Categorical(data[label_name[i]])\n",
    "#            data[label_name[i]] = data[label_name[i]].cat.codes\n",
    "\n",
    "#    assert args.train_num*(1+config.valid_ratio) <= len(data), 'Not enough valid data. Set smaller --train_num or smaller config.valid_ratio in config.py.'\n",
    "#    label_train, label_valid, label_test = np.split(data.sample(frac=1, random_state=random_seed), \n",
    "#                                                    [args.train_num, int(args.train_num*(1+config.valid_ratio))])\n",
    "#    \n",
    "#    print('Task: Pretraining')\n",
    "#    print('N = {0}'.format(args.train_num))\n",
    "#    print('meta-data: {0}\\n'.format(label_name))\n",
    "#    assert len(label_name) == len(config.alpha_list), 'len(label_name) and len(alpha_list) should match.'\n",
    "#    assert len(label_name) == len(config.label_type), 'len(label_name) and len(label_type) should match.'\n",
    "#    assert len(label_name) == len(config.sigma), 'len(alpha_list) and len(sigma) should match.'\n",
    "#    assert sum(config.alpha_list) == 1.0, 'Sum of alpha list should be 1.'\n",
    "\n",
    "else: # config.mode == FINE_TUNING:\n",
    "    labels = pd.read_csv(config.label)\n",
    "    print('Task: Fine-tuning for {0}'.format(args.task_name))\n",
    "    print('Task type: {0}'.format(config.task_type))\n",
    "    print('N = {0}'.format(args.train_num))\n",
    "    \n",
    "    if config.task_type == 'cls':\n",
    "        print('Policy: {0}'.format(args.stratify))\n",
    "        task_include = args.task_name.split('/')\n",
    "        assert len(task_include) == 2, 'Set two labels.'\n",
    "        assert config.num_classes == 2, 'Set config.num_classes == 2'\n",
    "        data_1 = labels[labels[label_name] == task_include[0]]\n",
    "        data_2 = labels[labels[label_name] == task_include[1]]\n",
    "        if args.stratify == 'strat':\n",
    "            ratio = len(data_1) / (len(data_1) + len(data_2))\n",
    "            len_1_train = round(args.train_num*ratio)\n",
    "            len_2_train = args.train_num - len_1_train\n",
    "            len_1_valid = round(int(args.train_num*config.valid_ratio)*ratio)\n",
    "            len_2_valid = int(args.train_num*config.valid_ratio) - len_1_valid\n",
    "            assert args.train_num*(1+config.valid_ratio) < (len(data_1) + len(data_2)), 'Not enough valid data. Set smaller --train_num or smaller config.valid_ratio in config.py.'\n",
    "            train1, valid1, test1 = np.split(data_1.sample(frac=1, random_state=random_seed), \n",
    "                                             [len_1_train, len_1_train + len_1_valid])\n",
    "            train2, valid2, test2 = np.split(data_2.sample(frac=1, random_state=random_seed), \n",
    "                                             [len_2_train, len_2_train + len_2_valid])\n",
    "            label_train = pd.concat([train1, train2]).sample(frac=1, random_state=random_seed)\n",
    "            label_valid = pd.concat([valid1, valid2]).sample(frac=1, random_state=random_seed)\n",
    "            label_test = pd.concat([test1, test2]).sample(frac=1, random_state=random_seed)\n",
    "            assert len(label_test) >= 100, 'Not enough test data. (Total: {0})'.format(len(label_test))\n",
    "        else: # args.stratify == 'balan'\n",
    "            if len(data_1) <= len(data_2):\n",
    "                limit = len(data_1)\n",
    "            else:\n",
    "                limit = len(data_2)\n",
    "            data_1 = data_1.sample(frac=1, random_state=random_seed)[:limit]\n",
    "            data_2 = data_2.sample(frac=1, random_state=random_seed)[:limit]\n",
    "            len_1_train = round(args.train_num*0.5)\n",
    "            len_2_train = args.train_num - len_1_train\n",
    "            len_1_valid = round(int(args.train_num*config.valid_ratio)*0.5)\n",
    "            len_2_valid = int(args.train_num*config.valid_ratio) - len_1_valid\n",
    "            assert args.train_num*(1+config.valid_ratio) < limit*2, 'Not enough data to make balanced set.'\n",
    "            train1, valid1, test1 = np.split(data_1.sample(frac=1, random_state=random_seed), \n",
    "                                             [len_1_train, len_1_train + len_1_valid])\n",
    "            train2, valid2, test2 = np.split(data_2.sample(frac=1, random_state=random_seed), \n",
    "                                             [len_2_train, len_2_train + len_2_valid])\n",
    "            label_train = pd.concat([train1, train2]).sample(frac=1, random_state=random_seed)\n",
    "            label_valid = pd.concat([valid1, valid2]).sample(frac=1, random_state=random_seed)\n",
    "            label_test = pd.concat([test1, test2]).sample(frac=1, random_state=random_seed)\n",
    "            assert len(label_test) >= 100, 'Not enough test data. (Total: {0})'.format(len(label_test))\n",
    "        print('\\nTrain data info:\\n{0}\\nTotal: {1}\\n'.format(label_train[label_name].value_counts().sort_index(), len(label_train)))\n",
    "        print('Valid data info:\\n{0}\\nTotal: {1}\\n'.format(label_valid[label_name].value_counts().sort_index(), len(label_valid)))\n",
    "        print('Test data info:\\n{0}\\nTotal: {1}\\n'.format(label_test[label_name].value_counts().sort_index(), len(label_test)))\n",
    "        label_train[label_name].replace({task_include[0]: 0, task_include[1]: 1}, inplace=True)\n",
    "        label_valid[label_name].replace({task_include[0]: 0, task_include[1]: 1}, inplace=True)\n",
    "        label_test[label_name].replace({task_include[0]: 0, task_include[1]: 1}, inplace=True)\n",
    "    else: # config.task_type = 'reg'\n",
    "        task_include = args.task_name.split('/')\n",
    "        assert len(task_include) == 1, 'Set only one label.'\n",
    "        assert config.num_classes == 1, 'Set config.num_classes == 1'\n",
    "        labels = pd.read_csv(config.label)\n",
    "        labels = labels[(np.abs(stats.zscore(labels[label_name])) < 3)] # remove outliers w.r.t. z-score > 3\n",
    "        assert args.train_num*(1+config.valid_ratio) <= len(labels), 'Not enough valid data. Set smaller --train_num or smaller config.valid_ratio in config.py.'\n",
    "        label_train, label_valid, label_test = np.split(labels.sample(frac=1, random_state=random_seed), \n",
    "                                                        [args.train_num, int(args.train_num*(1+config.valid_ratio))])\n",
    "        \n",
    "        print('\\nTrain data info:\\nTotal: {0}\\n'.format(len(label_train)))\n",
    "        print('Valid data info:\\nTotal: {0}\\n'.format(len(label_valid)))\n",
    "        print('Test data info:\\nTotal: {0}\\n'.format(len(label_test)))\n",
    "###\n",
    "\n",
    "### ADNI\n",
    "#if config.mode == PRETRAINING:\n",
    "#    dataset_train = ADNI_Dataset(config, label_train, data_type='train')\n",
    "#    dataset_val = ADNI_Dataset(config, label_valid, data_type='valid')\n",
    "#    dataset_test = ADNI_Dataset(config, label_test, data_type='test')\n",
    "elif config.mode == FINE_TUNING:\n",
    "    dataset_train = ADNI_Dataset(config, label_train, data_type='train')\n",
    "    dataset_val = ADNI_Dataset(config, label_valid, data_type='valid')\n",
    "    dataset_test = ADNI_Dataset(config, label_test, data_type='test')\n",
    "###\n",
    "loader_train = DataLoader(dataset_train,\n",
    "                          batch_size=config.batch_size,\n",
    "                          sampler=RandomSampler(dataset_train),\n",
    "                          collate_fn=dataset_train.collate_fn,\n",
    "                          pin_memory=config.pin_mem,\n",
    "                          num_workers=config.num_cpu_workers\n",
    "                          )\n",
    "loader_val = DataLoader(dataset_val,\n",
    "                        batch_size=config.batch_size,\n",
    "                        sampler=RandomSampler(dataset_val),\n",
    "                        collate_fn=dataset_val.collate_fn,\n",
    "                        pin_memory=config.pin_mem,\n",
    "                        num_workers=config.num_cpu_workers\n",
    "                        )\n",
    "### ADNI\n",
    "loader_test = DataLoader(dataset_test,\n",
    "                         batch_size=1,\n",
    "                         sampler=RandomSampler(dataset_test),\n",
    "                         collate_fn=dataset_test.collate_fn,\n",
    "                         pin_memory=config.pin_mem,\n",
    "                         num_workers=config.num_cpu_workers\n",
    "                         )\n",
    "###\n",
    "#if config.mode == PRETRAINING:\n",
    "#    if config.model == \"DenseNet\":\n",
    "#        net = densenet121(mode=\"encoder\", drop_rate=0.0)\n",
    "#    elif config.model == \"UNet\":\n",
    "#        net = UNet(config.num_classes, mode=\"simCLR\") # ?? why num_classes?\n",
    "#    else:\n",
    "#        raise ValueError(\"Unkown model: %s\"%config.model)\n",
    "else: # config.mode == FINETUNING:\n",
    "    if config.model == \"DenseNet\":\n",
    "        net = densenet121(mode=\"classifier\", drop_rate=0.0, num_classes=config.num_classes)\n",
    "    elif config.model == \"UNet\":\n",
    "        net = UNet(config.num_classes, mode=\"classif\")\n",
    "    else:\n",
    "        raise ValueError(\"Unkown model: %s\"%config.model)\n",
    "#if config.mode == PRETRAINING:\n",
    "#    loss = GeneralizedSupervisedNTXenLoss(config=config, # ADNI\n",
    "#                                          temperature=config.temperature,\n",
    "#                                          sigma=config.sigma,\n",
    "#                                          return_logits=True)\n",
    "elif config.mode == FINE_TUNING:\n",
    "    if config.task_type == 'cls':\n",
    "        loss = CrossEntropyLoss()\n",
    "    else: # config.task_type == 'reg': # ADNI\n",
    "        loss = MSELoss()\n",
    "#if config.mode == PRETRAINING:\n",
    "#    model = yAwareCLModel(net, loss, loader_train, loader_val, loader_test, config, \"no\", 0, \"no\", None, pretrained_path) # ADNI\n",
    "else:\n",
    "    model = yAwareCLModel(net, loss, loader_train, loader_val, loader_test, config, args.task_name, args.train_num, args.layer_control, None, pretrained_path) # ADNI\n",
    "#if config.mode == PRETRAINING:\n",
    "#    model.pretraining()\n",
    "else:\n",
    "    outGT, outPRED = model.fine_tuning() # ADNI\n",
    "    #print('outGT:', outGT)\n",
    "    #print('outPRED:', outPRED)\n",
    "\n",
    "### ADNI\n",
    "if config.mode == FINE_TUNING:\n",
    "    if config.task_type == 'cls':\n",
    "        outGTnp = outGT.cpu().numpy()\n",
    "        outPREDnp = outPRED.cpu().numpy()\n",
    "        print('\\n<<< Test Results: AUROC >>>')\n",
    "        outAUROC = []\n",
    "        for i in range(config.num_classes):\n",
    "            outAUROC.append(roc_auc_score(outGTnp[:, i], outPREDnp[:, i]))\n",
    "        aurocMean = np.array(outAUROC).mean()\n",
    "        print('MEAN', ': {:.4f}'.format(aurocMean))\n",
    "        fig, ax = plt.subplots(nrows = 1, ncols = config.num_classes)\n",
    "        ax = ax.flatten()\n",
    "        fig.set_size_inches((config.num_classes * 10, 10))\n",
    "        for i in range(config.num_classes):\n",
    "            print(task_include[i], ': {:.4f}'.format(outAUROC[i]))\n",
    "            fpr, tpr, threshold = metrics.roc_curve(outGT.cpu()[:, i], outPRED.cpu()[:, i])\n",
    "            roc_auc = metrics.auc(fpr, tpr)\n",
    "            ax[i].plot(fpr, tpr, label = 'AUC = %0.2f' % (roc_auc))\n",
    "            ax[i].set_title('ROC for {0}'.format(task_include[i]))\n",
    "            ax[i].legend(loc = 'lower right')\n",
    "            ax[i].plot([0, 1], [0, 1], 'r--')\n",
    "            ax[i].set_xlim([0, 1])\n",
    "            ax[i].set_ylim([0, 1])\n",
    "            ax[i].set_ylabel('True Positive Rate')\n",
    "            ax[i].set_xlabel('False Positive Rate')\n",
    "        \n",
    "        if args.layer_control == 'tune_all':\n",
    "            control = 'a'\n",
    "        elif args.layer_control == 'freeze':\n",
    "            control = 'f'\n",
    "        else:\n",
    "            control = 'd'\n",
    "        plt.savefig('./figs/{0}_ADNI_{1}{2}{3}_{4}_ROC.png'.format(str(datetime.datetime.now())[2:10].replace('-', ''),\n",
    "                                                                   args.task_name.replace('/', ''), \n",
    "                                                                   str(args.train_num)[0], \n",
    "                                                                   args.stratify[0], \n",
    "                                                                   control), dpi = 100)\n",
    "        plt.close()\n",
    "        \n",
    "        ############################# rename stats.txt ###########################################\n",
    "        os.rename('./stats.txt', \"./\"+args.task_name.replace('/', '')+\"_stats.txt\")\n",
    "        #stats_file = open(\"./\"+args.task_name.replace('/', '')+\"_stats.txt\", 'a', buffering=1)\n",
    "        #stats_file.write(\"until here_\"+args.task_name.replace('/', '')+\"_\"+str(args.train_num)[0]+\"_\"+args.stratify[0]+\"_\"+control)\n",
    "        #stats_file.write(\"#########################################################################\")\n",
    "        #stats_file.close()\n",
    "        \n",
    "        \n",
    "    else: # config.task_type == 'reg':\n",
    "        outGTnp = outGT.cpu().numpy()\n",
    "        outPREDnp = outPRED.cpu().numpy()\n",
    "        print('\\n<<< Test Results >>>')\n",
    "        print('MSE: {:.2f}'.format(mean_squared_error(outGTnp, outPREDnp)))\n",
    "        print('MAE: {:.2f}'.format(mean_absolute_error(outGTnp, outPREDnp)))\n",
    "        print('RMSE: {:.2f}'.format(np.sqrt(mean_squared_error(outGTnp, outPREDnp))))\n",
    "        print('R2-score: {:.4f}'.format(r2_score(outGTnp, outPREDnp)))\n",
    "end_time = time.time()\n",
    "print('\\nTotal', round((end_time - start_time) / 60), 'minutes elapsed.')\n",
    "now = datetime.datetime.now()\n",
    "nowDatetime = now.strftime('%Y-%m-%d %H:%M:%S') # ADNI\n",
    "print(\"[main.py finished at {0}]\".format(nowDatetime))\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc69a0a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VAE_3DCNN_older_MONAI_USE_THIS",
   "language": "python",
   "name": "vae_3dcnn_older_monai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
